<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-108" href="#">nips2013-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</h1>
<br/><p>Source: <a title="nips-2013-108-pdf" href="http://papers.nips.cc/paper/5119-error-minimizing-estimates-and-universal-entry-wise-error-bounds-for-low-rank-matrix-completion.pdf">pdf</a></p><p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>Reference: <a title="nips-2013-108-reference" href="../nips2013_reference/nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. [sent-7, score-0.34]
</p><p>2 We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. [sent-8, score-0.435]
</p><p>3 Almost all known methods performing matrix completion are optimization methods such as the max-norm and nuclear norm heuristics [3, 9, 10], or OptSpace [5], to name a few amongst many. [sent-12, score-0.363]
</p><p>4 By analyzing the algebraic-combinatorial structure Matrix Completion, the authors provide algorithms that identify, for any ﬁxed set of observations, exactly the entries that can be, in principle, reconstructed from them. [sent-17, score-0.224]
</p><p>5 Moreover, the theory developed indicates that, when a missing entry can be determined, it can be found by ﬁrst exposing combinatorially-determined polynomial relations between the known entries and the unknown ones and then selecting a common solution. [sent-18, score-0.471]
</p><p>6 †  1  noisy estimation are intimately related: we can treat each polynomial as providing an estimate of the missing entry, and we can then take as our estimate the variance minimizing weighted average. [sent-22, score-0.282]
</p><p>7 This technique also gives a priori lower bounds for a broad class of unbiased single-entry estimators in terms of the combinatorial structure of the observations and the noise model only. [sent-23, score-0.25]
</p><p>8 We depend on rank one only in the sense that we understand the combinatorial-algebraic structure of rank-one-matrix completion exactly, whereas the behavior in higher rank is not yet as well understood. [sent-25, score-0.395]
</p><p>9 In this sense, the present paper is a proof-of-concept for a new approach to estimating and denoising in algebraic settings based on combinatorially enumerating a set of polynomial estimators and then averaging them. [sent-27, score-0.167]
</p><p>10 If A is a partially known matrix, then the mask of A is the mask which has ones in exactly the positions which are known in A and zeros otherwise. [sent-34, score-0.656]
</p><p>11 We will call the unique bipartite graph G(M ) which has M as bipartite adjacency matrix the completion graph of M . [sent-38, score-0.43]
</p><p>12 We will refer to the m vertices of G(M ) corresponding to the rows of M as blue vertices, and to the n vertices of G(M ) corresponding to the columns as red vertices. [sent-39, score-0.226]
</p><p>13 If e = (i, j) is an edge in Km,n (where Km,n is the complete bipartite graph with m blue and n red vertices), we will also write Ae instead of Aij and for any (m ⇥ n) matrix A. [sent-40, score-0.306]
</p><p>14 Let A be a generic1 and partially known (m ⇥ n) matrix of rank r, let M be the mask of A, let i, j be integers. [sent-46, score-0.544]
</p><p>15 Whether Aij is reconstructible (uniquely, or up to ﬁnite choice) depends only on M and the true rank r; in particular, it does not depend on the true A. [sent-47, score-0.231]
</p><p>16 For rank one, as opposed to higher rank, the set of reconstructible entries is easily obtainable from G(M ) by combinatorial means: Theorem 2. [sent-48, score-0.466]
</p><p>17 Let G ✓ Km,n be the completion graph of a partially known (m ⇥ n) matrix A. [sent-52, score-0.338]
</p><p>18 Then the set of uniquely reconstructible entries of A is exactly the set Ae , with e in the transitive closure of G. [sent-53, score-0.383]
</p><p>19 Let P ✓ Km,n (or, C ✓ Km,n ) be a path (or, cycle), with a ﬁxed start and end. [sent-61, score-0.18]
</p><p>20 E + (C) and C) traversed from blue vertex to a red one, and by E (P ) the set of edges traversed from a red vertex to a blue one 2 . [sent-63, score-0.267]
</p><p>21 Let A = Aij be a (m ⇥ n) matrix of rank 1, and identify the entries Aij with the edges of Km,n . [sent-65, score-0.418]
</p><p>22 For an oriented cycle C, we deﬁne the polynomials Y Y PC (A) = Ae Ae , and e2E + (C)  LC (A) =  X  e2E (C)  log Ae  e2E + (C)  X  log Ae ,  e2E (C)  where for negative entries of A, we ﬁx a branch of the complex logarithm. [sent-66, score-0.545]
</p><p>23 Let A = Aij be a generic (m ⇥ n) matrix of rank 1. [sent-69, score-0.202]
</p><p>24 Proof: The determinantal ideal of rank one is a binomial ideal generated by the (2 ⇥ 2) minors of A (where entries of A are considered as variables). [sent-72, score-0.29]
</p><p>25 The minor equations are exactly PC (A), where C is an elementary oriented four-cycle; if C is an elementary 4-cycle, denote its edges by a(C), b(C), c(C), d(C), with E + (C) = {a(C), d(C)}. [sent-73, score-0.277]
</p><p>26 With this equivalence, it is straightforward that, for any oriented cycle D, LD (A) lies in the Z-span of elements of LC (A) and, therefore, formally, X LD (A) = ↵C · LC (A) C2C  with the ↵C 2 Z. [sent-76, score-0.22]
</p><p>27 Let P, Q be two oriented paths in Km,n starting at v and ending at w. [sent-86, score-0.26]
</p><p>28 There is an unknown ﬁxed, rank one, matrix A which is generic, and an (m ⇥ n) mask M 2 {0, 1}m⇥n which is known. [sent-92, score-0.516]
</p><p>29 There is a (stochastic) noise matrix E 2 Rm⇥n whose entries are uncorrelated and which is multiplicatively centered with ﬁnite variance, non-zero3 variance; i. [sent-93, score-0.362]
</p><p>30 That is, the observation is a matrix with entries Aij · Mij · Eij . [sent-99, score-0.274]
</p><p>31 3  The assumption of multiplicative noise is a necessary precaution in order for the presented estimator (and in fact, any estimator) for the missing entries to have bounded variance, as shown in Example 3. [sent-103, score-0.54]
</p><p>32 The previous observation implies that the multiplicative noise model is as powerful as any additive one that allows bounded variance estimates. [sent-106, score-0.282]
</p><p>33 The unique equation between the entries is then A11 A22 = A12 A21 . [sent-110, score-0.181]
</p><p>34 Solving for any entry will have another entry in the denominator, for A example A11 = A122221 . [sent-111, score-0.35]
</p><p>35 Thus we get an estimator for A11 when substituting observed and noisy A entries for A12 , A21 , A22 . [sent-112, score-0.34]
</p><p>36 2  Estimating entries and error bounds  In this section, we construct the unbiased estimator for the entries of a rank-one-matrix with minimal variance. [sent-117, score-0.65]
</p><p>37 We will denote by aij = log Aij and "ij = log Eij the logarithmic entries and noise. [sent-120, score-0.723]
</p><p>38 Thus, for some path P in Km,n we obtain X X LP (A) = ae ae . [sent-121, score-0.572]
</p><p>39 e2E + (P )  e2E (P )  Denote by bij = aij + "ij the logarithmic (observed) entries, and B the (incomplete) matrix which has the (observed) bij as entries. [sent-122, score-0.897]
</p><p>40 Let P be an oriented path in G(M ) starting at v and ending at w. [sent-128, score-0.385]
</p><p>41 Then, X X LP (B) = be be e2E + (P )  e2E (P )  is an unbiased estimator for ax with variance Var(LP (B)) =  P  e2P  e. [sent-129, score-0.356]
</p><p>42 In the following, we will consider the following parametric estimator as a candidate for estimating ae : Notations 3. [sent-132, score-0.322]
</p><p>43 Let P be a basis for the v–w path space and P denote #P by p. [sent-135, score-0.217]
</p><p>44 4 The multiplicative noise assumption causes the observed entries and the true entries to have the same sign. [sent-138, score-0.51]
</p><p>45 The change of sign can be modeled by adding another multiplicative binary random variable in the model which takes values ±1; this adds an independent combinatorial problem for the estimation of the sign which can be done by maximum likelihood. [sent-139, score-0.256]
</p><p>46 >  ↵ · bx ; in particular, X(↵) is an unbiased estimator for bx if and only  We will now show that minimizing the variance of X(↵) can be formulated as a quadratic program with coefﬁcients entirely determined by ax , the measurements be and the graph G(M ). [sent-146, score-0.476]
</p><p>47 The formula will make use of the following path kernel. [sent-148, score-0.207]
</p><p>48 For ﬁxed vertices s and t, an s–t path is the sum of a cycle H1 (G, Z) and ast . [sent-149, score-0.323]
</p><p>49 The s–t path space is the linear span of all the s–t paths. [sent-150, score-0.18]
</p><p>50 For an edge e and a path P , set ce,P = ±1 if e 2 E ± (P ) otherwise ce,P = 0. [sent-155, score-0.237]
</p><p>51 e2Km,n  Under our assumption that Var(be ) > 0 for all e 2 Km,n , the path kernel is positive deﬁnite, since it is a sum of p independent positive semi-deﬁnite functions; in particular, its kernel matrix has full rank. [sent-159, score-0.425]
</p><p>52 Let x = (s, t) be a pair of vertices, and P a basis for the s–t path space in G with p elements. [sent-162, score-0.217]
</p><p>53 Let ⌃ be the p ⇥ p kernel matrix of the path kernel with respect to the basis P. [sent-163, score-0.462]
</p><p>54 In order to determine the minimum of the variance in ↵, consider the Lagrangian ! [sent-169, score-0.164]
</p><p>55 The above setup works in wider generality: (i) if Var(be ) = 0 is allowed and there is an s–t path of all zero variance edges, the path kernel becomes positive semi-deﬁnite; (ii) similarly if P is replaced with any set of paths at all, the same may occur. [sent-174, score-0.625]
</p><p>56 4; (ii) produces the optimal estimator with respect to P, which is optimal provided that P is spanning, and adding paths to P does not make the estimate worse. [sent-176, score-0.181]
</p><p>57 Let Aij be any estimator for an entry Aij of the true matrix that is: (i) unbiased; (ii) a deterministic piecewise smooth function of the observations; (iii) independent of the noise model. [sent-180, score-0.454]
</p><p>58 Here, we prove the special case of log-normal noise, which gives an alternate viewpoint on the path kernel. [sent-185, score-0.21]
</p><p>59 5  Proof: As above, we work with the formal logarithm aij of Aij . [sent-186, score-0.484]
</p><p>60 It then follows that, for any P in the i–j path space, ! [sent-188, score-0.18]
</p><p>61 X LP (B) ⇠ N aij , e e2P  and the kernel matrix ⌃ of the path kernel is the covariance matrix for the LP in our path basis. [sent-189, score-1.155]
</p><p>62 Thus, the estimator A⇤ is the sample mean of the ij coordinates in the new parameterization. [sent-192, score-0.184]
</p><p>63 3  Rank 2 and higher  An estimator for rank 2 and higher, together with a variance analysis, can be constructed similarly once all the solving polynomials are known. [sent-195, score-0.455]
</p><p>64 5] and that they are not necessarily linear in the missing entry Ae . [sent-197, score-0.26]
</p><p>65 4  The algorithms  We now give the algorithms for estimating/denoising entries and computing the variance bounds; an implementation is available from [7]. [sent-200, score-0.315]
</p><p>66 Since the the path matrix C, the path kernel matrix ⌃, and the optimal ↵ are required for both, we show how to compute them ﬁrst. [sent-201, score-0.622]
</p><p>67 We can ﬁnd a basis Algorithm 1 Calculates path kernel ⌃ and ↵. [sent-202, score-0.293]
</p><p>68 Input: index (i, j), an (m ⇥ n) mask M , variances . [sent-203, score-0.402]
</p><p>69 Output: path matrix C, path kernel ⌃ and minimizer ↵. [sent-204, score-0.529]
</p><p>70 1: Find a linearly independent set of paths P in the graph G(M ), starting from i and ending at j. [sent-205, score-0.148]
</p><p>71 Algorithm 2 Calculates a basis P of the path space. [sent-214, score-0.217]
</p><p>72 Output: a basis P for the space of oriented i–j paths. [sent-216, score-0.189]
</p><p>73 6  Algorithms 3 and 4 then can make use of the calculated C, ↵, ⌃ to determine an estimate for any entry Aij and its minimum variance bound. [sent-225, score-0.378]
</p><p>74 Input: index (i, j), an (m ⇥ n) mask M , log-variances , the partially observed and noisy matrix B. [sent-229, score-0.468]
</p><p>75 | component-wise) contains an odd number of entries 1, else . [sent-235, score-0.181]
</p><p>76 Algorithm 4 Determines the variance of the entry log(Aij ). [sent-237, score-0.309]
</p><p>77 Input: index (i, j), an (m ⇥ n) mask M , log-variances . [sent-238, score-0.314]
</p><p>78 The variance bound is relative, due to its multiplicativity, and can be used to approximate absolute bounds when any (in particular not necessarily the one from Algorithm 3) reconstruction estimate b Aij is available. [sent-243, score-0.219]
</p><p>79 Namely, if bij is the estimated variance of the logarithm, we obtain an upper p b b conﬁdence/deviation bound Aij · exp bij for Aij , and a lower conﬁdence/deviation bound p p bij · exp b A bij , corresponding to the log-conﬁdence log Aij ± bij . [sent-244, score-0.963]
</p><p>80 Also note that if Aij is not reconstructible from the mask M , then the deviation bounds will be inﬁnite. [sent-245, score-0.475]
</p><p>81 1  Experiments  Universal error estimates  For three different masks, we calculated the predicted minimum variance for each entry of the mask. [sent-247, score-0.542]
</p><p>82 Figure 1 shows the predicted a-priori minimum variances for each of the masks. [sent-250, score-0.227]
</p><p>83 The structure of the mask affects the expected error. [sent-251, score-0.314]
</p><p>84 Known entries generally have least variance, and it is less than the initial variance of 1, which implies that the (independent) estimates coming from other paths can be used to successfully denoise observed data. [sent-252, score-0.37]
</p><p>85 For unknown entries, the structure of the mask is mirrored in the pattern of the predicted errors; a diffuse mask gives a similar error on each missing entry, while the more structured masks have structured error which is determined by combinatorial properties of the completion graph. [sent-253, score-1.243]
</p><p>86 1  Figure 1: The ﬁgure shows three pairs of masks and predicted variances. [sent-266, score-0.189]
</p><p>87 The left half is the mask which is depicted by red/blue heatmap with red entries known and blue unknown. [sent-268, score-0.616]
</p><p>88 The right half is a multicolor heatmap with color scale, showing the predicted variance of the completion. [sent-269, score-0.288]
</p><p>89 predicted variance  Figure 2: For 10 randomly chosen masks and 50 ⇥ 50 true matrix, matrix completions were performed with Nuclear Norm (green), OptSpace (red), and Algorithm 3 (blue) under multiplicative noise with variance increasing in increments of 0. [sent-287, score-0.698]
</p><p>90 For each completed entry, minimum variances were predicted by Algorithm 4. [sent-289, score-0.254]
</p><p>91 , (log |atrue | log |apredicted |) for an entry a). [sent-293, score-0.204]
</p><p>92 The ﬁgure shows the mean of the errors (second coordinate) of the value pairs with predicted variance (ﬁrst coordinate) in each of the bins, the color corresponds to the particular algorithm; each group of bars is centered on the minimum value of the associated bin. [sent-295, score-0.304]
</p><p>93 2  Inﬂuence of noise level  We generated 10 random mask of size 50 ⇥ 50 with 200 entries sampled uniformly and a random (50 ⇥ 50) matrix of rank one. [sent-297, score-0.757]
</p><p>94 The multiplicative noise was chosen entry-wise independent, with variance i = (i 1)/10 for each entry. [sent-298, score-0.282]
</p><p>95 Figure 2(b) compares the error of each of the methods with the variance predicted by Algorithm 4 each time the noise level changed. [sent-305, score-0.358]
</p><p>96 The ﬁgure shows that for any of the algorithms, the mean of the actual error increases with the predicted error, showing that the error estimate is useful for a-priori prediction of the actual error - independently of the particular algorithm. [sent-306, score-0.274]
</p><p>97 5  Conclusion  In this paper, we have introduced an algebraic combinatorics based method for reconstructing and denoising single entries of an incomplete and noisy matrix, and for calculating conﬁdence bounds of single entry estimations for arbitrary algorithms. [sent-310, score-0.678]
</p><p>98 We have evaluated these methods against stateof-the art matrix completion methods. [sent-311, score-0.27]
</p><p>99 Our method is competitive and yields the ﬁrst known a priori variance bounds for reconstruction. [sent-312, score-0.202]
</p><p>100 The algebraic combinatorial approach for low-rank matrix a completion. [sent-397, score-0.236]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aij', 0.457), ('mask', 0.314), ('var', 0.203), ('ae', 0.196), ('entries', 0.181), ('path', 0.18), ('completion', 0.177), ('entry', 0.175), ('optspace', 0.17), ('bij', 0.16), ('lp', 0.16), ('oriented', 0.152), ('variance', 0.134), ('lc', 0.129), ('estimator', 0.126), ('reconstructible', 0.122), ('predicted', 0.109), ('rank', 0.109), ('matrix', 0.093), ('algebraic', 0.089), ('variances', 0.088), ('multiplicative', 0.088), ('polynomials', 0.086), ('missing', 0.085), ('theran', 0.083), ('masks', 0.08), ('eij', 0.077), ('kernel', 0.076), ('vertices', 0.075), ('kir', 0.073), ('unbiased', 0.068), ('cycle', 0.068), ('pc', 0.068), ('nuclear', 0.061), ('noise', 0.06), ('ly', 0.06), ('ij', 0.058), ('sign', 0.057), ('edge', 0.057), ('paths', 0.055), ('ld', 0.055), ('error', 0.055), ('combinatorial', 0.054), ('tomioka', 0.053), ('ending', 0.053), ('exposition', 0.053), ('denoising', 0.048), ('reconstruction', 0.046), ('reconstructing', 0.046), ('heatmap', 0.045), ('elementary', 0.045), ('url', 0.043), ('reconstructed', 0.043), ('xe', 0.042), ('transitive', 0.042), ('cycles', 0.042), ('blue', 0.041), ('graph', 0.04), ('traversed', 0.04), ('bx', 0.04), ('bipartite', 0.04), ('bounds', 0.039), ('calculated', 0.039), ('closure', 0.038), ('doi', 0.038), ('issn', 0.038), ('basis', 0.037), ('reconstruct', 0.036), ('calculate', 0.036), ('red', 0.035), ('edges', 0.035), ('combinatorics', 0.035), ('calculates', 0.034), ('noisy', 0.033), ('nitesimal', 0.033), ('norm', 0.032), ('incomplete', 0.032), ('output', 0.031), ('errors', 0.031), ('viewpoint', 0.03), ('polynomial', 0.03), ('minimum', 0.03), ('priori', 0.029), ('log', 0.029), ('link', 0.029), ('rp', 0.029), ('ce', 0.028), ('partially', 0.028), ('bins', 0.028), ('uncorrelated', 0.028), ('spanning', 0.028), ('editors', 0.028), ('ax', 0.028), ('null', 0.027), ('completed', 0.027), ('logarithmic', 0.027), ('formula', 0.027), ('mse', 0.027), ('logarithm', 0.027), ('correctness', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="108-tfidf-1" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>2 0.35967728 <a title="108-tfidf-2" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>Author: Dimitris Achlioptas, Zohar S. Karnin, Edo Liberty</p><p>Abstract: We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes A B 2 . For large m n matrices, such that n m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O 1 computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal ofﬂine distribution. Note that the probabilities in the optimal ofﬂine distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model. 1</p><p>3 0.16628601 <a title="108-tfidf-3" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>4 0.16489007 <a title="108-tfidf-4" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>Author: Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees. Our algorithms exploit adaptivity to identify entries that are highly informative for learning the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analyses. In the absence of noise, we show that one can exactly recover a n ⇥ n matrix of rank r from merely ⌦(nr3/2 log(r)) matrix entries. We also show that one can recover an order T tensor using ⌦(nrT 1/2 T 2 log(r)) entries. For noisy recovery, our algorithm consistently estimates a low rank matrix corrupted with noise using ⌦(nr3/2 polylog(n)) entries. We complement our study with simulations that verify our theory and demonstrate the scalability of our algorithms. 1</p><p>5 0.15251127 <a title="108-tfidf-5" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>6 0.12780783 <a title="108-tfidf-6" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>7 0.1249911 <a title="108-tfidf-7" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>8 0.11403497 <a title="108-tfidf-8" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>9 0.11281052 <a title="108-tfidf-9" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>10 0.10710918 <a title="108-tfidf-10" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>11 0.10616366 <a title="108-tfidf-11" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>12 0.099441804 <a title="108-tfidf-12" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>13 0.091360763 <a title="108-tfidf-13" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>14 0.090913571 <a title="108-tfidf-14" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>15 0.090548359 <a title="108-tfidf-15" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>16 0.083585978 <a title="108-tfidf-16" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>17 0.081181772 <a title="108-tfidf-17" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>18 0.080152087 <a title="108-tfidf-18" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>19 0.076340787 <a title="108-tfidf-19" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>20 0.075488567 <a title="108-tfidf-20" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, 0.102), (2, 0.093), (3, 0.144), (4, 0.011), (5, -0.04), (6, -0.009), (7, -0.035), (8, -0.099), (9, 0.014), (10, 0.104), (11, -0.081), (12, 0.054), (13, -0.025), (14, -0.007), (15, -0.007), (16, -0.093), (17, -0.02), (18, -0.108), (19, 0.055), (20, -0.042), (21, -0.219), (22, -0.013), (23, -0.146), (24, -0.106), (25, 0.007), (26, 0.064), (27, -0.284), (28, -0.022), (29, -0.033), (30, -0.017), (31, -0.062), (32, -0.084), (33, -0.007), (34, -0.015), (35, -0.019), (36, 0.043), (37, -0.007), (38, -0.137), (39, -0.032), (40, 0.004), (41, -0.123), (42, -0.036), (43, -0.099), (44, -0.008), (45, -0.038), (46, -0.027), (47, 0.021), (48, 0.017), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95654672 <a title="108-lsi-1" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>2 0.87409806 <a title="108-lsi-2" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>Author: Dimitris Achlioptas, Zohar S. Karnin, Edo Liberty</p><p>Abstract: We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes A B 2 . For large m n matrices, such that n m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O 1 computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal ofﬂine distribution. Note that the probabilities in the optimal ofﬂine distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model. 1</p><p>3 0.82392514 <a title="108-lsi-3" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>4 0.76423073 <a title="108-lsi-4" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>Author: Miao Xu, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: In standard matrix completion theory, it is required to have at least O(n ln2 n) observed entries to perfectly recover a low-rank matrix M of size n × n, leading to a large number of observations when n is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix M can be dramatically reduced to O(ln n). We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning. 1</p><p>5 0.68639487 <a title="108-lsi-5" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>Author: Marco Cuturi</p><p>Abstract: Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms’ dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximumentropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn’s matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classiﬁcation problem.</p><p>6 0.67889464 <a title="108-lsi-6" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>7 0.63683087 <a title="108-lsi-7" href="./nips-2013-What_do_row_and_column_marginals_reveal_about_your_dataset%3F.html">352 nips-2013-What do row and column marginals reveal about your dataset?</a></p>
<p>8 0.62915915 <a title="108-lsi-8" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>9 0.61794305 <a title="108-lsi-9" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>10 0.56066513 <a title="108-lsi-10" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>11 0.55705571 <a title="108-lsi-11" href="./nips-2013-Scoring_Workers_in_Crowdsourcing%3A_How_Many_Control_Questions_are_Enough%3F.html">290 nips-2013-Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?</a></p>
<p>12 0.54021096 <a title="108-lsi-12" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>13 0.5136649 <a title="108-lsi-13" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>14 0.50565058 <a title="108-lsi-14" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>15 0.48870194 <a title="108-lsi-15" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>16 0.48773775 <a title="108-lsi-16" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>17 0.47900879 <a title="108-lsi-17" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>18 0.46971568 <a title="108-lsi-18" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>19 0.45112517 <a title="108-lsi-19" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>20 0.43981045 <a title="108-lsi-20" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.011), (33, 0.107), (34, 0.066), (41, 0.024), (49, 0.024), (56, 0.565), (70, 0.035), (85, 0.026), (89, 0.039), (93, 0.02), (95, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99299508 <a title="108-lda-1" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>Author: Samory Kpotufe, Vikas Garg</p><p>Abstract: We present the ﬁrst result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown H¨ lder-continuity of the regression function at x. The result holds with o high probability simultaneously at all points x in a general metric space X of unknown structure. 1</p><p>2 0.99050534 <a title="108-lda-2" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><p>same-paper 3 0.97909856 <a title="108-lda-3" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>Author: Franz Kiraly, Louis Theran</p><p>Abstract: We propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries. We describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually. In the noiseless case our algorithm is exact. For rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art Nuclear Norm and OptSpace methods. 1</p><p>4 0.96839857 <a title="108-lda-4" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>Author: Tzu-Kuo Huang, Jeff Schneider</p><p>Abstract: Learning dynamic models from observed data has been a central issue in many scientiﬁc studies or engineering tasks. The usual setting is that data are collected sequentially from trajectories of some dynamical system operation. In quite a few modern scientiﬁc modeling tasks, however, it turns out that reliable sequential data are rather difﬁcult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer’s, or certain biological processes. Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze. Inspired by recent advances in spectral learning methods, we propose to study this problem from a different perspective: moment matching and spectral decomposition. Under that framework, we identify reasonable assumptions on the generative process of non-sequence data, and propose learning algorithms based on the tensor decomposition method [2] to provably recover ﬁrstorder Markov models and hidden Markov models. To the best of our knowledge, this is the ﬁrst formal guarantee on learning from non-sequence data. Preliminary simulation results conﬁrm our theoretical ﬁndings. 1</p><p>5 0.96671546 <a title="108-lda-5" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>Author: Dan Russo, Benjamin Van Roy</p><p>Abstract: This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper conﬁdence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for speciﬁc model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. 1</p><p>6 0.95400298 <a title="108-lda-6" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>7 0.9426403 <a title="108-lda-7" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>8 0.93609381 <a title="108-lda-8" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>9 0.92537922 <a title="108-lda-9" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>10 0.89625853 <a title="108-lda-10" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>11 0.89048398 <a title="108-lda-11" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>12 0.88543898 <a title="108-lda-12" href="./nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</a></p>
<p>13 0.86643249 <a title="108-lda-13" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>14 0.86366153 <a title="108-lda-14" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>15 0.85757524 <a title="108-lda-15" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>16 0.85719937 <a title="108-lda-16" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>17 0.85651958 <a title="108-lda-17" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>18 0.84733337 <a title="108-lda-18" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>19 0.84570003 <a title="108-lda-19" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>20 0.83627242 <a title="108-lda-20" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
