<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2013-DESPOT: Online POMDP Planning with Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-79" href="#">nips2013-79</a> knowledge-graph by maker-knowledge-mining</p><h1>79 nips-2013-DESPOT: Online POMDP Planning with Regularization</h1>
<br/><p>Source: <a title="nips-2013-79-pdf" href="http://papers.nips.cc/paper/5189-despot-online-pomdp-planning-with-regularization.pdf">pdf</a></p><p>Author: Adhiraj Somani, Nan Ye, David Hsu, Wee Sun Lee</p><p>Abstract: POMDPs provide a principled framework for planning under uncertainty, but are computationally intractable, due to the “curse of dimensionality” and the “curse of history”. This paper presents an online POMDP algorithm that alleviates these difﬁculties by focusing the search on a set of randomly sampled scenarios. A Determinized Sparse Partially Observable Tree (DESPOT) compactly captures the execution of all policies on these scenarios. Our Regularized DESPOT (R-DESPOT) algorithm searches the DESPOT for a policy, while optimally balancing the size of the policy and its estimated value obtained under the sampled scenarios. We give an output-sensitive performance bound for all policies derived from a DESPOT, and show that R-DESPOT works well if a small optimal policy exists. We also give an anytime algorithm that approximates R-DESPOT. Experiments show strong results, compared with two of the fastest online POMDP algorithms. Source code along with experimental settings are available at http://bigbird.comp. nus.edu.sg/pmwiki/farm/appl/. 1</p><p>Reference: <a title="nips-2013-79-reference" href="../nips2013_reference/nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('despot', 0.681), ('policy', 0.403), ('pomdp', 0.316), ('pomcp', 0.213), ('plan', 0.138), ('scenario', 0.126), ('believ', 0.123), ('nod', 0.121), ('branch', 0.103), ('ag', 0.097), ('tag', 0.092), ('partic', 0.092), ('pocm', 0.085), ('onlin', 0.076), ('anytim', 0.076), ('default', 0.074), ('sarsop', 0.071), ('discount', 0.07), ('search', 0.061), ('tre', 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="79-tfidf-1" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>2 0.35101476 <a title="79-tfidf-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>3 0.33534601 <a title="79-tfidf-3" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>4 0.28530422 <a title="79-tfidf-4" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>5 0.26671454 <a title="79-tfidf-5" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>6 0.26372138 <a title="79-tfidf-6" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>7 0.2596263 <a title="79-tfidf-7" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>8 0.19862586 <a title="79-tfidf-8" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>9 0.18779719 <a title="79-tfidf-9" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>10 0.1814259 <a title="79-tfidf-10" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>11 0.17981021 <a title="79-tfidf-11" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>12 0.17881992 <a title="79-tfidf-12" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>13 0.17081006 <a title="79-tfidf-13" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>14 0.16916864 <a title="79-tfidf-14" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>15 0.16211142 <a title="79-tfidf-15" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>16 0.14577445 <a title="79-tfidf-16" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>17 0.14478439 <a title="79-tfidf-17" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>18 0.14002086 <a title="79-tfidf-18" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>19 0.13542777 <a title="79-tfidf-19" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>20 0.13070221 <a title="79-tfidf-20" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.352), (2, -0.138), (3, -0.208), (4, 0.009), (5, 0.052), (6, -0.004), (7, -0.025), (8, 0.072), (9, 0.008), (10, -0.001), (11, 0.007), (12, 0.079), (13, -0.018), (14, -0.025), (15, -0.023), (16, -0.071), (17, 0.016), (18, -0.011), (19, -0.043), (20, -0.062), (21, -0.015), (22, -0.036), (23, -0.015), (24, -0.012), (25, -0.05), (26, 0.031), (27, -0.003), (28, -0.039), (29, 0.026), (30, -0.017), (31, 0.026), (32, 0.006), (33, -0.009), (34, 0.011), (35, -0.006), (36, -0.001), (37, 0.09), (38, 0.123), (39, 0.065), (40, 0.066), (41, -0.001), (42, -0.1), (43, -0.022), (44, -0.051), (45, 0.042), (46, -0.035), (47, 0.001), (48, -0.001), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94976401 <a title="79-lsi-1" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>2 0.86073607 <a title="79-lsi-2" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>3 0.82657278 <a title="79-lsi-3" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>4 0.8106454 <a title="79-lsi-4" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>5 0.80588031 <a title="79-lsi-5" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>6 0.80058742 <a title="79-lsi-6" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>7 0.75326931 <a title="79-lsi-7" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>8 0.72354931 <a title="79-lsi-8" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>9 0.71790379 <a title="79-lsi-9" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>10 0.69716203 <a title="79-lsi-10" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>11 0.69346195 <a title="79-lsi-11" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>12 0.64795995 <a title="79-lsi-12" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>13 0.6448285 <a title="79-lsi-13" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>14 0.63269913 <a title="79-lsi-14" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>15 0.59857363 <a title="79-lsi-15" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>16 0.57183343 <a title="79-lsi-16" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>17 0.56746292 <a title="79-lsi-17" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>18 0.56737596 <a title="79-lsi-18" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>19 0.51623482 <a title="79-lsi-19" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>20 0.51297265 <a title="79-lsi-20" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.04), (14, 0.012), (20, 0.092), (23, 0.013), (25, 0.107), (37, 0.078), (47, 0.015), (68, 0.182), (70, 0.046), (80, 0.162), (86, 0.073), (87, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84732366 <a title="79-lda-1" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>same-paper 2 0.83573824 <a title="79-lda-2" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>3 0.77836686 <a title="79-lda-3" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>4 0.77264076 <a title="79-lda-4" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>5 0.77262962 <a title="79-lda-5" href="./nips-2013-Graphical_Models_for_Inference_with_Missing_Data.html">134 nips-2013-Graphical Models for Inference with Missing Data</a></p>
<p>6 0.77240139 <a title="79-lda-6" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>7 0.77088505 <a title="79-lda-7" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>8 0.76632291 <a title="79-lda-8" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>9 0.76540101 <a title="79-lda-9" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>10 0.76192528 <a title="79-lda-10" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>11 0.76133794 <a title="79-lda-11" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>12 0.76098704 <a title="79-lda-12" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>13 0.75973225 <a title="79-lda-13" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<p>14 0.75813246 <a title="79-lda-14" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>15 0.75760239 <a title="79-lda-15" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>16 0.75742018 <a title="79-lda-16" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>17 0.75684404 <a title="79-lda-17" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>18 0.75654769 <a title="79-lda-18" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>19 0.7560941 <a title="79-lda-19" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>20 0.75320548 <a title="79-lda-20" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
