<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2013-Generalized Random Utility Models with Multiple Types</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-129" href="#">nips2013-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2013-Generalized Random Utility Models with Multiple Types</h1>
<br/><p>Source: <a title="nips-2013-129-pdf" href="http://papers.nips.cc/paper/4998-generalized-random-utility-models-with-multiple-types.pdf">pdf</a></p><p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>Reference: <a title="nips-2013-129-reference" href="../nips2013_reference/nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. [sent-10, score-0.214]
</p><p>2 Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. [sent-11, score-0.214]
</p><p>3 We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. [sent-12, score-0.139]
</p><p>4 1  Introduction  Random utility models (RUM), which presume agent utility to be composed of a deterministic component and a stochastic unobserved error component, are frequently used to model choices by individuals over alternatives. [sent-14, score-0.818]
</p><p>5 In this paper, we focus on applications where the data is rankings by individuals over alternatives. [sent-15, score-0.097]
</p><p>6 Examples from economics include the popular random coefﬁcients logit model [7] where the data may involve a (partial) consumer ranking of products [9]. [sent-16, score-0.283]
</p><p>7 These ingredients are used to construct a posterior/likelihood function of speciﬁc data moments, such as the fraction of agents of each type that choose each alternative. [sent-18, score-0.529]
</p><p>8 To estimate preferences across heterogenous agents, one approach as allowed by prior work [20, 24] is to assume a mixture of agents with a ﬁnite number of types. [sent-19, score-0.531]
</p><p>9 We build upon this work by developing an algorithm to endogenously learn the classiﬁcation of agent types within this mixture. [sent-20, score-0.62]
</p><p>10 Empirical researchers are increasingly being presented with rich data on the choices made by individuals, and asked to classify these agents into different types [28, 29] and to estimate the preferences of each type [10, 23]. [sent-21, score-0.831]
</p><p>11 Examples of individual-level data used in economics include household purchases from supermarket-scanner data [1, 21], and patients’ hospital or treatment choices from healthcare data [22]. [sent-22, score-0.189]
</p><p>12 The partitioning of agents into latent, discrete sets (or “types”) allows for the study of the underlying distribution of preferences across a population of heterogeneous agents. [sent-23, score-0.495]
</p><p>13 For example, preferences may be correlated with an agent characteristic, such as income, and the true classiﬁcation of each agent’s type, such as his income bracket, may be unobserved. [sent-24, score-0.497]
</p><p>14 1  Our Contributions  This paper focuses on estimating generalized random utility models (GRUM1 ) when the observed data is partial orders of agents’ rankings over alternatives and when latent types are present. [sent-27, score-0.657]
</p><p>15 We build on recent work [3, 4] on estimating GRUMs by allowing for an interaction between agent characteristics and the characteristics of the agent’s chosen alternative. [sent-28, score-0.771]
</p><p>16 The interaction term helps us to avoid unrealistic substitution patterns due to the independence of irrelevant alternatives [26] by allowing agent utilities to be correlated across alternatives with similar characteristics. [sent-29, score-0.872]
</p><p>17 For example, this prevents a situation where removing the top choices of both a rich household and a poor household lead them to become equally likely to substitute to the same alternative choice. [sent-30, score-0.208]
</p><p>18 Our model also allows the marginal utilities associated with the characteristics of alternatives to vary across agent types. [sent-31, score-0.838]
</p><p>19 RJMCMC can be used for model selection and learning a posterior on the number of types in a mixture model [31]. [sent-33, score-0.298]
</p><p>20 Here, we use RJMCMC to cluster agents into different types, where each type exhibits demand for alternatives based on different preferences; i. [sent-34, score-0.745]
</p><p>21 The results show that our method is scalable, and that the clustering of types provides a better ﬁt to real world data. [sent-38, score-0.255]
</p><p>22 2 The main theoretical contribution establishes identiﬁability of mixture models over data consisting of partial orders. [sent-41, score-0.069]
</p><p>23 We do not provide results on the log concavity of the general likelihood problem with unknown types and leave it for future studies. [sent-44, score-0.214]
</p><p>24 This may lead to biased estimates, in particular when the number of alternatives grow large [5]. [sent-54, score-0.155]
</p><p>25 Previous work on clustering ranking data for variations of the Placket-Luce (PL) model [28, 29] has been restricted to settings without agent and alternative characteristics. [sent-55, score-0.592]
</p><p>26 In mixture models, assuming an arbitrary number of types can lead to biased results, and reduces the statistical efﬁciency of the estimators [15]. [sent-60, score-0.25]
</p><p>27 Our method applies to data involving individual-level observations, and partial orders with more than two alternatives. [sent-64, score-0.104]
</p><p>28 The inference method establishes a posterior on the number of types, resolving the common issue of how the researcher should select the number of types. [sent-65, score-0.085]
</p><p>29 2  Model  Suppose we have N agents and M alternatives {c1 , . [sent-66, score-0.593]
</p><p>30 , cM }, and there are S types (subgroups) of agents and s(n) is agent n’s type. [sent-68, score-1.058]
</p><p>31 Agent characteristics are observed and deﬁned as an N ×K matrix X, and alternative characteristics are observed and deﬁned as an L × M matrix Z, where K and L are the number of agent and alternative characteristics respectively. [sent-69, score-1.05]
</p><p>32 Let unm be agent n’s perceived utility for alternative m, and let W s(n) be a K × L real matrix that models the linear relation between the attributes of alternatives and the attributes of agents. [sent-70, score-0.913]
</p><p>33 We have, unm = δm + xn W s(n) (zm )T +  nm ,  (1)  where xn is the nth row of the matrix X and zm is the mth column of the matrix Z. [sent-71, score-0.262]
</p><p>34 In words, agent n’s utility for alternative m consists of the following three parts: 1. [sent-72, score-0.641]
</p><p>35 δm :gs The intrinsic utility of alternative m, which is the same across all agents; 2. [sent-73, score-0.268]
</p><p>36 xn W s(n) (zm )T : The agent-speciﬁc utility, which is unique to all agents of type s(n), and where W s(n) has at least one nonzero element; 3. [sent-74, score-0.575]
</p><p>37 nm : The random noise (agent-speciﬁc taste shock), which is generated independently across agents and alternatives. [sent-75, score-0.507]
</p><p>38 The number of parameters for each type is P = KL + M . [sent-76, score-0.091]
</p><p>39 We also need to shufﬂe the parameters for all types into a P × S matrix Ψ, such that ΨKL+m,s = δ and Ψ(k−1)L+l,s = (n)  s Wkl for 1 ≤ k ≤ K and 1 ≤ l ≤ L. [sent-79, score-0.214]
</p><p>40 We adopt BS×1 to indicate the type of agent n, with (n)  (n)  (n)  Bs(n),1 = 1 and Bs,1 = 0 for all s = s(n). [sent-80, score-0.497]
</p><p>41 We also deﬁne an M × 1 matrix, U (n) , as Um,1 = unm . [sent-81, score-0.068]
</p><p>42 We can now rewrite (1) as: U (n) = A(n) ΨB (n) +  (2)  Suppose that an agent has type s with probability γs . [sent-82, score-0.497]
</p><p>43 Given this, the random utility model can S (n) |X (n) , Z, Ψs ), where Ψs is the sth be written as, Pr(U (n) |X (n) , Z, Ψ, Γ) = s=1 γs Pr(U column of the matrix Ψ. [sent-83, score-0.156]
</p><p>44 An agent ranks the alternatives according to her perceived utilities for the alternatives. [sent-84, score-0.725]
</p><p>45 π n represents the full ranking [cπi (1) i cπi (2) i · · · i cπi (m) ] of the alternatives {c1 , . [sent-92, score-0.221]
</p><p>46 That is, for agent n, cm1 n cm2 if and only if unm1 > unm2 (In this model, situations with tied perceived utilities have zero probability measure). [sent-95, score-0.57]
</p><p>47 π = order (U ) is the ranking implied by U, and π(i) is the ith largest utility in U . [sent-97, score-0.222]
</p><p>48 We have that N  Pr(π (n) |X (n) , Z, Ψ, Γ)  Pr(D|X, Z, Ψ, Γ) = n=1  3  Strict Log-concavity and Identiﬁability  In this section, we establish conditions for identiﬁability of the types and parameters for the model. [sent-101, score-0.25]
</p><p>49 Establishing identiﬁability in a model with multiple types and ranking data requires a different approach from classical identiﬁability results for mixture models [2, 18, e. [sent-103, score-0.316]
</p><p>50 Moreover, we establish conditions for uni-modality of the likelihood for the parameters Γ and Ψ, when the types are observed. [sent-106, score-0.25]
</p><p>51 Although our main focus is on data with unobservable types, establishing the conditions for unimodality conditioned on known types remains an essential step because of the sampling and optimization aspects of RJMCMC. [sent-107, score-0.214]
</p><p>52 Despite adopting a Bayesian point of view in presenting the model, we adopt a uniform prior on the parameter set, and only impose nontrivial priors on the number of types in order to obtain some regularization. [sent-110, score-0.214]
</p><p>53 1  W  δ  ψ  γ(n) (n)  B  (n)  X  Z  (n)  A  (n)  u  π(n)  N  Figure 2: Graphical representation of the multiple type GRUM generative process. [sent-113, score-0.091]
</p><p>54 Strict Log-concavity of the Likelihood Function  For agent n, we deﬁne a set Gn of function g n ’s whose positivity is equivalent to giving an order n π n . [sent-114, score-0.406]
</p><p>55 , M − 1 where µnj = δj +  s(n)  k,l  xn (k)Wkl zj (l) for 1 ≤ j ≤ M . [sent-117, score-0.087]
</p><p>56 This is because gm (ψ, ) ≥ 0 is equivalent to saying n n alternative π (m) is preferred to alternative π (m + 1) in the RUM sense. [sent-123, score-0.2]
</p><p>57 By log-concavity of L(ψ, π) and using the fact that sum of concave functions is concave, we know that l(Ψ, D) is concave in Ψ, viewed as a vector in RSKL+M . [sent-128, score-0.126]
</p><p>58 A δ-shift results in a shift in the intrinsic utilities of all the products, which does not change the utility difference between products. [sent-141, score-0.334]
</p><p>59 2  Identiﬁability of the Model  In this section, we show that, for the case of unobserved types, our model is identiﬁable for a certain class of cdfs for the noise in random utility models. [sent-148, score-0.226]
</p><p>60 For any x1 , x2 ∈ R, the sequence R (as i → ∞) only if either x1 = x2 ; or x1 = −x2 and  gi (x1 ) gi (x2 ) gi (x1 ) gi (x2 )  converges to some value in → −1 as i → ∞. [sent-154, score-0.188]
</p><p>61 S  γs Pr(π|X (n) , Z, Ψ) = s=1  γs Pr(π|X (n) , Z, Ψ )  (4)  s=1  for all possible orders π of M products, and for all agents n. [sent-161, score-0.472]
</p><p>62 5  For now, let’s ﬁx the number of agent characteristics, K. [sent-163, score-0.406]
</p><p>63 One observation is that the number xn (k), for any characteristic k, reﬂects certain characteristics of agent n. [sent-164, score-0.614]
</p><p>64 Varying the agent n, this amount xn (k) is in a bounded interval in R. [sent-165, score-0.452]
</p><p>65 Deﬁne an (M − 1) × L matrix Z by setting Zm,l = zm (l) − zM (l). [sent-169, score-0.102]
</p><p>66 Here, we illustrate the idea for the simple case, with two alternatives (m = 2) and no agent or alternative characteristics (K = L = 1). [sent-174, score-0.802]
</p><p>67 Theorem 2 guarantees identiﬁability in the limit case that we observe agents with characteristics that are dense in an interval. [sent-186, score-0.6]
</p><p>68 Beyond the theoretical guarantee, we would in practice expect (6) to have a unique solution with a enough agents with different characteristics. [sent-187, score-0.438]
</p><p>69 4  RJMCMC for Parameter Estimation  We are using a uniform prior for the parameter space and regularize the number of types with a geometric prior. [sent-189, score-0.214]
</p><p>70 In each of T iterations, we sample utilities un for each agent, matrix ψs for each type, and set of assignments of agents to alternatives Sn . [sent-191, score-0.745]
</p><p>71 The utility of each agent for each alternative conditioned on the data and other parameters is sampled from a truncated Exponential Family (e. [sent-192, score-0.641]
</p><p>72 In order to sample agent i’s utility for alternative j (uij ), we set thresholds for lower and upper truncation based on agent i’s former samples of utility for the two alternatives that are ranked one below and one above alternative j, respectively. [sent-195, score-1.437]
</p><p>73 We consider three possible moves for sampling from the assignment function S(n):  6  (1) Increasing the number of types by one, through moving a random agent to a new type of its own. [sent-197, score-0.711]
</p><p>74 (2) Decrease the number of types by one, through merging two random types. [sent-203, score-0.214]
</p><p>75 1 p+1 Pr(S) S  (3) We do not change the number of types, and consider moving one random agent from one type to another. [sent-208, score-0.497]
</p><p>76 This case reduces to a standard Metropolis-Hastings, where because of the normal symmet(t+1) |D) ric proposal distribution, the proposal is accepted with probability: Prmh = min{1, Pr(M (t) |D) }. [sent-209, score-0.078]
</p><p>77 We consider case ν = 0: a noisy regime by generating data from noise Select two random types Ms1 and Ms2 and level of σ = 1, where all the characteristics Move a random agent n from Ms1 to Ms2 (X,Z) are generated from N (0, 1). [sent-220, score-0.782]
</p><p>78 Given this, we estiAccept S(t+1) (n) = s2 with probability mate 60%, 90% and 95% conﬁdence intervals Prmh for the number of types from the posterior samend switch ples. [sent-222, score-0.262]
</p><p>79 We also estimate the coverage percentage, which is deﬁned to be the percentage of samples which include the true number of types in the interval. [sent-223, score-0.248]
</p><p>80 Table 1 shows that the algorithm successfully provides larger log posterior when the number of types is the number of true types. [sent-230, score-0.262]
</p><p>81 Clustering Performance for Real World Data: We have tested our algorithm on a sushi dataset, where 5, 000 users provide rankings on M = 10 different kinds of sushi [25]. [sent-231, score-0.463]
</p><p>82 We ﬁt the multi-type  7  6  Conclusions  Number of Subgroups (S)  GRUM for different number of types, on 100 randomly chosen subsets of the sushi data with size N = 200 , using the same prior we used in synthetic case and provide the performance on the Sushi data in Table 1. [sent-232, score-0.236]
</p><p>83 It can be seen that GRUM with 3 types has signiﬁcantly better performance in terms of log posterior (with the prior that we chose, log posterior can be seen as log likelihood penalized for number of parameters) than GRUM with one, two or four types. [sent-233, score-0.31]
</p><p>84 We have taken non-categorical features as K = 4 feature for agents (age, time for ﬁlling the questionnaire, region ID, prefecture ID) and L = 3 features for sushi ( price,heaviness, sales volume). [sent-234, score-0.677]
</p><p>85 Frequency 10  10  8  8  6  6  4  4  In this paper, we have proposed an extension of 2 2 GRUMs in which we allow agents to adopt het0 00 2000 4000 6000 8000 10000 erogeneous types. [sent-235, score-0.438]
</p><p>86 We develop a theory estabIterations lishing the identiﬁability of the mixture model Figure 3: Left Panel: 10000 samples for S in Synwhen we observe ranking data. [sent-236, score-0.102]
</p><p>87 Right Panel: cal results for identiﬁability show that the num- Histogram of the samples for S with max at 5 and ber of types and the parameters associated with mean at 4. [sent-238, score-0.244]
</p><p>88 Moreover, we prove Synthetic True types Sushi uni-modality of the likelihood (or posterior) Type One two Three Four sushi function when types are observable. [sent-241, score-0.627]
</p><p>89 We proone type -2069 -2631 -2780 -2907 -2880 pose a scalable algorithm for inference, which two types -2755 -2522 -2545 -2692 -2849 can be parallelized for use on very large data three types -2796 -2642 -2582 -2790 -2819 sets. [sent-242, score-0.519]
</p><p>90 Our experimental results show that models four types -2778 -2807 -2803 -2593 -2850 with multiple types provide a signiﬁcantly betTable 1: Performance of the method for different ter ﬁt, in real-world data. [sent-243, score-0.428]
</p><p>91 By clustering agents number of true types and number of types in algorithm into multiple types, our estimation algorithm in terms of log posterior. [sent-244, score-0.907]
</p><p>92 All the standard deviations allows choices to be correlated across agents are between 15 and 20. [sent-245, score-0.469]
</p><p>93 Bold numbers indicate the of the same type, without making any a priori best performance in their column with statistical sigassumptions on how types of agents are to be niﬁcance of 95%. [sent-246, score-0.652]
</p><p>94 This use of machine learning techniques complements various approaches in economics [11, 7, 8] by allowing the researcher to have additional ﬂexibility in dealing with missing data or unobserved agent characteristics. [sent-248, score-0.555]
</p><p>95 In future research we intend to pursue applications of this method to problems of economic interest. [sent-250, score-0.071]
</p><p>96 Advertising, learning, and consumer choice in experience goods: An empirical examination. [sent-260, score-0.079]
</p><p>97 Discrete choice models as structural models of demand: Some economic implications of common approaches. [sent-279, score-0.071]
</p><p>98 A nested logit model of automobile holdings for one vehicle households. [sent-282, score-0.097]
</p><p>99 Efﬁcient construction of reversible jump Markov chain Monte Carlo proposal distributions. [sent-304, score-0.158]
</p><p>100 Measuring the implications of sales and consumer inventory behavior. [sent-336, score-0.119]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agents', 0.438), ('agent', 0.406), ('types', 0.214), ('sushi', 0.199), ('pr', 0.19), ('rjmcmc', 0.18), ('characteristics', 0.162), ('utility', 0.156), ('alternatives', 0.155), ('grum', 0.135), ('utilities', 0.115), ('grums', 0.113), ('zm', 0.102), ('wkl', 0.099), ('rum', 0.099), ('azari', 0.092), ('type', 0.091), ('levinsohn', 0.09), ('ability', 0.083), ('identi', 0.082), ('consumer', 0.079), ('skl', 0.079), ('alternative', 0.079), ('economics', 0.075), ('parkes', 0.073), ('ariel', 0.073), ('economic', 0.071), ('taste', 0.069), ('reversible', 0.069), ('unm', 0.068), ('berry', 0.067), ('steven', 0.067), ('ranking', 0.066), ('rankings', 0.065), ('concave', 0.063), ('logit', 0.063), ('demand', 0.061), ('ani', 0.06), ('hossein', 0.06), ('lirong', 0.06), ('sou', 0.06), ('goods', 0.06), ('xw', 0.058), ('preferences', 0.057), ('harvard', 0.056), ('econometrics', 0.055), ('preference', 0.053), ('jump', 0.05), ('household', 0.049), ('perceived', 0.049), ('posterior', 0.048), ('gi', 0.047), ('nice', 0.047), ('xn', 0.046), ('diao', 0.045), ('prmerge', 0.045), ('prmh', 0.045), ('prsplit', 0.045), ('rskl', 0.045), ('shocks', 0.045), ('subgroups', 0.045), ('james', 0.045), ('gm', 0.042), ('interaction', 0.041), ('zj', 0.041), ('clustering', 0.041), ('marina', 0.04), ('sales', 0.04), ('proposal', 0.039), ('synthetic', 0.037), ('un', 0.037), ('shock', 0.037), ('econometrica', 0.037), ('researcher', 0.037), ('seas', 0.037), ('involving', 0.037), ('unobserved', 0.037), ('mixture', 0.036), ('establish', 0.036), ('permutation', 0.035), ('income', 0.034), ('hospital', 0.034), ('differentiated', 0.034), ('akl', 0.034), ('automobile', 0.034), ('coverage', 0.034), ('orders', 0.034), ('intrinsic', 0.033), ('econometric', 0.033), ('tahoe', 0.033), ('ordinal', 0.033), ('cdfs', 0.033), ('partial', 0.033), ('individuals', 0.032), ('daniel', 0.031), ('choices', 0.031), ('ms', 0.03), ('ber', 0.03), ('saturation', 0.03), ('shift', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="129-tfidf-1" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>2 0.27533942 <a title="129-tfidf-2" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>3 0.23276375 <a title="129-tfidf-3" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>Author: Liam C. MacDermed, Charles Isbell</p><p>Abstract: We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard inﬁnite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the ﬁrst tractable value iteration method for DecPOMDPs that outperforms existing algorithms. 1</p><p>4 0.23271704 <a title="129-tfidf-4" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>5 0.2082223 <a title="129-tfidf-5" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>Author: Shahin Shahrampour, Sasha Rakhlin, Ali Jadbabaie</p><p>Abstract: This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance. Then, we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth, per individual. We observe that only one of the estimators recovers the optimal MSD, which underscores the impact of the objective function decomposition on the learning quality. Finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a ﬁnite time. 1</p><p>6 0.121314 <a title="129-tfidf-6" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>7 0.11433491 <a title="129-tfidf-7" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>8 0.087638095 <a title="129-tfidf-8" href="./nips-2013-Convergence_of_Monte_Carlo_Tree_Search_in_Simultaneous_Move_Games.html">71 nips-2013-Convergence of Monte Carlo Tree Search in Simultaneous Move Games</a></p>
<p>9 0.084839575 <a title="129-tfidf-9" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>10 0.076264627 <a title="129-tfidf-10" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>11 0.071667209 <a title="129-tfidf-11" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>12 0.067331858 <a title="129-tfidf-12" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>13 0.0646642 <a title="129-tfidf-13" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>14 0.06375891 <a title="129-tfidf-14" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>15 0.060582414 <a title="129-tfidf-15" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>16 0.058864556 <a title="129-tfidf-16" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>17 0.04989405 <a title="129-tfidf-17" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>18 0.048787974 <a title="129-tfidf-18" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>19 0.048220199 <a title="129-tfidf-19" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>20 0.047305442 <a title="129-tfidf-20" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, -0.016), (2, -0.026), (3, 0.016), (4, 0.016), (5, 0.038), (6, 0.113), (7, -0.05), (8, -0.036), (9, 0.037), (10, 0.042), (11, 0.05), (12, 0.039), (13, 0.019), (14, -0.205), (15, 0.297), (16, 0.09), (17, 0.14), (18, 0.147), (19, -0.03), (20, -0.261), (21, -0.09), (22, -0.052), (23, -0.036), (24, -0.149), (25, -0.032), (26, 0.004), (27, 0.155), (28, 0.082), (29, 0.038), (30, 0.047), (31, 0.073), (32, 0.032), (33, -0.163), (34, -0.078), (35, 0.07), (36, 0.155), (37, -0.065), (38, 0.029), (39, 0.074), (40, -0.01), (41, -0.002), (42, 0.061), (43, 0.047), (44, -0.01), (45, -0.022), (46, 0.094), (47, 0.056), (48, -0.021), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94266051 <a title="129-lsi-1" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>2 0.80812305 <a title="129-lsi-2" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>3 0.73812377 <a title="129-lsi-3" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>Author: Liam C. MacDermed, Charles Isbell</p><p>Abstract: We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard inﬁnite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the ﬁrst tractable value iteration method for DecPOMDPs that outperforms existing algorithms. 1</p><p>4 0.68457103 <a title="129-lsi-4" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>Author: Shahin Shahrampour, Sasha Rakhlin, Ali Jadbabaie</p><p>Abstract: This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance. Then, we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth, per individual. We observe that only one of the estimators recovers the optimal MSD, which underscores the impact of the objective function decomposition on the learning quality. Finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a ﬁnite time. 1</p><p>5 0.6226337 <a title="129-lsi-5" href="./nips-2013-Convergence_of_Monte_Carlo_Tree_Search_in_Simultaneous_Move_Games.html">71 nips-2013-Convergence of Monte Carlo Tree Search in Simultaneous Move Games</a></p>
<p>Author: Viliam Lisy, Vojta Kovarik, Marc Lanctot, Branislav Bosansky</p><p>Abstract: unkown-abstract</p><p>6 0.59076786 <a title="129-lsi-6" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>7 0.57570398 <a title="129-lsi-7" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>8 0.57240343 <a title="129-lsi-8" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>9 0.3059597 <a title="129-lsi-9" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>10 0.29908481 <a title="129-lsi-10" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>11 0.29062754 <a title="129-lsi-11" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>12 0.28999203 <a title="129-lsi-12" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>13 0.24581744 <a title="129-lsi-13" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>14 0.23924635 <a title="129-lsi-14" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>15 0.23243348 <a title="129-lsi-15" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>16 0.23176867 <a title="129-lsi-16" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>17 0.22633548 <a title="129-lsi-17" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>18 0.22619374 <a title="129-lsi-18" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>19 0.22452986 <a title="129-lsi-19" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>20 0.2236225 <a title="129-lsi-20" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.015), (16, 0.031), (33, 0.106), (34, 0.607), (41, 0.013), (49, 0.014), (56, 0.059), (85, 0.031), (89, 0.022), (93, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96690321 <a title="129-lda-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.96025789 <a title="129-lda-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.94985008 <a title="129-lda-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>same-paper 4 0.94890493 <a title="129-lda-4" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>5 0.93994415 <a title="129-lda-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>6 0.93343431 <a title="129-lda-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>7 0.92161852 <a title="129-lda-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.91781312 <a title="129-lda-8" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>9 0.90270674 <a title="129-lda-9" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>10 0.89675516 <a title="129-lda-10" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>11 0.78305221 <a title="129-lda-11" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>12 0.77353281 <a title="129-lda-12" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>13 0.77130699 <a title="129-lda-13" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>14 0.73897719 <a title="129-lda-14" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>15 0.73894286 <a title="129-lda-15" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>16 0.73052889 <a title="129-lda-16" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>17 0.73042589 <a title="129-lda-17" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>18 0.72331989 <a title="129-lda-18" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>19 0.72322887 <a title="129-lda-19" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>20 0.7225368 <a title="129-lda-20" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
