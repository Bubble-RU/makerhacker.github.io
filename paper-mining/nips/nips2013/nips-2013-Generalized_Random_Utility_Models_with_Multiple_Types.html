<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2013-Generalized Random Utility Models with Multiple Types</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-129" href="#">nips2013-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2013-Generalized Random Utility Models with Multiple Types</h1>
<br/><p>Source: <a title="nips-2013-129-pdf" href="http://papers.nips.cc/paper/4998-generalized-random-utility-models-with-multiple-types.pdf">pdf</a></p><p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>Reference: <a title="nips-2013-129-reference" href="../nips2013_reference/nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ag', 0.615), ('grum', 0.253), ('sush', 0.23), ('pr', 0.22), ('rjmcmc', 0.208), ('util', 0.152), ('econom', 0.148), ('rum', 0.138), ('zm', 0.118), ('wkl', 0.115), ('azar', 0.106), ('levinsohn', 0.104), ('skl', 0.092), ('economet', 0.09), ('altern', 0.086), ('shock', 0.085), ('ariel', 0.085), ('concav', 0.081), ('rank', 0.079), ('berry', 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="129-tfidf-1" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>2 0.28401637 <a title="129-tfidf-2" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>3 0.24472176 <a title="129-tfidf-3" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>4 0.20866777 <a title="129-tfidf-4" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>Author: Liam C. MacDermed, Charles Isbell</p><p>Abstract: We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard inﬁnite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the ﬁrst tractable value iteration method for DecPOMDPs that outperforms existing algorithms. 1</p><p>5 0.19143838 <a title="129-tfidf-5" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>Author: Shahin Shahrampour, Sasha Rakhlin, Ali Jadbabaie</p><p>Abstract: This paper addresses the problem of online learning in a dynamic setting. We consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period. Unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. We view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss. Based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state. We establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance. Then, we characterize explicit expressions for the steady state mean-square deviation(MSD) of the estimates from the truth, per individual. We observe that only one of the estimators recovers the optimal MSD, which underscores the impact of the objective function decomposition on the learning quality. Finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a ﬁnite time. 1</p><p>6 0.1220899 <a title="129-tfidf-6" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>7 0.11735152 <a title="129-tfidf-7" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>8 0.089538544 <a title="129-tfidf-8" href="./nips-2013-Convergence_of_Monte_Carlo_Tree_Search_in_Simultaneous_Move_Games.html">71 nips-2013-Convergence of Monte Carlo Tree Search in Simultaneous Move Games</a></p>
<p>9 0.089383952 <a title="129-tfidf-9" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>10 0.084887058 <a title="129-tfidf-10" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>11 0.082148865 <a title="129-tfidf-11" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>12 0.076597206 <a title="129-tfidf-12" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>13 0.074044779 <a title="129-tfidf-13" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>14 0.073759258 <a title="129-tfidf-14" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>15 0.069800608 <a title="129-tfidf-15" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>16 0.067273453 <a title="129-tfidf-16" href="./nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</a></p>
<p>17 0.064789467 <a title="129-tfidf-17" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>18 0.063910276 <a title="129-tfidf-18" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>19 0.062317323 <a title="129-tfidf-19" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>20 0.059452266 <a title="129-tfidf-20" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.011), (2, 0.023), (3, -0.035), (4, -0.023), (5, 0.015), (6, 0.029), (7, 0.124), (8, 0.083), (9, -0.012), (10, 0.001), (11, 0.049), (12, -0.068), (13, 0.054), (14, -0.078), (15, -0.002), (16, -0.154), (17, -0.152), (18, -0.206), (19, 0.04), (20, -0.285), (21, -0.034), (22, -0.106), (23, 0.02), (24, -0.147), (25, -0.032), (26, 0.011), (27, -0.133), (28, 0.002), (29, -0.085), (30, -0.053), (31, 0.033), (32, -0.093), (33, -0.002), (34, -0.064), (35, 0.014), (36, -0.033), (37, 0.058), (38, 0.089), (39, 0.037), (40, -0.023), (41, 0.029), (42, -0.2), (43, 0.073), (44, -0.031), (45, 0.026), (46, 0.072), (47, -0.047), (48, -0.025), (49, 0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94474602 <a title="129-lsi-1" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>2 0.78356397 <a title="129-lsi-2" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>Author: Liam C. MacDermed, Charles Isbell</p><p>Abstract: We present four major results towards solving decentralized partially observable Markov decision problems (DecPOMDPs) culminating in an algorithm that outperforms all existing algorithms on all but one standard inﬁnite-horizon benchmark problems. (1) We give an integer program that solves collaborative Bayesian games (CBGs). The program is notable because its linear relaxation is very often integral. (2) We show that a DecPOMDP with bounded belief can be converted to a POMDP (albeit with actions exponential in the number of beliefs). These actions correspond to strategies of a CBG. (3) We present a method to transform any DecPOMDP into a DecPOMDP with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression. (4) We show that the combination of these results opens the door for new classes of DecPOMDP algorithms based on previous POMDP algorithms. We choose one such algorithm, point-based valued iteration, and modify it to produce the ﬁrst tractable value iteration method for DecPOMDPs that outperforms existing algorithms. 1</p><p>3 0.72835803 <a title="129-lsi-3" href="./nips-2013-Convergence_of_Monte_Carlo_Tree_Search_in_Simultaneous_Move_Games.html">71 nips-2013-Convergence of Monte Carlo Tree Search in Simultaneous Move Games</a></p>
<p>Author: Viliam Lisy, Vojta Kovarik, Marc Lanctot, Branislav Bosansky</p><p>Abstract: unkown-abstract</p><p>4 0.70868915 <a title="129-lsi-4" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>5 0.68861765 <a title="129-lsi-5" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>Author: Xiaoxiao Guo, Satinder Singh, Richard L. Lewis</p><p>Abstract: We consider how to transfer knowledge from previous tasks (MDPs) to a current task in long-lived and bounded agents that must solve a sequence of tasks over a ﬁnite lifetime. A novel aspect of our transfer approach is that we reuse reward functions. While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent’s behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent. Speciﬁcally, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent’s performance relative to other approaches, including an approach that transfers policies. 1</p><p>6 0.67360997 <a title="129-lsi-6" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>7 0.66232079 <a title="129-lsi-7" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>8 0.60479796 <a title="129-lsi-8" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>9 0.43871638 <a title="129-lsi-9" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>10 0.39737856 <a title="129-lsi-10" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>11 0.37163702 <a title="129-lsi-11" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>12 0.35375196 <a title="129-lsi-12" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>13 0.35128632 <a title="129-lsi-13" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>14 0.34899104 <a title="129-lsi-14" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>15 0.34346396 <a title="129-lsi-15" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>16 0.33552051 <a title="129-lsi-16" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>17 0.33214158 <a title="129-lsi-17" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>18 0.32569695 <a title="129-lsi-18" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>19 0.32088825 <a title="129-lsi-19" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>20 0.3157039 <a title="129-lsi-20" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.019), (20, 0.156), (25, 0.063), (37, 0.105), (70, 0.057), (80, 0.088), (84, 0.261), (86, 0.082), (87, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78100425 <a title="129-lda-1" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>2 0.72599512 <a title="129-lda-2" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>3 0.69070721 <a title="129-lda-3" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>4 0.67740715 <a title="129-lda-4" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>Author: Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause</p><p>Abstract: Many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.) require selecting, out of a massive data set, a manageable yet representative subset. Such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints. Classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol G REE D I, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show, that under certain natural conditions, performance close to the (impractical) centralized approach can be achieved. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar-based clustering, on tens of millions of data points using Hadoop. 1</p><p>5 0.67360616 <a title="129-lda-5" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>6 0.6714505 <a title="129-lda-6" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>7 0.66815436 <a title="129-lda-7" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>8 0.66674238 <a title="129-lda-8" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>9 0.66672498 <a title="129-lda-9" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>10 0.66514373 <a title="129-lda-10" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>11 0.66344941 <a title="129-lda-11" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>12 0.66312832 <a title="129-lda-12" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>13 0.66310227 <a title="129-lda-13" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>14 0.66301811 <a title="129-lda-14" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>15 0.66294116 <a title="129-lda-15" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>16 0.66234624 <a title="129-lda-16" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>17 0.66219878 <a title="129-lda-17" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>18 0.66180706 <a title="129-lda-18" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>19 0.66156971 <a title="129-lda-19" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>20 0.66062528 <a title="129-lda-20" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
