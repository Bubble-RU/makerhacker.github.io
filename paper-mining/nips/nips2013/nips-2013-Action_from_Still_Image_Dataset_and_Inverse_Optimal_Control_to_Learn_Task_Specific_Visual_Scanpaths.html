<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-21" href="#">nips2013-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</h1>
<br/><p>Source: <a title="nips-2013-21-pdf" href="http://papers.nips.cc/paper/5196-action-from-still-image-dataset-and-inverse-optimal-control-to-learn-task-specific-visual-scanpaths.pdf">pdf</a></p><p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><p>Reference: <a title="nips-2013-21-reference" href="../nips2013_reference/nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 se  Abstract Human eye movements provide a rich source of information into the human visual information processing. [sent-6, score-0.723]
</p><p>2 The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. [sent-7, score-1.266]
</p><p>3 Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. [sent-13, score-0.442]
</p><p>4 Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. [sent-14, score-1.033]
</p><p>5 1  Introduction  Eye movements provide a rich source of knowledge into the human visual information processing and result from the complex interplay between the visual stimulus, prior knowledge of the visual world, and the task. [sent-16, score-0.653]
</p><p>6 The advent of modern eyetracking systems, powerful machine learning techniques, and visual features opens up the prospect of learning eye movement models directly from large real human eye movement datasets, collected under task constraints. [sent-18, score-1.433]
</p><p>7 This trend is still in its infancy, here we aim to advance it on several fronts: • We introduce a large scale dataset of human eye movements collected under the task constraints of both action and context recognition from a single image, for the VOC 2012 Actions dataset. [sent-19, score-1.002]
</p><p>8 The eye movement data is introduced in §3 and is publicly available at http://vision. [sent-20, score-0.477]
</p><p>9 The model integrates both spatial and sequential eye movement information, in order to better 1  Figure 1: Saliency maps obtained from the gaze patterns of 12 viewers under action recognition (left image in pair) and context recognition (right, in pair), from a single image. [sent-24, score-1.219]
</p><p>10 Note that human gaze signiﬁcantly depends on the task (see tab. [sent-25, score-0.39]
</p><p>11 We use the proposed AOI discovery tools to study inter-subject consistency and show that, on this dataset, task instructions have a signiﬁcant inﬂuence on human visual attention patterns, both spatial and sequential. [sent-31, score-0.526]
</p><p>12 • We leverage the large amount of collected ﬁxations and saccades in order to develop a novel, fully trainable, eye movement prediction model. [sent-33, score-0.62]
</p><p>13 The method combines inverse reinforcement learning and advanced computer vision descriptors in order to learn task sensitive reward functions based on human eye movements. [sent-34, score-0.791]
</p><p>14 The model has the important property of being able to efﬁciently predict scanpaths of arbitrary length, by integrating information over a long time horizon. [sent-35, score-0.41]
</p><p>15 This makes it adequate to using machine learning techniques for saliency modeling and eye movement prediction. [sent-42, score-0.726]
</p><p>16 The inﬂuence of task on eye movements has been investigated in early human vision studies[25, 3] for picture viewing, but these groundbreaking studies have been fundamentally qualitative. [sent-43, score-0.729]
</p><p>17 A quantitative analysis of task inﬂuence on visual search in the context of action recognition from video appears in our prior work[19]. [sent-45, score-0.559]
</p><p>18 Human visual saliency prediction has received signiﬁcant interest in computer vision (see [2] for an overview). [sent-46, score-0.5]
</p><p>19 The prediction of eye movements has been less studied. [sent-48, score-0.441]
</p><p>20 In contrast, predeﬁned visual saliency measures can be used to obtain scanpaths[11] in conjunction with non-maximum suppression. [sent-49, score-0.374]
</p><p>21 We are not aware of any eye movement models that are learned from eye movement data. [sent-53, score-0.954]
</p><p>22 3  Action from a Single Image – New Human Eye Movement Dataset  One objective of this work is to introduce eye movement recordings for the PASCAL VOC image dataset used for action recognition. [sent-54, score-0.697]
</p><p>23 Presented in [10], it is one of the largest and most challenging 2  Figure 2: Illustration of areas of interest (AOI) obtained from scanpaths of subjects on three stimuli for the action (left) and context (right) recognition tasks. [sent-55, score-0.895]
</p><p>24 The ﬁrst, action group (8 subjects) was asked to recognize the actions in the image and indicate them from the labels provided by the PASCAL VOC dataset. [sent-67, score-0.297]
</p><p>25 We asked subjects in the action group to solve a multi-target ‘detect and classify’ task: press a key each time they have identiﬁed a person performing an action from the given set and also list the actions they have seen. [sent-72, score-0.574]
</p><p>26 Such areas of interest (AOI) offer an important tool for human visual pattern analysis, e. [sent-87, score-0.352]
</p><p>27 In this section, we propose a model to automatically discover the AOI locations, their spatial support and the transitions between them, from human scanpaths recorded for a given image. [sent-91, score-0.746]
</p><p>28 Consider the case of several human subjects ﬁxating the face of a person and the book she is reading. [sent-95, score-0.395]
</p><p>29 3  agreement cross-stimulus control random baseline  task action context recognition recognition 92. [sent-104, score-0.642]
</p><p>30 8  1  context recognition  (b)  (a)  Figure 3: (a) Spatial inter-subject consistency for the tasks of action and context recognition, with standard deviations across subjects. [sent-132, score-0.414]
</p><p>31 (b) ROC curves for predicting the ﬁxations of one subject from the ﬁxations of the other subjects in the same group on the same image (blue) or on an image (green) randomly selected from the dataset. [sent-133, score-0.354]
</p><p>32 Image Speciﬁc Human Gaze Model: We model human gaze patterns in an image as a Hidden n Markov Model (HMM) where states {si }i=1 correspond to AOIs ﬁxated by the subjects and transitions correspond to saccades. [sent-136, score-0.623]
</p><p>33 We obtain AOIs, for each image and task, by training the HMM using the recorded human eye scanpaths. [sent-143, score-0.6]
</p><p>34 We compute the number of states N ∗ that maximizes the leave-one-out cross validation likelihood over the scanpaths within the training set, with N ∈ [1, 10]. [sent-144, score-0.42]
</p><p>35 2 shows several HMMs trained from the ﬁxations of subjects performing action recognition. [sent-147, score-0.307]
</p><p>36 Context recognition AOIs generally appear on the background and have larger spatial support, in agreement with the scale of the corresponding structures. [sent-152, score-0.298]
</p><p>37 It also allows us to evaluate the performance of eye movement prediction models (§6. [sent-160, score-0.507]
</p><p>38 We ﬁrst study spatial consistency using saliency maps, then analyze sequential consistency in terms of AOI ordering under various metrics. [sent-164, score-0.495]
</p><p>39 Evaluation Protocol: To measure the inter-subject agreement, we predict the regions ﬁxated by a particular subject from a saliency map derived from the ﬁxations of the other subjects on the same image. [sent-166, score-0.481]
</p><p>40 Samples represent image pixels and each pixel’s score is the empirical saliency map derived from training subjects[14]. [sent-167, score-0.336]
</p><p>41 4  Sequential Consistency using AOIs: Next we evaluate the degree to which scanpaths agree in the order in which interesting locations are ﬁxated. [sent-174, score-0.414]
</p><p>42 First, we map each ﬁxation to an AOI obtained with the HMM presented in §4, converting scanpaths to sequences of symbols. [sent-176, score-0.383]
</p><p>43 4c-left shows the fraction (AOIP) of ﬁxations of each human subject, with 2D positions that fall inside AOIs derived from scanpaths of other subjects, as a function of the scale factor. [sent-183, score-0.593]
</p><p>44 To measure inter-subject agreement, we match the scanpath of each subject i to the scanpaths belonging to other subjects, under the two metrics deﬁned above. [sent-189, score-0.632]
</p><p>45 First, for cross-stimulus control, we evaluate agreement as in the case of spatial consistency, when the test and reference scanpaths correspond to different randomly selected images. [sent-193, score-0.586]
</p><p>46 The average metric assigned to these scanpaths with respect to the subjects represents the baseline for sequential inter-subject agreement, in the absence of bias. [sent-195, score-0.626]
</p><p>47 Third, we randomize the order of each subject’s ﬁxations in each image, while keeping their locations ﬁxed, and compute inter-subject agreement with respect to the original scanpaths of the rest of the subjects. [sent-196, score-0.541]
</p><p>48 2 Inﬂuence of Task: We will next study the task inﬂuence on human visual patterns. [sent-202, score-0.384]
</p><p>49 We compare the visual patterns of the two subject groups using saliency map and sequential AOI metrics. [sent-203, score-0.526]
</p><p>50 Evaluation Protocol: For each image, we derive a saliency map from the ﬁxations of subjects doing action recognition, and report the average p-statistic at the locations ﬁxated by subjects performing context recognition. [sent-204, score-0.788]
</p><p>51 We also compute agreement under the AOI-based metrics between the scanpaths of subjects performing context recognition, and subjects from the action recognition group. [sent-205, score-1.164]
</p><p>52 1% of ﬁxations made during context recognition fall onto action recognition AOIs, with an average p-value of 0. [sent-207, score-0.417]
</p><p>53 Only 10% of the context recognition saccades have also been made by active subjects, and the AOIS metric between context and active subjects’ scanpaths is 23. [sent-209, score-0.684]
</p><p>54 6  Task-Speciﬁc Human Gaze Prediction  In this section, we show that it is possible to effectively predict task-speciﬁc human gaze patterns, both spatially and sequentially. [sent-212, score-0.365]
</p><p>55 2 Although harder to interpret numerically, the negative log likelihood of scanpaths under HMMs also deﬁnes a valid sequential consistency measure. [sent-214, score-0.487]
</p><p>56 We observe the following values for the action recognition task: agreement 9. [sent-215, score-0.377]
</p><p>57 5  consistency measure agreement agreement (random order) cross-stimulus control random scanpaths  task action recognition context recognition AOIP AOIT AOIS AOIP AOIT AOIS 79. [sent-220, score-1.193]
</p><p>58 A large fraction of each subject’s ﬁxations falls onto AOIs derived from the scanpaths of the other subjects (AOIP). [sent-270, score-0.535]
</p><p>59 1  Task-Speciﬁc Human Visual Saliency Prediction  We ﬁrst study the prediction of human visual saliency maps. [sent-273, score-0.591]
</p><p>60 Human ﬁxations typically fall onto image regions that are meaningful for the visual task (ﬁg. [sent-274, score-0.285]
</p><p>61 The KL divergence score for this baseline is derived by splitting the human subjects into two groups and computing the KL divergence between the saliency maps derived from these two groups, while the AUC metric is computed in a leave-one-out fashion, as for spatial consistency. [sent-289, score-0.8]
</p><p>62 Findings: Itti&Koch;’s model is not designed to predict task-speciﬁc saliency and cannot handle task inﬂuences on visual attention (ﬁg. [sent-295, score-0.473]
</p><p>63 2  Scanpath Prediction via Maximum Entropy Inverse Reinforcement Learning  We now consider the problem of eye movement prediction under speciﬁc task constraints. [sent-304, score-0.579]
</p><p>64 Models of human visual saliency can be used to generate scanpaths, e. [sent-305, score-0.561]
</p><p>65 However, current models are designed to predict saliency for the free-viewing condition and do not capture the focus induced by the cognitive task. [sent-308, score-0.295]
</p><p>66 Others [20, 4] hypothesize that the reward driving eye movements is the expected future information gain. [sent-309, score-0.472]
</p><p>67 Instead of specifying the reward function, we learn it directly from large amounts of human eye movement data, by exploiting policies that operate over long time horizons. [sent-311, score-0.725]
</p><p>68 We cast the problem as Inverse Reinforcement Learning (IRL), where we aim to recover the intrinsic reward function that induces, with high probability, the scanpaths recorded from human subjects solving a speciﬁc visual recognition task. [sent-312, score-1.031]
</p><p>69 66  feature human scanpaths random scanpaths IRL∗ Renninger [20] Itti & Koch [11]  0. [sent-335, score-0.953]
</p><p>70 6%  (b) eye movement prediction  (a) human visual saliency prediction 1  baselines action recognition AOIP AOIT AOIS 79. [sent-365, score-1.369]
</p><p>71 05 0 0  1  2 AOI scale factor  3  0 0  4  1  2 AOI scale factor  3  0 0  4  1  2 AOI scale factor  3  4  (c)  Figure 4: Task-speciﬁc human gaze prediction performance on the VOC 2012 actions dataset. [sent-389, score-0.406]
</p><p>72 (a) Our trained HOG detector outperforms existing saliency models, when evaluated under both the KL divergence and AUC metrics. [sent-390, score-0.318]
</p><p>73 (b-c) Learning techniques can also be used to predict eye movements under task constraints. [sent-391, score-0.51]
</p><p>74 Our proposed Inverse Reinforcement Learning (IRL) model better matches observed human visual search scanpaths when compared with two existing methods, under each of the AOI based metrics we introduce. [sent-392, score-0.766]
</p><p>75 The reward function rθ (st , at ) = f (st )θ at is the inner product between a feature vector f(st ) extracted at image location st and a vector of weights corresponding to action at . [sent-397, score-0.331]
</p><p>76 In our formulation, the goal of Maximum Entropy IRL is to ﬁnd the weights θ that maximize the likelihood of the demonstrated scanpaths across all the images in the dataset. [sent-400, score-0.41]
</p><p>77 For a single image and given the set of human scanpaths E, all starting at the image center sc , the likelihood is: 1 (sc ,T ) log pθ (δ) (2) Lθ = |E| δ∈E  This maximization problem can be solved using a two step dynamic programming formulation. [sent-401, score-0.751]
</p><p>78 In the backward step, we compute the state and state-action partition functions for each possible state s and action a, and for each scanpath length i = 1, T : i (i)  Zθ (s) = δ∈∆  (s,i)  i (i)  exp  rθ (st , at ) , Zθ (s, a) = t=1  exp (s,i)  δ∈∆ s. [sent-402, score-0.298]
</p><p>79 Note that we restrict ourselves to scanpaths of length T starting from the center of the screen and do not predeﬁne goal states. [sent-411, score-0.426]
</p><p>80 We then encode all scanpaths in this discrete (state,action) space, with an average positional error of 0. [sent-417, score-0.383]
</p><p>81 Findings: A trained MaxEnt IRL eye movement predictor performs better than the bottom up models of Itti&Koch;[11] and Renninger et al. [sent-421, score-0.477]
</p><p>82 The model is particularly powerful for predicting saccades (see the AOIT metric), as it can match more than twice the number of AOI transitions generated by bottom up models for the action recognition task. [sent-424, score-0.421]
</p><p>83 A gap still remains to human performance, underlining the difﬁculty of predicting eye movements in real world images and for complex tasks such as action recognition. [sent-427, score-0.799]
</p><p>84 7  Conclusions  We have collected a large set of eye movement recordings for VOC 2012 Actions, one of the most challenging datasets for action recognition in still images. [sent-431, score-0.76]
</p><p>85 Our data is obtained under the task constraints of action and context recognition and is made publicly available. [sent-432, score-0.371]
</p><p>86 We have leveraged this large amount of data (1 million human ﬁxations) in order to develop Hidden Markov Models that allow us to determine ﬁxated AOI locations, their spatial support and the transitions between them automatically from eyetracking data. [sent-433, score-0.397]
</p><p>87 This technique has made possible to develop novel evaluation metrics and to perform quantitative analysis regarding inter-subject consistency and the inﬂuence of task on eye movements. [sent-434, score-0.53]
</p><p>88 The results reveal that given real world unconstrained image stimuli, the task has a signiﬁcant inﬂuence on the observed eye movements both spatially and sequentially. [sent-435, score-0.568]
</p><p>89 We have also introduced a novel eye movement prediction model that combines state-of-the-art reinforcement learning techniques with advanced computer vision operators to learn task-speciﬁc human visual search patterns. [sent-437, score-0.95]
</p><p>90 To our knowledge, the method is the ﬁrst to learn eye movement models from human eyetracking data. [sent-438, score-0.726]
</p><p>91 When measured under various evaluation metrics, the model shows superior performance to existing bottom-up eye movement predictors. [sent-439, score-0.477]
</p><p>92 To close the human performance gap, better image features, and more complex joint state and action spaces, within reinforcement learning schemes, will be explored in future work. [sent-440, score-0.44]
</p><p>93 Viewing task inﬂuences eye movement control during active scene perception. [sent-472, score-0.573]
</p><p>94 Predicting human gaze using low-level saliency combined with face detection. [sent-486, score-0.567]
</p><p>95 Modeling search for people in 900 scenes: A combined source model of eye guidance. [sent-536, score-0.34]
</p><p>96 How to ﬁnd interesting locations in video: a spatiotemporal interest point detector learned from human eye movements. [sent-543, score-0.606]
</p><p>97 Dynamic eye movement datasets and learnt saliency models for visual action recognition. [sent-567, score-1.006]
</p><p>98 An eye ﬁxation database for saliency detection in images. [sent-584, score-0.569]
</p><p>99 Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search. [sent-591, score-0.441]
</p><p>100 Space-variant descriptor sampling for action recognition based on saliency and eye movements. [sent-598, score-0.819]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aois', 0.383), ('scanpaths', 0.383), ('eye', 0.32), ('aoi', 0.272), ('saliency', 0.249), ('xations', 0.245), ('human', 0.187), ('movement', 0.157), ('action', 0.155), ('subjects', 0.152), ('irl', 0.136), ('gaze', 0.131), ('agreement', 0.127), ('visual', 0.125), ('scanpath', 0.123), ('xation', 0.117), ('aoit', 0.111), ('aoip', 0.099), ('recognition', 0.095), ('movements', 0.091), ('saccades', 0.08), ('spatial', 0.076), ('judd', 0.074), ('renninger', 0.074), ('task', 0.072), ('xated', 0.07), ('consistency', 0.066), ('image', 0.065), ('eyetracking', 0.062), ('findings', 0.062), ('reward', 0.061), ('vision', 0.059), ('actions', 0.058), ('itti', 0.057), ('stimulus', 0.055), ('voc', 0.054), ('subject', 0.053), ('metrics', 0.051), ('st', 0.05), ('transitions', 0.05), ('detector', 0.05), ('context', 0.049), ('koch', 0.043), ('hog', 0.041), ('auc', 0.038), ('sequential', 0.038), ('patterns', 0.038), ('cross', 0.037), ('person', 0.035), ('collected', 0.033), ('reinforcement', 0.033), ('saccadic', 0.033), ('protocol', 0.033), ('locations', 0.031), ('hmm', 0.031), ('prediction', 0.03), ('object', 0.03), ('kl', 0.028), ('recorded', 0.028), ('sc', 0.028), ('metric', 0.028), ('uence', 0.027), ('predict', 0.027), ('images', 0.027), ('exposure', 0.026), ('baseline', 0.025), ('hmms', 0.025), ('castelhano', 0.025), ('cerf', 0.025), ('control', 0.024), ('groups', 0.023), ('fall', 0.023), ('center', 0.023), ('static', 0.022), ('score', 0.022), ('video', 0.022), ('areas', 0.022), ('durand', 0.022), ('match', 0.022), ('automatically', 0.022), ('humans', 0.021), ('book', 0.021), ('quantitative', 0.021), ('baselines', 0.021), ('stimuli', 0.021), ('inverse', 0.02), ('descriptors', 0.02), ('length', 0.02), ('search', 0.02), ('inter', 0.02), ('riding', 0.02), ('xating', 0.02), ('spatially', 0.02), ('cognitive', 0.019), ('divergence', 0.019), ('predicting', 0.019), ('computer', 0.019), ('asked', 0.019), ('entropy', 0.019), ('interest', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="21-tfidf-1" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><p>2 0.29438442 <a title="21-tfidf-2" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>3 0.17486063 <a title="21-tfidf-3" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>Author: Sheeraz Ahmad, He Huang, Angela J. Yu</p><p>Abstract: Humans and animals readily utilize active sensing, or the use of self-motion, to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment. Understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artiﬁcial systems. Recently, we proposed a goal-directed, context-sensitive, Bayesian control strategy for active sensing, C-DAC (ContextDependent Active Controller) (Ahmad & Yu, 2013). In contrast to previously proposed algorithms for human active vision, which tend to optimize abstract statistical objectives and therefore cannot adapt to changing behavioral context or task goals, C-DAC directly minimizes behavioral costs and thus, automatically adapts itself to different task conditions. However, C-DAC is limited as a model of human active sensing, given its computational/representational requirements, especially for more complex, real-world situations. Here, we propose a myopic approximation to C-DAC, which also takes behavioral costs into account, but achieves a signiﬁcant reduction in complexity by looking only one step ahead. We also present data from a human active visual search experiment, and compare the performance of the various models against human behavior. We ﬁnd that C-DAC and its myopic variant both achieve better ﬁt to human data than Infomax (Butko & Movellan, 2010), which maximizes expected cumulative future information gain. In summary, this work provides novel experimental results that differentiate theoretical models for human active sensing, as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving signiﬁcant computational savings. 1</p><p>4 0.12887976 <a title="21-tfidf-4" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>5 0.12512103 <a title="21-tfidf-5" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>Author: Ali Borji, Laurent Itti</p><p>Abstract: Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to ﬁnd the function’s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploits all the x values tried and all the f (x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further conﬁrm that Gaussian Processes provide a general and uniﬁed theoretical account to explain passive and active function learning and search in humans. 1</p><p>6 0.10035207 <a title="21-tfidf-6" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>7 0.09576416 <a title="21-tfidf-7" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>8 0.095322393 <a title="21-tfidf-8" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>9 0.090544701 <a title="21-tfidf-9" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>10 0.087817125 <a title="21-tfidf-10" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>11 0.084221274 <a title="21-tfidf-11" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>12 0.077908128 <a title="21-tfidf-12" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>13 0.077753425 <a title="21-tfidf-13" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>14 0.076751478 <a title="21-tfidf-14" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>15 0.074474469 <a title="21-tfidf-15" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>16 0.07200928 <a title="21-tfidf-16" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>17 0.070828892 <a title="21-tfidf-17" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>18 0.070357181 <a title="21-tfidf-18" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>19 0.069990329 <a title="21-tfidf-19" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>20 0.069910109 <a title="21-tfidf-20" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, -0.035), (2, -0.147), (3, -0.046), (4, 0.027), (5, -0.096), (6, 0.016), (7, -0.026), (8, -0.085), (9, 0.067), (10, -0.142), (11, -0.003), (12, -0.027), (13, -0.055), (14, -0.14), (15, 0.072), (16, -0.066), (17, -0.115), (18, -0.006), (19, -0.114), (20, -0.017), (21, 0.082), (22, -0.053), (23, -0.031), (24, -0.024), (25, -0.031), (26, -0.031), (27, -0.006), (28, -0.142), (29, 0.083), (30, -0.065), (31, -0.122), (32, -0.035), (33, -0.046), (34, 0.125), (35, -0.139), (36, -0.178), (37, 0.112), (38, -0.029), (39, 0.009), (40, 0.138), (41, 0.048), (42, 0.054), (43, -0.096), (44, 0.026), (45, 0.028), (46, -0.049), (47, -0.101), (48, 0.017), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95395273 <a title="21-lsi-1" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><p>2 0.75569546 <a title="21-lsi-2" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>3 0.66939461 <a title="21-lsi-3" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>Author: Sheeraz Ahmad, He Huang, Angela J. Yu</p><p>Abstract: Humans and animals readily utilize active sensing, or the use of self-motion, to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment. Understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artiﬁcial systems. Recently, we proposed a goal-directed, context-sensitive, Bayesian control strategy for active sensing, C-DAC (ContextDependent Active Controller) (Ahmad & Yu, 2013). In contrast to previously proposed algorithms for human active vision, which tend to optimize abstract statistical objectives and therefore cannot adapt to changing behavioral context or task goals, C-DAC directly minimizes behavioral costs and thus, automatically adapts itself to different task conditions. However, C-DAC is limited as a model of human active sensing, given its computational/representational requirements, especially for more complex, real-world situations. Here, we propose a myopic approximation to C-DAC, which also takes behavioral costs into account, but achieves a signiﬁcant reduction in complexity by looking only one step ahead. We also present data from a human active visual search experiment, and compare the performance of the various models against human behavior. We ﬁnd that C-DAC and its myopic variant both achieve better ﬁt to human data than Infomax (Butko & Movellan, 2010), which maximizes expected cumulative future information gain. In summary, this work provides novel experimental results that differentiate theoretical models for human active sensing, as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving signiﬁcant computational savings. 1</p><p>4 0.565332 <a title="21-lsi-4" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>Author: Shane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles Isbell, Andrea L. Thomaz</p><p>Abstract: A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of-the-art methods have approached this problem by mapping human information to rewards and values and iterating over them to compute better control policies. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct policy labels. We compare Advise to state-of-the-art approaches and show that it can outperform them and is robust to infrequent and inconsistent human feedback.</p><p>5 0.56319684 <a title="21-lsi-5" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>6 0.5113185 <a title="21-lsi-6" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>7 0.5025717 <a title="21-lsi-7" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>8 0.48750493 <a title="21-lsi-8" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>9 0.47874048 <a title="21-lsi-9" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>10 0.47024161 <a title="21-lsi-10" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>11 0.44197673 <a title="21-lsi-11" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>12 0.44190019 <a title="21-lsi-12" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>13 0.44186443 <a title="21-lsi-13" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>14 0.41893139 <a title="21-lsi-14" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>15 0.38652951 <a title="21-lsi-15" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>16 0.36959818 <a title="21-lsi-16" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>17 0.36159706 <a title="21-lsi-17" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>18 0.35095781 <a title="21-lsi-18" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>19 0.34735435 <a title="21-lsi-19" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>20 0.34396806 <a title="21-lsi-20" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.029), (27, 0.027), (33, 0.153), (34, 0.068), (41, 0.028), (49, 0.034), (56, 0.102), (70, 0.031), (75, 0.288), (85, 0.041), (89, 0.038), (93, 0.058), (95, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7928502 <a title="21-lda-1" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><p>2 0.74050039 <a title="21-lda-2" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>Author: Tuo Zhao, Han Liu</p><p>Abstract: We propose a semiparametric method for estimating sparse precision matrix of high dimensional elliptical distribution. The proposed method calibrates regularizations when estimating each column of the precision matrix. Thus it not only is asymptotically tuning free, but also achieves an improved ﬁnite sample performance. Theoretically, we prove that the proposed method achieves the parametric rates of convergence in both parameter estimation and model selection. We present numerical results on both simulated and real datasets to support our theory and illustrate the effectiveness of the proposed estimator. 1</p><p>3 0.72327811 <a title="21-lda-3" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>Author: Qiang Cheng, Qiang Liu, Feng Chen, Alex Ihler</p><p>Abstract: Markov Decision Processes (MDPs) are extremely useful for modeling and solving sequential decision making problems. Graph-based MDPs provide a compact representation for MDPs with large numbers of random variables. However, the complexity of exactly solving a graph-based MDP usually grows exponentially in the number of variables, which limits their application. We present a new variational framework to describe and solve the planning problem of MDPs, and derive both exact and approximate planning algorithms. In particular, by exploiting the graph structure of graph-based MDPs, we propose a factored variational value iteration algorithm in which the value function is ﬁrst approximated by the multiplication of local-scope value functions, then solved by minimizing a Kullback-Leibler (KL) divergence. The KL divergence is optimized using the belief propagation algorithm, with complexity exponential in only the cluster size of the graph. Experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at ﬁnding good policies. 1</p><p>4 0.70532936 <a title="21-lda-4" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>5 0.70248574 <a title="21-lda-5" href="./nips-2013-RNADE%3A_The_real-valued_neural_autoregressive_density-estimator.html">260 nips-2013-RNADE: The real-valued neural autoregressive density-estimator</a></p>
<p>Author: Benigno Uria, Iain Murray, Hugo Larochelle</p><p>Abstract: We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of onedimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradientbased optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, ﬁnding it outperforms mixture models in all but one case. 1</p><p>6 0.62735689 <a title="21-lda-6" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>7 0.625036 <a title="21-lda-7" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>8 0.59106624 <a title="21-lda-8" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>9 0.59080386 <a title="21-lda-9" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>10 0.59046966 <a title="21-lda-10" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>11 0.59038484 <a title="21-lda-11" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>12 0.5900262 <a title="21-lda-12" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>13 0.58954591 <a title="21-lda-13" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>14 0.5891102 <a title="21-lda-14" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>15 0.58880103 <a title="21-lda-15" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>16 0.58855587 <a title="21-lda-16" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>17 0.58815521 <a title="21-lda-17" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>18 0.5881542 <a title="21-lda-18" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>19 0.58761263 <a title="21-lda-19" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>20 0.58733428 <a title="21-lda-20" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
