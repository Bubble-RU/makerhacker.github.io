<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-27" href="#">nips2013-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</h1>
<br/><p>Source: <a title="nips-2013-27-pdf" href="http://papers.nips.cc/paper/5030-adaptive-multi-column-deep-neural-networks-with-application-to-robust-image-denoising.pdf">pdf</a></p><p>Author: Forest Agostinelli, Michael R. Anderson, Honglak Lee</p><p>Abstract: Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. To address this limitation, we present the adaptive multi-column stacked sparse denoising autoencoder (AMC-SSDA), a novel technique of combining multiple SSDAs by (1) computing optimal column weights via solving a nonlinear optimization program and (2) training a separate network to predict the optimal weights. We eliminate the need to determine the type of noise, let alone its statistics, at test time and even show that the system can be robust to noise not seen in the training set. We show that state-of-the-art denoising performance can be achieved with a single system on a variety of different noise types. Additionally, we demonstrate the efﬁcacy of AMC-SSDA as a preprocessing (denoising) algorithm by achieving strong classiﬁcation performance on corrupted MNIST digits. 1</p><p>Reference: <a title="nips-2013-27-reference" href="../nips2013_reference/nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. [sent-3, score-0.607]
</p><p>2 However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. [sent-4, score-0.54]
</p><p>3 We eliminate the need to determine the type of noise, let alone its statistics, at test time and even show that the system can be robust to noise not seen in the training set. [sent-6, score-0.313]
</p><p>4 We show that state-of-the-art denoising performance can be achieved with a single system on a variety of different noise types. [sent-7, score-0.461]
</p><p>5 1  Introduction  Digital images are often corrupted with noise during acquisition and transmission, degrading performance in later tasks such as: image recognition and medical diagnosis. [sent-9, score-0.485]
</p><p>6 Many denoising algorithms have been proposed to improve the accuracy of these tasks when corrupted images must be used. [sent-10, score-0.515]
</p><p>7 However, most of these methods are carefully designed only for a certain type of noise or require assumptions about the statistical properties of the corrupting noise. [sent-11, score-0.243]
</p><p>8 Median ﬁltering outperforms linear ﬁltering for suppressing noise in images with edges and gives good output for salt & pepper noise [2], but it is not as effective for the removal of additive Gaussian noise [1]. [sent-13, score-0.772]
</p><p>9 Periodic noise such as scan-line noise is difﬁcult to eliminate using spatial ﬁltering but is relatively easy to remove using Fourier domain band-stop ﬁlters once the period of the noise is known [6]. [sent-14, score-0.469]
</p><p>10 As radiation dose is decreased, noise levels in medical images increases [12, 16], so noise reduction techniques have been key to maintaining image quality while improving patient safety [27]. [sent-16, score-0.641]
</p><p>11 Recently, various types of neural networks have been evaluated for their denoising efﬁcacy. [sent-18, score-0.393]
</p><p>12 [31] had success at removing noise from corrupted images with the stacked sparse denoising 1  autoencoder (SSDA). [sent-20, score-0.842]
</p><p>13 The SSDA is trained on images corrupted with a particular noise type, so it too has a dependence on a priori knowledge about the general nature of the noise. [sent-21, score-0.425]
</p><p>14 In this paper, we present the adaptive multi-column sparse stacked denoising autoencoder (AMCSSDA), a method to improve the SSDA’s robustness to various noise types. [sent-22, score-0.662]
</p><p>15 In the AMC-SSDA, columns of single-noise SSDAs are run in parallel and their outputs are linearly combined to produce the ﬁnal denoised image. [sent-23, score-0.172]
</p><p>16 Taking advantage of the sparse autoencoder’s capability for learning features, the features encoded by the hidden layers of each SSDA are supplied to an additional network to determine the optimal weighting for each column in the ﬁnal linear combination. [sent-24, score-0.222]
</p><p>17 We demonstrate that a single AMC-SSDA network provides better denoising results for both noise types present in the training set and for noise types not seen by the denoiser during training. [sent-25, score-0.862]
</p><p>18 A given instance of noise corruption might have features in common with one or more of the training set noise types, allowing the best combination of denoisers to be chosen based on that image’s speciﬁc noise characteristics. [sent-26, score-0.525]
</p><p>19 2  Related work  Numerous approaches have been proposed for image denoising using signal processing techniques (e. [sent-29, score-0.437]
</p><p>20 Some methods transfer the image signal to an alternative domain where noise can be easily separated from the signal [25, 21]. [sent-32, score-0.293]
</p><p>21 More recent approaches exploit the “non-local” statistics of images: different patches in the same image are often similar in appearance, and thus they can be used together in denoising [11, 22, 8]. [sent-35, score-0.45]
</p><p>22 [9] showed that it is possible to achieve state-of-the-art denoising performance with a plain multi-layer perceptron (MLP) that maps noisy patches onto noise-free ones, once the capacity of the MLP, the patch size, and the training set are large enough. [sent-39, score-0.449]
</p><p>23 [29] introduced the stacked denoising autoencoders as a way of providing a good initial representation of the data in deep networks for classiﬁcation tasks. [sent-42, score-0.525]
</p><p>24 Our proposed AMC-SSDA builds upon this work by using the denoising autoencoder’s internal representation to determine the optimal column weighting for robust denoising. [sent-43, score-0.388]
</p><p>25 [10] presented a multi-column approach for image classiﬁcation, averaging the output ¸ of several deep neural networks (or columns) trained on inputs preprocessed in different ways. [sent-45, score-0.296]
</p><p>26 , simply averaging the output of each column) is not robust in denoising since each column has been trained on a different type of noise. [sent-48, score-0.511]
</p><p>27 To address this problem, we propose an adaptive weighting scheme that can handle a variety of noise types. [sent-49, score-0.196]
</p><p>28 [18] used deep convolutional neural networks for image denoising. [sent-51, score-0.223]
</p><p>29 Rather than using a convolutional approach, our proposed method applies multiple sparse autoencoder networks in combination to the denoising task. [sent-52, score-0.442]
</p><p>30 , extensions of the deep belief network with local receptive ﬁelds) to denoising and classifying MNIST digits. [sent-56, score-0.413]
</p><p>31 Then we will present the AMC-SSDA and describe the process of ﬁnding optimal column weights and predicting column weights for test images. [sent-59, score-0.218]
</p><p>32 1  Stacked sparse denoising autoencoders  A denoising autoencoder (DA) [29] is typically used as a way to pre-train layers in a deep neural network, avoiding the difﬁculty in training such a network as a whole from scratch by performing greedy layer-wise training (e. [sent-61, score-0.979]
</p><p>33 [31] showed, a denoising autoencoder is 2  also a natural ﬁt for performing denoising tasks, due to its behavior of taking a noisy signal as input and reconstructing the original, clean signal as output. [sent-65, score-0.805]
</p><p>34 Commonly, a series of DAs are connected to form a stacked denoising autoencoder (SDA)—a deep network formed by feeding the hidden layer’s activations of one DA into the input of the next DA. [sent-66, score-0.656]
</p><p>35 Typically, SDAs are pre-trained in an unsupervised fashion where each DA layer is trained by generating new noise [29]. [sent-67, score-0.254]
</p><p>36 ’s method of SDA training by calculating the ﬁrst layer activations for both the clean input and noisy input to use as training data for the second layer. [sent-69, score-0.22]
</p><p>37 As they showed, this modiﬁcation to the training process allows the SDA to better learn the features for denoising the original corrupting noise. [sent-70, score-0.424]
</p><p>38 In this work, two DAs are stacked as shown in Figure 1a, where the activation of the ﬁrst DA’s hidden layer provides the input to the second DA, which in turn provides the input to the output layer of the ﬁrst DA. [sent-80, score-0.248]
</p><p>39 2  Adaptive Multi-Column SSDA  The adaptive multi-column SSDA is the linear combination of several SSDAs, or columns, each trained on a single type of noise using optimized weights determined by the features of each given input image. [sent-85, score-0.37]
</p><p>40 Taking advantage of the SSDA’s capability of feature learning, we use the features generated by the activation of the SSDA’s hidden layers as inputs to a neural network-based regression component, referred to here as the weight prediction module. [sent-86, score-0.162]
</p><p>41 2 After pre-training, we initialized W(1) and W(4) from the encoding and decoding weights of the ﬁrst-layer DA, and W(2) and W(3) from the encoding and decoding weights of the second-layer DA, respectively. [sent-90, score-0.184]
</p><p>42 , fc denotes the concatenated hidden unit vectors h(1) (x) and h(2) (x) of the SSDA corresponding to c-th column) as input features to the weight prediction module for determining the optimal weight for each column of the AMC-SSDA. [sent-118, score-0.28]
</p><p>43 1  Training the AMC-SSDA  The AMC-SSDA has three training phases: training the SSDAs, determining optimal weights for a set of training images, and then training the weight prediction module. [sent-122, score-0.271]
</p><p>44 1, with each SSDA provided a noisy training set, corrupted by a single noise type along with the original versions of those images as a target set. [sent-124, score-0.498]
</p><p>45 Each SSDA column c then produces an output yc ∈ RD for an input x ∈ RD , which is the noisy version of original image y. [sent-125, score-0.249]
</p><p>46 2  Finding optimal column weights via quadratic program  Once the SSDAs are trained, we construct a new training set that pairs features extracted from the hidden layers of the SSDAs with optimal column weights. [sent-129, score-0.303]
</p><p>47 That is, for SSDA column c, the activations of hidden layers h(1) and h(2) (as shown in Figure 1a) are concatenated into a vector fc , and then f1 , f2 , . [sent-134, score-0.231]
</p><p>48 ˆ Additionally, the output of each column for each image is collected into a matrix Y = [y1 , . [sent-138, score-0.183]
</p><p>49 (8) helps to avoid degenerate cases where weights for very bright or dark spots 3 In addition to the L2 error shown in Equation (6), we also tested minimizing the L1 distance as the error function, which is a standard method in the related ﬁeld of image registration [3]. [sent-145, score-0.189]
</p><p>50 3  Learning to predict optimal column weights via RBF networks  The ﬁnal training phase is to train the weight prediction module. [sent-177, score-0.2]
</p><p>51 A radial basis function (RBF) network is trained to take the feature vector φ as input and produce a weight vector s, using the optimal weight training set described in Section 3. [sent-178, score-0.202]
</p><p>52 A noisy image x is supplied ˆ as input to each of the columns, which together produce the output matrix Y, each column of which is the output of a particular column of the AMC-SSDA. [sent-186, score-0.314]
</p><p>53 The ﬁnal denoised image y is produced by linearly combining the columns ˆ ˆ using these weights: y = Ys∗ . [sent-192, score-0.281]
</p><p>54 4 ˆ  4  Experiments  We performed a number of denoising tasks by corrupting and denoising images of computed tomography (CT) scans of the head from the Cancer Imaging Archive [17] (Section 4. [sent-193, score-0.765]
</p><p>55 Quantitative evaluation of denoising results was performed using peak signal-to-noise ratio (PSNR), a standard method used for evaluating denoising performance. [sent-195, score-0.622]
</p><p>56 We also tested the AMC-SSDA as pre-processing step in an image classiﬁcation task by corrupting MNIST database of handwritten digits [19] with various types of noise and then denoising and classifying the digits with a classiﬁer trained on the original images (Section 4. [sent-197, score-0.934]
</p><p>57 1  Image denoising  To evaluate general denoising performance, images of CT scans of the head were corrupted with seven variations of Gaussian, salt-and-pepper, and speckle noise, resulting in the 21 noise types shown in Table 1. [sent-203, score-1.286]
</p><p>58 Twenty-one individual SSDAs were trained on randomly selected 8-by-8 pixel patches from the corrupted images; each SSDA was trained on a single type of noise. [sent-204, score-0.362]
</p><p>59 To train the weight predictor for the AMC-SSDA, a set of images disjoint from the training set of the individual SSDAs were used. [sent-209, score-0.164]
</p><p>60 The training images for the AMC-SSDA were corrupted with the same noise types used to train its columns. [sent-210, score-0.462]
</p><p>61 4 100%  Table 2: Parameters of noise types used for testing. [sent-235, score-0.208]
</p><p>62 The Poisson and uniform noise types are not seen in the training set. [sent-236, score-0.279]
</p><p>63 (a) Original  (b) Noisy  (c) Mixed-SSDA  (d) AMC-SSDA  Figure 2: Visualization of the denoising performance of the Mixed-SSDA and AMC-SSDA. [sent-239, score-0.311]
</p><p>64 When testing, 64-by-64 pixel patches are denoised with a stride of 48. [sent-244, score-0.208]
</p><p>65 The AMCSSDA is ﬁrst tested on noise types that have been seen (i. [sent-248, score-0.264]
</p><p>66 , noise types that were in the training set) but have different statistics. [sent-250, score-0.258]
</p><p>67 It is then tested on noise not seen in the training examples, referred to as “unseen” noise. [sent-251, score-0.256]
</p><p>68 [31], one SSDA was trained on only the Gaussian noise types, one on only salt & pepper, one on only speckle, and one on all the noise types from Table 1. [sent-253, score-0.529]
</p><p>69 We refer to these as gaussian SSDA, s&p; SSDA, speckle SSDA, and mixed SSDA, respectively. [sent-254, score-0.291]
</p><p>70 These SSDAs were then tested on the same types of noise that the AMC-SSDA was tested on. [sent-255, score-0.278]
</p><p>71 The results for both seen and unseen noise can be found in Tables 3 and 4. [sent-256, score-0.201]
</p><p>72 Speciﬁcally, each test image corrupted with seen noise was denoised with an SSDA that had been trained on the exact type of noise and statistics that the test image has been corrupted with; we call this the “informed-SSDA. [sent-268, score-1.071]
</p><p>73 Figure 4: Average PSNR values for denoised images of various previously unseen noise types (P: Poisson noise; U: Uniform noise). [sent-427, score-0.464]
</p><p>74 and salt & pepper noise and slightly worse on speckle noise. [sent-428, score-0.6]
</p><p>75 This suggests that the AMC-SSDA can perform as well as using an ”ideally” trained network for speciﬁc noise type (i. [sent-433, score-0.305]
</p><p>76 , training and testing an SSDA for the same speciﬁc noise type). [sent-435, score-0.2]
</p><p>77 2  Digit recognition from denoised images  Since the results of denoising images from a visual standpoint can be more qualitative than quantitative, we have tested using denoising as a preprocessing step done before a classiﬁcation task. [sent-438, score-0.989]
</p><p>78 Speciﬁcally, we used the MNIST database of handwritten digits [19] as benchmark to evaluate the efﬁcacy of our denoising procedures. [sent-439, score-0.348]
</p><p>79 First, we trained a deep neural network digit classiﬁer from the MNIST training digits, following [15]. [sent-440, score-0.247]
</p><p>80 The MNIST digits are corrupted with Gaussian, salt & pepper, speckle, block, and border noise. [sent-443, score-0.297]
</p><p>81 Noisy images are shown on top and the corresponding denoised images by the AMC-SSDA are shown below. [sent-446, score-0.314]
</p><p>82 The goal of this experiment is to show that the potential cumbersome and time-consuming process of determining the type of noise that an image is corrupted with at test time is not needed to achieve good classiﬁcation results. [sent-452, score-0.448]
</p><p>83 As the results show in Table 3, the denoising performance was strongly correlated to the type of noise upon which the denoiser was trained. [sent-453, score-0.551]
</p><p>84 The bold-faced values show the best performing denoiser for a given noise type. [sent-454, score-0.185]
</p><p>85 06%), the SSDA that was trained with the same type of noise as in the test data. [sent-459, score-0.294]
</p><p>86 In terms of average error across all types of noises, the AMC-SSDA is signiﬁcantly better than any single denoising algorithms we compared. [sent-460, score-0.369]
</p><p>87 The results suggest that the AMC-SSDA consistently achieves strong classiﬁcation performance without having to determine the type of noise during test time. [sent-461, score-0.223]
</p><p>88 We show that we get better classiﬁcation accuracy for the block and border noise types. [sent-464, score-0.214]
</p><p>89 Method / Noise Type No denoising Gaussian SSDA Salt & Pepper SSDA Speckle SSDA Block SSDA Border SSDA AMC-SSDA Tang et al. [sent-469, score-0.331]
</p><p>90 Rows denote the performance of different denoising methods, including: “no denoising,” SSDA trained on a speciﬁc noise type, and AMC-SSDA. [sent-523, score-0.532]
</p><p>91 Columns represent images corrupted with the given noise type. [sent-524, score-0.354]
</p><p>92 Percentage values are classiﬁcation error rates for a set of test images corrupted with the given noise type and denoised prior to classiﬁcation. [sent-525, score-0.565]
</p><p>93 Bold-faced values represent the best performance for images corrupted by a given noise type. [sent-526, score-0.354]
</p><p>94 We have demonstrated that AMCSSDA can robustly denoise images corrupted by multiple different types of noise without knowledge of the noise type at testing time. [sent-530, score-0.637]
</p><p>95 It has also been shown to perform well on types of noise that were not in the training set. [sent-531, score-0.258]
</p><p>96 The good classiﬁcation results of denoised MNIST digits also support the hypothesis that the AMC-SSDA eliminates the need to know about the type of noise during test time. [sent-533, score-0.398]
</p><p>97 A review of image denoising algorithms, with a new one. [sent-585, score-0.42]
</p><p>98 Image denoising using scale mixtures of Gaussians in the wavelet domain. [sent-701, score-0.311]
</p><p>99 Radiation o o dose reduction in CT of the brain: Can advanced noise ﬁltering compensate for loss of image quality? [sent-719, score-0.311]
</p><p>100 Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. [sent-733, score-0.724]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ssda', 0.744), ('denoising', 0.311), ('speckle', 0.235), ('ssdas', 0.183), ('noise', 0.15), ('denoised', 0.138), ('psnr', 0.123), ('corrupted', 0.116), ('pepper', 0.115), ('image', 0.109), ('salt', 0.1), ('autoencoder', 0.09), ('images', 0.088), ('stacked', 0.087), ('da', 0.082), ('deep', 0.073), ('trained', 0.071), ('mnist', 0.065), ('avg', 0.064), ('tang', 0.059), ('types', 0.058), ('type', 0.055), ('column', 0.055), ('xie', 0.053), ('fc', 0.052), ('amcssda', 0.052), ('dose', 0.052), ('radiation', 0.052), ('training', 0.05), ('weights', 0.045), ('sc', 0.044), ('border', 0.044), ('noisy', 0.039), ('hidden', 0.038), ('corrupting', 0.038), ('activation', 0.038), ('sp', 0.037), ('digits', 0.037), ('module', 0.035), ('layers', 0.035), ('tested', 0.035), ('denoiser', 0.035), ('sda', 0.035), ('columns', 0.034), ('classi', 0.034), ('layer', 0.033), ('ltering', 0.032), ('unseen', 0.03), ('autoencoders', 0.03), ('patches', 0.03), ('decoding', 0.029), ('network', 0.029), ('activations', 0.028), ('gaussian', 0.028), ('mixed', 0.028), ('yc', 0.027), ('noises', 0.027), ('weight', 0.026), ('agostinelli', 0.026), ('amc', 0.026), ('features', 0.025), ('poisson', 0.025), ('networks', 0.024), ('adaptive', 0.024), ('digit', 0.024), ('das', 0.024), ('burger', 0.023), ('portilla', 0.023), ('concatenated', 0.023), ('medical', 0.022), ('rbf', 0.022), ('weighting', 0.022), ('stride', 0.021), ('ct', 0.021), ('cacy', 0.021), ('digital', 0.021), ('seen', 0.021), ('rd', 0.02), ('clean', 0.02), ('et', 0.02), ('denoise', 0.02), ('block', 0.02), ('pixel', 0.019), ('patch', 0.019), ('uncorrupted', 0.019), ('eliminate', 0.019), ('output', 0.019), ('imaging', 0.018), ('test', 0.018), ('supplied', 0.018), ('motwani', 0.018), ('patient', 0.018), ('preprocessing', 0.018), ('encoding', 0.018), ('ciresan', 0.017), ('scans', 0.017), ('convolutional', 0.017), ('anderson', 0.017), ('mc', 0.017), ('signal', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="27-tfidf-1" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>Author: Forest Agostinelli, Michael R. Anderson, Honglak Lee</p><p>Abstract: Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. To address this limitation, we present the adaptive multi-column stacked sparse denoising autoencoder (AMC-SSDA), a novel technique of combining multiple SSDAs by (1) computing optimal column weights via solving a nonlinear optimization program and (2) training a separate network to predict the optimal weights. We eliminate the need to determine the type of noise, let alone its statistics, at test time and even show that the system can be robust to noise not seen in the training set. We show that state-of-the-art denoising performance can be achieved with a single system on a variety of different noise types. Additionally, we demonstrate the efﬁcacy of AMC-SSDA as a preprocessing (denoising) algorithm by achieving strong classiﬁcation performance on corrupted MNIST digits. 1</p><p>2 0.10496832 <a title="27-tfidf-2" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>3 0.097287036 <a title="27-tfidf-3" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>4 0.09697891 <a title="27-tfidf-4" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>Author: Yoshua Bengio, Li Yao, Guillaume Alain, Pascal Vincent</p><p>Abstract: Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying datagenerating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justiﬁcation which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-inﬁnitesimal corruption noise (or non-inﬁnitesimal contractive penalty). 1</p><p>5 0.089011148 <a title="27-tfidf-5" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>6 0.081659459 <a title="27-tfidf-6" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>7 0.080966182 <a title="27-tfidf-7" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>8 0.078793362 <a title="27-tfidf-8" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>9 0.070950605 <a title="27-tfidf-9" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>10 0.069057025 <a title="27-tfidf-10" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>11 0.067575388 <a title="27-tfidf-11" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>12 0.064614646 <a title="27-tfidf-12" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>13 0.062688276 <a title="27-tfidf-13" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>14 0.061988425 <a title="27-tfidf-14" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>15 0.060612198 <a title="27-tfidf-15" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>16 0.060322944 <a title="27-tfidf-16" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>17 0.059851147 <a title="27-tfidf-17" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>18 0.059643533 <a title="27-tfidf-18" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>19 0.059586894 <a title="27-tfidf-19" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>20 0.058283791 <a title="27-tfidf-20" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.076), (2, -0.083), (3, -0.046), (4, 0.063), (5, -0.082), (6, -0.058), (7, 0.046), (8, -0.04), (9, -0.073), (10, 0.035), (11, 0.006), (12, -0.037), (13, 0.039), (14, -0.025), (15, 0.014), (16, -0.01), (17, -0.016), (18, -0.02), (19, -0.01), (20, -0.001), (21, -0.019), (22, 0.025), (23, -0.01), (24, -0.003), (25, 0.032), (26, -0.03), (27, 0.02), (28, 0.057), (29, -0.018), (30, 0.043), (31, 0.073), (32, -0.032), (33, 0.032), (34, -0.047), (35, -0.033), (36, 0.03), (37, -0.025), (38, -0.027), (39, 0.03), (40, -0.007), (41, -0.08), (42, -0.011), (43, 0.056), (44, 0.038), (45, 0.022), (46, -0.004), (47, 0.045), (48, -0.021), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93442237 <a title="27-lsi-1" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>Author: Forest Agostinelli, Michael R. Anderson, Honglak Lee</p><p>Abstract: Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. To address this limitation, we present the adaptive multi-column stacked sparse denoising autoencoder (AMC-SSDA), a novel technique of combining multiple SSDAs by (1) computing optimal column weights via solving a nonlinear optimization program and (2) training a separate network to predict the optimal weights. We eliminate the need to determine the type of noise, let alone its statistics, at test time and even show that the system can be robust to noise not seen in the training set. We show that state-of-the-art denoising performance can be achieved with a single system on a variety of different noise types. Additionally, we demonstrate the efﬁcacy of AMC-SSDA as a preprocessing (denoising) algorithm by achieving strong classiﬁcation performance on corrupted MNIST digits. 1</p><p>2 0.71239555 <a title="27-lsi-2" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>Author: Karen Simonyan, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: As massively parallel computations have become broadly available with modern GPUs, deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classiﬁcation benchmarks such as ImageNet. However, elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper, we explore the extent of this analogy, proposing a version of the stateof-the-art Fisher vector image encoding that can be stacked in multiple layers. This architecture signiﬁcantly improves on standard Fisher vectors, and obtains competitive results with deep convolutional networks at a smaller computational learning cost. Our hybrid architecture allows us to assess how the performance of a conventional hand-crafted image classiﬁcation pipeline changes with increased depth. We also show that convolutional networks and Fisher vector encodings are complementary in the sense that their combination further improves the accuracy. 1</p><p>3 0.67155212 <a title="27-lsi-3" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>4 0.64078516 <a title="27-lsi-4" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>5 0.63780373 <a title="27-lsi-5" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Speciﬁcally, by using auxiliary natural images, we train a stacked denoising autoencoder ofﬂine to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from ofﬂine training to the online tracking process. Online tracking involves a classiﬁcation neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classiﬁcation layer. Both the feature extractor and the classiﬁer can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU). 1</p><p>6 0.63310772 <a title="27-lsi-6" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>7 0.60296893 <a title="27-lsi-7" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>8 0.59224713 <a title="27-lsi-8" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>9 0.58650237 <a title="27-lsi-9" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>10 0.58132935 <a title="27-lsi-10" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>11 0.57336533 <a title="27-lsi-11" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>12 0.5638265 <a title="27-lsi-12" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>13 0.55968618 <a title="27-lsi-13" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>14 0.55045903 <a title="27-lsi-14" href="./nips-2013-Stochastic_Ratio_Matching_of_RBMs_for_Sparse_High-Dimensional_Inputs.html">315 nips-2013-Stochastic Ratio Matching of RBMs for Sparse High-Dimensional Inputs</a></p>
<p>15 0.54633373 <a title="27-lsi-15" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>16 0.53932416 <a title="27-lsi-16" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>17 0.53813225 <a title="27-lsi-17" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>18 0.52602738 <a title="27-lsi-18" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>19 0.52201736 <a title="27-lsi-19" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>20 0.51564568 <a title="27-lsi-20" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.012), (16, 0.034), (33, 0.166), (34, 0.118), (41, 0.017), (49, 0.036), (56, 0.082), (70, 0.044), (85, 0.036), (89, 0.046), (90, 0.245), (93, 0.053), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82670951 <a title="27-lda-1" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>Author: Lee H. Dicker, Dean P. Foster</p><p>Abstract: We model a “one-shot learning” situation, where very few observations y1 , ..., yn ∈ R are available. Associated with each observation yi is a very highdimensional vector xi ∈ Rd , which provides context for yi and enables us to predict subsequent observations, given their own context. One of the salient features of our analysis is that the problems studied here are easier when the dimension of xi is large; in other words, prediction becomes easier when more context is provided. The proposed methodology is a variant of principal component regression (PCR). Our rigorous analysis sheds new light on PCR. For instance, we show that classical PCR estimators may be inconsistent in the speciﬁed setting, unless they are multiplied by a scalar c > 1; that is, unless the classical estimator is expanded. This expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods (c < 1), which are far more common in big data analyses. 1</p><p>2 0.81895792 <a title="27-lda-2" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>Author: Ali Borji, Laurent Itti</p><p>Abstract: Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to ﬁnd the function’s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploits all the x values tried and all the f (x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further conﬁrm that Gaussian Processes provide a general and uniﬁed theoretical account to explain passive and active function learning and search in humans. 1</p><p>same-paper 3 0.79758382 <a title="27-lda-3" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>Author: Forest Agostinelli, Michael R. Anderson, Honglak Lee</p><p>Abstract: Stacked sparse denoising autoencoders (SSDAs) have recently been shown to be successful at removing noise from corrupted images. However, like most denoising techniques, the SSDA is not robust to variation in noise types beyond what it has seen during training. To address this limitation, we present the adaptive multi-column stacked sparse denoising autoencoder (AMC-SSDA), a novel technique of combining multiple SSDAs by (1) computing optimal column weights via solving a nonlinear optimization program and (2) training a separate network to predict the optimal weights. We eliminate the need to determine the type of noise, let alone its statistics, at test time and even show that the system can be robust to noise not seen in the training set. We show that state-of-the-art denoising performance can be achieved with a single system on a variety of different noise types. Additionally, we demonstrate the efﬁcacy of AMC-SSDA as a preprocessing (denoising) algorithm by achieving strong classiﬁcation performance on corrupted MNIST digits. 1</p><p>4 0.7198965 <a title="27-lda-4" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>Author: Mohammad Gheshlaghi azar, Alessandro Lazaric, Emma Brunskill</p><p>Abstract: Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may signiﬁcantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi–armed bandit framework, where the objective is to minimize the total regret over a sequence of tasks by transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for estimating the possible tasks and derive regret bounds for it. 1</p><p>5 0.69051367 <a title="27-lda-5" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>6 0.69038486 <a title="27-lda-6" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>7 0.69035774 <a title="27-lda-7" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>8 0.68835145 <a title="27-lda-8" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>9 0.68821394 <a title="27-lda-9" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>10 0.6879704 <a title="27-lda-10" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>11 0.68792838 <a title="27-lda-11" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>12 0.68730652 <a title="27-lda-12" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>13 0.68506241 <a title="27-lda-13" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>14 0.68500918 <a title="27-lda-14" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>15 0.68426591 <a title="27-lda-15" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>16 0.68414623 <a title="27-lda-16" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>17 0.68352574 <a title="27-lda-17" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>18 0.68321991 <a title="27-lda-18" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>19 0.683155 <a title="27-lda-19" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>20 0.68282002 <a title="27-lda-20" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
