<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-192" href="#">nips2013-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</h1>
<br/><p>Source: <a title="nips-2013-192-pdf" href="http://papers.nips.cc/paper/4983-minimax-theory-for-high-dimensional-gaussian-mixtures-with-sparse-mean-separation.pdf">pdf</a></p><p>Author: Martin Azizyan, Aarti Singh, Larry Wasserman</p><p>Abstract: While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. 1</p><p>Reference: <a title="nips-2013-192-reference" href="../nips2013_reference/nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. [sent-7, score-0.322]
</p><p>2 In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. [sent-8, score-0.792]
</p><p>3 If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. [sent-9, score-0.585]
</p><p>4 Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. [sent-10, score-0.167]
</p><p>5 1  Introduction  Gaussian mixture models provide a simple framework for several machine learning problems including clustering, density estimation and classiﬁcation. [sent-11, score-0.157]
</p><p>6 Perhaps the most common use of Gaussian mixtures is for clustering. [sent-13, score-0.21]
</p><p>7 Inspired by the success of variable selection methods in regression, several authors have considered variable selection for clustering. [sent-15, score-0.24]
</p><p>8 However, there appears to be no theoretical results justifying the advantage of variable selection in high dimensional setting. [sent-16, score-0.16]
</p><p>9 To see why some sort of variable selection might be useful, consider clustering n subjects using a vector of d genes for each subject. [sent-17, score-0.328]
</p><p>10 Typically d is much larger than n which suggests that statistical clustering methods will perform poorly. [sent-18, score-0.172]
</p><p>11 However, it may be the case that there are only a small number of relevant genes in which case we might expect better behavior by focusing on this small set of relevant genes. [sent-19, score-0.268]
</p><p>12 The purpose of this paper is to provide precise bounds on clustering error with mixtures of Gaussians. [sent-20, score-0.502]
</p><p>13 Mathematically, we model an irrelevant feature by requiring the mean of that feature to be the same across clusters, so that the feature does not serve to differentiate the groups. [sent-22, score-0.246]
</p><p>14 Throughout this paper, we use the probability of misclustering an observation, relative to the optimal clustering if we had known the true distribution, as our loss function. [sent-23, score-0.284]
</p><p>15 This also debunks the myth that there is a gap between 1  statistical and computational complexity of learning mixture of two isotropic Gaussians for small mean separation. [sent-26, score-0.38]
</p><p>16 Our bounds require non-standard arguments since our loss function does not satisfy the triangle inequality. [sent-27, score-0.16]
</p><p>17 • We consider the high-dimensional setting where only a subset of relevant dimensions determine the mean separation between mixture components and show that learning is substantially easier as the sample complexity only depends on the sparse set of relevant dimensions. [sent-28, score-0.935]
</p><p>18 This provides some theoretical basis for feature selection approaches to clustering. [sent-29, score-0.167]
</p><p>19 • We show that a simple computationally feasible procedure nearly achieves the information theoretic sample complexity even in high-dimensional sparse mean separation settings. [sent-30, score-0.522]
</p><p>20 There is a long and continuing history of research on mixtures of Gaussians. [sent-32, score-0.21]
</p><p>21 Perhaps the most popular method for estimating a mixture distribution is maximum likelihood. [sent-34, score-0.189]
</p><p>22 These methods require the mean separation to increase with dimension. [sent-39, score-0.327]
</p><p>23 The ﬁrst √ one requires the separation to be d while the latter two improve it to d1/4 . [sent-40, score-0.269]
</p><p>24 To avoid this problem, Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures of spherical Gaussians which makes mean separation independent of dimension. [sent-41, score-0.713]
</p><p>25 Their method only requires the components to be separated by a hyperplane and runs in polynomial time, but requires n = Ω(d4 log d) samples. [sent-43, score-0.121]
</p><p>26 (2012) use the method of moments to get estimates without requiring separation between components of the mixture components. [sent-48, score-0.46]
</p><p>27 (2009) give a modiﬁed k-means algorithm for estimating a mixture of two Gaus˜ sians. [sent-51, score-0.189]
</p><p>28 For the large mean separation setting µ > 1, Chaudhuri et al. [sent-52, score-0.327]
</p><p>29 They also provide an information theoretic bound on the necessary sample complexity of any algorithm which matches the sample complexity of their method (up to log factors) in ˜ d and µ. [sent-54, score-0.368]
</p><p>30 If the mean separation is small µ < 1, they show that n = Ω(d/µ4 ) samples are sufﬁcient. [sent-55, score-0.327]
</p><p>31 Our results for the small mean separation setting give a matching necessary condition. [sent-56, score-0.327]
</p><p>32 Assuming the separation between the component means is not too sparse, Chaudhuri and Rao (2008) provide an algorithm for learning the mixture that has polynomial computational and sample complexity. [sent-57, score-0.469]
</p><p>33 Most of these papers are concerned with computational efﬁciency and do not give precise, statistical minimax upper and lower bounds. [sent-58, score-0.27]
</p><p>34 None of them deal with the case we are interested in, namely, a high dimensional mixture with sparse mean separation. [sent-59, score-0.308]
</p><p>35 We should also point out that the results in different papers are not necessarily comparable since different authors use different loss functions. [sent-60, score-0.119]
</p><p>36 Finally, we remind the reader that our motivation for studying sparsely separated mixtures is that this provides a model for variable selection in clustering problems. [sent-65, score-0.502]
</p><p>37 There are some relevant recent papers on this problem in the high-dimensional setting. [sent-66, score-0.181]
</p><p>38 Pan and Shen (2007) use penalized mixture models to do variable selection and clustering simultaneously. [sent-67, score-0.489]
</p><p>39 None of these papers provide minimax bounds for the clustering error or provide theoretical evidence of the beneﬁt of using variable selection in unsupervised problems such as clustering. [sent-73, score-0.566]
</p><p>40 2  2  Problem Setup  In this paper, we consider the simple setting of learning a mixture of two isotropic Gaussians with equal mixing weights,1 given n data points X1 , . [sent-74, score-0.248]
</p><p>41 from a d-dimensional mixture density function 1 1 pθ (x) = f (x; µ1 , σ 2 I) + f (x; µ2 , σ 2 I), 2 2 where f (·; µ, Σ) is the density of N (µ, Σ), σ > 0 is a ﬁxed constant, and θ := (µ1 , µ2 ) ∈ Θ. [sent-80, score-0.157]
</p><p>42 The ﬁrst class deﬁnes mixtures where the components have a mean separation of at least λ > 0. [sent-82, score-0.571]
</p><p>43 The second class deﬁnes mixtures with mean separation λ > 0 along a sparse set of s ∈ {1, . [sent-83, score-0.59]
</p><p>44 For a mixture with parameter θ, the Bayes optimal classiﬁcation, that is, assignment of a point x ∈ Rd to the correct mixture component, is given by the function Fθ (x) = argmax f (x; µi , σ 2 I). [sent-88, score-0.347]
</p><p>45 We will also demonstrate a speciﬁc estimator that achieves the minimax scaling. [sent-96, score-0.167]
</p><p>46 1  Small mean separation setting without sparsity  We begin without assuming any sparsity, that is, all features are relevant. [sent-100, score-0.367]
</p><p>47 In this case, comparing the projections of the data to the projection of the sample mean onto the ﬁrst principal component sufﬁces to achieve both minimax optimal sample complexity and clustering loss. [sent-101, score-0.538]
</p><p>48 If n ≥ max(68, 4d), then sup Eθ Lθ (F ) ≤ 600 max θ∈Θλ 1  4σ 2 ,1 λ2  d log(nd) . [sent-105, score-0.143]
</p><p>49 n  We believe our results should also hold in the unequal mixture weight setting without major modiﬁcations. [sent-106, score-0.157]
</p><p>50 3  Furthermore, if  λ σ  √ ≥ 2 max(80, 14 5d), then sup Eθ Lθ (F ) ≤ 17 exp − θ∈Θλ  n λ2 + 9 exp − 32 80σ 2  . [sent-107, score-0.142]
</p><p>51 inf sup Eθ Lθ (Fn ) ≥ min , 500 3 λ2 n 4 Fn θ∈Θλ We believe that some of the constants (including lower bound on d and exact upper bound on λ/σ) can be tightened, but the results demonstrate matching scaling behavior of clustering error with d, n and λ/σ. [sent-113, score-0.498]
</p><p>52 However, inference is usually tractable since not all features are relevant to the learning task at hand. [sent-121, score-0.156]
</p><p>53 This sparsity of relevant feature set has been successfully exploited in supervised learning problems such as regression and classiﬁcation. [sent-122, score-0.167]
</p><p>54 We show next that the same is true for clustering under the Gaussian mixture model. [sent-123, score-0.329]
</p><p>55 2  Sparse and small mean separation setting  Now we consider the case where there are s < d relevant features. [sent-125, score-0.443]
</p><p>56 If n ≥ max(68, 4s), d ≥ 2 and α ≤ 4 , then √ 16σ 2 s log(ns) σ s log(nd) sup Eθ Lθ (F ) ≤ 603 max ,1 + 220 λ2 n λ n θ∈Θλ,s  Next we ﬁnd the lower bound. [sent-139, score-0.174]
</p><p>57 Assume that inf sup Eθ Lθ (Fn ) ≥ Fn θ∈Θλ,s  λ σ  ≤ 0. [sent-141, score-0.14]
</p><p>58 We remark again that the constants in our bounds can be tightened, but the results suggest that σ λ  s2 log d n  1/4  σ2 λ2  Rn  s log d , n  s2 log d for a constant target value of Rn . [sent-145, score-0.333]
</p><p>59 λ4 /σ 4 In this case, we have a gap between the upper and lower bounds for the clustering loss. [sent-146, score-0.349]
</p><p>60 If the number of relevant dimensions is small then we can expect good rates. [sent-149, score-0.168]
</p><p>61 We conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal component method as in Vu and Lei (2012) to ﬁnd the relevant features. [sent-151, score-0.376]
</p><p>62 or  n=Ω  We note that the upper bound is achieved by a two-stage method that ﬁrst ﬁnds the relevant dimensions and then estimates the clusters. [sent-153, score-0.264]
</p><p>63 This is in contrast to the methods described in the introduction which do clustering and variable selection simultaneously. [sent-154, score-0.292]
</p><p>64 This raises an interesting question: is it always possible to achieve the minimax rate with a two-stage procedure or are there cases where a simultaneous method outperforms a two-stage procedure? [sent-155, score-0.137]
</p><p>65 4  Proofs of the Lower Bounds  The lower bounds for estimation problems rely on a standard reduction from expected error to hypothesis testing that assumes the loss function is a semi-distance, which the clustering loss isn’t. [sent-158, score-0.383]
</p><p>66 The proof techniques of the sparse and non-sparse lower bounds are almost identical. [sent-161, score-0.197]
</p><p>67 The main difference is that in the non-sparse case, we use the Varshamov–Gilbert bound (Lemma 1) to construct a set of sufﬁciently dissimilar hypotheses, whereas in the sparse case we use an analogous result for sparse hypercubes (Lemma 2). [sent-162, score-0.165]
</p><p>68 If for all 1 ≤ i ≤ M , KL(Pθi , Pθ0 ) ≤ α log M , and if Lθi (F ) < γ implies Lθj (F ) ≥ γ for all 0 ≤ i = j ≤ M and n clusterings F , then inf Fn maxi∈[0. [sent-186, score-0.147]
</p><p>69 For any θ, θ ∈ Θλ , and any clustering F , let τ = Lθ (F ) + Lθ (Fθ ) + τ ≤ 1/2, then Lθ (Fθ ) − τ ≤ Lθ (F ) ≤ Lθ (Fθ ) + τ. [sent-191, score-0.172]
</p><p>70 Let θ = (µ0 −µ/2, µ0 +µ/2) and θ = (µ0 −µ /2, µ0 + T µ /2) for µ0 , µ, µ ∈ Rd such that µ = µ , and let cos β = |µµ µ2 | . [sent-194, score-0.325]
</p><p>71 i=1  ei  2  By Proposition 4, KL(Pθω , Pθν ) ≤ ξ 4 (1 − cos βω,ν ) where cos βω,ν = 1 − 2ρ(ω,ν) , ω, ν ∈ Ω, and λ2 2  1 ρ is the Hamming distance, so KL(Pθω , Pθν ) ≤ ξ 4 2(d−1) . [sent-210, score-0.703]
</p><p>72 By Proposition 3, since cos βω,ν ≥ 2 , λ2 √ 1 1 1 + cos βω,ν 4 d−1 Lθω (Fθν ) ≤ tan βω,ν ≤ , and 1 − cos βω,ν ≤ π π cos βω,ν π λ  √  ρ(ω, ν) λ where g(x) = φ(x)(φ(x) − xΦ(−x)). [sent-211, score-1.497]
</p><p>73 λ2 π λ 2 λ Lθω (Fθν ) ≥ 2g(ξ) sin βω,ν cos βω,ν ≥ g(ξ) 1 + cos βω,ν  Deﬁne γ = 1 (g(ξ) − 2ξ 2 ) 4  √  1 − cos βω,ν ≥  2g(ξ)  d−1 λ  Lθi (Fθj ) + Lθi (F ) +  . [sent-222, score-1.215]
</p><p>74 Also, KL(Pθi , Pθ0 ) ≤ (d − 1)ξ 4 2 2 ≤ log M for all 1 ≤ i ≤ M , λ 9n 2 because, by deﬁnition of , ξ 4 2 2 ≤ log 2 . [sent-228, score-0.174]
</p><p>75 1, λ 72n √ 1 log 2 σ 2 d − 1 1 min , inf max Eθi Lθi (Fn ) ≥ 0. [sent-230, score-0.21]
</p><p>76 4  5  Proofs of the Upper Bounds  Propositions 5 and 6 below bound the error in estimating the mean and principal direction, and can be obtained using standard concentration bounds and a variant of the Davis–Kahan theorem. [sent-251, score-0.269]
</p><p>77 Proposition 7 relates these errors to the clustering loss. [sent-252, score-0.172]
</p><p>78 For any δ > 0, we have µ0 − µn ≥ σ  2 max(d,8 log n  1 δ)  + µ 6  2 log n  1 δ  with probability at least 1 − 3δ. [sent-263, score-0.174]
</p><p>79 For any 0 < δ < d−1 , if e σ2 σ µ 2, µ  max  1 δ)  max(d,8 log n  1 160 ,  ≤  n then with probability at least 1 − 12δ − 2 exp − 20 ,  √  σ σ2 , 2 µ µ  sin β ≤ 14 max  10 d 10 d log max 1, log . [sent-273, score-0.721]
</p><p>80 If |(x0 − µ0 )T v| ≤ σ 1 + µ 2 for 0 1 some 1 ≥ 0 and 0 ≤ 2 ≤ 1 , and if sin β ≤ √5 , then 4 1 µ Lθ (F ) ≤ exp − max 0, −2 2 2σ (x0 −µ0 )T v cos β  Proof. [sent-277, score-0.659]
</p><p>81 Let r = Lθ (F ) ≤  1 2  ∞ −∞ ∞  ≤  1 x φ σ σ  and  2  µ σ 1 4,  ≤  1  µ 2 σ  φ max 0,  µ 2σ  φ(x) Φ  2  µ −r σ  −Φ  +  ∞ −∞ ∞  1  +  2  µ µ + 2 sin β 2 sin β +1 σ σ  µ − |x| tan β − r σ  µ + |x| tan β + r −Φ σ µ −r −Φ − |x| tan β σ  Φ  φ(x) Φ 1 2  2  . [sent-278, score-1.134]
</p><p>82 Since the clustering loss is invariant to rotation and translation,  −∞  Since tan β ≤  2 1  we have r ≤ 2σ  −2  1  +2 µ  . [sent-280, score-0.423]
</p><p>83 µ σ  and Φ  2,  µ −r σ  dx  −Φ  µ −r σ  ≤  , ∞  A  dx ≤ 2  φ(x)φ(y)dydx 0  A−x tan β  A cos β+(u+A sin β) tan β  φ(u)φ(v)dudv ≤ 2φ (A) tan β (A sin β + 1)  =2 −A sin β  A cos β  µ µ −2 1 + 2 1 sin β + 1 tan β 2 2σ σ where we used u = x cos β − y sin β and v = x sin β + y cos β in the second step. [sent-282, score-3.528]
</p><p>84 For any 0<δ<  1 √ e  such that  6 log n  1 δ  1 ≤ 2 , with probability at least 1 − 6dδ, for all i ∈ [d],  |Σn (i, i) − (σ 2 + µ(i)2 )| ≤ σ 2  6 log n  1 δ  + 2σ|µ(i)|  2 log n  1 δ  + (σ + |µ(i)|)2  2 log 1 δ . [sent-296, score-0.348]
</p><p>85 Let cos β = |v1 (ΣSn )T v1 (Σ)|, cos β = |v1 (ΣSn )T v1 (Σ)|, and cos β = |v1 (ΣSn )T v1 (ΣSn )| where Σ = σ 2 I + µµT , and for simplicity we deﬁne ΣSn and ΣSn to be the same as Σn and Σ in Sn , respectively, and 0 elsewhere. [sent-324, score-0.975]
</p><p>86 Then sin β ≤ sin β + sin β, and √ √ µ − µS(θ) µ − µS(θ) 4σ α |S(θ)| − |S(θ)| σ sα ≤ ≤ ≤8 . [sent-325, score-0.72]
</p><p>87 sin β = µ µ µ λ Using the same argument as the proof of Theorem 1, as long as the above bound is smaller than √ σ sα 3 σ2 s log(ns) ,1 + 104 + . [sent-326, score-0.34]
</p><p>88 Eθ Lθ (F ) ≤ 600 max √ 2 λ n λ n 2 − 4σ sα Using the fact Lθ (F ) ≤  6  1 2  always, and that α ≤  1 4  implies  log(nd) n  1 √ , 2 5  ≤ 1, the bound follows. [sent-327, score-0.122]
</p><p>89 Conclusion  We have provided minimax lower and upper bounds for estimating high dimensional mixtures. [sent-328, score-0.349]
</p><p>90 The bounds show explicitly how the statistical difﬁculty of the problem depends on dimension d, sample size n, separation λ and sparsity level s. [sent-329, score-0.384]
</p><p>91 For clarity, we focused on the special case where there are two spherical components with equal mixture weights. [sent-330, score-0.266]
</p><p>92 In future work, we plan to extend the results to general mixtures of k Gaussians. [sent-331, score-0.21]
</p><p>93 One of our motivations for this work is the recent interest in variable selection methods to facilitate clustering in high dimensional problems. [sent-332, score-0.332]
</p><p>94 (2010) provide promising numerical evidence that variable selection does improve high dimensional clustering. [sent-335, score-0.16]
</p><p>95 Indeed, as of now, there is no rigorous proof that the methods in those papers outperform a two stage approach where the ﬁrst stage screens for relevant features and the second stage applies standard clustering methods on the features found in the ﬁrst stage. [sent-338, score-0.609]
</p><p>96 We conjecture that there are conditions under which simultaneous feature selection and clustering outperforms a two stage method. [sent-339, score-0.384]
</p><p>97 Settling this question will require the aforementioned extension of our results to the general mixture case. [sent-340, score-0.157]
</p><p>98 Learning mixtures of product distributions using correlations and independence. [sent-359, score-0.21]
</p><p>99 Learning mixtures of spherical gaussians: moment methods and spectral decompositions. [sent-374, score-0.354]
</p><p>100 Regularized k-means clustering of high-dimensional data and its asymptotic consistency. [sent-399, score-0.172]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sn', 0.36), ('cos', 0.325), ('fn', 0.281), ('separation', 0.269), ('sin', 0.24), ('mixtures', 0.21), ('tan', 0.197), ('proposition', 0.187), ('clustering', 0.172), ('mixture', 0.157), ('minimax', 0.137), ('relevant', 0.116), ('chaudhuri', 0.096), ('isotropic', 0.091), ('kl', 0.087), ('log', 0.087), ('gaussians', 0.086), ('selection', 0.084), ('rd', 0.081), ('sup', 0.08), ('witten', 0.076), ('spherical', 0.075), ('raftery', 0.072), ('bounds', 0.072), ('santosh', 0.069), ('spectral', 0.069), ('kannan', 0.066), ('varshamov', 0.066), ('papers', 0.065), ('sanjoy', 0.064), ('vempala', 0.064), ('max', 0.063), ('theoretic', 0.062), ('inf', 0.06), ('bound', 0.059), ('pan', 0.059), ('brubaker', 0.058), ('misclustering', 0.058), ('mean', 0.058), ('propositions', 0.057), ('guo', 0.055), ('loss', 0.054), ('schulman', 0.054), ('achlioptas', 0.054), ('gilbert', 0.054), ('ei', 0.053), ('sparse', 0.053), ('dimensions', 0.052), ('feature', 0.051), ('tightened', 0.051), ('aarti', 0.051), ('precise', 0.048), ('principal', 0.048), ('dasgupta', 0.047), ('larry', 0.046), ('stage', 0.045), ('xn', 0.045), ('kamalika', 0.044), ('carnegie', 0.044), ('mellon', 0.044), ('sample', 0.043), ('vu', 0.041), ('proof', 0.041), ('lemma', 0.041), ('penalized', 0.04), ('dimensional', 0.04), ('sun', 0.04), ('features', 0.04), ('kalai', 0.039), ('shen', 0.038), ('belkin', 0.038), ('gap', 0.037), ('complexity', 0.037), ('upper', 0.037), ('wei', 0.037), ('ns', 0.037), ('variable', 0.036), ('genes', 0.036), ('excess', 0.036), ('irrelevant', 0.035), ('arora', 0.035), ('rn', 0.035), ('triangle', 0.034), ('components', 0.034), ('misclassi', 0.034), ('hamming', 0.034), ('assignment', 0.033), ('theorem', 0.032), ('xt', 0.032), ('estimating', 0.032), ('conjecture', 0.032), ('basis', 0.032), ('tibshirani', 0.031), ('hsu', 0.031), ('dean', 0.031), ('lower', 0.031), ('exp', 0.031), ('estimator', 0.03), ('maxi', 0.03), ('classi', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="192-tfidf-1" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>Author: Martin Azizyan, Aarti Singh, Larry Wasserman</p><p>Abstract: While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. 1</p><p>2 0.23425335 <a title="192-tfidf-2" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>Author: Alessandro Rudi, Guillermo D. Canas, Lorenzo Rosasco</p><p>Abstract: A large number of algorithms in machine learning, from principal component analysis (PCA), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples. In this paper we introduce a general formulation of this problem and derive novel learning error estimates. Our results rely on natural assumptions on the spectral properties of the covariance operator associated to the data distribution, and hold for a wide class of metrics between subspaces. As special cases, we discuss sharp error estimates for the reconstruction properties of PCA and spectral support estimation. Key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods. 1</p><p>3 0.19939046 <a title="192-tfidf-3" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>Author: Samory Kpotufe, Francesco Orabona</p><p>Abstract: We consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time. We prove that it is possible to maintain such a structure in time O (log n) at ˜ any time step n while achieving a nearly-optimal regression rate of O n−2/(2+d) in terms of the unknown metric dimension d. Finally we prove a new regression lower-bound which is independent of a given data size, and hence is more appropriate for the streaming setting. 1</p><p>4 0.16994061 <a title="192-tfidf-4" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>Author: Jason Lee, Ran Gilad-Bachrach, Rich Caruana</p><p>Abstract: In the mixture models problem it is assumed that there are K distributions θ1 , . . . , θK and one gets to observe a sample from a mixture of these distributions with unknown coeﬃcients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with diﬀerent mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the diﬀerences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data. 1</p><p>5 0.12511095 <a title="192-tfidf-5" href="./nips-2013-Sign_Cauchy_Projections_and_Chi-Square_Kernel.html">293 nips-2013-Sign Cauchy Projections and Chi-Square Kernel</a></p>
<p>Author: Ping Li, Gennady Samorodnitsk, John Hopcroft</p><p>Abstract: The method of stable random projections is useful for efﬁciently approximating the lα distance (0 < α ≤ 2) in high dimension and it is naturally suitable for data streams. In this paper, we propose to use only the signs of the projected data and we analyze the probability of collision (i.e., when the two signs differ). Interestingly, when α = 1 (i.e., Cauchy random projections), we show that the probability of collision can be accurately approximated as functions of the chi-square (χ2 ) similarity. In text and vision applications, the χ2 similarity is a popular measure when the features are generated from histograms (which are a typical example of data streams). Experiments conﬁrm that the proposed method is promising for large-scale learning applications. The full paper is available at arXiv:1308.1009. There are many future research problems. For example, when α → 0, the collision probability is a function of the resemblance (of the binary-quantized data). This provides an effective mechanism for resemblance estimation in data streams. 1</p><p>6 0.12370621 <a title="192-tfidf-6" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>7 0.11276466 <a title="192-tfidf-7" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>8 0.11257221 <a title="192-tfidf-8" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>9 0.10976581 <a title="192-tfidf-9" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>10 0.1096499 <a title="192-tfidf-10" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>11 0.10086819 <a title="192-tfidf-11" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>12 0.098982722 <a title="192-tfidf-12" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>13 0.094870619 <a title="192-tfidf-13" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>14 0.090989433 <a title="192-tfidf-14" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>15 0.09010002 <a title="192-tfidf-15" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>16 0.089154281 <a title="192-tfidf-16" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>17 0.088121571 <a title="192-tfidf-17" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>18 0.086236559 <a title="192-tfidf-18" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>19 0.085806221 <a title="192-tfidf-19" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>20 0.082166761 <a title="192-tfidf-20" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, 0.058), (2, 0.111), (3, 0.019), (4, 0.016), (5, 0.123), (6, 0.022), (7, 0.02), (8, -0.117), (9, 0.035), (10, 0.019), (11, 0.069), (12, -0.002), (13, 0.12), (14, 0.079), (15, 0.037), (16, 0.076), (17, -0.009), (18, 0.04), (19, 0.06), (20, 0.038), (21, 0.12), (22, 0.035), (23, 0.061), (24, -0.146), (25, -0.056), (26, -0.152), (27, 0.059), (28, -0.127), (29, -0.072), (30, -0.123), (31, -0.083), (32, 0.078), (33, 0.041), (34, 0.042), (35, 0.098), (36, 0.062), (37, 0.088), (38, 0.096), (39, 0.118), (40, 0.041), (41, 0.021), (42, 0.033), (43, 0.036), (44, 0.137), (45, 0.206), (46, -0.037), (47, 0.041), (48, 0.001), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94742161 <a title="192-lsi-1" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>Author: Martin Azizyan, Aarti Singh, Larry Wasserman</p><p>Abstract: While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. 1</p><p>2 0.71112233 <a title="192-lsi-2" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>Author: Jason Lee, Ran Gilad-Bachrach, Rich Caruana</p><p>Abstract: In the mixture models problem it is assumed that there are K distributions θ1 , . . . , θK and one gets to observe a sample from a mixture of these distributions with unknown coeﬃcients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with diﬀerent mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the diﬀerences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data. 1</p><p>3 0.65732419 <a title="192-lsi-3" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>Author: Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman</p><p>Abstract: unkown-abstract</p><p>4 0.65377343 <a title="192-lsi-4" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>Author: Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund</p><p>Abstract: We consider a situation in which we see samples Xn ∈ Rd drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give ﬁnite-sample convergence rates for both. 1</p><p>5 0.65118241 <a title="192-lsi-5" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><p>6 0.64346194 <a title="192-lsi-6" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>7 0.59461325 <a title="192-lsi-7" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>8 0.57955301 <a title="192-lsi-8" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>9 0.54591435 <a title="192-lsi-9" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>10 0.51279902 <a title="192-lsi-10" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>11 0.50401652 <a title="192-lsi-11" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>12 0.48821485 <a title="192-lsi-12" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>13 0.47520554 <a title="192-lsi-13" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>14 0.46637872 <a title="192-lsi-14" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>15 0.46186817 <a title="192-lsi-15" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>16 0.46185255 <a title="192-lsi-16" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>17 0.45901203 <a title="192-lsi-17" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>18 0.45574602 <a title="192-lsi-18" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>19 0.44117293 <a title="192-lsi-19" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>20 0.4360432 <a title="192-lsi-20" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.027), (16, 0.024), (33, 0.132), (34, 0.17), (41, 0.017), (49, 0.027), (56, 0.16), (70, 0.028), (79, 0.205), (85, 0.039), (89, 0.055), (93, 0.026), (95, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86108869 <a title="192-lda-1" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>Author: Martin Azizyan, Aarti Singh, Larry Wasserman</p><p>Abstract: While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. 1</p><p>2 0.84716648 <a title="192-lda-2" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><p>3 0.79648167 <a title="192-lda-3" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>4 0.79155272 <a title="192-lda-4" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><p>5 0.78460997 <a title="192-lda-5" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>Author: Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman</p><p>Abstract: unkown-abstract</p><p>6 0.78448451 <a title="192-lda-6" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>7 0.78409386 <a title="192-lda-7" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>8 0.78388596 <a title="192-lda-8" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>9 0.78305948 <a title="192-lda-9" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>10 0.78280163 <a title="192-lda-10" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>11 0.78248709 <a title="192-lda-11" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>12 0.78113818 <a title="192-lda-12" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>13 0.78075409 <a title="192-lda-13" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>14 0.7802211 <a title="192-lda-14" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>15 0.7802161 <a title="192-lda-15" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>16 0.77838421 <a title="192-lda-16" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>17 0.77777874 <a title="192-lda-17" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>18 0.77734345 <a title="192-lda-18" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>19 0.77709413 <a title="192-lda-19" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>20 0.77650887 <a title="192-lda-20" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
