<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>357 nips-2013-k-Prototype Learning for 3D Rigid Structures</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-357" href="#">nips2013-357</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>357 nips-2013-k-Prototype Learning for 3D Rigid Structures</h1>
<br/><p>Source: <a title="nips-2013-357-pdf" href="http://papers.nips.cc/paper/5095-k-prototype-learning-for-3d-rigid-structures.pdf">pdf</a></p><p>Author: Hu Ding, Ronald Berezney, Jinhui Xu</p><p>Abstract: In this paper, we study the following new variant of prototype learning, called k-prototype learning problem for 3D rigid structures: Given a set of 3D rigid structures, ﬁnd a set of k rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized. Prototype learning is a core problem in machine learning and has a wide range of applications in many areas. Existing results on this problem have mainly focused on the graph domain. In this paper, we present the ﬁrst algorithm for learning multiple prototypes from 3D rigid structures. Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efﬁcient with quality guarantee. We validate our approach using two type of data sets, random data and biological data of chromosome territories. Experiments suggest that our approach can effectively learn prototypes in both types of data. 1</p><p>Reference: <a title="nips-2013-357-reference" href="../nips2013_reference/nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we present the ﬁrst algorithm for learning multiple prototypes from 3D rigid structures. [sent-7, score-0.651]
</p><p>2 Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efﬁcient with quality guarantee. [sent-8, score-1.004]
</p><p>3 1  Introduction  Learning prototype from a set of given or observed objects is a core problem in machine learning, and has numerous applications in computer vision, pattern recognition, data mining, bioinformatics, etc. [sent-11, score-0.316]
</p><p>4 Other important applications of prototype include reconstructing object from partially observed snapshots and identifying common (or hidden) pattern from a set of data items. [sent-14, score-0.337]
</p><p>5 In this paper, we study a new prototype learning problem called k-prototype learning for 3D rigid structures, where a 3D rigid structure is a set of points in R3 whose pairwise distances remain invariant under rigid transformation. [sent-15, score-2.05]
</p><p>6 Since our problem needs to determine k prototypes, it thus can be viewed as two tightly coupled problems, clustering rigid structures and prototype reconstruction for each cluster. [sent-16, score-1.125]
</p><p>7 Our problem is motivated by an important application in biology for determining the spatial organization pattern of chromosome territories from a population of cells. [sent-17, score-0.374]
</p><p>8 has suggested that conﬁguration of chromosome territories could signiﬁcantly inﬂuence the cell molecular processes, and are closely related to cancer-promoting chromosome translocations. [sent-19, score-0.55]
</p><p>9 Thus, ﬁnding the spatial organization pattern of chromosome territories is a key step to understanding the cell molecular processes [6,7,10,25]. [sent-20, score-0.418]
</p><p>10 Since the set of observed chromosome territories in each cell can be represented as a 3D rigid structure, the problem can thus be formulated as a k-prototype learning problem for a set of 3D rigid structures. [sent-21, score-1.5]
</p><p>11 Most of the research has focused on ﬁnding prototype in the graph domain. [sent-23, score-0.334]
</p><p>12 [18] introduced the median graph concept, which can be viewed as the prototype of a set of input graphs, and presented a genetic approach to solve it. [sent-25, score-0.414]
</p><p>13 Our prototype learning problem is clearly related to the challenging 3D rigid structure clustering and alignment problem [1,2,4,5,13,17]. [sent-31, score-1.195]
</p><p>14 For 1-prototype learning, we ﬁrst present a practical algorithm for the alignment problem. [sent-36, score-0.246]
</p><p>15 Our result is based on a multi-level net technique which ﬁnds the proper Euler angles for the rigid transformation. [sent-37, score-0.625]
</p><p>16 With this alignment algorithm, we can then reduce the prototype learning problem to a new problem called chromatic clustering (see Figure 1(b) and 1(c )), and present two approximate solutions for it. [sent-38, score-0.986]
</p><p>17 For k-prototype learning, a key challenge is how to avoid the high complexity associated with clustering 3D rigid structures. [sent-40, score-0.66]
</p><p>18 Our idea is to map each rigid structure to a point in some metric space and build a correlation graph to capture their pairwise similarity. [sent-41, score-0.747]
</p><p>19 We show that the correlation graph is metric; this means that we can reduce the rigid structure clustering problem to a metric k-median clustering problem on the correlation graph. [sent-42, score-0.931]
</p><p>20 We also provide techniques to deal with several practical issues, such as the unequal sizes of rigid structures and the weaker metric property caused by imperfect alignment computation for the correlation graph. [sent-44, score-1.051]
</p><p>21 Experiments suggest that our approach can effectively reduce the cost in prototype learning. [sent-47, score-0.34]
</p><p>22 P is an m-rigid structure if the distance between any pair of vertices pi and pj in P remains the same under any rigid transformation, including translation, rotation, reﬂection and their combinations, on P . [sent-51, score-0.815]
</p><p>23 For any rigid transformation T , the image of P under T is denoted as T (P ). [sent-52, score-0.62]
</p><p>24 The bipartite matching of S1 and S2 , is the one-to-one match from S1 to S2 with the minimum total matching weight (denoted as Cost(S1 , S2 )) in G (see Figure 1(a)). [sent-55, score-0.261]
</p><p>25 1: (a) An example of bipartite matching (red edges); (b) 4 point-sets with each in a different color; (c ) chromatic clustering of point-sets in (b). [sent-59, score-0.596]
</p><p>26 Let P and Q be two m-rigid structures in 3D space with points {p1 , · · · , pm } and {q1 , · · · , qm } respectively. [sent-62, score-0.286]
</p><p>27 Their alignment is to ﬁnd a rigid transformation T for P so as to minimize the cost of the bipartite matching between T (P ) and Q. [sent-63, score-1.062]
</p><p>28 i=1  1≤j≤k  (1)  From Deﬁnition 4, we know that the k-prototype learning problem can be viewed as ﬁrst clustering the rigid structures into k clusters and then build a prototype for each cluster so as to minimize the total alignment cost. [sent-68, score-1.415]
</p><p>29 In each iteration, it constructs a new prototype using the one from previous iteration, and reduces the objective value. [sent-72, score-0.329]
</p><p>30 A ﬁnal prototype is obtained once the objective value becomes stable. [sent-73, score-0.329]
</p><p>31 Randomly select a rigid structure from the input {P1 , · · · , Pn } as the initial prototype Q. [sent-75, score-0.858]
</p><p>32 (a) For each Pi , ﬁnd the rigid transformation (approximately) realizing A(Pi , Q). [sent-78, score-0.639]
</p><p>33 , after the corresponding rigid transformation) of each Pi , construct an updated prototype Q which minimizes the objective value. [sent-81, score-0.898]
</p><p>34 This is also the main difﬁculty for this prototype learning problem. [sent-85, score-0.289]
</p><p>35 1  Alignment  To determine the alignment of two rigid structures, one way is to use our recent theoretical algorithm for point-set matching [13]. [sent-87, score-0.901]
</p><p>36 Applying this algorithm to our 3D rigid structures, the running time becomes O( 1 m8 log6 (m)). [sent-89, score-0.569]
</p><p>37 The algorithm then builds an -net around each base point to determine an approximate rigid transformation. [sent-93, score-0.628]
</p><p>38 Let P = {p1 , · · · , pm } and Q = {q1 , · · · , qm } be two m-rigid structures in 3D space, and T be the rigid transformation realizing the minimum bipartite matching cost (where the edge weight is replaced by the squared Euclidean distance of the corresponding points in Deﬁnition 2). [sent-99, score-1.171]
</p><p>39 Lemma 1 tells us that to align two rigid structures, we can ﬁrst translate them to share one common mean point, and then consider only the rotation in 3D space. [sent-101, score-0.611]
</p><p>40 (Note that we can ignore reﬂection in the rigid transformation, as it can be captured by computing the alignment twice, one for the original rigid structure, and the other for its mirror image. [sent-102, score-1.384]
</p><p>41 1  From the above Fact 1, we know that the main issue for aligning two rigid structures P and Q is to ﬁnd three proper angles φ, θ, ψ to minimize the cost. [sent-105, score-0.748]
</p><p>42 (b) Randomly select a representative point si ∈ Si , and compute the alignment cost under the rotational matrix corresponding to si via Hungarian algorithm. [sent-119, score-0.315]
</p><p>43 There are several existing alignment algorithms for 3D rigid structures, and each suffers from its own limitations. [sent-126, score-0.815]
</p><p>44 However, it does not generate the one-to-one match between the rigid structures. [sent-128, score-0.599]
</p><p>45 Instead, every point in one rigid structure is matched to its nearest neighbor in the other rigid structure. [sent-129, score-1.177]
</p><p>46 This means that some point could match multiple points in the other rigid structure. [sent-130, score-0.671]
</p><p>47 Obviously, this type of matching cannot meet our requirement, especially in the biological application where chromosome territory is expected to match only one  chromosome. [sent-131, score-0.371]
</p><p>48 Similar problem also occurs in some other alignment algorithms [1,5,17]. [sent-132, score-0.246]
</p><p>49 [2] presented an algebraic approach to ﬁnd the best alignment between two 3D point-sets. [sent-134, score-0.246]
</p><p>50 But for our alignment problem, it is challenging to obtain such accurate estimations. [sent-137, score-0.246]
</p><p>51 2  Prototype reconstruction  In this section, we discuss how to build a prototype from a set of 3D rigid structures. [sent-139, score-0.91]
</p><p>52 We ﬁrst ﬁx the position of each Pi , and then construct a new prototype Q to minimize the objective function in Deﬁnition 4. [sent-140, score-0.329]
</p><p>53 Our main idea is to introduce a new type of clustering problem called Chromatic Clustering which was ﬁrstly introduced by Ding and Xu [12], and reduce our prototype reconstruction problem to it. [sent-141, score-0.404]
</p><p>54 A chromatic partition of G is a partition of the n × m points into m sets, U1 , · · · , Um , such that each Uj contains exactly one point from each Gi . [sent-145, score-0.504]
</p><p>55 The chromatic clustering of G is to ﬁnd m median points m {q1 , · · · , qm } in the space and a chromatic partition U1 , · · · , Um of G such that j=1 p∈Uj ||p − qj || is minimized, where || · || denotes the Euclidean distance. [sent-148, score-1.354]
</p><p>56 From Deﬁnition 6, we know that chromatic clustering is quite similar to k-median clustering in Euclidean space; the only difference is that it has the chromatic requirement, i. [sent-149, score-0.92]
</p><p>57 , the obtained k clusters should be a chromatic partition (see Figure 1(b) and 1(c )). [sent-151, score-0.423]
</p><p>58 Since the position of each Pi is ﬁxed (note that with a slight abuse of notation, we still use Pi to denote its image T (Pi ) under the rigid transformation T obtained in Section 3. [sent-153, score-0.62]
</p><p>59 1), we can view each Pi as a point-set Gi , and the new prototype Q as the k median points {q1 , · · · , qm } in Deﬁnition 6. [sent-154, score-0.505]
</p><p>60 Further, if a point p ∈ Pi is matched to qj , then it is part of Uj . [sent-155, score-0.33]
</p><p>61 Since we compute the one-to-one match, Uj contains exactly one point from each Pi , which implies that {U1 , · · · , Um } is a chromatic partition on G. [sent-156, score-0.414]
</p><p>62 Then the objective j function in Deﬁnition 4 becomes n  m  m  n  ||pi − qj || = j i=1 j=1  m  ||pi − qj || = j j=1 i=1  ||p − qj ||,  (2)  j=1 p∈Uj  which is exactly the objective function in Deﬁnition 6. [sent-158, score-0.953]
</p><p>63 Step 2(b) in the algorithm of 1-prototype learning is equivalent to solving a chromatic clustering problem. [sent-161, score-0.451]
</p><p>64 Next, we give two constant approximation algorithms for the chromatic clustering problem; one is randomized, and the other is deterministic. [sent-162, score-0.471]
</p><p>65 Let G = {G1 , · · · , Gn } be an instance of chromatic clustering with each Gi consisting of m points in the space. [sent-164, score-0.505]
</p><p>66 If Gl is randomly selected from G as the m median points, then with probability at least 1/2, Gl yields a 3-approximation for chromatic clustering on G. [sent-166, score-0.531]
</p><p>67 If enumerating all point-sets in G as the m median points, there exists one Gi0 , which yields a 2-approximation for chromatic clustering on G. [sent-168, score-0.531]
</p><p>68 Let {q1 , · · · , qm } be the m median points in the optimal solution, and U1 , · · · , Um be the corresponding chromatic partition. [sent-171, score-0.576]
</p><p>69 , m  ||pl − qj || ≤ 2 j j=1  1 n  n  m  ||pi − qj ||. [sent-175, score-0.582]
</p><p>70 Compute the bipartite matching between Q ˜ ˜ ˜ yields a chromatic partition {U1 , · · · , Um } on G, where each Uj consists of all the points matched to qj . [sent-191, score-0.907]
</p><p>71 By Deﬁnition 6, we know that qj should be the geometric median point of Uj in order to make ˜ the objective value as low as possible. [sent-192, score-0.471]
</p><p>72 Thus, we can use the well known Weiszfelds algorithm [23] to ˜ compute the geometric median point for each Uj , and update qj to be the corresponding geometric ˜ median point. [sent-193, score-0.517]
</p><p>73 We can iteratively perform the following two steps, (1) computing the chromatic partition and (2) generating the geometric median points, until the objective value becomes stable. [sent-194, score-0.54]
</p><p>74 The alignment cost in Deﬁnition 3 satisﬁes the triangle inequality. [sent-199, score-0.342]
</p><p>75 We denote the correlation graph on the given m-rigid structures {P1 , · · · , Pn } as Γ , which contains n vertices {v1 , · · · , vn }. [sent-201, score-0.243]
</p><p>76 Each vi represents the rigid structure Pi , and the edge connecting vi and vj has the weight equal to A(Pi , Pj ). [sent-202, score-0.619]
</p><p>77 Let {Q1 , · · · , Qk } be the k rigid structures yielded in an optimal solution of the k-prototype learning, and {C1 , · · · , Ck } be the corresponding k optimal clusters. [sent-208, score-0.697]
</p><p>78 There exists one rigid structure Pij ∈ Cj such that A(Pij , Qj ) ≤  1 |Cj |  A(Pi , Qj ). [sent-210, score-0.569]
</p><p>79 Then, (7) directly implies that k  k  A(Pi , Pij ) ≤ 2 j=1 Pi ∈Cj  A(Pi , Qj ), j=1 Pi ∈Cj  (8)  (8) is similar to the deterministic solution in Theorem 2; the only difference is that the point-sets here need to be aligned through rigid transformation, while in Theorem 2, the point-sets are ﬁxed. [sent-212, score-0.569]
</p><p>80 Build the correlation graph Γ , and run the algorithm proposed in [9] to obtain a 1 2 6 3 -approximation for the metric k-median clustering on Γ , and consequently a 13 3 approximation for k-prototype learning. [sent-220, score-0.243]
</p><p>81 , for any three vertices va , vb and vc in Γ , their edge weights satisfy the inequality w(va vb ) ≤ δ(w(va vc ) + w(vb vc )) for some constant δ > 1, where w(va vb ) is the weight of the edge connecting va and vb ). [sent-236, score-0.459]
</p><p>82 For a given set of rigid structures, if a (1 + )-approximation of the alignment between any pair of rigid structures can be computed, then the algorithm for metric k-median clustering in [9] yields a 2( 23 (1 + ) − 1)(1 + )-approximation for the k-prototype learning problem. [sent-238, score-1.642]
</p><p>83 3 What if the rigid structures have unequal sizes? [sent-239, score-0.718]
</p><p>84 In some scenario, the rigid structures may not have the same number of points, and consequently the one-to-one match between rigid structures in Deﬁnition 2 is not available. [sent-240, score-1.424]
</p><p>85 Generally speaking, for any rigid structure Pi containing m m points for some m = m, we assign each point with a weight equal to m , and compute the alignment cost based on EMD, rather than the bipartite matching cost. [sent-242, score-1.107]
</p><p>86 We consider two types of data, the sets of randomly generated 3D rigid structures and a real biological data set which is used to determine the organization pattern (among a population of cells) of chromosome territories inside the cell nucleus. [sent-246, score-1.192]
</p><p>87 For each data set, we ﬁrst randomly generate k different rigid structures, {Q1 , · · · , Qk }. [sent-249, score-0.569]
</p><p>88 We randomly select one point from each of the m Gaussian distributions (around the m points of Qj ) to form an m-rigid structure, and transform it by a random rigid transformation. [sent-251, score-0.641]
</p><p>89 Thus, we build a cluster (denoted by Cj ) of m-rigid structures around each Qj , and Qj can be viewed as its k prototype (i. [sent-252, score-0.464]
</p><p>90 We run the algorithm of k-prototype learning in Section 4, and denote the resulting k rigid structures by {Q1 , · · · , Qk }. [sent-256, score-0.697]
</p><p>91 , build the bipartite graph between {Q1 , · · · , Qk } and {Q1 , · · · , Qk }, and for each pair Qi and Qj , connect an edge with a weight equal to the alignment cost A(Qi , Qj ). [sent-260, score-0.503]
</p><p>92 Secondly, we compute the average alignment cost (denoted by cj ) between the rigid structures in Cj and Qj for 1 ≤ j ≤ k, and compute k the sum t2 = j=1 cj . [sent-261, score-1.322]
</p><p>93 , t1 ) has been reduced by our prototype learning algorithm, comparing to the cost (i. [sent-265, score-0.34]
</p><p>94 The experiment suggests that our generated prototypes are much closer (at least 40% for each k) to the ground truth than the input rigid structures. [sent-271, score-0.651]
</p><p>95 2: (a) Experimental results for random data; (b)A 2D slice of the 3D microscopic image of 8 pairs of chromosome territories; (c ) Average alignment cost for biological data set. [sent-284, score-0.557]
</p><p>96 Each image includes 8 pairs of chromosome territories (see Fig. [sent-287, score-0.318]
</p><p>97 In this way, each cell is converted into a rigid structure of 16 points. [sent-291, score-0.613]
</p><p>98 Since there is no ground truth for the biological data, we directly use the average alignment cost between our generated solutions and the input rigid structures to evaluate the performance. [sent-292, score-1.047]
</p><p>99 44%  Conclusion  In this paper, we study a new prototype learning problem, called k-prototype learning, for 3D rigid structures, and present a practical optimization model for it. [sent-308, score-0.858]
</p><p>100 As the base case, we consider the 1prototype learning problem, and reduce it to the chromatic clustering problem. [sent-309, score-0.468]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rigid', 0.569), ('chromatic', 0.36), ('qj', 0.291), ('prototype', 0.289), ('alignment', 0.246), ('pi', 0.224), ('chromosome', 0.188), ('cj', 0.164), ('territories', 0.13), ('structures', 0.128), ('clustering', 0.091), ('buffalo', 0.086), ('bipartite', 0.083), ('prototypes', 0.082), ('qm', 0.082), ('median', 0.08), ('uj', 0.07), ('pij', 0.068), ('qk', 0.064), ('matching', 0.062), ('points', 0.054), ('biological', 0.053), ('transformation', 0.051), ('cost', 0.051), ('um', 0.048), ('correlation', 0.048), ('va', 0.047), ('triangle', 0.045), ('graph', 0.045), ('cell', 0.044), ('berezney', 0.043), ('chromosomes', 0.043), ('gi', 0.043), ('rotation', 0.042), ('ding', 0.041), ('objective', 0.04), ('pl', 0.04), ('vb', 0.04), ('metric', 0.039), ('sin', 0.039), ('cos', 0.039), ('territory', 0.038), ('shapes', 0.038), ('partition', 0.036), ('gn', 0.036), ('inequality', 0.035), ('mover', 0.033), ('angles', 0.033), ('nition', 0.031), ('earth', 0.03), ('gl', 0.03), ('match', 0.03), ('organization', 0.029), ('arun', 0.029), ('cremer', 0.029), ('ferrer', 0.029), ('gpa', 0.029), ('jinhui', 0.029), ('preservation', 0.029), ('build', 0.028), ('abstraction', 0.027), ('pattern', 0.027), ('clusters', 0.027), ('edge', 0.026), ('vik', 0.025), ('procrustes', 0.025), ('euclidean', 0.025), ('pn', 0.025), ('geometric', 0.024), ('determine', 0.024), ('reconstruction', 0.024), ('vc', 0.024), ('weight', 0.024), ('emd', 0.023), ('net', 0.023), ('searching', 0.023), ('shape', 0.023), ('jiang', 0.023), ('vertices', 0.022), ('nucleus', 0.022), ('pm', 0.022), ('hungarian', 0.021), ('ql', 0.021), ('unequal', 0.021), ('object', 0.021), ('matched', 0.021), ('approximation', 0.02), ('ratio', 0.02), ('realizing', 0.019), ('euler', 0.019), ('microscopic', 0.019), ('charikar', 0.019), ('cluster', 0.019), ('ection', 0.019), ('preserved', 0.019), ('quality', 0.018), ('know', 0.018), ('pre', 0.018), ('point', 0.018), ('base', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="357-tfidf-1" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>Author: Hu Ding, Ronald Berezney, Jinhui Xu</p><p>Abstract: In this paper, we study the following new variant of prototype learning, called k-prototype learning problem for 3D rigid structures: Given a set of 3D rigid structures, ﬁnd a set of k rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized. Prototype learning is a core problem in machine learning and has a wide range of applications in many areas. Existing results on this problem have mainly focused on the graph domain. In this paper, we present the ﬁrst algorithm for learning multiple prototypes from 3D rigid structures. Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efﬁcient with quality guarantee. We validate our approach using two type of data sets, random data and biological data of chromosome territories. Experiments suggest that our approach can effectively learn prototypes in both types of data. 1</p><p>2 0.090677358 <a title="357-tfidf-2" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>Author: Bruno Scherrer</p><p>Abstract: Given a Markov Decision Process (MDP) with n states and m actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal “-discounted optimal policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal Ï advantage. We show that Howard’s PI terminates 1 2Ì 1 1 22 1 1 nm 1 after at most n(m ≠ 1) 1≠“ log 1≠“ = O 1≠“ log 1≠“ iterations, improving by a factor O(log 1 a result by [3], while Simplex-PI terminates n) 1 22 1 2 1 22 2 1 1 2 after at most n (m ≠ 1) 1 + 1≠“ log 1≠“ = O n m log 1≠“ 1≠“ iterations, improving by a factor O(log n) a result by [11]. Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor “: given a measure of the maximal transient time ·t and the maximal time ·r to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most n2 (m≠ # $ 1) (Á·r log(n·r )Ë + Á·r log(n·t )Ë) (m ≠ 1)Án·t log(n·t )Ë + Án·t log(n2 ·t )Ë = !</p><p>3 0.068009526 <a title="357-tfidf-3" href="./nips-2013-Pass-efficient_unsupervised_feature_selection.html">245 nips-2013-Pass-efficient unsupervised feature selection</a></p>
<p>Author: Crystal Maung, Haim Schweitzer</p><p>Abstract: The goal of unsupervised feature selection is to identify a small number of important features that can represent the data. We propose a new algorithm, a modiﬁcation of the classical pivoted QR algorithm of Businger and Golub, that requires a small number of passes over the data. The improvements are based on two ideas: keeping track of multiple features in each pass, and skipping calculations that can be shown not to affect the ﬁnal selection. Our algorithm selects the exact same features as the classical pivoted QR algorithm, and has the same favorable numerical stability. We describe experiments on real-world datasets which sometimes show improvements of several orders of magnitude over the classical algorithm. These results appear to be competitive with recently proposed randomized algorithms in terms of pass efﬁciency and run time. On the other hand, the randomized algorithms may produce more accurate features, at the cost of small probability of failure. 1</p><p>4 0.065739155 <a title="357-tfidf-4" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>Author: Ari Pakman, Liam Paninski</p><p>Abstract: We present a new approach to sample from generic binary distributions, based on an exact Hamiltonian Monte Carlo algorithm applied to a piecewise continuous augmentation of the binary distribution of interest. An extension of this idea to distributions over mixtures of binary and possibly-truncated Gaussian or exponential variables allows us to sample from posteriors of linear and probit regression models with spike-and-slab priors and truncated parameters. We illustrate the advantages of these algorithms in several examples in which they outperform the Metropolis or Gibbs samplers. 1</p><p>5 0.058626365 <a title="357-tfidf-5" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>6 0.054706261 <a title="357-tfidf-6" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>7 0.052199353 <a title="357-tfidf-7" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>8 0.051689565 <a title="357-tfidf-8" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>9 0.050502397 <a title="357-tfidf-9" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>10 0.048023239 <a title="357-tfidf-10" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>11 0.047917873 <a title="357-tfidf-11" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>12 0.047528725 <a title="357-tfidf-12" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>13 0.047254279 <a title="357-tfidf-13" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>14 0.046032496 <a title="357-tfidf-14" href="./nips-2013-Understanding_Dropout.html">339 nips-2013-Understanding Dropout</a></p>
<p>15 0.044864003 <a title="357-tfidf-15" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>16 0.04462409 <a title="357-tfidf-16" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>17 0.042997312 <a title="357-tfidf-17" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>18 0.04282067 <a title="357-tfidf-18" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>19 0.04076273 <a title="357-tfidf-19" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>20 0.04068172 <a title="357-tfidf-20" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.036), (2, -0.008), (3, 0.009), (4, 0.027), (5, 0.019), (6, -0.003), (7, -0.035), (8, -0.019), (9, 0.024), (10, 0.02), (11, 0.008), (12, 0.068), (13, 0.014), (14, 0.016), (15, 0.019), (16, -0.053), (17, -0.041), (18, -0.015), (19, 0.036), (20, -0.036), (21, -0.013), (22, -0.073), (23, 0.007), (24, -0.085), (25, -0.075), (26, 0.008), (27, -0.004), (28, -0.075), (29, -0.07), (30, -0.048), (31, -0.004), (32, 0.04), (33, 0.036), (34, -0.024), (35, 0.049), (36, 0.018), (37, 0.023), (38, -0.053), (39, 0.041), (40, 0.04), (41, 0.076), (42, 0.005), (43, 0.054), (44, 0.014), (45, -0.132), (46, 0.018), (47, 0.104), (48, -0.037), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9199127 <a title="357-lsi-1" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>Author: Hu Ding, Ronald Berezney, Jinhui Xu</p><p>Abstract: In this paper, we study the following new variant of prototype learning, called k-prototype learning problem for 3D rigid structures: Given a set of 3D rigid structures, ﬁnd a set of k rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized. Prototype learning is a core problem in machine learning and has a wide range of applications in many areas. Existing results on this problem have mainly focused on the graph domain. In this paper, we present the ﬁrst algorithm for learning multiple prototypes from 3D rigid structures. Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efﬁcient with quality guarantee. We validate our approach using two type of data sets, random data and biological data of chromosome territories. Experiments suggest that our approach can effectively learn prototypes in both types of data. 1</p><p>2 0.60166389 <a title="357-lsi-2" href="./nips-2013-Analyzing_the_Harmonic_Structure_in_Graph-Based_Learning.html">35 nips-2013-Analyzing the Harmonic Structure in Graph-Based Learning</a></p>
<p>Author: Xiao-Ming Wu, Zhenguo Li, Shih-Fu Chang</p><p>Abstract: We ﬁnd that various well-known graph-based models exhibit a common important harmonic structure in its target function – the value of a vertex is approximately the weighted average of the values of its adjacent neighbors. Understanding of such structure and analysis of the loss deﬁned over such structure help reveal important properties of the target function over a graph. In this paper, we show that the variation of the target function across a cut can be upper and lower bounded by the ratio of its harmonic loss and the cut cost. We use this to develop an analytical tool and analyze ﬁve popular graph-based models: absorbing random walks, partially absorbing random walks, hitting times, pseudo-inverse of the graph Laplacian, and eigenvectors of the Laplacian matrices. Our analysis sheds new insights into several open questions related to these models, and provides theoretical justiﬁcations and guidelines for their practical use. Simulations on synthetic and real datasets conﬁrm the potential of the proposed theory and tool.</p><p>3 0.54858971 <a title="357-lsi-3" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>Author: Masayuki Karasuyama, Hiroshi Mamitsuka</p><p>Abstract: Label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph. Label propagation assumes that data points (nodes) connected in a graph should have similar labels. Consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair. We propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function. In this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation. For further justiﬁcation, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model. Experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets. 1</p><p>4 0.51721174 <a title="357-lsi-4" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>Author: Marco Cuturi</p><p>Abstract: Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms’ dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximumentropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn’s matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classiﬁcation problem.</p><p>5 0.50290513 <a title="357-lsi-5" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<p>Author: Ulrike Von Luxburg, Morteza Alamgir</p><p>Abstract: Consider an unweighted k-nearest neighbor graph on n points that have been sampled i.i.d. from some unknown density p on Rd . We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or any distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate a local function of the gradient of p, and that integrating this function along shortest paths leads to an estimate of the underlying density. 1</p><p>6 0.49708471 <a title="357-lsi-6" href="./nips-2013-The_Power_of_Asymmetry_in_Binary_Hashing.html">326 nips-2013-The Power of Asymmetry in Binary Hashing</a></p>
<p>7 0.47969514 <a title="357-lsi-7" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>8 0.47430104 <a title="357-lsi-8" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>9 0.46669832 <a title="357-lsi-9" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>10 0.46566549 <a title="357-lsi-10" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>11 0.45526886 <a title="357-lsi-11" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>12 0.4533712 <a title="357-lsi-12" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>13 0.44656825 <a title="357-lsi-13" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>14 0.43735299 <a title="357-lsi-14" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>15 0.42843851 <a title="357-lsi-15" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>16 0.42360303 <a title="357-lsi-16" href="./nips-2013-Heterogeneous-Neighborhood-based_Multi-Task_Local_Learning_Algorithms.html">135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</a></p>
<p>17 0.42023495 <a title="357-lsi-17" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>18 0.41707373 <a title="357-lsi-18" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>19 0.41614082 <a title="357-lsi-19" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>20 0.41208011 <a title="357-lsi-20" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.068), (33, 0.1), (34, 0.104), (41, 0.037), (49, 0.022), (56, 0.098), (61, 0.248), (70, 0.035), (85, 0.075), (89, 0.032), (92, 0.013), (93, 0.023), (95, 0.021), (99, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76420206 <a title="357-lda-1" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>Author: Hu Ding, Ronald Berezney, Jinhui Xu</p><p>Abstract: In this paper, we study the following new variant of prototype learning, called k-prototype learning problem for 3D rigid structures: Given a set of 3D rigid structures, ﬁnd a set of k rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized. Prototype learning is a core problem in machine learning and has a wide range of applications in many areas. Existing results on this problem have mainly focused on the graph domain. In this paper, we present the ﬁrst algorithm for learning multiple prototypes from 3D rigid structures. Our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efﬁcient with quality guarantee. We validate our approach using two type of data sets, random data and biological data of chromosome territories. Experiments suggest that our approach can effectively learn prototypes in both types of data. 1</p><p>2 0.61889565 <a title="357-lda-2" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>Author: Dae Il Kim, Prem Gopalan, David Blei, Erik Sudderth</p><p>Abstract: Stochastic block models characterize observed network relationships via latent community memberships. In large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size. We introduce a new model for these phenomena, the hierarchical Dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities. To allow scalable learning, we derive an online stochastic variational inference algorithm. Focusing on assortative models of undirected networks, we also propose an efﬁcient structured mean ﬁeld variational bound, and online methods for automatically pruning unused communities. Compared to state-of-the-art online learning methods for parametric relational models, we show signiﬁcantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes. We also showcase an analysis of LittleSis, a large network of who-knows-who at the heights of business and government. 1</p><p>3 0.60745442 <a title="357-lda-3" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>Author: Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael Jordan</p><p>Abstract: Research on distributed machine learning algorithms has focused primarily on one of two extremes—algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conﬂicts are unlikely and if conﬂicts do arise a conﬂict-resolution protocol is invoked. We view this “optimistic concurrency control” paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. 1</p><p>4 0.6073736 <a title="357-lda-4" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>Author: Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Muse, Guillermo Sapiro</p><p>Abstract: Graph matching is a challenging problem with very important applications in a wide range of ﬁelds, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsityrelated techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efﬁciently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available. 1</p><p>5 0.60432798 <a title="357-lda-5" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan</p><p>Abstract: We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the ﬁnal result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efﬁcient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data. 1</p><p>6 0.60424346 <a title="357-lda-6" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>7 0.6036405 <a title="357-lda-7" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>8 0.60356808 <a title="357-lda-8" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>9 0.6023711 <a title="357-lda-9" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>10 0.60033727 <a title="357-lda-10" href="./nips-2013-Binary_to_Bushy%3A_Bayesian_Hierarchical_Clustering_with_the_Beta_Coalescent.html">58 nips-2013-Binary to Bushy: Bayesian Hierarchical Clustering with the Beta Coalescent</a></p>
<p>11 0.60008281 <a title="357-lda-11" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>12 0.59979367 <a title="357-lda-12" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>13 0.59929168 <a title="357-lda-13" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>14 0.59902865 <a title="357-lda-14" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>15 0.59870213 <a title="357-lda-15" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>16 0.59806323 <a title="357-lda-16" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>17 0.59774464 <a title="357-lda-17" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>18 0.59774435 <a title="357-lda-18" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>19 0.59684217 <a title="357-lda-19" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>20 0.59593648 <a title="357-lda-20" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
