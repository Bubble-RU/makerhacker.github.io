<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-50" href="#">nips2013-50</a> knowledge-graph by maker-knowledge-mining</p><h1>50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</h1>
<br/><p>Source: <a title="nips-2013-50-pdf" href="http://papers.nips.cc/paper/5111-bayesian-mixture-modelling-and-inference-based-thompson-sampling-in-monte-carlo-tree-search.pdf">pdf</a></p><p>Author: Aijun Bai, Feng Wu, Xiaoping Chen</p><p>Abstract: Monte-Carlo tree search (MCTS) has been drawing great interest in recent years for planning and learning under uncertainty. One of the key challenges is the trade-off between exploration and exploitation. To address this, we present a novel approach for MCTS using Bayesian mixture modeling and inference based Thompson sampling and apply it to the problem of online planning in MDPs. Our algorithm, named Dirichlet-NormalGamma MCTS (DNG-MCTS), models the uncertainty of the accumulated reward for actions in the search tree as a mixture of Normal distributions. We perform inferences on the mixture in Bayesian settings by choosing conjugate priors in the form of combinations of Dirichlet and NormalGamma distributions and select the best action at each decision node using Thompson sampling. Experimental results conÔ¨Årm that our algorithm advances the state-of-the-art UCT approach with better values on several benchmark problems. 1</p><p>Reference: <a title="nips-2013-50-reference" href="../nips2013_reference/nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('uct', 0.498), ('mcts', 0.391), ('thompson', 0.313), ('policy', 0.208), ('rollout', 0.183), ('normalgamm', 0.174), ('mab', 0.174), ('st', 0.154), ('reward', 0.152), ('ctp', 0.13), ('ormalgamm', 0.13), ('ucto', 0.13), ('xs', 0.126), ('accum', 0.121), ('mdp', 0.113), ('racetrack', 0.109), ('thompsonsampl', 0.109), ('uctb', 0.109), ('mdps', 0.109), ('horizon', 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="50-tfidf-1" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>2 0.27187058 <a title="50-tfidf-2" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>3 0.25656623 <a title="50-tfidf-3" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>4 0.2441666 <a title="50-tfidf-4" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>5 0.20871899 <a title="50-tfidf-5" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>6 0.2010116 <a title="50-tfidf-6" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>7 0.19909804 <a title="50-tfidf-7" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>8 0.19828036 <a title="50-tfidf-8" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>9 0.1982757 <a title="50-tfidf-9" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>10 0.17981021 <a title="50-tfidf-10" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>11 0.17591718 <a title="50-tfidf-11" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>12 0.17159338 <a title="50-tfidf-12" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>13 0.16732937 <a title="50-tfidf-13" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>14 0.16732 <a title="50-tfidf-14" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>15 0.15877987 <a title="50-tfidf-15" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>16 0.15680429 <a title="50-tfidf-16" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>17 0.14566924 <a title="50-tfidf-17" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>18 0.14448453 <a title="50-tfidf-18" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>19 0.1299879 <a title="50-tfidf-19" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>20 0.12633201 <a title="50-tfidf-20" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.228), (1, -0.299), (2, -0.079), (3, -0.086), (4, 0.004), (5, -0.016), (6, 0.043), (7, 0.172), (8, 0.031), (9, 0.007), (10, 0.001), (11, 0.067), (12, -0.003), (13, 0.007), (14, -0.012), (15, 0.009), (16, -0.074), (17, 0.074), (18, -0.011), (19, -0.035), (20, -0.036), (21, 0.002), (22, -0.04), (23, -0.081), (24, 0.019), (25, -0.024), (26, -0.066), (27, -0.079), (28, 0.063), (29, -0.09), (30, 0.13), (31, -0.06), (32, 0.016), (33, -0.041), (34, 0.021), (35, -0.031), (36, -0.122), (37, 0.085), (38, 0.079), (39, -0.015), (40, 0.001), (41, 0.025), (42, 0.036), (43, 0.024), (44, -0.004), (45, -0.04), (46, 0.013), (47, 0.022), (48, -0.01), (49, -0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90169716 <a title="50-lsi-1" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>2 0.81715733 <a title="50-lsi-2" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>3 0.77224857 <a title="50-lsi-3" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>4 0.7039994 <a title="50-lsi-4" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>5 0.69595826 <a title="50-lsi-5" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>6 0.64723283 <a title="50-lsi-6" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>7 0.6468451 <a title="50-lsi-7" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>8 0.64637685 <a title="50-lsi-8" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>9 0.64290613 <a title="50-lsi-9" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>10 0.6408568 <a title="50-lsi-10" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>11 0.6295805 <a title="50-lsi-11" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>12 0.62745547 <a title="50-lsi-12" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>13 0.61744946 <a title="50-lsi-13" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>14 0.58320105 <a title="50-lsi-14" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>15 0.55982733 <a title="50-lsi-15" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>16 0.53918922 <a title="50-lsi-16" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>17 0.53286666 <a title="50-lsi-17" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>18 0.5193519 <a title="50-lsi-18" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>19 0.51241398 <a title="50-lsi-19" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>20 0.50768083 <a title="50-lsi-20" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(20, 0.101), (25, 0.079), (37, 0.046), (70, 0.044), (71, 0.301), (80, 0.189), (86, 0.079), (87, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88959771 <a title="50-lda-1" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>2 0.74731946 <a title="50-lda-2" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>same-paper 3 0.74307823 <a title="50-lda-3" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>4 0.73300022 <a title="50-lda-4" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>5 0.69906801 <a title="50-lda-5" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>6 0.6326564 <a title="50-lda-6" href="./nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</a></p>
<p>7 0.63068795 <a title="50-lda-7" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>8 0.63040543 <a title="50-lda-8" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>9 0.63009405 <a title="50-lda-9" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>10 0.6298784 <a title="50-lda-10" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>11 0.62905771 <a title="50-lda-11" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>12 0.62798554 <a title="50-lda-12" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>13 0.62782687 <a title="50-lda-13" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>14 0.62781584 <a title="50-lda-14" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>15 0.62720281 <a title="50-lda-15" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>16 0.6252985 <a title="50-lda-16" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>17 0.62391597 <a title="50-lda-17" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>18 0.62212092 <a title="50-lda-18" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>19 0.62162697 <a title="50-lda-19" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>20 0.62052345 <a title="50-lda-20" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<br/><br/><br/></body>
</html>
