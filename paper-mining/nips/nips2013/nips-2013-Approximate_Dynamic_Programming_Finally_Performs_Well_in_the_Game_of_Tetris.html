<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-38" href="#">nips2013-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</h1>
<br/><p>Source: <a title="nips-2013-38-pdf" href="http://papers.nips.cc/paper/5190-approximate-dynamic-programming-finally-performs-well-in-the-game-of-tetris.pdf">pdf</a></p><p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Bruno Scherrer</p><p>Abstract: Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the ﬁrst time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI’s results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE. 1</p><p>Reference: <a title="nips-2013-38-reference" href="../nips2013_reference/nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. [sent-7, score-0.154]
</p><p>2 This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. [sent-9, score-0.255]
</p><p>3 So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. [sent-10, score-0.304]
</p><p>4 In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. [sent-11, score-0.434]
</p><p>5 1  Introduction  Tetris is a popular video game created by Alexey Pajitnov in 1985. [sent-14, score-0.127]
</p><p>6 The game is played on a grid originally composed of 20 rows and 10 columns, where pieces of 7 different shapes fall from the top – see Figure 1. [sent-15, score-0.168]
</p><p>7 The goal is to remove as many rows as possible before the game is over, i. [sent-18, score-0.15]
</p><p>8 In this paper, we consider the variation of the game in which the player knows only the current falling piece, and not the next several coming pieces. [sent-21, score-0.181]
</p><p>9 This game constitutes an interesting optimization benchmark in which the goal is to ﬁnd a controller (policy) that maximizes the average (over multiple games) number of lines removed in a game (score). [sent-22, score-0.401]
</p><p>10 It contains a huge number of board conﬁgurations (about 2200 � 1. [sent-24, score-0.184]
</p><p>11 6 × 1060 ), and even in the case that the sequence of pieces is known in advance, Figure 1: A screen-shot of the game of Tetris with its seven pieces (shapes). [sent-25, score-0.163]
</p><p>12 These algorithms formulate Tetris as a Markov decision process (MDP) in which the state is deﬁned by the current board conﬁguration plus the falling piece, the actions are the ∗  1  Mohammad Ghavamzadeh is currently at Adobe Research, on leave of absence from INRIA. [sent-28, score-0.257]
</p><p>13 Note that this number is ﬁnite because it was shown that Tetris is a game that ends with probability one [3]. [sent-29, score-0.127]
</p><p>14 1  possible orientations of the piece and the possible locations that it can be placed on the board,2 and the reward is deﬁned such that maximizing the expected sum of rewards from each state coincides with maximizing the score from that state. [sent-30, score-0.178]
</p><p>15 Since the state space is large in Tetris, these methods use value function approximation schemes (often linear approximation) and try to tune the value function parameters (weights) from game simulations. [sent-31, score-0.198]
</p><p>16 They used the approximate value iteration algorithm with two state features: the board height and the number of holes in the board, and obtained a low score of 30 to 40. [sent-33, score-0.482]
</p><p>17 Bertsekas and Ioffe [1] proposed the λ-Policy Iteration (λ-PI) algorithm (a generalization of value and policy iteration) and applied it to Tetris. [sent-34, score-0.25]
</p><p>18 They approximated the value function as a linear combination of a more elaborate set of 22 features and reported the score of 3, 200 lines. [sent-35, score-0.226]
</p><p>19 The exact same empirical study was revisited recently by Scherrer [16], who corrected an implementation bug in [1], and reported more stable learning curves and the score of 4, 000 lines. [sent-36, score-0.151]
</p><p>20 Kakade [11] applied a natural policy gradient method to Tetris and reported a score of about 6, 800 lines. [sent-38, score-0.364]
</p><p>21 Farias and Van Roy [6] applied a linear programming algorithm to the game and achieved the score of 4, 700 lines. [sent-39, score-0.239]
</p><p>22 Furmston and Barber [8] proposed an approximate Newton method to search in a policy space and were able to obtain a score of about 14, 000. [sent-40, score-0.38]
</p><p>23 He used a heuristic evaluation function to give a score to each possible strategy (in a way similar to value function in ADP), and eventually returned the one with the highest score. [sent-42, score-0.138]
</p><p>24 Dellacherie’s evaluation function is made of 6 high-quality features with weights chosen by hand, and achieved a score of about 5, 000, 000 lines [19]. [sent-43, score-0.244]
</p><p>25 They reported the score of 350, 000 lines averaged over 30 games, outperforming the ADP and RL approaches that used the same features. [sent-45, score-0.202]
</p><p>26 This led to the best publicly known controller (to the best of our knowledge) with the score of around 35, 000, 000 lines. [sent-47, score-0.194]
</p><p>27 This observation makes us conjecture that Tetris is a game whose policy space is easier to represent, and as a result to search in, than its value function space. [sent-52, score-0.44]
</p><p>28 Therefore, in order to obtain a good performance with ADP algorithms in this game, we should use those ADP methods that search in a policy space, instead of the more traditional ones that search in a value function space. [sent-53, score-0.304]
</p><p>29 Fortunately a class of such ADP algorithms, called classiﬁcation-based policy iteration (CbPI), have been recently developed and analyzed [12, 7, 13, 9, 17]. [sent-54, score-0.286]
</p><p>30 These algorithms differ from the standard value function based ADP methods in how the greedy policy is computed. [sent-55, score-0.278]
</p><p>31 Since CbPI methods search in a policy space (deﬁned by a classiﬁer) instead of a value function space, we believe that they should perform better than their value function based counterparts in problems in which good policies are easier to represent than their corresponding value functions. [sent-57, score-0.419]
</p><p>32 In this paper, we put our conjecture to test by applying an algorithm in this class, called classiﬁcation-based modiﬁed policy iteration (CBMPI) [17], to the game of Tetris, and compare its performance with the CE method and the λ-PI algorithm. [sent-58, score-0.434]
</p><p>33 do Generate a random sample of n parameter vectors {θi }n ∼ N (µ, σ 2 I) i=1 For each θi , play L games and calculate the average number of rows removed (score) by the controller � � Select �ρn� parameters with the highest score θ1 , . [sent-67, score-0.288]
</p><p>34 A state s in Tetris consists of two components: the description of the board b and the type of the falling piece p. [sent-76, score-0.295]
</p><p>35 We can think of the parameter vector θ as a policy (controller) whose performance is speciﬁed by the corresponding evaluation function f (·) = φ(·)θ. [sent-83, score-0.253]
</p><p>36 The features used in Tetris for a state-action pair (s, a) may depend on the description of the board b� resulted from taking action a in state s, e. [sent-84, score-0.29]
</p><p>37 For each parameter θi , we play L games and calculate the average number of rows removed by this controller (an estimate of the evaluation � � function). [sent-93, score-0.216]
</p><p>38 vk , Tπk is the Bellman operator associated with the policy πk , and m ≥ 1 is a parameter. [sent-104, score-0.334]
</p><p>39 MPI generalizes the well-known value and policy iteration algorithms for the values m = 1 and m = ∞, respectively. [sent-105, score-0.306]
</p><p>40 CBMPI [17] is an approximation of MPI that uses an explicit representation for the policies πk , in addition to the one used for the value functions vk . [sent-106, score-0.211]
</p><p>41 The idea is similar to the classiﬁcation-based PI algorithms [12, 7, 13] in which we search for the greedy policy in a policy space Π (deﬁned by a classiﬁer) instead of computing it from the estimated value function (as in the standard implementation of MPI). [sent-107, score-0.55]
</p><p>42 As described in Figure 3, CBMPI begins with an arbitrary initial policy π1 ∈ Π and value function v0 ∈ F. [sent-108, score-0.25]
</p><p>43 3 At each iteration k, a new value func3  Note that the function space F and policy space Π are deﬁned by the choice of the regressor and classiﬁer. [sent-109, score-0.38]
</p><p>44 3  Input: value function space F, policy space Π, state distribution µ Initialize: Set π1 ∈ Π and v0 ∈ F to an arbitrary policy and value function for k = 1, 2, . [sent-110, score-0.546]
</p><p>45 To set up the regression problem, we build a rollout set Dk by sampling N states i. [sent-116, score-0.204]
</p><p>46 For each state s(i) ∈ Dk , we generate a rollout s(i) , a0 , r0 , s1 , . [sent-120, score-0.192]
</p><p>47 From this rollout, we compute an unbiased estimate vk (s(i) ) of (Tπk )m vk−1 (s(i) ) as � vk (s(i) ) = �  m−1 �  (i)  γ t rt + γ m vk−1 (s(i) ), m  (γ is the discount factor),  (1)  t=0  ��N �� � and use it to build a training set s(i) , vk (s(i) ) i=1 . [sent-124, score-0.345]
</p><p>48 This training set is then used by the regressor to compute vk as an estimate of (Tπk )m vk−1 . [sent-125, score-0.148]
</p><p>49 The regressor ﬁnds a function v ∈ F that minimizes the empirical error N �2 1 �� � µ (2) LF (�; v) = vk (s(i) ) − v(s(i) ) . [sent-126, score-0.148]
</p><p>50 � k N i=1 The greedy step at iteration k computes the policy πk+1 as the best approximation of � � G (Tπk )m vk−1 by minimizing the cost-sensitive empirical error (cost-sensitive classiﬁcation) N� �� � �� Π � � �k (�; π) = 1 max Qk (s(i) , a) − Qk s(i) , π(s(i) ) . [sent-127, score-0.334]
</p><p>51 L µ N � i=1 a∈A  (3)  � To set up this cost-sensitive classiﬁcation problem, we build a rollout set Dk by sampling N � states (i) � i. [sent-128, score-0.204]
</p><p>52 For each state s ∈ Dk and each action a ∈ A, we build M independent � (i,j) (i,j) (i,j) (i,j) (i,j) (i,j) �M rollouts of size m + 1, i. [sent-132, score-0.169]
</p><p>53 From these rollouts, we compute an unbiased estimate of Qk (s(i) , a) as Qk (s(i) , a) = �M j (i) 1 j=1 Rk (s , a) where each rollout estimate is deﬁned as M j Rk (s(i) , a) =  m �  (i,j)  γ t rt  (i,j)  + γ m+1 vk−1 (sm+1 ). [sent-138, score-0.195]
</p><p>54 4  In our implementation of CBMPI (DPI) in Tetris (Section 3), we use the same rollout set � (Dk = Dk ) and rollouts for the classiﬁer and regressor. [sent-140, score-0.297]
</p><p>55 We use the policies of the form πu (s) = argmaxa ψ(s, a)u, where ψ is the policy feature vector (possibly different from the value function feature vector φ) and u is the policy parameter vector. [sent-151, score-0.593]
</p><p>56 We compute �Π µ the next policy πk+1 by minimizing the empirical error Lk (�; πu ), deﬁned by (3), using the covariance matrix adaptation evolution strategy (CMA-ES) algorithm [10]. [sent-152, score-0.23]
</p><p>57 In order to evaluate a policy �Π µ u in CMA-ES, we only need to compute Lk (�; πu ), and given the training set, this procedure does not require any simulation of the game. [sent-153, score-0.23]
</p><p>58 This is in contrary with policy evaluation in CE that requires playing several games, and it is the main reason that we obtain the same performance as CE with CBMPI with almost 1/6 number of samples (see Section 3. [sent-154, score-0.253]
</p><p>59 1  Experimental Setup  In our experiments, the policies learned by the algorithms are evaluated by their score (average number of rows removed in a game) averaged over 200 games in the small 10 × 10 board and over 20 games in the large 10 × 20 board. [sent-161, score-0.607]
</p><p>60 The performance of each algorithm is represented by a learning curve whose value at each iteration is the average score of the policies learned by the algorithm at that iteration in 100 separate runs of the algorithm. [sent-162, score-0.33]
</p><p>61 2, this is due the fact that although the classiﬁer in CBMPI/DPI uses a direct search in the space of policies (for the greedy policy), it evaluates each candidate policy using the empirical error of Eq. [sent-166, score-0.387]
</p><p>62 In fact, the budget B of CBMPI/DPI is ﬁxed in advance by the number of rollouts N M and the rollout’s length m as B = (m + 1)N M |A|. [sent-168, score-0.164]
</p><p>63 In contrary, CE evaluates a candidate policy by playing several games, a process that can be extremely costly (sample-wise), especially for good policies in the large board. [sent-169, score-0.317]
</p><p>64 In our CBMPI/DPI experiments, we set the number of rollouts per state-action pair M = 1, as this value has shown the best performance. [sent-170, score-0.161]
</p><p>65 For large values of m, the size of the rollout set decreases as N = O(B/m), which in turn decreases the accuracy of both the regressor and classiﬁer. [sent-173, score-0.22]
</p><p>66 This leads to a trade-off between long rollouts and the number of states in the rollout set. [sent-174, score-0.311]
</p><p>67 We sample the rollout states in CBMPI/DPI from the trajectories generated by a very good policy for Tetris, namely the DU controller [20]. [sent-177, score-0.479]
</p><p>68 Since the DU policy is good, this rollout set is biased towards boards with small height. [sent-178, score-0.46]
</p><p>69 We noticed from our experiments that the performance can be signiﬁcantly improved if we use boards with different heights in the rollout sets. [sent-179, score-0.23]
</p><p>70 We set the initial value function parameter to w = ¯ and select the initial policy 0 π1 (policy parameter u) randomly. [sent-181, score-0.25]
</p><p>71 We also set n = 1000 and L = 10 in the small board and n = 100 and L = 1 in the large board. [sent-186, score-0.184]
</p><p>72 , the landing height of the falling piece, the number of eroded piece cells, the row transitions, the column transitions, the number of holes, and the number of board wells; plus 3 additional features proposed in [20], i. [sent-190, score-0.452]
</p><p>73 Note that the best policies reported in the literature have been learned using this set of features. [sent-193, score-0.175]
</p><p>74 2  Experiments  We ﬁrst run the algorithms on the small board to study the role of their parameters and to select the best features and parameters (Section 3. [sent-199, score-0.276]
</p><p>75 We then use the selected features and parameters and apply the algorithms to the large board (Figure 5 (d)) Finally, we compare the best policies found in our experiments with the best controllers reported in the literature (Tables 1 and 2). [sent-202, score-0.466]
</p><p>76 Here we use D-T features for the evaluation function in CE, the value function in λ-PI, and the policy in DPI and CBMPI. [sent-207, score-0.345]
</p><p>77 The CE method reaches the score 3000 after 10 iterations using an average budget B = 65, 000, 000. [sent-210, score-0.168]
</p><p>78 λ-PI with the best value of λ only manages to score 400. [sent-211, score-0.135]
</p><p>79 1, having short rollouts (m = 1) in DPI leads to poor action-value estimates Q, while having too long rollouts (m = 20) decreases the size of the training set of the classiﬁer N . [sent-215, score-0.242]
</p><p>80 More generally Figure 4 conﬁrms the superiority of the policy search and classiﬁcation-based PI methods to value function based ADP algorithms (λ-PI). [sent-223, score-0.277]
</p><p>81 This suggests that the D-T features are more suitable to represent the policies than the value functions in Tetris. [sent-224, score-0.179]
</p><p>82 CE achieves the score of 500 after about 60 iterations and outperforms λ-PI with score of 350. [sent-227, score-0.22]
</p><p>83 It is clear that the Bertsekas features lead to much weaker results than those obtained by the D-T features in Figure 4 for all the algorithms. [sent-228, score-0.144]
</p><p>84 We may conclude then that the D-T features are more suitable than the Bertsekas features to represent both value functions and policies in Tetris. [sent-229, score-0.251]
</p><p>85 5  6  500 400 300 200 100  Averaged lines removed  4000 3000 2000 1000  Averaged lines removed  Parameter λ 0 0. [sent-235, score-0.176]
</p><p>86 2  Large (10 × 20) Board  We now use the best parameters and features in the small board experiments, run CE, DPI, and CBMPI algorithms in the large board, and report their results in Figure 5 (d). [sent-251, score-0.276]
</p><p>87 While λ-PI with per iteration budget 620, 000, at its best, achieves the score of 2500 (due to space limitation, we do not report these results here), DPI and CBMPI, with m = 10, reach the scores of 12, 000, 000 and 21, 000, 000 after 3 and 6 iterations, respectively. [sent-253, score-0.209]
</p><p>88 Comparison of the best policies: So far the reported scores for each algorithm was averaged over the policies learned in 100 separate runs. [sent-258, score-0.193]
</p><p>89 We then compare these results with the best policies reported in the literature, i. [sent-260, score-0.146]
</p><p>90 The DT-10 and DT-20 policies, whose weights and features are given in Table 2, are policies learned by CBMPI with D-T features in the small and large boards, respectively. [sent-263, score-0.247]
</p><p>91 Note that DT-10 is the only policy among these four that has been learned in the small board. [sent-265, score-0.246]
</p><p>92 In the large board, DT-20 obtains the score of 51, 000, 000 and not only outperforms the other three policies, but also achieves the best reported result in the literature (to the best of our knowledge). [sent-266, score-0.187]
</p><p>93 7  600 500 400 300 200  Averaged lines removed  100  600 500 400 300 200  Averaged lines removed  100  Parameter λ 0 0. [sent-267, score-0.176]
</p><p>94 Boards \ Policies Small (10 × 10) board Large (10 × 20) board  DU 3800 31, 000, 000  BDU 4200 36, 000, 000  DT-10 5000 29, 000, 000  DT-20 4300 51, 000, 000  Table 1: Average (over 10, 000 games) score of DU, BDU, DT-10, and DT-20 policies. [sent-279, score-0.463]
</p><p>95 4  Conclusions  The game of Tetris has been always challenging for approximate dynamic programming (ADP) algorithms. [sent-299, score-0.154]
</p><p>96 In this paper, we applied a relatively novel ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to Tetris. [sent-301, score-0.286]
</p><p>97 In particular, the best policy learned by CBMPI obtained the performance of 51, 000, 000 lines on average, a new record in the large board of Tetris. [sent-303, score-0.487]
</p><p>98 Temporal differences-based policy iteration and applications in neuro-dynamic programming. [sent-307, score-0.286]
</p><p>99 A unifying perspective of parametric policy search methods for Markov decision processes. [sent-344, score-0.257]
</p><p>100 Modiﬁed policy iteration algorithms for discounted Markov decision problems. [sent-376, score-0.286]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cbmpi', 0.614), ('tetris', 0.351), ('dpi', 0.318), ('adp', 0.307), ('policy', 0.23), ('board', 0.184), ('rollout', 0.176), ('ce', 0.161), ('game', 0.127), ('rollouts', 0.121), ('vk', 0.104), ('score', 0.095), ('bertsekas', 0.088), ('policies', 0.087), ('features', 0.072), ('qk', 0.067), ('games', 0.06), ('controller', 0.059), ('iteration', 0.056), ('dk', 0.055), ('piece', 0.054), ('boards', 0.054), ('holes', 0.054), ('removed', 0.051), ('height', 0.044), ('regressor', 0.044), ('bdu', 0.044), ('dellacherie', 0.044), ('thiery', 0.044), ('budget', 0.043), ('falling', 0.041), ('reported', 0.039), ('classi', 0.038), ('lines', 0.037), ('du', 0.037), ('ghavamzadeh', 0.034), ('scherrer', 0.034), ('cbpi', 0.033), ('averaged', 0.031), ('controllers', 0.031), ('iterations', 0.03), ('szita', 0.029), ('er', 0.028), ('greedy', 0.028), ('mpi', 0.027), ('search', 0.027), ('gabillon', 0.025), ('rows', 0.023), ('evaluation', 0.023), ('cross', 0.022), ('lf', 0.022), ('eroded', 0.022), ('rincz', 0.022), ('wells', 0.022), ('conjecture', 0.021), ('value', 0.02), ('best', 0.02), ('documentation', 0.019), ('farias', 0.019), ('furmston', 0.019), ('landing', 0.019), ('team', 0.019), ('sm', 0.019), ('rt', 0.019), ('pieces', 0.018), ('dash', 0.018), ('transitions', 0.018), ('action', 0.018), ('achieved', 0.017), ('rl', 0.017), ('curves', 0.017), ('considerably', 0.017), ('adobe', 0.017), ('hole', 0.017), ('inria', 0.017), ('modi', 0.017), ('entropy', 0.017), ('lk', 0.016), ('learned', 0.016), ('state', 0.016), ('plus', 0.016), ('mohammad', 0.015), ('st', 0.015), ('lille', 0.015), ('tsitsiklis', 0.015), ('space', 0.015), ('box', 0.014), ('rk', 0.014), ('dynamic', 0.014), ('states', 0.014), ('lazaric', 0.014), ('build', 0.014), ('maxa', 0.013), ('approximate', 0.013), ('literature', 0.013), ('player', 0.013), ('fewer', 0.013), ('feature', 0.013), ('cells', 0.013), ('reward', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="38-tfidf-1" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Bruno Scherrer</p><p>Abstract: Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the ﬁrst time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI’s results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE. 1</p><p>2 0.20823631 <a title="38-tfidf-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>3 0.18116532 <a title="38-tfidf-3" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>4 0.15090032 <a title="38-tfidf-4" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>5 0.13429298 <a title="38-tfidf-5" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>Author: David J. Weiss, Ben Taskar</p><p>Abstract: Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Signiﬁcant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to ﬁnetune feature extraction to each input at run-time. We address the key challenge of learning to control ﬁne-grained feature extraction adaptively, exploiting nonhomogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efﬁcient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate signiﬁcant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task. 1</p><p>6 0.13066457 <a title="38-tfidf-6" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>7 0.11376252 <a title="38-tfidf-7" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>8 0.11191659 <a title="38-tfidf-8" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>9 0.10923959 <a title="38-tfidf-9" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>10 0.10912813 <a title="38-tfidf-10" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>11 0.10335988 <a title="38-tfidf-11" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>12 0.094121724 <a title="38-tfidf-12" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>13 0.093863226 <a title="38-tfidf-13" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>14 0.090612896 <a title="38-tfidf-14" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>15 0.088955723 <a title="38-tfidf-15" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>16 0.080320947 <a title="38-tfidf-16" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>17 0.078407928 <a title="38-tfidf-17" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>18 0.073899381 <a title="38-tfidf-18" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>19 0.069876634 <a title="38-tfidf-19" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>20 0.06486176 <a title="38-tfidf-20" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, -0.201), (2, -0.116), (3, 0.101), (4, 0.013), (5, 0.004), (6, -0.075), (7, 0.04), (8, 0.003), (9, 0.03), (10, 0.021), (11, -0.009), (12, -0.031), (13, 0.028), (14, 0.051), (15, -0.046), (16, 0.006), (17, -0.023), (18, -0.021), (19, 0.031), (20, 0.013), (21, -0.022), (22, -0.014), (23, 0.021), (24, 0.005), (25, -0.038), (26, 0.035), (27, 0.014), (28, 0.041), (29, -0.066), (30, -0.02), (31, 0.009), (32, 0.026), (33, -0.054), (34, 0.07), (35, 0.052), (36, 0.012), (37, -0.018), (38, 0.007), (39, 0.014), (40, 0.005), (41, -0.06), (42, 0.068), (43, -0.053), (44, -0.031), (45, -0.01), (46, 0.053), (47, -0.013), (48, 0.017), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93874472 <a title="38-lsi-1" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Bruno Scherrer</p><p>Abstract: Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the ﬁrst time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI’s results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE. 1</p><p>2 0.84486192 <a title="38-lsi-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>3 0.84308815 <a title="38-lsi-3" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>4 0.81162035 <a title="38-lsi-4" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>Author: Robert Lindsey, Michael Mozer, William J. Huggins, Harold Pashler</p><p>Abstract: Psychologists are interested in developing instructional policies that boost student learning. An instructional policy speciﬁes the manner and content of instruction. For example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence. Traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only diﬃcult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more diﬃcult (known as fading). We propose an alternative to the traditional methodology in which we deﬁne a parameterized space of policies and search this space to identify the optimal policy. For example, in concept learning, policies might be described by a fading function that speciﬁes exemplar diﬃculty over time. We propose an experimental technique for searching policy spaces using Gaussian process surrogate-based optimization and a generative model of student performance. Instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects. Even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and to identify the global optimum, and is as eﬃcient in its subject budget as a traditional A-B comparison. We evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans outside the educational arena. 1</p><p>5 0.79888707 <a title="38-lsi-5" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>Author: Aswin Raghavan, Roni Khardon, Alan Fern, Prasad Tadepalli</p><p>Abstract: This paper addresses the scalability of symbolic planning under uncertainty with factored states and actions. Our ﬁrst contribution is a symbolic implementation of Modiﬁed Policy Iteration (MPI) for factored actions that views policy evaluation as policy-constrained value iteration (VI). Unfortunately, a na¨ve approach ı to enforce policy constraints can lead to large memory requirements, sometimes making symbolic MPI worse than VI. We address this through our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent algorithm lying between VI and MPI, that applies policy constraints if it does not increase the size of the value function representation, and otherwise performs VI backups. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signiﬁcantly improved scalability over state-of-the-art symbolic planners. 1</p><p>6 0.73310012 <a title="38-lsi-6" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>7 0.71155161 <a title="38-lsi-7" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>8 0.71085125 <a title="38-lsi-8" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>9 0.69668013 <a title="38-lsi-9" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>10 0.62903798 <a title="38-lsi-10" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>11 0.61987644 <a title="38-lsi-11" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>12 0.56994253 <a title="38-lsi-12" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>13 0.52891296 <a title="38-lsi-13" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>14 0.52735901 <a title="38-lsi-14" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>15 0.52233976 <a title="38-lsi-15" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>16 0.49459699 <a title="38-lsi-16" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>17 0.49445075 <a title="38-lsi-17" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>18 0.45338154 <a title="38-lsi-18" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>19 0.44237378 <a title="38-lsi-19" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>20 0.42898399 <a title="38-lsi-20" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.031), (16, 0.015), (33, 0.087), (34, 0.553), (36, 0.011), (41, 0.023), (56, 0.071), (70, 0.027), (85, 0.025), (89, 0.017), (93, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96529281 <a title="38-lda-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.95762813 <a title="38-lda-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.95133823 <a title="38-lda-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>4 0.94577706 <a title="38-lda-4" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>5 0.94177109 <a title="38-lda-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>6 0.93289381 <a title="38-lda-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>same-paper 7 0.92511535 <a title="38-lda-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.92136997 <a title="38-lda-8" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>9 0.90225208 <a title="38-lda-9" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>10 0.89561081 <a title="38-lda-10" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>11 0.7834425 <a title="38-lda-11" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>12 0.77759612 <a title="38-lda-12" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>13 0.77736586 <a title="38-lda-13" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>14 0.74410111 <a title="38-lda-14" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>15 0.73897481 <a title="38-lda-15" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>16 0.7363717 <a title="38-lda-16" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>17 0.73102748 <a title="38-lda-17" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>18 0.72987348 <a title="38-lda-18" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<p>19 0.72983831 <a title="38-lda-19" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>20 0.72464687 <a title="38-lda-20" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
