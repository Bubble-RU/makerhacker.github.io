<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-136" href="#">nips2013-136</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</h1>
<br/><p>Source: <a title="nips-2013-136-pdf" href="http://papers.nips.cc/paper/4991-hierarchical-modular-optimization-of-convolutional-networks-achieves-representations-similar-to-macaque-it-and-human-ventral-stream.pdf">pdf</a></p><p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>Reference: <a title="nips-2013-136-reference" href="../nips2013_reference/nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. [sent-7, score-0.526]
</p><p>2 One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i. [sent-8, score-0.695]
</p><p>3 Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. [sent-11, score-1.248]
</p><p>4 In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. [sent-12, score-1.234]
</p><p>5 The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. [sent-13, score-0.919]
</p><p>6 There is substantial evidence that the human brain solves this invariant object recognition challenge via a hierarchical cortical neuronal network called the ventral visual stream [13, 17], which has highly homologous areas in non-human primates [19, 9]. [sent-15, score-1.216]
</p><p>7 A core, long-standing hypothesis is that the visual input captured by the retina is rapidly processed through the ventral stream into an effective, “invariant” representation of object shape and identity [11, 9, 8]. [sent-16, score-0.749]
</p><p>8 This hypothesis has been bolstered by recent developments in neuroscience which have shown that abstract category-level visual information is accessible in IT (inferotemporal) cortex, the highest ventral cortical area, but much less effectively accessible in lower areas such as V1, V2 or V4 [23]. [sent-17, score-0.507]
</p><p>9 This observation has been conﬁrmed both at the individual neural level, where single-unit responses can be decoded using linear classiﬁers ∗  web. [sent-18, score-0.257]
</p><p>10 These simple elements are convolutional and are stacked hierarchically to construct non-linear computations of increasingly greater power, ranging through low (L1), medium (L2), and high (L3) complexity structures. [sent-26, score-0.134]
</p><p>11 B) Several of these elements are combined to produce mixtures capturing heterogenous neural populations. [sent-27, score-0.198]
</p><p>12 ) can be considered an analogous to a neural visual area. [sent-31, score-0.203]
</p><p>13 to to yield category predictions [14, 23] and at the population code level, where response vector correlation matrices evidence clear semantic structure [19]. [sent-32, score-0.256]
</p><p>14 Developing encoding models, models that map the stimulus to the neural response, of visual area IT would likely help us to understand object recognition in humans. [sent-33, score-0.517]
</p><p>15 Encoding models of lower-level visual responses (RGC, LGN, V1, V2) have been relatively successful [21, 4] (but cf. [sent-34, score-0.314]
</p><p>16 This explanatory gap, between model responses and IT responses, is present at both the level of the individual neuron responses and at the population code level. [sent-38, score-0.561]
</p><p>17 Of particular interest for our analysis in this paper, current models of IT, such as HMAX, have been shown to fail to achieve the speciﬁc categorical structures present in neural populations [18]. [sent-39, score-0.185]
</p><p>18 In other related work, descriptions of higher areas (V4, IT) responses have been made for very narrow classes of artiﬁcial stimuli and do not deﬁne responses to arbitrary natural images [6, 3]. [sent-40, score-0.424]
</p><p>19 In a step toward bridging this explanatory gap, we describe advances in constructing models that capture the categorical structures present in IT neural populations and fMRI measurements of humans. [sent-41, score-0.185]
</p><p>20 We take a top-down functional approach focused on building invariant object representations, optimizing biologically-plausible computational architectures for high performance on a challenging object recognition screening task. [sent-42, score-0.612]
</p><p>21 We then show that these models capture key response properties of IT, both at the level of individual neuronal responses as well as the neuronal population code – even for entirely new objects and categories never seen in model selection. [sent-43, score-0.629]
</p><p>22 This general type of model has been successful in describing a variety of phenomenology throughout the ventral stream [30]. [sent-47, score-0.47]
</p><p>23 We will now formally deﬁne the class of heterogeneous hierarchical convolutional neural networks, N . [sent-51, score-0.223]
</p><p>24 To produce deep feedforward networks, single layers are stacked: F ilter  T hreshold  N ormalize  P ool  −1 PΘP,l−1 − − → FΘF,l − − − − TΘT ,l − − − − NΘN,l − − PΘP,l −− − − −→ − − −→ −→  (2)  We denote such a stacking operation as N (Θ1 , . [sent-58, score-0.192]
</p><p>25 These networks N can, of course, also be stacked, just like their singlestack constituents, to form more complicated, deeper heterogenous hierarchies. [sent-70, score-0.139]
</p><p>26 2  High-Throughput Screening via Hierarchical Modular Optimization  Our goal is to ﬁnd models within N that are effective at modeling neural responses to a wide variety of images. [sent-73, score-0.257]
</p><p>27 To do this, our basic strategy is to perform high-throughput optimization on a screening task [28]. [sent-74, score-0.25]
</p><p>28 By choosing a screening task that is sufﬁciently representative of the aspects that make the object recognition problem challenging, we should be able to ﬁnd network architectures that are generally applicable. [sent-75, score-0.46]
</p><p>29 For our screening set, we created a set of 4500 synthetic images composed of 125 images each containing one of 36 three-dimensional mesh models of everyday objects, placed on naturalistic backgrounds. [sent-76, score-0.325]
</p><p>30 The screening task we evaluated was 36-way object recognition. [sent-77, score-0.362]
</p><p>31 We trained Maximum Correlation Classiﬁers (MCC) with 3-fold cross-validated 50%/50% train/test splits, using testing classiﬁcation percent-correct as the screening objective function. [sent-78, score-0.213]
</p><p>32 Because N is a very large space, determining among the vast space of possibilities which parameter setting(s) produce visual representations that are high performing on the screening set, is a challenge. [sent-79, score-0.424]
</p><p>33 To achieve this, we implemented a version of adaptive hyperparameter boosting, in which rounds of optimization are interleaved with boosting and hierarchical stacking. [sent-82, score-0.183]
</p><p>34 Speciﬁcally, suppose that N ∈ N and S is a screening stimulus set. [sent-83, score-0.317]
</p><p>35 Let E be the binary-valued classiﬁcation correctness indicator, assigning to each stimulus image s 1 or 0 according to whether the screening task prediction was right or wrong. [sent-84, score-0.356]
</p><p>36 Boosting: Consider the set of networks explored during step 1 as a set of weak learners, and apply a standard boosting algorithm (Adaboost) to identify some number of networks N11 , . [sent-91, score-0.187]
</p><p>37 3  } }  Optimization of Adaboost Combination single-stack networks  Performance  A)  B)  Round 1 Error Pattern  Round 2 Error Pattern  N11 N12  Screening set  (1) θ thr  (3) θ pool  (1) θ sat  Error-based reweighting Performance  Optimizing reweighted objective  N21 N22 N23 N24  (3) θ norm  . [sent-97, score-0.144]
</p><p>38 (1) θ filter  Combined Model  N13  0 Hierarchical Layering  Parameter 1  Param  eter 2  Figure 2: A) The Hierarchical Modular Optimization is a mechanism for efﬁciently optimizing neural networks for object recognition performance. [sent-103, score-0.377]
</p><p>39 The process ﬁrst identiﬁes complementary performance gradients in the space of single-stack (non-heterogenous) convolutional neural networks by using version of adaptive boosting interleaved with hyperparameter optimization. [sent-105, score-0.331]
</p><p>40 Bottom Right: The confusion matrix of the heterogenous model produced by combining the round 1 and round 2 networks. [sent-110, score-0.256]
</p><p>41 to be selected at each boosting round, the number of times K that the interleaved boosting and optimization is repeated and the number of times M this procedure is stacked. [sent-125, score-0.2]
</p><p>42 With the ﬁxed screening set described above, and these metaparameter settings, we generated a network NHM O . [sent-132, score-0.25]
</p><p>43 3  Predicting IT Neural Responses  Utilizing the NHM O network, we construct models of IT in one of two ways: 1) we estimate a GLM model predicting individual neural responses or 2) we estimate linear classiﬁers of object categories to produce a candidate IT neural space. [sent-136, score-0.561]
</p><p>44 To construct models of individual neural responses we estimate a linear mapping from a non-linear space produced by a model. [sent-137, score-0.292]
</p><p>45 Because IT responses are highly non-linear functions of the input image, successful models must 4  capture the non-linearity of the IT response. [sent-139, score-0.184]
</p><p>46 We evaluate goodness of ﬁt of by measuring the regression r2 values between the neural response and the GLM predictions on held-out images, averaged over several train/test splits. [sent-142, score-0.147]
</p><p>47 We also produce a candidate IT neural space by estimating linear classiﬁers on an object recognition task. [sent-148, score-0.322]
</p><p>48 As we might expect different subregions of IT cortex to have different selectivities for object categories (for example face, body, and place patches [15, 10]), the output of the linear classiﬁers will also respond preferentially to different object categories. [sent-149, score-0.414]
</p><p>49 Speciﬁcally, we estimate a linear mapping W to be the weights of a set of linear classiﬁers trained from the NHM O features on a speciﬁc set of object recognition tasks. [sent-151, score-0.21]
</p><p>50 We can then evaluate this mapping on a novel set of images and compare to measured IT or human ventral stream data. [sent-152, score-0.652]
</p><p>51 This method may have traction even when individual neural response data are not available. [sent-153, score-0.147]
</p><p>52 4  Representational Dissimilarity Matrices  Implicit in this discussion is the idea of comparing two different representations (in this case, the model’s predicted population versus the real neural population) on a ﬁxed stimulus set. [sent-155, score-0.315]
</p><p>53 , sk and vectors of neural population responses R = r1 , . [sent-160, score-0.353]
</p><p>54 , rk in which rij is the response of the j-th neuron to the i-th stimulus, deﬁne RDM (R)ij = 1 −  cov(ri , rj ) . [sent-163, score-0.134]
</p><p>55 var(ri ) · var(rj )  The RDM characterizes the layout of the stimuli in high-dimensional neural population space. [sent-164, score-0.169]
</p><p>56 Following [19], we measured similarity between population representations as the Spearman rank correlations between the RDMs for two populations, in which both RDMs are treated as vectors in k(k − 1)/2-dimensional space. [sent-165, score-0.138]
</p><p>57 Two populations can have similar RDMs on a given stimulus set, even if details of the neural responses are different. [sent-166, score-0.435]
</p><p>58 1), we obtained our own neural data on a testing set of our own design and tested the NHM O model’s ability to predict individual-level neural responses using the linear regression methodology described above. [sent-170, score-0.33]
</p><p>59 1  The Neural Representation Benchmark Image Set  We analyzed neural data collected on the Neural Representation Benchmark (NRB) dataset, which was originally developed to compare monkey neural and human behavioral responses [23, 2]. [sent-177, score-0.574]
</p><p>60 The objects come from eight “basic” categories (animals, boats, cars, chairs, faces, fruits, planes, tables), with eight exemplars per category (e. [sent-179, score-0.151]
</p><p>61 These parameters were varied concomitantly, picked randomly from a uniform ranges at three levels of object identity-preserving variation (low, medium, and high). [sent-182, score-0.24]
</p><p>62 The NRB set was designed to test explicitly the transformations of pose, position and size that are at the crux of the invariant object recognition problem. [sent-183, score-0.21]
</p><p>63 Performances are normalized relative to human behavioral data collected from Amazon Mechanical Turk experiments. [sent-186, score-0.126]
</p><p>64 High levels variation strongly separates the HMO model and the high-level IT neural features from the other representations. [sent-187, score-0.164]
</p><p>65 This neuron shows high functional selectivity for faces, which is effectively mirrored by the predicted unit. [sent-190, score-0.137]
</p><p>66 D) As populations increase in complexity and abstraction power, they become progressively more like that of IT, as category structure that was blurred out at lower levels by variability becomes abstracted at the higher levels. [sent-193, score-0.202]
</p><p>67 objects, categories or backgrounds used in the HMO screening set appeared in the NRB set; moreover, the NRB image set was created with different image and lighting parameters, with different rendering software. [sent-195, score-0.334]
</p><p>68 Neural data was obtained via large-scale parallel array electrophysiology recordings in the visual cortex of awake behaving macaques. [sent-196, score-0.241]
</p><p>69 Testing set images were presented foveally (central 10 deg) with a Rapid Serial Visual Presentation (RSVP) paradigm, involving passively viewing animals shown random stimulus sequences with durations comparable to those in natural primate ﬁxations (e. [sent-197, score-0.206]
</p><p>70 A total of 296 multi-unit responses were recorded from two animals. [sent-201, score-0.184]
</p><p>71 For each testing stimulus and neuron, ﬁnal neuron output responses were obtained by averaging data from between 25 and 50 repeated trials. [sent-202, score-0.348]
</p><p>72 6  Performance was assessed for three types of tasks, including 8-way basic category classiﬁcation, 8-way car object identiﬁcation, and 8-way face object identiﬁcation. [sent-204, score-0.384]
</p><p>73 Performances were also measured for neural output features, building on previous results showing that V4 neurons performed less well than IT neurons at higher variation levels[23], and conﬁrming that the testing tasks meaningfully engaged higherlevel vision processing. [sent-208, score-0.219]
</p><p>74 Since the testing set contains entirely different objects in non-overlapping basic categories, with none of the same backgrounds, this suggests that the nonlinearity identiﬁed in the HMO screening phase is able to achieve signiﬁcant generalization across image domains. [sent-211, score-0.348]
</p><p>75 Given that the model evidenced high transferable performance, we next determined the ability of the model to explain low-level neuronal responses using regression. [sent-212, score-0.252]
</p><p>76 Using the same transformation matrices W obtained from the regression ﬁtting, we also computed RDMs, which show signiﬁcant similarity to IT populations at both nearly comparable to the split-half similarity range of the IT population itself (ﬁg. [sent-215, score-0.17]
</p><p>77 A key comparison between models and data shows that as populations ascend the ventral hierarchy and increase in complexity, they become progressively closer to IT, with category structure that was blurred out at lower levels by variation becoming properly abstracted away at the higher levels (ﬁg. [sent-217, score-0.619]
</p><p>78 analyzed neural recordings made in an anterior patch of macaque IT on a small number of widely varying naturalistic images of every-day objects, and additionally obtained fMRI recordings from the analogous region of human visual cortex [19]. [sent-222, score-0.597]
</p><p>79 These objects included human and animal faces and body parts, as well as a variety of natural and man-made inanimate objects. [sent-223, score-0.289]
</p><p>80 Three striking ﬁndings of this work were that (i) the population code (as measured by RDMs) of the macaque neurons strongly mirrors the structure present in the human fMRI data, (ii) this structure appears to be dominated by the separation of animate vs inanimate object classes (ﬁg. [sent-224, score-0.56]
</p><p>81 Individual unit neural response data from these experiments is not publicly available. [sent-226, score-0.147]
</p><p>82 However, we were able to obtain a set of approximately 1000 additional training images with roughly similar categorical distinctions to that of the original target images, including distributions of human and animal faces and body parts, and a variety of other non-animal objects [16]. [sent-227, score-0.387]
</p><p>83 We posited that the population code structure present in the anterior region of IT recorded in the original experiment is guided by functional goals similar to the task distinctions supported by this dataset. [sent-228, score-0.216]
</p><p>84 In fact, the HMO-based population RDM strongly qualitatively matches that of the monkey IT RDM and, to a signiﬁcant but lesser extent, that of the human IT RDM (ﬁg. [sent-233, score-0.34]
</p><p>85 4  Discussion  High consistency with neural data at individual neuronal response and population code levels across several diverse datasets suggests that the HMO model is a good candidate model of the higher ventral stream processing. [sent-237, score-0.863]
</p><p>86 B) Representational Dissimilarity Matrices show a clear qualitative similarity between monkey IT and human IT on the one hand [19] and between these and the HMO model representation. [sent-240, score-0.244]
</p><p>87 The picture that emerges is of a general-purpose object recognition architecture – approximated by the NHM O network – situtated just posterior to a set of several downstream regions that can be thought of as specialized linear projections – the matrices W – from the more general upstream region. [sent-243, score-0.292]
</p><p>88 This two-step arrangement makes sense if there is a core set of object recognition primitives that are comparatively difﬁcult to discover, but which, once found, underlie many recognition tasks. [sent-245, score-0.271]
</p><p>89 This hypothesis makes testable predictions about how monkey and human visual systems should both respond to certain real-time training interventions (e. [sent-247, score-0.374]
</p><p>90 [20], to determine whether these algorithms provide further insight into ventral stream mechanisms. [sent-254, score-0.47]
</p><p>91 The neural representation benchmark and its evaluation on brain and machine. [sent-271, score-0.133]
</p><p>92 A cortical area selective for visual processing of the human body. [sent-344, score-0.307]
</p><p>93 The lateral occipital complex and its role in object recognition. [sent-362, score-0.188]
</p><p>94 Fast readout of object identity from macaque inferior temporal cortex. [sent-371, score-0.254]
</p><p>95 The fusiform face area: a module in human extrastriate cortex specialized for face perception. [sent-378, score-0.199]
</p><p>96 Object category structure in response patterns of neuronal population in monkey inferior temporal cortex. [sent-385, score-0.447]
</p><p>97 Representation of perceived object shape by the human lateral occipital complex. [sent-390, score-0.314]
</p><p>98 Matching categorical object representations in inferior temporal cortex of man and monkey. [sent-407, score-0.344]
</p><p>99 Coding of color and form in the geniculostriate visual pathway (invited review). [sent-416, score-0.13]
</p><p>100 A uniﬁed neuronal population code fully explains human object recognition. [sent-429, score-0.476]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmo', 0.429), ('ventral', 0.326), ('nhm', 0.254), ('rdm', 0.215), ('screening', 0.213), ('responses', 0.184), ('rdms', 0.176), ('object', 0.149), ('stream', 0.144), ('visual', 0.13), ('human', 0.126), ('monkey', 0.118), ('nrb', 0.117), ('stimulus', 0.104), ('kriegeskorte', 0.098), ('population', 0.096), ('heterogenous', 0.086), ('convolutional', 0.086), ('boosting', 0.081), ('yamins', 0.079), ('populations', 0.074), ('response', 0.074), ('neural', 0.073), ('cortex', 0.073), ('hmax', 0.071), ('dicarlo', 0.069), ('mcgovern', 0.069), ('neuronal', 0.068), ('faces', 0.065), ('hierarchical', 0.064), ('cadieu', 0.063), ('macaque', 0.063), ('dissimilarity', 0.063), ('recognition', 0.061), ('neuron', 0.06), ('brain', 0.06), ('objects', 0.059), ('chairs', 0.059), ('representational', 0.058), ('images', 0.056), ('networks', 0.053), ('reweighting', 0.052), ('boats', 0.052), ('fruits', 0.052), ('lterbank', 0.052), ('pinto', 0.052), ('cortical', 0.051), ('ers', 0.051), ('neurons', 0.05), ('round', 0.05), ('pooling', 0.05), ('category', 0.049), ('cars', 0.049), ('stacked', 0.048), ('animals', 0.046), ('variation', 0.046), ('upstream', 0.045), ('levels', 0.045), ('categories', 0.043), ('categorization', 0.043), ('spearman', 0.043), ('distinctions', 0.043), ('fmri', 0.042), ('inferior', 0.042), ('representations', 0.042), ('neurophysiol', 0.041), ('filter', 0.041), ('modular', 0.04), ('massachusetts', 0.04), ('functional', 0.04), ('neurosci', 0.039), ('planes', 0.039), ('esteky', 0.039), ('hahong', 0.039), ('hreshold', 0.039), ('inanimate', 0.039), ('kourtzi', 0.039), ('kreiman', 0.039), ('occipital', 0.039), ('ool', 0.039), ('ormalize', 0.039), ('thr', 0.039), ('produce', 0.039), ('image', 0.039), ('categorical', 0.038), ('interleaved', 0.038), ('recordings', 0.038), ('basic', 0.037), ('code', 0.037), ('network', 0.037), ('glm', 0.037), ('selectivity', 0.037), ('bars', 0.036), ('institute', 0.036), ('stacking', 0.036), ('produced', 0.035), ('confusion', 0.035), ('connor', 0.034), ('kiani', 0.034), ('abstracted', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="136-tfidf-1" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>2 0.28979808 <a title="136-tfidf-2" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>3 0.17497848 <a title="136-tfidf-3" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>4 0.15192895 <a title="136-tfidf-4" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>5 0.14260595 <a title="136-tfidf-5" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>Author: Yannick Schwartz, Bertrand Thirion, Gael Varoquaux</p><p>Abstract: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. 1</p><p>6 0.11600924 <a title="136-tfidf-6" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>7 0.11425063 <a title="136-tfidf-7" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>8 0.11263958 <a title="136-tfidf-8" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>9 0.10559162 <a title="136-tfidf-9" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>10 0.10146707 <a title="136-tfidf-10" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>11 0.099630676 <a title="136-tfidf-11" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>12 0.092089295 <a title="136-tfidf-12" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>13 0.091326222 <a title="136-tfidf-13" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>14 0.088985763 <a title="136-tfidf-14" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>15 0.088200718 <a title="136-tfidf-15" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>16 0.086533882 <a title="136-tfidf-16" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>17 0.084484249 <a title="136-tfidf-17" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>18 0.084221274 <a title="136-tfidf-18" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>19 0.080433726 <a title="136-tfidf-19" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>20 0.079376169 <a title="136-tfidf-20" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, 0.098), (2, -0.189), (3, -0.129), (4, -0.065), (5, -0.132), (6, -0.084), (7, -0.032), (8, -0.088), (9, 0.076), (10, -0.109), (11, -0.016), (12, -0.018), (13, -0.026), (14, -0.059), (15, 0.012), (16, -0.057), (17, -0.161), (18, -0.021), (19, -0.001), (20, 0.01), (21, 0.045), (22, -0.023), (23, -0.022), (24, -0.04), (25, -0.019), (26, -0.001), (27, 0.042), (28, -0.022), (29, 0.001), (30, -0.004), (31, -0.055), (32, -0.088), (33, -0.132), (34, -0.022), (35, -0.068), (36, -0.052), (37, -0.033), (38, 0.051), (39, -0.063), (40, -0.04), (41, -0.041), (42, -0.071), (43, 0.074), (44, 0.067), (45, 0.046), (46, 0.043), (47, -0.086), (48, 0.012), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94385052 <a title="136-lsi-1" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>2 0.81346405 <a title="136-lsi-2" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>3 0.76542962 <a title="136-lsi-3" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>Author: Yannick Schwartz, Bertrand Thirion, Gael Varoquaux</p><p>Abstract: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. 1</p><p>4 0.72821146 <a title="136-lsi-4" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>5 0.69653219 <a title="136-lsi-5" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>Author: Matjaz Jogan, Alan Stocker</p><p>Abstract: How do humans perceive the speed of a coherent motion stimulus that contains motion energy in multiple spatiotemporal frequency bands? Here we tested the idea that perceived speed is the result of an integration process that optimally combines speed information across independent spatiotemporal frequency channels. We formalized this hypothesis with a Bayesian observer model that combines the likelihood functions provided by the individual channel responses (cues). We experimentally validated the model with a 2AFC speed discrimination experiment that measured subjects’ perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We found that the perceived speeds of the combined stimuli are independent of the relative phase of the underlying grating components. The results also show that the discrimination thresholds are smaller for the combined stimuli than for the individual grating components, supporting the cue combination hypothesis. The proposed Bayesian model ﬁts the data well, accounting for the full psychometric functions of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization. Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for coherent motion stimuli of arbitrary spatial structure. 1</p><p>6 0.64084089 <a title="136-lsi-6" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>7 0.62452227 <a title="136-lsi-7" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>8 0.62335908 <a title="136-lsi-8" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>9 0.61202991 <a title="136-lsi-9" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>10 0.60732418 <a title="136-lsi-10" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>11 0.57725644 <a title="136-lsi-11" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>12 0.55695397 <a title="136-lsi-12" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>13 0.5559327 <a title="136-lsi-13" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>14 0.54277307 <a title="136-lsi-14" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>15 0.54184252 <a title="136-lsi-15" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>16 0.53789777 <a title="136-lsi-16" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>17 0.53438967 <a title="136-lsi-17" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>18 0.5074141 <a title="136-lsi-18" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>19 0.50460523 <a title="136-lsi-19" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>20 0.48437864 <a title="136-lsi-20" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.015), (16, 0.024), (25, 0.274), (33, 0.134), (34, 0.109), (41, 0.04), (49, 0.043), (56, 0.068), (70, 0.075), (85, 0.034), (89, 0.055), (93, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79708141 <a title="136-lda-1" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>2 0.67830002 <a title="136-lda-2" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>Author: Liu Yang, Jaime Carbonell</p><p>Abstract: In many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time. This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch. In this work, we study the label complexity of active learning algorithms that request labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufﬁcient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once. In particular, we ﬁnd that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increase the total number of labels requested, it reduces the total cost required for learning. 1</p><p>3 0.6066075 <a title="136-lda-3" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>4 0.60443264 <a title="136-lda-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.60162067 <a title="136-lda-5" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>6 0.60052085 <a title="136-lda-6" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>7 0.59968483 <a title="136-lda-7" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>8 0.59944433 <a title="136-lda-8" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>9 0.59936309 <a title="136-lda-9" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>10 0.59771913 <a title="136-lda-10" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>11 0.5972864 <a title="136-lda-11" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>12 0.596991 <a title="136-lda-12" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>13 0.59688789 <a title="136-lda-13" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>14 0.5959326 <a title="136-lda-14" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>15 0.59359527 <a title="136-lda-15" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>16 0.59283036 <a title="136-lda-16" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>17 0.59233272 <a title="136-lda-17" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>18 0.59159708 <a title="136-lda-18" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>19 0.59081972 <a title="136-lda-19" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>20 0.59072471 <a title="136-lda-20" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
