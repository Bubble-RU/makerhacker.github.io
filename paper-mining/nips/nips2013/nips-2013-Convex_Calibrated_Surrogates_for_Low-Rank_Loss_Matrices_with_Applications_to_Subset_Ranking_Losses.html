<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-72" href="#">nips2013-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</h1>
<br/><p>Source: <a title="nips-2013-72-pdf" href="http://papers.nips.cc/paper/4906-convex-calibrated-surrogates-for-low-rank-loss-matrices-with-applications-to-subset-ranking-losses.pdf">pdf</a></p><p>Author: Harish G. Ramaswamy, Shivani Agarwal, Ambuj Tewari</p><p>Abstract: The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement. 1</p><p>Reference: <a title="nips-2013-72-reference" href="../nips2013_reference/nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. [sent-9, score-0.751]
</p><p>2 We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement. [sent-11, score-0.981]
</p><p>3 For algorithms minimizing a surrogate loss, the question of consistency reduces to the question of calibration of the surrogate loss with respect to the target loss of interest [5–7, 16]; in general, one is interested in convex surrogates that can be minimized efﬁciently. [sent-13, score-1.226]
</p><p>4 In this paper, we develop a general result which allows us to give an explicit convex, calibrated surrogate deﬁned on a low-dimensional surrogate space for any ﬁnite-output learning problem for which the loss matrix has low rank. [sent-15, score-1.1]
</p><p>5 Working in the same general setting as theirs, we give an explicit construction that leads to a simple least-squares type surrogate. [sent-17, score-0.068]
</p><p>6 When there are r documents to be ranked for each query, the score-based surrogates operate on an r-dimensional surrogate space. [sent-20, score-0.628]
</p><p>7 We then turn to the MAP and PD losses, which are both widely used in practice, and for which it has been shown that no convex score-based surrogate can be calibrated for all probability distributions [11, 15, 16]. [sent-21, score-0.716]
</p><p>8 [11] gave certain low-noise conditions on the probability distribution under which a convex, calibrated score-based surrogate could be designed; 1  we are unaware of such a result for the MAP loss. [sent-23, score-0.716]
</p><p>9 A straightforward application of our low-rank result to these losses yields convex calibrated surrogates deﬁned on O(r2 )-dimensional surrogate spaces, but in both cases, the mapping needed to transform back to predictions in the original space involves solving a computationally hard problem. [sent-24, score-1.177]
</p><p>10 Inspired by these surrogates, we then give a convex score-based surrogate with an efﬁcient mapping that is calibrated with respect to MAP under certain conditions on the probability distribution; this is the ﬁrst such result for the MAP loss that we are aware of. [sent-25, score-0.918]
</p><p>11 We also give a family of convex score-based surrogates calibrated with the PD loss under certain noise conditions, generalizing the surrogate and conditions of Duchi et al. [sent-26, score-1.14]
</p><p>12 Finally, we give an efﬁcient mapping for the O(r2 )-dimensional surrogate for the PD loss, and show that this leads to a convex surrogate calibrated with the PD loss under a more general condition, i. [sent-28, score-1.175]
</p><p>13 Section 3 gives our primary result, namely an explicit convex surrogate calibrated for low-rank loss matrices, deﬁned on a surrogate space of dimension at most the rank of the matrix. [sent-33, score-1.156]
</p><p>14 , n}, and a ﬁnite set of target labels (possible predictions) T = [k] = {1, . [sent-41, score-0.061]
</p><p>15 Often, T = Y, but this is not always the case (for example, in the subset ranking problems we consider, the labels in Y are typically relevance vectors or preference graphs over a set of r documents, while the target labels in T are permutations over the r documents). [sent-52, score-0.289]
</p><p>16 The performance of a prediction model h : X →T is measured via a loss function � : Y × T →R+ (where R+ = [0, ∞)); here �(y, t) denotes the loss incurred on predicting t ∈ T when the label is y ∈ Y. [sent-53, score-0.199]
</p><p>17 Speciﬁcally, the goal is to learn a model h with low expected loss or �-error er� [h] = E(X,Y )∼D [�(Y, h(X))]; ideally, one D wants the �-error of the learned model to be close to the optimal �-error er�,∗ = inf h:X →T er� [h]. [sent-54, score-0.144]
</p><p>18 Predictions on new instances x ∈ X are then made by applying the learned model f and mapping back to predictions in the target space T via some mapping pred : Rd →T , giving h(x) = pred(f (x)). [sent-60, score-0.799]
</p><p>19 to converge in probability to the optimal ψ-error erψ,∗ = inf f :X →Rd erψ [f ]. [sent-63, score-0.051]
</p><p>20 A desirable property of ψ is that it be D D calibrated w. [sent-64, score-0.379]
</p><p>21 �; we give a formal deﬁnition of calibration and statement of this result below. [sent-73, score-0.066]
</p><p>22 We will ﬁnd it convenient to view the loss function � : Y ×T →R+ as an n × k matrix with elements �yt = �(y, t) for y ∈ [n], t ∈ [k], and column vectors �t = ¯ (�1t , . [sent-76, score-0.093]
</p><p>23 We will also represent the surrogate loss ψ : Y × Rd →R+ + d ¯ n with ψy (u) = ψ(y, u) for y ∈ [n], u ∈ Rd , and ψ(u) = as a vector function ψ : R →R+ ¯ (ψ1 (u), . [sent-80, score-0.382]
</p><p>24 A surrogate loss ψ : Y × Rd →R+ d is said to be calibrated w. [sent-86, score-0.761]
</p><p>25 � over P if there exists a function pred : R →T such that p� ψ(u) > inf p� ψ(u) . [sent-89, score-0.662]
</p><p>26 ∀p ∈ P : inf u∈Rd :pred(u)∈argmint p� �t /  1  P  P  u∈Rd  Here − denotes convergence in probability: Xm − a if ∀� > 0, P(|Xm − a| ≥ �) → 0 as m → ∞. [sent-90, score-0.051]
</p><p>27 � over Δn iff ∃ a function pred : Rd →T such that for all distributions D on X × Y and all sequences of random (vector) functions fm : X →Rd (depending on (X1 , Y1 ), . [sent-97, score-0.634]
</p><p>28 Then one can extend the above result to show that for P ⊂ Δn , ψ is calibrated w. [sent-105, score-0.379]
</p><p>29 � over P iff ∃ a function pred : Rd →T such that the above implication holds for all distributions D on X × Y for which p(x) ∈ P ∀x ∈ X . [sent-108, score-0.611]
</p><p>30 Subset ranking problems arise frequently in information retrieval applications. [sent-110, score-0.117]
</p><p>31 In a subset ranking problem, each instance in X consists of a query together with a set of say r documents to be ranked. [sent-111, score-0.193]
</p><p>32 The label space Y varies from problem to problem: in some cases, labels consist of binary or multi-level relevance judgements for the r documents, in which case Y = {0, 1}r or Y = {0, 1, . [sent-112, score-0.095]
</p><p>33 , s}r for some appropriate s ∈ Z+ ; in other cases, labels consist of pairwise preference graphs over the r documents, represented as (possibly weighted) directed acyclic graphs (DAGs) over r nodes. [sent-115, score-0.134]
</p><p>34 Given examples of such instance-label pairs, the goal is to learn a model to rank documents for new queries/instances; in most cases, the desired ranking takes the form of a permutation over the r documents, so that T = Sr (where Sr denotes the group of permutations on r objects). [sent-116, score-0.236]
</p><p>35 As noted earlier, various loss functions are used in practice, and there has been much interest in understanding questions of consistency and calibration for these losses in recent years [9–15, 17]. [sent-117, score-0.269]
</p><p>36 Below we will apply our result on calibrated surrogates for low-rank loss matrices to obtain new calibrated surrogates – both r-dimensional, score-based surrogates and, in some cases, higher-dimensional surrogates – for several subset ranking losses. [sent-119, score-2.08]
</p><p>37 The result gives an explicit construction for a convex, calibrated, least-squares type surrogate loss deﬁned on a low-dimensional surrogate space for any target loss matrix that has a low-rank structure. [sent-121, score-0.841]
</p><p>38 Let � : Y × T →R+ be a loss function such that there exist d ∈ Z+ , vectors α1 , . [sent-123, score-0.093]
</p><p>39 Also, if regret� (t) = 0 for all p p � � t ∈ [k], then trivially pred∗ (u) ∈ argmint p� �t ∀u ∈ Rd (and there is nothing to prove in this case). [sent-140, score-0.048]
</p><p>40 � = min p t∈[k]:regret� (t)>0 p Then we have inf  u∈Rd :pred∗ (u)∈argmint p� �t / �  p� ψ ∗ (u) �  = =  inf  u∈Rd :regret� (pred∗ (u))≥� p �  p� ψ ∗ (u) �  inf  u∈Rd :regret� (pred∗ (u))≥regret� (pred∗ (up ))+� p p � �  p� ψ ∗ (u) . [sent-142, score-0.153]
</p><p>41 �  Now, we claim that the mapping u �→ regret� (pred∗ (u)) is continuous at u = up . [sent-143, score-0.053]
</p><p>42 Then we have regret� (pred∗ (um )) � p  =  (up )� β pred∗ (um ) − min (up )� β t� � �  =  (u − um ) β pred∗ (um ) + u� β pred∗ (um ) − min (up )� β t� m � � �  =  p p  t ∈[k]  � �  (u − um ) β pred∗ (um ) + min � �  t ∈[k]  holds by deﬁnition of pred∗ . [sent-145, score-0.158]
</p><p>43 Thus regret� (pred∗ (um )) converges to p �  The last equality as um converges continuity at up . [sent-147, score-0.079]
</p><p>44 p p � � inf  u∈Rd :regret� (pred∗ (u))≥regret� (pred∗ (up ))+� p p � �  p� ψ ∗ (u) �  ≥ >  inf  u∈Rd :�u−up �≥δ  p� ψ ∗ (u) �  inf p� ψ ∗ (u) , �  u∈Rd  where the last inequality holds since p� ψ ∗ (u) is a strictly convex function of u and up is its unique � minimizer. [sent-149, score-0.201]
</p><p>45 The above sequence of inequalities give us that inf  u∈Rd :pred∗ (u)∈argmint p� �t / �  p� ψ ∗ (u) �  >  inf p� ψ ∗ (u) . [sent-150, score-0.126]
</p><p>46 �  We note that Ramaswamy and Agarwal [16] showed a similar least-squares type surrogate calibrated for any loss � : Y × T →R+ ; indeed our proof technique above draws inspiration from the proof technique there. [sent-152, score-0.761]
</p><p>47 However, the surrogate they gave was deﬁned on a surrogate space of dimension n − 1, where n is the number of class labels in Y. [sent-153, score-0.622]
</p><p>48 For example, as noted above, in the subset ranking problems we consider, the number of class labels is typically exponential in r, the number of documents associated with each query. [sent-155, score-0.237]
</p><p>49 On the other hand, as we will see below, many subset ranking losses have a low-rank structure, with rank linear or quadratic in r, allowing us to use the above result to design convex calibrated surrogates on an O(r) or O(r2 )-dimensional space. [sent-156, score-0.931]
</p><p>50 Our result above combines the beneﬁts of both these previous results, allowing explicit construction of low-dimensional least-squares type surrogates for any low-rank loss matrix. [sent-158, score-0.412]
</p><p>51 4  4  Calibrated Surrogates for Precision@q  The Precision@q is a popular performance measure for subset ranking problems in information retrieval. [sent-160, score-0.129]
</p><p>52 As noted above, in a subset ranking problem, each instance in X consists of a query together with a set of r documents to be ranked. [sent-161, score-0.209]
</p><p>53 For y ∈ {0, 1}r and σ ∈ Sr , where σ(i) denotes the position of document i under σ, the Precision@q loss for any integer q ∈ [r] can be written as follows: q  =  1� y −1 q i=1 σ (i)  1−  =  �P@q (y, σ)  1−  r  1� yi · 1(σ(i) ≤ q) . [sent-165, score-0.162]
</p><p>54 q i=1  ∗ ¯ Therefore, by Theorem 3, for the r-dimensional surrogate ψP@q : {0, 1}r × Rr →R+ and pred∗ : P@q r R →Sr deﬁned as r � ∗ ψP@q (y, u) = (ui − yi )2 i=1  pred∗ (u) P@q  argmaxσ∈Sr  ∈  r � i=1  ui · 1(σ(i) ≤ q) ,  ∗ we have that (ψP@q , pred∗ ) is �P@q -calibrated. [sent-166, score-0.387]
</p><p>55 It can easily be seen that for any u ∈ Rr , any P@q permutation σ which places the top q documents sorted in decreasing order of scores ui in the top q positions achieves the maximum in pred∗ (u); thus pred∗ (u) can be implemented efﬁciently P@q P@q using a standard sorting or selection algorithm. [sent-167, score-0.187]
</p><p>56 Note that the popular winner-take-all (WTA) loss, which assigns a loss of 0 if the top-ranked item is relevant (i. [sent-168, score-0.093]
</p><p>57 if yσ−1 (1) = 1) and 1 otherwise, is simply a special case of the above loss with q = 1; therefore the above construction also yields a calibrated surrogate for the WTA loss. [sent-170, score-0.779]
</p><p>58 To our knowledge, this is the ﬁrst example of convex, calibrated surrogates for the Precision@q and WTA losses. [sent-171, score-0.654]
</p><p>59 5  Calibrated Surrogates for Expected Rank Utility  The expected rank utility (ERU) is a popular subset ranking performance measure used in recommender systems displaying short ranked lists [18]. [sent-172, score-0.178]
</p><p>60 In this case the labels consist of multi-level relevance judgements (such as 0 to 5 stars), so that Y = {0, 1, . [sent-173, score-0.082]
</p><p>61 Thus, by Theorem 3, for the ∗ ¯ r-dimensional surrogate ψERU : {0, 1, . [sent-183, score-0.289]
</p><p>62 , s}r × Rr →R+ and pred∗ : Rr →Sr deﬁned as ERU r � ∗ ψERU (y, u) = (ui − max(yi − v, 0))2 i=1  pred∗ (u) ERU  ∈  argmaxσ∈Sr  r � i=1  ui · 2  1−σ(i) w−1  ,  ∗ we have that (ψERU , pred∗ ) is �ERU -calibrated. [sent-186, score-0.064]
</p><p>63 It can easily be seen that for any u ∈ Rr , any ERU permutation σ satisfying the condition  ui > uj ∗ predERU (u), and  =⇒ σ(i) < σ(j)  therefore pred∗ (u) can be implemented efﬁciently achieves the maximum in ERU by simply sorting the r documents in decreasing order of scores ui . [sent-187, score-0.259]
</p><p>64 As for Precision@q, to our knowledge, this is the ﬁrst example of a convex, calibrated surrogate for the ERU loss. [sent-188, score-0.668]
</p><p>65 5  6  Calibrated Surrogates for Mean Average Precision  The mean average precision (MAP) is a widely used ranking performance measure in information retrieval and related applications [15, 19]. [sent-189, score-0.167]
</p><p>66 For y ∈ {0, 1}r and σ ∈ Sr , where σ(i) denotes the position of document i under σ, the MAP loss is deﬁned as follows: σ(i) � 1 � 1 �MAP (y, σ) = 1 − y −1 . [sent-191, score-0.128]
</p><p>67 |{γ : yγ = 1}| i:y =1 σ(i) j=1 σ (j) i  It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the MAP loss [15]. [sent-192, score-0.747]
</p><p>68 We now re-write the MAP loss above in a manner that allows us to show the existence of an O(r2 )-dimensional convex, calibrated surrogate. [sent-193, score-0.489]
</p><p>69 uij ·  1 , max(σ(i), σ(j))  Note however that the optimization problem associated with computing pred∗ (u) above can be MAP written as a quadratic assignment problem (QAP), and most QAPs are known to be NP-hard. [sent-196, score-0.048]
</p><p>70 We conjecture that the QAP associated with the mapping pred∗ above is also NP-hard. [sent-197, score-0.053]
</p><p>71 Below we describe an alternate mapping in place of pred∗ which can be computed efﬁciently, and show MAP ∗ that under certain conditions on the probability distribution, the surrogate ψMAP together with this mapping is still calibrated for �MAP . [sent-199, score-0.806]
</p><p>72 Clearly, predMAP (u) can be implemented efﬁciently by simply sorting the ‘diagonal’ elements uii for i ∈ [r]. [sent-201, score-0.047]
</p><p>73 Also, let ΔY denote the probability simplex over Y, and for each p ∈ ΔY , deﬁne up ∈ Rr(r+1)/2 as follows: � � � � yi y j � Yi Yj py � r up = EY ∼p �r = ∀i, j ∈ [r] : i ≥ j . [sent-202, score-0.067]
</p><p>74 In fact, since the mapping predMAP depends on only the diagonal elements of u, we can equivalently deﬁne an r-dimensional surrogate that is calibrated w. [sent-208, score-0.721]
</p><p>75 Let ψMAP : {0, 1}r × Rr →R+ and predMAP : Rr →Sr be deﬁned as r �2 �� yi � � ui − � r � ψMAP (y, u) = γ=1 yγ i=1 � � � predMAP (� ) ∈ u σ ∈ Sr : ui > uj =⇒ σ(i) < σ(j) . [sent-213, score-0.185]
</p><p>76 To our knowledge, this is the ﬁrst example of conditions on the probability distribution under which a convex calibrated (and moreover, score-based) surrogate can be designed for the MAP loss. [sent-216, score-0.748]
</p><p>77 7  Calibrated Surrogates for Pairwise Disagreement  The pairwise disagreement (PD) loss is a natural and widely used loss in subset ranking [11, 17]. [sent-217, score-0.356]
</p><p>78 For y ∈ Y and σ ∈ Sr , where σ(i) denotes the position of document i under σ, the PD loss is deﬁned as follows: r �� � � yij 1 σ(i) > σ(j) . [sent-221, score-0.191]
</p><p>79 �PD (y, σ) = i=1 j�=i  It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the ∗ ¯ PD loss [15, 16]. [sent-222, score-0.747]
</p><p>80 1, we give a family of score-based (r-dimensional) surrogates that are calibrated with the PD loss under different conditions on the probability distribution; these surrogates and conditions generalize those of Duchi et al. [sent-228, score-1.11]
</p><p>81 1  Family of r-Dimensional Surrogates Calibrated with �PD Under Noise Conditions  The following gives a family of score-based surrogates, parameterized by functions f : Y→Rr , that are calibrated with the PD loss under different conditions on the probability distribution: 7  Theorem 6. [sent-236, score-0.504]
</p><p>82 Let ¯ ψf : Y × Rr →R+ , pred : Rr →Sr and Pf ⊂ ΔY be deﬁned as r �� �2 ψf (y, u) = ui − fi (y) � � i=1 pred(u) ∈ σ ∈ Sr : ui > uj =⇒ σ(i) < σ(j) � � Pf = p ∈ ΔY : EY ∼p [Yij ] > EY ∼p [Yji ] =⇒ EY ∼p [fi (Y )] > EY ∼p [fj (Y )] . [sent-238, score-0.776]
</p><p>83 [11] can be written in our notation as r r �� � ψDMJ (y, u) = yij (uj − ui ) + ν λ(ui ) , i=1 j�=i  i=1  where λ is a strictly convex and 1-coercive function and ν > 0. [sent-242, score-0.175]
</p><p>84 Taking λ(z) = z 2 and ν = 1 gives 2 a special case of the family of score-based surrogates in Theorem 6 above obtained by taking f as � (yij − yji ) . [sent-243, score-0.313]
</p><p>85 fi (y) = j�=i  Indeed, the set of noise conditions under which the surrogate ψDMJ is shown to be calibrated with the PD loss in Duchi et al. [sent-244, score-0.807]
</p><p>86 We also note that f can be viewed as a ‘standardization function’ [13] for the PD loss over Pf . [sent-246, score-0.093]
</p><p>87 2  An O(r2 )-dimensional Surrogate Calibrated with �PD Under More General Conditions  ∗ Consider now the r(r − 1)-dimensional surrogate ψPD : Y × Rr(r−1) deﬁned in Eq. [sent-248, score-0.289]
</p><p>88 We noted ∗ the corresponding mapping predPD involved an NP-hard optimization problem. [sent-250, score-0.069]
</p><p>89 Here we give an alternate mapping predPD : Rr(r−1) →Sr that can be computed efﬁciently, and show that under ∗ certain conditions on the probability distribution , the surrogate ψPD together with this mapping predPD is calibrated for �PD . [sent-251, score-0.83]
</p><p>90 The mapping predPD is described by Algorithm 1 below:  Algorithm 1 predPD (Input: u ∈ Rr(r−1) ; Output: Permutation σ ∈ Sr ) Construct a directed graph over [r] with edge (i, j) having weight (uij − uji )+ . [sent-252, score-0.053]
</p><p>91 If the graph has cycles, sort the edges in ascending order by weight and delete them one by one (smallest weight ﬁrst) until the graph becomes acyclic; return any topological sorted order of the resulting acyclic graph. [sent-254, score-0.06]
</p><p>92 It is easy to see that PDAG � Pf ∀f (where Pf is as deﬁned in Theorem 6), so that the above result yields a low-dimensional, convex surrogate with an efﬁciently computable pred mapping that is calibrated for the PD loss under a broader set of conditions than the previous surrogates. [sent-258, score-1.505]
</p><p>93 8  Conclusion  Calibration of surrogate losses is an important property in designing consistent learning algorithms. [sent-259, score-0.357]
</p><p>94 We have given an explicit method for constructing calibrated surrogates for any learning problem with a low-rank loss structure, and have used this to obtain several new results for subset ranking, including new calibrated surrogates for the Precision@q, ERU, MAP and PD losses. [sent-260, score-1.453]
</p><p>95 On the Bayes-risk consistency of regularized boosting a methods. [sent-266, score-0.05]
</p><p>96 Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. [sent-272, score-0.098]
</p><p>97 How to compare different loss functions and their risks. [sent-289, score-0.093]
</p><p>98 Learning scoring e e functions with order-preserving losses and standardized supervision. [sent-304, score-0.068]
</p><p>99 On the (non-)existence of convex, e e calibrated surrogate losses for ranking. [sent-310, score-0.736]
</p><p>100 Statistical consistency of ranking methods in a rank-differentiable probability space. [sent-319, score-0.153]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pred', 0.611), ('calibrated', 0.379), ('surrogate', 0.289), ('surrogates', 0.275), ('pd', 0.245), ('sr', 0.185), ('eru', 0.176), ('rr', 0.158), ('predmap', 0.129), ('ranking', 0.103), ('preinforce', 0.094), ('loss', 0.093), ('map', 0.084), ('predpd', 0.082), ('rd', 0.081), ('um', 0.079), ('losses', 0.068), ('regret', 0.066), ('documents', 0.064), ('ui', 0.064), ('yij', 0.063), ('ramaswamy', 0.057), ('pf', 0.057), ('mapping', 0.053), ('inf', 0.051), ('consistency', 0.05), ('precision', 0.05), ('ey', 0.049), ('uij', 0.048), ('argmint', 0.048), ('convex', 0.048), ('calibration', 0.042), ('yji', 0.038), ('er', 0.037), ('wta', 0.036), ('harish', 0.035), ('pdag', 0.035), ('standardization', 0.035), ('yi', 0.034), ('duchi', 0.034), ('target', 0.033), ('py', 0.033), ('conditions', 0.032), ('rank', 0.032), ('predictions', 0.031), ('shivani', 0.031), ('acyclic', 0.029), ('labels', 0.028), ('ambuj', 0.027), ('sorting', 0.026), ('subset', 0.026), ('explicit', 0.026), ('dags', 0.026), ('disagreement', 0.024), ('tewari', 0.024), ('multiclass', 0.024), ('give', 0.024), ('calauz', 0.023), ('dmj', 0.023), ('agarwal', 0.023), ('fm', 0.023), ('uj', 0.023), ('tong', 0.022), ('document', 0.022), ('relevance', 0.021), ('xm', 0.021), ('listwise', 0.021), ('qap', 0.021), ('uii', 0.021), ('hm', 0.02), ('nicolas', 0.02), ('ingo', 0.019), ('permutations', 0.019), ('construction', 0.018), ('permutation', 0.018), ('back', 0.018), ('judgements', 0.018), ('pairwise', 0.017), ('utility', 0.017), ('preference', 0.017), ('argmax', 0.017), ('existence', 0.017), ('transform', 0.016), ('ment', 0.016), ('gave', 0.016), ('topological', 0.016), ('noted', 0.016), ('sorted', 0.015), ('consist', 0.015), ('minimized', 0.014), ('cl', 0.014), ('jj', 0.014), ('graphs', 0.014), ('fi', 0.014), ('retrieval', 0.014), ('classi', 0.014), ('automation', 0.014), ('label', 0.013), ('ym', 0.013), ('position', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="72-tfidf-1" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>Author: Harish G. Ramaswamy, Shivani Agarwal, Ambuj Tewari</p><p>Abstract: The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement. 1</p><p>2 0.13457921 <a title="72-tfidf-2" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>Author: Julien Mairal</p><p>Abstract: Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with largescale or possibly inﬁnite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence √ rate of O(1/ n) after n iterations, and of O(1/n) for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efﬁcient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale ℓ1 logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems. 1</p><p>3 0.12420163 <a title="72-tfidf-3" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>Author: Nagarajan Natarajan, Inderjit Dhillon, Pradeep Ravikumar, Ambuj Tewari</p><p>Abstract: In this paper, we theoretically study the problem of binary classiﬁcation in the presence of random classiﬁcation noise — the learner, instead of seeing the true labels, sees labels that have independently been ﬂipped with some small probability. Moreover, random label noise is class-conditional — the ﬂip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisﬁes a simple symmetry condition, we show that the method leads to an efﬁcient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classiﬁcation with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence — methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88% accuracy even when 40% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.</p><p>4 0.11232115 <a title="72-tfidf-4" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>5 0.11091886 <a title="72-tfidf-5" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>Author: Harikrishna Narasimhan, Shivani Agarwal</p><p>Abstract: We investigate the relationship between three fundamental problems in machine learning: binary classiﬁcation, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classiﬁcation model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classiﬁcation model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classiﬁcation model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model). 1</p><p>6 0.105696 <a title="72-tfidf-6" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>7 0.066281766 <a title="72-tfidf-7" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>8 0.059458733 <a title="72-tfidf-8" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>9 0.058943778 <a title="72-tfidf-9" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>10 0.053210236 <a title="72-tfidf-10" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>11 0.052023128 <a title="72-tfidf-11" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<p>12 0.049995672 <a title="72-tfidf-12" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>13 0.047522601 <a title="72-tfidf-13" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>14 0.047418728 <a title="72-tfidf-14" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>15 0.044927388 <a title="72-tfidf-15" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>16 0.04454302 <a title="72-tfidf-16" href="./nips-2013-Blind_Calibration_in_Compressed_Sensing_using_Message_Passing_Algorithms.html">59 nips-2013-Blind Calibration in Compressed Sensing using Message Passing Algorithms</a></p>
<p>17 0.044237308 <a title="72-tfidf-17" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>18 0.042104602 <a title="72-tfidf-18" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>19 0.040944461 <a title="72-tfidf-19" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>20 0.04009961 <a title="72-tfidf-20" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.006), (2, 0.059), (3, -0.013), (4, 0.046), (5, 0.011), (6, -0.027), (7, -0.028), (8, -0.031), (9, 0.023), (10, -0.001), (11, 0.014), (12, -0.014), (13, -0.005), (14, 0.035), (15, -0.08), (16, 0.008), (17, 0.072), (18, 0.03), (19, 0.008), (20, -0.084), (21, 0.018), (22, 0.071), (23, 0.059), (24, -0.041), (25, -0.006), (26, -0.038), (27, 0.006), (28, 0.098), (29, -0.002), (30, -0.111), (31, 0.038), (32, -0.061), (33, -0.016), (34, -0.037), (35, -0.12), (36, 0.042), (37, -0.045), (38, 0.046), (39, 0.04), (40, 0.066), (41, -0.071), (42, -0.03), (43, -0.121), (44, 0.032), (45, 0.068), (46, -0.131), (47, -0.042), (48, -0.013), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93258721 <a title="72-lsi-1" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>Author: Harish G. Ramaswamy, Shivani Agarwal, Ambuj Tewari</p><p>Abstract: The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement. 1</p><p>2 0.58249503 <a title="72-lsi-2" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>Author: Nagarajan Natarajan, Inderjit Dhillon, Pradeep Ravikumar, Ambuj Tewari</p><p>Abstract: In this paper, we theoretically study the problem of binary classiﬁcation in the presence of random classiﬁcation noise — the learner, instead of seeing the true labels, sees labels that have independently been ﬂipped with some small probability. Moreover, random label noise is class-conditional — the ﬂip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisﬁes a simple symmetry condition, we show that the method leads to an efﬁcient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classiﬁcation with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence — methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88% accuracy even when 40% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.</p><p>3 0.56189692 <a title="72-lsi-3" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>4 0.55433422 <a title="72-lsi-4" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>Author: Xiaojin Zhu</p><p>Abstract: What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for ﬁnding the optimal teaching set. Our algorithm optimizes the aggregate sufﬁcient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework. 1</p><p>5 0.55290902 <a title="72-lsi-5" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>Author: Harikrishna Narasimhan, Shivani Agarwal</p><p>Abstract: We investigate the relationship between three fundamental problems in machine learning: binary classiﬁcation, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classiﬁcation model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classiﬁcation model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classiﬁcation model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model). 1</p><p>6 0.51180387 <a title="72-lsi-6" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>7 0.47892913 <a title="72-lsi-7" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>8 0.47790051 <a title="72-lsi-8" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>9 0.47665223 <a title="72-lsi-9" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>10 0.4747673 <a title="72-lsi-10" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>11 0.46090555 <a title="72-lsi-11" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>12 0.42462811 <a title="72-lsi-12" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>13 0.41982737 <a title="72-lsi-13" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>14 0.41797179 <a title="72-lsi-14" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>15 0.39325026 <a title="72-lsi-15" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>16 0.38952202 <a title="72-lsi-16" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>17 0.38709715 <a title="72-lsi-17" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>18 0.38248187 <a title="72-lsi-18" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>19 0.38079584 <a title="72-lsi-19" href="./nips-2013-Blind_Calibration_in_Compressed_Sensing_using_Message_Passing_Algorithms.html">59 nips-2013-Blind Calibration in Compressed Sensing using Message Passing Algorithms</a></p>
<p>20 0.37732163 <a title="72-lsi-20" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.024), (16, 0.011), (19, 0.388), (33, 0.099), (34, 0.091), (41, 0.021), (49, 0.036), (56, 0.081), (70, 0.022), (85, 0.027), (89, 0.026), (93, 0.054), (95, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69769818 <a title="72-lda-1" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>Author: Harish G. Ramaswamy, Shivani Agarwal, Ambuj Tewari</p><p>Abstract: The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement. 1</p><p>2 0.67879272 <a title="72-lda-2" href="./nips-2013-A_Kernel_Test_for_Three-Variable_Interactions.html">9 nips-2013-A Kernel Test for Three-Variable Interactions</a></p>
<p>Author: Dino Sejdinovic, Arthur Gretton, Wicher Bergsma</p><p>Abstract: We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak inﬂuence on a third dependent variable, but their combined effect has a strong inﬂuence. This makes the Lancaster test especially suited to ﬁnding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.</p><p>3 0.66037095 <a title="72-lda-3" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>Author: Mohammad Amin Sadeghi, David Forsyth</p><p>Abstract: Applying linear templates is an integral part of many object detection systems and accounts for a signiﬁcant portion of computation time. We describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy. Our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade. Our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of Vector Quantization levels, and by choosing to rescore windows or not. Our method can be directly plugged into any recognition system that relies on linear templates. We demonstrate our method to speed up the original Exemplar SVM detector [1] by an order of magnitude and Deformable Part models [2] by two orders of magnitude with no loss of accuracy. 1</p><p>4 0.51177311 <a title="72-lda-4" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>Author: Kevin Swersky, Jasper Snoek, Ryan P. Adams</p><p>Abstract: Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost. 1</p><p>5 0.48959261 <a title="72-lda-5" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical signiﬁcance as conﬁdence intervals or p-values. We consider here a broad class of regression problems, and propose an efﬁcient algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁdence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a ‘de-biased’ version of regularized Mestimators. The new construction improves over recent work in the ﬁeld in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem. 1</p><p>6 0.45741275 <a title="72-lda-6" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>7 0.4385789 <a title="72-lda-7" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>8 0.43820786 <a title="72-lda-8" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>9 0.43021071 <a title="72-lda-9" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>10 0.42861456 <a title="72-lda-10" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>11 0.42633289 <a title="72-lda-11" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>12 0.41139778 <a title="72-lda-12" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>13 0.40937641 <a title="72-lda-13" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>14 0.40914744 <a title="72-lda-14" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>15 0.40700853 <a title="72-lda-15" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>16 0.4049826 <a title="72-lda-16" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>17 0.40291199 <a title="72-lda-17" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>18 0.40263897 <a title="72-lda-18" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>19 0.40185386 <a title="72-lda-19" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>20 0.40171257 <a title="72-lda-20" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
