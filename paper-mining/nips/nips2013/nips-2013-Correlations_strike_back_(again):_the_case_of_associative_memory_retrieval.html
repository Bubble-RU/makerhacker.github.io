<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-77" href="#">nips2013-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</h1>
<br/><p>Source: <a title="nips-2013-77-pdf" href="http://papers.nips.cc/paper/4871-correlations-strike-back-again-the-case-of-associative-memory-retrieval.pdf">pdf</a></p><p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>Reference: <a title="nips-2013-77-reference" href="../nips2013_reference/nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Engineering, University of Cambridge, UK 2 Gatsby Computational Neuroscience Unit, University College London, UK  Abstract It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. [sent-11, score-0.343]
</p><p>2 Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. [sent-12, score-0.827]
</p><p>3 We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. [sent-13, score-0.41]
</p><p>4 We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. [sent-14, score-1.817]
</p><p>5 These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. [sent-15, score-0.488]
</p><p>6 We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. [sent-16, score-0.868]
</p><p>7 The idea is to see memory storage as a form of lossy compression – information on the item being stored is mapped into a set of synaptic changes – with the neural dynamics during retrieval representing a biological analog of a corresponding decompression algorithm. [sent-19, score-1.133]
</p><p>8 This implies there should be a tight, and indeed testable, link between the learning rule used for encoding and the neural dynamics used for retrieval [2]. [sent-20, score-0.482]
</p><p>9 One issue that has been either ignored or trivialized in these treatments of recall is correlations among the synapses [1–4] – beyond the perfect (anti-)correlations emerging between reciprocal synapses with precisely (anti-)symmetric learning rules [5]. [sent-21, score-1.223]
</p><p>10 The study of neural coding has indicated that it is essential to treat correlations in neural activity appropriately in order to extract stimulus information well [7– 9]. [sent-23, score-0.505]
</p><p>11 Similarly, it becomes pressing to examine the nature of correlations among synaptic weights in auto-associative memories, the consequences for retrieval of ignoring them, and methods by which they might be accommodated. [sent-24, score-1.012]
</p><p>12 1  Here, we consider several well-known learning rules, from simple additive ones to bounded synapses with metaplasticity, and show that, with a few signiﬁcant exceptions, they induce correlations between synapses that share a pre- or a post-synaptic partner. [sent-25, score-1.009]
</p><p>13 1 As is conventional, and indeed plausibly underpinned by neuromodulatory interactions [12], we assume that network dynamics do not play a role during storage (with stimuli being imposed as patterns of activity on the neurons), and that learning does not occur during retrieval. [sent-29, score-0.677]
</p><p>14 To isolate the effects of different plasticity rules on synaptic correlations from other sources of correlations, we assume that the patterns of activity inducing the synaptic changes have no particular structure, i. [sent-30, score-2.021]
</p><p>15 For further simplicity, we take these activity patterns to be binary with pattern density f , i. [sent-33, score-0.356]
</p><p>16 a prior over patterns deﬁned as: Pstore (x)  =  i  Pstore (xi ) = f xi · (1 − f )1−xi  Pstore (xi )  (1)  During recall, the network is presented with a cue, x, which is a noisy or partial version of one ˜ of the originally stored patterns. [sent-35, score-0.335]
</p><p>17 the activity of neuron i should depend exclusively on inputs through incoming synapses Wi,· . [sent-39, score-0.674]
</p><p>18 Since information storage by synaptic plasticity is lossy, recall is inherently a probabilistic inference problem [1, 13] (Fig. [sent-40, score-0.769]
</p><p>19 In this paper, we focus on the last term P(W|x), which expresses the probability of obtaining W as the synaptic weight matrix when x is stored along with T − 1 random patterns (sampled from the prior, Eq. [sent-42, score-0.79]
</p><p>20 Critically, this is where we diverge from previous analyses that assumed this distribution was factorised, or only trivially correlated due to reciprocal synapses being precisely (anti-)symmetric [1, 2, 4]. [sent-44, score-0.44]
</p><p>21 In contrast, we explicitly study the emergence and effects of non-trivial correlations in the synaptic weight matrixdistribtion, because almost all synaptic plasticity rules induce statistical dependencies between the synaptic weights of each neuron (Fig. [sent-45, score-2.48]
</p><p>22 We start by analysing the class of additive learning rules, to get a sense for the effect of correlations on retrieval. [sent-51, score-0.339]
</p><p>23 These have been used to capture a variety of important biological constraints such as bounds on synaptic strengths and metaplasticity, i. [sent-53, score-0.573]
</p><p>24 the fact that synaptic changes induced by a certain activity pattern depend on the history of activity at the synapse [15]. [sent-55, score-1.019]
</p><p>25 The two classes of learning rule are radically different; so if synaptic correlations matter during retrieval in both cases, then the conclusion likely applies in general. [sent-56, score-1.038]
</p><p>26 1 Complete connectivity simpliﬁes the computation of the parameters for the optimal dynamics for cascadelike learning rules considered in the following, but is not necessary for the theory. [sent-57, score-0.383]
</p><p>27 2  1  covariance rule simple Hebb rule cortical data (Song 2005)  0. [sent-58, score-0.322]
</p><p>28 Bottom: The activity of neuron i across the stored patterns is ˜ a source of shared variability between synapses connecting it to neurons j and k. [sent-68, score-0.89]
</p><p>29 Covariance rule: patterns of synaptic correlations and recall performance for retrieval dynamics ignoring or considering synaptic correlations; T = 5. [sent-70, score-1.927]
</p><p>30 As synaptic changes are deterministic, the only source of uncertainty in the distribution P(W|x) is the identity of the other stored patterns. [sent-77, score-0.664]
</p><p>31 Hence, the distribution over possible weight values at recall, given that pattern x is stored along with T − 1 other, random, patterns has mean µW = Ω(x) + (T − 1) · µ, and covariance CW = (T − 1) · C. [sent-80, score-0.356]
</p><p>32 Most importantly, because the rule is additive, in the limit of many stored patterns (and in practice even for modest values of T ), the distribution P(W|x) approaches a multivariate Gaussian that is characterized completely by these two quantities; moreover, its covariance is independent of x. [sent-81, score-0.401]
</p><p>33 For retrieval dynamics based on Gibbs sampling, the key quantity is the log-odds ratio P(xi = 1|x¬i , W, x) ˜ (4) Ii = log P(xi = 0|x¬i , W, x) ˜ for neuron i, which could be represented by the total current entering the unit. [sent-82, score-0.535]
</p><p>34 It is easy to see that for the covariance rule, Ω (xi , xj ) = (xi − f )(xj − f ), synapses sharing a single pre- or post-synaptic partner happen to be uncorrelated (Fig. [sent-86, score-0.611]
</p><p>35 Reassuringly, the optimal decoder for the covariance rule recovers a form for the input current that is closely related to classic Hopﬁeld-like [5] dynamics (with external ﬁeld [1, 18]): feedback inhibition is needed only when the stored patterns are not balanced (f = 0. [sent-91, score-0.857]
</p><p>36 In sum, for the covariance rule, synapses are fortuitously uncorrelated (except for symmetric pairs which are perfectly correlated), and thus simple, classical linear recall dynamics sufﬁce (Fig. [sent-93, score-0.709]
</p><p>37 For example, for simple Hebbian learning, Ω (xi , xj ) = xi ·xj , synapses sharing a pre- or post-synaptic partner are correlated (Fig. [sent-96, score-0.665]
</p><p>38 6, but feedback inhibition becomes a non-linear function of the total activity in the network [16]. [sent-99, score-0.371]
</p><p>39 For the generalized Hebbian case, Ω (xi , xj ) = (xi −α)(xj −β) with α = β, the optimal decoder becomes even more complex, with the total current including additional terms accounting for pairwise correlations between any two synapses that have neuron i as a pre- or post-synaptic partner [16]. [sent-102, score-0.999]
</p><p>40 4  Palimpsest learning rules  Though additive learning rules are attractive for their analytical tractability, they ignore several important aspects of synaptic plasticity, e. [sent-104, score-0.931]
</p><p>41 We investigate the effects of bounded weights by considering another class of learning rules, which assumes synaptic efﬁcacies can only take binary values, with stochastic transitions between the two underpinned by paired cascades of latent internal states [14] (Fig. [sent-107, score-0.679]
</p><p>42 Additionally, such rules can account for experimentally observed synaptic metaplasticity [15]. [sent-110, score-0.842]
</p><p>43 3 For additive learning rules, the current to neuron i always depends only on synapses local to a neuron, but these can also include outgoing synapses of which the weight, W·i , should not inﬂuence its dynamics. [sent-111, score-0.888]
</p><p>44 4  cortex data (Song 2005)  d  correlated synapses  20 10  0. [sent-115, score-0.375]
</p><p>45 3  *  10  0  exact approx  0  simple dynamics  *  corr-dependent dynamics  Figure 2: Palimpsest learning. [sent-120, score-0.452]
</p><p>46 Colored circles are latent states (V ) that belong to two different synaptic weights (W ), arrows are state transitions (blue: depression, red: potentiation) b. [sent-123, score-0.626]
</p><p>47 Learning rule Learning is stochastic and local, with changes in the state of a synapse Vij being determined only by the activation of the pre- and post-synaptic neurons, xj and xi . [sent-129, score-0.378]
</p><p>48 In general, one could deﬁne separate transition matrices for each activity pattern, M(xi , xj ), describing the probability of a synaptic state transitioning between any two states Vij to Vij following an activity pattern, (xi , xj ). [sent-130, score-1.187]
</p><p>49 22, was speciﬁcally designed to eliminate correlations between synapses, and can be viewed as a version of the classic covariance rule fashioned for binary synapses. [sent-135, score-0.449]
</p><p>50 As in the additive case, the states of synapses sharing a pre- or post- synaptic partner will be correlated (Figs. [sent-143, score-1.117]
</p><p>51 The degree of correlations for different synaptic conﬁgurations can be estimated by generalising the above procedure to computing the joint distribution of the states of pairs of synapses, which we represent as a matrix ρ. [sent-145, score-0.841]
</p><p>52 for a pair of synapses sharing a postsynaptic partner (Figs. [sent-148, score-0.507]
</p><p>53 Hence, the presentation of an activity pattern (xpre1 , xpre2 , xpost ) induces changes in the corresponding pair of 4  Other models, e. [sent-150, score-0.32]
</p><p>54 5  5  incoming synapses to neuron post as ρ(1) = M(xpost , xpre1 ) · ρ(0) · M(xpost , xpre2 )T , where ρ(0) is the stationary distribution corresponding to storing an inﬁnite number of triplets from the pattern distribution [16]. [sent-154, score-0.55]
</p><p>55 Replacing π V with ρ (which is now a function of the triplet (xpre1 , xpre2 , xpost )), and the multiplication by M with the slightly more complicated operator above, we can estimate the evolution of the joint distribution over synaptic states in a manner very similar to Eq. [sent-155, score-0.637]
</p><p>56 7: ρ(t) =  xi  ˆ ˆ Pstore (xi ) · M(xi ) · ρ(t−1) · M(xi )T ,  (8)  ˆ where M(xi ) = xj Pstore (xj )M(xi , xj ). [sent-156, score-0.319]
</p><p>57 Also as above, the ﬁnal joint distribution over states can be mapped into a joint distribution over synaptic weights as MV →W · ρ(t) · MT →W . [sent-157, score-0.626]
</p><p>58 This V approach can be naturally extended to all other correlated pairs of synapses [16]. [sent-158, score-0.375]
</p><p>59 The structure of correlations for different synaptic pairs varies signiﬁcantly as a function of the learning rule (Fig. [sent-159, score-0.922]
</p><p>60 The ﬁrst two variants of the learning rule considered are not symmetric, and so induce different patterns of correlations than the additive rules above. [sent-162, score-0.766]
</p><p>61 The XOR rule is similar to the covariance rule, but the reciprocal connections are no longer perfectly correlated (due to metaplasticity), which means that it is no longer possible to factorize P(W|x). [sent-163, score-0.366]
</p><p>62 Approximately optimal retrieval when synapses are independent If we ignore synaptic correlations, the evidence from the weights factorizes, P(W|x) = 3 i,j P(Wij |xi , xj ), and so the exact dynamics would be semi-local . [sent-165, score-1.383]
</p><p>63 We can further approximate the contribution of the outgoing weights by its mean, which recovers the same simple dynamics derived for the additive case: P(xi = 1|x¬i , W, x) ˜ Ii = log = c1 Wij xj + c2 Wij + c3 xj + c4 xi + c5 (9) ˜ j j j P(xi = 0|x¬i , W, x) ˜ The parameters c. [sent-166, score-0.669]
</p><p>64 Again, the optimal decoder is similar to previously derived attractor dynamics; in particular, for stochastic binary synapses with presynaptically gated learning the optimal dynamics require dynamic inhibition only for sparse patterns, and no homeostatic term, as used in [21] . [sent-168, score-0.939]
</p><p>65 To validate these dynamics, we remove synaptic correlations by a pseudo-storage procedure in which synapses are allowed to evolve independently according to transition matrix M, rather than changing as actual intermediate patterns are stored. [sent-169, score-1.267]
</p><p>66 Hence, ignoring correlations is highly detrimental for this class of learning rules too. [sent-175, score-0.458]
</p><p>67 Approximately optimal retrieval when synapses are correlated To accommodate synaptic correlations, we approximate P(W|x) with a maximum entropy distribution with the same marginals and covariance structure, ignoring the higher order moments. [sent-176, score-1.141]
</p><p>68 6 This is just a generalisation of the simple dynamics which assume a ﬁrst order max entropy model; moreover, the resulting weight distribution is a binary analog of the multivariate normal used in the additive case, allowing the two to be directly compared. [sent-178, score-0.327]
</p><p>69 7 Here, we ask whether it is possible to accommodate correlations in appropriate neural dynamics at all, ignoring the issue of how the optimal values for the parameters of the network dynamics would come about. [sent-179, score-0.825]
</p><p>70 10, but not respecting locality constraints, work substantially better in the presence of synaptic correlations, for all rules (Fig. [sent-206, score-0.701]
</p><p>71 As in the additive case, exact recall dynamics are biologically implausible, as the total current to the neuron depends on the full weight matrix. [sent-209, score-0.583]
</p><p>72 It is possible to approximate the dynamics using strictly local information by replacing the nonlocal term by its mean, which, however, is no longer a constant, but rather a linear function of the total activity in the network, nb = j=i xj [16]. [sent-210, score-0.631]
</p><p>73 The functions kij (x) = kij (x(1) )−kij (x(0) ), J(ij)(kl) (x) = J(ij)(kl) x(1) −J(ij)(kl) x(0) , and Z = log Z x(1) − log Z x(0) depend on the local activity at the indexed synapses, modulated by the number of active neurons in the network, nb . [sent-212, score-0.33]
</p><p>74 in the absence of synaptic correlations, the complex dynamics recover the simple case presented before. [sent-215, score-0.77]
</p><p>75 For post-synaptically gated learning, comparing the parameters of the dynamics in the case of independent versus correlated synapses (Fig. [sent-218, score-0.738]
</p><p>76 More importantly, the net current to the postsynaptic neuron depends non-linearly (formally, quadratically) on the number of co-active inputs, nW 1 = j xj Wij , (Fig. [sent-220, score-0.382]
</p><p>77 Conversely, for the presynaptically gated learning rule, approximately optimal dynamics predict a non-monotonic modulation of activity by lateral inhibition (Fig. [sent-223, score-0.737]
</p><p>78 8 Lastly, retrieval based on the XOR rule has the same form as the simple dynamics derived for the factorized case [16]. [sent-226, score-0.454]
</p><p>79 However, the total current has to be rescaled to compensate for the correlations introduced by reciprocal connections. [sent-227, score-0.368]
</p><p>80 8 The difference between the two rules emerges exclusively because of the constraint of strict locality of the approximation, since the exact form of the dynamics is essentially the same for the two. [sent-228, score-0.383]
</p><p>81 gated XOR  EXACT DYNAMICS strictly local, linear strictly local, nonlinear semi-local, nonlinear nonlocal, nonlinear nonlocal, nonlinear beyond correlations  NEURAL IMPLEMENTATION linear feedback inh. [sent-231, score-0.671]
</p><p>82 5  Discussion  Statistical dependencies between synaptic efﬁcacies are a natural consequence of activity dependent synaptic plasticity, and yet their implications for network function have been unexplored. [sent-240, score-1.348]
</p><p>83 Here, in the context of an auto-associative memory network, we investigated the patterns of synaptic correlations induced by several well-known learning rules and their consequent effects on retrieval. [sent-241, score-1.165]
</p><p>84 We showed that most rules considered do indeed induce synaptic correlations and that failing to take them into account greatly damages recall. [sent-242, score-1.021]
</p><p>85 One fortuitous exception is the covariance rule, for which there are no synaptic correlations. [sent-243, score-0.615]
</p><p>86 This might explain why the bulk of classical treatments of autoassociative memories, using the covariance rule, could achieve satisfying capacity levels despite overlooking the issue of synaptic correlations [5, 24, 25]. [sent-244, score-0.976]
</p><p>87 In general, taking correlations into account optimally during recall requires dynamics in which there are non-local interactions between neurons. [sent-245, score-0.584]
</p><p>88 Examples include the modulation of neural responses by the total activity of the population, which could be mediated by feedback inhibition, and speciﬁc dendritic nonlinearities. [sent-247, score-0.386]
</p><p>89 In general, the tight coupling between the synaptic plasticity used for encoding (manifested in patterns of synaptic correlations) and circuit dynamics offers an important route for experimental validation [2]. [sent-249, score-1.606]
</p><p>90 None of the rules governing synaptic plasticity that we considered perfectly reproduced the pattern of correlations in [6]; and indeed, exactly which rule applies in what region of the brain under which neuromodulatory inﬂuences is unclear. [sent-250, score-1.307]
</p><p>91 Nonetheless, our analysis has shown that synaptic correlations matter for a range of very different learning rules that span the spectrum of empirical observations. [sent-252, score-0.967]
</p><p>92 Another strategy to handle the negative effects of synaptic correlations is to weaken or eliminate them. [sent-253, score-0.836]
</p><p>93 For instance, in the palimpsest synaptic model [14], the deeper the cascade, the weaker the correlations, and so metaplasticity may have the beneﬁcial effect of making recall easier. [sent-254, score-0.781]
</p><p>94 More speculatively, one might imagine a process of off-line synaptic pruning or recoding, in which strong correlations are removed or the weights adjusted so that simple recall methods will work. [sent-256, score-0.923]
</p><p>95 Finally, we know from work on neural coding of sensory stimuli that there are regimes in which correlations either help or hurt the informational quality of the code, assuming that decoding takes them into account. [sent-260, score-0.329]
</p><p>96 Given our results, it becomes important to look at the relative quality of different plasticity rules, assuming realizable decoding – it is not clear whether rules that strive to eliminate correlations will be bested by ones that do not. [sent-261, score-0.589]
</p><p>97 Highly nonrandom features of synaptic o o connectivity in local cortical circuits. [sent-305, score-0.571]
</p><p>98 Sparseness constrains the prolongation of memory lifetime via synaptic metaplasticity. [sent-384, score-0.582]
</p><p>99 Capacity analysis in multi-state synaptic models: a retrieval probability perspective. [sent-396, score-0.66]
</p><p>100 Long memory lifetimes require complex synapses and limited sparseness. [sent-401, score-0.361]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('synaptic', 0.544), ('synapses', 0.323), ('correlations', 0.266), ('dynamics', 0.226), ('pstore', 0.202), ('activity', 0.183), ('rules', 0.157), ('gated', 0.137), ('patterns', 0.134), ('neuron', 0.132), ('plasticity', 0.131), ('xj', 0.123), ('retrieval', 0.116), ('rule', 0.112), ('wij', 0.106), ('palimpsest', 0.093), ('postsynaptic', 0.09), ('stored', 0.084), ('metaplasticity', 0.082), ('inhibition', 0.08), ('dayan', 0.076), ('additive', 0.073), ('xi', 0.073), ('hebbian', 0.073), ('xor', 0.071), ('covariance', 0.071), ('partner', 0.069), ('reciprocal', 0.065), ('feedback', 0.064), ('dendritic', 0.062), ('homeostatic', 0.062), ('presynaptically', 0.062), ('xpost', 0.062), ('recall', 0.062), ('depression', 0.059), ('potentiation', 0.059), ('recurrent', 0.059), ('memories', 0.057), ('storing', 0.056), ('vij', 0.055), ('cue', 0.052), ('correlated', 0.052), ('weights', 0.051), ('cascade', 0.049), ('modulation', 0.049), ('decoder', 0.049), ('neuroscience', 0.045), ('network', 0.044), ('rec', 0.041), ('autoassociative', 0.041), ('lengyel', 0.041), ('connections', 0.039), ('pattern', 0.039), ('memory', 0.038), ('kij', 0.038), ('nonlinear', 0.038), ('current', 0.037), ('nb', 0.037), ('inputs', 0.036), ('changes', 0.036), ('fusi', 0.036), ('nonlocal', 0.036), ('ignoring', 0.035), ('decoding', 0.035), ('neurons', 0.034), ('hippocampal', 0.034), ('synapse', 0.034), ('neuronal', 0.034), ('dependencies', 0.033), ('storage', 0.032), ('cacies', 0.031), ('neuromodulatory', 0.031), ('postsynaptically', 0.031), ('states', 0.031), ('account', 0.03), ('experimentally', 0.029), ('hop', 0.029), ('mv', 0.029), ('biological', 0.029), ('ij', 0.029), ('weight', 0.028), ('neural', 0.028), ('excitability', 0.027), ('treatments', 0.027), ('underpinned', 0.027), ('capacity', 0.027), ('cortical', 0.027), ('circuit', 0.027), ('perfectly', 0.027), ('strictly', 0.026), ('integration', 0.026), ('effects', 0.026), ('amit', 0.026), ('coactive', 0.025), ('biologically', 0.025), ('sharing', 0.025), ('induce', 0.024), ('motifs', 0.024), ('entering', 0.024), ('latham', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="77-tfidf-1" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>2 0.48322684 <a title="77-tfidf-2" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>Author: Subhaneil Lahiri, Surya Ganguli</p><p>Abstract: An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming ﬁnite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on ﬁrst passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function. 1</p><p>3 0.30132973 <a title="77-tfidf-3" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Christian Albers, Maren Westkott, Klaus Pawelzik</p><p>Abstract: Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</p><p>4 0.25557739 <a title="77-tfidf-4" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>5 0.20235211 <a title="77-tfidf-5" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>6 0.18220909 <a title="77-tfidf-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.17312349 <a title="77-tfidf-7" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>8 0.15615876 <a title="77-tfidf-8" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>9 0.13656227 <a title="77-tfidf-9" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>10 0.12422514 <a title="77-tfidf-10" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>11 0.12294561 <a title="77-tfidf-11" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>12 0.09582562 <a title="77-tfidf-12" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>13 0.08072342 <a title="77-tfidf-13" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>14 0.075218618 <a title="77-tfidf-14" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>15 0.073957339 <a title="77-tfidf-15" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>16 0.066211239 <a title="77-tfidf-16" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>17 0.062466852 <a title="77-tfidf-17" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>18 0.059761114 <a title="77-tfidf-18" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>19 0.054279484 <a title="77-tfidf-19" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>20 0.052859511 <a title="77-tfidf-20" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, 0.082), (2, -0.121), (3, -0.088), (4, -0.341), (5, -0.026), (6, -0.027), (7, -0.106), (8, 0.039), (9, 0.031), (10, 0.1), (11, -0.011), (12, 0.127), (13, 0.03), (14, -0.006), (15, -0.013), (16, -0.069), (17, 0.071), (18, 0.045), (19, -0.062), (20, 0.034), (21, -0.088), (22, 0.096), (23, 0.405), (24, -0.051), (25, 0.261), (26, 0.142), (27, -0.043), (28, -0.164), (29, 0.014), (30, 0.138), (31, 0.059), (32, 0.073), (33, -0.02), (34, 0.006), (35, 0.005), (36, 0.01), (37, 0.049), (38, -0.036), (39, 0.092), (40, 0.088), (41, -0.048), (42, -0.047), (43, -0.157), (44, 0.009), (45, 0.047), (46, 0.031), (47, 0.044), (48, -0.021), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97100121 <a title="77-lsi-1" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>2 0.94362628 <a title="77-lsi-2" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>Author: Subhaneil Lahiri, Surya Ganguli</p><p>Abstract: An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming ﬁnite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on ﬁrst passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function. 1</p><p>3 0.82014412 <a title="77-lsi-3" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Christian Albers, Maren Westkott, Klaus Pawelzik</p><p>Abstract: Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</p><p>4 0.69553781 <a title="77-lsi-4" href="./nips-2013-Capacity_of_strong_attractor_patterns_to_model_behavioural_and_cognitive_prototypes.html">61 nips-2013-Capacity of strong attractor patterns to model behavioural and cognitive prototypes</a></p>
<p>Author: Abbas Edalat</p><p>Abstract: We solve the mean ﬁeld equations for a stochastic Hopﬁeld network with temperature (noise) in the presence of strong, i.e., multiply stored, patterns, and use this solution to obtain the storage capacity of such a network. Our result provides for the ﬁrst time a rigorous solution of the mean ﬁled equations for the standard Hopﬁeld model and is in contrast to the mathematically unjustiﬁable replica technique that has been used hitherto for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when the sum of the squares of degrees of the patterns is negligible compared to the network size. In the case of a single strong pattern, when the ratio of the number of all stored pattens and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean ﬁeld and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justiﬁcation for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy. 1 Introduction: Multiply learned patterns in Hopﬁeld networks The Hopﬁeld network as a model of associative memory and unsupervised learning was introduced in [23] and has been intensively studied from a wide range of viewpoints in the past thirty years. However, properties of a strong pattern, as a pattern that has been multiply stored or learned in these networks, have only been examined very recently, a surprising delay given that repetition of an activity is the basis of learning by the Hebbian rule and long term potentiation. In particular, while the storage capacity of a Hopﬁeld network with certain correlated patterns has been tackled [13, 25], the storage capacity of a Hopﬁeld network in the presence of strong as well as random patterns has not been hitherto addressed. The notion of a strong pattern of a Hopﬁeld network has been proposed in [15] to model attachment types and behavioural prototypes in developmental psychology and psychotherapy. This suggestion has been motivated by reviewing the pioneering work of Bowlby [9] in attachment theory and highlighting how a number of academic biologists, psychiatrists, psychologists, sociologists and neuroscientists have consistently regarded Hopﬁeld-like artiﬁcial neural networks as suitable tools to model cognitive and behavioural constructs as patterns that are deeply and repeatedly learned by individuals [11, 22, 24, 30, 29, 10]. A number of mathematical properties of strong patterns in Hopﬁeld networks, which give rise to strong attractors, have been derived in [15]. These show in particular that strong attractors are strongly stable; a series of experiments have also been carried out which conﬁrm the mathematical 1 results and also indicate that a strong pattern stored in the network can be retrieved even in the presence of a large number of simple patterns, far exceeding the well-known maximum load parameter or storage capacity of the Hopﬁeld network with random patterns (αc ≈ 0.138). In this paper, we consider strong patterns in stochastic Hopﬁeld model with temperature, which accounts for various types of noise in the network. In these networks, the updating rule is probabilistic and depend on the temperature. Since analytical solution of such a system is not possible in general, one strives to obtain the average behaviour of the network when the input to each node, the so-called ﬁeld at the node, is replaced with its mean. This is the basis of mean ﬁeld theory for these networks. Due to the close connection between the Hopﬁeld network and the Ising model in ferromagnetism [1, 8], the mean ﬁeld approach for the Hopﬁeld network and its variations has been tackled using the replica method, starting with the pioneering work of Amit, Gutfreund and Sompolinsky [3, 2, 4, 19, 31, 1, 13]. Although this method has been widely used in the theory of spin glasses in statistical physics [26, 16] its mathematical justiﬁcation has proved to be elusive as we will discuss in the next section; see for example [20, page 264], [14, page 27], and [7, page 9]. In [17] and independently in [27], an alternative technique to the replica method for solving the mean ﬁeld equations has been proposed which is reproduced and characterised as heuristic in [20, section 2.5] since it relies on a number of assumptions that are not later justiﬁed and uses a number of mathematical steps that are not validated. Here, we use the basic idea of the above heuristic to develop a veriﬁable mathematical framework with provable results grounded on elements of probability theory, with which we assume the reader is familiar. This technique allows us to solve the mean ﬁeld equations for the Hopﬁeld network in the presence of strong patterns and use the results to study, ﬁrst, the stability of these patterns in the presence of temperature (noise) and, second, the storage capacity of the network with a single strong pattern at temperature zero. We show that the critical temperature for the stability of a strong pattern is equal to its degree (i.e., its multiplicity) when the ratio of the sum of the squares of degrees of the patterns to the network size tends to zero when the latter tends to inﬁnity. In the case that there is only one strong pattern present with its degree small compared to the number of patterns and the latter is a ﬁxed multiple of the number of nodes, we ﬁnd the distribution of the overlap of the mean ﬁeld and the patterns when the strong pattern is being retrieved. We use these distributions to prove that the storage capacity for retrieving a strong pattern exceeds that for a simple pattern by a multiplicative factor equal to the square of the degree of the strong attractor. This result matches the ﬁnding in [15] regarding the capacity of a network to recall strong patterns as mentioned above. Our results therefore show that strong patterns are robust and persistent in the network memory as attachment types and behavioural prototypes are in the human memory system. In this paper, we will several times use Lyapunov’s theorem in probability which provides a simple sufﬁcient condition to generalise the Central Limit theorem when we deal with independent but not necessarily identically distributed random variables. We require a general form of this theorem kn as follows. Let Yn = N i=1 Yni , for n ∈ I , be a triangular array of random variables such that for each n, the random variables Yni , for 1 ≤ i ≤ kn are independent with E(Yni ) = 0 2 2 and E(Yni ) = σni , where E(X) stands for the expected value of the random variable X. Let kn 2 2 sn = i=1 σni . We use the notation X ∼ Y when the two random variables X and Y have the same distribution (for large n if either or both of them depend on n). Theorem 1.1 (Lyapunov’s theorem [6, page 368]) If for some δ > 0, we have the condition: 1 E(|Yn |2+δ |) → 0 s2+δ n d d as n → ∞ then s1 Yn −→ N (0, 1) as n → ∞ where −→ denotes convergence in distribution, and we denote n by N (a, σ 2 ) the normal distribution with mean a and variance σ 2 . Thus, for large n we have Yn ∼ N (0, s2 ). n 2 2 Mean ﬁeld theory We consider a Hopﬁeld network with N neurons i = 1, . . . , N with values Si = ±1 and follow the notations in [20]. As in [15], we assume patterns can be multiply stored and the degree of a pattern is deﬁned as its multiplicity. The total number of patterns, counting their multiplicity, is denoted by p and we assume there are n patterns ξ 1 , . . . , ξ n with degrees d1 , . . . , dn ≥ 1 respectively and that n the remaining p − k=1 dk ≥ 0 patterns are simple, i.e., each has degree one. Note that by our assumptions there are precisely n p0 = p + n − dk k=1 distinct patterns, which we assume are independent and identically distributed with equal probability of taking value ±1 for each node. More generally, for any non-negative integer k ∈ I , we let N p0 dk . µ pk = µ=1 p µ µ 0 1 We use the generalized Hebbian rule for the synaptic couplings: wij = N µ=1 dµ ξi ξj for i = j with wii = 0 for 1 ≤ i, j ≤ N . As in the standard stochastic Hopﬁeld model [20], we use Glauber dynamics [18] for the stochastic updating rule with pseudo-temperature T > 0, which accounts for various types of noise in the network, and assume zero bias in the local ﬁeld. Putting β = 1/T (i.e., with the Boltzmann constant kB = 1) and letting fβ (h) = 1/(1 + exp(−2βh)), the stochastic updating rule at time t is given by: N Pr(Si (t + 1) = ±1) = fβ (±hi (t)), where hi (t) = wij Sj (t), (1) j=1 is the local ﬁeld at i at time t. The updating is implemented asynchronously in a random way. The energy of the network in the conﬁguration S = (Si )N is given by i=1 N 1 Si Sj wij . H(S) = − 2 i,j=1 For large N , this speciﬁes a complex system, with an underlying state space of dimension 2N , which in general cannot be solved exactly. However, mean ﬁeld theory has proved very useful in studying Hopﬁeld networks. The average updated value of Si (t + 1) in Equation (1) is Si (t + 1) = 1/(1 + e−2βhi (t) ) − 1/(1 + e2βhi (t) ) = tanh(βhi (t)), (2) where . . . denotes taking average with respect to the probability distribution in the updating rule in Equation (1). The stationary solution for the mean ﬁeld thus satisﬁes: Si = tanh(βhi ) , (3) The average overlap of pattern ξ µ with the mean ﬁeld at the nodes of the network is given by: mν = 1 N N ν ξi Si (4) i=1 The replica technique for solving the mean ﬁeld problem, used in the case p/N = α > 0 as N → ∞, seeks to obtain the average of the overlaps in Equation (4) by evaluating the partition function of the system, namely, Z = TrS exp(−βH(S)), where the trace TrS stands for taking sum over all possible conﬁgurations S = (Si )N . As it i=1 is generally the case in statistical physics, once the partition function of the system is obtained, 3 all required physical quantities can in principle be computed. However, in this case, the partition function is very difﬁcult to compute since it entails computing the average log Z of log Z, where . . . indicates averaging over the random distribution of the stored patterns ξ µ . To overcome this problem, the identity Zk − 1 log Z = lim k→0 k is used to reduce the problem to ﬁnding the average Z k of Z k , which is then computed for positive integer values of k. For such k, we have: Z k = TrS 1 TrS 2 . . . TrS k exp(−β(H(S 1 ) + H(S 1 ) + . . . + H(S k ))), where for each i = 1, . . . , k the super-scripted conﬁguration S i is a replica of the conﬁguration state. In computing the trace over each replica, various parameters are obtained and the replica symmetry condition assumes that these parameters are independent of the particular replica under consideration. Apart from this assumption, there are two basic mathematical problems with the technique which makes it unjustiﬁable [20, page 264]. Firstly, the positive integer k above is eventually treated as a real number near zero without any mathematical justiﬁcation. Secondly, the order of taking limits, in particular the order of taking the two limits k → 0 and N → ∞, are several times interchanged again without any mathematical justiﬁcation. Here, we develop a mathematically rigorous method for solving the mean ﬁeld problem, i.e., computing the average of the overlaps in Equation (4) in the case of p/N = α > 0 as N → ∞. Our method turns the basic idea of the heuristic presented in [17] and reproduced in [20] for solving the mean ﬁeld equation into a mathematically veriﬁable formalism, which for the standard Hopﬁeld network with random stored patterns gives the same result as the replica method, assuming replica symmetry. In the presence of strong patterns we obtain a set of new results as explained in the next two sections. The mean ﬁeld equation is obtained from Equation (3) by approximating the right hand side of N this equation by the value of tanh at the mean ﬁeld hi = j=1 wij Sj , ignoring the sum N j=1 wij (Sj − Sj ) for large N [17, page 32]: Si = tanh(β hi ) = tanh β N N j=1 p0 µ=1 µ µ dµ ξi ξj Sj . (5) Equation (5) gives the mean ﬁeld equation for the Hopﬁeld network with n possible strong patterns n ξ µ (1 ≤ µ ≤ n) and p − µ=1 dµ simple patterns ξ µ with n + 1 ≤ µ ≤ p0 . As in the standard Hopﬁeld model, where all patterns are simple, we have two cases to deal with. However, we now have to account for the presence of strong attractors and our two cases will be as follows: (i) In the p0 ﬁrst case we assume p2 := µ=1 d2 = o(N ), which includes the simpler case p2 N when p2 µ is ﬁxed and independent of N . (ii) In the second case we assume we have a single strong attractor with the load parameter p/N = α > 0. 3 Stability of strong patterns with noise: p2 = o(N ) The case of constant p and N → ∞ is usually referred to as α = 0 in the standard Hopﬁeld model. Here, we need to consider the sum of degrees of all stored patterns (and not just the number of patterns) compared to N . We solve the mean ﬁeld equation with T > 0 by using a method similar in spirit to [20, page 33] for the standard Hopﬁeld model, but in our case strong patterns induce a sequence of independent but non-identically distributed random variables in the crosstalk term, where the Central Limit Theorem cannot be used; we show however that Lyapunov’s theorem (Theorem (1.1) can be invoked. In retrieving pattern ξ 1 , we look for a solution of the mean ﬁled 1 equation of the form: Si = mξi , where m > 0 is a constant. Using Equation (5) and separating 1 the contribution of ξ in the argument of tanh, we obtain:  1 mξi = tanh    mβ  1 d1 ξi + N 4 µ µ 1 dµ ξi ξj ξj  . j=i,µ>1 (6) For each N , µ > 1 and j = i, let dµ µ µ 1 (7) ξ ξ ξ . N i j j 2 This gives (p0 − 1)(N − 1) independent random variables with E(YN µj ) = 0, E(YN µj ) = d2 /N 2 , µ 3 3 3 and E(|YN µj |) = dµ /N . We have: YN µj = s2 := N 2 E(YN µj ) = µ>1,j=i 1 N −1 d2 ∼ N 2 µ>1 µ N d2 . µ (8) µ>1 Thus, as N → ∞, we have: 1 s3 N 3 E(|YN µj |) ∼ √ µ>1,j=i µ>1 N( d3 µ µ>1 d2 )3/2 µ → 0. (9) as N → ∞ since for positive numbers dµ we always have µ>1 d3 < ( µ>1 d2 )3/2 . Thus the µ µ Lyapunov condition is satisﬁed for δ = 1. By Lyapunov’s theorem we deduce: 1 N µ µ 1 dµ ξi ξj ξj ∼ N d2 /N µ 0, (10) µ>1 µ>1,j=i Since we also have p2 = o(N ), it follows that we can ignore the second term, i.e., the crosstalk term, in the argument of tanh in Equation (6) as N → ∞; we thus obtain: m = tanh βd1 m. (11) To examine the ﬁxed points of the Equation (11), we let d = d1 for convenience and put x = βdm = dm/T , so that tanh x = T x/d; see Figure 1. It follows that Tc = d is the critical temperature. If T < d then there is a non-zero (non-trivial) solution for m, whereas for T > d we only have the trivial solution. For d = 1 our solution is that of the standard Hopﬁeld network as in [20, page 34]. (d < T) y>x y = x ( d = T) y = tanh x y</p><p>5 0.59167767 <a title="77-lsi-5" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>6 0.55429465 <a title="77-lsi-6" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>7 0.53734154 <a title="77-lsi-7" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>8 0.43226436 <a title="77-lsi-8" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>9 0.38677892 <a title="77-lsi-9" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>10 0.38589239 <a title="77-lsi-10" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>11 0.35775101 <a title="77-lsi-11" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>12 0.34971493 <a title="77-lsi-12" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>13 0.28962624 <a title="77-lsi-13" href="./nips-2013-Linear_decision_rule_as_aspiration_for_simple_decision_heuristics.html">176 nips-2013-Linear decision rule as aspiration for simple decision heuristics</a></p>
<p>14 0.26480126 <a title="77-lsi-14" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>15 0.24198873 <a title="77-lsi-15" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>16 0.23778416 <a title="77-lsi-16" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>17 0.22841135 <a title="77-lsi-17" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>18 0.21078014 <a title="77-lsi-18" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>19 0.20645304 <a title="77-lsi-19" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>20 0.20464024 <a title="77-lsi-20" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (16, 0.064), (26, 0.012), (33, 0.086), (34, 0.126), (41, 0.189), (49, 0.064), (56, 0.085), (70, 0.144), (85, 0.024), (89, 0.028), (93, 0.022), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8887527 <a title="77-lda-1" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>2 0.88291562 <a title="77-lda-2" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>Author: Nils E. Napp, Ryan P. Adams</p><p>Abstract: Recent work on molecular programming has explored new possibilities for computational abstractions with biomolecules, including logic gates, neural networks, and linear systems. In the future such abstractions might enable nanoscale devices that can sense and control the world at a molecular scale. Just as in macroscale robotics, it is critical that such devices can learn about their environment and reason under uncertainty. At this small scale, systems are typically modeled as chemical reaction networks. In this work, we develop a procedure that can take arbitrary probabilistic graphical models, represented as factor graphs over discrete random variables, and compile them into chemical reaction networks that implement inference. In particular, we show that marginalization based on sum-product message passing can be implemented in terms of reactions between chemical species whose concentrations represent probabilities. We show algebraically that the steady state concentration of these species correspond to the marginal distributions of the random variables in the graph and validate the results in simulations. As with standard sum-product inference, this procedure yields exact results for tree-structured graphs, and approximate solutions for loopy graphs.</p><p>3 0.8582859 <a title="77-lda-3" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>Author: Philip S. Thomas, William C. Dabney, Stephen Giguere, Sridhar Mahadevan</p><p>Abstract: Natural actor-critics form a popular class of policy search algorithms for ﬁnding locally optimal policies for Markov decision processes. In this paper we address a drawback of natural actor-critics that limits their real-world applicability—their lack of safety guarantees. We present a principled algorithm for performing natural gradient descent over a constrained domain. In the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space. While deriving our class of constrained natural actor-critic algorithms, which we call Projected Natural ActorCritics (PNACs), we also elucidate the relationship between natural gradient descent and mirror descent. 1</p><p>4 0.80628163 <a title="77-lda-4" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>Author: Jonas Peters, Dominik Janzing, Bernhard Schölkopf</p><p>Abstract: Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identiﬁability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufﬁcient or the model is misspeciﬁed, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artiﬁcial and real data and code is provided. 1</p><p>5 0.80053121 <a title="77-lda-5" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>6 0.79151887 <a title="77-lda-6" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>7 0.7856009 <a title="77-lda-7" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>8 0.78492713 <a title="77-lda-8" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>9 0.78229553 <a title="77-lda-9" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>10 0.78172278 <a title="77-lda-10" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>11 0.7812413 <a title="77-lda-11" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>12 0.77313548 <a title="77-lda-12" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>13 0.76220769 <a title="77-lda-13" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>14 0.74826515 <a title="77-lda-14" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>15 0.74525982 <a title="77-lda-15" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>16 0.74349374 <a title="77-lda-16" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>17 0.72905046 <a title="77-lda-17" href="./nips-2013-Linear_Convergence_with_Condition_Number_Independent_Access_of_Full_Gradients.html">175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</a></p>
<p>18 0.72743058 <a title="77-lda-18" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>19 0.72238243 <a title="77-lda-19" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>20 0.71043348 <a title="77-lda-20" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
