<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-208" href="#">nips2013-208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</h1>
<br/><p>Source: <a title="nips-2013-208-pdf" href="http://papers.nips.cc/paper/5052-neural-representation-of-action-sequences-how-far-can-a-simple-snippet-matching-model-take-us.pdf">pdf</a></p><p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>Reference: <a title="nips-2013-208-reference" href="../nips2013_reference/nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). [sent-11, score-1.295]
</p><p>2 For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. [sent-12, score-1.125]
</p><p>3 Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? [sent-15, score-0.364]
</p><p>4 We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. [sent-16, score-1.083]
</p><p>5 1  Introduction  For humans and other primates, action recognition is an important ability that facilitates social interaction, as well as recognition of threats and intentions. [sent-18, score-0.255]
</p><p>6 For action recognition, in addition to the challenge of position and scale invariance (which are common to many forms of visual recognition), there are additional challenges. [sent-19, score-0.418]
</p><p>7 The action being performed needs to be recognized in a manner invariant to the actor performing it. [sent-20, score-0.532]
</p><p>8 Conversely, the actor also needs to be recognized in a manner invariant to the action being performed. [sent-21, score-0.507]
</p><p>9 Ultimately, however, both the particular action and actor also need to be “bound” together by the visual system, so that the speciﬁc conjunction of a particular actor performing a particular action is recognized and experienced as a coherent percept. [sent-22, score-1.069]
</p><p>10 For the “what is where” vision problem, one common simpliﬁcation of the primate visual system is that the ventral stream handles the “what” problem, while the dorsal stream handles the “where” problem [1]. [sent-23, score-1.214]
</p><p>11 Prior work has found that brain cells in the macaque Superior Temporal Sulcus (STS) — a brain area that receives converging inputs from dorsal and ventral streams — play a major role in solving the problem. [sent-25, score-1.154]
</p><p>12 Even with a small population subset of only about 120 neurons, STS contains sufﬁcient information for action and actor to be decoded independently of one another [2]. [sent-26, score-0.472]
</p><p>13 Moreover, the particular conjunction of actor and action (i. [sent-27, score-0.458]
</p><p>14 In other words, 1  STS neurons have been shown to have successfully tackled the three challenges of actor-invariance, action-invariance and actor-action binding. [sent-30, score-0.269]
</p><p>15 We ﬁnd that instead of distinct clusters of actor-invariant and action-invariant neurons, the neurons cover a broad, continuous range of invariance. [sent-39, score-0.269]
</p><p>16 Despite its simplicity, modeling STS neurons as a linear weighted sum of inputs over a short temporal window produces surprisingly good ﬁts to the data. [sent-43, score-0.501]
</p><p>17 2  Background: the Superior Temporal Sulcus  The macaque visual system is commonly described as being separated into the ventral (“what”) and dorsal (“where”) streams [1]. [sent-44, score-1.098]
</p><p>18 The Superior Temporal Sulcus (STS) is a high-level brain area that receive inputs from both streams [3, 4]. [sent-45, score-0.282]
</p><p>19 In particular, it receives inputs from the highest levels of the processing hierarchy of either stream — inferotemporal (IT) cortex for the ventral stream, and the Medial Superior Temporal (MST) cortex for the dorsal stream. [sent-46, score-1.123]
</p><p>20 Accordingly, neurons that are biased more towards either encoding form information or motion information have been found in the STS [5]. [sent-47, score-0.426]
</p><p>21 The upper bank of the STS has been found to contain neurons more selective for motion, with some invariance to form [6, 7]. [sent-48, score-0.469]
</p><p>22 Relative to the upper bank, neurons in the lower bank of the STS have been found to be more sensitive to form, with some “snapshot” neurons selective for static poses within action sequences [7]. [sent-49, score-0.897]
</p><p>23 Using functional MRI (fMRI), neurons in the lower bank were found to respond to point-light ﬁgures [8] performing biological actions [9], consistent with the idea that actions can be recognized from distinctive static poses [10]. [sent-50, score-0.611]
</p><p>24 However, there is no clear, quantitative evidence for a neat separation between motion-sensitive, form-invariant neurons in the upper bank and form-sensitive, motion-invariant neurons in the lower bank. [sent-51, score-0.583]
</p><p>25 STS neurons have been found to be selective for speciﬁc combinations of form and motion [3, 11]. [sent-52, score-0.391]
</p><p>26 Two male rhesus macaques (monkeys G and S) were trained to perform an action recognition task, while neural activity from a total of 119 single neurons (59 and 60 from G and S respectively) was recorded during task performance. [sent-57, score-0.524]
</p><p>27 A sample movie of one actor performing the 8 actions can be found at http://www. [sent-64, score-0.37]
</p><p>28 At the start of each trial, after the monkey maintained ﬁxation for 450ms, a blank screen was shown for 500ms, and then one of the actors was displayed (subtending 6◦ of visual angle vertically). [sent-68, score-0.405]
</p><p>29 Regardless of action, the actor was ﬁrst displayed motionless in an upright neutral pose for 300ms, then began 2  performing one of the 8 actions. [sent-69, score-0.397]
</p><p>30 A button-press response at any point by the monkey immediately ended the trial, and the screen was blanked. [sent-71, score-0.272]
</p><p>31 Similar to [2], we assumed that all neurons had a response latency of 130ms. [sent-75, score-0.33]
</p><p>32 Bottom row: sample frames of actor 5 performing the 8 actions; frames are from the same time-point within each action. [sent-78, score-0.33]
</p><p>33 The 64 stimuli were an 8-by-8 cross of each actor performing each action. [sent-79, score-0.339]
</p><p>34 We characterized each neuron’s response characteristics along two dimensions: invariance to actor and to action. [sent-81, score-0.431]
</p><p>35 Then, we calculated the Pearson correlation between the neuron’s actual responses and the responses that would be seen if the neuron were completely actor-invariant (i. [sent-83, score-0.375]
</p><p>36 Speciﬁcally, we use the HMAX family of models, which include models of the ventral [14] and dorsal [15] streams. [sent-90, score-0.729]
</p><p>37 While ventral stream processing has traditionally been modeled as producing outputs in response to static images, in practice, neurons in the ventral stream are also sensitive to temporal aspects [16]. [sent-93, score-1.54]
</p><p>38 As such, we extend the ventral stream model to be more biologically realistic. [sent-94, score-0.531]
</p><p>39 Speciﬁcally, the V1 neurons that project to the ventral stream now have temporal receptive ﬁelds (RFs) [17], not just spatial ones. [sent-95, score-0.893]
</p><p>40 These temporal and spatial RFs are separable, unlike those for V1 neurons that project to the dorsal stream [18]. [sent-96, score-0.9]
</p><p>41 Such space-time separable V1 neurons that project to the ventral stream are not directionally-selective and are not sensitive to motion per se. [sent-97, score-0.907]
</p><p>42 They are still sensitive to form rather than motion, but are better models of form processing, since in reality input to the visual system consists of a continuous stream of images. [sent-98, score-0.336]
</p><p>43 Importantly, the parameters of dorsal and ventral encoding models were ﬁxed, and there was no optimization done to produce better ﬁts to the current data. [sent-99, score-0.8]
</p><p>44 As a ﬁrst approximation, we model the neural processing by STS neurons as a linear weighted sum of inputs. [sent-102, score-0.307]
</p><p>45 In other words, at any point in time, the output of a model STS neuron is a linear combination of the C2 outputs produced by the ventral and dorsal encoding models. [sent-104, score-0.91]
</p><p>46 Each model neuron has its own set of static weights that determine its unique pattern of neural responses to the 64 action clips. [sent-109, score-0.477]
</p><p>47 Importantly, at no point are ground-truth actor or action labels used. [sent-121, score-0.43]
</p><p>48 Rather than use the 960 dorsal and/or 960 ventral C2 features directly as inputs to the linear regression, we ﬁrst performed PCA on these features (separately for the two streams) to reduce the dimensionality. [sent-122, score-0.8]
</p><p>49 Fitting was also performed using the combination of dorsal and ventral C2 features. [sent-125, score-0.729]
</p><p>50 Overall, the neurons span a broad range of action and actor invariance (95% of invariance index values span the ranges [0. [sent-131, score-0.937]
</p><p>51 Figure 2: Actor- and action-invariance indices for 59 neurons from monkey G (blue) and 60 neurons from monkey S (red). [sent-142, score-0.944]
</p><p>52 Figure 3 shows the response waveforms of some example neurons to give a sense of what response patterns correspond to low and high invariance indices. [sent-144, score-0.608]
</p><p>53 Neuron G20 is highly invariant to both action and actor, while the invariance of S10 is close to the mean invariance of the population. [sent-147, score-0.458]
</p><p>54 We ﬁnd that there are no distinct clusters of neurons with high actor-invariance or action-invariance. [sent-148, score-0.269]
</p><p>55 Such clusters would correspond to a representation scheme in which certain neurons specialize in coding for action invariant to actor, and vice-versa. [sent-149, score-0.52]
</p><p>56 A cluster of neurons with both low actor- and action-invariance could correspond to cells that code for a speciﬁc conjunction (binding) of actor and action, but no such cluster is seen. [sent-150, score-0.577]
</p><p>57 rest of this paper, we explore how well a linear, feedforward encoding model of STS ventral/dorsal integration can reproduce the neural responses and invariance properties found here. [sent-159, score-0.349]
</p><p>58 In this extreme case, action recognition is performed by the matching of individual frames to the neuron’s preferred stimulus. [sent-178, score-0.295]
</p><p>59 Neurons sensitive to such action snippets have been found using techniques such as fMRI [9, 12] and electrophysiology [7]. [sent-182, score-0.249]
</p><p>60 While there is some evidence for the snippet model in terms of the existence of neurons responsive and selective for short action sequences, it is still unclear how feasible such an encoding model is. [sent-184, score-0.71]
</p><p>61 For instance, given some visual input, if a neuron simply tries to match that sequence to its preferred stimulus, how exactly does the neuron ignore the motion aspects (to recognize actor invariant to action) or ignore the form aspects (to recognize action invariant to actors)? [sent-185, score-1.008]
</p><p>62 Furthermore, there is a diversity of neural response patterns, both between different neurons (see Figs. [sent-191, score-0.368]
</p><p>63 First, we examine the results of the leave-one-out ﬁtting procedure when the inputs to STS model neurons are from either the dorsal or ventral streams alone. [sent-198, score-1.215]
</p><p>64 For monkey G, the mean goodness-of-ﬁt (correlation between actual and predicted neural responses on left-out test stimuli) over all 59 neurons are 0. [sent-199, score-0.609]
</p><p>65 For monkey S, the mean goodness-of-ﬁt over all 60 neurons is 0. [sent-204, score-0.45]
</p><p>66 Averaged over all neurons and both streams, the mean goodness-of-ﬁt is 0. [sent-207, score-0.269]
</p><p>67 This complexity is seen most clearly from the responses to the 8 actions averaged over actors, where the number and height of peaks in the waveform vary considerably from one action to another. [sent-216, score-0.409]
</p><p>68 The ﬁt is remarkable considering the simplicity of the snippet model, in which there is only one set of static linear weights; all ﬂuctuations in the predicted waveforms arise purely from changes in the inputs to this model STS neuron. [sent-217, score-0.341]
</p><p>69 Over the whole population, the ﬁts to the dorsal model (mean r=0. [sent-218, score-0.368]
</p><p>70 42) are better than to the ventral model (mean r=0. [sent-219, score-0.361]
</p><p>71 Is there a systematic relationship between the difference in goodness-of-ﬁt to the two streams and the invariance indices calculated in Section 4? [sent-221, score-0.351]
</p><p>72 For instance, one might expect that neurons with high actor-invariance would be better ﬁt to the dorsal than ventral model. [sent-222, score-0.998]
</p><p>73 5, we see that this is exactly the case for actor invariance. [sent-224, score-0.251]
</p><p>74 There is a strong positive correlation between actor invariance and difference (dorsal minus ventral) in goodness-of-ﬁt (monkey G: r=0. [sent-225, score-0.407]
</p><p>75 strong action invariance predicts better ﬁt to ventral model) for monkey S (r=-0. [sent-230, score-0.84]
</p><p>76 It is unclear why this is the case, but it may be linked to the robust correlation between actor- and action-invariance indices for monkey G (r=0. [sent-234, score-0.285]
</p><p>77 6  Figure 4: Predicted (blue) and actual (red) waveforms for two example neurons, both ﬁt to the dorsal stream. [sent-239, score-0.466]
</p><p>78 Y-axis: difference between r from ﬁtting to dorsal versus ventral streams. [sent-246, score-0.729]
</p><p>79 Interestingly, either stream can produce actor-invariant and action-invariant responses (Fig. [sent-248, score-0.263]
</p><p>80 While G54 is better ﬁt to the dorsal than ventral stream (0. [sent-250, score-0.899]
</p><p>81 So far, we have performed linear ﬁtting using the dorsal and ventral streams separately. [sent-256, score-0.875]
</p><p>82 Interestingly, this ﬁtting to a combination of streams without prior knowledge of which stream is more suitable, produces ﬁts that are as good or better than if we knew a priori which stream would produce a better ﬁt for a speciﬁc neuron (0. [sent-262, score-0.596]
</p><p>83 How much better compared to low-level controls does our snippet model ﬁt to the combined outputs of dorsal and ventral stream models? [sent-269, score-1.009]
</p><p>84 These were 25% (G) and 16% (S) worse than ﬁtting to the combination of dorsal and ventral models. [sent-276, score-0.729]
</p><p>85 In contrast, the same random shifts did not change the average ﬁt numbers for the combination of dorsal and ventral models (0. [sent-283, score-0.729]
</p><p>86 Finally, how do the actor- and action-invariance indices calculated from the predicted responses compare to those calculated from the ground-truth data? [sent-287, score-0.249]
</p><p>87 Averaged over all 119 neurons ﬁtted to a combination of dorsal and ventral streams, the actor- and action-invariance indices are within 0. [sent-288, score-1.042]
</p><p>88 7  Conclusions  We found that at the level of individual neurons, the neuronal representation in STS spans a broad, continuous range of actor- and action-invariance, rather than having groups of neurons with distinct invariance properties. [sent-296, score-0.444]
</p><p>89 Simply as a baseline model, we investigated how well a linear weighted sum of dorsal and ventral stream responses to action “snippets” could reproduce the neural response patterns found in these STS neurons. [sent-297, score-1.298]
</p><p>90 Other ventral and dorsal models can also be tested (e. [sent-302, score-0.729]
</p><p>91 Nonetheless, this simple “snippet-matching” model is able to grossly reproduce the pattern of neural responses and invariance properties found in the STS. [sent-305, score-0.278]
</p><p>92 Sheinberg, “Temporal cortex neurons encode articulated actions as slow sequences of integrated poses. [sent-319, score-0.449]
</p><p>93 Perrett, “Integration of form and motion in the anterior superior temporal polysensory area of the macaque monkey. [sent-328, score-0.358]
</p><p>94 Desimone, “Organization of visual inputs to the inferior temporal and posterior parietal cortex in macaques. [sent-338, score-0.347]
</p><p>95 Perrett, “Single cell integration of animate form, motion and location in the superior temporal cortex of the macaque monkey. [sent-347, score-0.362]
</p><p>96 Gross, “Visual properties of neurons in a polysensory area in superior temporal sulcus of the macaque. [sent-356, score-0.538]
</p><p>97 Vogels, “Functional differentiation of macaque visual temporal cortical neurons using a parametric action space. [sent-364, score-0.739]
</p><p>98 Bender, “Visual properties of neurons in inferotemporal cortex of the macaque. [sent-459, score-0.359]
</p><p>99 Newsome, “Visual response properties of striate cortical neurons projecting to area MT in macaque monkeys. [sent-473, score-0.437]
</p><p>100 Pack, “Hierarchical processing of complex motion along the primate dorsal visual pathway. [sent-509, score-0.574]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sts', 0.466), ('dorsal', 0.368), ('ventral', 0.361), ('neurons', 0.269), ('actor', 0.251), ('monkey', 0.181), ('action', 0.179), ('stream', 0.17), ('streams', 0.146), ('visual', 0.12), ('invariance', 0.119), ('neuron', 0.11), ('snippet', 0.11), ('waveforms', 0.098), ('temporal', 0.093), ('responses', 0.093), ('sheinberg', 0.09), ('motion', 0.086), ('actors', 0.081), ('macaque', 0.078), ('sulcus', 0.075), ('encoding', 0.071), ('actions', 0.071), ('inputs', 0.071), ('waveform', 0.066), ('stimuli', 0.063), ('cortex', 0.063), ('response', 0.061), ('snippets', 0.049), ('neuroscience', 0.048), ('poggio', 0.046), ('perrett', 0.045), ('bank', 0.045), ('monkeys', 0.044), ('indices', 0.044), ('superior', 0.042), ('calculated', 0.042), ('population', 0.042), ('invariant', 0.041), ('neutral', 0.039), ('biological', 0.039), ('recognition', 0.038), ('neural', 0.038), ('correlation', 0.037), ('brain', 0.036), ('recognized', 0.036), ('selective', 0.036), ('perception', 0.035), ('singer', 0.035), ('static', 0.034), ('stimulus', 0.034), ('serre', 0.033), ('fmri', 0.032), ('tting', 0.032), ('ts', 0.032), ('fr', 0.031), ('representation', 0.031), ('cheston', 0.03), ('ended', 0.03), ('eyes', 0.03), ('lappe', 0.03), ('motionless', 0.03), ('polysensory', 0.03), ('area', 0.029), ('cells', 0.029), ('pose', 0.029), ('reproduce', 0.028), ('predicted', 0.028), ('conjunction', 0.028), ('frames', 0.027), ('clip', 0.027), ('inferotemporal', 0.027), ('haxby', 0.027), ('ungerleider', 0.027), ('preferred', 0.026), ('individual', 0.025), ('system', 0.025), ('performing', 0.025), ('trial', 0.025), ('rfs', 0.024), ('memoryless', 0.024), ('mechanisms', 0.024), ('neurophysiology', 0.024), ('window', 0.024), ('weights', 0.023), ('articulated', 0.023), ('desimone', 0.023), ('sequences', 0.023), ('unclear', 0.023), ('displayed', 0.023), ('movie', 0.023), ('pixel', 0.022), ('surprisingly', 0.022), ('xation', 0.022), ('clips', 0.022), ('movshon', 0.022), ('recognize', 0.022), ('short', 0.022), ('poses', 0.021), ('sensitive', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="208-tfidf-1" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>2 0.28979808 <a title="208-tfidf-2" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>3 0.16839267 <a title="208-tfidf-3" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>4 0.15669596 <a title="208-tfidf-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.1485471 <a title="208-tfidf-5" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>6 0.14627549 <a title="208-tfidf-6" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>7 0.11288264 <a title="208-tfidf-7" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>8 0.11131484 <a title="208-tfidf-8" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>9 0.10979135 <a title="208-tfidf-9" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>10 0.10783771 <a title="208-tfidf-10" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>11 0.098726921 <a title="208-tfidf-11" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>12 0.093969278 <a title="208-tfidf-12" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>13 0.092560895 <a title="208-tfidf-13" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>14 0.091943078 <a title="208-tfidf-14" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>15 0.091811806 <a title="208-tfidf-15" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>16 0.085988507 <a title="208-tfidf-16" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>17 0.085896537 <a title="208-tfidf-17" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>18 0.078733146 <a title="208-tfidf-18" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>19 0.077908128 <a title="208-tfidf-19" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>20 0.069476984 <a title="208-tfidf-20" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.034), (2, -0.164), (3, -0.112), (4, -0.219), (5, -0.124), (6, -0.036), (7, -0.07), (8, -0.057), (9, 0.105), (10, -0.076), (11, 0.02), (12, 0.031), (13, 0.007), (14, -0.016), (15, 0.028), (16, -0.01), (17, -0.124), (18, -0.031), (19, -0.027), (20, -0.024), (21, 0.094), (22, -0.032), (23, -0.015), (24, -0.022), (25, -0.085), (26, -0.036), (27, 0.014), (28, -0.034), (29, 0.032), (30, -0.026), (31, -0.142), (32, -0.045), (33, -0.073), (34, -0.023), (35, -0.102), (36, -0.064), (37, 0.033), (38, 0.053), (39, -0.056), (40, -0.025), (41, -0.047), (42, 0.01), (43, 0.117), (44, -0.017), (45, -0.01), (46, -0.066), (47, -0.076), (48, 0.014), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97198075 <a title="208-lsi-1" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>2 0.70716172 <a title="208-lsi-2" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>3 0.6538825 <a title="208-lsi-3" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>4 0.64761519 <a title="208-lsi-4" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>5 0.64203393 <a title="208-lsi-5" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>6 0.62037069 <a title="208-lsi-6" href="./nips-2013-Multisensory_Encoding%2C_Decoding%2C_and_Identification.html">205 nips-2013-Multisensory Encoding, Decoding, and Identification</a></p>
<p>7 0.60812056 <a title="208-lsi-7" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>8 0.58633399 <a title="208-lsi-8" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>9 0.57930082 <a title="208-lsi-9" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>10 0.55332857 <a title="208-lsi-10" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>11 0.50376171 <a title="208-lsi-11" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>12 0.49467272 <a title="208-lsi-12" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>13 0.47046253 <a title="208-lsi-13" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>14 0.46221203 <a title="208-lsi-14" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>15 0.45705721 <a title="208-lsi-15" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>16 0.45435074 <a title="208-lsi-16" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>17 0.44744867 <a title="208-lsi-17" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>18 0.43671709 <a title="208-lsi-18" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>19 0.42836782 <a title="208-lsi-19" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>20 0.42700645 <a title="208-lsi-20" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.036), (33, 0.12), (34, 0.126), (41, 0.018), (49, 0.074), (56, 0.087), (70, 0.054), (75, 0.012), (85, 0.018), (89, 0.049), (93, 0.036), (96, 0.275)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79900914 <a title="208-lda-1" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>Author: Cheston Tan, Jedediah M. Singer, Thomas Serre, David Sheinberg, Tomaso Poggio</p><p>Abstract: The macaque Superior Temporal Sulcus (STS) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively). For the processing of articulated actions, prior work has shown that even a small population of STS neurons contains sufﬁcient information for the decoding of actor invariant to action, action invariant to actor, as well as the speciﬁc conjunction of actor and action. This paper addresses two questions. First, what are the invariance properties of individual neural representations (rather than the population representation) in STS? Second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images? We ﬁnd that a simple model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action “snippets”, produces surprisingly good ﬁts to the neural data. Interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be accounted for, by having different linear weights. 1</p><p>2 0.70931536 <a title="208-lda-2" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<p>Author: Ulrike Von Luxburg, Morteza Alamgir</p><p>Abstract: Consider an unweighted k-nearest neighbor graph on n points that have been sampled i.i.d. from some unknown density p on Rd . We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or any distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate a local function of the gradient of p, and that integrating this function along shortest paths leads to an estimate of the underlying density. 1</p><p>3 0.66924083 <a title="208-lda-3" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>Author: Arash Amini, Xuanlong Nguyen</p><p>Abstract: We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.</p><p>4 0.6181072 <a title="208-lda-4" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>5 0.61380631 <a title="208-lda-5" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>6 0.61264688 <a title="208-lda-6" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>7 0.6098361 <a title="208-lda-7" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>8 0.60351866 <a title="208-lda-8" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>9 0.60291386 <a title="208-lda-9" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>10 0.60254192 <a title="208-lda-10" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>11 0.6025244 <a title="208-lda-11" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>12 0.60152131 <a title="208-lda-12" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>13 0.60023916 <a title="208-lda-13" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>14 0.59896058 <a title="208-lda-14" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>15 0.59842795 <a title="208-lda-15" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>16 0.5983783 <a title="208-lda-16" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>17 0.59725589 <a title="208-lda-17" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>18 0.5971387 <a title="208-lda-18" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>19 0.59699643 <a title="208-lda-19" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>20 0.59693736 <a title="208-lda-20" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
