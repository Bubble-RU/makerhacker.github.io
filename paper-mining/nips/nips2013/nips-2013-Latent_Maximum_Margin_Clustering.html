<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 nips-2013-Latent Maximum Margin Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-148" href="#">nips2013-148</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>148 nips-2013-Latent Maximum Margin Clustering</h1>
<br/><p>Source: <a title="nips-2013-148-pdf" href="http://papers.nips.cc/paper/5078-latent-maximum-margin-clustering.pdf">pdf</a></p><p>Author: Guang-Tong Zhou, Tian Lan, Arash Vahdat, Greg Mori</p><p>Abstract: We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. 1</p><p>Reference: <a title="nips-2013-148-reference" href="../nips2013_reference/nips-2013-Latent_Maximum_Margin_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We present a maximum margin framework that clusters data using latent variables. [sent-3, score-0.508]
</p><p>2 Using latent representations enables our framework to model unobserved information embedded in the data. [sent-4, score-0.28]
</p><p>3 We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. [sent-6, score-2.006]
</p><p>4 Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. [sent-7, score-0.62]
</p><p>5 Popular clustering approaches include the k-means algorithm [7], mixture models [22], normalized cuts [27], and spectral clustering [18]. [sent-10, score-0.374]
</p><p>6 Recent progress has been made using maximum margin clustering (MMC) [32], which extends the supervised large margin theory (e. [sent-11, score-0.586]
</p><p>7 MMC performs clustering by simultaneously optimizing cluster-speciﬁc models and instance-speciﬁc labeling assignments, and often generates better performance than conventional methods [33, 29, 37, 38, 16, 6]. [sent-14, score-0.307]
</p><p>8 For example, DPMs are learned in a latent SVM framework [5] for object detection; similar models have been shown to improve human action recognition [31]. [sent-25, score-0.332]
</p><p>9 Motivated by their success in supervised learning, we believe latent variable models can also help in unsupervised clustering – data instances with similar latent representations should be grouped together in one cluster. [sent-27, score-0.833]
</p><p>10 As the latent variables are unobserved in the original data, we need a learning framework to handle this latent knowledge. [sent-28, score-0.559]
</p><p>11 To implement this idea, we develop a novel clustering algorithm based on MMC that incorporates latent variables – we call this latent maximum margin clustering (LMMC). [sent-29, score-1.094]
</p><p>12 Each iteration involves three steps: inferring latent variables for each sample point, optimizing cluster assignments, and updating cluster model parameters. [sent-31, score-0.543]
</p><p>13 To evaluate the efﬁcacy of this clustering algorithm, we instantiate LMMC for tag-based video clustering, where each video is modeled with latent variables controlling the presence or absence of a set of descriptive tags. [sent-32, score-0.931]
</p><p>14 We describe tag-based video clustering in Section 4, followed by experimental results reported in Section 5. [sent-37, score-0.405]
</p><p>15 [5] formulate latent SVM by extending binary linear SVM with latent variables. [sent-46, score-0.49]
</p><p>16 All of this work demonstrates the power of max-margin latent variable models for supervised learning; our framework conducts unsupervised clustering while modeling data with latent variables. [sent-53, score-0.79]
</p><p>17 Our framework deals with multi-cluster clustering, and we model data instances with latent variables to exploit rich representations. [sent-69, score-0.322]
</p><p>18 Hoai and Zisserman [8] form a joint framework of maximum margin classiﬁcation and clustering to improve sub-categorization. [sent-77, score-0.383]
</p><p>19 Tagging videos with relevant concepts or attributes is common in video analysis. [sent-79, score-0.394]
</p><p>20 [20] predict multiple correlative tags in a structural SVM framework. [sent-81, score-0.403]
</p><p>21 Yang and Toderici [34] exploit latent sub-categories of tags in large-scale videos. [sent-82, score-0.619]
</p><p>22 Izadinia and Shah [10] model low-level event tags (e. [sent-90, score-0.374]
</p><p>23 people dancing, animal eating) as latent variables to recognize complex video events (e. [sent-92, score-0.56]
</p><p>24 Instead of supervised recognition of tags or video categories, we focus on unsupervised tag-based video clustering. [sent-95, score-0.897]
</p><p>25 In fact, recently research collects various sources of tags for video clustering. [sent-96, score-0.592]
</p><p>26 Our paper uses latent tag models, and our LMMC framework is general enough to handle various types of tags. [sent-101, score-0.508]
</p><p>27 2  3  Latent Maximum Margin Clustering  As stated above, modeling data with latent variables can be beneﬁcial in a variety of supervised applications. [sent-102, score-0.321]
</p><p>28 For unsupervised clustering, we believe it also helps to group data instances based on latent representations. [sent-103, score-0.333]
</p><p>29 When ﬁtting an instance to a cluster, we ﬁnd the optimal values for latent variables and use the corresponding latent representation of the instance. [sent-106, score-0.524]
</p><p>30 Furthermore, as the latent variables are unobserved in the original data, we need a learning framework to exploit this latent knowledge. [sent-110, score-0.559]
</p><p>31 1  Maximum Margin Clustering  MMC [32, 37, 38] extends the maximum margin principle popularized by supervised SVMs to unsupervised clustering, where the input instances are unlabeled. [sent-118, score-0.326]
</p><p>32 Suppose there are N instances {xi }N to be clustered into K clusters, MMC is formulated as follows [33, 38]: i=1 min  W,Y,ξ≥0  1 2  K  ||wt ||2 + t=1  C K  N  K  ξir  (1)  i=1 r=1  K  yit wt xi − wr xi ≥ 1 − yir − ξir , ∀i, r  s. [sent-120, score-0.443]
</p><p>33 t=1  K  yit = 1, ∀i  yit ∈ {0, 1}, ∀i, t t=1  where W = {wt }K are the linear model parameters for each cluster, ξ = {ξir } (i ∈ {1, . [sent-122, score-0.422]
</p><p>34 , K}), where yit = 1 indicates that the instance xi is clustered into the t-th cluster, and yit = 0 otherwise. [sent-135, score-0.464]
</p><p>35 1 enforces a large margin between clusters by constraining that the score of xi to the assigned cluster is sufﬁciently larger than the score of xi to any other clusters. [sent-141, score-0.496]
</p><p>36 Note that MMC is an unsupervised clustering method, which jointly estimates the model parameters W and ﬁnds the best labeling Y. [sent-142, score-0.302]
</p><p>37 Note that we explicitly enforce cluster balance using a hard constraint on the cluster sizes. [sent-148, score-0.291]
</p><p>38 Formally, we denote h as the latent variable of an instance x associated to a cluster parameterized by w. [sent-156, score-0.403]
</p><p>39 We call the resultant framework latent maximum margin clustering (LMMC). [sent-167, score-0.672]
</p><p>40 LMMC ﬁnds clusters via the following optimization: 1 2  min  W,Y,ξ≥0  K  ||wt ||2 + t=1  C K  N  K  ξir  (4)  i=1 r=1  K  yit fwt (xi ) − fwr (xi ) ≥ 1 − yir − ξir , ∀i, r  s. [sent-168, score-0.453]
</p><p>41 t=1  K  yit ∈ {0, 1}, ∀i, t  N  yit = 1, ∀i  L≤  t=1  yit ≤ U, ∀t i=1  We adopt the notation Y from the MMC formulation to denote the labeling assignment. [sent-170, score-0.703]
</p><p>42 4 enforces the large margin criterion where the score of ﬁtting xi to the assigned cluster is marginally larger than the score of ﬁtting xi to any other clusters. [sent-172, score-0.429]
</p><p>43 Note that LMMC jointly optimizes the model parameters W and ﬁnds the best labeling assignment Y, while inferring the optimal latent variables. [sent-175, score-0.344]
</p><p>44 4 is non-convex due to the optimization over the labeling assignment variables Y and the latent variables H = {hit } (i ∈ {1, . [sent-178, score-0.412]
</p><p>45 4 equivalently as: K C 1 ||wt ||2 + R(W) (5) min W 2 K t=1 where R(W) is the risk function deﬁned by: N  R(W)  =  K  max 0, 1 − yir + fwr (xi ) −  min Y  K  i=1 r=1  yit fwt (xi )  K  s. [sent-186, score-0.416]
</p><p>46 (6)  t=1  yit ∈ {0, 1}, ∀i, t  N  yit = 1, ∀i  L≤  t=1  yit ≤ U, ∀t i=1  Note that Eq. [sent-188, score-0.633]
</p><p>47 6 minimizes over the labeling assignment variables Y while inferring the latent variables H. [sent-190, score-0.412]
</p><p>48 We ﬁrst infer the latent variables H and then optimize the labeling assignment Y. [sent-195, score-0.378]
</p><p>49 3, the latent variable hit of an instance xi associated to cluster t can be obtained via: argmaxhit wt Φ(xi , hit ). [sent-197, score-0.601]
</p><p>50 For our latent tag model, we present an efﬁcient inference method in Section 4. [sent-199, score-0.508]
</p><p>51 After obtaining the latent variables H, we optimize the labeling assignment Y from Eq. [sent-200, score-0.378]
</p><p>52 Intuitively, this is to minimize the total risk of labeling all instances yet maintaining the cluster balance constraints. [sent-202, score-0.302]
</p><p>53 They are added for a better understanding of the video content and the latent tag representations. [sent-212, score-0.726]
</p><p>54 Our goal is to jointly learn video clusters and tags in a single framework. [sent-229, score-0.659]
</p><p>55 We treat tags of a video as latent variables and capture the correlations between clusters and tags. [sent-230, score-0.938]
</p><p>56 Intuitively, videos with a similar set of tags should be assigned to the same cluster. [sent-231, score-0.55]
</p><p>57 We assume a separate training dataset consisting of videos with ground-truth tag labels exists, from which we train tag detectors independently. [sent-232, score-0.702]
</p><p>58 During clustering, we are given a set of new videos without the ground-truth tag labels, and our goal is to assign cluster labels to these videos. [sent-233, score-0.571]
</p><p>59 We employ a latent tag model to represent videos. [sent-234, score-0.508]
</p><p>60 We are particularly interested in tags which describe different aspects of videos. [sent-235, score-0.374]
</p><p>61 For example, a video from the cluster “feeding animal” (see Figure 1) may be annotated with “dog”, “food”, “man”, etc. [sent-236, score-0.35]
</p><p>62 For a video being assigned to a particular cluster, we know it could have a number of tags from T describing its visual content related to the cluster. [sent-238, score-0.592]
</p><p>63 However, we do not know which tags are present in the video. [sent-239, score-0.374]
</p><p>64 To address this problem, we associate latent variables to the video to denote the presence and absence of tags. [sent-240, score-0.497]
</p><p>65 Formally, given a cluster parameterized by w, we associate a latent variable h to a video x, where h = {ht }t∈T and ht ∈ {0, 1} is a binary variable denoting the presence/absence of each tag t. [sent-241, score-0.972]
</p><p>66 ht = 1 means x has the tag t, while ht = 0 means x does not have the tag t. [sent-242, score-0.65]
</p><p>67 Figure 1 shows the latent tag representations of two sample videos. [sent-243, score-0.508]
</p><p>68 3: fw (x) = maxh w Φ(x, h), where the potential function w Φ(x, h) is deﬁned as follows: w Φ(x, h) =  1 |T |  ht · ωt φt (x)  (8)  t∈T  This potential function measures the compatibility between the video x and tag t associated with the current cluster. [sent-245, score-0.583]
</p><p>69 Note that w = {ωt }t∈T are the cluster-speciﬁc model parameters, and Φ = {ht · φt (x)}t∈T is the feature vector depending on the video x and its tags h. [sent-246, score-0.592]
</p><p>70 Here φt (x) ∈ Rd is the feature vector extracted from the video x, and the parameter ωt is a template for tag t. [sent-247, score-0.54]
</p><p>71 In our current implementation, instead of keeping φt (x) as a high dimensional vector of video features, we 5  simply represent it as a scalar score of detecting tag t on x by a pre-trained binary tag detector. [sent-248, score-0.77]
</p><p>72 8, the term corresponding to tag t is ht · ωt φt (x). [sent-253, score-0.325]
</p><p>73 TRECVID MED 11 dataset [19]: This dataset contains web videos collected by the Linguistic Data Consortium from various web video hosting sites. [sent-258, score-0.394]
</p><p>74 By removing 13 short videos that contain no visual content, we ﬁnally have a total of 2,379 videos for clustering. [sent-265, score-0.352]
</p><p>75 We use tags that were generated in Vahdat and Mori [28] for the TRECVID MED 11 dataset. [sent-266, score-0.374]
</p><p>76 In [28], text analysis tools are employed to extract binary tags based on frequent nouns in the judgment ﬁles. [sent-270, score-0.403]
</p><p>77 Examples of 74 frequent tags used in this work are: “music”, “person”, “food”, “kitchen”, “bird”, “bike”, “car”, “street”, “boat”, “water”, etc. [sent-271, score-0.374]
</p><p>78 The complete list of tags are available on our website. [sent-272, score-0.374]
</p><p>79 To train tag detectors, we use the DEV-T and DEV-O videos that belong to the 15 event categories. [sent-273, score-0.439]
</p><p>80 To remove biases between tag detectors, we normalize the detection scores by z-score normalization. [sent-278, score-0.34]
</p><p>81 Note that we make no use of the ground-truth tags on the Event-Kit videos that are to be clustered. [sent-279, score-0.55]
</p><p>82 We use Action Bank [24] to generate tags for this dataset. [sent-282, score-0.374]
</p><p>83 Speciﬁcally, on each video and for each template action, we use the set of Action Bank action detection scores collected at different spatiotemporal scales and correlation volumes. [sent-286, score-0.415]
</p><p>84 We perform max-pooling on the scores to obtain the corresponding tag detection score. [sent-287, score-0.34]
</p><p>85 The tags and tag detection scores are generated from Action Bank, in the same way as KTH Actions. [sent-291, score-0.714]
</p><p>86 Baselines: To evaluate the efﬁcacy of LMMC, we implement three conventional clustering methods for comparison, including the k-means algorithm (KM), normalized cut (NC) [27], and spectral clustering (SC) [18]. [sent-292, score-0.424]
</p><p>87 Therefore, for a fair comparison with LMMC, they are directly performed on the data where each video is represented by a vector of tag detection scores. [sent-358, score-0.523]
</p><p>88 In order to show the beneﬁts of incorporating latent variables, we further develop a baseline called MMC by replacing the latent variable model fw (x) in Eq. [sent-362, score-0.556]
</p><p>89 This is equivalent to running an ordinary maximum margin clustering algorithm on the video data represented by tag detection scores. [sent-364, score-0.906]
</p><p>90 Performance measures: Following the convention of maximum margin clustering [32, 33, 29, 37, 38, 16, 6], we set the number of clusters to be the ground-truth number of classes for all the compared methods. [sent-374, score-0.45]
</p><p>91 This demonstrates that learning the latent presence and absence of tags can exploit rich representations of videos, and boost clustering performance. [sent-382, score-0.806]
</p><p>92 This provides evidence for the effectiveness of maximum margin clustering as well as the proposed alternating descent algorithm for optimizing the non-convex objective. [sent-386, score-0.411]
</p><p>93 6  Conclusion  We have presented a latent maximum margin framework for unsupervised clustering. [sent-389, score-0.486]
</p><p>94 By representing instances with latent variables, our method features the ability to exploit the unobserved information embedded in data. [sent-390, score-0.323]
</p><p>95 We label each cluster by the dominating video class, e. [sent-392, score-0.35]
</p><p>96 A “” sign indicates that the video label is consistent with the cluster label; otherwise, a “” sign is used. [sent-395, score-0.35]
</p><p>97 Below each video, we show the top eight inferred tags sorted by the potential calculated from Eq. [sent-397, score-0.374]
</p><p>98 We instantiate our framework with tag-based video clustering, where each video is represented by a latent tag model with latent presence and absence of video tags. [sent-400, score-1.436]
</p><p>99 video clustering with latent key segments, image clustering with latent region-of-interest, etc. [sent-404, score-1.082]
</p><p>100 Discriminative tag learning on YouTube videos with latent sub-tags. [sent-625, score-0.684]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tags', 0.374), ('mmc', 0.346), ('lmmc', 0.334), ('tag', 0.263), ('latent', 0.245), ('video', 0.218), ('yit', 0.211), ('clustering', 0.187), ('videos', 0.176), ('trecvid', 0.174), ('margin', 0.161), ('cluster', 0.132), ('med', 0.106), ('man', 0.09), ('wedding', 0.09), ('fwt', 0.087), ('ucf', 0.087), ('ceremony', 0.077), ('pur', 0.073), ('labeling', 0.07), ('clusters', 0.067), ('svm', 0.065), ('beach', 0.064), ('birthday', 0.064), ('actions', 0.063), ('animal', 0.063), ('ht', 0.062), ('wt', 0.061), ('action', 0.061), ('nmi', 0.059), ('template', 0.059), ('sc', 0.057), ('km', 0.056), ('feeding', 0.053), ('dpms', 0.051), ('vahdat', 0.051), ('ir', 0.051), ('sports', 0.051), ('conventional', 0.05), ('party', 0.049), ('board', 0.049), ('kth', 0.048), ('boat', 0.047), ('fm', 0.047), ('unsupervised', 0.045), ('resultant', 0.044), ('bike', 0.044), ('fwr', 0.044), ('woodworking', 0.044), ('yir', 0.044), ('instances', 0.043), ('bank', 0.043), ('sh', 0.043), ('nc', 0.043), ('xi', 0.042), ('supervised', 0.042), ('detection', 0.042), ('fw', 0.04), ('food', 0.039), ('indoors', 0.038), ('swinging', 0.038), ('lady', 0.038), ('ilp', 0.038), ('parade', 0.038), ('boy', 0.035), ('unobserved', 0.035), ('scores', 0.035), ('maximum', 0.035), ('variables', 0.034), ('cvpr', 0.034), ('baby', 0.033), ('hit', 0.033), ('street', 0.032), ('cacy', 0.031), ('risk', 0.03), ('assignment', 0.029), ('instantiate', 0.029), ('kitchen', 0.029), ('wood', 0.029), ('argmaxhit', 0.029), ('clapping', 0.029), ('correlative', 0.029), ('grooming', 0.029), ('hoai', 0.029), ('izadinia', 0.029), ('judgment', 0.029), ('men', 0.029), ('parkour', 0.029), ('valizadegan', 0.029), ('walking', 0.028), ('alternating', 0.028), ('balance', 0.027), ('dog', 0.027), ('balanced', 0.026), ('human', 0.026), ('score', 0.026), ('variable', 0.026), ('water', 0.026), ('landing', 0.026), ('dancing', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="148-tfidf-1" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Arash Vahdat, Greg Mori</p><p>Abstract: We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. 1</p><p>2 0.1544304 <a title="148-tfidf-2" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>Author: Özlem Aslan, Hao Cheng, Xinhua Zhang, Dale Schuurmans</p><p>Abstract: Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction. Unfortunately, such models are difﬁcult to train because inference over latent variables must be performed concurrently with parameter optimization—creating a highly non-convex problem. Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training. Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer. The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics. 1</p><p>3 0.15233055 <a title="148-tfidf-3" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>4 0.14133644 <a title="148-tfidf-4" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>Author: Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan</p><p>Abstract: Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a “relevance topic model” for jointly learning meaningful mid-level representations upon bagof-words (BoW) video representations and a classiﬁer with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectiﬁed linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efﬁcient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classiﬁcation accuracy, particularly in the case of a very small number of labeled training videos. 1</p><p>5 0.13108855 <a title="148-tfidf-5" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>Author: Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin</p><p>Abstract: This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a lowvariance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. 1</p><p>6 0.10293508 <a title="148-tfidf-6" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>7 0.085221693 <a title="148-tfidf-7" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>8 0.083183348 <a title="148-tfidf-8" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>9 0.077825896 <a title="148-tfidf-9" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>10 0.07770016 <a title="148-tfidf-10" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>11 0.07754641 <a title="148-tfidf-11" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>12 0.075308599 <a title="148-tfidf-12" href="./nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</a></p>
<p>13 0.075189158 <a title="148-tfidf-13" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>14 0.073981948 <a title="148-tfidf-14" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>15 0.071860395 <a title="148-tfidf-15" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>16 0.069955625 <a title="148-tfidf-16" href="./nips-2013-A_Latent_Source_Model_for_Nonparametric_Time_Series_Classification.html">10 nips-2013-A Latent Source Model for Nonparametric Time Series Classification</a></p>
<p>17 0.068422876 <a title="148-tfidf-17" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>18 0.06533663 <a title="148-tfidf-18" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>19 0.063612401 <a title="148-tfidf-19" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>20 0.059487879 <a title="148-tfidf-20" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.046), (2, -0.053), (3, -0.034), (4, 0.082), (5, -0.036), (6, 0.04), (7, 0.012), (8, 0.055), (9, 0.043), (10, -0.111), (11, 0.032), (12, 0.074), (13, -0.038), (14, -0.004), (15, 0.01), (16, 0.012), (17, -0.015), (18, 0.107), (19, -0.096), (20, 0.019), (21, 0.108), (22, 0.1), (23, 0.001), (24, 0.051), (25, -0.119), (26, -0.03), (27, -0.116), (28, -0.048), (29, -0.079), (30, -0.004), (31, -0.103), (32, 0.261), (33, -0.026), (34, 0.039), (35, -0.014), (36, -0.064), (37, -0.098), (38, -0.047), (39, 0.098), (40, 0.076), (41, 0.078), (42, 0.116), (43, -0.086), (44, -0.035), (45, 0.005), (46, 0.033), (47, 0.008), (48, -0.003), (49, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96669358 <a title="148-lsi-1" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Arash Vahdat, Greg Mori</p><p>Abstract: We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. 1</p><p>2 0.71397102 <a title="148-lsi-2" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>3 0.58409756 <a title="148-lsi-3" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>Author: Soravit Changpinyo, Kuan Liu, Fei Sha</p><p>Abstract: Measuring similarity is crucial to many learning tasks. To this end, metric learning has been the dominant paradigm. However, similarity is a richer and broader notion than what metrics entail. For example, similarity can arise from the process of aggregating the decisions of multiple latent components, where each latent component compares data in its own way by focusing on a different subset of features. In this paper, we propose Similarity Component Analysis (SCA), a probabilistic graphical model that discovers those latent components from data. In SCA, a latent component generates a local similarity value, computed with its own metric, independently of other components. The ﬁnal similarity measure is then obtained by combining the local similarity values with a (noisy-)OR gate. We derive an EM-based algorithm for ﬁtting the model parameters with similarity-annotated data from pairwise comparisons. We validate the SCA model on synthetic datasets where SCA discovers the ground-truth about the latent components. We also apply SCA to a multiway classiﬁcation task and a link prediction task. For both tasks, SCA attains signiﬁcantly better prediction accuracies than competing methods. Moreover, we show how SCA can be instrumental in exploratory analysis of data, where we gain insights about the data by examining patterns hidden in its latent components’ local similarity values. 1</p><p>4 0.57871968 <a title="148-lsi-4" href="./nips-2013-Deep_content-based_music_recommendation.html">85 nips-2013-Deep content-based music recommendation</a></p>
<p>Author: Aaron van den Oord, Sander Dieleman, Benjamin Schrauwen</p><p>Abstract: Automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally. Most recommender systems rely on collaborative ﬁltering. However, this approach suffers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs. In this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data. We compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantitatively and qualitatively on the Million Song Dataset. We show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal. We also show that recent advances in deep learning translate very well to the music recommendation setting, with deep convolutional neural networks signiﬁcantly outperforming the traditional approach. 1</p><p>5 0.57457048 <a title="148-lsi-5" href="./nips-2013-A_Latent_Source_Model_for_Nonparametric_Time_Series_Classification.html">10 nips-2013-A Latent Source Model for Nonparametric Time Series Classification</a></p>
<p>Author: George H. Chen, Stanislav Nikolov, Devavrat Shah</p><p>Abstract: For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justiﬁcation for the effectiveness of nearest-neighbor-like classiﬁcation of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren’t actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a “weighted majority voting” classiﬁcation rule that can be approximated by a nearest-neighbor classiﬁer. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classiﬁcation under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassiﬁcation rate as nearest-neighbor classiﬁcation while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such “trending topics” in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%. 1</p><p>6 0.57382143 <a title="148-lsi-6" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>7 0.55613714 <a title="148-lsi-7" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>8 0.54665095 <a title="148-lsi-8" href="./nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</a></p>
<p>9 0.50299269 <a title="148-lsi-9" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>10 0.44236055 <a title="148-lsi-10" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>11 0.42792004 <a title="148-lsi-11" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>12 0.41338745 <a title="148-lsi-12" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>13 0.40812898 <a title="148-lsi-13" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>14 0.39352548 <a title="148-lsi-14" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>15 0.392396 <a title="148-lsi-15" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>16 0.39015967 <a title="148-lsi-16" href="./nips-2013-Robust_Bloom_Filters_for_Large_MultiLabel_Classification_Tasks.html">279 nips-2013-Robust Bloom Filters for Large MultiLabel Classification Tasks</a></p>
<p>17 0.38827369 <a title="148-lsi-17" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<p>18 0.36889893 <a title="148-lsi-18" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>19 0.35914099 <a title="148-lsi-19" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>20 0.35792315 <a title="148-lsi-20" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.043), (18, 0.011), (19, 0.011), (33, 0.102), (34, 0.123), (41, 0.028), (49, 0.089), (56, 0.065), (70, 0.04), (81, 0.275), (85, 0.036), (89, 0.027), (93, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76705021 <a title="148-lda-1" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Arash Vahdat, Greg Mori</p><p>Abstract: We present a maximum margin framework that clusters data using latent variables. Using latent representations enables our framework to model unobserved information embedded in the data. We implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem. We instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags. Experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches. 1</p><p>2 0.62809169 <a title="148-lda-2" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>Author: Tim Roughgarden, Michael Kearns</p><p>Abstract: We consider a number of classical and new computational problems regarding marginal distributions, and inference in models specifying a full joint distribution. We prove general and efﬁcient reductions between a number of these problems, which demonstrate that algorithmic progress in inference automatically yields progress for “pure data” problems. Our main technique involves formulating the problems as linear programs, and proving that the dual separation oracle required by the ellipsoid method is provided by the target problem. This technique may be of independent interest in probabilistic inference. 1</p><p>3 0.5868367 <a title="148-lda-3" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>4 0.58087862 <a title="148-lda-4" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>5 0.58058351 <a title="148-lda-5" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>6 0.57046127 <a title="148-lda-6" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>7 0.56925732 <a title="148-lda-7" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>8 0.567837 <a title="148-lda-8" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>9 0.56775934 <a title="148-lda-9" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>10 0.56750381 <a title="148-lda-10" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>11 0.56476349 <a title="148-lda-11" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>12 0.56413686 <a title="148-lda-12" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>13 0.56365865 <a title="148-lda-13" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>14 0.56248862 <a title="148-lda-14" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>15 0.56232756 <a title="148-lda-15" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>16 0.56200546 <a title="148-lda-16" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>17 0.56076306 <a title="148-lda-17" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>18 0.56002361 <a title="148-lda-18" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>19 0.55821168 <a title="148-lda-19" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>20 0.55729342 <a title="148-lda-20" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
