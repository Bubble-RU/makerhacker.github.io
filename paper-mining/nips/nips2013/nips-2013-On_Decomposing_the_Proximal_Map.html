<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>215 nips-2013-On Decomposing the Proximal Map</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-215" href="#">nips2013-215</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>215 nips-2013-On Decomposing the Proximal Map</h1>
<br/><p>Source: <a title="nips-2013-215-pdf" href="http://papers.nips.cc/paper/4863-on-decomposing-the-proximal-map.pdf">pdf</a></p><p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>Reference: <a title="nips-2013-215-reference" href="../nips2013_reference/nips-2013-On_Decomposing_the_Proximal_Map_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. [sent-3, score-0.275]
</p><p>2 For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. [sent-4, score-0.291]
</p><p>3 Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. [sent-5, score-0.536]
</p><p>4 As real data become more and more complex, different types of regularizers, usually nonsmooth functions, have been designed. [sent-9, score-0.045]
</p><p>5 Since many interesting regularizers are nonsmooth, they are harder to optimize numerically, especially in large-scale high-dimensional settings. [sent-11, score-0.039]
</p><p>6 Thanks to recent advances [3–5], gradient-type algorithms have been generalized to take nonsmooth regularizers explicitly into account. [sent-12, score-0.068]
</p><p>7 The key step of such gradient-type algorithms is to compute the proximal map (of the nonsmooth regularizer), which is available in closed-form for some speciﬁc regularizers. [sent-14, score-0.272]
</p><p>8 However, the proximal map becomes highly nontrivial when we start to combine regularizers. [sent-15, score-0.261]
</p><p>9 The main goal of this paper is to systematically investigate when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual functions, which we simply term prox-decomposition. [sent-16, score-0.497]
</p><p>10 Our motivation comes from a few known decomposition results scattered in the literature [6–8], all in the form of our interest. [sent-17, score-0.049]
</p><p>11 After setting the context in Section 2, we motivate the decomposition rule with some justiﬁcations, as well as some cautionary results. [sent-20, score-0.056]
</p><p>12 1, we study how “invariance” of the subdifferential of one function would lead to nontrivial proxdecompositions. [sent-22, score-0.141]
</p><p>13 The generalization to cone invariance is considered in Section 3. [sent-25, score-0.082]
</p><p>14 The symbol ¯ Id stands for the identity map and the extended real line R ∪ {∞} is denoted as R. [sent-29, score-0.064]
</p><p>15 Throughout the paper we denote ∂f (x) as the subdifferential of the function f at point x. [sent-30, score-0.123]
</p><p>16 ¯ For any closed convex proper function f : H → R, we deﬁne its Moreau envelop as [11] 1 x∈H 2  ∀y ∈ H, Mf (y) = min  x−y  2  + f (x),  (1)  and the related proximal map Pf (y) = argmin 1 x − y 2  2  + f (x). [sent-33, score-0.316]
</p><p>17 When f = ιC is the indicator of some closed convex set C, the proximal map reduces to the usual projection. [sent-36, score-0.295]
</p><p>18 Pf +g = Pf ◦ Pg = Pg ◦ Pf ,  (5)  where f, g ∈ Γ0 , the set of all closed convex proper functions on H, and f ◦ g denotes the mapping composition. [sent-44, score-0.068]
</p><p>19 c] proved that P : H → H is a proximal map iff it is nonexpansive and it is the subdifferential of some convex function in Γ0 . [sent-54, score-0.452]
</p><p>20 Although the latter condition in general is not easy to verify, it reduces to monotonic increasing when H = R (note that P must be continuous). [sent-55, score-0.045]
</p><p>21 In a general Hilbert space H, we again easily conclude that the composition Pf ◦ Pg is always a nonexpansion, which means that it is “close” to be a proximal map. [sent-57, score-0.211]
</p><p>22 This justiﬁes the composition Pf ◦ Pg as a candidate for the decomposition of Pf +g . [sent-58, score-0.063]
</p><p>23 The proximal maps in this case are simply projections: Pf (x) = ( x1 +x2 , x1 +x2 ) and Pg (x) = (x1 , 0). [sent-65, score-0.194]
</p><p>24 We easily verify that the inequality 2 2 Pf (Pg (x)) − Pf (Pg (y))  2  ≤ Pf (Pg (x)) − Pf (Pg (y)), x − y  is not always true, contradiction if Pf ◦ Pg was a proximal map [11, Eq. [sent-67, score-0.264]
</p><p>25 Nevertheless, as we will see, the equality in (5) does hold in many scenarios, and an interesting theory can be suitably developed. [sent-76, score-0.039]
</p><p>26 Using the ﬁrst order optimality condition and the deﬁnition of the proximal map (2), we have Pf +g (y) − y + ∂(f + g)(Pf +g (y)) Pg (y) − y + ∂g(Pg (y)) Pf (Pg (y)) − Pg (y) + ∂f (Pf (Pg (y)))  0 0 0. [sent-80, score-0.288]
</p><p>27 Then by (9) and the subdifferential rule ∂(f + g) ⊇ ∂f + ∂g we verify that Pf (Pg (y)) satisﬁes (6), hence follows Pf +g = Pf ◦ Pg since the proximal map is single-valued. [sent-85, score-0.404]
</p><p>28 We note that a special form of our sufﬁcient condition has appeared in the proof of [8, Theorem 1], whose main result also follows immediately from our Theorem 4 below. [sent-86, score-0.085]
</p><p>29 The condition ∂(g1 +g2 ) = ∂g1 +∂g2 in Proposition 2 is purely technical; it is satisﬁed when, say g1 is continuous at a single, arbitrary point in dom g1 ∩ dom g2 . [sent-91, score-0.311]
</p><p>30 , in fact, they are the only differentiable functions that are s. [sent-115, score-0.05]
</p><p>31 2  A gauge is a positively homogeneous convex function that vanishes at the origin. [sent-131, score-0.113]
</p><p>32 On the other hand, if f, g are differentiable, then we actually have equality in (10), which is clearly necessary in this case. [sent-137, score-0.045]
</p><p>33 Since convex functions are almost everywhere differentiable (in the interior of their domain), we expect the sufﬁcient condition (10) to be necessary “almost everywhere” too. [sent-138, score-0.121]
</p><p>34 Thus we see that the key for the decomposition (5) to hold is to let the proximal map of f and the subdifferential of g “interact well” in the sense of (10). [sent-139, score-0.411]
</p><p>35 The second follows from the optimality condition Pf = (Id + ∂f )−1 . [sent-145, score-0.045]
</p><p>36 Therefore some properties of the proximal map will transfer to some properties of the function f itself, and vice versa. [sent-148, score-0.243]
</p><p>37 Pf (U x) = U Pf (x) for all unitary U iff f (U x) = f (x) for all unitary U ; iii). [sent-153, score-0.071]
</p><p>38 Pf (Qx) = QPf (x) for all permutation Q (under some ﬁxed basis) iff f is permutation invariant, that is f (Qx) = f (x) for all permutation Q. [sent-154, score-0.126]
</p><p>39 In the following, we will put some invariance assumptions on the subdifferential of g and accordingly ﬁnd the right family of f whose proximal map “respects” that invariance. [sent-155, score-0.422]
</p><p>40 This way we will meet (10) by construction therefore effortlessly have the decomposition (5). [sent-156, score-0.05]
</p><p>41 2  No Invariance  To begin with, consider ﬁrst the trivial case where no invariance on the subdifferential of g is assumed. [sent-158, score-0.193]
</p><p>42 Pf +g = Pf ◦ Pg for all g ∈ Γ0 if and only if • dim(H) ≥ 2; f ≡ c or f = ι{w} + c for some c ∈ R and w ∈ H; • dim(H) = 1 and f = ιC + c for some closed and convex set C and c ∈ R. [sent-163, score-0.052]
</p><p>43 Since x ∈ dom f is arbitrary, f is constant on its domain. [sent-168, score-0.139]
</p><p>44 We consider the other case where dim(H) ≥ 2 and dom f contains at least two points. [sent-170, score-0.139]
</p><p>45 If dom f = H, there exists z ∈ dom f such that Pf (z) = y for some y ∈ dom f , and closed convex set C ∩ dom f = ∅ with y ∈ C z. [sent-171, score-0.608]
</p><p>46 Let g = ιC we obtain Pf +g (z) ∈ C ∩ dom f while Pf (Pg (z)) = Pf (z) = y ∈ C, contradiction. [sent-172, score-0.139]
</p><p>47 Observe that the decomposition (5) is not symmetric in f and g, also reﬂected in the next result: Theorem 3. [sent-173, score-0.048]
</p><p>48 Pf +g = Pf ◦ Pg for all f ∈ Γ0 iff g is a continuous afﬁne function. [sent-175, score-0.039]
</p><p>49 4  Naturally, the next step is to put invariance assumptions on the subdifferential of g, effectively restricting the function class of g. [sent-183, score-0.179]
</p><p>50 3  Scaling Invariance  The ﬁrst invariance property we consider is scaling-invariance. [sent-186, score-0.056]
</p><p>51 What kind of convex functions have their subdifferential invariant to (positive) scaling? [sent-187, score-0.188]
</p><p>52 Assuming 0 ∈ dom g and by simple integration t  t  g(tx) − g(0) =  ∂g(sx), x ds = t · [g(x) − g(0)],  g (sx)ds = 0  0  where the last equality follows from the scaling invariance of the subdifferential of g. [sent-188, score-0.356]
</p><p>53 (automatically 0 ∈ dom g), then from deﬁnition we verify that ∂g is scaling-invariant. [sent-194, score-0.16]
</p><p>54 Consequently, the right function class for f is to have the proximal map Pf (x) = λ · x for some λ ∈ [0, 1] that may depend on x as well3 . [sent-198, score-0.243]
</p><p>55 When dim(H) = 1, ii) is equivalent as requiring f to attain its minimum at 0, in which case the implication ii) =⇒ iv), under the redundant condition that f is differentiable, was proved by Combettes and Pesquet [14, Proposition 3. [sent-212, score-0.058]
</p><p>56 Therefore there exists λ ∈ [0, 1] such that λu minimizes the Moreau envelop Mf hence we have Pf (u) = λu due to uniqueness. [sent-220, score-0.038]
</p><p>57 iii) =⇒ iv): Note ﬁrst that iii) implies 0 ∈ ∂f (0), therefore 0 ∈ dom f . [sent-221, score-0.154]
</p><p>58 Since the subdifferential of κ is scaling-invariant, iii) implies the sufﬁcient condition (10) hence iv). [sent-222, score-0.188]
</p><p>59 iv) =⇒ iii): Fix y and construct the gauge function κ(z) =  0, ∞,  if z = λ · y for some λ ≥ 0 . [sent-223, score-0.066]
</p><p>60 On the other hand, 1 Mf +κ (y) = min 2 x − y x  3  2 2  + f (x) + κ(x) = minλ≥0  1 2  Note that λ ≤ 1 is necessary since any proximal map is nonexpansive. [sent-225, score-0.243]
</p><p>61 iii) =⇒ ii): First note that iii) implies that Pf (0) = 0 hence 0 ∈ ∂f (0), in particular, 0 ∈ dom f . [sent-231, score-0.171]
</p><p>62 Using the ﬁrst order optimality condition for the proximal map we have 1 0 ∈ λx − x + ∂f (λx), that is ( λ − 1)y ∈ ∂f (y) for each y ∈ ran(Pf ) = H due to our assumption dom f = H. [sent-238, score-0.427]
</p><p>63 For the case when dom f ⊂ H, we consider the proximal average [16] 1 g = A(f, q) = [( 2 (f ∗ + q)∗ + 1 q)∗ − q]∗ , 4  (12)  where q = 1 · 2 . [sent-240, score-0.318]
</p><p>64 Importantly, since q is deﬁned on the whole space, the proximal average g has 2 1 1 full domain too [16, Corollary 4. [sent-241, score-0.179]
</p><p>65 It is easy to check that i) is preserved under taking the Fenchel conjugation (note that the convexity of f implies that of h). [sent-245, score-0.041]
</p><p>66 Let dim(H) ≥ 2, C ⊆ H be a closed convex set that contains the origin. [sent-252, score-0.052]
</p><p>67 Then the projection onto C is simply a shrinkage towards the origin iff C is a ball (of the norm · ). [sent-253, score-0.069]
</p><p>68 In many applications, in addition to the regularizer κ (usually a gauge), one adds the 2 regularizer λq either for stability or grouping effect or strong 2 convexity. [sent-257, score-0.064]
</p><p>69 This incurs no computational cost in the sense of computing the proximal map: We easily 1 1 compute that Pλq = λ+1 Id. [sent-258, score-0.179]
</p><p>70 By Theorem 4, for any gauge κ, Pκ+λq = λ+1 Pκ , whence it is also clear that adding an extra 2 regularizer tends to double “shrink” the solution. [sent-259, score-0.098]
</p><p>71 In particular, let H = Rd and take κ = · 1 (the sum of absolute values) we recover the proximal map for the elastic-net regularizer [17]. [sent-260, score-0.286]
</p><p>72 function f ∈ Γ0 satisﬁes any item of Theorem 4 iff it is a positive multiple of the norm · . [sent-271, score-0.069]
</p><p>73 convex function f that satisﬁes Pf +κ = Pf ◦ Pκ for all gauge κ. [sent-275, score-0.092]
</p><p>74 Then P  m i=1  ·  gi  =P  ·  g1  ◦ ··· ◦ P  ·  gm  ,  where we arrange the groups so that gi ⊂ gj =⇒ i > j, and the notation Hilbertian norm that is restricted to the coordinates indexed by the group gi . [sent-283, score-0.342]
</p><p>75 ·  gi  denotes the  m  Proof: Let f = · g1 and κ = i=2 · gi . [sent-284, score-0.14]
</p><p>76 By the tree-structured assumption we can partition κ = κ1 + κ2 , where gi ⊂ g1 for all gi appearing in κ1 while gj ∩ g1 = ∅ for all gj appearing in κ2 . [sent-288, score-0.224]
</p><p>77 On the other hand, due to the non-overlapping property, nothing will be affected by adding κ2 , thus P  m i=1  ·  =P  gi  ·  g1  ◦P  m i=2  ·  gi  . [sent-291, score-0.14]
</p><p>78 We can clearly iterate the argument to unravel the proximal map as claimed. [sent-292, score-0.263]
</p><p>79 For notational clarity, we have chosen not to incorporate weights in the sum of group seminorms: Such can be absorbed into the seminorm and the corollary clearly remains intact. [sent-293, score-0.088]
</p><p>80 Our proof also reveals the fundamental reason why Corollary 3 is true: The 2 norm admits the decomposition (5) for any gauge g! [sent-294, score-0.153]
</p><p>81 4  Cone Invariance  In the previous subsection, we restricted the subdifferential of g to be constant along each ray. [sent-297, score-0.123]
</p><p>82 Speciﬁcally, consider the gauge function κ(x) = max aj , x ,  (13)  j∈J  where J is a ﬁnite index set and each aj ∈ H. [sent-299, score-0.116]
</p><p>83 Such polyhedral gauge functions have become extremely important due to the work of Chandrasekaran et al. [sent-300, score-0.115]
</p><p>84 Deﬁne the polyhedral cones Kj = {x ∈ H : aj , x = κ(x)}. [sent-302, score-0.042]
</p><p>85 (15)  In other words, each cone Kj is “ﬁxed” under the proximal map of f . [sent-305, score-0.269]
</p><p>86 Denote E a collection of pairs (m, n), and deﬁne the total variational norm x tv = 4 {m,n}∈E wm,n |xm − xn |, where wm,n ≥ 0. [sent-308, score-0.068]
</p><p>87 Then for any permutation invariant function f , Pf +  ·  tv  = Pf ◦ P  ·  tv  . [sent-309, score-0.094]
</p><p>88 Since f is permutation invariant, its proximal map Pf (x) maintains the order of x, hence we establish (15). [sent-312, score-0.289]
</p><p>89 1 p  We call the permutation invariant function f symmetric if ∀x, f (|x|) = f (x), where | · | denotes the componentwise absolute value. [sent-320, score-0.069]
</p><p>90 The proof for the next corollary is almost the same as that of Corollary 4, except that we also use the fact sign([Pf (x)]m ) = sign(xm ) for symmetric functions. [sent-321, score-0.111]
</p><p>91 As in Corollary 4, deﬁne the norm x oct = {m,n}∈E wm,n max{|xm |, |xn |}. [sent-323, score-0.112]
</p><p>92 Then for any symmetric function f , Pf + · oct = Pf ◦ P · oct . [sent-324, score-0.181]
</p><p>93 4  All we need is the weaker condition: For all {m, n} ∈ E, xm ≥ xn =⇒ [Pf (x)]m ≥ [Pf (x)]n . [sent-325, score-0.071]
</p><p>94 This norm · oct is proposed in [21] for feature grouping. [sent-327, score-0.112]
</p><p>95 The proximal map P · oct is derived in [22], which turns out to be another decomposition result. [sent-329, score-0.356]
</p><p>96 The proof is by recursion: Write · oct = f + g, where f = κ|I| . [sent-337, score-0.108]
</p><p>97 Note that the subdifferential of g depends only on the ordering and sign of the ﬁrst |I| − 1 coordinates while the proximal map of f preserves the ordering and sign of the ﬁrst |I| − 1 coordinates (due to symmetry). [sent-338, score-0.456]
</p><p>98 If we pre-sort x, the individual proximal maps Pκi (x) become easy to compute sequentially and we recover the algorithm in [22] with some bookkeeping. [sent-339, score-0.233]
</p><p>99 As in Corollary 3, let G ⊆ 2I be a collection of tree-structured groups, then P m · gi ,k = P · g1 ,k ◦ · · · ◦ P · gm ,k , i=1 k  where we arrange the groups so that gi ⊂ gj =⇒ i > j, and x gi ,k = j=1 |xgi |[j] is the sum of the k (absolute-value) largest elements in the group gi , i. [sent-341, score-0.363]
</p><p>100 Therefore · as symmetric and the rest follows the proof of Corollary 5. [sent-349, score-0.043]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pf', 0.81), ('pg', 0.399), ('proximal', 0.179), ('dom', 0.139), ('subdifferential', 0.123), ('dim', 0.114), ('oct', 0.082), ('mf', 0.077), ('gi', 0.07), ('corollary', 0.068), ('gauge', 0.066), ('map', 0.064), ('kf', 0.059), ('invariance', 0.056), ('ph', 0.055), ('iii', 0.05), ('moreau', 0.045), ('kj', 0.042), ('iv', 0.041), ('hilbertian', 0.041), ('xm', 0.04), ('proposition', 0.039), ('regularizers', 0.039), ('iff', 0.039), ('ii', 0.038), ('differentiable', 0.034), ('condition', 0.033), ('composition', 0.032), ('regularizer', 0.032), ('decomposition', 0.031), ('gj', 0.03), ('norm', 0.03), ('permutation', 0.029), ('nonsmooth', 0.029), ('pq', 0.026), ('appeared', 0.026), ('cone', 0.026), ('sign', 0.026), ('fix', 0.026), ('closed', 0.026), ('proof', 0.026), ('convex', 0.026), ('id', 0.025), ('cautionary', 0.025), ('pnf', 0.025), ('aj', 0.025), ('implication', 0.025), ('equality', 0.025), ('theorem', 0.024), ('invariant', 0.023), ('verify', 0.021), ('tv', 0.021), ('sx', 0.021), ('envelop', 0.021), ('arrange', 0.021), ('qx', 0.021), ('nonexpansive', 0.021), ('homogeneous', 0.021), ('clearly', 0.02), ('fenchel', 0.019), ('yaoliang', 0.019), ('effortlessly', 0.019), ('remark', 0.019), ('satis', 0.019), ('coordinates', 0.019), ('pmf', 0.018), ('nontrivial', 0.018), ('scattered', 0.018), ('symmetric', 0.017), ('hence', 0.017), ('xn', 0.017), ('polyhedral', 0.017), ('become', 0.016), ('unitary', 0.016), ('groups', 0.016), ('prevalent', 0.016), ('gm', 0.016), ('falling', 0.016), ('functions', 0.016), ('ei', 0.015), ('maps', 0.015), ('implies', 0.015), ('convexity', 0.014), ('es', 0.014), ('trivial', 0.014), ('weaker', 0.014), ('hold', 0.014), ('ds', 0.013), ('everywhere', 0.012), ('decomposes', 0.012), ('appearing', 0.012), ('numerically', 0.012), ('formula', 0.012), ('easy', 0.012), ('optimality', 0.012), ('af', 0.012), ('nition', 0.012), ('orthonormal', 0.012), ('importantly', 0.012), ('recover', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="215-tfidf-1" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>2 0.28971586 <a title="215-tfidf-2" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>3 0.1037762 <a title="215-tfidf-3" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><p>4 0.078594781 <a title="215-tfidf-4" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>5 0.074286282 <a title="215-tfidf-5" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>Author: Daniel S. Levine, Jonathan P. How</p><p>Abstract: We consider the sensor selection problem on multivariate Gaussian distributions where only a subset of latent variables is of inferential interest. For pairs of vertices connected by a unique path in the graph, we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efﬁciently from the output of message passing algorithms. We integrate these decompositions into a computationally efﬁcient greedy selector where the computational expense of quantiﬁcation can be distributed across nodes in the network. Experimental results demonstrate the comparative efﬁciency of our algorithms for sensor selection in high-dimensional distributions. We additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that, when such a valid augmentation exists, is applicable for any distribution with nuisances. 1</p><p>6 0.062755518 <a title="215-tfidf-6" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>7 0.058943778 <a title="215-tfidf-7" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>8 0.056949258 <a title="215-tfidf-8" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>9 0.054732662 <a title="215-tfidf-9" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>10 0.051691238 <a title="215-tfidf-10" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>11 0.046056911 <a title="215-tfidf-11" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>12 0.042880118 <a title="215-tfidf-12" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>13 0.040598962 <a title="215-tfidf-13" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>14 0.040073585 <a title="215-tfidf-14" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>15 0.036699496 <a title="215-tfidf-15" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>16 0.03538752 <a title="215-tfidf-16" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>17 0.035188917 <a title="215-tfidf-17" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>18 0.03312847 <a title="215-tfidf-18" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>19 0.033046279 <a title="215-tfidf-19" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>20 0.031031936 <a title="215-tfidf-20" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.08), (1, 0.026), (2, 0.056), (3, 0.048), (4, 0.003), (5, 0.045), (6, -0.07), (7, -0.025), (8, 0.002), (9, -0.031), (10, 0.008), (11, 0.041), (12, -0.005), (13, -0.104), (14, -0.059), (15, -0.006), (16, 0.028), (17, -0.01), (18, 0.044), (19, 0.016), (20, -0.014), (21, 0.057), (22, 0.078), (23, 0.076), (24, -0.007), (25, -0.093), (26, 0.083), (27, 0.028), (28, 0.054), (29, 0.221), (30, -0.17), (31, 0.078), (32, -0.036), (33, -0.08), (34, -0.127), (35, -0.112), (36, 0.016), (37, 0.141), (38, 0.051), (39, 0.039), (40, 0.056), (41, -0.016), (42, -0.019), (43, -0.03), (44, -0.008), (45, 0.09), (46, 0.016), (47, -0.142), (48, -0.139), (49, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97042179 <a title="215-lsi-1" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>2 0.87277132 <a title="215-lsi-2" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>3 0.65185547 <a title="215-lsi-3" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>4 0.44222721 <a title="215-lsi-4" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>5 0.43225023 <a title="215-lsi-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>6 0.43139893 <a title="215-lsi-6" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>7 0.43068504 <a title="215-lsi-7" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>8 0.41588658 <a title="215-lsi-8" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>9 0.37665302 <a title="215-lsi-9" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>10 0.36491278 <a title="215-lsi-10" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>11 0.31176627 <a title="215-lsi-11" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>12 0.27116504 <a title="215-lsi-12" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>13 0.26862407 <a title="215-lsi-13" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>14 0.26109275 <a title="215-lsi-14" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>15 0.26067236 <a title="215-lsi-15" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>16 0.25391906 <a title="215-lsi-16" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>17 0.24960621 <a title="215-lsi-17" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>18 0.24318898 <a title="215-lsi-18" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>19 0.22987956 <a title="215-lsi-19" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>20 0.22560917 <a title="215-lsi-20" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.02), (33, 0.117), (34, 0.072), (41, 0.04), (49, 0.034), (56, 0.106), (70, 0.065), (85, 0.03), (89, 0.038), (93, 0.333), (95, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97316766 <a title="215-lda-1" href="./nips-2013-Understanding_Dropout.html">339 nips-2013-Understanding Dropout</a></p>
<p>Author: Pierre Baldi, Peter J. Sadowski</p><p>Abstract: Dropout is a relatively new algorithm for training neural networks which relies on stochastically “dropping out” neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function. 1</p><p>2 0.94268233 <a title="215-lda-2" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>Author: Hristo S. Paskov, Robert West, John C. Mitchell, Trevor Hastie</p><p>Abstract: This paper addresses the problem of unsupervised feature learning for text data. Our method is grounded in the principle of minimum description length and uses a dictionary-based compression scheme to extract a succinct feature set. Specifically, our method ﬁnds a set of word k-grams that minimizes the cost of reconstructing the text losslessly. We formulate document compression as a binary optimization task and show how to solve it approximately via a sequence of reweighted linear programs that are efﬁcient to solve and parallelizable. As our method is unsupervised, features may be extracted once and subsequently used in a variety of tasks. We demonstrate the performance of these features over a range of scenarios including unsupervised exploratory analysis and supervised text categorization. Our compressed feature space is two orders of magnitude smaller than the full k-gram space and matches the text categorization accuracy achieved in the full feature space. This dimensionality reduction not only results in faster training times, but it can also help elucidate structure in unsupervised learning tasks and reduce the amount of training data necessary for supervised learning. 1</p><p>3 0.9403491 <a title="215-lda-3" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>Author: Carlos J. Becker, Christos M. Christoudias, Pascal Fua</p><p>Abstract: A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-speciﬁc decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for speciﬁc a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a signiﬁcant improvement over the state of the art. 1</p><p>4 0.92074543 <a title="215-lda-4" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>Author: Huahua Wang, Arindam Banerjee, Cho-Jui Hsieh, Pradeep Ravikumar, Inderjit Dhillon</p><p>Abstract: We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in columnblocks and only involves elementwise operations and parallel matrix multiplications. We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efﬁciency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores. 1</p><p>5 0.90272045 <a title="215-lda-5" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>Author: Andriy Mnih, Koray Kavukcuoglu</p><p>Abstract: Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and ﬁnd that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones. 1</p><p>same-paper 6 0.83406574 <a title="215-lda-6" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>7 0.76991987 <a title="215-lda-7" href="./nips-2013-A_Novel_Two-Step_Method_for_Cross_Language_Representation_Learning.html">12 nips-2013-A Novel Two-Step Method for Cross Language Representation Learning</a></p>
<p>8 0.75225264 <a title="215-lda-8" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>9 0.73753273 <a title="215-lda-9" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>10 0.73035735 <a title="215-lda-10" href="./nips-2013-Decision_Jungles%3A_Compact_and_Rich_Models_for_Classification.html">82 nips-2013-Decision Jungles: Compact and Rich Models for Classification</a></p>
<p>11 0.71632689 <a title="215-lda-11" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>12 0.70414633 <a title="215-lda-12" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>13 0.69834977 <a title="215-lda-13" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>14 0.69763696 <a title="215-lda-14" href="./nips-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.html">96 nips-2013-Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>15 0.69384974 <a title="215-lda-15" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>16 0.69149983 <a title="215-lda-16" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>17 0.67904007 <a title="215-lda-17" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>18 0.67512423 <a title="215-lda-18" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>19 0.66529727 <a title="215-lda-19" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>20 0.65814263 <a title="215-lda-20" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
