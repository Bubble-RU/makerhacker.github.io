<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-240" href="#">nips2013-240</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</h1>
<br/><p>Source: <a title="nips-2013-240-pdf" href="http://papers.nips.cc/paper/5147-optimization-learning-and-games-with-predictable-sequences.pdf">pdf</a></p><p>Author: Sasha Rakhlin, Karthik Sridharan</p><p>Abstract: We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror Prox algorithm for ofﬂine optimization, prove an extension to H¨ lder-smooth o functions, and apply the results to saddle-point type problems. Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a ﬁnite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T )￿T ). This addresses a question of Daskalakis et al [6]. Further, we consider a partial information version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem. 1</p><p>Reference: <a title="nips-2013-240-reference" href="../nips2013_reference/nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Optimization, Learning, and Games with Predictable Sequences Alexander Rakhlin University of Pennsylvania  Karthik Sridharan University of Pennsylvania  Abstract We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. [sent-1, score-0.192]
</p><p>2 Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a ﬁnite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T )￿T ). [sent-3, score-0.516]
</p><p>3 We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem. [sent-6, score-0.108]
</p><p>4 1  Introduction  Recently, no-regret algorithms have received increasing attention in a variety of communities, including theoretical computer science, optimization, and game theory [3, 1]. [sent-7, score-0.078]
</p><p>5 The wide applicability of these algorithms is arguably due to the black-box regret guarantees that hold for arbitrary sequences. [sent-8, score-0.135]
</p><p>6 However, such regret guarantees can be loose if the sequence being encountered is not “worst-case”. [sent-9, score-0.166]
</p><p>7 For instance, in some applications of online methods, the sequence comes from an additional computation done by the learner, thus being far from arbitrary. [sent-11, score-0.06]
</p><p>8 One way to formally capture the partially benign nature of data is through a notion of predictable sequences [11]. [sent-12, score-0.229]
</p><p>9 First, we show that the Mirror Prox method [9], designed for optimizing non-smooth structured saddle-point problems, can be viewed as an instance of the predictable sequence approach. [sent-14, score-0.215]
</p><p>10 Predictability in this case is due precisely to smoothness of the inner optimization part and the saddle-point structure of the problem. [sent-15, score-0.058]
</p><p>11 Second, we address the question raised in [6] about existence of “simple” algorithms that converge ˜ at the rate of O(T −1 ) when employed in an uncoupled manner by players in a zero-sum ﬁnite matrix game, yet maintain the usual O(T −1￿2 ) rate against arbitrary sequences. [sent-17, score-0.413]
</p><p>12 We give a positive answer and exhibit a fully adaptive algorithm that does not require the prior knowledge of whether the other player is collaborating. [sent-18, score-0.341]
</p><p>13 Here, the additional predictability comes from the fact that both players attempt to converge to the minimax value. [sent-19, score-0.411]
</p><p>14 We also tackle a partial information version of the problem where the player has only access to the real-valued payoff of the mixed actions played by the two players on each round rather than the entire vector. [sent-20, score-0.785]
</p><p>15 Our third application is to convex programming: optimization of a linear function subject to convex constraints. [sent-21, score-0.164]
</p><p>16 This problem often arises in theoretical computer science, and we show that the idea of 1  predictable sequences can be used here too. [sent-22, score-0.212]
</p><p>17 2  Online Learning with Predictable Gradient Sequences  Let us describe the online convex optimization (OCO) problem and the basic algorithm studied in [4, 11]. [sent-24, score-0.126]
</p><p>18 , T , the learner makes a prediction ft ∈ F and observes a convex function Gt on F. [sent-29, score-0.687]
</p><p>19 The objective is to keep regret T 1 G (f ) − Gt (f ∗ ) small for any f ∗ ∈ F. [sent-30, score-0.116]
</p><p>20 Suppose that at the beginning of every round t, the learner has access to Mt , a vector computable based on the past observations or side information. [sent-35, score-0.079]
</p><p>21 The method adheres to the OCO protocol since Mt is available at the beginning of round t, and ∇Gt (ft ) becomes available after the prediction ft is made. [sent-37, score-0.635]
</p><p>22 Let R ∶ B → R be a 1-strongly convex function on F with respect to some norm ￿ ⋅ ￿, and let ￿ ⋅ ￿∗ denote the dual norm. [sent-41, score-0.083]
</p><p>23 ∗  When applying the lemma, we will often use the simple fact that ⇢ 1 2 2 ￿∇t − Mt ￿∗ ￿gt − ft ￿ = inf ￿ ￿∇t − Mt ￿∗ + ￿gt − ft ￿ ￿ . [sent-43, score-1.23]
</p><p>24 2 2⇢ ⇢>0  (3)  In particular, by setting ⇢ = ⌘, we obtain the (unnormalized) regret bound of ⌘ −1 R2 + ￿ 2 2 (⌘￿2) ∑T ￿∇t − Mt ￿∗ , which is R 2 ∑T ￿∇t − Mt ￿∗ by choosing ⌘ optimally. [sent-44, score-0.116]
</p><p>25 Then regret of the Optimistic Mirror Descent algorithm is upper ￿ 2 bounded by 3. [sent-47, score-0.116]
</p><p>26 t=1  These results indicate that tighter regret bounds are possible if one can guess the next gradient ∇t by computing Mt . [sent-49, score-0.139]
</p><p>27 One such case arises in ofﬂine optimization of a smooth function, whereby the previous gradient turns out to be a good proxy for the next one. [sent-50, score-0.135]
</p><p>28 In this optimization setting, no guessing of Mt is needed: we may simply query the oracle for the gradient and set Mt = ∇G(gt−1 ). [sent-52, score-0.055]
</p><p>29 The Optimistic Mirror Descent then becomes ft = argmin ⌘t ￿f, ∇G(gt−1 )￿ + DR (f, gt−1 ) , gt = argmin ⌘t ￿g, ∇G(ft )￿ + DR (g, gt−1 ) f ∈F  g∈F  2  which can be recognized as the Mirror Prox method, due to Nemirovski [9]. [sent-53, score-1.05]
</p><p>30 (3) and ⇢ = ⌘ = 1￿H immediately yields a bound T  ∗ 2 ￿ G(ft ) − G(f ) ≤ HR ,  t=1  1 ¯ ¯ which implies that the average fT = T ∑T ft satisﬁes G(fT ) − G(f ∗ ) ≤ HR2 ￿T , a known bound t=1 for Mirror Prox. [sent-56, score-0.577]
</p><p>31 We now extend this result to arbitrary ↵-H¨ lder smooth functions, that is convex o functions G such that ￿∇G(f ) − ∇G(g)￿∗ ≤ H￿f − g￿↵ for all f, g ∈ F. [sent-57, score-0.165]
</p><p>32 Let F be a convex set in a Banach space B and let R ∶ B → R be a 1-strongly convex function on F with respect to some norm ￿ ⋅ ￿. [sent-59, score-0.149]
</p><p>33 Let G be a convex ↵-H¨ lder smooth function with o ¯ = 1 ∑T ft of the trajectory given by Optimistic constant H > 0 and ↵ ∈ [0, 1]. [sent-60, score-0.742]
</p><p>34 Then the average fT T t=1 Mirror Descent Algorithm enjoys  ¯ G(fT ) − inf G(f ) ≤  8HR1+↵  f ∈F  where R ≥ 0 is such that supf ∈F DR (f, g0 ) ≤ R. [sent-61, score-0.076]
</p><p>35 T  1+↵ 2  This result provides a smooth interpolation between the T −1￿2 rate at ↵ = 0 (that is, no predictability of the gradient is possible) and the T −1 rate when the smoothness structure allows for a dramatic speed up with a very simple modiﬁcation of the original Mirror Descent. [sent-62, score-0.212]
</p><p>36 3  Structured Optimization  In this section we consider the structured optimization problem argmin G(f ) f ∈F  where G(f ) is of the form G(f ) = supx∈X (f, x) with (⋅, x) convex for every x ∈ X and (f, ⋅) concave for every f ∈ F. [sent-63, score-0.172]
</p><p>37 While G itself need not be smooth, it has been recognized that the structure can be exploited to improve rates of optimization if the function is smooth [10]. [sent-65, score-0.136]
</p><p>38 From the point of view of online learning, we will see that the optimization problem of the saddle point type can be solved by playing two online convex optimization algorithms against each other (henceforth called Players I and II). [sent-66, score-0.267]
</p><p>39 , fT by using a regret-minimization algorithm, such that 1 T 1 T 1 ￿ (ft , xt ) − inf ￿ (f, xt ) ≤ Rate (x1 , . [sent-70, score-0.41]
</p><p>40 , xT with  1 T 1 T 2 ￿ (− (ft , xt )) − inf ￿ (− (ft , x)) ≤ Rate (f1 , . [sent-76, score-0.243]
</p><p>41 [7]), inf f  1 T ¯ ￿ (f, xt ) ≤ inf (f, xT ) ≤ sup inf (f, x) T t=1 x f f  ¯ ≤ inf sup (f, x) ≤ sup ￿fT , x￿ ≤ sup f  ¯ where fT =  sup x∈X  1 T  T ¯ ∑t=1 ft and xT =  1 T  x  x  T ∑t=1 xt . [sent-82, score-1.52]
</p><p>42 By adding (4) and (5), we have  x  1 T ￿ (ft , x) T t=1  1 T 1 T 1 2 ￿ (ft , x) − inf ￿ (f, xt ) ≤ Rate (x1 , . [sent-83, score-0.243]
</p><p>43 , fT ) T t=1 f ∈F T t=1  (6)  which sandwiches the previous sequence of inequalities up to the sum of regret rates and implies ¯ near-optimality of fT and xT . [sent-89, score-0.17]
</p><p>44 Suppose both players employ the Optimistic Mirror Descent algorithm with, respectively, predictable sequences Mt1 and Mt2 , 1-strongly convex functions R1 on F (w. [sent-91, score-0.583]
</p><p>45 Let {ft } and {xt } denote the primary sequences of the players while let {gt }, {yt } denote the secondary. [sent-98, score-0.353]
</p><p>46 We obtain the following corollary: ∶ F × X ￿ R is H¨ lder smooth in the following sense: o  Corollary 5. [sent-101, score-0.099]
</p><p>47 Suppose both players employ Optimistic Mirror Descent with Mt1 = ∇f (gt−1 , yt−1 ) and Mt2 = ∇x (gt−1 , yt−1 ), where {gt } and {yt } 2 are the secondary sequences updated by the two algorithms, and with step sizes ⌘ = ⌘ ′ = (R1 + 2 R2 )  1− 2  (2H)−1 ￿ T ￿ 2  −1 2  . [sent-104, score-0.353]
</p><p>48 Such a coupling of the upper bounds on regret of the two players can be seen as leading to faster rates under the appropriate assumptions, and this idea will be exploited to a great extent in the proofs of the next section. [sent-106, score-0.443]
</p><p>49 4  Zero-sum Game and Uncoupled Dynamics  The notions of a zero-sum matrix game and a minimax equilibrium are arguably the most basic and important notions of game theory. [sent-107, score-0.277]
</p><p>50 The tight connection between linear programming and minimax equilibrium suggests that there might be simple dynamics that can lead the two players of the game to eventually converge to the equilibrium value. [sent-108, score-0.585]
</p><p>51 Existence of such simple or natural dynamics is of interest in behavioral economics, where one asks whether agents can discover static solution concepts of the game iteratively and without extensive communication. [sent-109, score-0.1]
</p><p>52 The two players aim to ¯¯ ¯ x ﬁnd a pair of near-optimal mixed strategies (f , x) ∈ n × m such that f T A¯ is close to the minimax value minf ∈ n maxx∈ m f T Ax, where n is the probability simplex over n actions. [sent-111, score-0.446]
</p><p>53 It is well-known (and follows immediately from (6)) that the players can compute near-optimal strategies by simply playing no-regret algorithms [7]. [sent-113, score-0.375]
</p><p>54 More precisely, on round t, the players I and II “predict” the mixed strategies ft and xt and observe Axt and ftT A, respectively. [sent-114, score-1.201]
</p><p>55 While black-box regret minimization algorithms, such as Exponential Weights, immediately yield O(T −1￿2 ) convergence rates, Daskalakis et al [6] asked whether faster methods exist. [sent-115, score-0.156]
</p><p>56 The authors of [6] exhibited a near-optimal algorithm that, if used by both players, yields a pair of 4  +(log(m+n)) mixed strategies that constitutes an O ￿ log(m+n)(log TT  3￿2  )  ￿-approximate minimax equi-  librium. [sent-117, score-0.141]
</p><p>57 Furthermore, the method has a regret bound of the same order as Exponential Weights when faced with an arbitrary sequence. [sent-118, score-0.135]
</p><p>58 Furthermore, by choosing the step size adaptively, the same method guarantees the typical O(T −1￿2 ) regret if not faced with a compliant player, thus ensuring robustness. [sent-125, score-0.135]
</p><p>59 1, we analyze the “ﬁrst-order information” version of the problem, as described above: upon playing the respective mixed strategies ft and xt on round t, Player I observes Axt and Player II observes ftT A. [sent-127, score-0.961]
</p><p>60 2, we consider an interesting extension to partial information, whereby the players submit their moves ft , xt but only observe the real value ftT Axt . [sent-129, score-1.108]
</p><p>61 Other than the “mixing in” of the uniform distribution, the algorithm for both players is simply the Optimistic Mirror Descent with the (negative) entropy function. [sent-134, score-0.305]
</p><p>62 In fact, the step of mixing in the uniform distribution is only needed when some coordinate of gt (resp. [sent-135, score-0.359]
</p><p>63 Furthermore, this step is also not needed if none of the players deviate from the prescribed method. [sent-137, score-0.326]
</p><p>64 In such a case, the resulting algorithm is simply the constant step-size Exponential Weights ft (i) ∝ exp{−⌘ ∑t−2 [Axs−1 ]i + 2⌘[Axt−1 ]i }, but with a factor 2 in front of the latest loss vector! [sent-138, score-0.577]
</p><p>65 T Furthermore, if only one player (say, Player I) follows the above algorithm, her regret against any sequence x1 , . [sent-142, score-0.46]
</p><p>66 O￿ (9) T ￿ ￿ t=1 ￿￿ 5  √ In particular, this implies the worst-case regret of O ￿ log(nT ) ￿ in the general setting of online linear T optimization. [sent-146, score-0.144]
</p><p>67 We remark that (9) can give intermediate rates for regret in the case that the second player deviates from the prescribed strategy but produces “stable” moves. [sent-147, score-0.494]
</p><p>68 For instance, if the second player employs a mirror descent algorithm (or Follow the Regularized Leader / Exponential Weights method) with step size ⌘, one can typically show stability ￿xt − xt−1 ￿ = O(⌘). [sent-148, score-0.539]
</p><p>69 In this case, (9) yields the rate log O ￿ ⌘ √T T ￿ for the ﬁrst player. [sent-149, score-0.072]
</p><p>70 A typical setting of ⌘ ∝ T −1￿2 for the second player still ensures the O(log T ￿T ) regret for the ﬁrst player. [sent-150, score-0.428]
</p><p>71 The reason for the extra step of “mixing in” the uniform distribution stems from the goal of having an adaptive and robust method that still attains O(T −1￿2 ) regret if the other player deviates from using the algorithm. [sent-152, score-0.502]
</p><p>72 If one is only interested in the dynamics when both players cooperate, this step is not necessary, and in this case the extraneous log T factor disappears from the above bound, leading to the O ￿ log n+log m ￿ convergence. [sent-153, score-0.434]
</p><p>73 It is possible that the doubling trick or the analysis of Auer et al [2] (who encountered the same problem for the Exponential Weights algorithm) can remove the extra log T factor while still preserving the regret minimization property. [sent-156, score-0.243]
</p><p>74 We also remark that Rmax is small when R1 is instead the p-norm; hence, the use of this regularizer avoids the extraneous logarithmic in T factor while still preserving the logarithmic dependence on n and m. [sent-157, score-0.059]
</p><p>75 Recall that the matrix A is not known to the players, yet we are interested in ﬁnding ✏-optimal minimax strategies. [sent-161, score-0.066]
</p><p>76 On each round, the two players choose mixed strategies ft ∈ n and xt ∈ m , respectively, and observe ftT Axt . [sent-162, score-1.143]
</p><p>77 Now the question is, how many such observations do we need to get to an ✏-optimal minimax strategy? [sent-163, score-0.066]
</p><p>78 The speciﬁc setting we consider below requires that on each round t, the two players play four times, and that these four plays are -close to each other (that is, ￿fti − ftj ￿1 ≤ for i, j ∈ {1, . [sent-165, score-0.42]
</p><p>79 Interestingly, up to logarithmic factors, the fast rate of the previous section is possible even in this scenario, but we do require the knowledge of the number of actions of the opposing player (or, an upper bound on this number). [sent-169, score-0.383]
</p><p>80 We leave it as an open problem the question of whether one can attain the 1￿T -type rate with only one play per round. [sent-170, score-0.068]
</p><p>81 Furthermore, if only one player (say, Player I) follows the above algorithm, her regret against any sequence x1 , . [sent-182, score-0.46]
</p><p>82 5  Approximate Smooth Convex Programming  In this section we show how one can use the structured optimization results from Section 3 for approximately solving convex programming problems. [sent-187, score-0.159]
</p><p>83 c￿ f  (10)  ∀i ∈ [d], Gi (f ) ≤ 1  where G is a convex set and each Gi is an H-smooth convex function. [sent-190, score-0.132]
</p><p>84 The convex programming problem in (10) can now be reformulated as d  argmin max Gi (f ) = argmin sup ￿ x(i)Gi (f ) . [sent-193, score-0.279]
</p><p>85 We may think of the ﬁrst player as aiming to minimize the above expression over F, while the second player maximizes over a mixture of constraints with the aim of violating at least one of them. [sent-195, score-0.624]
</p><p>86 Consider the solution where ↵ =  T ∑t=1 ft ∈ F is the average of the trajectory of the procedure in Lemma 4 2 for the optimization problem (11). [sent-200, score-0.609]
</p><p>87 Set ⌘ = argmin ￿ B + ⌘ log d ￿, ⌘ ′ = ⌘ − H, ⌘ 1−⌘H  = such that Mt1  ✏ ✏+  ¯ and fT =  ˆ ¯ fT = (1 − ↵)fT + ↵f0  1 T  d ∑i=1 yt−1 (i)∇Gi (gt−1 )  and  Mt2  T>  ⌘≤H −1  = (G1 (gt−1 ), . [sent-203, score-0.096]
</p><p>88 Let number of iterations T be  1 B 2 ⌘ log d inf ￿ + ￿ ✏ ⌘≤H −1 ⌘ 1 − ⌘H 7  ˆ We then have that fT ∈ G satisﬁes all d constraints and is ✏ -approximate, that is ˆ c￿ fT ≥ ￿1 −  ✏  ￿F∗ . [sent-207, score-0.117]
</p><p>89 Lemma 8 tells us that using the predictable sequences approach for the two players, one can obtain an ✏ -approximate solution to the smooth convex programming problem in number of iterations at most order 1￿✏. [sent-208, score-0.381]
</p><p>90 T2 ) is the time complexity for single update of the predictable sequence algorithm of Player I (resp. [sent-210, score-0.213]
</p><p>91 The Max Flow problem can be seen as an instance of a convex (linear) programming problem, and we apply the proposed algorithm for structured optimization to obtain an approximate solution. [sent-215, score-0.159]
</p><p>92 Applying the procedure for smooth convex programming from Lemma 8 to the Max Flow problem with f0 = 0 ∈ G the 0 ﬂow, the time complexity to compute an ✏-approximate Max Flow is bounded by √ d3￿2 log d O￿ ￿. [sent-223, score-0.21]
</p><p>93 ✏  This time complexity matches the known result from [8], but with a much simpler procedure (gradient descent for the ﬂow player and Exponential Weights for the constraints). [sent-224, score-0.365]
</p><p>94 As we showed, the notion of using extra information about the sequence is a powerful tool with applications in optimization, convex programming, game theory, to name a few. [sent-228, score-0.215]
</p><p>95 All the applications considered in this paper, however, used some notion of smoothness for constructing the predictable process Mt . [sent-229, score-0.207]
</p><p>96 An interesting direction of further research is to isolate more general conditions under which the next gradient is predictable, perhaps even when the functions are not smooth in any sense. [sent-230, score-0.084]
</p><p>97 This could then be used to solve for the right predictable sequence to use so as to optimize the bounds. [sent-232, score-0.196]
</p><p>98 Using this notion of selecting predictable sequences one can hope to derive adaptive optimization procedures that in practice can provide rapid convergence. [sent-233, score-0.29]
</p><p>99 The multiplicative weights update method: A meta-algorithm and applications. [sent-240, score-0.047]
</p><p>100 Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. [sent-296, score-0.135]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ft', 0.577), ('gt', 0.342), ('player', 0.312), ('players', 0.305), ('mirror', 0.174), ('axt', 0.17), ('xt', 0.167), ('predictable', 0.164), ('mt', 0.158), ('yt', 0.121), ('regret', 0.116), ('ftt', 0.105), ('flow', 0.094), ('optimistic', 0.093), ('vjt', 0.088), ('uit', 0.08), ('game', 0.078), ('dr', 0.077), ('inf', 0.076), ('nt', 0.072), ('minimax', 0.066), ('convex', 0.066), ('axi', 0.062), ('smooth', 0.061), ('sup', 0.061), ('round', 0.058), ('argmin', 0.055), ('descent', 0.053), ('daskalakis', 0.053), ('gi', 0.052), ('sequences', 0.048), ('prox', 0.047), ('unif', 0.047), ('rmax', 0.047), ('uncoupled', 0.046), ('ow', 0.044), ('mixed', 0.043), ('saddle', 0.043), ('rt', 0.042), ('programming', 0.042), ('log', 0.041), ('predictability', 0.04), ('lemma', 0.04), ('lder', 0.038), ('playing', 0.038), ('play', 0.037), ('equilibrium', 0.036), ('sequence', 0.032), ('optimization', 0.032), ('strategies', 0.032), ('rate', 0.031), ('oco', 0.031), ('weights', 0.03), ('adaptive', 0.029), ('exponential', 0.028), ('online', 0.028), ('corollary', 0.027), ('banach', 0.027), ('smoothness', 0.026), ('extraneous', 0.025), ('fit', 0.024), ('doubling', 0.024), ('actions', 0.023), ('adaptively', 0.023), ('observes', 0.023), ('capacity', 0.023), ('payoff', 0.023), ('deviates', 0.023), ('gradient', 0.023), ('rates', 0.022), ('dynamics', 0.022), ('extra', 0.022), ('al', 0.022), ('ii', 0.022), ('rakhlin', 0.021), ('partial', 0.021), ('learner', 0.021), ('exp', 0.021), ('recognized', 0.021), ('prescribed', 0.021), ('plays', 0.02), ('arguably', 0.019), ('draw', 0.019), ('structured', 0.019), ('observe', 0.019), ('whereby', 0.019), ('faced', 0.019), ('pennsylvania', 0.018), ('asked', 0.018), ('st', 0.018), ('suppose', 0.018), ('auer', 0.018), ('games', 0.018), ('encountered', 0.018), ('mixing', 0.017), ('norm', 0.017), ('ine', 0.017), ('notion', 0.017), ('update', 0.017), ('logarithmic', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="240-tfidf-1" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>Author: Sasha Rakhlin, Karthik Sridharan</p><p>Abstract: We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror Prox algorithm for ofﬂine optimization, prove an extension to H¨ lder-smooth o functions, and apply the results to saddle-point type problems. Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a ﬁnite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T )￿T ). This addresses a question of Daskalakis et al [6]. Further, we consider a partial information version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem. 1</p><p>2 0.44421226 <a title="240-tfidf-2" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>Author: Brendan McMahan, Jacob Abernethy</p><p>Abstract: We design and analyze minimax-optimal algorithms for online linear optimization games where the player’s choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set, we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game, and we give a thorough analysis of the minimax behavior of the game, providing characterizations for the value of the game, as well as both the player’s and the adversary’s optimal strategy. We show how these objects can be computed efﬁciently under certain circumstances, and by selecting an appropriate benchmark, we construct a novel hedging strategy for an unconstrained betting game. 1</p><p>3 0.37526459 <a title="240-tfidf-3" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>Author: Nicolò Cesa-Bianchi, Ofer Dekel, Ohad Shamir</p><p>Abstract: We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player’s performance using a new notion of regret, also known as policy regret, which better captures the adversary’s adaptiveness to the player’s behavior. In a setting where losses are allowed to drift, we characterize —in a nearly complete manner— the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switch� ing costs, the attainable rate with bandit feedback is Θ(T 2/3 ). Interestingly, this √ rate is signiﬁcantly worse than the Θ( T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also � show that a bounded memory adversary can force Θ(T 2/3 ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies. 1</p><p>4 0.30460721 <a title="240-tfidf-4" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><p>5 0.30268183 <a title="240-tfidf-5" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>Author: Francesco Orabona</p><p>Abstract: I present a new online learning algorithm that extends the exponentiated gradient framework to inﬁnite dimensional spaces. My analysis shows that the algorithm is implicitly able to estimate the L2 norm of the unknown competitor, U , achieving √ a regret bound of the order of O(U log(U T + 1)) T ), instead of the standard √ O((U 2 + 1) T ), achievable without knowing U . For this analysis, I introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness. Through a lower bound, I also show that the algorithm is optimal up to log(U T ) term for linear and Lipschitz losses. 1</p><p>6 0.26273695 <a title="240-tfidf-6" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>7 0.25422397 <a title="240-tfidf-7" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>8 0.21983594 <a title="240-tfidf-8" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>9 0.20534599 <a title="240-tfidf-9" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>10 0.16408999 <a title="240-tfidf-10" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>11 0.14728083 <a title="240-tfidf-11" href="./nips-2013-Linear_Convergence_with_Condition_Number_Independent_Access_of_Full_Gradients.html">175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</a></p>
<p>12 0.1400902 <a title="240-tfidf-12" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>13 0.13880499 <a title="240-tfidf-13" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>14 0.11857431 <a title="240-tfidf-14" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>15 0.10579733 <a title="240-tfidf-15" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>16 0.099043541 <a title="240-tfidf-16" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>17 0.093224742 <a title="240-tfidf-17" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>18 0.083943136 <a title="240-tfidf-18" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>19 0.083900221 <a title="240-tfidf-19" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>20 0.077097073 <a title="240-tfidf-20" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, -0.17), (2, 0.327), (3, -0.296), (4, -0.036), (5, -0.126), (6, -0.019), (7, 0.027), (8, 0.074), (9, -0.1), (10, -0.049), (11, -0.157), (12, 0.025), (13, 0.018), (14, -0.051), (15, -0.065), (16, 0.032), (17, -0.099), (18, 0.002), (19, 0.052), (20, -0.022), (21, 0.03), (22, -0.015), (23, 0.021), (24, -0.064), (25, -0.1), (26, 0.155), (27, 0.029), (28, 0.129), (29, -0.011), (30, 0.043), (31, -0.103), (32, 0.032), (33, -0.119), (34, 0.225), (35, 0.012), (36, 0.054), (37, 0.12), (38, 0.058), (39, 0.08), (40, 0.056), (41, -0.08), (42, 0.037), (43, 0.087), (44, -0.077), (45, -0.055), (46, -0.017), (47, 0.066), (48, -0.039), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96430129 <a title="240-lsi-1" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>Author: Sasha Rakhlin, Karthik Sridharan</p><p>Abstract: We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror Prox algorithm for ofﬂine optimization, prove an extension to H¨ lder-smooth o functions, and apply the results to saddle-point type problems. Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a ﬁnite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T )￿T ). This addresses a question of Daskalakis et al [6]. Further, we consider a partial information version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem. 1</p><p>2 0.87538624 <a title="240-lsi-2" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>Author: Brendan McMahan, Jacob Abernethy</p><p>Abstract: We design and analyze minimax-optimal algorithms for online linear optimization games where the player’s choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set, we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game, and we give a thorough analysis of the minimax behavior of the game, providing characterizations for the value of the game, as well as both the player’s and the adversary’s optimal strategy. We show how these objects can be computed efﬁciently under certain circumstances, and by selecting an appropriate benchmark, we construct a novel hedging strategy for an unconstrained betting game. 1</p><p>3 0.81939185 <a title="240-lsi-3" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>Author: Nicolò Cesa-Bianchi, Ofer Dekel, Ohad Shamir</p><p>Abstract: We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player’s performance using a new notion of regret, also known as policy regret, which better captures the adversary’s adaptiveness to the player’s behavior. In a setting where losses are allowed to drift, we characterize —in a nearly complete manner— the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switch� ing costs, the attainable rate with bandit feedback is Θ(T 2/3 ). Interestingly, this √ rate is signiﬁcantly worse than the Θ( T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also � show that a bounded memory adversary can force Θ(T 2/3 ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies. 1</p><p>4 0.65985811 <a title="240-lsi-4" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>Author: Wouter M. Koolen</p><p>Abstract: Performance guarantees for online learning algorithms typically take the form of regret bounds, which express that the cumulative loss overhead compared to the best expert in hindsight is small. In the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts, at the cost of modest additional overhead compared to more complex others. We study which such regret trade-offs can be achieved, and how. We analyse regret w.r.t. each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss. We characterise the achievable and Pareto optimal trade-offs, and the corresponding optimal strategies for each sample size both exactly for each ﬁnite horizon and asymptotically. 1</p><p>5 0.65248543 <a title="240-lsi-5" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>Author: Francesco Orabona</p><p>Abstract: I present a new online learning algorithm that extends the exponentiated gradient framework to inﬁnite dimensional spaces. My analysis shows that the algorithm is implicitly able to estimate the L2 norm of the unknown competitor, U , achieving √ a regret bound of the order of O(U log(U T + 1)) T ), instead of the standard √ O((U 2 + 1) T ), achievable without knowing U . For this analysis, I introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness. Through a lower bound, I also show that the algorithm is optimal up to log(U T ) term for linear and Lipschitz losses. 1</p><p>6 0.5812676 <a title="240-lsi-6" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>7 0.51878399 <a title="240-lsi-7" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>8 0.47806671 <a title="240-lsi-8" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>9 0.42026782 <a title="240-lsi-9" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>10 0.41778266 <a title="240-lsi-10" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>11 0.39836368 <a title="240-lsi-11" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>12 0.39282459 <a title="240-lsi-12" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>13 0.36107573 <a title="240-lsi-13" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>14 0.33270541 <a title="240-lsi-14" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>15 0.3200745 <a title="240-lsi-15" href="./nips-2013-Learning_Prices_for_Repeated_Auctions_with_Strategic_Buyers.html">159 nips-2013-Learning Prices for Repeated Auctions with Strategic Buyers</a></p>
<p>16 0.31425342 <a title="240-lsi-16" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>17 0.30663517 <a title="240-lsi-17" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>18 0.30417162 <a title="240-lsi-18" href="./nips-2013-More_Effective_Distributed_ML_via_a_Stale_Synchronous_Parallel_Parameter_Server.html">198 nips-2013-More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server</a></p>
<p>19 0.30068949 <a title="240-lsi-19" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>20 0.27730164 <a title="240-lsi-20" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.098), (16, 0.014), (23, 0.123), (33, 0.14), (34, 0.075), (41, 0.062), (49, 0.051), (56, 0.193), (70, 0.026), (85, 0.054), (89, 0.021), (93, 0.022), (95, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92004216 <a title="240-lda-1" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>Author: Sasha Rakhlin, Karthik Sridharan</p><p>Abstract: We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea of predictable sequences. First, we recover the Mirror Prox algorithm for ofﬂine optimization, prove an extension to H¨ lder-smooth o functions, and apply the results to saddle-point type problems. Next, we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a ﬁnite zero-sum matrix game to converge to the minimax equilibrium at the rate of O((log T )￿T ). This addresses a question of Daskalakis et al [6]. Further, we consider a partial information version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the approximate Max Flow problem. 1</p><p>2 0.88044059 <a title="240-lda-2" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>Author: Christina E. Lee, Asuman Ozdaglar, Devavrat Shah</p><p>Abstract: Computing the stationary distribution of a large ﬁnite or countably inﬁnite state space Markov Chain (MC) has become central in many problems such as statistical inference and network analysis. Standard methods involve large matrix multiplications as in power iteration, or simulations of long random walks, as in Markov Chain Monte Carlo (MCMC). Power iteration is costly, as it involves computation at every state. For MCMC, it is difﬁcult to determine whether the random walks are long enough to guarantee convergence. In this paper, we provide a novel algorithm that answers whether a chosen state in a MC has stationary probability larger than some ∆ ∈ (0, 1), and outputs an estimate of the stationary probability. Our algorithm is constant time, using information from a local neighborhood of the state on the graph induced by the MC, which has constant size relative to the state space. The multiplicative error of the estimate is upper bounded by a function of the mixing properties of the MC. Simulation results show MCs for which this method gives tight estimates. 1</p><p>3 0.87705374 <a title="240-lda-3" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>Author: Brendan McMahan, Jacob Abernethy</p><p>Abstract: We design and analyze minimax-optimal algorithms for online linear optimization games where the player’s choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set, we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game, and we give a thorough analysis of the minimax behavior of the game, providing characterizations for the value of the game, as well as both the player’s and the adversary’s optimal strategy. We show how these objects can be computed efﬁciently under certain circumstances, and by selecting an appropriate benchmark, we construct a novel hedging strategy for an unconstrained betting game. 1</p><p>4 0.86609191 <a title="240-lda-4" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>Author: Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Yishay Mansour</p><p>Abstract: We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir [14]. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph (which must be accessible before selecting an action). In the undirected case, we show that the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efﬁcient manner. 1</p><p>5 0.86331844 <a title="240-lda-5" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>Author: Nicolò Cesa-Bianchi, Ofer Dekel, Ohad Shamir</p><p>Abstract: We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player’s performance using a new notion of regret, also known as policy regret, which better captures the adversary’s adaptiveness to the player’s behavior. In a setting where losses are allowed to drift, we characterize —in a nearly complete manner— the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switch� ing costs, the attainable rate with bandit feedback is Θ(T 2/3 ). Interestingly, this √ rate is signiﬁcantly worse than the Θ( T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also � show that a bounded memory adversary can force Θ(T 2/3 ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies. 1</p><p>6 0.8530516 <a title="240-lda-6" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>7 0.85241759 <a title="240-lda-7" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>8 0.85233498 <a title="240-lda-8" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>9 0.85145909 <a title="240-lda-9" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>10 0.84814596 <a title="240-lda-10" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>11 0.84786105 <a title="240-lda-11" href="./nips-2013-Point_Based_Value_Iteration_with_Optimal_Belief_Compression_for_Dec-POMDPs.html">248 nips-2013-Point Based Value Iteration with Optimal Belief Compression for Dec-POMDPs</a></p>
<p>12 0.84684068 <a title="240-lda-12" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>13 0.84253097 <a title="240-lda-13" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>14 0.84224057 <a title="240-lda-14" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>15 0.84147692 <a title="240-lda-15" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>16 0.83649111 <a title="240-lda-16" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>17 0.83580446 <a title="240-lda-17" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>18 0.83565813 <a title="240-lda-18" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>19 0.83440083 <a title="240-lda-19" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>20 0.83319145 <a title="240-lda-20" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
