<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-242" href="#">nips2013-242</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</h1>
<br/><p>Source: <a title="nips-2013-242-pdf" href="http://papers.nips.cc/paper/4903-pac-bayes-empirical-bernstein-inequality.pdf">pdf</a></p><p>Author: Ilya O. Tolstikhin, Yevgeny Seldin</p><p>Abstract: We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. 1</p><p>Reference: <a title="nips-2013-242-reference" href="../nips2013_reference/nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. [sent-6, score-0.433]
</p><p>2 We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. [sent-7, score-1.185]
</p><p>3 The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. [sent-9, score-0.433]
</p><p>4 PAC-Bayesian analysis provides concentration inequalities for the divergence between expected and empirical loss of randomized prediction rules. [sent-13, score-0.555]
</p><p>5 For a hypothesis space H a randomized prediction rule associated with a distribution ρ over H operates by picking a hypothesis at random according to ρ from H each time it has to make a prediction. [sent-14, score-0.303]
</p><p>6 If ρ is a delta-distribution we recover classical prediction rules that pick a single hypothesis h ∈ H. [sent-15, score-0.134]
</p><p>7 Otherwise, the prediction strategy resembles Bayesian prediction from the posterior distribution, with a distinction that ρ does not have to be the Bayes posterior. [sent-16, score-0.176]
</p><p>8 Importantly, many of PAC-Bayesian inequalities hold for all posterior distributions ρ simultaneously (with high probability over a random draw of a training set). [sent-17, score-0.351]
</p><p>9 Ideally, we prefer to derive new algorithms that ﬁnd the posterior distribution ρ that minimizes the PAC-Bayesian bound on the expected loss. [sent-19, score-0.317]
</p><p>10 However, we can also use PAC-Bayesian bounds in order to estimate the expected loss of posterior distributions ρ that were found by other algorithms, such as empirical risk minimization, regularized empirical risk minimization, Bayesian posteriors, and so forth. [sent-20, score-0.655]
</p><p>11 There are two forms of PAC-Bayesian inequalities that are currently known to be the tightest depending on a situation. [sent-22, score-0.123]
</p><p>12 One is the PAC-Bayes-kl inequality of Seeger [7] and the other is the PACBayes-Bernstein inequality of Seldin et. [sent-23, score-0.58]
</p><p>13 However, the PAC-Bayes-Bernstein inequality is expressed in terms of the true expected variance, which is rarely accessible in practice. [sent-26, score-0.388]
</p><p>14 Therefore, in order to apply the PAC-Bayes-Bernstein inequality we need an upper bound on the expected variance 1  (or, more precisely, on the average of the expected variances of losses of each hypothesis h ∈ H weighted according to the randomized prediction rule ρ). [sent-27, score-1.156]
</p><p>15 In fact, for the binary loss this result cannot be signiﬁcantly improved (see Section 3). [sent-29, score-0.144]
</p><p>16 However, when the loss is not binary it may be possible to obtain a tighter bound on the variance, which will lead to a tighter bound on the loss than the PAC-Bayes-kl inequality. [sent-30, score-0.906]
</p><p>17 [6] a deterministic upper bound on the variance of importance-weighted sampling combined with PAC-Bayes-Bernstein inequality yielded an order of magnitude improvement relative to application of PAC-Bayes-kl inequality to the same problem. [sent-33, score-0.927]
</p><p>18 We note that the bound on the variance used by Seldin et. [sent-34, score-0.304]
</p><p>19 In this work we derive the PAC-Bayes-Empirical-Bernstein bound, in which the expected average variance of the loss weighted by ρ is replaced by the weighted average of the empirical variance of the loss. [sent-37, score-0.572]
</p><p>20 Bounding the expected variance by the empirical variance is generally tighter than bounding it by the empirical loss. [sent-38, score-0.762]
</p><p>21 Therefore, the PAC-Bayes-Empirical-Bernstein bound is generally tighter than the PAC-Bayes-kl bound, although the exact comparison also depends on the divergence between the posterior and the prior and the sample size. [sent-39, score-0.466]
</p><p>22 In Section 5 we provide an empirical comparison of the two bounds on several synthetic and UCI datasets. [sent-40, score-0.213]
</p><p>23 In the ﬁrst step we combine the PAC-Bayesian bounding technique with the Empirical Bernstein inequality [9] and derive a PACBayesian bound on the variance. [sent-42, score-0.604]
</p><p>24 The PAC-Bayesian bound on the variance bounds the divergence between averages [weighted by ρ] of expected and empirical variances of the losses of hypotheses in H and holds with high probability for all averaging distributions ρ simultaneously. [sent-43, score-0.903]
</p><p>25 In the second step the PAC-Bayesian bound on the variance is substituted into the PAC-Bayes-Bernstein inequality yielding the PAC-Bayes-Empirical-Bernstein bound. [sent-44, score-0.594]
</p><p>26 training sample S = {(Xi , Yi )}n drawn according to an unknown distribution D on the product-space i=1 X × Y, a loss function : Y 2 → [0, 1], and a hypothesis class H. [sent-53, score-0.285]
</p><p>27 We use h (X, Y ) = (Y, h(X)) to denote the loss of a hypothesis h on a pair (X, Y ). [sent-55, score-0.245]
</p><p>28 For a ﬁxed hypothesis h ∈ H denote its expected loss by L(h) = E(X,Y )∼D [ h (X, Y )], n 1 the empirical loss Ln (h) = n i=1 h (Xi , Yi ), and the variance of the loss V(h) = Var(X,Y )∼D [ h (X, Y )] = E(X,Y )∼D  h (X, Y  ) − E(X,Y )∼D [ h (X, Y )]  2  . [sent-56, score-0.828]
</p><p>29 We deﬁne Gibbs regression rule Gρ associated with a distribution ρ over H in the following way: for each point X Gibbs regression rule draws a hypothesis h according to ρ and applies it to X. [sent-57, score-0.279]
</p><p>30 The expected loss of Gibbs regression rule is denoted by L(Gρ ) = Eh∼ρ [L(h)] and the empirical loss is ρ(h) denoted by Ln (Gρ ) = Eh∼ρ [Ln (h)]. [sent-58, score-0.539]
</p><p>31 We use KL(ρ π) = Eh∼ρ ln π(h) to denote the KullbackLeibler divergence between two probability distributions [10]. [sent-59, score-0.501]
</p><p>32 2  PAC-Bayes-kl bound  Before presenting our results we review several existing PAC-Bayesian bounds. [sent-63, score-0.202]
</p><p>33 The result in Theorem 1 was presented by Maurer [11, Theorem 5] and is one of the tightest known concentration bounds on the expected loss of Gibbs regression rule. [sent-64, score-0.434]
</p><p>34 Theorem 1 generalizes (and slightly tightens) PAC-Bayes-kl inequality of Seeger [7, Theorem 1] from binary to arbitrary loss functions bounded in the [0, 1] interval. [sent-65, score-0.473]
</p><p>35 For any ﬁxed probability distribution π over H, for any n ≥ 8 and δ > 0, with probability greater than 1 − δ over a random draw of a sample S, for all distributions ρ over H simultaneously: √ KL(ρ π) + ln 2 δ n kl Ln (Gρ ) L(Gρ ) ≤ . [sent-67, score-0.864]
</p><p>36 (1) n kl(q p)/2, Theorem 1 directly implies (up to minor Since by Pinsker’s inequality |p − q| ≤ factors) the more explicit PAC-Bayesian bound of McAllester [12]: √  KL(ρ π) + ln 2 δ n L(Gρ ) ≤ Ln (Gρ ) + , (2) 2n which holds with probability greater than 1 − δ for all ρ simultaneously. [sent-68, score-0.964]
</p><p>37 We note that kl is easy to invert numerically and for small values of Ln (Gρ ) (less than 1/4) the implicit bound in (1) is signiﬁcantly tighter than the explicit bound in (2). [sent-69, score-0.723]
</p><p>38 This can be seen from another relaxation suggested by McAllester [2], which follows from (1) by the inequality p ≤ q + 2qkl(q p) + 2kl(q p) for p < q: √  L(Gρ ) ≤ Ln (Gρ ) +  √  2Ln (Gρ ) KL(ρ π) + ln 2 δ n  2 KL(ρ π) + ln 2 δ n  + . [sent-70, score-1.108]
</p><p>39 (3) n n From inequality (3) we clearly see that inequality (1) achieves “fast convergence rate” or, in other √ words, when L(Gρ ) is zero (or small compared to 1/ n) the bound converges at the rate of 1/n √ rather than 1/ n as a function of n. [sent-71, score-0.751]
</p><p>40 [8] introduced a general technique for combining PAC-Bayesian analysis with concentration of measure inequalities and derived the PAC-Bayes-Bernstein bound cited below. [sent-75, score-0.364]
</p><p>41 n ¯ Furthermore, the result holds if Eρ [V(h)] is replaced by an upper bound V (ρ), as long as ¯ (ρ) ≤ 1 for all ρ. [sent-84, score-0.241]
</p><p>42 worked with cumulative losses and variances, whereas we work with normalized losses and variances, which means that their losses and variances differ by a multiplicative factor of n from our deﬁnitions. [sent-88, score-0.375]
</p><p>43 Second, we note that the statement on the possibility of replacing Eρ [V(h)] by an upper bound is not part of [8, Theorem 8], but it is mentioned and analyzed explicitly in the text. [sent-89, score-0.214]
</p><p>44 Since 4 is a trivial upper bound on the variance of a random variable bounded in the [0, 1] interval, the requirement is not a limitation. [sent-91, score-0.386]
</p><p>45 Finally, we note that since we are working with “one-sided” variables (namely, the loss is bounded in the [0, 1] interval rather than “two-sided” [−1, 1] interval, which was considered in [8]) the variance is bounded by 1 (rather than 1), which leads to a slight improvement in the value of ν1 . [sent-92, score-0.396]
</p><p>46 4 Since in reality we rarely have access to the expected variance Eρ [V(h)] the tightness of Theorem ¯ 2 entirely depends on the tightness of the upper bound V (ρ). [sent-93, score-0.525]
</p><p>47 If we use the trivial upper bound Eρ [V(h)] ≤ 1 the result is roughly equivalent to (2), which is inferior to Theorem 1. [sent-94, score-0.247]
</p><p>48 Design of a 4 tighter upper bound on Eρ [V(h)] that holds for all ρ simultaneously is the subject of the following section. [sent-95, score-0.463]
</p><p>49 3  Main Results  The key result of our paper is a PAC-Bayesian bound on the average expected variance Eρ [V(h)] given in terms of the average empirical variance Eρ [Vn (h)] = Eh∼ρ [Vn (h)], where Vn (h) =  1 n−1  n h (Xi , Yi )  − Ln (h)  2  (5)  i=1  is an unbiased estimate of the variance V(h). [sent-96, score-0.732]
</p><p>50 The bound is given in Theorem 3 and it holds with high probability for all distributions ρ simultaneously. [sent-97, score-0.252]
</p><p>51 Substitution of this bound into Theorem 2 yields the PAC-Bayes-Empirical-Bernstein inequality given in Theorem 4. [sent-98, score-0.461]
</p><p>52 Thus, the PAC-BayesEmpirical-Bernstein inequality is based on two subsequent applications of the PAC-Bayesian bounding technique. [sent-99, score-0.391]
</p><p>53 1  PAC-Bayesian bound on the variance  Theorem 3 is based on an application of the PAC-Bayesian bounding technique to the difference Eρ [V(h)] − Eρ [Vn (h)]. [sent-101, score-0.447]
</p><p>54 We note that Vn (h) is a second-order U-statistics [13] and Theorem 3 provides an interesting example of combining PAC-Bayesian analysis with concentration inequalities for self-bounding functions. [sent-102, score-0.151]
</p><p>55 Note that (6) closely resembles the explicit bound on L(Gρ ) in (3). [sent-105, score-0.202]
</p><p>56 If the empirical variance Vn (h) √ is close to zero the impact of the second term of the bound (that scales with 1/ n) is relatively small and we obtain “fast convergence rate” of Eρ [Vn (h)] to Eρ [V(h)]. [sent-106, score-0.399]
</p><p>57 Finally, we note that the impact of c2 on ln ν2 is relatively small and so c2 can be taken very close to 1. [sent-107, score-0.409]
</p><p>58 2  PAC-Bayes-Empirical-Bernstein bound  Theorem 3 controls the average variance Eρ [V(h)] for all posterior distributions ρ simultaneously. [sent-109, score-0.473]
</p><p>59 δ By taking δ1 = δ2 = 2 we have the claims of Theorems 2 and 3 holding simultaneously with 4  probability greater than 1 − δ. [sent-110, score-0.151]
</p><p>60 Substitution of the bound on Eρ [V(h)] from Theorem 3 into the PAC-Bayes-Bernstein inequality in Theorem 2 yields the main result of our paper, the PAC-BayesEmpirical-Bernstein inequality, that controls the loss of Gibbs regression rule Eρ [L(h)] for all posterior distributions ρ simultaneously. [sent-111, score-0.863]
</p><p>61 We also note that when the loss is bounded in the [0,1] interval we have Vn (h) ≤ (n/(n − 1))Ln (h) (since h (X, Y )2 ≤ h (X, Y )). [sent-118, score-0.224]
</p><p>62 Therefore, the PB-EB bound is never much worse than the PB-kl bound and if the empirical variance is small compared to the empirical loss it can be much tighter. [sent-119, score-0.809]
</p><p>63 We note that for the binary loss ( (y, y ) ∈ {0, 1}) we have V(h) = L(h)(1 − L(h)) and in this case the empirical variance cannot be signiﬁcantly smaller than the empirical loss and PB-EB does not provide an advantage over PB-kl. [sent-120, score-0.611]
</p><p>64 We also note that the unrelaxed version of the PB-kl inequality in equation (1) has better behavior for very small sample sizes and in such cases PB-kl can be tighter than PB-EB even when the empirical variance is small. [sent-121, score-0.732]
</p><p>65 7Ln (Gρ ) the PB-EB inequality can be signiﬁcantly tighter than the PB-kl bound and otherwise it is comparable (except for very small sample sizes). [sent-123, score-0.639]
</p><p>66 For any function fn : H × (X × Y) → R and for any distribution π over H, such that π is independent of S, with probability greater than 1 − δ over a random draw of S, for all distributions ρ over H simultaneously: 1 Eρ [fn (h, S)] ≤ KL(ρ π) + ln + ln Eπ ES ∼Dn efn (h,S ) . [sent-130, score-1.067]
</p><p>67 (8) δ The smart part is to choose fn (h, S) so that we get the quantities of interest on the left hand side of (8) and at the same time are able to bound the last term on the right hand side of (8). [sent-131, score-0.248]
</p><p>68 1  2  Average sample variance  1  2  2  Average sample variance  2. [sent-147, score-0.346]
</p><p>69 1 Average empirical loss  (a) n = 1000  (b) n = 4000  Figure 1: The Ratio of the gap between PB-EB and Ln (Gρ ) to the gap between PB-kl and Ln (Gρ ) for different values of n, Eρ [Vn (h)], and Ln (Gρ ). [sent-153, score-0.239]
</p><p>70 PB-EB is tighter below the dashed line with label 1. [sent-154, score-0.138]
</p><p>71 We take fn (h, S) = λ nV(h) − nVn (h) − λ n−1 V(h) 2 and substitute fn and the bound on its moment generating function in (9) into (8). [sent-157, score-0.365]
</p><p>72 To complete the proof it is left to optimize the bound with respect to λ. [sent-158, score-0.206]
</p><p>73 Since it is impossible to minimize the bound simultaneously for all ρ with a single value of λ, we follow the technique suggested by Seldin et. [sent-159, score-0.297]
</p><p>74 and take a grid of λ-s in a form of a geometric progression and apply a union bound over this grid. [sent-161, score-0.209]
</p><p>75 Then, for each ρ we pick a value of λ from the grid, which is the closest to the value of λ that minimizes the bound for the corresponding ρ. [sent-162, score-0.171]
</p><p>76 (The approximation of the optimal λ by the closest λ from the grid is behind the factor c2 in the bound and the ln ν2 factor is the result of the union bound over the grid of λ-s. [sent-163, score-0.827]
</p><p>77 By our choice of δ1 = δ2 = 2 the upper bounds of Theorems 2 and 3 hold simultaneously with probability greater than 1 − δ. [sent-166, score-0.261]
</p><p>78 b we examine the ratio of the complexity parts of the two bounds PB-EB − Ln (Gρ ) , PB-kl − Ln (Gρ ) where PB-EB is used to denote the value of the PB-EB bound in equation (7) and PB-kl is used to denote the value of the PB-kl bound in equation (1). [sent-171, score-0.445]
</p><p>79 As we already said, the advantage of the PB-EB inequality over the PB-kl inequality is most prominent in regression (for classiﬁcation with zero-one loss it is roughly comparable to PB-kl). [sent-182, score-0.772]
</p><p>80 Below we provide regression experiments with L1 loss on synthetic data and three datasets from the UCI repository [15]. [sent-183, score-0.306]
</p><p>81 We use the PB-EB and PB-kl bounds to bound the loss of a regularized empirical 6  risk minimization algorithm. [sent-184, score-0.504]
</p><p>82 In all our experiments the inputs Xi lie in a d-dimensional unit ball centered at the origin ( Xi 2 ≤ 1) and the outputs Y take values in [−0. [sent-185, score-0.157]
</p><p>83 The hypothesis class HW is deﬁned as HW = hw (X) = w, X : w ∈ Rd , w  2  ≤ 0. [sent-188, score-0.243]
</p><p>84 This construction ensures that the L1 regression loss (y, y ) = |y − y | is bounded in the [0, 1] −1 interval. [sent-190, score-0.231]
</p><p>85 The posterior distribution ρw is taken to ˆ ˆ be a uniform distribution on a d-dimensional ball of radius centered at the weight vector w, where ˆ w is the solution of the following minimization problem: 1 ˆ w = arg min w n  n  |Yi − w, Xi | + λ∗ w 2 . [sent-192, score-0.231]
</p><p>86 We use binary search in order to ﬁnd the minimal (non-negative) ˆ λ∗ , such that the posterior ρw is supported by HW (meaning that the ball of radius around w is ˆ within the ball of radius 0. [sent-195, score-0.319]
</p><p>87 The sigmoid function creates a mismatch between the data generating distribution and the linear hypothesis class. [sent-216, score-0.132]
</p><p>88 1) this results in small empirical variance of the loss Vn (h) and medium to high empirical loss Ln (h). [sent-218, score-0.611]
</p><p>89 We see that except for very small sample sizes (n < 1000) the PB-EB bound outperforms the PB-kl bound. [sent-229, score-0.247]
</p><p>90 Inferior performance for very small sample sizes is a result of domination of the O(1/n) term in the PB-EB bound (7). [sent-230, score-0.247]
</p><p>91 2  UCI datasets  We compare our PAC-Bayes-Empirical-Bernstein inequality (7) with the PAC-Bayes-kl inequality (1) on three UCI regression datasets: Wine Quality, Parkinsons Telemonitoring, and Concrete Compressive Strength. [sent-233, score-0.657]
</p><p>92 6  Conclusions and future work  We derived a new PAC-Bayesian bound that controls the convergence of averages of empirical variances of losses of hypotheses in H to averages of expected variances of losses of hypothesis in H simultaneously for all averaging distributions ρ. [sent-238, score-1.078]
</p><p>93 This bound is an interesting example of combination 7  0. [sent-239, score-0.171]
</p><p>94 5  PB−EB PB−kl Train error Test error  Expected loss  0. [sent-248, score-0.144]
</p><p>95 5  PB−EB PB−kl Train error Test error  Expected loss  Expected loss  0. [sent-249, score-0.288]
</p><p>96 2  1000 2000 3000 Sample size  (d) d = 6, w0  0  =3  Figure 2: The values of the PAC-Bayes-kl and PAC-Bayes-Empirical-Bernstein bounds together with the test and train errors on synthetic data. [sent-256, score-0.19]
</p><p>97 0011  of PAC-Bayesian bounding technique with concentration inequalities for self-bounding functions. [sent-282, score-0.294]
</p><p>98 We also demonstrated an empirical advantage of the new PAC-BayesEmpirical-Bernstein inequality over the PAC-Bayes-kl inequality on several synthetic and real-life regression datasets. [sent-284, score-0.774]
</p><p>99 Another interesting direction would be to decrease the last term in the bound in Theorem 3, as it is done in the PAC-Bayes-kl inequality. [sent-287, score-0.171]
</p><p>100 This can probably be achieved by deriving a PAC-Bayes-kl inequality for the variance. [sent-288, score-0.29]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ln', 0.409), ('vn', 0.383), ('inequality', 0.29), ('seldin', 0.276), ('kl', 0.243), ('bound', 0.171), ('pb', 0.155), ('loss', 0.144), ('hw', 0.142), ('tighter', 0.138), ('variance', 0.133), ('eh', 0.112), ('hypothesis', 0.101), ('bounding', 0.101), ('uci', 0.099), ('losses', 0.096), ('empirical', 0.095), ('yevgeny', 0.094), ('nvn', 0.093), ('variances', 0.087), ('simultaneously', 0.084), ('inequalities', 0.083), ('theorem', 0.083), ('eb', 0.083), ('posterior', 0.079), ('fn', 0.077), ('seeger', 0.071), ('ball', 0.069), ('concentration', 0.068), ('expected', 0.067), ('bounds', 0.067), ('greater', 0.067), ('laviolette', 0.054), ('pacbayesian', 0.054), ('john', 0.054), ('distributions', 0.054), ('bernstein', 0.054), ('radius', 0.051), ('synthetic', 0.051), ('draw', 0.051), ('centred', 0.05), ('russian', 0.05), ('parkinsons', 0.05), ('francois', 0.05), ('xi', 0.048), ('regression', 0.048), ('maurer', 0.047), ('gibbs', 0.046), ('train', 0.045), ('yi', 0.045), ('upper', 0.043), ('technique', 0.042), ('substitution', 0.041), ('interval', 0.041), ('rule', 0.041), ('sample', 0.04), ('tightness', 0.04), ('tightest', 0.04), ('substitute', 0.04), ('nv', 0.04), ('bounded', 0.039), ('grid', 0.038), ('divergence', 0.038), ('averages', 0.036), ('ratio', 0.036), ('controls', 0.036), ('sizes', 0.036), ('australian', 0.036), ('proof', 0.035), ('illustrative', 0.035), ('shorthand', 0.035), ('repository', 0.034), ('brevity', 0.034), ('pac', 0.034), ('inferior', 0.033), ('prediction', 0.033), ('hypotheses', 0.032), ('centered', 0.032), ('resembles', 0.031), ('mcallester', 0.031), ('rarely', 0.031), ('sigmoid', 0.031), ('presenting', 0.031), ('datasets', 0.029), ('origin', 0.029), ('andreas', 0.028), ('dn', 0.028), ('risk', 0.027), ('joy', 0.027), ('osokin', 0.027), ('thankful', 0.027), ('telemonitoring', 0.027), ('laureate', 0.027), ('kullbackleibler', 0.027), ('cantly', 0.027), ('inputs', 0.027), ('holds', 0.027), ('randomized', 0.027), ('ai', 0.027), ('test', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="242-tfidf-1" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>Author: Ilya O. Tolstikhin, Yevgeny Seldin</p><p>Abstract: We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. 1</p><p>2 0.25345826 <a title="242-tfidf-2" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>Author: Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund</p><p>Abstract: We consider a situation in which we see samples Xn ∈ Rd drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give ﬁnite-sample convergence rates for both. 1</p><p>3 0.14810802 <a title="242-tfidf-3" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>4 0.1443319 <a title="242-tfidf-4" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>Author: Justin Domke, Xianghang Liu</p><p>Abstract: Inference in general Ising models is difﬁcult, due to high treewidth making treebased algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We ﬁnd that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.</p><p>5 0.12937176 <a title="242-tfidf-5" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>Author: Navid Zolghadr, Gabor Bartok, Russell Greiner, András György, Csaba Szepesvari</p><p>Abstract: This paper introduces the online probing problem: In each round, the learner is able to purchase the values of a subset of feature values. After the learner uses this information to come up with a prediction for the given round, he then has the option of paying to see the loss function that he is evaluated against. Either way, the learner pays for both the errors of his predictions and also whatever he chooses to observe, including the cost of observing the loss function for the given round and the cost of the observed features. We consider two variations of this problem, depending on whether the learner can observe the label for free or not. We provide algorithms and upper and lower bounds on the regret for both variants. We show that a positive cost for observing the label signiﬁcantly increases the regret of the problem. 1</p><p>6 0.12513146 <a title="242-tfidf-6" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>7 0.12400406 <a title="242-tfidf-7" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>8 0.12235564 <a title="242-tfidf-8" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>9 0.11512452 <a title="242-tfidf-9" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>10 0.10164367 <a title="242-tfidf-10" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>11 0.098982722 <a title="242-tfidf-11" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>12 0.096224546 <a title="242-tfidf-12" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>13 0.09472049 <a title="242-tfidf-13" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>14 0.093110286 <a title="242-tfidf-14" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>15 0.089334808 <a title="242-tfidf-15" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>16 0.086159185 <a title="242-tfidf-16" href="./nips-2013-Documents_as_multiple_overlapping_windows_into_grids_of_counts.html">98 nips-2013-Documents as multiple overlapping windows into grids of counts</a></p>
<p>17 0.085215062 <a title="242-tfidf-17" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>18 0.080423005 <a title="242-tfidf-18" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>19 0.079099372 <a title="242-tfidf-19" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>20 0.077249661 <a title="242-tfidf-20" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, 0.017), (2, 0.098), (3, -0.035), (4, 0.043), (5, 0.113), (6, 0.044), (7, 0.016), (8, -0.01), (9, 0.026), (10, 0.049), (11, 0.04), (12, 0.018), (13, -0.046), (14, 0.108), (15, -0.059), (16, -0.03), (17, 0.035), (18, -0.027), (19, 0.051), (20, -0.055), (21, 0.001), (22, 0.029), (23, 0.104), (24, -0.253), (25, 0.068), (26, -0.147), (27, 0.061), (28, -0.108), (29, -0.042), (30, -0.049), (31, -0.074), (32, -0.113), (33, -0.014), (34, -0.006), (35, -0.017), (36, -0.069), (37, -0.131), (38, 0.127), (39, -0.084), (40, 0.064), (41, 0.02), (42, 0.274), (43, -0.046), (44, 0.078), (45, -0.043), (46, 0.019), (47, 0.019), (48, 0.066), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96447861 <a title="242-lsi-1" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>Author: Ilya O. Tolstikhin, Yevgeny Seldin</p><p>Abstract: We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. 1</p><p>2 0.76396561 <a title="242-lsi-2" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>Author: Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund</p><p>Abstract: We consider a situation in which we see samples Xn ∈ Rd drawn i.i.d. from some distribution with mean zero and unknown covariance A. We wish to compute the top eigenvector of A in an incremental fashion - with an algorithm that maintains an estimate of the top eigenvector in O(d) space, and incrementally adjusts the estimate with each new data point that arrives. Two classical such schemes are due to Krasulina (1969) and Oja (1983). We give ﬁnite-sample convergence rates for both. 1</p><p>3 0.60362279 <a title="242-lsi-3" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>Author: Samory Kpotufe, Vikas Garg</p><p>Abstract: We present the ﬁrst result for kernel regression where the procedure adapts locally at a point x to both the unknown local dimension of the metric space X and the unknown H¨ lder-continuity of the regression function at x. The result holds with o high probability simultaneously at all points x in a general metric space X of unknown structure. 1</p><p>4 0.59829324 <a title="242-lsi-4" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><p>5 0.5707348 <a title="242-lsi-5" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><p>6 0.4967736 <a title="242-lsi-6" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>7 0.4927597 <a title="242-lsi-7" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>8 0.48513985 <a title="242-lsi-8" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>9 0.48398513 <a title="242-lsi-9" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>10 0.47077185 <a title="242-lsi-10" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>11 0.46396425 <a title="242-lsi-11" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>12 0.45353183 <a title="242-lsi-12" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>13 0.45158243 <a title="242-lsi-13" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>14 0.45132318 <a title="242-lsi-14" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>15 0.4479754 <a title="242-lsi-15" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>16 0.44679096 <a title="242-lsi-16" href="./nips-2013-Documents_as_multiple_overlapping_windows_into_grids_of_counts.html">98 nips-2013-Documents as multiple overlapping windows into grids of counts</a></p>
<p>17 0.44420573 <a title="242-lsi-17" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>18 0.43142068 <a title="242-lsi-18" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>19 0.42966527 <a title="242-lsi-19" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>20 0.4273988 <a title="242-lsi-20" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (16, 0.026), (33, 0.184), (34, 0.108), (41, 0.016), (46, 0.149), (49, 0.019), (56, 0.198), (70, 0.044), (79, 0.014), (85, 0.052), (89, 0.03), (93, 0.054), (95, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9362452 <a title="242-lda-1" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><p>same-paper 2 0.92185074 <a title="242-lda-2" href="./nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</a></p>
<p>Author: Ilya O. Tolstikhin, Yevgeny Seldin</p><p>Abstract: We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. 1</p><p>3 0.91883129 <a title="242-lda-3" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>Author: Justin Domke</p><p>Abstract: A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is “smoothed” through the addition of entropy terms, for ﬁxed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an “oracle” exists to minimize a logistic loss.</p><p>4 0.88856953 <a title="242-lda-4" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>5 0.88792253 <a title="242-lda-5" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>Author: Yuchen Zhang, John Duchi, Michael Jordan, Martin J. Wainwright</p><p>Abstract: We establish lower bounds on minimax risks for distributed statistical estimation under a communication budget. Such lower bounds reveal the minimum amount of communication required by any procedure to achieve the centralized minimax-optimal rates for statistical estimation. We study two classes of protocols: one in which machines send messages independently, and a second allowing for interactive communication. We establish lower bounds for several problems, including various types of location models, as well as for parameter estimation in regression models. 1</p><p>6 0.88325465 <a title="242-lda-6" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>7 0.88275093 <a title="242-lda-7" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>8 0.88201433 <a title="242-lda-8" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>9 0.88195896 <a title="242-lda-9" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>10 0.88168067 <a title="242-lda-10" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>11 0.88051873 <a title="242-lda-11" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>12 0.88039976 <a title="242-lda-12" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>13 0.88026953 <a title="242-lda-13" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>14 0.8799718 <a title="242-lda-14" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>15 0.87942123 <a title="242-lda-15" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>16 0.87867194 <a title="242-lda-16" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>17 0.87841117 <a title="242-lda-17" href="./nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</a></p>
<p>18 0.87756383 <a title="242-lda-18" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>19 0.87565053 <a title="242-lda-19" href="./nips-2013-Buy-in-Bulk_Active_Learning.html">60 nips-2013-Buy-in-Bulk Active Learning</a></p>
<p>20 0.87536657 <a title="242-lda-20" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
