<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-4" href="#">nips2013-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</h1>
<br/><p>Source: <a title="nips-2013-4-pdf" href="http://papers.nips.cc/paper/5104-a-comparative-framework-for-preconditioned-lasso-algorithms.pdf">pdf</a></p><p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>Reference: <a title="nips-2013-4-reference" href="../nips2013_reference/nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. [sent-11, score-0.112]
</p><p>2 We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. [sent-13, score-0.063]
</p><p>3 , the support set S(β ∗ ) {i|βi = 0} has cardinality k < n), a mainstay algorithm for such settings is the Lasso [10]: 1 2 ˆ Lasso: β = argminβ∈Rp ||y − Xβ||2 + λ ||β||1 . [sent-21, score-0.057]
</p><p>4 2n  (2)  For a particular choice of λ, the variable selection properties of the Lasso can be analyzed by quanˆ tifying how well the estimated support S(β) approximates the true support S(β ∗ ). [sent-22, score-0.167]
</p><p>5 More careful analyses focus instead on recovering the signed support S± (β ∗ ), ∗ S± (βi )  ∗ +1 if βi > 0 ∗ −1 if βi < 0 . [sent-23, score-0.334]
</p><p>6 (3)  Theoretical developments during the last decade have shed light onto the support recovery properties of the Lasso and highlighted practical difﬁculties when the columns of X are correlated. [sent-26, score-0.235]
</p><p>7 These developments have led to various conditions on X for support recovery, such as the mutual incoherence or the irrepresentable condition [1, 3, 8, 12, 13]. [sent-27, score-0.138]
</p><p>8 1  In recent years, several modiﬁcations of the standard Lasso have been proposed to improve its support recovery properties [2, 7, 14, 15]. [sent-28, score-0.18]
</p><p>9 (4) improves or degrades signed support recovery relative to the standard Lasso of Eq. [sent-41, score-0.456]
</p><p>10 A major roadblock to a one-to-one comparison are the auxiliary penalty param¯ eters, λ, λ, which trade off the 1 penalty to the quadratic objective in both Eq. [sent-43, score-0.196]
</p><p>11 A correct choice of penalty parameter is essential for signed support recovery: If it is too small, the algorithm behaves like ordinary least squares; if it is too large, the estimated support may be empty. [sent-46, score-0.509]
</p><p>12 Unfortunately, in all but the simplest cases, pre-multiplying data X, y by matrices PX , Py changes the relative geometry of the 1 penalty contours to the elliptical objective contours in a nontrivial way. [sent-47, score-0.181]
</p><p>13 For a fair comparison, the resulting mapping would have to capture the change of relative geometry induced by preconditioning of X, y, ¯ i. [sent-51, score-0.167]
</p><p>14 In the Preconditioned Lasso literature this problem is commonly sidestepped either by resorting to asymptotic comparisons [6, 9], empirically comparing regularization paths [5], or using modelselection techniques which aim to choose reasonably “good” matching penalty parameters [6]. [sent-56, score-0.164]
</p><p>15 We deem these approaches to be unsatisfactory—asymptotic and empirical analyses provide limited insight, and model selection strategies add a layer of complexity that may lead to unfair comparisons. [sent-57, score-0.076]
</p><p>16 It is our view that all of these approaches place unnecessary emphasis on particular choices of penalty parameter. [sent-58, score-0.098]
</p><p>17 In this paper we propose an alternative strategy that instead compares the Lasso to the Preconditioned Lasso by comparing data-dependent upper and lower penalty parameter bounds. [sent-59, score-0.138]
</p><p>18 (2) is guaranteed to recover the signed support iff λl < λ < λu . [sent-61, score-0.336]
</p><p>19 Consequently, if λl > λu signed support recovery is not possible. [sent-62, score-0.44]
</p><p>20 The comparison of Lasso and Preconditioned Lasso on an instance X, β ∗ ¯ ¯ bounds (λ ¯ then proceeds by suitably comparing the bounds on λ and λ. [sent-65, score-0.119]
</p><p>21 The advantage of this approach is that the upper and lower bounds are easy to compute, even though a general mapping between speciﬁc penalty parameters cannot be readily derived. [sent-66, score-0.137]
</p><p>22 We outline our comparative framework in Section 3 and highlight some immediate consequences for [5] and [9] on general matrices X in Section 4. [sent-72, score-0.134]
</p><p>23 More detailed comparisons can be made by considering a generative model for X. [sent-73, score-0.079]
</p><p>24 In Section 5 we introduce such a model based on a block-wise SVD of X and then analyze [6] for speciﬁc instances of this generative model. [sent-74, score-0.068]
</p><p>25 Finally, we show that in terms of signed support recovery, this generative model can be thought of as a limit point of a Gaussian 2  construction. [sent-75, score-0.375]
</p><p>26 Huang and Jojic proposed Correlation Sifting [5], which, although not presented as a preconditioning algorithm, can be rewritten as one. [sent-85, score-0.136]
</p><p>27 An earlier instance of the preconditioning idea was put forward by Paul et al. [sent-91, score-0.136]
</p><p>28 Jia and Rohe [6] propose a preconditioning method that amounts to whitening the matrix X. [sent-99, score-0.136]
</p><p>29 3  Comparative Framework  In this section we propose a new comparative approach for Preconditioned Lasso algorithms which ¯ avoids choosing particular penalty parameters λ, λ. [sent-106, score-0.133]
</p><p>30 We ﬁrst derive upper and lower bounds for λ ¯ respectively so that signed support recovery can be guaranteed iff λ and λ satisfy the bounds. [sent-107, score-0.498]
</p><p>31 1  Conditions for signed support recovery  Before proceeding, we make some deﬁnitions motivated by Wainwright [12]. [sent-110, score-0.44]
</p><p>32 Suppose that the support set of β ∗ is S S(β ∗ ), with |S| = k. [sent-111, score-0.057]
</p><p>33 Denote by Xj column j of X and by XA the submatrix of X consisting of columns indexed by set A. [sent-119, score-0.094]
</p><p>34 (9) i = ei n S n S n 1  The choice of smallest singular vectors is considered for matrices X with sharply decaying spectrum. [sent-121, score-0.064]
</p><p>35 Figure 1: Empirical evaluation of the penalty parameter bounds of Lemma 1. [sent-138, score-0.151]
</p><p>36 Then we ran Lasso using penalty parameters f λl in Figure (a) and f λu in Figure (b), where the factor f = 0. [sent-140, score-0.098]
</p><p>37 The ﬁgures show the empirical probability of signed support recovery as a function of the factor f for both λl and λu . [sent-146, score-0.44]
</p><p>38 (2), results in (for example) Wainwright [12] connect settings of λ with instances of X, β ∗ , w to certify whether or not Lasso will recover the signed support. [sent-149, score-0.289]
</p><p>39 We invert these results and, for particular instances of X, β ∗ , w, derive bounds on λ so that signed support recovery is guaranteed if and only if the bounds are satisﬁed. [sent-150, score-0.547]
</p><p>40 Then ˆ ˆ the Lasso has a unique solution β which recovers the signed support (i. [sent-154, score-0.317]
</p><p>41 On the other hand, if XS XS is not invertible, then the signed support cannot in general be recovered. [sent-157, score-0.317]
</p><p>42 Lemma 1 recapitulates well-worn intuitions about when the Lasso has difﬁculty recovering the signed support. [sent-158, score-0.291]
</p><p>43 In extreme cases we might have λl > λu so that signed support recovery is impossible. [sent-162, score-0.454]
</p><p>44 Figure 1 empirically validates the bounds of Lemma 1 by estimating probabilities of signed support recovery for a range of penalty parameters on synthetic Lasso problems. [sent-163, score-0.577]
</p><p>45 2  Comparisons  In this paper we propose to compare a preconditioning algorithm to the traditional Lasso by comparing the penalty parameter bounds produced by Lemma 1. [sent-165, score-0.33]
</p><p>46 Provided the conditions of Lemma 1 hold for X, β ∗ we can ¯ ¯ u , λl on the penalty parameter λ can ¯ ¯ deﬁne updated variables µj , γi , ηj , ¯i from which the bounds λ ¯ ¯ ¯ be derived. [sent-170, score-0.222]
</p><p>47 In order for our comparison to be scale-invariant, we will compare algorithms by ratios of resulting penalty parameter bounds. [sent-171, score-0.146]
</p><p>48 ¯ disproportionately larger than λ ¯ u = 0, λl = 0 in which case we deﬁne λu /λl ¯ ¯ ¯ We will later encounter the special case λ ∞ to ¯ ¯ indicate that the preconditioned problem is very easy. [sent-174, score-0.293]
</p><p>49 If λu /λl < 1 then signed support recovery is ¯ ¯ ¯ ¯ in general impossible. [sent-175, score-0.44]
</p><p>50 4  4  General Comparisons  We begin our comparisons with some immediate consequences of Lemma 1 for HJ and PBHT. [sent-179, score-0.078]
</p><p>51 As we will see, both HJ and PBHT have the potential to improve signed support recovery relative to the traditional Lasso, provided the matrices PX , Py are suitably estimated. [sent-182, score-0.515]
</p><p>52 We also let US be a minimal basis for the column space of the submatrix XS , and deﬁne span(US ) = x ∃c ∈ Rk s. [sent-184, score-0.084]
</p><p>53 Recall from Section 2 that HJ uses PX = Py = UA UA , where UA is a column basis estimated from X. [sent-189, score-0.092]
</p><p>54 If span(US ) ⊆ span(UA ), then after preconditioning using HJ the conditions continue to hold, and ¯ λu λu (12) ¯ , λl λl where the stochasticity on both sides is due to independent noise vectors w. [sent-192, score-0.261]
</p><p>55 On the other hand, if XS PX PX XS is not invertible, then HJ cannot in general recover the signed support. [sent-193, score-0.26]
</p><p>56 Notice that because µj and γi are un¯ ¯ ¯ changed, if the conditions of Lemma 1 hold for the original Lasso problem (i. [sent-197, score-0.071]
</p><p>57 , XS XS is invertible, ∗ |µj | < 1 ∀j ∈ S c and sgn(βi )γi > 0 ∀i ∈ S), they will continue to hold for the preconditioned problem. [sent-199, score-0.37]
</p><p>58 In extreme cases, XS PX PX XS is singular and so signed support recovery is not in general possible. [sent-205, score-0.473]
</p><p>59 Recall from Section 2 that PBHT uses PX = In×n , Py = UA UA , where UA is a column basis estimated from X. [sent-207, score-0.092]
</p><p>60 If span(US ) ⊆ span(UA ), after preconditioning using PBHT the conditions continue to hold, and ¯ λu λu (16) ¯ , λl λl where the stochasticity on both sides is due to independent noise vectors w. [sent-211, score-0.261]
</p><p>61 On the other hand, if span(US c ) = span(UA ), then PBHT cannot recover the signed support. [sent-212, score-0.26]
</p><p>62 ’s of penalty parameter bounds ratios estimated from 1000 variable selection problems. [sent-238, score-0.238]
</p><p>63 Because µj and γi are again unchanged, ¯ ¯ ¯ the conditions of Lemma 1 will continue to hold for the preconditioned problem if they hold for the original Lasso problem. [sent-255, score-0.441]
</p><p>64 Because P(λl ≥ 0) = 1, signed support recovery is not possible. [sent-261, score-0.44]
</p><p>65 Our theoretical analyses show that both HJ and PBHT can indeed lead to improved signed support recovery relative to the Lasso on ﬁnite datasets. [sent-263, score-0.473]
</p><p>66 ’s for penalty parameter bounds ratios of Lasso and Preconditioned Lasso for various subspaces UA . [sent-267, score-0.204]
</p><p>67 Further comparison of HJ and PBHT must thus analyze how the subspaces span(UA ) are estimated in the context of the assumptions made in [5] and [9]. [sent-270, score-0.056]
</p><p>68 Both HJ and PBHT were proposed with the implicit goal of ﬁnding a basis UA that has the same span as US . [sent-272, score-0.221]
</p><p>69 Theorems 1 and 2 suggest that underestimating k can be more detrimental to signed support recovery than overestimating it. [sent-274, score-0.462]
</p><p>70 In this section we brieﬂy present this model and will show the resulting penalty parameter bounds for JR. [sent-280, score-0.151]
</p><p>71 1  Generative model for X  As discussed in Section 2, many preconditioning algorithms can be phrased as truncating or reweighting column subspaces associated with X [5, 6, 9]. [sent-282, score-0.194]
</p><p>72 Furthermore, we let U, VS , VS c be orthonormal bases of dimension n × n, k × k and p − k × p − k respectively. [sent-286, score-0.057]
</p><p>73 Then we let the Lasso problem be y = Xβ ∗ + w  with  X = U Σ S V S , ΣS c V S c  w ∼ N (0, σ 2 In×n ),  (19)  To ensure that the column norms of X are controlled, we compute the spectra ΣS , ΣS c by normalˆ ˆ izing spectra ΣS and ΣS c with arbitrary positive elements on the diagonal. [sent-292, score-0.099]
</p><p>74 (20)  We verify in the supplementary material that with these assumptions the squared column norms of X are in expectation n (provided the orthonormal bases are chosen uniformly at random). [sent-294, score-0.128]
</p><p>75 Note that any matrix X can be decomposed using a block-wise SVD as X = [XS , XS c ] = U ΣS VS , T ΣS c VS c ,  (21)  with orthonormal bases U, T, VS , VS c . [sent-296, score-0.057]
</p><p>76 In previous sections we used US to denote a basis for the column space of XS . [sent-305, score-0.069]
</p><p>77 We will continue to use this notation, and let US contain the ﬁrst k columns of U . [sent-306, score-0.061]
</p><p>78 We let the diagonal elements of ΣS , ΣS , ΣS c , ΣS c ˆ be identiﬁed by their column indices. [sent-308, score-0.066]
</p><p>79 That is, the diagonal entries σS,c of ΣS and σS,c of ΣS ˆ ˆ S c are indexed are indexed by c ∈ {1, . [sent-309, score-0.076]
</p><p>80 Each of the diagonal entries in ΣS , ΣS c is associated with a column of U . [sent-316, score-0.085]
</p><p>81 Then the conditions of Lemma 1 hold before and after preconditioning using JR. [sent-332, score-0.207]
</p><p>82 λ  (22)  In other words, JR deterministically scales the ratio of penalty parameter bounds. [sent-334, score-0.146]
</p><p>83 (19) we ﬁnd that the SVD becomes X = U DV , where U is the same column basis as in Eq. [sent-340, score-0.069]
</p><p>84 Substituting this into the deﬁnitions of µj , γi , ηj , ¯i , we have that after preconditioning using JR ¯ ¯ ¯ n(p − k)κ2 µj = µj ¯ γi = n + 2 ¯ γi (23) kκ + n − k (kκ2 + n − k) ηj = ¯ ηj ¯i = i . [sent-342, score-0.136]
</p><p>85 (24) n(p − k) Thus, if the conditions of Lemma 1 hold for X, β ∗ , they will continue to hold after preconditioning using JR. [sent-343, score-0.284]
</p><p>86 (19) uses an orthonormal matrix U as the column basis of X. [sent-350, score-0.096]
</p><p>87 However, as we show in the supplementary material, one can construct Lasso problems using a Gaussian basis W m which lead to penalty parameter bounds ratios that converge in distribution to those of the Lasso problem in Eq. [sent-352, score-0.233]
</p><p>88 (19), and one according to 1 m y m = X m β ∗ + wm with X m = √ W m ΣS VS , ΣS c VS c wm ∼ N 0, σ 2 Im×m , (25) n n where W m is an m × n standard Gaussian ensemble. [sent-355, score-0.06]
</p><p>89 The increased variance is necessary because the matrix W m has expected column length m, while columns in U are of length 1. [sent-363, score-0.064]
</p><p>90 Let the penalty parameter bounds ratio induced by the problem in Eq. [sent-365, score-0.185]
</p><p>91 If the conditions of Lemma 1 hold for X, β ∗ , then for m large enough they will hold for X m , β ∗ . [sent-371, score-0.112]
</p><p>92 Furthermore, as m → ∞ λm d λu u → , (26) λm λl l where the stochasticity on the left is due to W m , wm and on the right is due to w. [sent-372, score-0.06]
</p><p>93 Thus, with respect to the bounds ratio λu /λl , the construction of Eq. [sent-373, score-0.077]
</p><p>94 Indeed, as the experiment in Figure 2(b) shows, Theorem 3 can be used to predict the scaling factor for penalty parameter bounds ¯ ¯ ratios (i. [sent-378, score-0.185]
</p><p>95 6  Conclusions  This paper proposes a new framework for comparing Preconditioned Lasso algorithms to the standard Lasso which skirts the difﬁculty of choosing penalty parameters. [sent-381, score-0.141]
</p><p>96 By eliminating this parameter from consideration, ﬁnite data comparisons can be greatly simpliﬁed, avoiding the use of model selection strategies. [sent-382, score-0.084]
</p><p>97 To demonstrate the framework’s usefulness, we applied it to a number of Preconditioned Lasso algorithms and in the process conﬁrmed intuitions and revealed fragilities and mitigation strategies. [sent-383, score-0.06]
</p><p>98 Additionally, we presented an SVD-based generative model for Lasso problems that can be thought of as the limit point of a less restrictive Gaussian model. [sent-384, score-0.058]
</p><p>99 Stable recovery of sparse overcomplete representations in the presence of noise. [sent-392, score-0.123]
</p><p>100 Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (Lasso). [sent-458, score-0.123]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ua', 0.562), ('lasso', 0.357), ('preconditioned', 0.293), ('px', 0.269), ('signed', 0.26), ('xs', 0.231), ('py', 0.203), ('span', 0.191), ('pbht', 0.187), ('preconditioning', 0.136), ('hj', 0.129), ('recovery', 0.123), ('penalty', 0.098), ('jr', 0.094), ('vs', 0.08), ('jojic', 0.059), ('support', 0.057), ('lemma', 0.055), ('rohe', 0.055), ('jia', 0.051), ('sgn', 0.045), ('hold', 0.041), ('comparisons', 0.04), ('column', 0.039), ('generative', 0.039), ('bounds', 0.039), ('irrepresentable', 0.038), ('consequences', 0.038), ('continue', 0.036), ('paul', 0.035), ('svd', 0.035), ('comparative', 0.035), ('ratios', 0.034), ('dv', 0.032), ('invertible', 0.032), ('intuitions', 0.031), ('basis', 0.03), ('theorems', 0.03), ('spectra', 0.03), ('stochasticity', 0.03), ('wm', 0.03), ('bases', 0.03), ('selection', 0.03), ('conditions', 0.03), ('deem', 0.029), ('fragilities', 0.029), ('instances', 0.029), ('orthonormal', 0.027), ('matrices', 0.027), ('diagonal', 0.027), ('comparing', 0.026), ('misaligned', 0.025), ('huang', 0.025), ('columns', 0.025), ('xj', 0.025), ('us', 0.024), ('estimated', 0.023), ('overestimating', 0.022), ('plugging', 0.021), ('contours', 0.02), ('ratio', 0.019), ('thought', 0.019), ('construction', 0.019), ('iff', 0.019), ('subspaces', 0.019), ('entries', 0.019), ('dd', 0.019), ('singular', 0.019), ('sharply', 0.018), ('supplementary', 0.018), ('gaussian', 0.017), ('highlight', 0.017), ('framework', 0.017), ('meinshausen', 0.017), ('met', 0.017), ('analyses', 0.017), ('highlighted', 0.017), ('traditional', 0.017), ('relative', 0.016), ('dim', 0.016), ('xa', 0.016), ('indexed', 0.015), ('oracle', 0.015), ('deterministically', 0.015), ('submatrix', 0.015), ('suitably', 0.015), ('noise', 0.015), ('derivations', 0.015), ('induced', 0.015), ('suppose', 0.014), ('differ', 0.014), ('extreme', 0.014), ('sides', 0.014), ('theorem', 0.014), ('parameter', 0.014), ('assumptions', 0.014), ('developments', 0.013), ('piecewise', 0.013), ('curve', 0.013), ('nitions', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="4-tfidf-1" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>2 0.19582835 <a title="4-tfidf-2" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>3 0.18645141 <a title="4-tfidf-3" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>4 0.17476028 <a title="4-tfidf-4" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>5 0.14963304 <a title="4-tfidf-5" href="./nips-2013-A_Kernel_Test_for_Three-Variable_Interactions.html">9 nips-2013-A Kernel Test for Three-Variable Interactions</a></p>
<p>Author: Dino Sejdinovic, Arthur Gretton, Wicher Bergsma</p><p>Abstract: We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak inﬂuence on a third dependent variable, but their combined effect has a strong inﬂuence. This makes the Lancaster test especially suited to ﬁnding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.</p><p>6 0.12545508 <a title="4-tfidf-6" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>7 0.1135815 <a title="4-tfidf-7" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>8 0.099571869 <a title="4-tfidf-8" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>9 0.098360583 <a title="4-tfidf-9" href="./nips-2013-On_Poisson_Graphical_Models.html">217 nips-2013-On Poisson Graphical Models</a></p>
<p>10 0.094886228 <a title="4-tfidf-10" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>11 0.092932239 <a title="4-tfidf-11" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>12 0.085500427 <a title="4-tfidf-12" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>13 0.080871306 <a title="4-tfidf-13" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>14 0.073830858 <a title="4-tfidf-14" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>15 0.072862633 <a title="4-tfidf-15" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>16 0.069379114 <a title="4-tfidf-16" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>17 0.065550379 <a title="4-tfidf-17" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>18 0.061340827 <a title="4-tfidf-18" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<p>19 0.059912361 <a title="4-tfidf-19" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>20 0.053871386 <a title="4-tfidf-20" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.061), (2, 0.069), (3, 0.073), (4, -0.022), (5, 0.066), (6, -0.021), (7, 0.027), (8, -0.157), (9, -0.026), (10, 0.114), (11, -0.052), (12, -0.12), (13, -0.208), (14, -0.122), (15, -0.07), (16, 0.151), (17, -0.112), (18, -0.052), (19, -0.03), (20, 0.004), (21, 0.11), (22, -0.035), (23, 0.088), (24, 0.047), (25, 0.053), (26, -0.027), (27, 0.03), (28, -0.038), (29, 0.033), (30, 0.088), (31, -0.057), (32, 0.111), (33, -0.005), (34, -0.033), (35, 0.09), (36, 0.099), (37, 0.047), (38, -0.165), (39, -0.079), (40, -0.075), (41, -0.06), (42, 0.054), (43, -0.044), (44, 0.077), (45, -0.098), (46, -0.005), (47, -0.061), (48, -0.047), (49, -0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96475869 <a title="4-lsi-1" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>2 0.7405228 <a title="4-lsi-2" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>3 0.72793525 <a title="4-lsi-3" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>4 0.64215666 <a title="4-lsi-4" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>Author: Divyanshu Vats, Richard Baraniuk</p><p>Abstract: We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.</p><p>5 0.6378817 <a title="4-lsi-5" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>6 0.62934798 <a title="4-lsi-6" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>7 0.59302074 <a title="4-lsi-7" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>8 0.54554385 <a title="4-lsi-8" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>9 0.51879138 <a title="4-lsi-9" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>10 0.46815541 <a title="4-lsi-10" href="./nips-2013-On_Poisson_Graphical_Models.html">217 nips-2013-On Poisson Graphical Models</a></p>
<p>11 0.43005598 <a title="4-lsi-11" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>12 0.39378867 <a title="4-lsi-12" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>13 0.36826435 <a title="4-lsi-13" href="./nips-2013-Designed_Measurements_for_Vector_Count_Data.html">88 nips-2013-Designed Measurements for Vector Count Data</a></p>
<p>14 0.3634873 <a title="4-lsi-14" href="./nips-2013-A_Kernel_Test_for_Three-Variable_Interactions.html">9 nips-2013-A Kernel Test for Three-Variable Interactions</a></p>
<p>15 0.33706564 <a title="4-lsi-15" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>16 0.32308927 <a title="4-lsi-16" href="./nips-2013-q-OCSVM%3A_A_q-Quantile_Estimator_for_High-Dimensional_Distributions.html">358 nips-2013-q-OCSVM: A q-Quantile Estimator for High-Dimensional Distributions</a></p>
<p>17 0.32282451 <a title="4-lsi-17" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>18 0.32256499 <a title="4-lsi-18" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>19 0.30904782 <a title="4-lsi-19" href="./nips-2013-Conditional_Random_Fields_via_Univariate_Exponential_Families.html">67 nips-2013-Conditional Random Fields via Univariate Exponential Families</a></p>
<p>20 0.29682916 <a title="4-lsi-20" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.042), (33, 0.171), (34, 0.134), (41, 0.021), (49, 0.027), (56, 0.088), (70, 0.02), (85, 0.044), (86, 0.261), (89, 0.04), (93, 0.024), (95, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82533175 <a title="4-lda-1" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>2 0.80683064 <a title="4-lda-2" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>Author: Remi Gribonval, Pierre Machart</p><p>Abstract: There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, Bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors. While these may seem radically different approaches, recent results have shown that, in the context of additive white Gaussian denoising, the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First, we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness. This sheds light on some tradeoff between computational efﬁciency and estimation accuracy in sparse regularization, and draws some connections between Bayesian estimation and proximal optimization. 1</p><p>3 0.79147792 <a title="4-lda-3" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>Author: Ben Shababo, Brooks Paige, Ari Pakman, Liam Paninski</p><p>Abstract: With the advent of modern stimulation techniques in neuroscience, the opportunity arises to map neuron to neuron connectivity. In this work, we develop a method for efﬁciently inferring posterior distributions over synaptic strengths in neural microcircuits. The input to our algorithm is data from experiments in which action potentials from putative presynaptic neurons can be evoked while a subthreshold recording is made from a single postsynaptic neuron. We present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for signiﬁcant prior information about the connectivity and neuronal cell types to be incorporated if available. Due to the technical challenges and sparsity of these systems, it is important to focus experimental time stimulating the neurons whose synaptic strength is most ambiguous, therefore we also develop an online optimal design algorithm for choosing which neurons to stimulate at each trial. 1</p><p>4 0.75830895 <a title="4-lda-4" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>5 0.69527429 <a title="4-lda-5" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><p>6 0.68313944 <a title="4-lda-6" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>7 0.68146664 <a title="4-lda-7" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>8 0.67738354 <a title="4-lda-8" href="./nips-2013-Similarity_Component_Analysis.html">294 nips-2013-Similarity Component Analysis</a></p>
<p>9 0.67654246 <a title="4-lda-9" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>10 0.67606938 <a title="4-lda-10" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>11 0.67524648 <a title="4-lda-11" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>12 0.6745643 <a title="4-lda-12" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>13 0.6744222 <a title="4-lda-13" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>14 0.67357838 <a title="4-lda-14" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>15 0.67347187 <a title="4-lda-15" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>16 0.6734001 <a title="4-lda-16" href="./nips-2013-Distributed_Submodular_Maximization%3A_Identifying_Representative_Elements_in_Massive_Data.html">97 nips-2013-Distributed Submodular Maximization: Identifying Representative Elements in Massive Data</a></p>
<p>17 0.67279041 <a title="4-lda-17" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>18 0.67224157 <a title="4-lda-18" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>19 0.67192221 <a title="4-lda-19" href="./nips-2013-A_simple_example_of_Dirichlet_process_mixture_inconsistency_for_the_number_of_components.html">18 nips-2013-A simple example of Dirichlet process mixture inconsistency for the number of components</a></p>
<p>20 0.67169279 <a title="4-lda-20" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
