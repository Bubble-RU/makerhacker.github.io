<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-231" href="#">nips2013-231</a> knowledge-graph by maker-knowledge-mining</p><h1>231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</h1>
<br/><p>Source: <a title="nips-2013-231-pdf" href="http://papers.nips.cc/paper/5151-online-learning-with-switching-costs-and-other-adaptive-adversaries.pdf">pdf</a></p><p>Author: Nicolò Cesa-Bianchi, Ofer Dekel, Ohad Shamir</p><p>Abstract: We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player’s performance using a new notion of regret, also known as policy regret, which better captures the adversary’s adaptiveness to the player’s behavior. In a setting where losses are allowed to drift, we characterize —in a nearly complete manner— the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switch� ing costs, the attainable rate with bandit feedback is Θ(T 2/3 ). Interestingly, this √ rate is signiﬁcantly worse than the Θ( T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also � show that a bounded memory adversary can force Θ(T 2/3 ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies. 1</p><p>Reference: <a title="nips-2013-231-reference" href="../nips2013_reference/nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('advers', 0.519), ('regret', 0.332), ('ft', 0.308), ('oblivy', 0.296), ('bandit', 0.284), ('switch', 0.262), ('play', 0.234), ('feedback', 0.21), ('round', 0.193), ('loss', 0.167), ('fll', 0.125), ('mem', 0.118), ('xt', 0.107), ('invest', 0.083), ('bound', 0.073), ('gam', 0.067), ('lt', 0.056), ('drift', 0.055), ('dt', 0.054), ('cost', 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="231-tfidf-1" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>2 0.41944587 <a title="231-tfidf-2" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>3 0.36980706 <a title="231-tfidf-3" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>4 0.33976883 <a title="231-tfidf-4" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>5 0.33395725 <a title="231-tfidf-5" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>6 0.32684159 <a title="231-tfidf-6" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>7 0.27716205 <a title="231-tfidf-7" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>8 0.27453133 <a title="231-tfidf-8" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>9 0.26512235 <a title="231-tfidf-9" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>10 0.26451033 <a title="231-tfidf-10" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>11 0.25154793 <a title="231-tfidf-11" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>12 0.19274113 <a title="231-tfidf-12" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>13 0.179713 <a title="231-tfidf-13" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>14 0.16972193 <a title="231-tfidf-14" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>15 0.16926727 <a title="231-tfidf-15" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>16 0.1683768 <a title="231-tfidf-16" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>17 0.16470453 <a title="231-tfidf-17" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>18 0.15586069 <a title="231-tfidf-18" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>19 0.13783009 <a title="231-tfidf-19" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>20 0.12621005 <a title="231-tfidf-20" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, -0.254), (2, 0.151), (3, 0.431), (4, 0.052), (5, -0.179), (6, 0.012), (7, -0.0), (8, -0.035), (9, -0.048), (10, 0.056), (11, -0.09), (12, 0.137), (13, 0.008), (14, 0.018), (15, 0.088), (16, 0.094), (17, -0.044), (18, -0.017), (19, -0.004), (20, 0.008), (21, 0.045), (22, -0.031), (23, 0.03), (24, 0.011), (25, 0.016), (26, -0.007), (27, 0.005), (28, -0.03), (29, 0.073), (30, -0.072), (31, -0.03), (32, -0.031), (33, -0.085), (34, -0.032), (35, -0.007), (36, 0.025), (37, -0.063), (38, 0.03), (39, 0.055), (40, -0.009), (41, -0.04), (42, -0.004), (43, -0.019), (44, 0.002), (45, -0.001), (46, 0.015), (47, 0.053), (48, 0.025), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96299356 <a title="231-lsi-1" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>2 0.82967913 <a title="231-lsi-2" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>3 0.77473831 <a title="231-lsi-3" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>4 0.77323294 <a title="231-lsi-4" href="./nips-2013-Minimax_Optimal_Algorithms_for_Unconstrained_Linear_Optimization.html">191 nips-2013-Minimax Optimal Algorithms for Unconstrained Linear Optimization</a></p>
<p>5 0.76734406 <a title="231-lsi-5" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>6 0.6933198 <a title="231-lsi-6" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>7 0.67322701 <a title="231-lsi-7" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>8 0.63375187 <a title="231-lsi-8" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>9 0.60223484 <a title="231-lsi-9" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>10 0.57945031 <a title="231-lsi-10" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>11 0.5650022 <a title="231-lsi-11" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>12 0.53059459 <a title="231-lsi-12" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>13 0.52340031 <a title="231-lsi-13" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>14 0.51701313 <a title="231-lsi-14" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>15 0.47014895 <a title="231-lsi-15" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>16 0.45758957 <a title="231-lsi-16" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>17 0.45472357 <a title="231-lsi-17" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>18 0.43520865 <a title="231-lsi-18" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>19 0.43023333 <a title="231-lsi-19" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>20 0.38495007 <a title="231-lsi-20" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(20, 0.102), (24, 0.191), (25, 0.125), (37, 0.08), (70, 0.044), (72, 0.043), (80, 0.076), (86, 0.156), (87, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89445299 <a title="231-lda-1" href="./nips-2013-Online_Learning_with_Switching_Costs_and_Other_Adaptive_Adversaries.html">231 nips-2013-Online Learning with Switching Costs and Other Adaptive Adversaries</a></p>
<p>2 0.88977313 <a title="231-lda-2" href="./nips-2013-From_Bandits_to_Experts%3A_A_Tale_of_Domination_and_Independence.html">125 nips-2013-From Bandits to Experts: A Tale of Domination and Independence</a></p>
<p>3 0.87118506 <a title="231-lda-3" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>4 0.84560061 <a title="231-lda-4" href="./nips-2013-The_Pareto_Regret_Frontier.html">325 nips-2013-The Pareto Regret Frontier</a></p>
<p>5 0.82032043 <a title="231-lda-5" href="./nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</a></p>
<p>6 0.81918615 <a title="231-lda-6" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>7 0.81821334 <a title="231-lda-7" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>8 0.79781204 <a title="231-lda-8" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>9 0.79651111 <a title="231-lda-9" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>10 0.7956382 <a title="231-lda-10" href="./nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</a></p>
<p>11 0.79035151 <a title="231-lda-11" href="./nips-2013-Sign_Cauchy_Projections_and_Chi-Square_Kernel.html">293 nips-2013-Sign Cauchy Projections and Chi-Square Kernel</a></p>
<p>12 0.78277373 <a title="231-lda-12" href="./nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</a></p>
<p>13 0.78201556 <a title="231-lda-13" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>14 0.78185254 <a title="231-lda-14" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>15 0.77610832 <a title="231-lda-15" href="./nips-2013-Prior-free_and_prior-dependent_regret_bounds_for_Thompson_Sampling.html">253 nips-2013-Prior-free and prior-dependent regret bounds for Thompson Sampling</a></p>
<p>16 0.77607161 <a title="231-lda-16" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>17 0.77046597 <a title="231-lda-17" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>18 0.76959413 <a title="231-lda-18" href="./nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</a></p>
<p>19 0.76725399 <a title="231-lda-19" href="./nips-2013-Learning_Prices_for_Repeated_Auctions_with_Strategic_Buyers.html">159 nips-2013-Learning Prices for Repeated Auctions with Strategic Buyers</a></p>
<p>20 0.76623917 <a title="231-lda-20" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
