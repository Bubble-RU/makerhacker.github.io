<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-342" href="#">nips2013-342</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</h1>
<br/><p>Source: <a title="nips-2013-342-pdf" href="http://papers.nips.cc/paper/4862-unsupervised-spectral-learning-of-finite-state-transducers.pdf">pdf</a></p><p>Author: Raphael Bailly, Xavier Carreras, Ariadna Quattoni</p><p>Abstract: Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identiﬁability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. 1</p><p>Reference: <a title="nips-2013-342-reference" href="../nips2013_reference/nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. [sent-3, score-0.132]
</p><p>2 [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. [sent-5, score-0.245]
</p><p>3 In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. [sent-6, score-0.105]
</p><p>4 We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. [sent-7, score-0.321]
</p><p>5 Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. [sent-9, score-0.315]
</p><p>6 A pair of sequences is made of an input sequence, built from an input alphabet, and an output sequence, built from an output alphabet. [sent-11, score-0.198]
</p><p>7 A variety of algorithms for learning FST have been proposed in the literature, most of them are based on EM optimizations [9, 11] or grammatical inference techniques [8, 6]. [sent-13, score-0.071]
</p><p>8 GAATTCAG| | || | GGA-TC-GA  GAATTCAG| || | | GGAT-C-GA  GAATTC-AG | | || | GGA-TCGA-  GAATTC-AG | || | | GGAT-CGA-  To be able to handle different alignments, a special empty symbol ∗ is added to the input and output alphabets. [sent-17, score-0.088]
</p><p>9 With this enlarged set of bi-symbols, the model is able to generate an input symbol (resp. [sent-18, score-0.068]
</p><p>10 As an example, the ﬁrst alignment above will correspond ∗ y to the two possible representations G A ∗ A T T C A G ∗ and G ∗ A A T T C A G ∗ . [sent-23, score-0.069]
</p><p>11 Under this model the G∗ GA∗ T C ∗ GA GG∗ A∗ T C ∗ GA probability of observing a pair of un-aligned input-output sequences is obtained by integrating over all possible alignments. [sent-24, score-0.108]
</p><p>12 Recently, following a recent trend of work on spectral learning algorithms for ﬁnite state machines [14, 2, 17, 18, 7, 16, 10, 5], Balle et al. [sent-25, score-0.105]
</p><p>13 [4] presented an algorithm for learning FST where the input to the algorithm are samples of aligned input-output sequences. [sent-26, score-0.14]
</p><p>14 As with most spectral methods the core idea of this algorithm is to exploit low-rank decompositions of some Hankel matrix representing 1  the distribution of aligned sequences. [sent-27, score-0.285]
</p><p>15 To estimate this Hankel matrix it is assumed that the algorithm can sample aligned sequences, i. [sent-28, score-0.18]
</p><p>16 While the problem of learning FST from fully aligned sequences (what we sometimes refer to as supervised learning) has been solved, the problem of deriving an unsupervised spectral method that can be trained from samples of input-output sequences alone (i. [sent-31, score-0.515]
</p><p>17 In this paper we address this unsupervised setting and present a spectral algorithm that can approximate the distribution of paired sequences generated by an FST without having access to aligned sequences. [sent-35, score-0.442]
</p><p>18 To the best of our knowledge this is the ﬁrst spectral algorithm for this problem. [sent-36, score-0.105]
</p><p>19 The main challenge in the unsupervised setting is that since the alignment information is not available, the Hankel matrices (as in [4]) can no longer be directly estimated from observable statistics. [sent-37, score-0.231]
</p><p>20 However, a key observation is that we can nevertheless compute observable statistics that can constraint the coefﬁcients of the Hankel matrix. [sent-38, score-0.098]
</p><p>21 This is because the probability of observing a pair of un-aligned input-output sequences (i. [sent-39, score-0.108]
</p><p>22 an observable statistic) is computed by summing over all possible alignments; i. [sent-41, score-0.097]
</p><p>23 The main idea of our algorithm is to exploit these constraints and ﬁnd a Hankel matrix (from which we can directly recover the model) which both agrees on the observed statistics and has a low-rank matrix factorization. [sent-44, score-0.168]
</p><p>24 In brief, our main contribution is to show that an FST can be approximated by solving an optimization which is based on ﬁnding a low-rank matrix satisfying a set of constraints derived from observable statistics and Hankel structure. [sent-45, score-0.189]
</p><p>25 We provide sample complexity bounds and some identiﬁability results for this optimization that show that, theoretically, the rank and the parameters of an FST distribution can be identiﬁed. [sent-46, score-0.117]
</p><p>26 Following previous work on rank minimization, we propose a regularized convex relaxation of the proposed objective which is based on minimizing a nuclear norm penalty subject to linear constraints. [sent-47, score-0.266]
</p><p>27 The proposed relaxation balances a trade-off between model complexity (measured by the nuclear norm penalty) and ﬁtting the observed statistics. [sent-48, score-0.111]
</p><p>28 Synthetic experiments show that the performance of our unsupervised algorithm efﬁciently approximates that of a supervised method trained from fully aligned sequences. [sent-49, score-0.232]
</p><p>29 Section 2 gives preliminaries on FST and spectral learning methods, and establishes that an FST can be induced from a Hankel matrix of observable aligned statistics. [sent-51, score-0.367]
</p><p>30 Section 3 presents a generalized form of Hankel matrices for FST that allows to express observation constraints efﬁciently. [sent-52, score-0.162]
</p><p>31 One can not observe generalized Hankel matrices without assumming access to aligned samples. [sent-53, score-0.251]
</p><p>32 To solve this problem, Section 4 formulates ﬁnding the Hankel matrix of an FST from unaligned samples as rank minimization problem. [sent-54, score-0.27]
</p><p>33 This section also presents the main theoretical results of the method, as well as a convex relaxation of the rank minimization problem. [sent-55, score-0.218]
</p><p>34 A Finite-State Transducer (FST) of rank d is given by: • alphabets Σ+ = {x1 , . [sent-59, score-0.146]
</p><p>35 An alignment of (s, t) is 1 n given by a sequence of pairs x1 . [sent-67, score-0.087]
</p><p>36 yn ) y y by removing the empty symbols ∗ equals s (resp. [sent-77, score-0.059]
</p><p>37 The set of alignments for a pair of sequences (s, t) is denoted [s, t]. [sent-80, score-0.213]
</p><p>38 2  Computing with an FST  In order to compute the value of a pair of sequences (s, t), one needs to sum over all possible alignments, which is generally exponential in the length of s and t. [sent-100, score-0.108]
</p><p>39 0 01 Let us consider the model A which satisﬁes the constraints above. [sent-114, score-0.049]
</p><p>40 One has rA (0 ∗ 1 ) = 00 1 1/96, rA (∗ 0 1 ) = 1/192 and, as those two aligned sequences are the only possible alignments for 0 01 (01, 001), one has rA ((01, 001)) = 1/64. [sent-115, score-0.334]
</p><p>41 y y y y A Hankel matrix on U and V is a matrix with rows corresponding to elements u ∈ U and columns corresponding to v ∈ V , which satisﬁes uv = u′ v ′ → H(u, v) = H(u′ , v ′ ). [sent-124, score-0.153]
</p><p>42 One supposes that U is preﬁx-closed, V is sufﬁx-closed, and that rank(Hε ) = rank(H). [sent-130, score-0.049]
</p><p>43 The rank equality comes from the fact that the WA deﬁned above has the same rank as Hε , and that the rank of a mapping f which satisﬁes f (uv) = H(u, v) is at least the rank of H. [sent-133, score-0.485]
</p><p>44 The complete Hankel b matrix has at least rank 7. [sent-139, score-0.157]
</p><p>45 3  Inducing FST from Generalized Hankel Matrices  Proposition 2 tells us that if we had access to certain sub-blocks of the Hankel matrix for aligned sequences we could recover the FST model. [sent-140, score-0.298]
</p><p>46 However, we do not have access to the hidden alignment information: we only have access to the statistics p(s, t), which we will call observations. [sent-141, score-0.152]
</p><p>47 One natural idea would be to search for a Hankel matrix that agrees with the observations. [sent-142, score-0.079]
</p><p>48 To do so, we introduce observable constraints, which are linear constraints of the form n 1 n 1 p(s, t) = ∑x1 . [sent-143, score-0.131]
</p><p>49 n y y y y y y 1  From a matrix satisfying the hypothesis of Proposition 2 and the observation constraints, one can derive an FST computing a mapping which agrees on the observations. [sent-153, score-0.13]
</p><p>50 1n 1n We will denote sets of alignments as follows: x1 n will denote the set {x1 n }, which contains a single y y x1 n x1 n xn+1 n+k xn+1 n+k aligned sequence; then y1 n [s, t] will denote the set {y1 n yn+1 n+k yn+1 n+k ∈ [s, t]}, which extends 1n 1n {x1 n } with all ways of aligning (s, t). [sent-161, score-0.245]
</p><p>51 generalized sufﬁx) is the empty set or a set of the form x1 n x1 n [s, t]y1 n (resp. [sent-165, score-0.077]
</p><p>52 y1 n [s, t]), where the aligned part is possibly empty. [sent-166, score-0.14]
</p><p>53 1  Generalized Hankel  One needs to extend the deﬁnition of a Hankel matrix to the generalized preﬁxes and sufﬁxes. [sent-168, score-0.092]
</p><p>54 Let U be a set of generalized preﬁxes, V be a set of generalized sufﬁxes. [sent-170, score-0.104]
</p><p>55 H is a generalized Hankel matrix if it satisﬁes: ′ ′ ′ ′ ∀¯, u′ ⊂ U, ∀¯, v ′ ⊂ V, u ¯ v ¯ ⊍ uv = ⊍ u v ⇒ ∑ H(u, v) = ∑ H(u v ) u∈¯,v∈¯ u v  u′ ∈¯′ ,v ′ ∈¯′ u v  where ⊍ is the disjoint union. [sent-172, score-0.165]
</p><p>56 Let U be a set of generalized preﬁxes, V be a set of generalized sufﬁxes. [sent-179, score-0.104]
</p><p>57 y y A key result is the following, which is analogous to Proposition 2 for generalized Hankel matrices: Proposition 3. [sent-182, score-0.052]
</p><p>58 Let U and V be two sets of generalized preﬁxes and generalized sufﬁxes. [sent-183, score-0.104]
</p><p>59 One supposes that rank(Hε ) = rank(H), U is preﬁx⊺ ⊺ + + closed and V is sufﬁx-closed. [sent-185, score-0.049]
</p><p>60 The sets U and V contain all the observed pairs of unaligned sequences, and one can check that the sizes of U Σ and ΣV are polynomial in the size of S. [sent-194, score-0.097]
</p><p>61 4  FST Learning as Non-convex Optimization  Proposition 3 shows that FST models can be recovered from generalized Hankel matrices. [sent-199, score-0.052]
</p><p>62 In this section we show how FST learning can be framed as an optimization problem where one searches for a low-rank generalized Hankel matrix that agrees with observation constraints derived from a sample. [sent-200, score-0.196]
</p><p>63 We will denote by z = (p([s, t]))s∈Σ∗ ,t∈Σ∗ the vector built from observable probabilities, and by + − z S = (pS ([s, t]))s∈Σ∗ ,t∈Σ∗ the set of empirical observable probabilities, where pS is the frequency + − deduced from an i. [sent-202, score-0.188]
</p><p>64 Let K be the matrix such that K H = 0 rep⃗ = z represents resents the Hankel constraints (cf. [sent-207, score-0.089]
</p><p>65 Let O be the matrix such that OH the observation constraints (i. [sent-209, score-0.105]
</p><p>66 Let us remark that the set of matrices satisfying the constraints is a compact set. [sent-215, score-0.097]
</p><p>67 Let p be a distribution over i/o sequences computed by an FST. [sent-222, score-0.089]
</p><p>68 The ﬁrst one concerns the rank identiﬁcation, while the second one concerns the consistency of the method. [sent-228, score-0.149]
</p><p>69 Let p be a rank d distribution computed by an FST. [sent-230, score-0.117]
</p><p>70 Let p be a rank d distribution computed by an FST. [sent-238, score-0.117]
</p><p>71 Let us ﬁrst remark that, among all the values used to build the Hankel matrices in Example 3, some of them correspond to observable statistics, as there is only one possible alignment for them. [sent-247, score-0.181]
</p><p>72 Then the rank minimization objective is not sufﬁcient, as it allows any value for those variables. [sent-249, score-0.151]
</p><p>73 It is clear that H ′ has a rank greater or equal than 2. [sent-258, score-0.117]
</p><p>74 0∗ The only way to reach rank 2 under the constraints is h22 = h51 = 1/72, h52 = 1/216, h63 = 1/192, h92 = 1/96 Thus, the process of rank minimization under linear constraints leads to one single model, which is identical to the original one. [sent-259, score-0.366]
</p><p>75 Of course, in the general case, the rank minimization objective may lead to several models. [sent-260, score-0.151]
</p><p>76 One can solve instead a convex relaxation of the problem (1), obtained by replacing the rank objective by the nuclear norm. [sent-263, score-0.219]
</p><p>77 The nuclear norm ∗ plays the same role than the 1 norm in convex relaxations of the 0 norm, used to reach sparsity under linear constraints. [sent-266, score-0.131]
</p><p>78 5  Experiments  We ran synthetic experiments using samples generated from random FST with input-output alphabets of size two. [sent-267, score-0.051]
</p><p>79 The main goal of our experiments was to compare our algorithm to a supervised spectral algorithm for FST that has access to alignments. [sent-268, score-0.176]
</p><p>80 Each run consists of generating a target FST, and generating N aligned samples according to the target distribution. [sent-270, score-0.186]
</p><p>81 Then, we removed the alignment information from each sample (thus obtaining a pair of unaligned strings), and we used them to train an FST with our general learning algorithm, trying different values for a C parameter that trades-off the nuclear norm term and the observation term. [sent-272, score-0.258]
</p><p>82 We ran this experiment for 8 target models of 5 states, for different sampling sizes. [sent-273, score-0.045]
</p><p>83 We measure the L1 error of the learned models with respect to the target distribution on all unaligned pairs of strings whose sizes sum up to 6. [sent-274, score-0.136]
</p><p>84 First a factorized method that assumes that the two sequences are generated independently, and learns two separate weighted automata using a spectral 7  0. [sent-277, score-0.217]
</p><p>85 0001 10k  20k  50k  100k  200k  500k  1M  Number of Samples (logscale)  Figure 1: Learning curves for different methods: SUP, supervised; UNS, unsupervised with different regularizers; EM, expectation maximization. [sent-286, score-0.05]
</p><p>86 Clearly, our algorithm is able to estimate the target distribution and gets close to the performance of the supervised method, while making use of much simpler statistics. [sent-294, score-0.065]
</p><p>87 EM performed slightly better than the spectral methods, but nonetheless at the same levels of performance. [sent-295, score-0.105]
</p><p>88 One can ﬁnd other experimental results for the unsupervised spectral method in [1], where it is shown that, under certain circumstances, unsupervised spectral method can perform better than supervised EM. [sent-296, score-0.352]
</p><p>89 6  Conclusion  In this paper we presented a spectral algorithm for learning FST from unaligned sequences. [sent-298, score-0.184]
</p><p>90 This is the ﬁrst paper to derive a spectral algorithm for the unsupervised FST learning setting. [sent-299, score-0.155]
</p><p>91 We prove that there is theoretical identiﬁability of the rank and the parameters of an FST distribution, using a rank minimization formulation. [sent-300, score-0.268]
</p><p>92 Classically, rank minimization problems are solved with convex relaxations such as the nuclear norm minimization we have proposed. [sent-302, score-0.291]
</p><p>93 In experiments, we show that our method is comparable to a fully supervised spectral method, and close to the performance of EM. [sent-303, score-0.147]
</p><p>94 Our approach follows a similar idea to that of [3] since both works combine classic ideas from spectral learning with classic ideas from low rank matrix completion. [sent-304, score-0.262]
</p><p>95 The basic idea is to frame learning of distributions over structured objects as a low-rank matrix factorization subject to linear constraints derived from observable statistics. [sent-305, score-0.208]
</p><p>96 This method applies to other grammatical inference domains, such as unsupervised spectral learning of PCFGs ([1]). [sent-306, score-0.226]
</p><p>97 Unsupervised spectral learning of wcfg as low-rank matrix completion. [sent-316, score-0.145]
</p><p>98 Spectral learning of general weighted automata via constrained matrix completion. [sent-328, score-0.063]
</p><p>99 Local loss optimization in operator models: A new insight into spectral learning. [sent-338, score-0.105]
</p><p>100 Inference of ﬁnite-state transducers by using regular grammars and morphisms. [sent-357, score-0.103]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fst', 0.631), ('hankel', 0.571), ('pre', 0.193), ('xes', 0.157), ('aligned', 0.14), ('rank', 0.117), ('alignments', 0.105), ('spectral', 0.105), ('balle', 0.097), ('ra', 0.089), ('sequences', 0.089), ('observable', 0.082), ('unaligned', 0.079), ('uv', 0.073), ('transducers', 0.071), ('grammatical', 0.071), ('alignment', 0.069), ('rt', 0.066), ('mx', 0.052), ('generalized', 0.052), ('unsupervised', 0.05), ('nuclear', 0.05), ('constraints', 0.049), ('bailly', 0.049), ('msi', 0.049), ('supposes', 0.049), ('uns', 0.049), ('proposition', 0.046), ('hx', 0.043), ('boots', 0.043), ('symbol', 0.042), ('supervised', 0.042), ('matrix', 0.04), ('carreras', 0.039), ('quattoni', 0.039), ('siddiqi', 0.039), ('agrees', 0.039), ('wa', 0.037), ('relaxation', 0.036), ('nition', 0.035), ('minimization', 0.034), ('xavier', 0.034), ('strings', 0.034), ('xn', 0.033), ('oh', 0.032), ('ariadna', 0.032), ('gaattcag', 0.032), ('pcfgs', 0.032), ('matrices', 0.03), ('access', 0.029), ('paired', 0.029), ('suf', 0.029), ('borja', 0.029), ('edit', 0.029), ('uu', 0.029), ('alphabets', 0.029), ('enlarged', 0.026), ('ps', 0.026), ('norm', 0.025), ('hidden', 0.025), ('empty', 0.025), ('ga', 0.025), ('em', 0.024), ('built', 0.024), ('automata', 0.023), ('kh', 0.023), ('target', 0.023), ('alphabet', 0.022), ('ran', 0.022), ('subject', 0.022), ('computes', 0.022), ('output', 0.021), ('let', 0.021), ('ux', 0.02), ('pair', 0.019), ('regular', 0.018), ('sequence', 0.018), ('symbols', 0.018), ('identi', 0.018), ('satisfying', 0.018), ('check', 0.018), ('mapping', 0.017), ('convex', 0.016), ('observation', 0.016), ('concerns', 0.016), ('linguistics', 0.016), ('yn', 0.016), ('frame', 0.015), ('summing', 0.015), ('song', 0.015), ('relaxations', 0.015), ('presents', 0.015), ('rapha', 0.014), ('universitat', 0.014), ('zwald', 0.014), ('grammars', 0.014), ('morphology', 0.014), ('ability', 0.014), ('language', 0.014), ('completion', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="342-tfidf-1" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>Author: Raphael Bailly, Xavier Carreras, Ariadna Quattoni</p><p>Abstract: Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identiﬁability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. 1</p><p>2 0.065523371 <a title="342-tfidf-2" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><p>3 0.061896153 <a title="342-tfidf-3" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>4 0.054215249 <a title="342-tfidf-4" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>Author: Le Song, Bo Dai</p><p>Abstract: Kernel embedding of distributions has led to many recent advances in machine learning. However, latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting. Furthermore, no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspeciﬁed. In this paper, we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspeciﬁcation. We also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation. 1</p><p>5 0.053274617 <a title="342-tfidf-5" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>6 0.053143673 <a title="342-tfidf-6" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>7 0.051790781 <a title="342-tfidf-7" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>8 0.050758235 <a title="342-tfidf-8" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>9 0.049958978 <a title="342-tfidf-9" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>10 0.047316551 <a title="342-tfidf-10" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>11 0.04301111 <a title="342-tfidf-11" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>12 0.042477556 <a title="342-tfidf-12" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>13 0.040039986 <a title="342-tfidf-13" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>14 0.039980218 <a title="342-tfidf-14" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>15 0.039588235 <a title="342-tfidf-15" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>16 0.038348891 <a title="342-tfidf-16" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>17 0.036913473 <a title="342-tfidf-17" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>18 0.036108028 <a title="342-tfidf-18" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>19 0.034945391 <a title="342-tfidf-19" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>20 0.033686932 <a title="342-tfidf-20" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.095), (1, 0.036), (2, 0.04), (3, 0.062), (4, -0.003), (5, -0.028), (6, -0.007), (7, -0.003), (8, -0.027), (9, -0.007), (10, 0.001), (11, 0.005), (12, 0.038), (13, 0.035), (14, 0.004), (15, 0.029), (16, -0.012), (17, 0.021), (18, -0.017), (19, -0.004), (20, -0.028), (21, -0.045), (22, -0.003), (23, 0.028), (24, 0.006), (25, 0.0), (26, -0.024), (27, -0.075), (28, -0.041), (29, 0.009), (30, -0.016), (31, -0.048), (32, -0.025), (33, -0.018), (34, 0.022), (35, 0.056), (36, 0.073), (37, -0.009), (38, 0.039), (39, 0.027), (40, 0.016), (41, -0.011), (42, 0.049), (43, -0.032), (44, 0.022), (45, 0.012), (46, -0.019), (47, -0.004), (48, 0.005), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87975663 <a title="342-lsi-1" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>Author: Raphael Bailly, Xavier Carreras, Ariadna Quattoni</p><p>Abstract: Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identiﬁability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. 1</p><p>2 0.73080391 <a title="342-lsi-2" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><p>3 0.72378737 <a title="342-lsi-3" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>Author: Troy Lee, Adi Shraibman</p><p>Abstract: In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries. This problem comes up in many application areas, and has received a great deal of attention in the context of the netﬂix prize. A central approach to this problem is to output a matrix of lowest possible complexity (e.g. rank or trace norm) that agrees with the partially speciﬁed matrix. The performance of this approach under the assumption that the revealed entries are sampled randomly has received considerable attention (e.g. [1, 2, 3, 4, 5, 6, 7, 8]). In practice, often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using. We present a means to obtain performance guarantees with respect to any set of initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possible complexity that agrees with the partially speciﬁed matrix. We give a new way to interpret the output of this algorithm by next ﬁnding a probability distribution over the non-revealed entries with respect to which a bound on the generalization error can be proven. The more complex the set of revealed entries according to a certain measure, the better the bound on the generalization error. 1</p><p>4 0.71982008 <a title="342-lsi-4" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>Author: Miao Xu, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: In standard matrix completion theory, it is required to have at least O(n ln2 n) observed entries to perfectly recover a low-rank matrix M of size n × n, leading to a large number of observations when n is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix M can be dramatically reduced to O(ln n). We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning. 1</p><p>5 0.67202818 <a title="342-lsi-5" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>Author: Dimitris Achlioptas, Zohar S. Karnin, Edo Liberty</p><p>Abstract: We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes A B 2 . For large m n matrices, such that n m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O 1 computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal ofﬂine distribution. Note that the probabilities in the optimal ofﬂine distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model. 1</p><p>6 0.65751237 <a title="342-lsi-6" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>7 0.62185675 <a title="342-lsi-7" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>8 0.60065663 <a title="342-lsi-8" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>9 0.58038723 <a title="342-lsi-9" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>10 0.56842142 <a title="342-lsi-10" href="./nips-2013-What_do_row_and_column_marginals_reveal_about_your_dataset%3F.html">352 nips-2013-What do row and column marginals reveal about your dataset?</a></p>
<p>11 0.54329473 <a title="342-lsi-11" href="./nips-2013-Provable_Subspace_Clustering%3A_When_LRR_meets_SSC.html">259 nips-2013-Provable Subspace Clustering: When LRR meets SSC</a></p>
<p>12 0.54274082 <a title="342-lsi-12" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>13 0.53979814 <a title="342-lsi-13" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>14 0.53951985 <a title="342-lsi-14" href="./nips-2013-Sinkhorn_Distances%3A_Lightspeed_Computation_of_Optimal_Transport.html">296 nips-2013-Sinkhorn Distances: Lightspeed Computation of Optimal Transport</a></p>
<p>15 0.51776057 <a title="342-lsi-15" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>16 0.5032655 <a title="342-lsi-16" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>17 0.4982512 <a title="342-lsi-17" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>18 0.49717033 <a title="342-lsi-18" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>19 0.49501684 <a title="342-lsi-19" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>20 0.47120956 <a title="342-lsi-20" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.01), (10, 0.338), (16, 0.015), (33, 0.126), (34, 0.077), (36, 0.011), (41, 0.026), (49, 0.029), (56, 0.123), (70, 0.022), (85, 0.037), (89, 0.032), (93, 0.033), (95, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69579923 <a title="342-lda-1" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>Author: Raphael Bailly, Xavier Carreras, Ariadna Quattoni</p><p>Abstract: Finite-State Transducers (FST) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications, ranging from computational biology to natural language processing. Recently Balle et al. [4] presented a spectral algorithm for learning FST from samples of aligned input-output sequences. In this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm. We frame FST learning as ﬁnding a low rank Hankel matrix satisfying constraints derived from observable statistics. Under this formulation, we provide identiﬁability results for FST distributions. Then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efﬁciently. 1</p><p>2 0.65792727 <a title="342-lda-2" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>Author: Boqing Gong, Kristen Grauman, Fei Sha</p><p>Abstract: In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difﬁcult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identiﬁed domains to be different from each other to the maximum extent; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric formulation and efﬁcient optimization procedure that can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks. 1</p><p>3 0.6156925 <a title="342-lda-3" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<p>Author: Daniel Vainsencher, Shie Mannor, Huan Xu</p><p>Abstract: We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufﬁciently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efﬁciently. We demonstrate the robustness beneﬁts of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a ﬁxed percentage of adversarial unbounded outliers. 1</p><p>4 0.6042279 <a title="342-lda-4" href="./nips-2013-Distributed_Exploration_in_Multi-Armed_Bandits.html">95 nips-2013-Distributed Exploration in Multi-Armed Bandits</a></p>
<p>Author: Eshcar Hillel, Zohar S. Karnin, Tomer Koren, Ronny Lempel, Oren Somekh</p><p>Abstract: We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an ε-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to com√ municate only once, they are able to learn k times faster than a single player. √ That is, distributing learning to k players gives rise to a factor k parallel speedup. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε. 1</p><p>5 0.51462501 <a title="342-lda-5" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>6 0.5139311 <a title="342-lda-6" href="./nips-2013-Information-theoretic_lower_bounds_for_distributed_statistical_estimation_with_communication_constraints.html">142 nips-2013-Information-theoretic lower bounds for distributed statistical estimation with communication constraints</a></p>
<p>7 0.5128696 <a title="342-lda-7" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>8 0.51201278 <a title="342-lda-8" href="./nips-2013-Using_multiple_samples_to_learn_mixture_models.html">344 nips-2013-Using multiple samples to learn mixture models</a></p>
<p>9 0.51183915 <a title="342-lda-9" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>10 0.51158601 <a title="342-lda-10" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>11 0.51157057 <a title="342-lda-11" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>12 0.51105034 <a title="342-lda-12" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>13 0.51092279 <a title="342-lda-13" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>14 0.51055306 <a title="342-lda-14" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>15 0.50990993 <a title="342-lda-15" href="./nips-2013-Which_Space_Partitioning_Tree_to_Use_for_Search%3F.html">355 nips-2013-Which Space Partitioning Tree to Use for Search?</a></p>
<p>16 0.50980562 <a title="342-lda-16" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>17 0.50872797 <a title="342-lda-17" href="./nips-2013-Local_Privacy_and_Minimax_Bounds%3A_Sharp_Rates_for_Probability_Estimation.html">177 nips-2013-Local Privacy and Minimax Bounds: Sharp Rates for Probability Estimation</a></p>
<p>18 0.50859618 <a title="342-lda-18" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>19 0.50857919 <a title="342-lda-19" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>20 0.50855958 <a title="342-lda-20" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
