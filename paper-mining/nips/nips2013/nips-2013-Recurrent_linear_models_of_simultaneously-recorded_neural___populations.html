<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-266" href="#">nips2013-266</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</h1>
<br/><p>Source: <a title="nips-2013-266-pdf" href="http://papers.nips.cc/paper/4877-recurrent-linear-models-of-simultaneously-recorded-neural-populations.pdf">pdf</a></p><p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>Reference: <a title="nips-2013-266-reference" href="../nips2013_reference/nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Recurrent linear models of simultaneously-recorded neural populations  Marius Pachitariu, Biljana Petreska, Maneesh Sahani Gatsby Computational Neuroscience Unit University College London, UK {marius,biljana,maneesh}@gatsby. [sent-1, score-0.062]
</p><p>2 uk  Abstract Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. [sent-4, score-0.172]
</p><p>3 Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. [sent-5, score-0.079]
</p><p>4 We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. [sent-6, score-0.152]
</p><p>5 We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. [sent-7, score-0.26]
</p><p>6 We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. [sent-8, score-0.263]
</p><p>7 We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. [sent-9, score-0.205]
</p><p>8 The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. [sent-10, score-0.038]
</p><p>9 The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. [sent-11, score-0.061]
</p><p>10 The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. [sent-12, score-0.033]
</p><p>11 1  Introduction  Many essential neural computations are implemented by large populations of neurons working in concert, and recent studies have sought both to monitor increasingly large groups of neurons [1, 2] and to characterise their collective behaviour [3, 4]. [sent-13, score-0.278]
</p><p>12 In this paper we introduce a new computational tool to model coordinated behaviour in very large neural data sets. [sent-14, score-0.11]
</p><p>13 While we explicitly discuss only multi-electrode extracellular recordings, the same model can be readily used to characterise 2-photon calcium-marker image data, EEG, fMRI or even large-scale biologically-faithful simulations. [sent-15, score-0.081]
</p><p>14 Populational neural data may be represented at each time point by a vector yt with as many dimensions as neurons, and as many indices t as time points in the experiment. [sent-16, score-0.634]
</p><p>15 For spiking neurons, yt will have positive integer elements corresponding to the number of spikes ﬁred by each neuron in the time interval corresponding to the t-th bin. [sent-17, score-0.627]
</p><p>16 As others have before [5, 6], we assume that the coordinated activity reﬂected in the measurement yt arises from a low-dimensional set of processes, collected into a vector xt , which is not directly observed. [sent-18, score-0.869]
</p><p>17 However, unlike the previous studies, we construct a recurrent model in which the hidden processes xt are driven directly and explicitly by the measured neural signals y1 . [sent-19, score-0.361]
</p><p>18 We assume for simplicity that xt evolves with linear dynamics and affects the future state of the neural signal yt in a generalised-linear manner, although both assumptions may be relaxed. [sent-24, score-0.907]
</p><p>19 As in the latent dynamical system, the resulting model enforces a “bottleneck”, whereby predictions of yt based on y1 . [sent-25, score-0.795]
</p><p>20 1  State prediction in the RLM is related to the Kalman ﬁlter [7] and we show in the next section a formal equivalence between the likelihoods of the RLM and the latent dynamical model when observation noise is Gaussian distributed. [sent-29, score-0.217]
</p><p>21 However, spiking data is not well modelled as Gaussian, and the generalisation of our approach to Poisson noise leads to a departure from the latent dynamical approach. [sent-30, score-0.336]
</p><p>22 Unlike latent linear models with conditionally Poisson observations, the parameters of our model can be estimated efﬁciently and without approximation. [sent-31, score-0.093]
</p><p>23 We show that, perhaps in consequence, the RLM can provide superior descriptions of neural population data. [sent-32, score-0.082]
</p><p>24 2  From the Kalman ﬁlter to the recurrent linear model (RLM)  Consider a latent linear dynamical system (LDS) model with linear-Gaussian observations. [sent-33, score-0.34]
</p><p>25 The latent process is parametrised by a dynamics matrix A and innovations covariance Q that describe the evolution of the latent state xt : P (xt |xt−1 ) = N (xt |Axt−1 , Q) , where N (x|µ, Σ) represents a normal distribution on x with mean µ and (co)variance Σ. [sent-36, score-0.661]
</p><p>26 The output distribution is determined by an observation loading matrix C and a noise covariance R often taken to be diagonal so that all covariance is modelled by the latent process: P (yt |xt ) = N (yt |Cxt , R) . [sent-38, score-0.363]
</p><p>27 In the LDS, the joint likelihood of the observations {yt } can be written as the product: T  P (yt |y1 . [sent-39, score-0.041]
</p><p>28 yT ) = P (y1 ) t=2  and in the Gaussian case can be computed using the usual Kalman ﬁlter approach to ﬁnd the conditional distributon at time t iteratively: P (yt+1 |y1 . [sent-45, score-0.023]
</p><p>29 yt ) ˆ dxt+1 N (yt+1 |Cxt+1 , R) N (xt+1 |Axt , Vt+1 )  ˆ = N (yt+1 |CAxt , CVt+1 C + R) , ˆ where we have introduced the (ﬁltered) state estimate xt = E [xt |y1 . [sent-51, score-0.824]
</p><p>30 yt ] and (predictive) unˆ certainty Vt+1 = E (xt+1 − Axt )2 |y1 . [sent-54, score-0.601]
</p><p>31 Both quantities are computed recursively using the Kalman gain Kt = Vt C (CVt C + R)−1 , giving the following recursive recipe to calculate the conditional likelihood of yt+1 : ˆ ˆ ˆ xt = Axt−1 + Kt (yt − yt ) Vt+1 = A(I − Kt C)Vt A + Q ˆ ˆ yt+1 = CAxt ˆ P (yt+1 |y1 . [sent-58, score-0.865]
</p><p>32 yt ) = N (yt+1 |yt+1 , CVt+1 C + R) For the Gaussian LDS, the Kalman gain Kt and state uncertainty Vt+1 (and thus the output covariance CVt+1 C + R) depend on the model parameters (A, C, R, Q) and on the time step—although as time grows they both converge to stationary values. [sent-61, score-0.767]
</p><p>33 Thus, we might consider a relaxation of the Gaussian LDS model in which these matrices are taken to be stationary from the outset, and are parametrised independently so that they are no longer constrained to take on the “correct” values as computed for Kalman inference. [sent-63, score-0.071]
</p><p>34 Let us call this parametric form of the Kalman gain W and the parametric form of the output covariance S. [sent-64, score-0.121]
</p><p>35 Then the conditional likelihood iteration becomes ˆ ˆ ˆ xt = Axt−1 + W (yt − yt ) ˆ ˆ yt+1 = CAxt ˆ P (yt+1 |y1 . [sent-65, score-0.824]
</p><p>36 Shaded LDS variables are observed, unshaded    circles are latent random variables     and squares are variables that depend deterministically on their parents. [sent-70, score-0.121]
</p><p>37 In B the LDS is redrawn in terms of the random innovations ηt = xt − Axt−1 , facilitating the transition towards the RLM. [sent-71, score-0.307]
</p><p>38 The RLM RLM is then obtained by replacing ηt with a deterministically derived ˆ estimate W (yt − yt ). [sent-72, score-0.65]
</p><p>39 This is a relaxation of the Gaussian latent LDS model because W has more degrees of freedom than Q, as does S than R (at least if R is constrained to be diagonal). [sent-74, score-0.093]
</p><p>40 The new model has a recurrent linear structure in that the random ˆ observation yt is fed back linearly to perturb the otherwise deterministic evolution of the state xt . [sent-75, score-1.054]
</p><p>41 The RLM can be viewed as replacing the random innovation variables ηt = xt − Axt−1 ˆ with data-derived estimates W (yt − yt ); estimates which are made possible by the fact that ηt ˆ contributes to the variability of yt around yt . [sent-78, score-2.026]
</p><p>42 3  Recurrent linear models with Poisson observations  The discussion above has transformed a stochastic-latent LDS model with Gaussian output to an RLM with deterministic latent, but still with Gaussian output. [sent-79, score-0.088]
</p><p>43 Our goal, however, is to ﬁt a model with an output distribution better suited to the binned point-processes that characterise neural spiking. [sent-80, score-0.143]
</p><p>44 Both linear Kalman-ﬁltering steps above and the eventual stationarity of the inference parameters depend on the joint Gaussian structure of the assumed LDS model. [sent-81, score-0.039]
</p><p>45 They would not apply if we were to begin a similar derivation from an LDS with Poisson output. [sent-82, score-0.026]
</p><p>46 However, a tractable approach to modelling point-process data with low-dimensional temporal structure may be provided by introducing a generalised-linear output stage directly to the RLM. [sent-83, score-0.076]
</p><p>47 This model is given by: ˆ ˆ ˆ xt = Axt−1 + W (yt − yt ) ˆ ˆ g(yt+1 ) = CAxt ˆ P (yt+1 |y1 . [sent-84, score-0.803]
</p><p>48 yt ) = ExpFam(yt+1 |yt+1 )  (1)  where ExpFam is an exponential-family distribution such as Poisson, and the element-wise link ˆ function g allows for a nonlinear mapping from xt to the predicted mean yt+1 . [sent-87, score-0.849]
</p><p>49 In the following, we ˆ ˆ will write f for the inverse-link as is more common for neural models, so that yt+1 = f(CAxt ). [sent-88, score-0.033]
</p><p>50 The simplest Poisson-based generalised-linear RLM might take as its output distribution ˆ P (yt |yt ) =  Poisson(yti |ˆti ); y  ˆ ˆ yt = f(CAxt−1 )) ,  i  where yti is the spike count of the ith cell in bin t and the function f is non-negative. [sent-89, score-0.813]
</p><p>51 However, comparison with the output distribution derived for the Gaussian RLM suggests that this choice would fail to capture the instantaneous covariance that the LDS formulation transfers to the output distribution (and which appears in the low-rank structure of S above). [sent-90, score-0.204]
</p><p>52 One option is to bin the data more ﬁnely, thus diminishing the inﬂuence of the instantaneous covariance. [sent-92, score-0.082]
</p><p>53 The alternative is to replace the independent Poissons with a correlated output distribution on spike counts. [sent-93, score-0.118]
</p><p>54 The cascaded generalised-linear model introduced below is a natural choice, and we will show that it captures instantaneous correlations faithfully with very few hidden dimensions. [sent-94, score-0.172]
</p><p>55 3  In practice, we also sometimes add a ﬁxed input µt to equation 1 that varies in time and determines the average behavior of the population or the peri-stimulus time histogram (PSTH). [sent-95, score-0.049]
</p><p>56 The matrix A controls the evolution of the dynamical process xt . [sent-97, score-0.364]
</p><p>57 The phenomenology of its dynamics is determined by the complex eigenvalues of A. [sent-98, score-0.111]
</p><p>58 Eigenvalues with moduli close to 1 correspond to long timescales of ﬂuctuation around the PSTH. [sent-99, score-0.042]
</p><p>59 Eigenvalues with non-zero imaginary part correspond to oscillatory components. [sent-100, score-0.02]
</p><p>60 Finally, the dynamics will be stable iff all the eigenvalues lie within the unit disc. [sent-101, score-0.088]
</p><p>61 The matrix C describes the dependence of the high-dimensional neural signals on the lowdimensional latent processes xt . [sent-102, score-0.347]
</p><p>62 This generalised-linear stage ensures that the ﬁring rates are positive through the link function f, and the observation process is Poisson. [sent-104, score-0.096]
</p><p>63 For other types of data, the generalised-linear stage might be replaced by other appropriate link functions and output distributions. [sent-105, score-0.122]
</p><p>64 1  Relationship to other models  RLMs are related to recurrent neural networks [8]. [sent-107, score-0.159]
</p><p>65 The differences lie in the state evolution, which in the neural network is nonlinear: xt = h (Axt−1 + W yt−1 ); and in the recurrent term which depends on the observation rather than the prediction error. [sent-108, score-0.405]
</p><p>66 On the data considered here, we found that using sigmoidal or threshold-linear functions h resulted in models comparable in likelihood to the RLM, and so we restricted our attention to simple linear dynamics. [sent-109, score-0.045]
</p><p>67 We also found that ˆ using the prediction error term W (yt−1 − yt ) resulted in better models than the simple neural-net formulation, and we attribute this difference to the link between the RLM and Kalman inference. [sent-110, score-0.671]
</p><p>68 It is also possible to work within the stochatic latent LDS framework, replacing the Gaussian output distribution with a generalised-linear Poisson output (e. [sent-111, score-0.212]
</p><p>69 For an unobserved latent process xt , an inference procedure needs to be devised to estimate the posterior distribution on the entire sequence x1 . [sent-115, score-0.295]
</p><p>70 However, with generalised-linear observations, inference becomes intractable and the necessary approximations [6] are computationally intense and can jeopardize the quality of the ﬁtted models. [sent-120, score-0.022]
</p><p>71 By contrast, in the RLM xt is a deterministic function of data. [sent-121, score-0.221]
</p><p>72 In effect, the Kalman ﬁlter has been built into the model as the accurate estimation procedure, and efﬁcient ﬁtting is possible by direct gradient ascent on the log-likelihood. [sent-122, score-0.02]
</p><p>73 Note that to estimate the matrices A and W the gradient must be backpropagated through successive iterations of equation 1. [sent-125, score-0.022]
</p><p>74 This technique, known as backpropagation-through-time, was ﬁrst described by [10] as a technique to ﬁt recurrent neural network models. [sent-126, score-0.159]
</p><p>75 Backpropagation-through-time is thought to be inherently unstable when propagated past many timesteps and often the gradient is truncated prematurely [11]. [sent-128, score-0.023]
</p><p>76 We found that using large values of momentum in the gradient ascent alleviated these instabilities and allowed us to use backpropagation without the truncation. [sent-129, score-0.043]
</p><p>77 4  The cascaded generalised-linear model (CGLM)  The link between the RLM and the LDS raises the possibility that a model for simultaneouslyrecorded correlated spike counts might be derived in a similar way, starting from a non-dynamical, but low-dimensional, Gaussian model. [sent-130, score-0.24]
</p><p>78 Stationary models of population activity have attracted recent interest for their own sake (e. [sent-131, score-0.068]
</p><p>79 [1]), and would also provide a way model correlations introduced by common innovations that were neglected by the simple Poisson form of the RLM. [sent-133, score-0.101]
</p><p>80 Thus, we consider vectors y of spike counts from N neurons, without explicit reference to the time at which they were collected. [sent-134, score-0.09]
</p><p>81 A Gaussian model for y can certainly describe correlations between the cells, but is ill-matched to discrete count observations. [sent-135, score-0.064]
</p><p>82 Thus, as with the derivation of the RLM from the Kalman ﬁlter, we derive here a new generalisation of a low-dimensional, structured Gaussian model to spike count data. [sent-136, score-0.185]
</p><p>83 4  The distribution of any multivariate variable y can be factorized into a “cascaded” product of multiple one-dimensional distributions: N  P (yn |y < n and ≤ n restrict the matrix to the ﬁrst (n − 1) and n rows and/or columns respectively. [sent-137, score-0.019]
</p><p>84 Thus, we might construct suitably structured linear weights for the CGLM by applying this result to the covariance matrix induced by the low-dimensional Gaussian model known as factor analysis [12]. [sent-138, score-0.069]
</p><p>85 Factor analysis assumes that data are generated from a K-dimensional latent process x ∼ N (0, I), where I is the K×K identity matrix, and y has the conditional distribution P (y|x) = N (Λx, Ψ) with Ψ a diagonal matrix and Λ an N × K loading matrix. [sent-139, score-0.155]
</p><p>86 This leads to a covariance of y given by Σ = Ψ + ΛΛT . [sent-140, score-0.05]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yt', 0.601), ('rlm', 0.446), ('lds', 0.328), ('xt', 0.202), ('caxt', 0.164), ('cglm', 0.16), ('axt', 0.155), ('kalman', 0.152), ('recurrent', 0.126), ('dynamical', 0.101), ('cvt', 0.094), ('latent', 0.093), ('poisson', 0.082), ('cascaded', 0.081), ('spike', 0.069), ('vt', 0.066), ('generalisation', 0.061), ('characterise', 0.061), ('instantaneous', 0.056), ('expfam', 0.053), ('rlms', 0.053), ('kt', 0.051), ('dynamics', 0.05), ('covariance', 0.05), ('output', 0.049), ('population', 0.049), ('neurons', 0.048), ('coordinated', 0.047), ('parametrised', 0.047), ('link', 0.046), ('gaussian', 0.046), ('lter', 0.045), ('innovations', 0.044), ('dxt', 0.043), ('redrawn', 0.043), ('loading', 0.043), ('evolution', 0.042), ('cxt', 0.041), ('yti', 0.039), ('recordings', 0.038), ('eigenvalues', 0.038), ('ca', 0.037), ('modelled', 0.036), ('correlations', 0.035), ('neural', 0.033), ('behaviour', 0.03), ('collective', 0.029), ('count', 0.029), ('populations', 0.029), ('deterministically', 0.028), ('stage', 0.027), ('spiking', 0.026), ('derivation', 0.026), ('bin', 0.026), ('stationary', 0.024), ('resulted', 0.024), ('sn', 0.024), ('simultaneouslyrecorded', 0.023), ('outset', 0.023), ('phenomenology', 0.023), ('prematurely', 0.023), ('moduli', 0.023), ('instabilities', 0.023), ('distributon', 0.023), ('concert', 0.023), ('pachitariu', 0.023), ('ring', 0.023), ('observation', 0.023), ('gain', 0.022), ('neglected', 0.022), ('intense', 0.022), ('backpropagated', 0.022), ('generalising', 0.022), ('marius', 0.022), ('poissons', 0.022), ('petreska', 0.022), ('state', 0.021), ('replacing', 0.021), ('counts', 0.021), ('likelihood', 0.021), ('perturb', 0.02), ('uctuation', 0.02), ('imaginary', 0.02), ('eventual', 0.02), ('extracellular', 0.02), ('ascent', 0.02), ('observations', 0.02), ('system', 0.02), ('timescales', 0.019), ('maneesh', 0.019), ('stationarity', 0.019), ('nely', 0.019), ('recipe', 0.019), ('departure', 0.019), ('matrix', 0.019), ('activity', 0.019), ('deterministic', 0.019), ('eeg', 0.018), ('facilitating', 0.018), ('sahani', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="266-tfidf-1" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>2 0.29188111 <a title="266-tfidf-2" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>3 0.19481342 <a title="266-tfidf-3" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>Author: Jonas Peters, Dominik Janzing, Bernhard Schölkopf</p><p>Abstract: Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identiﬁability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufﬁcient or the model is misspeciﬁed, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artiﬁcial and real data and code is provided. 1</p><p>4 0.15724128 <a title="266-tfidf-4" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>Author: Huahua Wang, Arindam Banerjee, Cho-Jui Hsieh, Pradeep Ravikumar, Inderjit Dhillon</p><p>Abstract: We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in columnblocks and only involves elementwise operations and parallel matrix multiplications. We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efﬁciency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores. 1</p><p>5 0.14011398 <a title="266-tfidf-5" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>Author: Josh S. Merel, Roy Fox, Tony Jebara, Liam Paninski</p><p>Abstract: In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user’s neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modiﬁed decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.</p><p>6 0.13917147 <a title="266-tfidf-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>7 0.13880499 <a title="266-tfidf-7" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>8 0.13800612 <a title="266-tfidf-8" href="./nips-2013-Online_Learning_with_Costly_Features_and_Labels.html">230 nips-2013-Online Learning with Costly Features and Labels</a></p>
<p>9 0.11872735 <a title="266-tfidf-9" href="./nips-2013-Multilinear_Dynamical_Systems_for_Tensor_Time_Series.html">203 nips-2013-Multilinear Dynamical Systems for Tensor Time Series</a></p>
<p>10 0.1154085 <a title="266-tfidf-10" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>11 0.11289948 <a title="266-tfidf-11" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>12 0.10081164 <a title="266-tfidf-12" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>13 0.09766873 <a title="266-tfidf-13" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>14 0.097563073 <a title="266-tfidf-14" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<p>15 0.092171304 <a title="266-tfidf-15" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>16 0.090122007 <a title="266-tfidf-16" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>17 0.089003786 <a title="266-tfidf-17" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>18 0.08343982 <a title="266-tfidf-18" href="./nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</a></p>
<p>19 0.081234321 <a title="266-tfidf-19" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>20 0.078255653 <a title="266-tfidf-20" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, 0.024), (2, 0.028), (3, -0.066), (4, -0.223), (5, -0.018), (6, 0.015), (7, 0.098), (8, 0.044), (9, -0.15), (10, -0.094), (11, -0.238), (12, -0.038), (13, 0.071), (14, -0.131), (15, -0.016), (16, 0.032), (17, 0.106), (18, 0.063), (19, 0.078), (20, 0.077), (21, 0.042), (22, 0.126), (23, -0.054), (24, 0.005), (25, -0.076), (26, 0.053), (27, -0.034), (28, 0.034), (29, -0.006), (30, 0.055), (31, -0.063), (32, 0.04), (33, 0.049), (34, -0.071), (35, 0.072), (36, -0.089), (37, -0.023), (38, 0.082), (39, 0.007), (40, 0.041), (41, 0.032), (42, 0.019), (43, 0.031), (44, -0.008), (45, 0.022), (46, 0.127), (47, -0.069), (48, -0.007), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97299081 <a title="266-lsi-1" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>2 0.76151741 <a title="266-lsi-2" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>Author: Jonas Peters, Dominik Janzing, Bernhard Schölkopf</p><p>Abstract: Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identiﬁability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufﬁcient or the model is misspeciﬁed, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artiﬁcial and real data and code is provided. 1</p><p>3 0.70810622 <a title="266-lsi-3" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>Author: Josh S. Merel, Roy Fox, Tony Jebara, Liam Paninski</p><p>Abstract: In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user’s neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modiﬁed decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.</p><p>4 0.62133205 <a title="266-lsi-4" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>Author: David Pfau, Eftychios A. Pnevmatikakis, Liam Paninski</p><p>Abstract: Recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics. Finding these dynamics requires models that take into account biophysical constraints and can be ﬁt efﬁciently and robustly. Here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global inﬂuences. The results can be combined with spectral methods to learn dynamical systems models. The basic method extends PCA to the exponential family using nuclear norm minimization. We evaluate the effectiveness of this method using an exact decomposition of the Bregman divergence that is analogous to variance explained for PCA. We show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace. We also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics. Finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to ﬁtting linear dynamical models without nuclear norm smoothing. 1</p><p>5 0.61611652 <a title="266-lsi-5" href="./nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</a></p>
<p>Author: Botond Cseke, Manfred Opper, Guido Sanguinetti</p><p>Abstract: We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid ﬁxed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and signiﬁcant computational savings compared to discrete-time approaches in a neural application. 1</p><p>6 0.56342351 <a title="266-lsi-6" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>7 0.544469 <a title="266-lsi-7" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>8 0.50290132 <a title="266-lsi-8" href="./nips-2013-Large_Scale_Distributed_Sparse_Precision_Estimation.html">146 nips-2013-Large Scale Distributed Sparse Precision Estimation</a></p>
<p>9 0.47765619 <a title="266-lsi-9" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>10 0.4420318 <a title="266-lsi-10" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>11 0.44074732 <a title="266-lsi-11" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>12 0.43011779 <a title="266-lsi-12" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>13 0.42781442 <a title="266-lsi-13" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<p>14 0.42271006 <a title="266-lsi-14" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>15 0.41006637 <a title="266-lsi-15" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>16 0.40674096 <a title="266-lsi-16" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>17 0.40143552 <a title="266-lsi-17" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>18 0.39405757 <a title="266-lsi-18" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>19 0.38815817 <a title="266-lsi-19" href="./nips-2013-Optimization%2C_Learning%2C_and_Games_with_Predictable_Sequences.html">240 nips-2013-Optimization, Learning, and Games with Predictable Sequences</a></p>
<p>20 0.38599327 <a title="266-lsi-20" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.038), (33, 0.101), (34, 0.085), (41, 0.024), (49, 0.46), (56, 0.086), (70, 0.03), (85, 0.015), (89, 0.024), (93, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91050082 <a title="266-lda-1" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>Author: Tuan A. Nguyen, Subbarao Kambhampati, Minh Do</p><p>Abstract: Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we ﬁrst introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of ﬁnding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner. 1</p><p>2 0.89681029 <a title="266-lda-2" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>3 0.85403818 <a title="266-lda-3" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>Author: Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan</p><p>Abstract: Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a “relevance topic model” for jointly learning meaningful mid-level representations upon bagof-words (BoW) video representations and a classiﬁer with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectiﬁed linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efﬁcient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classiﬁcation accuracy, particularly in the case of a very small number of labeled training videos. 1</p><p>same-paper 4 0.82940727 <a title="266-lda-4" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>5 0.77939051 <a title="266-lda-5" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>6 0.7610324 <a title="266-lda-6" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>7 0.73692948 <a title="266-lda-7" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>8 0.67801738 <a title="266-lda-8" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>9 0.66080689 <a title="266-lda-9" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>10 0.64107579 <a title="266-lda-10" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>11 0.58746606 <a title="266-lda-11" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>12 0.54529023 <a title="266-lda-12" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>13 0.54379308 <a title="266-lda-13" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>14 0.53260154 <a title="266-lda-14" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>15 0.49990547 <a title="266-lda-15" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>16 0.49790543 <a title="266-lda-16" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>17 0.49630278 <a title="266-lda-17" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>18 0.49384776 <a title="266-lda-18" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>19 0.49071664 <a title="266-lda-19" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>20 0.48990458 <a title="266-lda-20" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
