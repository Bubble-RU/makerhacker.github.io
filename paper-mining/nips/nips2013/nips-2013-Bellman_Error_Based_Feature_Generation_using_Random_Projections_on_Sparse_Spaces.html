<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-55" href="#">nips2013-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</h1>
<br/><p>Source: <a title="nips-2013-55-pdf" href="http://papers.nips.cc/paper/5182-bellman-error-based-feature-generation-using-random-projections-on-sparse-spaces.pdf">pdf</a></p><p>Author: Mahdi Milani Fard, Yuri Grinberg, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a ﬁnite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging. 1</p><p>Reference: <a title="nips-2013-55-reference" href="../nips2013_reference/nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. [sent-3, score-0.364]
</p><p>2 Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. [sent-4, score-0.221]
</p><p>3 We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. [sent-5, score-0.155]
</p><p>4 We provide a ﬁnite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. [sent-6, score-0.357]
</p><p>5 The accuracy of parametrized policy evaluation depends crucially on the quality of the features used in the function approximator, and thus often a lot of time and effort is spent on this step. [sent-12, score-0.332]
</p><p>6 The desire to make this process more automatic has led to a lot of recent work on feature generation and feature selection in RL (e. [sent-13, score-0.312]
</p><p>7 An approach that offers good theoretical guarantees is to generate features in the direction of the Bellman error of the current value estimates (Bellman Error Based features, or BEBF). [sent-16, score-0.243]
</p><p>8 Successively adding exact BEBFs has been shown to reduce the error of a linear value function estimator at a rate similar to value iteration, which is the best one could hope to achieve [6]. [sent-17, score-0.165]
</p><p>9 Several successful methods have been proposed for generating features related to the Bellman error [5, 1, 4, 6, 3]. [sent-19, score-0.204]
</p><p>10 In this paper, we present an algorithm that uses the idea of applying random projections speciﬁcally in very large and sparse feature spaces (e. [sent-22, score-0.485]
</p><p>11 Random projections have been studied extensively in signal processing [8, 9] as well as machine learning [10, 11, 12, 13]. [sent-27, score-0.243]
</p><p>12 [14] have used random projections in conjunction with LSTD and have shown that this can reduce the estimation error, 1  at the cost of a controlled bias. [sent-29, score-0.213]
</p><p>13 Instead of compressing the feature space for LSTD, we focus on the BEBF generation setting, which offers better scalability and more ﬂexibility in practice. [sent-30, score-0.272]
</p><p>14 Our algorithm is well suited for sparse feature spaces, naturally occurring in domains with audio and video inputs [15], and also in tile-coded and discretized spaces. [sent-31, score-0.223]
</p><p>15 Our analysis holds for both ﬁnite and continuous state spaces and is easy to apply with discretized or tile-coded features, which are popular in many RL applications. [sent-33, score-0.214]
</p><p>16 The proposed method compares favourably, from a computational point of view, to many other feature extraction methods in high dimensional spaces, as each iteration takes only poly-logarithmic time in the number of dimensions. [sent-34, score-0.232]
</p><p>17 |s, a) deﬁnes the distribution of next state given that action a is taken in state s, and a (possibly stochastic) bounded reward function R : S × A → M([0, Rmax ]). [sent-61, score-0.193]
</p><p>18 A policy is a (possibly stochastic) function from states to actions. [sent-65, score-0.182]
</p><p>19 The value of a state s for policy π, denoted by V π (s), is the expected value of the discounted sum of rewards ( t γ t rt ) if the agent starts in state s and acts according to policy π. [sent-66, score-0.617]
</p><p>20 Let R(s, π(s)) be the expected reward at state s under policy π. [sent-67, score-0.286]
</p><p>21 (1)  Many methods have been developed for ﬁnding the value of a policy (policy evaluation) when the transition and reward functions are known. [sent-69, score-0.266]
</p><p>22 When the state space is very large or continuous, the value function is also approximated using a feature vector xs , which is a function of the state s. [sent-71, score-0.292]
</p><p>23 To simplify the derivations, we use V (x) to directly refer to the value estimate of a state with feature vector x. [sent-73, score-0.235]
</p><p>24 Using LSTD in spaces induced by random projections is a way of dealing with this problem [14]. [sent-76, score-0.36]
</p><p>25 As we show in our experiments, if the observation space is sparse, we can also use conjugate gradient descent methods to solve the regularized LSTD problem. [sent-77, score-0.165]
</p><p>26 Stochastic gradient descent methods are alternatives to LSTD in high-dimensional state spaces, as their memory and computational complexity per time step are linear in the number of state features, while providing convergence guarantees [20]. [sent-78, score-0.191]
</p><p>27 Feature selection/extraction methods have thus been used to build better approximation spaces for the value functions [1, 2, 3, 4, 5]. [sent-82, score-0.185]
</p><p>28 Among these, we focus on methods that aim to generate features in the direction of the Bellman error deﬁned as: eV (. [sent-83, score-0.204]
</p><p>29 Given an estimate V of the value function, temporal difference (TD) errors are deﬁned to be: δt = rt + γV (xt+1 ) − V (xt ). [sent-88, score-0.173]
</p><p>30 (4) It is easy to show that the expectation of the temporal difference at xt equals the Bellman error at that point [16]. [sent-89, score-0.245]
</p><p>31 [6] later showed that any BEBF extraction method with small angular error will provably tighten the approximation error of the value function estimate. [sent-97, score-0.333]
</p><p>32 We note that these algorithms, although theoretically interesting, are difﬁcult to apply to very large state spaces or need speciﬁc domain knowledge to generate good features. [sent-100, score-0.176]
</p><p>33 Our proposed solution leverages the use of simple random projections to alleviate this problem. [sent-102, score-0.213]
</p><p>34 3  Random Projections and Inner Product  Random projections have been introduced in signal processing, as an efﬁcient method for compressing very high-dimensional signals (such as images or video). [sent-104, score-0.282]
</p><p>35 It is well known that random projections of appropriate sizes preserve enough information to exactly reconstruct the original signal with high probability [22, 9]. [sent-105, score-0.325]
</p><p>36 This is because random projections are norm and distance-preserving in many classes of feature spaces. [sent-106, score-0.371]
</p><p>37 (5) Recently, it has been shown that random projections of appropriate sizes preserve linearity of a target function on sparse feature spaces. [sent-112, score-0.511]
</p><p>38 Then, for prj0 = 48k log 4D , with probability > 1 − ξ0 : d ξ0 ∀x ∈ X : (ΦT w)T (ΦT x) − wT x ≤  3  (ξ0 ) prj  w  x ,  (6)  ˜ Hence, projections of size O(k log D) preserve the linearity up to an arbitrary constant. [sent-117, score-0.418]
</p><p>39 Along with the analysis of the variance of the estimators, this helps bound the prediction error of the linear ﬁt in the compressed space. [sent-118, score-0.377]
</p><p>40 We ﬁrst project the state features into a much smaller space and then regress a hyperplane to the TD-errors. [sent-120, score-0.204]
</p><p>41 For simplicity, we assume that regardless of the current estimate of the value function, the Bellman error is always linearly representable in the original feature space. [sent-121, score-0.233]
</p><p>42 This seems like a strong assumption, but is true, for example, in virtually any discretized space, and is also likely to hold in very high dimensional feature spaces1 . [sent-122, score-0.178]
</p><p>43 Let Vm be an estimated value function described in a linear space deﬁned by a feature set Ψ = {ψ1 , . [sent-124, score-0.174]
</p><p>44 [6] show that if we add a new BEBF ψm+1 = eVm to the feature set, (with mild assumptions) the approximation error on the new linear space shrinks by a factor of γ. [sent-129, score-0.251]
</p><p>45 They also show that if we can estimate the Bellman error within a constant angular error, cos−1 (γ), the error will still shrink. [sent-130, score-0.21]
</p><p>46 Estimating the Bellman error by regressing to temporal differences in high-dimensional sparse spaces can result in large prediction error. [sent-131, score-0.369]
</p><p>47 This is due to the large estimation error of regression in high dimensional spaces (over-ﬁtting). [sent-132, score-0.3]
</p><p>48 However, as discussed in Lemma 1, random projections were shown to exponentially reduce the dimension of a sparse feature space, only at the cost of a controlled constant bias. [sent-133, score-0.368]
</p><p>49 A variance analysis along with proper mixing conditions can also bound the estimation error due to the variance in MDP returns. [sent-134, score-0.153]
</p><p>50 The computational cost of the estimation is also much smaller when the regression is applied in the compressed space. [sent-135, score-0.28]
</p><p>51 The algorithm iteratively constructs new features using compressed linear regression to the TD-errors, and uses these features with a policy evaluation algorithm to update the estimate of the value function. [sent-138, score-0.768]
</p><p>52 Algorithm 1 Compressed Bellman Error Based Feature Generation (CBEBF) Input: Sample trajectory Sn = ((xt , rt )n ), where xt is the observation received at time t, and t=1 rt is the observed reward; Number of BEBFs: m; Projection size schedule: d1 , d2 , . [sent-139, score-0.25]
</p><p>53 Apply compressed regression: Let udi ×1 be the result of OLS regression in the compressed space, using ΦT xt as inputs and δt as outputs. [sent-148, score-0.578]
</p><p>54 Apply policy evaluation with features {ˆv (x) = xT v | v ∈ Ψ} to update V (. [sent-150, score-0.332]
</p><p>55 e end for The optimal number of BEBFs and the schedule of projection sizes need to be determined and are subjects of future work. [sent-152, score-0.194]
</p><p>56 But we show in the next section that logarithmic size projections should be enough to guarantee the reduction of error in value function prediction at each step. [sent-153, score-0.448]
</p><p>57 This makes the algorithm very attractive when it comes to computational and memory complexity, as the regression at each step is only on a small projected feature space. [sent-154, score-0.241]
</p><p>58 We assume linearity of the Bellman error to simplify the derivations. [sent-157, score-0.178]
</p><p>59 In the simpliﬁed version, instead of storing the features in each iteration, new features are added to the value function approximator with constant weight 1. [sent-161, score-0.309]
</p><p>60 It is important to note that once we use linear value function approximation, the entire BEBF generation process can be viewed as a regularized value iteration algorithm. [sent-165, score-0.258]
</p><p>61 The coefﬁcients of this linear backup are conﬁned to a lower-dimensional random subspace implicitly induced by the random projection used in each iteration. [sent-167, score-0.169]
</p><p>62 The following theorem shows how well we can estimate the Bellman error by regression to the TDerrors in a compressed space. [sent-172, score-0.367]
</p><p>63 Let Sn = ((xt , rt )n ) be t=1 a sample trajectory collected on an MDP with a ﬁxed policy with stationary distribution ρ, in a D-dimensional k-sparse feature space, with D > d ≥ 10. [sent-176, score-0.41]
</p><p>64 Assume that the Bellman error is linear in the features with parameter w. [sent-179, score-0.204]
</p><p>65 (Φ) With compressed OLS regression we have wols = (XΦ)† δ, where X is the matrix containing xt ’s and δ is the vector of TD-errors. [sent-180, score-0.408]
</p><p>66 The sketch of the proof is as follows: Lemma 1 suggests that if the Bellman error is linear in the original features, the bias due to the projection can be bounded within a controlled constant error with logarithmic size projections. [sent-184, score-0.381]
</p><p>67 Theorem 2 can be further simpliﬁed by using concentration bounds on random projections as deﬁned in Eqn 5. [sent-187, score-0.213]
</p><p>68 (8)  The ﬁrst term is a part of the bias due to the projection (excess approximation error). [sent-193, score-0.167]
</p><p>69 We clearly observe the trade-off with respect to the compressed dimension d. [sent-195, score-0.217]
</p><p>70 Thus, the bound is tight enough to prove reduction in the error as new BEBFs are added to the feature set. [sent-197, score-0.224]
</p><p>71 It adds up all the features with weight 1 to approximate the value function. [sent-204, score-0.156]
</p><p>72 Let V π be the value function of a policy π imposing stationary measure ρ, and let eV be the Bellman error under policy π for an estimate V . [sent-209, score-0.49]
</p><p>73 Therefore, if there is a large gap between these terms, we cannot expect to see shrinkage in the error (we can only show that the error can be shrunk to a bounded uncontrolled constant). [sent-212, score-0.237]
</p><p>74 These cases include when the features are rescaled orthonormal basis functions and also with speciﬁc classes of wavelet functions. [sent-215, score-0.162]
</p><p>75 The dependence on the norm of w is conjectured to be tight by the compressed sensing literature [24], making this bound asymptotically the best one can hope for. [sent-216, score-0.268]
</p><p>76 The upper bound on the error of compressed regression is also smaller when the norm of w is small. [sent-219, score-0.418]
</p><p>77 Lemma 4 shows that with enough sampled transitions, using 1+γ ˜ random projections of size d = O ( γ0 −γ )2 k log D guarantees contraction in the error by a factor of γ0 . [sent-224, score-0.341]
</p><p>78 Using union bound over m iterations of the algorithm, we prove that projections of size 1+γ 1+γ ˜ ˜ d = O ( γ0 −γ )2 k log(mD) and a sample of transitions of size n = O ( γ0 −γ )2 d2 log(md) m sufﬁces to shrink the error by a factor of γ0 after m iterations. [sent-225, score-0.394]
</p><p>79 Our goal is to evaluate the value function associated with the manually tuned policy provided with the simulator. [sent-228, score-0.221]
</p><p>80 We let the helicopter free fall for 5 time-steps before the policy takes control. [sent-229, score-0.287]
</p><p>81 We then collect 100 transitions while the helicopter hovers. [sent-230, score-0.193]
</p><p>82 The original state space of the helicopter domain consists of 12 continuous features. [sent-232, score-0.192]
</p><p>83 6 of these features corresponding to the velocities and position, capture most of the data needed for policy evaluation. [sent-233, score-0.299]
</p><p>84 Since the true value function is not known in our case, we evaluate the performance of the algorithm by measuring the normalized return prediction error (NRPE) on a large test set. [sent-238, score-0.166]
</p><p>85 Right: Comparison of the prediction error of different methods for varying sample sizes. [sent-268, score-0.163]
</p><p>86 These conjugate gradient solvers exploit the sparsity of the feature space to converge faster to the solution of linear equations [26]. [sent-274, score-0.231]
</p><p>87 The projection schedule for SCBEBF is set to d = 500e−i/300 for all sample sizes. [sent-277, score-0.197]
</p><p>88 We conjecture that L2-LSTD is beneﬁting from the sparsity of the features space, not only in running time (due to the use of conjugate gradient solvers), but also in sample complexity. [sent-286, score-0.218]
</p><p>89 This makes L2LSTD an attractive choice when the features are observed in the sparse basis. [sent-287, score-0.165]
</p><p>90 However, if the features are sparse in some unknown basis (observation is not sparse), then the time complexity of any linear solver in the observation basis can be prohibitive. [sent-288, score-0.282]
</p><p>91 SCBEBF, however, scales much better in such cases as the main computation is done in the compressed space. [sent-289, score-0.217]
</p><p>92 5  Discussion  We provided a simple, fast and robust feature extraction algorithm for policy evaluation in sparse and high dimensional state spaces. [sent-298, score-0.517]
</p><p>93 Using recent results on the properties of random projections, we proved that in sparse spaces, random projections of sizes logarithmic in the original dimension are sufﬁcient to preserve linearity. [sent-299, score-0.382]
</p><p>94 Therefore, BEBFs can be generated on compressed spaces induced by small random projections. [sent-300, score-0.364]
</p><p>95 Our ﬁnite sample analysis provides guarantees on the reduction in prediction error after the addition of such BEBFs. [sent-301, score-0.193]
</p><p>96 Our assumption of the linearity of the Bellman error in the original feature space might be too strong for some problems. [sent-302, score-0.283]
</p><p>97 If features are observed in the sparse basis, then conjugate gradient solvers can be used for regularized value function approximation. [sent-309, score-0.345]
</p><p>98 Finding the optimal choice of the projection size schedule and the number of iterations is an interesting subject of future research. [sent-311, score-0.161]
</p><p>99 One would expect a slow reduction in the projection size to be favourable. [sent-314, score-0.169]
</p><p>100 Regularization and feature selection in least-squares temporal difference learning. [sent-326, score-0.184]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bellman', 0.326), ('lstd', 0.293), ('bebfs', 0.285), ('scbebf', 0.285), ('cbebf', 0.237), ('compressed', 0.217), ('bebf', 0.213), ('projections', 0.213), ('policy', 0.182), ('clstd', 0.142), ('nrpe', 0.142), ('parr', 0.126), ('spaces', 0.117), ('features', 0.117), ('feature', 0.107), ('projection', 0.106), ('helicopter', 0.105), ('generation', 0.098), ('ev', 0.095), ('prj', 0.095), ('error', 0.087), ('rl', 0.086), ('eqn', 0.084), ('reinforcement', 0.081), ('xt', 0.081), ('temporal', 0.077), ('ghavamzadeh', 0.074), ('fard', 0.071), ('grinberg', 0.071), ('maillard', 0.069), ('regression', 0.063), ('linearity', 0.061), ('simpli', 0.061), ('state', 0.059), ('transitions', 0.058), ('rt', 0.057), ('schedule', 0.055), ('extraction', 0.055), ('norm', 0.051), ('wt', 0.05), ('preserve', 0.049), ('sparse', 0.048), ('ols', 0.048), ('menache', 0.047), ('wols', 0.047), ('reward', 0.045), ('basis', 0.045), ('mdp', 0.045), ('regularized', 0.045), ('pineau', 0.043), ('biconjugate', 0.042), ('farahmand', 0.042), ('mcgill', 0.042), ('lemma', 0.042), ('contraction', 0.041), ('prediction', 0.04), ('fix', 0.039), ('value', 0.039), ('compressing', 0.039), ('approximators', 0.039), ('logarithmic', 0.039), ('memory', 0.039), ('discretized', 0.038), ('iteration', 0.037), ('angular', 0.036), ('keller', 0.036), ('approximator', 0.036), ('ight', 0.036), ('wakin', 0.036), ('sample', 0.036), ('compressive', 0.036), ('gradient', 0.034), ('dimensional', 0.033), ('expect', 0.033), ('evaluation', 0.033), ('backup', 0.033), ('davenport', 0.033), ('rep', 0.033), ('sizes', 0.033), ('variance', 0.033), ('cand', 0.033), ('sn', 0.032), ('bias', 0.032), ('projected', 0.032), ('rmax', 0.032), ('solvers', 0.031), ('conjugate', 0.031), ('signal', 0.03), ('simplify', 0.03), ('reduction', 0.03), ('induced', 0.03), ('audio', 0.03), ('bounded', 0.03), ('collect', 0.03), ('grids', 0.029), ('approximation', 0.029), ('trajectory', 0.028), ('discovery', 0.028), ('space', 0.028), ('observation', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="55-tfidf-1" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>Author: Mahdi Milani Fard, Yuri Grinberg, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a ﬁnite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging. 1</p><p>2 0.18264101 <a title="55-tfidf-2" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>3 0.18184887 <a title="55-tfidf-3" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>Author: David J. Weiss, Ben Taskar</p><p>Abstract: Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Signiﬁcant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to ﬁnetune feature extraction to each input at run-time. We address the key challenge of learning to control ﬁne-grained feature extraction adaptively, exploiting nonhomogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efﬁcient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate signiﬁcant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task. 1</p><p>4 0.1804584 <a title="55-tfidf-4" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><p>5 0.15639868 <a title="55-tfidf-5" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>Author: Prashanth L.A., Mohammad Ghavamzadeh</p><p>Abstract: In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance-related risk measures are among the most common risk-sensitive criteria in ﬁnance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we ﬁrst deﬁne a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a trafﬁc signal control application. 1</p><p>6 0.1527805 <a title="55-tfidf-6" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>7 0.15049 <a title="55-tfidf-7" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>8 0.14707209 <a title="55-tfidf-8" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>9 0.12547523 <a title="55-tfidf-9" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>10 0.12319611 <a title="55-tfidf-10" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>11 0.11468542 <a title="55-tfidf-11" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>12 0.10837424 <a title="55-tfidf-12" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>13 0.10694963 <a title="55-tfidf-13" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>14 0.10494861 <a title="55-tfidf-14" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>15 0.10373117 <a title="55-tfidf-15" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>16 0.10334521 <a title="55-tfidf-16" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>17 0.09758848 <a title="55-tfidf-17" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>18 0.094948396 <a title="55-tfidf-18" href="./nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</a></p>
<p>19 0.092215084 <a title="55-tfidf-19" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>20 0.091723382 <a title="55-tfidf-20" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.173), (2, -0.053), (3, 0.102), (4, -0.023), (5, 0.025), (6, -0.089), (7, 0.121), (8, -0.038), (9, 0.012), (10, 0.002), (11, -0.014), (12, -0.015), (13, -0.017), (14, -0.0), (15, 0.014), (16, 0.044), (17, 0.009), (18, -0.006), (19, 0.019), (20, -0.008), (21, -0.008), (22, -0.002), (23, 0.019), (24, -0.02), (25, -0.017), (26, -0.003), (27, -0.037), (28, 0.004), (29, -0.093), (30, 0.015), (31, -0.01), (32, 0.019), (33, 0.105), (34, 0.04), (35, -0.044), (36, 0.017), (37, -0.038), (38, 0.069), (39, -0.082), (40, 0.06), (41, 0.056), (42, -0.003), (43, -0.035), (44, 0.018), (45, -0.013), (46, 0.05), (47, 0.016), (48, -0.059), (49, -0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91775072 <a title="55-lsi-1" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>Author: Mahdi Milani Fard, Yuri Grinberg, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a ﬁnite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging. 1</p><p>2 0.74035496 <a title="55-lsi-2" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>Author: Bruno Scherrer</p><p>Abstract: Given a Markov Decision Process (MDP) with n states and m actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal “-discounted optimal policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal Ï advantage. We show that Howard’s PI terminates 1 2Ì 1 1 22 1 1 nm 1 after at most n(m ≠ 1) 1≠“ log 1≠“ = O 1≠“ log 1≠“ iterations, improving by a factor O(log 1 a result by [3], while Simplex-PI terminates n) 1 22 1 2 1 22 2 1 1 2 after at most n (m ≠ 1) 1 + 1≠“ log 1≠“ = O n m log 1≠“ 1≠“ iterations, improving by a factor O(log n) a result by [11]. Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor “: given a measure of the maximal transient time ·t and the maximal time ·r to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most n2 (m≠ # $ 1) (Á·r log(n·r )Ë + Á·r log(n·t )Ë) (m ≠ 1)Án·t log(n·t )Ë + Án·t log(n2 ·t )Ë = !</p><p>3 0.74026668 <a title="55-lsi-3" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>Author: Aswin Raghavan, Roni Khardon, Alan Fern, Prasad Tadepalli</p><p>Abstract: This paper addresses the scalability of symbolic planning under uncertainty with factored states and actions. Our ﬁrst contribution is a symbolic implementation of Modiﬁed Policy Iteration (MPI) for factored actions that views policy evaluation as policy-constrained value iteration (VI). Unfortunately, a na¨ve approach ı to enforce policy constraints can lead to large memory requirements, sometimes making symbolic MPI worse than VI. We address this through our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent algorithm lying between VI and MPI, that applies policy constraints if it does not increase the size of the value function representation, and otherwise performs VI backups. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signiﬁcantly improved scalability over state-of-the-art symbolic planners. 1</p><p>4 0.73930258 <a title="55-lsi-4" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>Author: Matteo Pirotta, Marcello Restelli, Luca Bascetta</p><p>Abstract: In the last decade, policy gradient methods have signiﬁcantly grown in popularity in the reinforcement–learning ﬁeld. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identiﬁcation of effective gradient directions and the proposal of efﬁcient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly inﬂuenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simpliﬁed version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem. 1</p><p>5 0.73680604 <a title="55-lsi-5" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>Author: Beomjoon Kim, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: We propose a Learning from Demonstration (LfD) algorithm which leverages expert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error interactions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert’s suggestions are used to deﬁne linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-ﬁnding task. 1</p><p>6 0.72925454 <a title="55-lsi-6" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>7 0.70407778 <a title="55-lsi-7" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>8 0.70038015 <a title="55-lsi-8" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>9 0.67894012 <a title="55-lsi-9" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>10 0.66505003 <a title="55-lsi-10" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>11 0.63711643 <a title="55-lsi-11" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>12 0.63504046 <a title="55-lsi-12" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>13 0.63269216 <a title="55-lsi-13" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>14 0.62711698 <a title="55-lsi-14" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>15 0.61938852 <a title="55-lsi-15" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>16 0.60669708 <a title="55-lsi-16" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>17 0.57542354 <a title="55-lsi-17" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>18 0.56069821 <a title="55-lsi-18" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>19 0.53584617 <a title="55-lsi-19" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>20 0.53571087 <a title="55-lsi-20" href="./nips-2013-Blind_Calibration_in_Compressed_Sensing_using_Message_Passing_Algorithms.html">59 nips-2013-Blind Calibration in Compressed Sensing using Message Passing Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (16, 0.039), (33, 0.123), (34, 0.125), (41, 0.035), (49, 0.026), (56, 0.138), (57, 0.231), (70, 0.034), (85, 0.042), (89, 0.044), (93, 0.057), (95, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79754132 <a title="55-lda-1" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>Author: Mahdi Milani Fard, Yuri Grinberg, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a ﬁnite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging. 1</p><p>2 0.79372978 <a title="55-lda-2" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>3 0.73267466 <a title="55-lda-3" href="./nips-2013-Accelerating_Stochastic_Gradient_Descent_using_Predictive_Variance_Reduction.html">20 nips-2013-Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is signiﬁcantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning. 1</p><p>4 0.71385187 <a title="55-lda-4" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>Author: David J. Weiss, Ben Taskar</p><p>Abstract: Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Signiﬁcant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to ﬁnetune feature extraction to each input at run-time. We address the key challenge of learning to control ﬁne-grained feature extraction adaptively, exploiting nonhomogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efﬁcient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate signiﬁcant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task. 1</p><p>5 0.71383631 <a title="55-lda-5" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>6 0.71284097 <a title="55-lda-6" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>7 0.71104658 <a title="55-lda-7" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>8 0.70906591 <a title="55-lda-8" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>9 0.70666885 <a title="55-lda-9" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>10 0.70470905 <a title="55-lda-10" href="./nips-2013-Robust_Data-Driven_Dynamic_Programming.html">280 nips-2013-Robust Data-Driven Dynamic Programming</a></p>
<p>11 0.70386761 <a title="55-lda-11" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>12 0.70373815 <a title="55-lda-12" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>13 0.70326358 <a title="55-lda-13" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>14 0.70310301 <a title="55-lda-14" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>15 0.70251507 <a title="55-lda-15" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>16 0.70250058 <a title="55-lda-16" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>17 0.7024911 <a title="55-lda-17" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>18 0.7018953 <a title="55-lda-18" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>19 0.70153165 <a title="55-lda-19" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>20 0.70098799 <a title="55-lda-20" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
