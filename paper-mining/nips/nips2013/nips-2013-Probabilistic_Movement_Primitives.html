<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 nips-2013-Probabilistic Movement Primitives</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-255" href="#">nips2013-255</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>255 nips-2013-Probabilistic Movement Primitives</h1>
<br/><p>Source: <a title="nips-2013-255-pdf" href="http://papers.nips.cc/paper/5177-probabilistic-movement-primitives.pdf">pdf</a></p><p>Author: Alexandros Paraschos, Christian Daniel, January Peters, Gerhard Neumann</p><p>Abstract: Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios. 1</p><p>Reference: <a title="nips-2013-255-reference" href="../nips2013_reference/nips-2013-Probabilistic_Movement_Primitives_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. [sent-4, score-0.75]
</p><p>2 Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. [sent-5, score-0.556]
</p><p>3 A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. [sent-6, score-0.43]
</p><p>4 To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. [sent-7, score-0.167]
</p><p>5 In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. [sent-10, score-1.466]
</p><p>6 We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios. [sent-11, score-0.278]
</p><p>7 The aim of MPs is to allow for composing complex robot skills out of elemental movements with a modular control architecture. [sent-19, score-0.603]
</p><p>8 Hence, we require a MP architecture that supports parallel activation and smooth blending of MPs for composing complex movements of sequentially [9] and simultaneously [10] activated primitives. [sent-20, score-0.358]
</p><p>9 Moreover, adaptation to a new task or a new situation requires modulation of the MP to an altered desired target position, target velocity or via-points [3]. [sent-21, score-0.263]
</p><p>10 Additionally, the execution speed of the movement needs to be adjustable to change the speed of, for example, a ball-hitting movement. [sent-22, score-0.507]
</p><p>11 As we want to learn the movement from data, another crucial requirement is that the parameters of the MPs should be straightforward to learn from demonstrations as well as through trial and error for reinforcement learning approaches. [sent-23, score-0.593]
</p><p>12 However, this approach heavily depends on the quality of the used planner and the 1  movement can not be temporally scaled. [sent-27, score-0.396]
</p><p>13 [12, 16] use a combination of primitives, yet, their control policy of the MP is based on heuristics and it is unclear how the combination of MPs affects the resulting movements. [sent-30, score-0.177]
</p><p>14 In this paper, we introduce the concept of probabilistic movement primitives (ProMPs) as a general probabilistic framework for representing and learning MPs. [sent-31, score-0.702]
</p><p>15 For example, modulation of a movement to a novel target can be realized by conditioning on the desired target’s positions or velocities. [sent-34, score-0.734]
</p><p>16 Similarly, consistent parallel activation of two elementary behaviors can be accomplished by a product of two independent trajectory probability distributions. [sent-35, score-0.307]
</p><p>17 Moreover, a trajectory distribution can also encode the variance of the movement, and, hence, a ProMP can often directly encode optimal behavior in stochastic systems [17]. [sent-36, score-0.291]
</p><p>18 Finally, a probabilistic framework allows us to model the covariance between trajectories of different degrees of freedom, that can be used to couple the joints of the robot. [sent-37, score-0.197]
</p><p>19 Such properties of trajectory distributions have so far not been properly exploited for representing and learning MPs. [sent-38, score-0.306]
</p><p>20 The main reason for the absence of such an approach has been the difﬁculty of extracting a policy for controlling the robot from a trajectory distribution. [sent-39, score-0.562]
</p><p>21 We show how this step can be accomplished and derive a control policy that exactly reproduces a given trajectory distribution. [sent-40, score-0.42]
</p><p>22 2  Probabilistic Movement Primitives (ProMPs)  A movement primitive representation should exhibit several desirable properties, such as co- Table 1: Desirable properties and their implemenactivation, adaptability and optimality in order tation in the ProMP to be a powerful MP representation. [sent-44, score-0.493]
</p><p>23 As Rhythmic Movements Periodic Basis crucial part of our objective, we will introduce conditioning and a product of ProMPs as new operations that can be applied on the ProMPs due to the probabilistic formulation. [sent-50, score-0.173]
</p><p>24 Finally, we show how to derive a controller which follows a given trajectory distribution. [sent-51, score-0.399]
</p><p>25 1  Probabilistic Trajectory Representation  We model a single movement execution as a trajectory τ = {qt }t=0. [sent-53, score-0.693]
</p><p>26 Our movement primitive representation models the time-varying variance of the trajectories to be able to capture multiple demonstrations with high-variability. [sent-59, score-0.723]
</p><p>27 Representing the variance information is crucial as it reﬂects the importance of 2  single time points for the movement execution and it is often a requirement for representing optimal behavior in stochastic systems [17]. [sent-60, score-0.519]
</p><p>28 The trajectory distribution p(τ ; θ) can now be computed ´ by marginalizing out the weight vector w, i. [sent-68, score-0.248]
</p><p>29 We introduce a phase variable z to decouple the movement from the time signal as for previous non-probabilistic approaches [18]. [sent-74, score-0.458]
</p><p>30 Without loss of generality, we deﬁne the phase as z0 = 0 at the beginning of the movement and as zT = 1 at the end. [sent-77, score-0.458]
</p><p>31 The choice of the basis functions depends on the type of movement, which can be either rhythmic or stroke-based. [sent-80, score-0.203]
</p><p>32 For stroke-based movements, we use Gaussian basis functions bG , while for rhythmic movements we use Von-Mises basis functions bVM i i to model periodicity in the phase variable z, i. [sent-81, score-0.439]
</p><p>33 However, for many tasks we have to coordinate the movement of the joints. [sent-87, score-0.396]
</p><p>34 A common way to implement such coordination is via the phase variable zt that couples the mean of the trajectory distribution [18]. [sent-88, score-0.375]
</p><p>35 As a ProMP represents multiple ways to execute an elemental movement, we also need multiple demonstrations to learn p(w; θ). [sent-119, score-0.192]
</p><p>36 The parameters θ = {µw , Σw } can be learned from multiple demonstrations by maximum likelihood estimation, for example, by using the expectation maximization algorithm for HBMs with Gaussian distributions [19]. [sent-120, score-0.21]
</p><p>37 , conditioning for modulating the trajectory and a product of distributions for co-activating MPs. [sent-124, score-0.416]
</p><p>38 In our probabilistic formulation, such operations can be described by conditioning the MP to reach a certain state y ∗ at time t. [sent-128, score-0.21]
</p><p>39 For example, by specifying a desired joint position q1 for the ﬁrst joint the t trajectory distribution will automatically infer the most probable joint positions for the other joints. [sent-134, score-0.479]
</p><p>40 For Gaussian trajectory distributions the conditional distribution p (w|x∗ ) for w is Gaussian with t mean and variance [new] µw  = µw + Σw Ψt Σ∗ + ΨT Σw Ψt y t  [new] Σw  = Σw − Σw Ψt Σ∗ + ΨT Σw Ψt t y  −1 −1  y ∗ − ΨT µw , t t Ψ T Σw . [sent-135, score-0.318]
</p><p>41 We can see that, despite the modulation of the ProMP by conditioning, the ProMP stays within the original distribution, and, hence, the modulation is also learned from the original demonstrations. [sent-137, score-0.222]
</p><p>42 , the part of the trajectory space where all MPs have high probability mass. [sent-147, score-0.248]
</p><p>43 However, we also want to be able to modulate the activations of the primitives, for example, to continuously blend the movement execution from one primitive to the next. [sent-148, score-0.667]
</p><p>44 Hence, we decompose [i] the trajectory into single time steps and use time-varying activation functions αt , i. [sent-149, score-0.307]
</p><p>45 The blue shaded area represents the learned trajectory distribution. [sent-165, score-0.341]
</p><p>46 The trajectory distributions are indicated by the blue and red shaded areas. [sent-169, score-0.342]
</p><p>47 Both primitives have to reach via-points at different points in time, indicated by the ‘x’-markers. [sent-170, score-0.204]
</p><p>48 We co-activate both primitives with the same activation factor. [sent-171, score-0.226]
</p><p>49 The trajectory distribution generated by the resulting feedback controller now goes through all four via-points. [sent-172, score-0.483]
</p><p>50 We smoothly blend from the red primitive to the blue primitive. [sent-174, score-0.184]
</p><p>51 The resulting movement (green) ﬁrst follows the red primitive and, subsequently, switches to following the blue primitive. [sent-176, score-0.523]
</p><p>52 3  Using Trajectory Distributions for Robot Control  In order to fully exploit the properties of trajectory distributions, a policy for controlling the robot is needed that reproduces these distributions. [sent-178, score-0.623]
</p><p>53 To this effect, we analytically derivate a stochastic feedback controller that can accurately reproduce the mean vectors µt and the variances Σt for all t of a given trajectory distribution. [sent-179, score-0.483]
</p><p>54 We assume a stochastic linear feedback controller with time varying feedback gains is generating the control actions, i. [sent-182, score-0.42]
</p><p>55 (9), we rewrite the next state of the system as y t+dt = (I + (At + B t K t ) dt) y t + B t dt(kt + with F t = (I + (At + B t K t ) dt) ,  u)  + cdt = F t y t + f t + B t dt f t = B t kt dt + cdt. [sent-188, score-0.744]
</p><p>56 2 As we multiply the noise by Bdt, we need to divide the covariance Σu of the control noise u by dt to obtain this desired behavior. [sent-193, score-0.517]
</p><p>57 12 are Gaussian distributions, where the left-hand side can also be computed by our desired trajectory distribution p(τ ; θ). [sent-197, score-0.315]
</p><p>58 Σt+dt − Σt  =  By rearranging terms, the covariance constraint becomes T  Σs dt + (A + BK) Σt dt + Σt (A + BK) dt + O(dt2 ),  (14)  where O(dt2 ) denotes all second order terms in dt. [sent-204, score-1.091]
</p><p>59 After dividing by dt and taking the limit of dt → 0, the second order terms disappear and we obtain the time derivative of the covariance Σt+dt − Σt T ˙ Σt = lim = (A + BK)Σt + Σt (A + BK) + Σs . [sent-205, score-0.714]
</p><p>60 dt→0 dt  (15)  ˙ ˙ ˙T ˙ The matrix Σt can also be obtained from the trajectory distribution Σt = Ψt Σw Ψt + ΨT Σw Ψt , t which we substitute into Eq. [sent-206, score-0.587]
</p><p>61 Similarly, we obtain the feed-forward control signal k by matching the mean of the trajectory distribution µt+dt with the mean computed with the forward model. [sent-211, score-0.323]
</p><p>62 After rearranging terms, dividing by dt and taking the limit of dt → 0, we arrive at the continuous time constraint for the vector k, ˙ µt = (A + BK)µt + Bk + c. [sent-212, score-0.716]
</p><p>63 (18)  ˙ ˙ We can again use the trajectory distribution p(τ ; θ) to obtain µt = Ψt µw and µt = Ψt µw and solve Eq. [sent-213, score-0.248]
</p><p>64 In order to match a trajectory distribution, we also need to match the control noise matrix Σu which has been applied to generate the distribution. [sent-215, score-0.375]
</p><p>65 We ﬁrst compute the system noise covariance Σs = BΣu B T by examining the cross-correlation between time steps of the trajectory distribution. [sent-216, score-0.316]
</p><p>66 The joint distribution for y t and y t+dt is obtained by our system dynamics by p y t , y t+dt = N (y t |µt , Σt ) N y t+dt |F y t + f , Σu which yields p y t , y t+dt = N  yt y t+dt  µt F µt + f  ,  Σt F T F Σt F T + Σs dt  Σt F Σt  . [sent-219, score-0.401]
</p><p>67 (20) and (21), Σs dt = Σt+dt − F Σt F T = Σt+dt − F Σt Σ−1 Σt F T = Σt+dt − C T Σ−1 C t t t t †  †T  (22)  The variance Σu of the control noise is then given by Σu = B Σs B . [sent-221, score-0.457]
</p><p>68 (22) the variance of our stochastic feedback controller does not depend on the controller gains and can be pre-computed before estimating the controller gains. [sent-223, score-0.606]
</p><p>69 0s  6 4 2  y−axis [m]  0 6 4 2 0 6 4 2 0 −2  0  2  4  6  −2  0  2  4  6  −2  0  2  4  x−axis [m]  6  −2  0  2  4  6  −2  0  2  4  6  Figure 2: A 7-link planar robot has to reach a target position at T = 1. [sent-228, score-0.412]
</p><p>70 The plot shows the mean posture of the robot at different time steps in black and samples generated by the ProMP in gray. [sent-232, score-0.278]
</p><p>71 The resulting movement reached both viapoints with high accuracy. [sent-235, score-0.396]
</p><p>72 We demonstrate ten straight shots for varying distances and ten shots for varying angles. [sent-238, score-0.367]
</p><p>73 The pictures show samples from the ProMP model for straight shots (b) and angled shots (c). [sent-239, score-0.311]
</p><p>74 Multiplying the individual models leads to a model that only reproduces shots where both models had probability mass, in the center at medium distance (e). [sent-241, score-0.2]
</p><p>75 3  Experiments  We evaluated our approach on two different real robot tasks, one stroke based movement and one rhythmic movements. [sent-243, score-0.859]
</p><p>76 For all real robot experiments we use a seven degrees of freedom KUKA lightweight robot arm. [sent-245, score-0.584]
</p><p>77 In this task, a seven link planar robot has to reach a target position in end-effector space. [sent-248, score-0.44]
</p><p>78 We generated the demonstrations for learning the MPs with an optimal control law [22]. [sent-250, score-0.232]
</p><p>79 In the ﬁrst set of demonstrations, the robot has to reach the via-point at t1 = 0. [sent-251, score-0.315]
</p><p>80 We learned the coupling of all seven joints with one ProMP. [sent-254, score-0.172]
</p><p>81 Moreover, the ProMP could also reproduce the coupling of the joints from the optimal control law which can be seen by the small variance of the end-effector in comparison to the rather large variance of the single joints at the via-points. [sent-256, score-0.356]
</p><p>82 We combined the ProMPs learned from both demonstrations, which resulted in the movement illustrated in Figure 2(bottom). [sent-260, score-0.45]
</p><p>83 By modulating the speed of the phase signal zt , the speed of the movement can be adapted. [sent-284, score-0.633]
</p><p>84 The plot shows the desired distribution in blue and the generated distribution from the feedback controller in green. [sent-285, score-0.332]
</p><p>85 (c) Blending between two rhythmic movements (blue and red shaded areas) for playing maracas. [sent-287, score-0.322]
</p><p>86 In the hockey task, the robot has to shoot a hockey puck in different directions and distances. [sent-290, score-0.405]
</p><p>87 We record two different sets of demonstrations, one that contains straight shots with varying distances while the second set contains shots with a varying shooting angle. [sent-292, score-0.311]
</p><p>88 Sampling from the two models generated by the different data sets yields shots that exhibit the demonstrated variance in either angle or distance, as shown in Figure 3(b) and 3(c). [sent-294, score-0.213]
</p><p>89 Demonstrating fast movements can be difﬁcult on the robot arm, due to the inertia of the arm. [sent-302, score-0.406]
</p><p>90 Instead, we demonstrate a slower movement of ten periods to learn the motion. [sent-303, score-0.424]
</p><p>91 We use this slow demonstration and change the phase after learning the model to achieve a shaking movement of appropriate speed to generate the desired sound of the instrument. [sent-304, score-0.687]
</p><p>92 We show an example movement of the robot in Figure 4(a). [sent-306, score-0.674]
</p><p>93 The desired trajectory distribution of the rhythmic movement and the resulting distribution generated from the feedback controller are shown in Figure 4(b). [sent-307, score-1.103]
</p><p>94 We also demonstrated a second type of rhythmic shaking movement which we use to continuously blend between both movements to produce different sounds. [sent-309, score-0.816]
</p><p>95 4  Conclusion  Probabilistic movement primitives are a promising approach for learning, modulating, and re-using movements in a modular control architecture. [sent-311, score-0.811]
</p><p>96 To effectively take advantage of such a control architecture, ProMPs support simultaneous activation, match the quality of the encoded behavior from the demonstrations, are able to adapt to different desired target positions, and efﬁciently learn by imitation. [sent-312, score-0.203]
</p><p>97 We parametrize the desired trajectory distribution of the primitive by a Hierarchical Bayesian Model with Gaussian distributions. [sent-313, score-0.412]
</p><p>98 The trajectory distribution can be easily obtained from demonstrations. [sent-314, score-0.248]
</p><p>99 Our probabilistic formulation allows for new operations for movement primitives, including conditioning and combination of primitives. [sent-315, score-0.602]
</p><p>100 Future work will focus on using the ProMPs in a modular control architecture and improving upon imitation learning by reinforcement learning. [sent-316, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('movement', 0.396), ('dt', 0.339), ('robot', 0.278), ('promps', 0.261), ('trajectory', 0.248), ('mps', 0.23), ('promp', 0.226), ('primitives', 0.167), ('demonstrations', 0.157), ('rhythmic', 0.157), ('controller', 0.151), ('mp', 0.15), ('blending', 0.139), ('shots', 0.139), ('movements', 0.128), ('modulation', 0.098), ('primitive', 0.097), ('conditioning', 0.093), ('bk', 0.092), ('demonstration', 0.085), ('feedback', 0.084), ('joints', 0.077), ('control', 0.075), ('desired', 0.067), ('zt', 0.065), ('phase', 0.062), ('reproduces', 0.061), ('activation', 0.059), ('robotics', 0.057), ('blend', 0.057), ('motor', 0.054), ('qt', 0.054), ('probabilistic', 0.054), ('maracas', 0.052), ('rad', 0.051), ('execution', 0.049), ('neumann', 0.048), ('modulating', 0.048), ('basis', 0.046), ('hockey', 0.046), ('calinon', 0.046), ('shaking', 0.046), ('shoots', 0.046), ('positions', 0.045), ('modular', 0.045), ('variance', 0.043), ('skills', 0.042), ('coupling', 0.041), ('reinforcement', 0.04), ('robots', 0.039), ('rearranging', 0.038), ('reach', 0.037), ('pi', 0.037), ('shaded', 0.037), ('policy', 0.036), ('modulate', 0.036), ('covariance', 0.036), ('ct', 0.035), ('target', 0.035), ('bvm', 0.035), ('darmstadt', 0.035), ('elemental', 0.035), ('kormushev', 0.035), ('paraschos', 0.035), ('puck', 0.035), ('rozo', 0.035), ('kt', 0.034), ('combination', 0.033), ('straight', 0.033), ('planar', 0.033), ('system', 0.032), ('velocities', 0.032), ('architecture', 0.032), ('continuously', 0.032), ('speed', 0.031), ('representing', 0.031), ('nakanishi', 0.031), ('angle', 0.031), ('blue', 0.03), ('periodic', 0.03), ('trajectories', 0.03), ('joint', 0.03), ('angles', 0.029), ('peters', 0.029), ('position', 0.029), ('toussaint', 0.028), ('bg', 0.028), ('stroke', 0.028), ('altered', 0.028), ('konidaris', 0.028), ('illustrated', 0.028), ('seven', 0.028), ('ten', 0.028), ('distributions', 0.027), ('gains', 0.026), ('learned', 0.026), ('operations', 0.026), ('match', 0.026), ('reproduced', 0.025), ('kober', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="255-tfidf-1" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>Author: Alexandros Paraschos, Christian Daniel, January Peters, Gerhard Neumann</p><p>Abstract: Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios. 1</p><p>2 0.26970196 <a title="255-tfidf-2" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<p>Author: Ashesh Jain, Brian Wojcik, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion deﬁning a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only inﬂuenced by the object being manipulated but also by the surrounding environment.1 1</p><p>3 0.12030232 <a title="255-tfidf-3" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>Author: Beomjoon Kim, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: We propose a Learning from Demonstration (LfD) algorithm which leverages expert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error interactions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert’s suggestions are used to deﬁne linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-ﬁnding task. 1</p><p>4 0.10851076 <a title="255-tfidf-4" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>5 0.090739958 <a title="255-tfidf-5" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>Author: Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin</p><p>Abstract: This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a lowvariance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. 1</p><p>6 0.087817125 <a title="255-tfidf-6" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>7 0.074841179 <a title="255-tfidf-7" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>8 0.074794136 <a title="255-tfidf-8" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>9 0.069444887 <a title="255-tfidf-9" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>10 0.06699647 <a title="255-tfidf-10" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>11 0.064309195 <a title="255-tfidf-11" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>12 0.063883014 <a title="255-tfidf-12" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>13 0.059614178 <a title="255-tfidf-13" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>14 0.057219602 <a title="255-tfidf-14" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>15 0.057144154 <a title="255-tfidf-15" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>16 0.052320763 <a title="255-tfidf-16" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>17 0.051746447 <a title="255-tfidf-17" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>18 0.045058142 <a title="255-tfidf-18" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>19 0.044214856 <a title="255-tfidf-19" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>20 0.043453015 <a title="255-tfidf-20" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, -0.053), (2, -0.055), (3, 0.0), (4, -0.037), (5, 0.001), (6, 0.0), (7, 0.053), (8, 0.008), (9, -0.042), (10, -0.038), (11, -0.058), (12, -0.016), (13, 0.022), (14, -0.1), (15, 0.064), (16, -0.073), (17, -0.005), (18, 0.012), (19, 0.029), (20, -0.021), (21, -0.085), (22, 0.019), (23, 0.026), (24, -0.058), (25, -0.04), (26, 0.0), (27, 0.057), (28, -0.002), (29, -0.055), (30, -0.003), (31, 0.067), (32, 0.114), (33, -0.07), (34, -0.044), (35, -0.037), (36, -0.097), (37, -0.062), (38, -0.029), (39, -0.144), (40, -0.104), (41, 0.099), (42, -0.05), (43, -0.146), (44, 0.151), (45, -0.035), (46, -0.242), (47, -0.147), (48, 0.106), (49, -0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95237523 <a title="255-lsi-1" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>Author: Alexandros Paraschos, Christian Daniel, January Peters, Gerhard Neumann</p><p>Abstract: Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios. 1</p><p>2 0.87472296 <a title="255-lsi-2" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<p>Author: Ashesh Jain, Brian Wojcik, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion deﬁning a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only inﬂuenced by the object being manipulated but also by the surrounding environment.1 1</p><p>3 0.55552655 <a title="255-lsi-3" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>Author: Beomjoon Kim, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: We propose a Learning from Demonstration (LfD) algorithm which leverages expert data, even if they are very few or inaccurate. We achieve this by using both expert data, as well as reinforcement signals gathered through trial-and-error interactions with the environment. The key idea of our approach, Approximate Policy Iteration with Demonstration (APID), is that expert’s suggestions are used to deﬁne linear constraints which guide the optimization performed by Approximate Policy Iteration. We prove an upper bound on the Bellman error of the estimate computed by APID at each iteration. Moreover, we show empirically that APID outperforms pure Approximate Policy Iteration, a state-of-the-art LfD algorithm, and supervised learning in a variety of scenarios, including when very few and/or suboptimal demonstrations are available. Our experiments include simulations as well as a real robot path-ﬁnding task. 1</p><p>4 0.39976159 <a title="255-lsi-4" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>5 0.39859608 <a title="255-lsi-5" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>Author: Josh S. Merel, Roy Fox, Tony Jebara, Liam Paninski</p><p>Abstract: In a closed-loop brain-computer interface (BCI), adaptive decoders are used to learn parameters suited to decoding the user’s neural response. Feedback to the user provides information which permits the neural tuning to also adapt. We present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic Gaussian (LQG) control problem. In simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement. We then propose a novel, modiﬁed decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics. Our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of BCI control in practical settings.</p><p>6 0.38883731 <a title="255-lsi-6" href="./nips-2013-Policy_Shaping%3A_Integrating_Human_Feedback_with_Reinforcement_Learning.html">250 nips-2013-Policy Shaping: Integrating Human Feedback with Reinforcement Learning</a></p>
<p>7 0.37061834 <a title="255-lsi-7" href="./nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</a></p>
<p>8 0.33949575 <a title="255-lsi-8" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>9 0.33108693 <a title="255-lsi-9" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>10 0.31618655 <a title="255-lsi-10" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>11 0.31536421 <a title="255-lsi-11" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>12 0.31208152 <a title="255-lsi-12" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>13 0.30467668 <a title="255-lsi-13" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>14 0.28746611 <a title="255-lsi-14" href="./nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</a></p>
<p>15 0.27718702 <a title="255-lsi-15" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>16 0.27661979 <a title="255-lsi-16" href="./nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</a></p>
<p>17 0.26743245 <a title="255-lsi-17" href="./nips-2013-Graphical_Models_for_Inference_with_Missing_Data.html">134 nips-2013-Graphical Models for Inference with Missing Data</a></p>
<p>18 0.264036 <a title="255-lsi-18" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>19 0.25924537 <a title="255-lsi-19" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>20 0.25715786 <a title="255-lsi-20" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.018), (16, 0.034), (33, 0.085), (34, 0.126), (41, 0.053), (42, 0.287), (49, 0.067), (56, 0.078), (70, 0.081), (73, 0.01), (85, 0.026), (89, 0.013), (93, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76091462 <a title="255-lda-1" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>Author: Alexandros Paraschos, Christian Daniel, January Peters, Gerhard Neumann</p><p>Abstract: Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios. 1</p><p>2 0.57543266 <a title="255-lda-2" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>Author: Cristina Savin, Peter Dayan, Mate Lengyel</p><p>Abstract: It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate. 1</p><p>3 0.5616585 <a title="255-lda-3" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>4 0.55664742 <a title="255-lda-4" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>Author: Subhaneil Lahiri, Surya Ganguli</p><p>Abstract: An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming ﬁnite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on ﬁrst passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function. 1</p><p>5 0.55662602 <a title="255-lda-5" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>Author: Srini Turaga, Lars Buesing, Adam M. Packer, Henry Dalgleish, Noah Pettit, Michael Hausser, Jakob Macke</p><p>Abstract: Simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed. It is now possible to measure the activity of hundreds of neurons using 2-photon calcium imaging. However, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex. Here we contribute a statistical method for “stitching” together sequentially imaged sets of neurons into one model by phrasing the problem as ﬁtting a latent dynamical system with missing observations. This method allows us to substantially expand the population-sizes for which population dynamics can be characterized—beyond the number of simultaneously imaged neurons. In particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs. 1</p><p>6 0.55570441 <a title="255-lda-6" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>7 0.55138129 <a title="255-lda-7" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>8 0.54795361 <a title="255-lda-8" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>9 0.54388517 <a title="255-lda-9" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>10 0.53585422 <a title="255-lda-10" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>11 0.53447998 <a title="255-lda-11" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>12 0.53419828 <a title="255-lda-12" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>13 0.53400582 <a title="255-lda-13" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>14 0.53062105 <a title="255-lda-14" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>15 0.53061962 <a title="255-lda-15" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>16 0.5300808 <a title="255-lda-16" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>17 0.52932101 <a title="255-lda-17" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>18 0.52860683 <a title="255-lda-18" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>19 0.52703619 <a title="255-lda-19" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>20 0.52656037 <a title="255-lda-20" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
