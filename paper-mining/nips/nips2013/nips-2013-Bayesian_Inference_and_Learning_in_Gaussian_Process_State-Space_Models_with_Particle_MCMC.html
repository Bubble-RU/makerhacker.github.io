<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-48" href="#">nips2013-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</h1>
<br/><p>Source: <a title="nips-2013-48-pdf" href="http://papers.nips.cc/paper/5085-bayesian-inference-and-learning-in-gaussian-process-state-space-models-with-particle-mcmc.pdf">pdf</a></p><p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><p>Reference: <a title="nips-2013-48-reference" href="../nips2013_reference/nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 se  Abstract State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. [sent-16, score-0.117]
</p><p>2 state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. [sent-19, score-0.241]
</p><p>3 We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. [sent-20, score-0.405]
</p><p>4 To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. [sent-21, score-0.621]
</p><p>5 Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. [sent-22, score-0.542]
</p><p>6 Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. [sent-23, score-0.184]
</p><p>7 Their main feature is the presence of a latent variable, the state xt ∈ X Rnx , which condenses all aspects of the system that can have an impact on its future. [sent-25, score-0.44]
</p><p>8 A discrete-time SSM with nonlinear dynamics can be represented as xt+1 = f (xt , ut ) + vt , yt = g(xt , ut ) + et ,  (1a) (1b)  where ut denotes a known external input, yt denotes the measurements, vt and et denote i. [sent-26, score-0.627]
</p><p>9 noises acting on the dynamics and the measurements, respectively. [sent-29, score-0.154]
</p><p>10 The function f encodes the dynamics and g describes the relationship between the observation and the unobserved states. [sent-30, score-0.154]
</p><p>11 To this effect, we employ a Bayesian nonparametric model for the dynamics (1a). [sent-33, score-0.227]
</p><p>12 We adopt a fully Bayesian approach whereby we ﬁnd a posterior distribution over all the latent entities of interest, namely the state transition function f , the hidden state trajectory x0:T {xi }T i=0 1  and any hyper-parameter θ of the model. [sent-40, score-0.513]
</p><p>13 This is in contrast with existing approaches for using GPs to model SSMs, which tend to model the GP using a ﬁnite set of target points, in effect making the model parametric [2]. [sent-41, score-0.177]
</p><p>14 Inferring the distribution over the state trajectory p(x0:T | y0:T , u0:T ) is an important problem in itself known as smoothing. [sent-42, score-0.198]
</p><p>15 We use a tailored particle Markov Chain Monte Carlo (PMCMC) algorithm [3] to efﬁciently sample from the smoothing distribution whilst marginalizing over the state transition function. [sent-43, score-0.993]
</p><p>16 This contrasts with conventional approaches to smoothing which require a ﬁxed model of the transition dynamics. [sent-44, score-0.438]
</p><p>17 Once we have obtained an approximation of the smoothing distribution, with the dynamics of the model marginalized out, learning the function f is straightforward since its posterior is available in closed form given the state trajectory. [sent-45, score-0.58]
</p><p>18 We report very good mixing enabled by the use of recently developed PMCMC samplers [4] and the exact marginalization of the transition dynamics. [sent-47, score-0.163]
</p><p>19 [5, 6] presented reﬁned approximation methods for ﬁltering and smoothing for already learned GP dynamics and measurement functions. [sent-50, score-0.455]
</p><p>20 [2] applied the EM algorithm to obtain a maximum likelihood estimate of parametric models which had the form of GPs where both inputs and outputs were parameters to be optimized. [sent-53, score-0.132]
</p><p>21 They apply the learning in cases where the dimension of the observation vector is much higher than that of the latent state in what becomes a form of dynamic dimensionality reduction. [sent-57, score-0.161]
</p><p>22 This procedure would have the risk of overﬁtting in the common situation where the state-space is high-dimensional and there is signiﬁcant uncertainty in the smoothing distribution. [sent-58, score-0.254]
</p><p>23 Also, note that the measurement model (2c) and the prior on x0 can take any form since we do not rely on their properties for efﬁcient inference. [sent-61, score-0.146]
</p><p>24 An interesting property of the GP-SSM is that any a priori insight into the dynamics of the system can be readily encoded in the mean function. [sent-63, score-0.243]
</p><p>25 by using a simple parametric model or a model based on ﬁrst principles. [sent-66, score-0.147]
</p><p>26 If no speciﬁc prior model is available, the linear mean function m(xt ) = xt is a good generic choice. [sent-70, score-0.32]
</p><p>27 Interestingly, the prior information encoded in this model will normally be more vague than the prior information encoded in parametric models. [sent-71, score-0.317]
</p><p>28 A challenge with this approach is that marginalization of f will introduce dependencies across time for the state variables that lead to the loss of the Markovian structure of the state-process. [sent-79, score-0.136]
</p><p>29 1  Marginalizing out the State Transition Function  Targeting the joint posterior distribution of the hyper-parameters, the latent states and the latent function f is problematic due to the strong dependencies between x0:T and f . [sent-86, score-0.167]
</p><p>30 2  Sequential Monte Carlo  With the prior (4) in place, we now turn to posterior inference and we start by considering the joint smoothing distribution p(x0:T | θ, y0:T ). [sent-104, score-0.374]
</p><p>31 To propagate this sample to time t, we introduce the auxiliary variables {ai }N , referred to as t i=1 ancestor indices. [sent-113, score-0.351]
</p><p>32 The variable ai is the index of the ancestor particle at time t − 1, of particle xi . [sent-114, score-1.252]
</p><p>33 t t j Hence, xi is generated by ﬁrst sampling ai with P(ai = j) = wt−1 . [sent-115, score-0.209]
</p><p>34 Then, xi is generated as, t t t t ai  t xi ∼ p(xt | θ, x0:t−1 , y0:t ), t  (5) ai t  for i = 1, . [sent-116, score-0.27]
</p><p>35 The particle trajectories are then augmented according to xi = {x0:t−1 , xi }. [sent-120, score-0.658]
</p><p>36 In the above formulation the resampling step is implicit and corresponds to sampling the ancestor indices (cf. [sent-122, score-0.397]
</p><p>37 Finally, the i particles are weighted according to the measurement model, wt ∝ p(yt | θ, xi ) for i = 1, . [sent-124, score-0.321]
</p><p>38 That is, the successive resampling steps cause the particle diversity to be very low for time points t far from the ﬁnal time instant T . [sent-130, score-0.442]
</p><p>39 To address these issues, we propose to use a particle Markov chain Monte Carlo (PMCMC, [3, 13]) sampler. [sent-131, score-0.464]
</p><p>40 PMCMC relies on SMC to generate samples of the highly correlated state trajectory within an MCMC sampler. [sent-132, score-0.198]
</p><p>41 We employ a speciﬁc PMCMC sampler referred to as particle Gibbs with ancestor sampling (PGAS, [4]), given in Algorithm 1. [sent-133, score-0.853]
</p><p>42 PGAS uses Gibbs-like steps for the state trajectory x0:T and the hyper-parameters θ, respectively. [sent-134, score-0.198]
</p><p>43 Instead, we draw samples from specially tailored Markov kernels, leaving these conditionals invariant. [sent-137, score-0.124]
</p><p>44 Algorithm 1 Particle Gibbs with ancestor sampling (PGAS) 1. [sent-139, score-0.367]
</p><p>45 1  Sampling the State Trajectories  To sample the state trajectory, PGAS makes use of an SMC-like procedure referred to as a conditional particle ﬁlter with ancestor sampling (CPF-AS). [sent-150, score-0.904]
</p><p>46 The difference between a standard particle ﬁlter (PF) and the CPF-AS is that, for the latter, one particle at each time step is speciﬁed a priori. [sent-152, score-0.824]
</p><p>47 The N th particle is set deterministically: xN = xt . [sent-161, score-0.633]
</p><p>48 t To be able to construct the N th particle trajectory, xN has to be associated with an ancestor particle t at time t − 1. [sent-162, score-1.117]
</p><p>49 This is done by sampling a value for the corresponding ancestor index aN . [sent-163, score-0.367]
</p><p>50 Following t [4], the ancestor sampling probabilities are computed as i i wt−1|T ∝ wt−1  p({xi p({xi 0:t−1 , xt:T }, y0:T ) 0:t−1 , xt:T }) i i ∝ wt−1 = wt−1 p(xt:T | xi 0:t−1 ). [sent-164, score-0.457]
</p><p>51 The above expression can be computed by using the prior over state i trajectories given by (4). [sent-168, score-0.231]
</p><p>52 The ancestor sampling weights {wt−1|T }N are then normalized to sum i=1 j to 1 and the ancestor index aN is sampled with P(aN = j) = wt−1|t . [sent-169, score-0.66]
</p><p>53 Set x0:T to one of the resulting particle trajectories according to P(x0:T = xi ) = wT . [sent-174, score-0.568]
</p><p>54 0:T N For any N ≥ 2, this procedure deﬁnes an ergodic Markov kernel Mθ (x0:T | x0:T ) on XT +1 , leaving the exact smoothing distribution p(x0:T | θ, y0:T ) invariant [4]. [sent-175, score-0.254]
</p><p>55 2  Sampling the Hyper-parameters  Next, we consider sampling the hyper-parameters given a state trajectory and sequence of observations, i. [sent-182, score-0.272]
</p><p>56 In the following, we consider the common situation where there are distinct hyper-parameters for the likelihood p(y0:T | x0:T , θ y ) and for the prior over trajectories p(x0:T | θ x ). [sent-185, score-0.135]
</p><p>57 However, we note that once the state trajectory is ﬁxed, we are left with a problem analogous to Gaussian process regression where x0:T −1 are the inputs, x1:T are the outputs and Q is the likelihood covariance matrix. [sent-190, score-0.246]
</p><p>58 Given that the latent dynamics can be marginalized out analytically, sampling the hyper-parameters with slice sampling is straightforward [15]. [sent-191, score-0.413]
</p><p>59 1  FIC Prior over the State Trajectory  An important alternative to GP-SSM is given by exchanging the vanilla GP prior over f for a sparse counterpart. [sent-202, score-0.168]
</p><p>60 Most sparse GP methods can be formulated in terms of a set of so called inducing variables [17]. [sent-205, score-0.137]
</p><p>61 These variables live in the space of the latent function and have a set I of corresponding inducing inputs. [sent-206, score-0.161]
</p><p>62 The assumption is that, conditionally on the inducing variables, the latent function values are mutually independent. [sent-207, score-0.199]
</p><p>63 Although the inducing variables are marginalized analytically – this is key for the model to remain nonparametric – the inducing inputs have to be chosen in such a way that they, informally speaking, cover the same region of the input space covered by the data. [sent-208, score-0.356]
</p><p>64 In the following, we will use the fully independent conditional (FIC) sparse GP prior as deﬁned in [17] due to its very good empirical performance [16]. [sent-210, score-0.141]
</p><p>65 Moreover, its potential drawback of interference between hyperparameter learning and active set selection is not an issue in our case since hyper-parameters will be ﬁxed for a given run of the particle ﬁlter. [sent-220, score-0.412]
</p><p>66 There are two costly operations of the CPF-AS algorithm: (i) sampling from the prior (5), requiring the computation of (3b) and (3c) and (ii) evaluating the ancestor sampling probabilities according to (6). [sent-224, score-0.51]
</p><p>67 Both of these operations can be carried out efﬁciently by keeping track of a Cholesky faci i torization of the matrix K({xi 0:t−1 , xt:T −1 }) = Lt Lt , for each particle i = 1, . [sent-225, score-0.412]
</p><p>68 Here, i K({x0:t−1 , xt:T −1 }) is a matrix deﬁned analogously to K0:T −1 , but where the covariance function i is evaluated for the concatenated state trajectory {xi 0:t−1 , xt:T −1 }. [sent-229, score-0.276]
</p><p>69 From Lt , it is possible to identify sub-matrices corresponding to the Cholesky factors for the covariance matrix Σt (xi 0:t−1 ) as well as for the matrices needed to efﬁciently evaluate the ancestor sampling probabilities (6). [sent-230, score-0.415]
</p><p>70 As we move from time t+1 t to t + 1 in the algorithm, xt will be replaced by xi in the concatenated trajectory. [sent-232, score-0.341]
</p><p>71 The goal of learning the state transition dynamics is equivalent to that of obtaining a predictive distribution over f ∗ = f (x∗ ), evaluated at an arbitrary test point x∗ , p(f ∗ | x∗ , y1:T ) =  p(f ∗ | x∗ , x0:T , θ) p(x0:T , θ | y1:T ) dx0:T dθ. [sent-241, score-0.442]
</p><p>72 1  Experiments Learning a Nonlinear System Benchmark  Consider a system with dynamics given by xt+1 = axt + bxt /(1 + x2 ) + cut + vt , vt ∼ N (0, q) t and observations given by yt = dx2 + et , et ∼ N (0, r), with parameters (a, b, c, d, q, r) = t (0. [sent-253, score-0.354]
</p><p>73 One of the difﬁculties of this system is that the smoothing density p(x0:T | y0:T ) is multimodal since no information about the sign of xt is available in the observations. [sent-257, score-0.533]
</p><p>74 To illustrate the capability of the GP-SSM to make use of a parametric model as baseline, we use a mean function with the same parametric form as the true system, but parameters (a, b, c) = (0. [sent-259, score-0.204]
</p><p>75 This function, denoted model B, is manifestly different to the actual state transition (green vs. [sent-262, score-0.249]
</p><p>76 It is apparent that the distribution covers two alternative state trajectories at particular times (e. [sent-265, score-0.162]
</p><p>77 In Figure 2 (right) we plot samples from the smoothing distribution, where each circle corresponds to (xt , ut , E[ft ]). [sent-269, score-0.331]
</p><p>78 Although the parametric model used in the mean function of the GP (green) is clearly not representative of the true dynamics (black), the samples from the smoothing distribution accurately portray the underlying system. [sent-270, score-0.525]
</p><p>79 The smoothness prior embodied by the GP allows for accurate sampling from the smoothing distribution even when the parametric model of the dynamics fails to capture important features. [sent-271, score-0.705]
</p><p>80 To measure the predictive capability of the learned transition dynamics, we generate a new dataset consisting of 10 000 time steps and present the RMSE between the predicted value of f (xt , ut ) and the actual one. [sent-272, score-0.269]
</p><p>81 We compare the results from GP-SSM with the predictions obtained from two parametric models (one with the true model structure and one linear model) and two known models (the ground truth model and model B). [sent-273, score-0.273]
</p><p>82 We also report results for the sparse GP-SSM using an FIC prior with 40 inducing points. [sent-274, score-0.206]
</p><p>83 We also report the RMSE from the joint smoothing sample to the ground truth trajectory. [sent-276, score-0.35]
</p><p>84 RMSE Ground truth model (known parameters) GP-SSM (proposed, model B mean function) Sparse GP-SSM (proposed, model B mean function) Model B (ﬁxed parameters) Ground truth model, learned parameters Linear model, learned parameters  7  prediction of f ∗ |x∗ , u∗ , data t t – 1. [sent-278, score-0.184]
</p><p>85 Right: State transition function (black: actual transition function, green: mean function (model B) and red: smoothing samples). [sent-303, score-0.5]
</p><p>86 16  10 2  5  ˙ θ  x 0 ˙  10  1  0  θ 0  −5  14  x 12  8  2  −1  −2  −2  −10 300  350  300  350  300  350  300  350  Figure 3: One step ahead predictive distribution for each of the states of the cart and pole system. [sent-304, score-0.283]
</p><p>87 2  Learning a Cart and Pole System  We apply our approach to learn a model of a cart and pole system used in reinforcement learning. [sent-308, score-0.235]
</p><p>88 The system’s dynamics can be described by four states and a set of nonlinear ordinary differential equations [20]. [sent-311, score-0.235]
</p><p>89 Although the training set only explores a small region of the 4-dimensional state space, we can learn a model of the dynamics which can produce one step ahead predictions such the ones in Figure 3. [sent-313, score-0.31]
</p><p>90 This is made possible by marginalizing out the state transition function, which results in a nontrivial inference problem that we solve using a tailored PGAS sampler. [sent-320, score-0.378]
</p><p>91 A particular characteristic of our approach is that the latent states can be sampled from the smoothing distribution even when the state transition function is unknown. [sent-321, score-0.575]
</p><p>92 Assumptions about smoothness and parsimony of this function embodied by the GP prior sufﬁce to obtain high-quality smoothing distributions. [sent-322, score-0.36]
</p><p>93 Once samples from the smoothing distribution are available, they can be used to describe a posterior over the state transition function. [sent-323, score-0.473]
</p><p>94 This contrasts with the conventional approach to inference in dynamical systems where smoothing is performed conditioned on a model of the state transition dynamics. [sent-324, score-0.672]
</p><p>95 Sch¨ n, “Ancestor sampling for particle Gibbs,” in Advances in Neural o Information Processing Systems 25, P. [sent-356, score-0.486]
</p><p>96 Rasmussen, “Robust ﬁltering and smoothing with Gaussian processes,” IEEE Transactions on Automatic Control, vol. [sent-369, score-0.254]
</p><p>97 Roweis, “Learning nonlinear dynamical systems using an EM algorithm,” in Advances in Neural Information Processing Systems 11, M. [sent-385, score-0.131]
</p><p>98 Johansen, “A tutorial on particle ﬁltering and smoothing: Fifteen years later,” in The Oxford Handbook of Nonlinear Filtering, D. [sent-408, score-0.412]
</p><p>99 Shephard, “Filtering via simulation: Auxiliary particle ﬁlters,” Journal of the American Statistical Association, vol. [sent-420, score-0.412]
</p><p>100 Sch¨ n, “On the use of backward simulation in the particle Gibbs sampler,” in o Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Kyoto, Japan, Mar. [sent-434, score-0.412]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('particle', 0.412), ('gp', 0.32), ('ancestor', 0.293), ('smoothing', 0.254), ('xt', 0.221), ('fic', 0.204), ('dynamics', 0.154), ('pgas', 0.154), ('pmcmc', 0.129), ('transition', 0.123), ('wt', 0.108), ('lindsten', 0.103), ('trajectory', 0.102), ('kt', 0.099), ('deisenroth', 0.098), ('state', 0.096), ('inducing', 0.096), ('smc', 0.091), ('xi', 0.09), ('parametric', 0.087), ('dynamical', 0.087), ('cart', 0.084), ('nx', 0.081), ('carlo', 0.08), ('monte', 0.08), ('ssms', 0.077), ('ut', 0.077), ('particles', 0.076), ('sampling', 0.074), ('prior', 0.069), ('predictive', 0.069), ('ssm', 0.068), ('trajectories', 0.066), ('latent', 0.065), ('pole', 0.063), ('rmse', 0.061), ('vanilla', 0.058), ('system', 0.058), ('yt', 0.056), ('turner', 0.056), ('sch', 0.056), ('tailored', 0.056), ('rasmussen', 0.053), ('marginalizing', 0.052), ('chain', 0.052), ('inference', 0.051), ('reuse', 0.05), ('gaussian', 0.049), ('ground', 0.049), ('covariance', 0.048), ('gaussians', 0.048), ('measurement', 0.047), ('truth', 0.047), ('cholesky', 0.046), ('marginalized', 0.046), ('inputs', 0.045), ('ltering', 0.045), ('ai', 0.045), ('sampler', 0.045), ('xj', 0.044), ('nonlinear', 0.044), ('gps', 0.044), ('nonparametric', 0.043), ('vt', 0.043), ('herding', 0.042), ('targeting', 0.042), ('sparse', 0.041), ('marginalization', 0.04), ('omitting', 0.039), ('sweden', 0.039), ('lt', 0.039), ('markov', 0.039), ('conditionally', 0.038), ('mcmc', 0.038), ('embodied', 0.037), ('expressivity', 0.037), ('filtering', 0.037), ('states', 0.037), ('doucet', 0.036), ('lter', 0.035), ('exible', 0.035), ('conditionals', 0.034), ('specially', 0.034), ('burges', 0.033), ('processes', 0.033), ('ft', 0.033), ('ghahramani', 0.033), ('contrasts', 0.031), ('fully', 0.031), ('express', 0.031), ('encoded', 0.031), ('gibbs', 0.031), ('ahead', 0.03), ('resampling', 0.03), ('model', 0.03), ('concatenated', 0.03), ('conditioning', 0.029), ('auxiliary', 0.029), ('referred', 0.029), ('black', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="48-tfidf-1" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><p>2 0.2233869 <a title="48-tfidf-2" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><p>3 0.21802182 <a title="48-tfidf-3" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>4 0.20210426 <a title="48-tfidf-4" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>Author: Ali Borji, Laurent Itti</p><p>Abstract: Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to ﬁnd the function’s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploits all the x values tried and all the f (x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further conﬁrm that Gaussian Processes provide a general and uniﬁed theoretical account to explain passive and active function learning and search in humans. 1</p><p>5 0.16417363 <a title="48-tfidf-5" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>Author: Sergey Levine, Vladlen Koltun</p><p>Abstract: In order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task. While random exploration can work well in simple domains, complex and highdimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible. We present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search. A variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself. We demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks. 1</p><p>6 0.15007807 <a title="48-tfidf-6" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>7 0.14514332 <a title="48-tfidf-7" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>8 0.14298202 <a title="48-tfidf-8" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>9 0.13917147 <a title="48-tfidf-9" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>10 0.13110189 <a title="48-tfidf-10" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>11 0.12130996 <a title="48-tfidf-11" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>12 0.11505996 <a title="48-tfidf-12" href="./nips-2013-Dimension-Free_Exponentiated_Gradient.html">89 nips-2013-Dimension-Free Exponentiated Gradient</a></p>
<p>13 0.11236294 <a title="48-tfidf-13" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>14 0.1102808 <a title="48-tfidf-14" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>15 0.10814615 <a title="48-tfidf-15" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>16 0.10492283 <a title="48-tfidf-16" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>17 0.10311462 <a title="48-tfidf-17" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>18 0.10047433 <a title="48-tfidf-18" href="./nips-2013-Stochastic_Convex_Optimization_with_Multiple__Objectives.html">311 nips-2013-Stochastic Convex Optimization with Multiple  Objectives</a></p>
<p>19 0.1004426 <a title="48-tfidf-19" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>20 0.098144092 <a title="48-tfidf-20" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.265), (1, 0.001), (2, 0.016), (3, -0.045), (4, -0.091), (5, 0.111), (6, 0.13), (7, 0.177), (8, 0.144), (9, -0.158), (10, -0.16), (11, -0.204), (12, -0.146), (13, 0.065), (14, -0.103), (15, 0.105), (16, -0.129), (17, 0.072), (18, 0.035), (19, -0.048), (20, 0.057), (21, -0.015), (22, -0.06), (23, 0.069), (24, 0.08), (25, 0.049), (26, -0.025), (27, -0.052), (28, -0.081), (29, 0.065), (30, -0.083), (31, 0.046), (32, 0.07), (33, 0.027), (34, 0.015), (35, 0.011), (36, 0.07), (37, -0.051), (38, 0.005), (39, -0.07), (40, 0.03), (41, 0.016), (42, -0.036), (43, -0.047), (44, -0.034), (45, -0.036), (46, 0.035), (47, -0.01), (48, -0.011), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93903512 <a title="48-lsi-1" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><p>2 0.82367396 <a title="48-lsi-2" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><p>3 0.70467299 <a title="48-lsi-3" href="./nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</a></p>
<p>Author: Botond Cseke, Manfred Opper, Guido Sanguinetti</p><p>Abstract: We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid ﬁxed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and signiﬁcant computational savings compared to discrete-time approaches in a neural application. 1</p><p>4 0.69559383 <a title="48-lsi-4" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><p>5 0.67358416 <a title="48-lsi-5" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>Author: Kevin Swersky, Jasper Snoek, Ryan P. Adams</p><p>Abstract: Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efﬁciency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to ﬁnd optimal hyperparameter settings more efﬁciently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method signiﬁcantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost. 1</p><p>6 0.65770668 <a title="48-lsi-6" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>7 0.63603812 <a title="48-lsi-7" href="./nips-2013-Bayesian_optimization_explains_human_active_search.html">54 nips-2013-Bayesian optimization explains human active search</a></p>
<p>8 0.6093663 <a title="48-lsi-8" href="./nips-2013-Small-Variance_Asymptotics_for_Hidden_Markov_Models.html">298 nips-2013-Small-Variance Asymptotics for Hidden Markov Models</a></p>
<p>9 0.59979624 <a title="48-lsi-9" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>10 0.57668805 <a title="48-lsi-10" href="./nips-2013-Causal_Inference_on_Time_Series_using_Restricted_Structural_Equation_Models.html">62 nips-2013-Causal Inference on Time Series using Restricted Structural Equation Models</a></p>
<p>11 0.57180291 <a title="48-lsi-11" href="./nips-2013-A_multi-agent_control_framework_for_co-adaptation_in_brain-computer_interfaces.html">17 nips-2013-A multi-agent control framework for co-adaptation in brain-computer interfaces</a></p>
<p>12 0.55851072 <a title="48-lsi-12" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>13 0.54802328 <a title="48-lsi-13" href="./nips-2013-Locally_Adaptive_Bayesian_Multivariate_Time_Series.html">178 nips-2013-Locally Adaptive Bayesian Multivariate Time Series</a></p>
<p>14 0.51132494 <a title="48-lsi-14" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>15 0.50679433 <a title="48-lsi-15" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>16 0.47114968 <a title="48-lsi-16" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>17 0.45539057 <a title="48-lsi-17" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>18 0.44693255 <a title="48-lsi-18" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>19 0.44214106 <a title="48-lsi-19" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>20 0.43228453 <a title="48-lsi-20" href="./nips-2013-Solving_inverse_problem_of_Markov_chain_with_partial_observations.html">299 nips-2013-Solving inverse problem of Markov chain with partial observations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.052), (33, 0.099), (34, 0.559), (41, 0.029), (49, 0.017), (56, 0.086), (70, 0.022), (85, 0.014), (89, 0.022), (93, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97385293 <a title="48-lda-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.96667945 <a title="48-lda-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.96318316 <a title="48-lda-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>4 0.95522183 <a title="48-lda-4" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>5 0.95023698 <a title="48-lda-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>same-paper 6 0.94766933 <a title="48-lda-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>7 0.9336971 <a title="48-lda-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.93337113 <a title="48-lda-8" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>9 0.91588324 <a title="48-lda-9" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>10 0.91197371 <a title="48-lda-10" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>11 0.80481392 <a title="48-lda-11" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>12 0.80136847 <a title="48-lda-12" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>13 0.79910266 <a title="48-lda-13" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>14 0.76918489 <a title="48-lda-14" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>15 0.76036012 <a title="48-lda-15" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>16 0.75931829 <a title="48-lda-16" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>17 0.75599319 <a title="48-lda-17" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>18 0.75582725 <a title="48-lda-18" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>19 0.75324082 <a title="48-lda-19" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>20 0.7472856 <a title="48-lda-20" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
