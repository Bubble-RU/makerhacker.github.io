<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-351" href="#">nips2013-351</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</h1>
<br/><p>Source: <a title="nips-2013-351-pdf" href="http://papers.nips.cc/paper/5195-what-are-the-invariant-occlusive-components-of-image-patches-a-probabilistic-generative-approach.pdf">pdf</a></p><p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>Reference: <a title="nips-2013-351-reference" href="../nips2013_reference/nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. [sent-8, score-0.599]
</p><p>2 Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. [sent-10, score-0.547]
</p><p>3 All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. [sent-11, score-0.776]
</p><p>4 Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. [sent-12, score-0.388]
</p><p>5 By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. [sent-13, score-0.293]
</p><p>6 In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. [sent-14, score-0.639]
</p><p>7 We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. [sent-16, score-0.439]
</p><p>8 On natural image patches, the model learns component masks and features for typical image components. [sent-17, score-0.553]
</p><p>9 We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. [sent-19, score-0.443]
</p><p>10 Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. [sent-20, score-0.792]
</p><p>11 Two properties that have, since long, been identiﬁed as crucial for models of images are object occlusions [1–5] and the invariance of object identity to translations [6–13]. [sent-32, score-0.347]
</p><p>12 In this work, we for the ﬁrst time study the encoding of natural image patches using a model with both non-linear feature combinations and translation invariances. [sent-40, score-0.575]
</p><p>13 2  A Generative Model with Non-linear and Invariant Components  The model used to study image patch encoding assumes an exclusive component combination, i. [sent-41, score-0.523]
</p><p>14 The applied model is a novel version of the invariant occlusive components model studied for mid-level vision earlier [22]. [sent-47, score-0.499]
</p><p>15 We consider image patches y with D2 observed scalar variables, y = (y1 , . [sent-49, score-0.347]
</p><p>16 An image patch is assumed to contain a subset from a set of H components. [sent-53, score-0.262]
</p><p>17 Each component h can be located at a different position denoted by an index variable xh ∈ {1, . [sent-54, score-0.574]
</p><p>18 Each component h is modeled to appear in an image patch with probability πh ∈ (0, 1). [sent-61, score-0.39]
</p><p>19 Following [22], we do not model component presence and absence explicitly but, for mathematical convenience, assign the special ‘position’ −1 to all the components which are not chosen to generate the patch. [sent-62, score-0.331]
</p><p>20 Assuming a uniform distribution for the positions, the prior distribution for components and their positions is thus given by: p(xh |πh ), p(xh |πh ) =  p(x|π) = h  1 − πh , xh = −1 , πh , otherwise D2  (1)  where the hidden variable x = (x1 , . [sent-63, score-0.683]
</p><p>21 , xH ) contains the information on presence/absence and position of all the image components. [sent-66, score-0.259]
</p><p>22 In contrast to linear models, the studied approach requires two sets of parameters for the encoding of image components: component masks and component features. [sent-67, score-0.631]
</p><p>23 Component masks describe where an image component is located, and component features describe what a component encodes (compare [2, 3, 14, 15]). [sent-68, score-0.653]
</p><p>24 High values of mask parameters αh encode the pixels most associated with a component h but the encoding has to be understood relative to a global component position. [sent-69, score-0.741]
</p><p>25 1 shows an example 2  of the mask and feature parameters for two typical low-level visual features. [sent-72, score-0.485]
</p><p>26 Given a particular position, the mask and feature parameters of the component are transformed to the target position by multiplying a corresponding translation matrix like Txh αh and Txh wh . [sent-73, score-0.75]
</p><p>27 When generating an image patch, two or more components may occupy the same pixel, but according to occlusion the pixel value is exclusively determined by only one of them. [sent-74, score-0.464]
</p><p>28 This exclusiveness is formulated by deﬁning a mask variable m = (m1 , . [sent-75, score-0.368]
</p><p>29 For a pixel at a position d, md determines which component is responsible for the pixel value. [sent-79, score-0.479]
</p><p>30 , αH ) contains the mask parameters for all the components, and α0 deﬁnes the mask parameter for background. [sent-83, score-0.607]
</p><p>31 The mask variable md chooses the component h with a high likelihood if the translated mask parameter of the corresponding component is high at the position d. [sent-84, score-1.067]
</p><p>32 Note that md follows a mixture model given the presence/absence and positions of all the components x. [sent-85, score-0.422]
</p><p>33 This can be thought of as an approximation to the distribution of mask variables marginalizing the depth orderings and pixel transparency in the exact occlusive model (see Supplement A for a comparison). [sent-86, score-0.666]
</p><p>34 Compared to an occlusive model with exact EM (see Supplement A), our approach will use the exclusiveness approximation and a truncated posterior approximation in order to make learning tractable. [sent-89, score-0.376]
</p><p>35 The model described in (1) to (3) has been optimized for the encoding of image patches. [sent-90, score-0.289]
</p><p>36 As a scalar value is much less distinctive than the sophisticated image features used in [22], the pre-selection of components has been changed to the complete component instead of only salient features. [sent-94, score-0.552]
</p><p>37 3  Efﬁcient Likelihood Optimization  Given a set of image patches Y = (y (1) , . [sent-95, score-0.31]
</p><p>38 The equations of the mask parameter αh , and feature parameter wh are the same as in [22]. [sent-103, score-0.426]
</p><p>39 (b) The parameters of the eight components used to generate the data set. [sent-114, score-0.249]
</p><p>40 The 1st row shows the mask parameters A; the 2nd row shows the feature parameters W ; the 3rd row gives a good visualization of only the frequent used elements/pixels (setting the feature parameter whd of the elements/pixels with αhd < 0. [sent-118, score-0.483]
</p><p>41 (d) The result of inference given a image patch (shown on the left). [sent-120, score-0.262]
</p><p>42 The 1st and 2nd rows show the mask and features parameters shifted according to the MAP inference xMAP , and the 3rd row shows the inferred posterior p(md |xMAP , y, Θ). [sent-122, score-0.452]
</p><p>43 To select a proper subspace Kn , τ features (pixel intensities) are chosen according to their mask parameters. [sent-125, score-0.345]
</p><p>44 We select H components, denoted as H, for the candidates that may appear in the given image according to the probability p(y, xh |Θ). [sent-127, score-0.499]
</p><p>45 xh corresponds to the vector x with xh = x∗ and the rest components absent ˇ ˇ h (xh = −1, h = h), where x∗ is the best position of the component h w. [sent-128, score-1.092]
</p><p>46 For each component, we select the set of its candidate positions Xh , xh ∈ Xh , which contains the p best positions w. [sent-133, score-0.577]
</p><p>47 Then the truncated subspace Kn is deﬁned as: Kn = {x | (  sj ≤ γ and si = 0, ∀i ∈ H) or / j  sj ≤ 1},  (7)  j  where sh represents the presence/absence state of the component h (sh = 0 if xh = −1 ∪ xh ∈ Xh / and sh = 1 if xh ∈ Xh ). [sent-137, score-1.352]
</p><p>48 4  Numerical Experiments on Artiﬁcial Data  The goal of the experiment on artiﬁcial data is to verify that the model and inference method can recover the correct parameters, and to investigate inference on the data generated according to occlusions with explicit depth variable. [sent-139, score-0.258]
</p><p>49 When generating an image patch, a subset of components is selected according to their prior probabilities πh = 0. [sent-143, score-0.364]
</p><p>50 A component with smaller depth will occlude the components with larger depth, and for each image patch we sample a new depthordering. [sent-145, score-0.615]
</p><p>51 The artiﬁcial data is similar to data generated by the occlusive components analysis model (OCA; [2]), except of the use of scalar features and the assumption of shift-invariance. [sent-153, score-0.484]
</p><p>52 The initial mask parameter A was independently and uniformly drawn from the interval (0, 1). [sent-158, score-0.289]
</p><p>53 In this result, all the eight generative components have been successfully learned. [sent-168, score-0.316]
</p><p>54 Given an image patch on the left, the present components and their positions are correctly inferred. [sent-177, score-0.554]
</p><p>55 Furthermore, as shown on the 3rd row, the posterior probabilities of the mask variable p(md |x, y, Θ) give a clear assignment of the contributing component for each pixel. [sent-178, score-0.5]
</p><p>56 The algorithm recovers all the generative components in 11 out of 20 repetitive runs. [sent-181, score-0.271]
</p><p>57 As training set we used N = 100, 000 patches of size 16 × 16 pixels extracted at random positions from random images of the van Hateren natural image database [24]. [sent-187, score-0.508]
</p><p>58 3a shows some samples of the image patches after preprocessing. [sent-193, score-0.31]
</p><p>59 Our algorithm learned H = 100 components from the natural image data set. [sent-194, score-0.382]
</p><p>60 The annealing temperature was initialized with T = 10, kept constant for 10 iterations, the temperature linearly decreased to 1 in 100 iterations. [sent-196, score-0.25]
</p><p>61 3bc show the learned mask parameters and the learned feature values for all the 100 components. [sent-204, score-0.488]
</p><p>62 Mask parameters deﬁne the frequently used areas within a component, and feature parameters reveal the appearance of a component on image patches. [sent-205, score-0.41]
</p><p>63 As can be observed, image components are very differently represented from linear models. [sent-206, score-0.331]
</p><p>64 3d as an example: mask parameters are localized and all positive; feature parameters have positive and negative values across the whole patch. [sent-208, score-0.415]
</p><p>65 All the above shown component representations are sorted in descending order according to the learned prior probabilities of occurrence π (see Fig. [sent-211, score-0.27]
</p><p>66 6  Estimation of Receptive Fields  For visualization, mask and feature parameters can be combined via point-wise multiplication. [sent-213, score-0.386]
</p><p>67 To more systematically and quantitatively interpret the learned components and to compare them to biological experimental ﬁndings, we estimated the predicted receptive ﬁelds (RFs). [sent-214, score-0.389]
</p><p>68 Reverse correlation can be deﬁned as procedure to ﬁnd the best linear approximation of the components’ presence given an image patch y (n) . [sent-216, score-0.29]
</p><p>69 sh is a binary variable representing the presence/absence state of the component h, where sh = 0 if xh = −1, and sh = 1 5  (a)  (e)  (b)  RF  (c)  (d)  (f)  Figure 3: The invariant occlusive components from natural image patches. [sent-221, score-1.372]
</p><p>70 (b) shows the mask parameter and (c) shows the feature parameter. [sent-223, score-0.357]
</p><p>71 The dashed line marks the RFs that have a more globular structure. [sent-228, score-0.277]
</p><p>72 As our model allows the components to be at different locations, the reverse correlation ¯ is computed by shifting the stimuli according to the inferred location of each components. [sent-234, score-0.238]
</p><p>73 The estimated receptive ﬁelds are not sensitive to the value of the regularization coefﬁcient λ as long as λ is large enough to resolve the numerical instability (see Supplement for a comparison of the receptive ﬁelds estimated with different λ values). [sent-246, score-0.326]
</p><p>74 If we factored in the occurrence probabilities, we found that the model considered about 17% of all components of the patches to be globular, 56% to be Gabor-like and 27% to have another structure (see Supplement for details). [sent-251, score-0.387]
</p><p>75 The prevalence of ‘center-on’ globular ﬁelds may be a consequence of the prevalence of convex object shapes. [sent-252, score-0.377]
</p><p>76 7  Discussion  The encoding of image patches investigated in this study separates feature and position information of visual components. [sent-253, score-0.713]
</p><p>77 Many state-of-the-art systems for visual object classiﬁcation make use of convolutional neural networks [12, 25, 26]. [sent-257, score-0.262]
</p><p>78 As the here studied approach is based on a generative data model, the integration across positions can directly be interpreted as inversion of the generation process. [sent-260, score-0.272]
</p><p>79 Crucially, the inversion can take occlusions of visual features into account while convolutional networks do not model occlusions. [sent-261, score-0.47]
</p><p>80 , it assigns probabilities to positions and features of a joint feature and position space. [sent-264, score-0.377]
</p><p>81 In contrast, convolutional networks use one position for each feature as representation. [sent-266, score-0.292]
</p><p>82 In this sense a convolutional encoding could be regarded as MAP estimate for the feature position while the generative integration could be interpreted as probabilistic pooling. [sent-267, score-0.55]
</p><p>83 Likewise, invariant versions of NMF [21] or ICA (in the form of ISA [9] have been applied to image patches. [sent-272, score-0.264]
</p><p>84 Gabor ﬁlters with different orientations, phase and frequencies, as well as globular ﬁelds and ﬁelds with more complex structures (Fig. [sent-274, score-0.308]
</p><p>85 Gabors have been studied since several decades, globular and more complex ﬁelds have attracted attention in the last couple of years. [sent-276, score-0.308]
</p><p>86 In particular, globular ﬁelds have attracted attention [5, 27, 28] as they have been reported together with Gabors in macaques and other species ([29] and [5] for further references). [sent-277, score-0.277]
</p><p>87 Such ﬁelds have been associated with occlusions before [5, 28, 30]; and our study now for the ﬁrst time reports globular ﬁelds for an occlusive and translation invariant approach. [sent-278, score-0.83]
</p><p>88 The results may be taken as further evidence of the connection between occlusions and globular ﬁelds. [sent-279, score-0.442]
</p><p>89 Linear approaches seem to require a high degree of overcompleteness or speciﬁc priors while globular ﬁelds naturally emerge for occlusion-like non-linearities. [sent-281, score-0.396]
</p><p>90 The invariant non-linear model assigns high degrees of occurrences (high πh ) to Gabor-like and to globular ﬁelds (ﬁrst rows in Fig. [sent-284, score-0.385]
</p><p>91 Such high percentages may be related to the high percentages of globular ﬁelds (∼16-23%) measured in vivo ([29] and [5] for references). [sent-288, score-0.337]
</p><p>92 7  Because of their separate encoding of features and positions, all models with separate position encoding can represent high degrees of over-completeness. [sent-296, score-0.425]
</p><p>93 In terms of components per observed variable, invariant models are therefore now computationally feasible in a regime the visual cortex is estimated to operate in [33]. [sent-303, score-0.41]
</p><p>94 The hidden units associated with component feature are fully translation invariant. [sent-304, score-0.359]
</p><p>95 Also globular ﬁelds or ﬁelds that seem sensitive to structures such as corners would warrant such units the label ‘complex cell’. [sent-306, score-0.328]
</p><p>96 In contrast to passive pooling mechanisms across units representing linear ﬁlters (such as simple cells), it would involve neural units with explicit position encoding. [sent-312, score-0.248]
</p><p>97 As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]). [sent-314, score-0.255]
</p><p>98 Base on the presented results, we believe, however, that advances in analytical and computational training technology will enable an increasingly sophisticated modeling of image patches in the future. [sent-322, score-0.31]
</p><p>99 Are V1 receptive ﬁelds shaped by low-level visual occlusions? [sent-353, score-0.234]
</p><p>100 Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. [sent-482, score-0.304]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xh', 0.343), ('txh', 0.314), ('mask', 0.289), ('globular', 0.277), ('occlusive', 0.216), ('components', 0.175), ('occlusions', 0.165), ('image', 0.156), ('elds', 0.154), ('patches', 0.154), ('rfs', 0.144), ('receptive', 0.135), ('encoding', 0.133), ('md', 0.13), ('component', 0.128), ('convolutional', 0.121), ('positions', 0.117), ('invariant', 0.108), ('patch', 0.106), ('position', 0.103), ('rh', 0.102), ('visual', 0.099), ('generative', 0.096), ('kn', 0.083), ('sh', 0.082), ('temperature', 0.079), ('exclusiveness', 0.079), ('gabors', 0.079), ('gabor', 0.076), ('wh', 0.069), ('feature', 0.068), ('translation', 0.064), ('emerge', 0.06), ('cke', 0.059), ('overcompleteness', 0.059), ('pixel', 0.059), ('occurrence', 0.058), ('masks', 0.057), ('decreased', 0.057), ('features', 0.056), ('yd', 0.053), ('transparency', 0.052), ('units', 0.051), ('learned', 0.051), ('translations', 0.051), ('posterior', 0.05), ('depth', 0.05), ('hidden', 0.048), ('qn', 0.048), ('dog', 0.048), ('images', 0.047), ('eight', 0.045), ('explicit', 0.043), ('turner', 0.043), ('object', 0.042), ('kavukcuoglu', 0.041), ('superposition', 0.041), ('occlusion', 0.041), ('lters', 0.041), ('supplement', 0.041), ('bornschein', 0.039), ('exarchakis', 0.039), ('jet', 0.039), ('noninvariant', 0.039), ('xmap', 0.039), ('frequencies', 0.037), ('bars', 0.037), ('scalar', 0.037), ('stimulus', 0.035), ('annealing', 0.035), ('pursuit', 0.035), ('lgn', 0.035), ('occluding', 0.035), ('hateren', 0.035), ('reverse', 0.035), ('pixels', 0.034), ('probabilities', 0.033), ('exclusively', 0.033), ('cadieu', 0.032), ('truncated', 0.031), ('complex', 0.031), ('infer', 0.03), ('cells', 0.03), ('injected', 0.03), ('invariances', 0.03), ('percentages', 0.03), ('generation', 0.03), ('inversion', 0.029), ('probabilistic', 0.029), ('dai', 0.029), ('prevalence', 0.029), ('neurally', 0.029), ('gregor', 0.029), ('alongside', 0.029), ('parameters', 0.029), ('optima', 0.028), ('tted', 0.028), ('presence', 0.028), ('estimated', 0.028), ('inferred', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="351-tfidf-1" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>2 0.19572642 <a title="351-tfidf-2" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>3 0.16620187 <a title="351-tfidf-3" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>4 0.13463968 <a title="351-tfidf-4" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>5 0.13128249 <a title="351-tfidf-5" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>Author: Carl Doersch, Abhinav Gupta, Alexei A. Efros</p><p>Abstract: Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical “visual words”, but lower than full-blown semantic objects. Several approaches [5, 6, 12, 23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difﬁcult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classiﬁcation, demonstrating state-of-the-art performance on the MIT Scene-67 dataset. 1</p><p>6 0.11548235 <a title="351-tfidf-6" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>7 0.11403497 <a title="351-tfidf-7" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>8 0.10146707 <a title="351-tfidf-8" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>9 0.10042195 <a title="351-tfidf-9" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>10 0.099291958 <a title="351-tfidf-10" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>11 0.087801062 <a title="351-tfidf-11" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>12 0.085067175 <a title="351-tfidf-12" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>13 0.084443748 <a title="351-tfidf-13" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>14 0.082624026 <a title="351-tfidf-14" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>15 0.081891686 <a title="351-tfidf-15" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>16 0.079348795 <a title="351-tfidf-16" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>17 0.077532187 <a title="351-tfidf-17" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>18 0.076487176 <a title="351-tfidf-18" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>19 0.075420849 <a title="351-tfidf-19" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>20 0.073850229 <a title="351-tfidf-20" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, 0.102), (2, -0.152), (3, -0.072), (4, 0.022), (5, -0.081), (6, -0.029), (7, 0.032), (8, -0.055), (9, -0.008), (10, -0.074), (11, 0.08), (12, -0.003), (13, 0.047), (14, -0.052), (15, 0.023), (16, -0.042), (17, -0.186), (18, -0.104), (19, 0.025), (20, 0.043), (21, 0.014), (22, -0.024), (23, -0.083), (24, -0.063), (25, -0.022), (26, 0.039), (27, 0.007), (28, 0.015), (29, -0.032), (30, 0.043), (31, 0.027), (32, 0.012), (33, 0.006), (34, -0.145), (35, 0.022), (36, 0.057), (37, 0.054), (38, 0.039), (39, -0.055), (40, 0.039), (41, -0.137), (42, -0.072), (43, -0.097), (44, -0.024), (45, -0.013), (46, -0.045), (47, 0.113), (48, 0.021), (49, -0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94951963 <a title="351-lsi-1" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>2 0.81714982 <a title="351-lsi-2" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>Author: Dan Rosenbaum, Daniel Zoran, Yair Weiss</p><p>Abstract: Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical ﬂow to learn the local statistics of optical ﬂow and compare the learned models to prior models assumed by computer vision researchers. We ﬁnd that a Gaussian mixture model (GMM) with 64 components provides a signiﬁcantly better model for local ﬂow statistics when compared to commonly used models. We investigate the source of the GMM’s success and show it is related to an explicit representation of ﬂow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical ﬂow. In accordance with the assumptions often made in computer vision, the model learns that ﬂow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the beneﬁt of conditioning ﬂow estimation on the local intensity pattern is marginal. 1</p><p>3 0.72703266 <a title="351-lsi-3" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>Author: Marius Pachitariu, Adam M. Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani</p><p>Abstract: Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identiﬁcation of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the KSVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We ﬁt the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The ﬂexibility of the block-based representation is reﬂected in the variability of the recovered cell shapes. 1</p><p>4 0.71581125 <a title="351-lsi-4" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>Author: Chen-Ping Yu, Wen-Yu Hua, Dimitris Samaras, Greg Zelinsky</p><p>Abstract: Visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept. Our approach models clutter as the number of proto-objects segmented from an image, with proto-objects deﬁned as groupings of superpixels that are similar in intensity, color, and gradient orientation features. We introduce a novel parametric method of clustering superpixels by modeling mixture of Weibulls on Earth Mover’s Distance statistics, then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception. We validated this model using a new 90-image dataset of real world scenes rank ordered by human raters for clutter, and showed that our method not only predicted clutter extremely well (Spearman’s ρ = 0.8038, p < 0.001), but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth. We conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features. 1</p><p>5 0.69991326 <a title="351-lsi-5" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>Author: Carl Doersch, Abhinav Gupta, Alexei A. Efros</p><p>Abstract: Recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical “visual words”, but lower than full-blown semantic objects. Several approaches [5, 6, 12, 23] have been proposed to discover mid-level visual elements, that are both 1) representative, i.e., frequently occurring within a visual dataset, and 2) visually discriminative. However, the current approaches are rather ad hoc and difﬁcult to analyze and evaluate. In this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm [2, 1, 4, 8]. Given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels. One advantage of our formulation is that it requires only a single pass through the data. We also propose the Purity-Coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the Paris Street View dataset of [5]. We also evaluate our method on the task of scene classiﬁcation, demonstrating state-of-the-art performance on the MIT Scene-67 dataset. 1</p><p>6 0.68998748 <a title="351-lsi-6" href="./nips-2013-Approximate_Bayesian_Image_Interpretation_using_Generative_Probabilistic_Graphics_Programs.html">37 nips-2013-Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</a></p>
<p>7 0.65936816 <a title="351-lsi-7" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>8 0.65401244 <a title="351-lsi-8" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>9 0.64517319 <a title="351-lsi-9" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>10 0.62585175 <a title="351-lsi-10" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>11 0.62421077 <a title="351-lsi-11" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>12 0.59898812 <a title="351-lsi-12" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>13 0.58955079 <a title="351-lsi-13" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>14 0.58794558 <a title="351-lsi-14" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>15 0.58313376 <a title="351-lsi-15" href="./nips-2013-Fast_Template_Evaluation_with_Vector_Quantization.html">119 nips-2013-Fast Template Evaluation with Vector Quantization</a></p>
<p>16 0.55710518 <a title="351-lsi-16" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>17 0.5316869 <a title="351-lsi-17" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>18 0.51954776 <a title="351-lsi-18" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>19 0.49098283 <a title="351-lsi-19" href="./nips-2013-RNADE%3A_The_real-valued_neural_autoregressive_density-estimator.html">260 nips-2013-RNADE: The real-valued neural autoregressive density-estimator</a></p>
<p>20 0.48135626 <a title="351-lsi-20" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.043), (33, 0.11), (34, 0.568), (41, 0.015), (49, 0.028), (56, 0.048), (70, 0.033), (85, 0.019), (89, 0.033), (93, 0.027), (95, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97003978 <a title="351-lda-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>same-paper 2 0.96591979 <a title="351-lda-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.95507562 <a title="351-lda-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>4 0.9496873 <a title="351-lda-4" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>5 0.94254225 <a title="351-lda-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>6 0.9380008 <a title="351-lda-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>7 0.924007 <a title="351-lda-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.9225136 <a title="351-lda-8" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>9 0.90706509 <a title="351-lda-9" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>10 0.90229952 <a title="351-lda-10" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>11 0.79089606 <a title="351-lda-11" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>12 0.78051203 <a title="351-lda-12" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>13 0.77753425 <a title="351-lda-13" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>14 0.75713152 <a title="351-lda-14" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>15 0.75049156 <a title="351-lda-15" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>16 0.74804491 <a title="351-lda-16" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>17 0.73976761 <a title="351-lda-17" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>18 0.7373476 <a title="351-lda-18" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>19 0.73436695 <a title="351-lda-19" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>20 0.73370427 <a title="351-lda-20" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
