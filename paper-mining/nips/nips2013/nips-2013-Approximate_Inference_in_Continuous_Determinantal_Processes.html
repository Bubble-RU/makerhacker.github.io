<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 nips-2013-Approximate Inference in Continuous Determinantal Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-40" href="#">nips2013-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 nips-2013-Approximate Inference in Continuous Determinantal Processes</h1>
<br/><p>Source: <a title="nips-2013-40-pdf" href="http://papers.nips.cc/paper/4916-approximate-inference-in-continuous-determinantal-processes.pdf">pdf</a></p><p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><p>Reference: <a title="nips-2013-40-reference" href="../nips2013_reference/nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. [sent-8, score-0.432]
</p><p>2 While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. [sent-10, score-0.152]
</p><p>3 In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. [sent-11, score-0.359]
</p><p>4 We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. [sent-12, score-0.48]
</p><p>5 1  Introduction  Samples from a determinantal point process (DPP) [15] are sets of points that tend to be spread out. [sent-13, score-0.16]
</p><p>6 More speciﬁcally, given Ω ⊆ Rd and a positive semideﬁnite kernel function L : Ω × Ω → R, the probability density of a point conﬁguration A ⊂ Ω under a DPP with kernel L is given by PL (A) ∝ det(LA ) ,  (1)  where LA is the |A| × |A| matrix with entries L(x, y) for each x, y ∈ A. [sent-14, score-0.437]
</p><p>7 The tendency for repulsion is captured by the determinant since it depends on the volume spanned by the selected points in the associated Hilbert space of L. [sent-15, score-0.137]
</p><p>8 [10], which relies on the eigendecomposition of the kernel matrix to recursively sample points based on their projections onto the subspace spanned by the selected eigenvectors. [sent-19, score-0.387]
</p><p>9 Repulsive point processes, like hard core processes [7, 16], many based on thinned Poisson processes and Gibbs/Markov distributions, have a long history in the spatial statistics community, where considering continuous Ω is key. [sent-20, score-0.183]
</p><p>10 Repulsive processes on continuous spaces have garnered interest in machine learning as well, especially relating to generative mixture modeling [18, 29]. [sent-22, score-0.24]
</p><p>11 On the surface, it seems that the eigendecomposition and projection algorithm of [10] for discrete DPPs would naturally extend to the continuous case. [sent-24, score-0.209]
</p><p>12 The absence of a tractable DPP sampling algorithm for general kernels in continuous spaces has hindered progress in developing DPP-based models for repulsion. [sent-26, score-0.278]
</p><p>13 In this paper, we propose an efﬁcient algorithm to sample from DPPs in continuous spaces using low-rank approximations of the kernel function. [sent-27, score-0.369]
</p><p>14 For k-DPPs, which only place positive probability on sets of cardinality k [13], we also devise a Gibbs sampler that iteratively samples points in the k-set conditioned on all k − 1 other points. [sent-30, score-0.148]
</p><p>15 Our methods allow us to handle a broad range of typical kernels and continuous subspaces, provided certain simple integrals of the kernel function can be computed efﬁciently. [sent-32, score-0.316]
</p><p>16 Decomposing our kernel into quality and similarity terms as in [13], this includes, but is not limited to, all cases where the (i) spectral density of the quality and (ii) characteristic function of the similarity kernel can be computed efﬁciently. [sent-33, score-0.635]
</p><p>17 2, we review sampling algorithms for discrete DPPs and the challenges associated with sampling from continuous DPPs. [sent-36, score-0.299]
</p><p>18 We then propose continuous DPP sampling algorithms based on low-rank kernel approximations in Sec. [sent-37, score-0.398]
</p><p>19 Finally, we apply our methods to repulsive mixture modeling and human pose synthesis in Sec. [sent-42, score-0.322]
</p><p>20 2  Sampling from a DPP  When Ω is discrete with cardinality N , an efﬁcient algorithm for sampling from a DPP is given in [10]. [sent-44, score-0.165]
</p><p>21 The algorithm, which is detailed in the supplement, uses an eigendecomposition of the N kernel matrix L = n=1 λn vn vn and recursively samples points xi as follows, resulting in a set A ∼ DPP(L) with A = {xi }: Phase 1 Select eigenvector vn with probability  λn λn +1 . [sent-45, score-0.461]
</p><p>22 When Ω is discrete, both steps are straightforward since the ﬁrst phase involves eigendecomposing a kernel matrix and the second phase involves sampling from discrete probability distributions based on inner products between points and eigenvectors. [sent-52, score-0.492]
</p><p>23 Extending this algorithm to a continuous space was considered by [14], but for a very limited set of kernels L and spaces Ω. [sent-53, score-0.178]
</p><p>24 Extending Phase 1 to a continuous space requires knowledge of the eigendecomposition of the kernel function. [sent-55, score-0.361]
</p><p>25 When Ω is a compact rectangle in Rd , [14] suggest approximating the eigendecomposition using an orthonormal Fourier basis. [sent-56, score-0.132]
</p><p>26 Even if we are able to obtain the eigendecomposition of the kernel function (either directly or via approximations as considered in [14] and Sec. [sent-57, score-0.344]
</p><p>27 Whereas the discrete case only requires sampling from a discrete probability function, here we have to sample from a probability density. [sent-59, score-0.192]
</p><p>28 When Ω is compact, [14] suggest using a rejection sampler with a uniform proposal on Ω. [sent-60, score-0.139]
</p><p>29 The authors note that the acceptance rate of this rejection sampler decreases with the number of points sampled, making the method inefﬁcient in sampling large sets from a DPP. [sent-61, score-0.247]
</p><p>30 In most other cases, implementing Phase 2 even via rejection sampling is infeasible since the target density is in general non-standard with unknown normalization. [sent-62, score-0.192]
</p><p>31 In summary, current algorithms can sample approximately from a continuous DPP only for translationinvariant kernels deﬁned on a compact space. [sent-64, score-0.151]
</p><p>32 3, we propose a sampling algorithm that allows us to sample approximately from DPPs for a wide range of kernels L and spaces Ω. [sent-66, score-0.261]
</p><p>33 2  3  Sampling from a low-rank continuous DPP  Again considering Ω discrete with cardinality N , the sampling algorithm of Sec. [sent-67, score-0.229]
</p><p>34 The basic idea is to exploit the fact that L and the dual kernel matrix C = BB , which is D × D, share the same nonzero eigenvalues, and for each eigenvector vk of L, Bvk is the corresponding eigenvector of C. [sent-72, score-0.329]
</p><p>35 While the dependence on N in the dual is sharply reduced, in continuous spaces, N is inﬁnite. [sent-74, score-0.114]
</p><p>36 Generically, consider sampling from a DPP ∞ on a continuous space Ω with kernel L(x, y) = n=1 λn φn (x)φn (y),where λn and φn (x) are eigenvalues and eigenfunctions, and φn (y) is the complex conjugate of φn (y). [sent-76, score-0.417]
</p><p>37 Using such a low-rank representation, we propose an analog of the dual sampling algorithm for continuous spaces, described in Algorithm 1. [sent-86, score-0.214]
</p><p>38 Algorithm 1 Dual sampler for a low-rank continuous DPP ˜ Input: L(x, y) = B(x)∗ B(y), PHASE 2 a rank-D DPP kernel X←∅ PHASE 1 while |V | > 0 do 1 Compute C = Ω B(x)B(x)∗ dx ˆ Sample x from f (x) = |V | v∈V |v∗ B(x)|2 D ∗ X ← X ∪ {ˆ } x Compute eigendecomp. [sent-90, score-0.367]
</p><p>39 v1 , v2 = v1 Cv2 V ← { √ vk }k∈J ∗ Cv Output: X vk k In this dual view, we still have the same two-phase structure, and must address two key challenges: Phase 1 Assuming a low-rank kernel function decomposition as in Eq. [sent-97, score-0.329]
</p><p>40 (2), we need to able to compute the dual kernel matrix, given by an integral: B(x)B(x)∗ dx . [sent-98, score-0.265]
</p><p>41 C=  (3)  Ω  Phase 2 In general, sampling directly from the density f (x) is difﬁcult; instead, we can compute the cumulative distribution function (CDF) and sample x using the inverse CDF method [21]: d  xl ˆ  l=1  −∞  F (ˆ = (ˆ1 , . [sent-99, score-0.297]
</p><p>42 (4)  ˜ Assuming (i) the kernel function L is ﬁnite-rank and (ii) the terms C and f (x) are computable, ˜ Algorithm 1 provides exact samples from a DPP with kernel L. [sent-103, score-0.374]
</p><p>43 In what follows, approximations only ˜ arise from approximating general kernels L with low-rank kernels L. [sent-104, score-0.199]
</p><p>44 If given a ﬁnite-rank kernel L to begin with, the sampling procedure is exact. [sent-105, score-0.287]
</p><p>45 For our approximation algorithm to work, not only do we need methods that approximate the kernel function well, but also that enable us to solve Eq. [sent-110, score-0.212]
</p><p>46 (3) and (4) directly for many different kernel functions. [sent-111, score-0.187]
</p><p>47 The frequencies are sampled independently from the Fourier transform of the kernel function, ωj ∼ F(k(x − y)), and letting: 1 ˜ k(x − y) = D  D  exp{iωj (x − y)} ,  x, y ∈ Ω . [sent-115, score-0.187]
</p><p>48 (5)  j=1  To apply RFFs, we factor L into a quality function q and similarity kernel k (i. [sent-116, score-0.275]
</p><p>49 (5), we can approximate the similarity kernel function to obtain a low-rank kernel and dual matrix: 1 ˜ LRF F (x, y) = D  D RF q(x) exp{iωj (x − y)}q(y), Cjk F = j=1  1 D  q 2 (x) exp{i(ωj − ωk ) x}dx. [sent-123, score-0.479]
</p><p>50 Ω  The CDF of the sampling distribution f (x) in Algorithm 1 is given by: 1 FRF F (ˆ ) = x |V |  D  D  d ∗ vj vk  v∈V j=1 k=1  xl ˆ  q 2 (x) exp{i(ωj − ωk ) x}1{xl ∈Ω} dxl . [sent-124, score-0.326]
</p><p>51 In fact, this method works for any combination of (i) translation-invariant similarity kernel k with known characteristic function and (ii) quality function q with known spectral density. [sent-127, score-0.297]
</p><p>52 The resulting kernel L need not be translation invariant. [sent-128, score-0.187]
</p><p>53 In the supplement, we illustrate this method by considering a common and important example where Ω = Rd , q(x) is Gaussian, and k(x, y) is any kernel with known Fourier transform. [sent-129, score-0.187]
</p><p>54 2 Sampling from a Nystr¨ m-approximated DPP o Another approach to kernel approximation is the Nystr¨ m method [27]. [sent-131, score-0.212]
</p><p>55 , zD o landmarks sampled from Ω, we can approximate the kernel function and dual matrix as, D  D  D  D  N 2 Wjk L(x, zj )L(zk , y), Cjkys =  ˜ LN ys (x, y) =  D n=1  where Wjk = L(zj , zk )−1/2 . [sent-135, score-0.324]
</p><p>56 1 is:  d  xl ˆ  l=1  −∞  wj (v)wk (v) v∈V j=1 k=1  L(zn , x)L(x, zm )dx, Ω  L(x, zj )L(zk , x)1{xl ∈Ω} dxl . [sent-137, score-0.203]
</p><p>57 Here, there are no translation-invariant requirements, even for the similarity kernel k. [sent-139, score-0.242]
</p><p>58 4  Gibbs sampling  For k-DPPs, we can consider a Gibbs sampling scheme. [sent-141, score-0.2]
</p><p>59 Let the kernel function be represented as before: L(x, y) = q(x)k(x, y)q(y). [sent-143, score-0.187]
</p><p>60 Denoting J \k = {xj }j=k and M \k = L−1 the full conditional can J \k be simpliﬁed using Schur’s determinantal equality [22]: \k  p(xk |{xj }j=k ) ∝ L(xk , xk ) −  Mij L(xi , xk )L(xj , xk ). [sent-144, score-0.265]
</p><p>61 2 0 0  20  40  (c)  60  80  100  (d)  Figure 1: Estimates of total variational distance for Nystr¨ m and RFF approximation methods to a DPP with o  Gaussian quality and similarity with covariances Γ = diag(ρ2 , . [sent-161, score-0.19]
</p><p>62 However, for a wide range of kernel functions, including those which can be handled by the Nystr¨ m approximation in Sec. [sent-171, score-0.237]
</p><p>63 We use this same Schur complement scheme for sampling from the full conditionals in the mixture model application of Sec. [sent-174, score-0.201]
</p><p>64 A key advantage of this scheme for several types of kernels is that the complexity of sampling scales linearly with the number of dimensions d making it suitable in handling high-dimensional spaces. [sent-176, score-0.165]
</p><p>65 In cases where the kernel introduces low repulsion we expect the Gibbs sampler to mix well, while in a high repulsion setting the sampler can mix slowly due to the strong dependencies between points and fact that we are only doing one-point-at-a-time moves. [sent-178, score-0.531]
</p><p>66 5  Empirical analysis  To evaluate the performance of the RFF and Nystr¨ m approximations, we compute the total variational o distance PL − PL 1 = 1 X |PL (X) − PL (X)|, where PL (X) denotes the probability of set X ˜ ˜ 2 under a DPP with kernel L, as given by Eq. [sent-185, score-0.239]
</p><p>67 We restrict our analysis to the case where the quality function and similarity kernel are Gaussians with isotropic covariances Γ = diag(ρ2 , . [sent-187, score-0.3]
</p><p>68 1 displays estimates of the total variational distance for the RFF and Nystr¨ m approximations o when ρ2 = 1, varying σ 2 (the repulsion strength) and the dimension d. [sent-197, score-0.168]
</p><p>69 While this phenomenon seems perplexing at ﬁrst, a study of the eigenvalues of the Gaussian kernel across dimensions sheds light on the rationale (see Fig. [sent-199, score-0.274]
</p><p>70 It has been previously demonstrated that the Nystr¨ m o method performs favorably in kernel learning tasks compared to RFF in cases where there is a large eigengap in the kernel matrix [28]. [sent-202, score-0.374]
</p><p>71 For the RFF, while the kernel approximation is guaranteed to be an unbiased estimate of the true kernel element-wise, the variance is fairly high [19]. [sent-206, score-0.399]
</p><p>72 For the Nystr¨ m method, on the other hand, the quality of the approximation depends on how well the o landmarks cover Ω. [sent-208, score-0.117]
</p><p>73 We believe the same result holds for continuous Ω by extending the eigenvalues and spectral norm of the kernel matrix to operator eigenvalues and operator norms, respectively. [sent-215, score-0.383]
</p><p>74 6  Repulsive priors for mixture models  Mixture models are used in a wide range of applications from clustering to density estimation. [sent-217, score-0.193]
</p><p>75 Instead, [18] show that sampling the location parameters using repulsive priors leads to better separated clusters while maintaining the accuracy of the density estimate. [sent-222, score-0.438]
</p><p>76 They propose a class of repulsive priors that rely on explicitly deﬁning a distance metric and the manner in which small distances are penalized. [sent-223, score-0.243]
</p><p>77 The theoretical properties of DPPs make them an appealing choice as a repulsive prior. [sent-225, score-0.194]
</p><p>78 In fact, [29] considered using DPPs as repulsive priors in latent variable models. [sent-226, score-0.219]
</p><p>79 However, in the absence of a feasible continuous DPP sampling algorithm, their method was restricted to performing MAP inference. [sent-227, score-0.164]
</p><p>80 In the common case of mixtures of Gaussians (MoG), our posterior computations can be performed using Gibbs sampling with nearly the same simplicity of the standard case where the location parameters µk are assumed to be i. [sent-229, score-0.154]
</p><p>81 For the location parameters, instead of sampling each µk independently from its conditional posterior, our full conditional depends upon the other locations µ\k as well. [sent-237, score-0.125]
</p><p>82 We assess the clustering and density estimation performance of the DPP-based model on both synthetic and real datasets. [sent-241, score-0.117]
</p><p>83 Synthetic data To assess the role of the prior in a density estimation task, we generated a small sample of 100 observations from a mixture of two Gaussians. [sent-246, score-0.195]
</p><p>84 As a measure of simplicity of the resulting density description, we compute the average entropy of the posterior mixture membership distribution, which is a reasonable metric given the similarity of the overall densities. [sent-256, score-0.307]
</p><p>85 We also assess the accuracy of the density estimate by computing both (i) Hamming distance error relative to true cluster labels and (ii) held-out log-likelihood on 100 observations. [sent-258, score-0.117]
</p><p>86 2 −2  0  2  4  0 −1  0  1  2  3  4  0 −3  Well-Sep Poor-Sep Galaxy Enzyme Acidity Figure 2: For each synthetic and real dataset: (top) histogram of data overlaid with actual Gaussian mixture generating the synthetic data, and posterior mean mixture model for (middle) IID and (bottom) DPP. [sent-355, score-0.237]
</p><p>87 Table 1: For IID and DPP on synthetic datasets: mean (stdev) for mixture membership entropy, cluster assignment error rate and held-out log-likelihood of 100 observations under the posterior mean density estimate. [sent-357, score-0.23]
</p><p>88 We once again judge the complexity of the density estimates using the posterior mixture membership entropy as a proxy. [sent-376, score-0.252]
</p><p>89 Finally, we consider a classiﬁcation task based on the iris dataset: 150 observations from three iris species with four length measurements. [sent-381, score-0.124]
</p><p>90 A typical approximation is to use a set of perturbed expert trajectories as a comparison set, where a good set of trajectories should cover as large a part of the space as possible. [sent-391, score-0.119]
</p><p>91 7  Table 2: For IID and DPP, mean (stdev) of (left) mixture membership entropy and held-out log-likelihood for three density estimation tasks and (right) classiﬁcation error under 2 vs. [sent-392, score-0.223]
</p><p>92 2 0 0  Original  DPP IID 50  100  ε  DPP Samples Figure 3: Left: Diverse set of human poses relative to an original pose by sampling from an RFF (top) and Nystr¨ m (bottom) approximations with kernel based on MoCap of the activity dance. [sent-419, score-0.474]
</p><p>93 To achieve this, we build a kernel with Gaussian quality and similarity using covariances estimated from the training data associated with the activity. [sent-427, score-0.3]
</p><p>94 The Gaussian quality is centered about the selected reference pose and we synthesize new poses by sampling from our continuous DPP using the low-rank approximation scheme. [sent-428, score-0.329]
</p><p>95 For the activity dance, to quantitatively assess our performance in covering the activity space, we compute a coverage rate metric based on a random sample of 50 poses from a DPP. [sent-431, score-0.223]
</p><p>96 case by inﬂating the variance to match the diverse DPP sample, the DPP poses still provide better average coverage over 100 runs. [sent-440, score-0.157]
</p><p>97 Our low-rank approach harnessed approximations provided by Nystr¨ m and random Fourier o feature methods and then utilized a continuous dual DPP representation. [sent-454, score-0.161]
</p><p>98 The resulting approximate sampler garners the same efﬁciencies that led to the success of the DPP in the discrete case. [sent-455, score-0.123]
</p><p>99 Finally, we demonstrated that continuous-DPP sampling is useful both for repulsive mixture modeling (which utilizes the Gibbs sampling scheme) and in synthesizing diverse human poses (which we demonstrated with the low-rank approximation method). [sent-458, score-0.663]
</p><p>100 Using the Nystr¨ m method to speed up kernel machines. [sent-637, score-0.187]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dpp', 0.674), ('rff', 0.288), ('nystr', 0.236), ('dpps', 0.221), ('repulsive', 0.194), ('kernel', 0.187), ('determinantal', 0.13), ('xl', 0.112), ('eigendecomposition', 0.11), ('sampling', 0.1), ('iid', 0.096), ('sampler', 0.088), ('acidity', 0.085), ('mixture', 0.08), ('supplement', 0.07), ('phase', 0.07), ('fourier', 0.069), ('repulsion', 0.069), ('kulesza', 0.068), ('dxl', 0.068), ('mocap', 0.068), ('eigenvalues', 0.066), ('gibbs', 0.066), ('kernels', 0.065), ('continuous', 0.064), ('density', 0.063), ('iris', 0.062), ('diverse', 0.058), ('schur', 0.057), ('poses', 0.056), ('pl', 0.056), ('galaxy', 0.055), ('cdf', 0.055), ('similarity', 0.055), ('eigenfunctions', 0.052), ('dual', 0.05), ('spaces', 0.049), ('processes', 0.047), ('approximations', 0.047), ('entropy', 0.046), ('vk', 0.046), ('xk', 0.045), ('coverage', 0.043), ('enzyme', 0.041), ('nystrom', 0.041), ('spanned', 0.038), ('landmarks', 0.037), ('vn', 0.037), ('activity', 0.036), ('trajectories', 0.036), ('discrete', 0.035), ('cls', 0.034), ('frf', 0.034), ('jrss', 0.034), ('mog', 0.034), ('wjn', 0.034), ('membership', 0.034), ('quality', 0.033), ('clusters', 0.031), ('points', 0.03), ('assess', 0.03), ('affandi', 0.03), ('wjk', 0.03), ('cardinality', 0.03), ('posterior', 0.029), ('rejection', 0.029), ('dx', 0.028), ('mij', 0.028), ('die', 0.028), ('stdev', 0.028), ('synthesizing', 0.028), ('variational', 0.028), ('zk', 0.027), ('pose', 0.026), ('hough', 0.026), ('nerve', 0.026), ('diversity', 0.026), ('approximation', 0.025), ('xj', 0.025), ('covariances', 0.025), ('spatial', 0.025), ('location', 0.025), ('wide', 0.025), ('synthesize', 0.025), ('priors', 0.025), ('synthetic', 0.024), ('distance', 0.024), ('zj', 0.023), ('eigenvector', 0.023), ('heldout', 0.023), ('proposal', 0.022), ('cover', 0.022), ('sample', 0.022), ('approximating', 0.022), ('human', 0.022), ('characteristic', 0.022), ('phenomenon', 0.021), ('diag', 0.021), ('complement', 0.021), ('growing', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="40-tfidf-1" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><p>2 0.4932549 <a title="40-tfidf-2" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>Author: Jie Wang, Jiayu Zhou, Peter Wonka, Jieping Ye</p><p>Abstract: Lasso is a widely used regression technique to ﬁnd sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efﬁciency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efﬁcient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no “exact” screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso. 1</p><p>3 0.39629638 <a title="40-tfidf-3" href="./nips-2013-Fast_Determinantal_Point_Process_Sampling_with_Application_to_Clustering.html">118 nips-2013-Fast Determinantal Point Process Sampling with Application to Clustering</a></p>
<p>Author: Byungkon Kang</p><p>Abstract: Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive deﬁnite matrix that deﬁnes the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time. In addition, we show that this framework can be extended to sampling from cardinalityconstrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.</p><p>4 0.22694342 <a title="40-tfidf-4" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>5 0.13474727 <a title="40-tfidf-5" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><p>6 0.1016748 <a title="40-tfidf-6" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>7 0.090936139 <a title="40-tfidf-7" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>8 0.08871223 <a title="40-tfidf-8" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>9 0.088117801 <a title="40-tfidf-9" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>10 0.082747661 <a title="40-tfidf-10" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>11 0.081785195 <a title="40-tfidf-11" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>12 0.079868078 <a title="40-tfidf-12" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>13 0.068249144 <a title="40-tfidf-13" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>14 0.062371727 <a title="40-tfidf-14" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<p>15 0.059403572 <a title="40-tfidf-15" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>16 0.059191044 <a title="40-tfidf-16" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>17 0.058947958 <a title="40-tfidf-17" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>18 0.05801563 <a title="40-tfidf-18" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>19 0.057438817 <a title="40-tfidf-19" href="./nips-2013-High-Dimensional_Gaussian_Process_Bandits.html">137 nips-2013-High-Dimensional Gaussian Process Bandits</a></p>
<p>20 0.055903137 <a title="40-tfidf-20" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.073), (2, -0.014), (3, 0.016), (4, -0.052), (5, 0.119), (6, 0.094), (7, -0.009), (8, -0.048), (9, 0.017), (10, -0.061), (11, -0.046), (12, -0.113), (13, -0.223), (14, 0.296), (15, 0.136), (16, -0.052), (17, -0.296), (18, 0.488), (19, 0.049), (20, 0.081), (21, -0.225), (22, -0.023), (23, -0.043), (24, 0.022), (25, -0.022), (26, 0.062), (27, 0.028), (28, 0.001), (29, -0.02), (30, 0.026), (31, 0.026), (32, -0.082), (33, 0.041), (34, 0.031), (35, 0.037), (36, -0.003), (37, -0.007), (38, -0.001), (39, 0.023), (40, 0.003), (41, -0.027), (42, 0.007), (43, -0.031), (44, 0.035), (45, 0.051), (46, 0.025), (47, -0.021), (48, 0.029), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90224719 <a title="40-lsi-1" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><p>2 0.83978105 <a title="40-lsi-2" href="./nips-2013-Fast_Determinantal_Point_Process_Sampling_with_Application_to_Clustering.html">118 nips-2013-Fast Determinantal Point Process Sampling with Application to Clustering</a></p>
<p>Author: Byungkon Kang</p><p>Abstract: Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive deﬁnite matrix that deﬁnes the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time. In addition, we show that this framework can be extended to sampling from cardinalityconstrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.</p><p>3 0.80500883 <a title="40-lsi-3" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>Author: Jie Wang, Jiayu Zhou, Peter Wonka, Jieping Ye</p><p>Abstract: Lasso is a widely used regression technique to ﬁnd sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efﬁciency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efﬁcient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no “exact” screening rule for group Lasso. We have evaluated our screening rule using many real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso. 1</p><p>4 0.3990013 <a title="40-lsi-4" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><p>5 0.39427638 <a title="40-lsi-5" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>6 0.3344011 <a title="40-lsi-6" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>7 0.3075836 <a title="40-lsi-7" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>8 0.29297307 <a title="40-lsi-8" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>9 0.27280724 <a title="40-lsi-9" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>10 0.26727363 <a title="40-lsi-10" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>11 0.25499058 <a title="40-lsi-11" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>12 0.25164786 <a title="40-lsi-12" href="./nips-2013-Rapid_Distance-Based_Outlier_Detection_via_Sampling.html">261 nips-2013-Rapid Distance-Based Outlier Detection via Sampling</a></p>
<p>13 0.24916406 <a title="40-lsi-13" href="./nips-2013-Parallel_Sampling_of_DP_Mixture_Models_using_Sub-Cluster_Splits.html">243 nips-2013-Parallel Sampling of DP Mixture Models using Sub-Cluster Splits</a></p>
<p>14 0.24752928 <a title="40-lsi-14" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>15 0.24632169 <a title="40-lsi-15" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>16 0.24443455 <a title="40-lsi-16" href="./nips-2013-Scalable_kernels_for_graphs_with_continuous_attributes.html">289 nips-2013-Scalable kernels for graphs with continuous attributes</a></p>
<p>17 0.23406123 <a title="40-lsi-17" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>18 0.23341954 <a title="40-lsi-18" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>19 0.22468291 <a title="40-lsi-19" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>20 0.21927695 <a title="40-lsi-20" href="./nips-2013-Analyzing_Hogwild_Parallel_Gaussian_Gibbs_Sampling.html">34 nips-2013-Analyzing Hogwild Parallel Gaussian Gibbs Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.027), (16, 0.06), (33, 0.145), (34, 0.108), (39, 0.019), (41, 0.035), (49, 0.049), (56, 0.094), (59, 0.232), (70, 0.044), (85, 0.025), (89, 0.038), (93, 0.025), (95, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85497355 <a title="40-lda-1" href="./nips-2013-Graphical_Models_for_Inference_with_Missing_Data.html">134 nips-2013-Graphical Models for Inference with Missing Data</a></p>
<p>Author: Karthika Mohan, Judea Pearl, Jin Tian</p><p>Abstract: We address the problem of recoverability i.e. deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called ‘Missingness Graphs’ to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation, we derive conditions that the graph should satisfy to ensure recoverability and devise algorithms to detect the presence of these conditions in the graph. 1</p><p>2 0.83098245 <a title="40-lda-2" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>Author: Myunghwan Kim, Jure Leskovec</p><p>Abstract: unkown-abstract</p><p>same-paper 3 0.78828716 <a title="40-lda-3" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><p>4 0.69478101 <a title="40-lda-4" href="./nips-2013-Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits.html">330 nips-2013-Thompson Sampling for 1-Dimensional Exponential Family Bandits</a></p>
<p>Author: Nathaniel Korda, Emilie Kaufmann, Remi Munos</p><p>Abstract: Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (through the Jeffreys prior) available in an exponential family. This allow us to give a ﬁnite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families. 1</p><p>5 0.68553513 <a title="40-lda-5" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>6 0.68464249 <a title="40-lda-6" href="./nips-2013-Bayesian_Inference_and_Online_Experimental_Design_for_Mapping_Neural_Microcircuits.html">49 nips-2013-Bayesian Inference and Online Experimental Design for Mapping Neural Microcircuits</a></p>
<p>7 0.68071592 <a title="40-lda-7" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>8 0.68000865 <a title="40-lda-8" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>9 0.6799069 <a title="40-lda-9" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>10 0.67842954 <a title="40-lda-10" href="./nips-2013-Statistical_analysis_of_coupled_time_series_with_Kernel_Cross-Spectral_Density_operators..html">310 nips-2013-Statistical analysis of coupled time series with Kernel Cross-Spectral Density operators.</a></p>
<p>11 0.6777072 <a title="40-lda-11" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>12 0.6775344 <a title="40-lda-12" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>13 0.67566359 <a title="40-lda-13" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>14 0.67545784 <a title="40-lda-14" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>15 0.67498803 <a title="40-lda-15" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>16 0.67467481 <a title="40-lda-16" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>17 0.67430657 <a title="40-lda-17" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>18 0.67402995 <a title="40-lda-18" href="./nips-2013-Fast_Determinantal_Point_Process_Sampling_with_Application_to_Clustering.html">118 nips-2013-Fast Determinantal Point Process Sampling with Application to Clustering</a></p>
<p>19 0.67347199 <a title="40-lda-19" href="./nips-2013-EDML_for_Learning_Parameters_in_Directed_and_Undirected_Graphical_Models.html">101 nips-2013-EDML for Learning Parameters in Directed and Undirected Graphical Models</a></p>
<p>20 0.67341286 <a title="40-lda-20" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
