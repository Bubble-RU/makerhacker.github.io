<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2013-Mapping paradigm ontologies to and from the brain</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-183" href="#">nips2013-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2013-Mapping paradigm ontologies to and from the brain</h1>
<br/><p>Source: <a title="nips-2013-183-pdf" href="http://papers.nips.cc/paper/5168-mapping-paradigm-ontologies-to-and-from-the-brain.pdf">pdf</a></p><p>Author: Yannick Schwartz, Bertrand Thirion, Gael Varoquaux</p><p>Abstract: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. 1</p><p>Reference: <a title="nips-2013-183-reference" href="../nips2013_reference/nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Mapping cognitive ontologies to and from the brain  Yannick Schwartz, Bertrand Thirion, and Gael Varoquaux Parietal Team, Inria Saclay Ile-de-France Saclay, France firstname. [sent-1, score-0.739]
</p><p>2 fr  Abstract Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. [sent-3, score-0.719]
</p><p>3 To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. [sent-5, score-0.789]
</p><p>4 Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. [sent-6, score-0.306]
</p><p>5 We rely on a large corpus of imaging studies and a predictive engine. [sent-7, score-0.455]
</p><p>6 Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. [sent-8, score-0.229]
</p><p>7 The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. [sent-9, score-0.531]
</p><p>8 To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. [sent-10, score-0.679]
</p><p>9 1  Introduction  Functional brain imaging, in particular fMRI, is the workhorse of brain mapping, the systematic study of which areas of the brain are recruited during various experiments. [sent-12, score-0.993]
</p><p>10 To date, 33K papers on pubmed mention “fMRI”, revealing an accumulation of activation maps related to speciﬁc tasks or cognitive concepts. [sent-13, score-0.794]
</p><p>11 From this literature has emerged the notion of brain modules specialized to a task, such as the celebrated fusiform face area (FFA) dedicated to face recognition [1]. [sent-14, score-0.466]
</p><p>12 However, the link between the brain images and high-level notions from psychology is mostly done manually, due to the lack of co-analysis framework. [sent-15, score-0.35]
</p><p>13 Beyond this lack of speciﬁcity, individual studies are seldom comprehensive, in the sense that they do not recruit every brain region. [sent-19, score-0.535]
</p><p>14 Prior work on such large scale cognitive mapping of the brain has mostly relied on coordinate-based meta-analysis, that forgo activation maps and pool results across publications via the reported Talairach coordinates of activation foci [3, 4]. [sent-20, score-1.208]
</p><p>15 Such large corpuses can be used to evaluate the occurrence of the cognitive and behavioral 1  terms associated with activations and formulate reverse inference as a Bayesian inversion on standard (forward) fMRI inference [2, 4]. [sent-22, score-0.802]
</p><p>16 On the opposite end of the spectrum, [5] shows that using a machine-learning approach on studies with different cognitive content can predict this content from the images, thus demonstrating principled reverse inference across studies. [sent-23, score-0.975]
</p><p>17 Two trends thus appear in the quest for explicit correspondences between brain regions and cognitive concepts. [sent-25, score-0.76]
</p><p>18 One is grounded on counting term frequency on a large corpus of studies described by coordinates. [sent-26, score-0.39]
</p><p>19 Our purpose here is to outline a strategy to accumulate knowledge from a brain functional image database in order to provide grounds for principled bidirectional reasoning from brain activation to behavior and cognition. [sent-29, score-1.01]
</p><p>20 To increase the breadth in co-analysis and scale up from [5], which used only 8 studies with 22 different cognitive concepts, we have to tackle several challenges. [sent-30, score-0.561]
</p><p>21 For this very reason we choose to describe studies with terms that come from a cognitive paradigm ontology instead of a high-level cognitive process one. [sent-32, score-1.106]
</p><p>22 This setting enables not only to span the terms across all the studies, but also to use atypical studies that do not clearly share cognitive processes. [sent-33, score-0.624]
</p><p>23 A second challenge is that of diminishing statistical power with increasing number of cognitive terms under study. [sent-34, score-0.332]
</p><p>24 We perform a brain mapping experiment across these studies, in which we consider both forward and reverse inference. [sent-37, score-0.659]
</p><p>25 In section 2, we introduce our methodology for establishing correspondence between studies and performing forward and reverse inference across them. [sent-40, score-0.679]
</p><p>26 In section 3, we present our data, a corpus of studies and the corresponding paradigm descriptions. [sent-41, score-0.384]
</p><p>27 In section 4 we show empirically that our approach can predict these descriptions in unseen studies, and that it gives promising maps for brain mapping. [sent-42, score-0.459]
</p><p>28 1  Methodology: annotations, statistics and learning Labeling activation maps with common terms across studies  A standard task-based fMRI study results in activation maps per subject that capture the brain response to each experimental condition. [sent-45, score-1.258]
</p><p>29 They are combined to single out responses to high-level cognitive functions in so-called contrast maps, for which the inference is most often performed at the group level, across subjects. [sent-46, score-0.492]
</p><p>30 For example, to highlight computation processes, one might contrast visual calculation with visual sentences, to suppress the effect of the stimulus modality (visual instructions), and the explicit stimulus (reading the numbers). [sent-48, score-0.508]
</p><p>31 When considering a corpus of different studies, ﬁnding correspondences between the effects highlighted by the contrasts can be challenging. [sent-49, score-0.243]
</p><p>32 Indeed, beyond classical localizers, capturing only very wide cognitive domains, each study tends to investigate fairly unique questions, such as syntactic structure in language rather than language in general [8]. [sent-50, score-0.332]
</p><p>33 Indeed, there are important ongoing efforts in cognitive science and neuroscience to organize the scientiﬁc concepts into formal ontologies [9]. [sent-53, score-0.47]
</p><p>34 2  Forward inference: which regions are recruited by tasks containing a given term? [sent-56, score-0.228]
</p><p>35 Armed with the term labels, we can use the standard fMRI analysis framework and ask using a General Linear Model (GLM) across studies for each voxels of the subject-level activation images if it is signiﬁcantly-related to a term in the corpus of images. [sent-57, score-0.823]
</p><p>36 If x ∈ Rp is the observed activation map with p voxels, the GLM tests P(xi = 0|T ) for each voxel i and term T . [sent-58, score-0.299]
</p><p>37 The beneﬁt of the GLM formulation is that it estimates the effect of each term partialing out the effects of the other terms, and thus imposes some form of functional speciﬁcity in the results. [sent-61, score-0.24]
</p><p>38 3  Reverse inference: which regions are predictive of tasks containing a given term? [sent-64, score-0.234]
</p><p>39 Poldrack 2006 [2] formulates reverse inferences as reasoning on P(T |x), the probability of a term T being involved in the experiment given the activation map x. [sent-65, score-0.471]
</p><p>40 Approaches to build a reverse inference framework upon this description have relied on Bayesian inversion to go from P(xi = 0|T ), as output by the GLM, to P(T |xi = 0) [2, 4]. [sent-67, score-0.269]
</p><p>41 Learning voxels-level parameters independently is a limitation as it makes it harder to capture distributed effects, such as large-scale functional networks, that can be better predictors of stimuli class than localized regions [6]. [sent-69, score-0.23]
</p><p>42 The choice of linear models is crucial to our brain-mapping goals, as their decision frontier is fully represented by a brain map1 β ∈ Rp . [sent-73, score-0.306]
</p><p>43 2 Reducing even further down to 2K parcels does not impact the classiﬁcation performance, however the brain maps β are then less spatially resolved. [sent-88, score-0.459]
</p><p>44 1  An image database Studies  We need a large collection of task fMRI datasets to cover the cognitive space. [sent-93, score-0.399]
</p><p>45 The datasets include risk-taking tasks [13, 14], classiﬁcation tasks [15, 16, 17], language tasks [18, 8, 19], stop-signal tasks [20], cueing tasks [21], object recognition tasks [22, 23], functional localizers tasks [24, 25], and ﬁnally a saccades & arithmetic task [26]. [sent-96, score-0.761]
</p><p>46 The database accounts for 486 subjects, 131 activation map types, and 3 826 individual maps, the number of subjects and map types varying across the studies. [sent-97, score-0.508]
</p><p>47 To avoid biases due to heterogeneous data analysis procedures, we re-process from scratch all the studies with the SPM (Statistical Parametric Mapping) software. [sent-98, score-0.229]
</p><p>48 The neuroscience community recognizes the value of such vocabularies and develops ontologies to cover the different aspects of the ﬁeld such as protocols, paradigms, brain regions and cognitive processes. [sent-101, score-0.853]
</p><p>49 Among the many initiatives, CogPO (The Cognitive Paradigm Ontology) [9] aims to represent the cognitive paradigms used in fMRI studies. [sent-102, score-0.399]
</p><p>50 As an example a stimulus modality may be auditory or visual, the explicit stimulus a non-vocal sound or a shape. [sent-105, score-0.493]
</p><p>51 For instance, we only have visual or auditory stimulus modalities. [sent-110, score-0.309]
</p><p>52 While a handful of contrasts display both stimulus modalities, the fact that a stimulus is not auditory mostly amounts to it being visual. [sent-111, score-0.435]
</p><p>53 For this reason, we exclude from our forward inference visual, which will be captured by negative effects on auditory, and digits, that amounts mainly to the instruction being count. [sent-112, score-0.321]
</p><p>54 To evaluate the spatial layout of the different CogPO categories, we report the different term effects as outlines in the brain, and show the 5% top values for each term to avoid clutter in Figure 3. [sent-114, score-0.183]
</p><p>55 Forward inference 4  outlines many regions relevant to the terms, such as the primary visual and auditory systems on the stimulus modality maps, or pattern and object-recognition areas in the ventral stream, on the explicit stimulus maps. [sent-115, score-0.734]
</p><p>56 It can be difﬁcult to impose a functional speciﬁcity in forward inference because of several phenomena: i) the correlation present in the design matrix, makes it hard to separate highly associated (often anti-correlated) factors, as can be seen in Fig. [sent-116, score-0.329]
</p><p>57 This assumption ignores modulations and interactions effects that are very likely to occur; however their joint occurrence is related to the protocol, making it impossible to disentangle these factors with the database used here. [sent-119, score-0.19]
</p><p>58 2  Reverse inference  The promise of predictive modeling on a large statistical map database is to provide principled reverse inference, going from observations of neural activity to well-deﬁned cognitive processes. [sent-123, score-0.814]
</p><p>59 Figure 1 highlights some confounding effects that can captured by a predictive model: two statistical maps originating from the same study are closer than two maps labeled as sharing a same experimental condition in the sense of a Euclidean distance. [sent-125, score-0.517]
</p><p>60 Second, we only test the classiﬁers on previously unseen studies and if possible subjects, using for example a leave-one-study out cross validation scheme. [sent-129, score-0.331]
</p><p>61 We compare this approach to training independent predictive models for each term and use three types of classiﬁers: a naive Bayes, a logistic regression, and a weighted logistic regression. [sent-136, score-0.24]
</p><p>62 It conﬁrms the idea outlined in Figure 1, that an Euclidean distance alone is not appropriate to discriminate underlying brain functions because of overwhelming confounding effects4 . [sent-147, score-0.408]
</p><p>63 On the contrary, the methods using a logistic regression show better results, and yield performance scores above the chance levels which are represented by the red horizontal bars for the leave-onestudy out cross validation scheme in Figure 2. [sent-149, score-0.243]
</p><p>64 5  Nb of m a ps  All Sa m e la be l Sa m e s tudy Sa m e c ontra s t  0  Dis ta nc e be twe e n two m a ps  Figure 1: (Left) Histogram of the distance between maps owing to their commonalities: study of origin, functional labels, functional contrast. [sent-157, score-0.381]
</p><p>65 We evaluate the spatial layout of maps representing CogPO categories for reverse inference as well, and report boundaries of the 5% top values from the weighted logistic coefﬁcients. [sent-160, score-0.524]
</p><p>66 Figure 3 reports the outlined regions that include motor cortex activations in the instructions category, and activations in the auditory cortex and FFA respectively for the words and faces terms in the explicit stimulus category. [sent-161, score-0.732]
</p><p>67 Despite being very noisy, those regions report ﬁndings consistent with the literature and complementary to the forward inference maps. [sent-162, score-0.292]
</p><p>68 For instance, the move instruction map comprises the motor cortex, unlike for forward inference. [sent-163, score-0.262]
</p><p>69 Similarly, the saccades over response map segments the intra-parietal sulci and the frontal eye ﬁelds, which corresponds to the well known signature of saccades, unlike the corresponding forward inference map, which is very non speciﬁc of saccades5 . [sent-164, score-0.355]
</p><p>70 5  Discussion and conclusion  Linking cognitive concepts to brain maps can give solid grounds to the diffuse knowledge derived in imaging neuroscience. [sent-165, score-0.872]
</p><p>71 Common studies provide evidence on which brain regions are recruited in given tasks. [sent-166, score-0.687]
</p><p>72 However coming to conclusions on the tasks in which regions are specialized requires data accumulation across studies to overcome the small coverage in cognitive domain of the tasks assessed in a single study. [sent-167, score-0.909]
</p><p>73 Indeed, ﬁnding correspondence between studies is a key step to going beyond idiosyncrasies of the experimental designs. [sent-171, score-0.229]
</p><p>74 Yet the framework should not discard rare but repeatable features of the experiments as these provide richness to the description of brain function. [sent-172, score-0.362]
</p><p>75 Previous work [5] showed high classiﬁcation scores for several mental states across multiple studies, using cross-validation with a leave-one-subject out strategy. [sent-176, score-0.199]
</p><p>76 As ﬁgure 1 shows, predicting studies is much easier albeit of little neuroscientiﬁc interest. [sent-179, score-0.229]
</p><p>77 Interestingly, [5] also explores the ability of a model to be predictive on two different studies sharing the same cognitive task, and a few subjects. [sent-180, score-0.642]
</p><p>78 When using the common subjects, their model performs worse than without these subjects, as it partially mistakes cognitive 5  This failure of forward inference is probably due to the small sample size of saccades. [sent-181, score-0.547]
</p><p>79 To avoid this loophole, we included in our corpus only studies that had terms in common with at least on other study and performed cross-validation by leaving a study out, and thus predicting from completely new activation maps. [sent-187, score-0.51]
</p><p>80 Our labeled corpus is riddled with very infrequent terms giving rise to class imbalance problems in which the rare occurrences are the most difﬁcult to model. [sent-190, score-0.262]
</p><p>81 Interestingly, though coordinates databases such as Neurosynth [4] cover a larger set of studies and a broader range of cognitive processes, they suffer from a similar imbalance bias, which is given by the state of the literature. [sent-191, score-0.625]
</p><p>82 For instance, the reverse inference map corresponding to the term digits is empty, whereas the forward inference map is well deﬁned 6 . [sent-194, score-0.671]
</p><p>83 Neurosynth draws from almost 5K studies while our work is based on 19 studies; however, unlike Neurosynth, we are able to beneﬁt from the different contrasts and subjects in our studies, which provides us with 3 826 training samples. [sent-195, score-0.37]
</p><p>84 This paper shows the ﬁrst demonstration of zero-shot learning for prediction of tasks from brain activity: paradigm description is given for images from unseen studies, acquired on different scanners, in different institutions, on different cognitive domains. [sent-197, score-0.809]
</p><p>85 To minimize clutter, we set the outline so as to encompass 5% of the voxels in the brain on each ﬁgure, thus highlighting only the salient features of the maps. [sent-201, score-0.398]
</p><p>86 In reverse inference, to reduce the visual effect of the parcellation, maps were smoothed using a σ of 2 voxels. [sent-202, score-0.392]
</p><p>87 accumulation, combined with the predictive model can provide good proxies of reverse inference maps, giving regions whose activation supports certain cognitive functions. [sent-203, score-0.936]
</p><p>88 These maps should, in principle, be better suited for causal interpretation than maps estimated from standard brain mapping correlational analysis. [sent-204, score-0.658]
</p><p>89 In future work, we plan to control the signiﬁcance of the reverse inference maps, that show promising results but would probably beneﬁt from thresholding out non-signiﬁcant regions. [sent-205, score-0.269]
</p><p>90 In addition, we hope that further progress, in terms of spatial and cognitive resolution in mapping the brain to cognitive ontologies, will come from enriching the database with new studies, that will bring more images, and new low and high-level concepts. [sent-206, score-1.037]
</p><p>91 Chun, “The fusiform face area: a module in human extrastriate cortex specialized for face perception. [sent-212, score-0.235]
</p><p>92 Poldrack, “Can cognitive processes be inferred from neuroimaging data? [sent-217, score-0.377]
</p><p>93 Hanson, “Decoding the large-scale structure of brain function by classifying mental states across individuals,” Psychological Science, vol. [sent-239, score-0.465]
</p><p>94 Halchenko, “Brain reading using full brain support vector machines for object recognition: there is no face identiﬁcation area,” Neural Computation, vol. [sent-244, score-0.366]
</p><p>95 Laird, “The cognitive paradigm ontology: design and application,” Neuroinformatics, vol. [sent-266, score-0.383]
</p><p>96 Thirion, “A supervised clustering approach for fMRI-based inference of brain states,” Pattern Recognition, vol. [sent-275, score-0.403]
</p><p>97 Shimodaira, “Improving predictive inference under covariate shift by weighting the log-likelihood function,” Journal of statistical planning and inference, vol. [sent-279, score-0.178]
</p><p>98 Milham, “Competition between functional brain networks mediates behavioral variability,” Neuroimage, vol. [sent-353, score-0.42]
</p><p>99 Dehaene, “Fast reproducible identiﬁcation and large-scale databasing of individual functional cognitive networks,” BMC neuroscience, vol. [sent-382, score-0.446]
</p><p>100 Dehaene, “Genetic and environmental contributions to brain activation during calculation,” NeuroImage, vol. [sent-387, score-0.483]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cognitive', 0.332), ('brain', 0.306), ('poldrack', 0.243), ('studies', 0.229), ('activation', 0.177), ('reverse', 0.172), ('ontology', 0.162), ('maps', 0.153), ('stimulus', 0.123), ('auditory', 0.119), ('forward', 0.118), ('cogpo', 0.115), ('dehaene', 0.115), ('neurosynth', 0.115), ('functional', 0.114), ('fmri', 0.111), ('corpus', 0.104), ('ontologies', 0.101), ('inference', 0.097), ('mental', 0.096), ('voxels', 0.092), ('modality', 0.083), ('predictive', 0.081), ('regions', 0.077), ('tasks', 0.076), ('cortex', 0.075), ('saccades', 0.075), ('recruited', 0.075), ('thirion', 0.075), ('instructions', 0.074), ('glm', 0.072), ('city', 0.071), ('subjects', 0.071), ('contrasts', 0.07), ('ffa', 0.069), ('openfmri', 0.069), ('effects', 0.069), ('neuroimage', 0.068), ('visual', 0.067), ('database', 0.067), ('paradigms', 0.067), ('map', 0.065), ('imbalance', 0.064), ('across', 0.063), ('neurosci', 0.061), ('confounding', 0.061), ('face', 0.06), ('term', 0.057), ('rare', 0.056), ('accumulation', 0.056), ('occurrence', 0.054), ('cross', 0.054), ('wager', 0.053), ('logistic', 0.051), ('paradigm', 0.051), ('categories', 0.051), ('classi', 0.05), ('activations', 0.05), ('chance', 0.05), ('validation', 0.048), ('aron', 0.046), ('brainmap', 0.046), ('correlational', 0.046), ('devlin', 0.046), ('gluck', 0.046), ('halchenko', 0.046), ('milham', 0.046), ('pallier', 0.046), ('pinel', 0.046), ('trepel', 0.046), ('explicit', 0.045), ('neuroimaging', 0.045), ('images', 0.044), ('faces', 0.044), ('fox', 0.042), ('comprises', 0.042), ('content', 0.041), ('discriminate', 0.041), ('imaging', 0.041), ('inhibit', 0.04), ('localizers', 0.04), ('substrates', 0.04), ('acad', 0.04), ('fusiform', 0.04), ('natl', 0.04), ('scanners', 0.04), ('attend', 0.04), ('grounds', 0.04), ('scores', 0.04), ('stimuli', 0.039), ('occurrences', 0.038), ('hanson', 0.037), ('hindered', 0.037), ('saclay', 0.037), ('salience', 0.037), ('commonalities', 0.037), ('instruction', 0.037), ('neuroinformatics', 0.037), ('sa', 0.037), ('neuroscience', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="183-tfidf-1" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>Author: Yannick Schwartz, Bertrand Thirion, Gael Varoquaux</p><p>Abstract: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. 1</p><p>2 0.14260595 <a title="183-tfidf-2" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>3 0.12141729 <a title="183-tfidf-3" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>4 0.10691844 <a title="183-tfidf-4" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vamsi Ithapu, Qinyuan Sun, Sterling C. Johnson, Vikas Singh</p><p>Abstract: Multiple hypothesis testing is a signiﬁcant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given α-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub–sampled — on the order of 0.5% — matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacriﬁcing the ﬁdelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly 50× can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated α-threshold is also recovered faithfully, and is stable. 1</p><p>5 0.097295523 <a title="183-tfidf-5" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>Author: Qianli Liao, Joel Z. Leibo, Tomaso Poggio</p><p>Abstract: One approach to computer object recognition and modeling the brain’s ventral stream involves unsupervised learning of representations that are invariant to common transformations. However, applications of these ideas have usually been limited to 2D afﬁne transformations, e.g., translation and scaling, since they are easiest to solve via convolution. In accord with a recent theory of transformationinvariance [1], we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identitypreserving transformations. The model’s wiring can be learned from videos of transforming objects—or any other grouping of images into sets by their depicted object. Through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations. First, we empirically conﬁrm theoretical predictions (from [1]) for the case of 2D afﬁne transformations. Next, we apply the model to non-afﬁne transformations; as expected, it performs well on face veriﬁcation tasks requiring invariance to the relatively smooth transformations of 3D rotation-in-depth and changes in illumination direction. Surprisingly, it can also tolerate clutter “transformations” which map an image of a face on one background to an image of the same face on a different background. Motivated by these empirical ﬁndings, we tested the same model on face veriﬁcation benchmark tasks from the computer vision literature: Labeled Faces in the Wild, PubFig [2, 3, 4] and a new dataset we gathered—achieving strong performance in these highly unconstrained cases as well. 1</p><p>6 0.095322393 <a title="183-tfidf-6" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>7 0.092732966 <a title="183-tfidf-7" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>8 0.087695912 <a title="183-tfidf-8" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>9 0.085988507 <a title="183-tfidf-9" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>10 0.083982691 <a title="183-tfidf-10" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>11 0.083656631 <a title="183-tfidf-11" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>12 0.082515635 <a title="183-tfidf-12" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>13 0.078858607 <a title="183-tfidf-13" href="./nips-2013-Spectral_methods_for_neural_characterization_using_generalized_quadratic_models.html">305 nips-2013-Spectral methods for neural characterization using generalized quadratic models</a></p>
<p>14 0.077562377 <a title="183-tfidf-14" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>15 0.075725675 <a title="183-tfidf-15" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>16 0.071773276 <a title="183-tfidf-16" href="./nips-2013-On_Flat_versus_Hierarchical_Classification_in_Large-Scale_Taxonomies.html">216 nips-2013-On Flat versus Hierarchical Classification in Large-Scale Taxonomies</a></p>
<p>17 0.071052164 <a title="183-tfidf-17" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>18 0.070391297 <a title="183-tfidf-18" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>19 0.06577605 <a title="183-tfidf-19" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>20 0.065262109 <a title="183-tfidf-20" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.077), (2, -0.133), (3, -0.073), (4, -0.007), (5, -0.053), (6, -0.035), (7, -0.026), (8, -0.058), (9, 0.073), (10, -0.071), (11, -0.017), (12, -0.033), (13, -0.053), (14, -0.095), (15, -0.036), (16, -0.05), (17, -0.069), (18, -0.037), (19, -0.013), (20, 0.008), (21, -0.014), (22, -0.028), (23, 0.007), (24, -0.028), (25, 0.04), (26, 0.036), (27, 0.038), (28, -0.093), (29, 0.025), (30, -0.04), (31, -0.054), (32, -0.051), (33, -0.058), (34, 0.02), (35, -0.124), (36, -0.031), (37, -0.039), (38, 0.028), (39, 0.06), (40, -0.086), (41, 0.034), (42, -0.095), (43, 0.067), (44, -0.004), (45, 0.043), (46, 0.059), (47, -0.078), (48, -0.014), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9562642 <a title="183-lsi-1" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>Author: Yannick Schwartz, Bertrand Thirion, Gael Varoquaux</p><p>Abstract: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. 1</p><p>2 0.82332951 <a title="183-lsi-2" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>Author: Daniel L. Yamins, Ha Hong, Charles Cadieu, James J. DiCarlo</p><p>Abstract: Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition. 1</p><p>3 0.70683783 <a title="183-lsi-3" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>Author: Yangqing Jia, Joshua T. Abbott, Joseph Austerweil, Thomas Griffiths, Trevor Darrell</p><p>Abstract: Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to ﬁnd the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly recognized. We present an algorithm for learning visual concepts directly from images, using probabilistic predictions generated by visual classiﬁers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children. We compare the performance of our system to several baseline algorithms, and show a signiﬁcant advantage results from combining visual classiﬁers with the ability to identify an appropriate level of abstraction using Bayesian generalization. 1</p><p>4 0.6785202 <a title="183-lsi-4" href="./nips-2013-One-shot_learning_by_inverting_a_compositional_causal_process.html">226 nips-2013-One-shot learning by inverting a compositional causal process</a></p>
<p>Author: Brenden M. Lake, Ruslan Salakhutdinov, Josh Tenenbaum</p><p>Abstract: People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classiﬁcation task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also tested the model on another conceptual task, generating new examples, by using a “visual Turing test” to show that our model produces human-like performance. 1</p><p>5 0.67637581 <a title="183-lsi-5" href="./nips-2013-Optimal_integration_of_visual_speed_across_different_spatiotemporal_frequency_channels.html">237 nips-2013-Optimal integration of visual speed across different spatiotemporal frequency channels</a></p>
<p>Author: Matjaz Jogan, Alan Stocker</p><p>Abstract: How do humans perceive the speed of a coherent motion stimulus that contains motion energy in multiple spatiotemporal frequency bands? Here we tested the idea that perceived speed is the result of an integration process that optimally combines speed information across independent spatiotemporal frequency channels. We formalized this hypothesis with a Bayesian observer model that combines the likelihood functions provided by the individual channel responses (cues). We experimentally validated the model with a 2AFC speed discrimination experiment that measured subjects’ perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings. We found that the perceived speeds of the combined stimuli are independent of the relative phase of the underlying grating components. The results also show that the discrimination thresholds are smaller for the combined stimuli than for the individual grating components, supporting the cue combination hypothesis. The proposed Bayesian model ﬁts the data well, accounting for the full psychometric functions of both simple and combined stimuli. Fits are improved if we assume that the channel responses are subject to divisive normalization. Our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for coherent motion stimuli of arbitrary spatial structure. 1</p><p>6 0.66177195 <a title="183-lsi-6" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>7 0.63707602 <a title="183-lsi-7" href="./nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</a></p>
<p>8 0.63545775 <a title="183-lsi-8" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>9 0.61058134 <a title="183-lsi-9" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>10 0.57984942 <a title="183-lsi-10" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>11 0.5446375 <a title="183-lsi-11" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>12 0.5264405 <a title="183-lsi-12" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>13 0.52106661 <a title="183-lsi-13" href="./nips-2013-Linear_decision_rule_as_aspiration_for_simple_decision_heuristics.html">176 nips-2013-Linear decision rule as aspiration for simple decision heuristics</a></p>
<p>14 0.51768214 <a title="183-lsi-14" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>15 0.50541562 <a title="183-lsi-15" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>16 0.5049206 <a title="183-lsi-16" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>17 0.50053793 <a title="183-lsi-17" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>18 0.4950853 <a title="183-lsi-18" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>19 0.48864391 <a title="183-lsi-19" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>20 0.48093152 <a title="183-lsi-20" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (16, 0.029), (33, 0.156), (34, 0.139), (41, 0.017), (49, 0.078), (56, 0.076), (70, 0.05), (75, 0.01), (85, 0.027), (89, 0.039), (93, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95531839 <a title="183-lda-1" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>Author: Christian Albers, Maren Westkott, Klaus Pawelzik</p><p>Abstract: Recent extensions of the Perceptron as the Tempotron and the Chronotron suggest that this theoretical concept is highly relevant for understanding networks of spiking neurons in the brain. It is not known, however, how the computational power of the Perceptron might be accomplished by the plasticity mechanisms of real synapses. Here we prove that spike-timing-dependent plasticity having an anti-Hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of Hebbian shape for inhibitory synapses are sufﬁcient for realizing the original Perceptron Learning Rule if these respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons. We also show that with these simple yet biologically realistic dynamics Tempotrons and Chronotrons are learned. The proposed mechanism enables incremental associative learning from a continuous stream of patterns and might therefore underly the acquisition of long term memories in cortex. Our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</p><p>same-paper 2 0.87375653 <a title="183-lda-2" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>Author: Yannick Schwartz, Bertrand Thirion, Gael Varoquaux</p><p>Abstract: Imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. Due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. To come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. Here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. We rely on a large corpus of imaging studies and a predictive engine. Technically, the challenges are to ﬁnd commonality between the studies without denaturing the richness of the corpus. The key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. To our knowledge, our approach is the ﬁrst demonstration of predicting the cognitive content of completely new brain images. To that end, we propose a method that predicts the experimental paradigms across different studies. 1</p><p>3 0.82731962 <a title="183-lda-3" href="./nips-2013-Learning_Stochastic_Inverses.html">161 nips-2013-Learning Stochastic Inverses</a></p>
<p>Author: Andreas Stuhlmüller, Jacob Taylor, Noah Goodman</p><p>Abstract: We describe a class of algorithms for amortized inference in Bayesian networks. In this setting, we invest computation upfront to support rapid online inference for a wide range of queries. Our approach is based on learning an inverse factorization of a model’s joint distribution: a factorization that turns observations into root nodes. Our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization. These stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly ﬁnd a likely explanation. We show that estimated inverses converge asymptotically in number of (prior or posterior) training samples. To make use of inverses before convergence, we describe the Inverse MCMC algorithm, which uses stochastic inverses to make block proposals for a Metropolis-Hastings sampler. We explore the efﬁciency of this sampler for a variety of parameter regimes and Bayes nets. 1</p><p>4 0.78377259 <a title="183-lda-4" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>5 0.77647024 <a title="183-lda-5" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>6 0.77187735 <a title="183-lda-6" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>7 0.77111852 <a title="183-lda-7" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>8 0.7689504 <a title="183-lda-8" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>9 0.7675736 <a title="183-lda-9" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>10 0.76729578 <a title="183-lda-10" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>11 0.76690924 <a title="183-lda-11" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>12 0.76554674 <a title="183-lda-12" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>13 0.76426113 <a title="183-lda-13" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>14 0.76203912 <a title="183-lda-14" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>15 0.76171398 <a title="183-lda-15" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>16 0.76098967 <a title="183-lda-16" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>17 0.75927353 <a title="183-lda-17" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>18 0.7578811 <a title="183-lda-18" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>19 0.75690365 <a title="183-lda-19" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>20 0.75630289 <a title="183-lda-20" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
