<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>300 nips-2013-Solving the multi-way matching problem by permutation synchronization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-300" href="#">nips2013-300</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>300 nips-2013-Solving the multi-way matching problem by permutation synchronization</h1>
<br/><p>Source: <a title="nips-2013-300-pdf" href="http://papers.nips.cc/paper/4987-solving-the-multi-way-matching-problem-by-permutation-synchronization.pdf">pdf</a></p><p>Author: Deepti Pachauri, Risi Kondor, Vikas Singh</p><p>Abstract: The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including ﬁnding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, Permutation Synchronization, which ﬁnds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efﬁcient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods. 1</p><p>Reference: <a title="nips-2013-300-reference" href="../nips2013_reference/nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Solving the multi-way matching problem by permutation synchronization Deepti Pachauri,† Risi Kondor§ and Vikas Singh‡† Dept. [sent-1, score-0.925]
</p><p>2 edu †  Abstract The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including ﬁnding the correspondence between feature points across multiple images in computer vision. [sent-10, score-0.409]
</p><p>3 At present it is usually solved by matching the sets pairwise, in series. [sent-11, score-0.217]
</p><p>4 In contrast, we propose a new method, Permutation Synchronization, which ﬁnds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. [sent-12, score-0.291]
</p><p>5 1  Introduction  Finding the correct bijection between two sets of objects X = {x1 , x2 , . [sent-14, score-0.119]
</p><p>6 In this paper, we consider its generalization to matching not just two, but m different sets X1 , X2 , . [sent-21, score-0.217]
</p><p>7 However, our approach is fully general and equally applicable to problems such as matching multiple graphs [10, 11]. [sent-26, score-0.217]
</p><p>8 Presently, multi-matching is usually solved sequentially, by ﬁrst ﬁnding a putative permutation τ12 matching X1 to X2 , then a permutation τ23 matching X2 to X3 , and so on, up to τm−1,m . [sent-27, score-1.186]
</p><p>9 While one can conceive of various strategies for optimizing this process, the fact remains that when the data are noisy, a single error in the sequence will typically create a large number of erroneous pairwise matches [12, 13, 14]. [sent-28, score-0.19]
</p><p>10 For consistency, the recovered matchings must satisfy τkj τji = τki . [sent-30, score-0.205]
</p><p>11 While ﬁnding an optimal matrix of permutations satisfying these relations is, in general, combinatorially hard, we show that for the most natural choice of loss function the problem has a natural relaxation to just ﬁnding the n leading eigenvectors of the cost matrix. [sent-31, score-0.425]
</p><p>12 In addition to vastly reducing the computational cost, using recent results from random ( ) theory, we show that the eigenvectors are very effective at aggregating matrix information from all m pairwise matches, and therefore make the algorithm surprisingly robust to 2 noise. [sent-32, score-0.302]
</p><p>13 Our experiments show that in landmark matching problems Permutation Synchronization can recover the correct correspondence between landmarks across a large number of images with small error, even when a signiﬁcant fraction of the pairwise matches are incorrect. [sent-33, score-0.943]
</p><p>14 on a similar problem involving ﬁnding the right rotations (rather than matchings) between electron microscopic 1  images [15][16][17]. [sent-35, score-0.088]
</p><p>15 However, independently of, and concurrently with the present work, Huang and Guibas [18] have recently proposed a semideﬁnite programming based solution, which parallels our approach, and in problems involving occlusion might perform even better. [sent-37, score-0.085]
</p><p>16 2  Synchronizing permutations  Consider a collection of m sets X1 , X2 , . [sent-38, score-0.171]
</p><p>17 , xi }, such n 1 2 i that for each pair (Xi , Xj ), each xp in Xi has a natural counterpart xj in Xj . [sent-44, score-0.233]
</p><p>18 For example, in q computer vision, given m images of the same scene taken from different viewpoints, xi , xi , . [sent-45, score-0.196]
</p><p>19 , xi n 1 2 might be n visual landmarks detected in image i, while xj , xj , . [sent-48, score-0.467]
</p><p>20 , xj are n landmarks detected in n 1 2 i i image j, in which case xp ∼ xj signiﬁes that xp and xj correspond to the same physical feature. [sent-51, score-0.735]
</p><p>21 q q i Since the correspondence between Xi and Xj is a bijection, one can write it as xp ∼ xjτji(p) for some permutation τji : {1, 2, . [sent-52, score-0.521]
</p><p>22 , Xm is consistent if xp ∼ xj and q j k i k xq ∼ x r together imply that xp ∼ x r . [sent-67, score-0.306]
</p><p>23 In terms of permutations this is equivalent to requiring that the array (τij )m satisfy i,j=1 τkj τji = τki ∀i, j, k. [sent-68, score-0.222]
</p><p>24 , xn , we can think of each Xi as realizing its own permutation σi (in the sense of xℓ ∼ xi i(ℓ) ), and then τji becomes σ −1 τji = σj σi . [sent-72, score-0.414]
</p><p>25 Rather, in a typical application we have some tentative (noisy) τji matchings which we ˜ must synchronize into the form (2) by ﬁnding the underlying σ1 , . [sent-79, score-0.205]
</p><p>26 In this paper we limit ourselves to the simplest choice d(σ, τ ) = n − ⟨P (σ), P (τ )⟩ , where P (σ) ∈ R  (4)  n×n  are the usual permutation matrices { 1 if σ(p) = q [P (σ)]q,p := 0 otherwise, ∑n and ⟨A, B⟩ is the matrix inner product ⟨A, B⟩ := tr(A⊤ B) = p,q=1 Ap,q Bp,q . [sent-88, score-0.45]
</p><p>27 Intuitively, each Tji is an objective i matrix, the (q, p) element of which captures the utility of matching xp in Xi to xj in Xj . [sent-97, score-0.412]
</p><p>28 This q i generalization is very useful when the assignments of the different xp ’s have different conﬁdences. [sent-98, score-0.111]
</p><p>29 For example, in the landmark matching case, if, due to occlusion or for some other reason, the i counterpart of xp is not present in Xj , then we can simply set [Tji ]q,p = 0 for all q. [sent-99, score-0.562]
</p><p>30 1  Representations and eigenvectors  The generalized Permutation Synchronization problem (5) can also be written as maximize ⟨P, T ⟩ ,  (6)  σ1 ,σ2 ,. [sent-101, score-0.143]
</p><p>31 Tmm  (7)  A matrix valued function ρ : Sn → Cd×d is said to be a representation of the symmetric group if ρ(σ2 ) ρ(σ1 ) = ρ(σ2 σ1 ) for any pair of permutations σ1 , σ2 ∈ Sn . [sent-135, score-0.26]
</p><p>32 The synchronization matrix P is of rank n and is of the form P = U · U ⊤ , where   P (σ1 )  . [sent-140, score-0.376]
</p><p>33 m [P (σm )]ℓ are mutually orthogonal unit eigenvectors of P with the same eigenvalue m, and together span the row/column space of P. [sent-170, score-0.235]
</p><p>34 The columns of U are orthogonal because the columns of each constituent P (σi ) are orthogonal. [sent-172, score-0.122]
</p><p>35 However, Proposition 1 and its corollary suggest relaxing it to maximize ⟨P, T ⟩ , n P∈Mm  (10)  where Mm is the set of mn–dimensional rank n symmetric matrices whose non-zero eigenvalues n are m. [sent-177, score-0.127]
</p><p>36 , vℓ are the n leading normalized eigenvectors of T . [sent-181, score-0.209]
</p><p>37 | Thus, in contrast to the original combinatorial problem, (10) can be solved by just ﬁnding the m leading eigenvectors of T . [sent-192, score-0.179]
</p><p>38 Of course, from P we must still recover the inAlgorithm 1 Permutation Synchronization dividual permutations σ1 , σ2 , . [sent-193, score-0.204]
</p><p>39 However, as long as P is relatively close in form Input: the objective matrix T Compute the n leading eigenvectors (v1 , v2 , . [sent-197, score-0.223]
</p><p>40 , vn ) (7), this is quite a simple and stable process. [sent-200, score-0.105]
</p><p>41 , when Tji = P (˜ji ) for some array (˜ji )j,i of permutations that alτ τ ready satisfy the consistency relations (1), T will have precisely the same structure as described by Proposition 1 for P. [sent-209, score-0.261]
</p><p>42 In particular, it will have n mutually orthogonal eigenvectors   [P (˜1 )]ℓ σ 1   . [sent-210, score-0.191]
</p><p>43 Due to the n–fold degeneracy, however, the matrix of eigenvectors (12) is only deﬁned up to multiplication by an arbitrary rotation matrix O on the right, which means that instead of the “correct” U (whose columns are (13)), the eigenvector decomposition of T may return any U ′ = U O. [sent-217, score-0.358]
</p><p>44 Fortunately, when forming the product P = U′ · U′  ⊤  = U O O⊤ U ⊤ = U · U ⊤  this rotation cancels, conﬁrming that our algorithm recovers P = T , and hence the matchings τji = τji , with no error. [sent-218, score-0.24]
</p><p>45 ˜ Of course, rather than the case when the solution is handed to us from the start, we are more interested in how the algorithm performs in situations when either the Tji blocks are not permutation matrices, or they are not synchronized. [sent-219, score-0.376]
</p><p>46 To this end, we set T = T0 + N ,  (14)  where T0 is the correct “ground truth” synchronization matrix, while N is a symmetric perturbation matrix with entries drawn independently from a zero-mean normal distribution with variance η 2 . [sent-220, score-0.508]
</p><p>47 In general, to ﬁnd the permutation best aligned with a given n × n matrix T , the Kuhn–Munkres algorithm solves for τ = arg maxτ ∈Sn ⟨P (τ ), T ⟩ = arg maxτ ∈Sn (vec(P (τ )) · vec(T )). [sent-221, score-0.42]
</p><p>48 85} is replaced by a random permutation (m = 100, n = 30). [sent-226, score-0.376]
</p><p>49 writing T = P (τ0 ) + ϵ, where P (τ0 ) is the “ground truth”, while ϵ is an error term, it is guaranteed to return the correct permutation as long as ∥ vec(ϵ) ∥ < ′ min ∥ vec(τ0 ) − vec(τ ′ ) ∥ /2. [sent-229, score-0.431]
</p><p>50 , the permutation that swaps 1 with 2 and leaves 3, 4, . [sent-236, score-0.376]
</p><p>51 The corresponding permutation matrix differs from the idenity in exactly 4 entries, therefore a sufﬁcient condition for correct reconstruction is that √ ∥ϵ∥Frob = ⟨ϵ, ϵ⟩1/2 = ∥vec(ϵ)∥ < 1 4 = 1. [sent-240, score-0.475]
</p><p>52 As n grows, ∥ϵ∥Frob becomes tightly concentrated 2 around ηn, so the condition for recovering the correct permutation is η < 1/n. [sent-241, score-0.431]
</p><p>53 Permutation Synchronization can achieve a lower error, especially in the large m regime, because the eigenvectors aggregate information from all the Tji matrices, and tend to be very stable to perturbations. [sent-242, score-0.143]
</p><p>54 As long as the largest eigenvalue of the random matrix N falls below a given multiple of the smallest non-zero eigenvalue of T0 , adding N will have very little effect on the eigenvectors of T . [sent-244, score-0.275]
</p><p>55 If N is a symmetric matrix with independent N (0, η 2 ) entries, as nm → ∞, its spectrum will tend to Wigner’s famous semicircle distribution supported on the interval (−2η(nm)1/2 , 2η(nm)1/2 ), and with probability one the largest eigenvalue will approach 2η(nm)1/2 [20, 21]. [sent-246, score-0.218]
</p><p>56 In contrast, the nonzero eigenvalues of T0 scale with m, which guarantees that for large enough m the two spectra will be nicely separated and Permutation Synchronization will have very low error. [sent-247, score-0.086]
</p><p>57 max min Below this limit, to quantify the actual expected error, we write each leading normalized eigenvector ∗ ⊥ ∗ v1 , v2 , . [sent-251, score-0.121]
</p><p>58 , vn of T as vi = vi + vi , where vi is the projection of vi to the space U0 spanned by the 0 0 0 non-zero eigenvectors v1 , v2 , . [sent-254, score-0.693]
</p><p>59 ⊥ ⊥ ∗ ∗ ⊥ ⊥ It is easy to see that ⟨vi , vj ⟩ − → 0, which implies ⟨vi , vj ⟩ = ⟨vi , vj ⟩ − ⟨vi , vj ⟩ − → 0, − − ∗ ∗ so, setting λ = (1 − η 2 n/m)−1/2 , the normalized vectors λv1 , . [sent-268, score-0.194]
</p><p>60 In terms of the individual Pji blocks of P = U U ⊤ , neglecting second order terms, 0 0 ⊤ Pji = (Uj + λEj )(Ui0 + λEi )⊤ ≈ P (τji ) + λUj Ei + λEj Ui0⊤ ,  where τji is the ground truth matching and Ui0 and Ei denote the appropriate n × n submatrices of U 0 and E. [sent-287, score-0.301]
</p><p>61 1 + 4(m/n)−1 This is much better than our η < 1/n result for the naive algorithm, and remarkably only slightly stricter than the condition η < (m/n)1/2 for recovering the eigenvectors with any accuracy at all. [sent-290, score-0.143]
</p><p>62 The baseline method is to compute (˜ji )m τ i,j=1 by solving m inde2 pendent linear assignment problems based on matching landmarks by their shape context features [23]. [sent-297, score-0.562]
</p><p>63 Our method takes the same pairwise matches and synchronizes them with the eigenvector based procedure. [sent-298, score-0.245]
</p><p>64 Figure 3 shows that this clearly outperforms the baseline, which tends to degrade progressively as the number of images increases. [sent-299, score-0.088]
</p><p>65 This is due to the fact that the appearance (or descriptors) of keypoints differ considerably for large offset pairs (which is likely when the image set is large), leading to many false matches. [sent-300, score-0.193]
</p><p>66 While simple, this experiment demonstrates the utility of Permutation Synchronization for multi-view stereo matching, showing that instead of heuristically propagating local pairwise matches, it can ﬁnd a much more accurate globally consistent matching at little additional cost. [sent-302, score-0.475]
</p><p>67 (Green circles) landmark points, (green lines) ground truth matchings, (red lines) found matches. [sent-311, score-0.233]
</p><p>68 Next, we considered a dataset with severe geometric ambiguities due to repetitive structures. [sent-315, score-0.099]
</p><p>69 There is some consensus in the community that even sophisticated features (like SIFT) yield unsatisfactory results in this scenario, and deriving a good initial matching for structure from motion is problematic (see [24] and references therein). [sent-316, score-0.217]
</p><p>70 Our evaluations included 16 images from the Building dataset [24]. [sent-317, score-0.088]
</p><p>71 We identiﬁed 25 “similar looking” landmark points in the scene, and hand annotated them across all images. [sent-318, score-0.186]
</p><p>72 Many landmarks were occluded due to the camera angle. [sent-319, score-0.23]
</p><p>73 Qualitative results for pairwise matching and Permutation Synchronization are shown in Fig 4 (top). [sent-320, score-0.332]
</p><p>74 First, our method resolved geometrical ambiguities by enforcing mutual consistency efﬁciently. [sent-322, score-0.13]
</p><p>75 In contrast, pairwise matching struggles with occlusion in the presence of similar looking landmarks (and feature descriptors). [sent-324, score-0.594]
</p><p>76 The Books dataset (Fig 4, bottom) contains m = 20 images of multiple books on a “L” shaped study table [24], and suffers geometrical ambiguities similar to the above with severe occlusion. [sent-329, score-0.226]
</p><p>77 Here we identiﬁed n = 34 landmark points, many of which were occluded in most images. [sent-330, score-0.202]
</p><p>78 Our ﬁnal experiment deals with matching problems where keypoints in each image preserve a common structure. [sent-335, score-0.374]
</p><p>79 In the literature, this is usually tackled as a graph matching problem, with the keypoints deﬁning the vertices, and their structural relationships being encoded by the edges of the graph. [sent-336, score-0.33]
</p><p>80 Ideally, one wants to solve the problem for all images at once but most practical solutions operate on image (or graph) pairs. [sent-337, score-0.172]
</p><p>81 In contrast, in keypoint matching, the background is not controlled and even sophisticated descriptors may go wrong. [sent-340, score-0.099]
</p><p>82 Recent solutions often leverage supervision to make the problem tractable [25, 26]. [sent-341, score-0.142]
</p><p>83 Instead of learning parameters [25, 27], we utilize supervision directly to provide the correct matches on a small subset of randomly picked image pairs (e. [sent-342, score-0.356]
</p><p>84 For our experiments, we used the baseline method output to set up our objective matrix T but with a ﬁxed “supervision probability”, we replaced the Tji block by the correct permutation matrix, and ran Permutation Synchronization. [sent-346, score-0.524]
</p><p>85 We considered the “Bikes” sub-class from the Caltech 256 dataset, which contains multiple images of common objects with varying backdrops, and chose to match images in the “touring bike” class. [sent-347, score-0.209]
</p><p>86 Our analysis included 28 out of 110 images in this dataset that were taken “side-on”. [sent-348, score-0.088]
</p><p>87 SUSAN corner detector was used to identify landmarks in each image. [sent-349, score-0.177]
</p><p>88 Further, we identiﬁed 6 interest points in each image that correspond to the frame of the bicycle. [sent-350, score-0.156]
</p><p>89 We modeled the matching cost for an image pair as the shape distance between interest points in the pair. [sent-351, score-0.392]
</p><p>90 For a ﬁxed degree of supervision, we randomly selected image pairs for supervision and estimated matchings for the rest of the image pairs. [sent-353, score-0.515]
</p><p>91 Mean error and standard deviation is shown in Fig 5 as supervision increases. [sent-355, score-0.142]
</p><p>92 Fig 6 demonstrates qualitative results by our Figure 5: Normalized error as the degree of supervision varies. [sent-356, score-0.142]
</p><p>93 5  line method PLA (red) and Permutation Synchronization (blue)  Conclusions  Estimating the correct matching between two sets from noisy similarity data, such as the visual feature based similarity matrices that arise in computer vision is an error-prone process. [sent-358, score-0.302]
</p><p>94 However, ( ) when we have not just two, but m different sets, the consistency conditions between the m pair2 wise matchings severely constrain the solution. [sent-359, score-0.244]
</p><p>95 Our eigenvector decomposition based algorithm, Permutation Synchronization, exploits this fact and pools information from all pairwise similarity matrices to jointly estimate a globally consistent array of matchings in a single shot. [sent-360, score-0.487]
</p><p>96 (Green lines) Ground truth matching for image pairs (left-center) and (center-right). [sent-368, score-0.345]
</p><p>97 Shape matching and object recognition using low distortion correspondences. [sent-415, score-0.252]
</p><p>98 Three-dimensional structure determination from common lines in cryo-EM by eigenvectors and semideﬁnite programming. [sent-481, score-0.184]
</p><p>99 The eigenvalues and eigenvectors of ﬁnite, low rank perturbations of large random matrices. [sent-516, score-0.195]
</p><p>100 An integer projected ﬁxed point method for graph matching and map inference. [sent-547, score-0.257]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('permutation', 0.376), ('synchronization', 0.332), ('ji', 0.285), ('tji', 0.219), ('matching', 0.217), ('matchings', 0.205), ('landmarks', 0.177), ('permutations', 0.171), ('landmark', 0.149), ('eigenvectors', 0.143), ('supervision', 0.142), ('sn', 0.121), ('pairwise', 0.115), ('stereo', 0.112), ('xp', 0.111), ('vn', 0.105), ('frob', 0.097), ('munkres', 0.097), ('vi', 0.089), ('images', 0.088), ('nm', 0.085), ('fig', 0.085), ('occlusion', 0.085), ('xj', 0.084), ('image', 0.084), ('vec', 0.08), ('matches', 0.075), ('keypoints', 0.073), ('kuhn', 0.068), ('assignment', 0.065), ('green', 0.065), ('wigner', 0.064), ('snavely', 0.064), ('ej', 0.062), ('keypoint', 0.059), ('ei', 0.058), ('ambiguities', 0.056), ('eigenvector', 0.055), ('correct', 0.055), ('shape', 0.054), ('occluded', 0.053), ('eigenvalues', 0.052), ('array', 0.051), ('baseline', 0.049), ('caetano', 0.049), ('curless', 0.049), ('hadani', 0.049), ('pachauri', 0.049), ('risi', 0.049), ('touring', 0.049), ('orthogonal', 0.048), ('books', 0.047), ('wisconsin', 0.047), ('surely', 0.045), ('singer', 0.045), ('symmetric', 0.045), ('house', 0.044), ('truth', 0.044), ('eigenvalue', 0.044), ('matrix', 0.044), ('mcauley', 0.043), ('repetitive', 0.043), ('lines', 0.041), ('vj', 0.041), ('descriptors', 0.04), ('graph', 0.04), ('ground', 0.04), ('hotel', 0.04), ('shot', 0.04), ('consistency', 0.039), ('xi', 0.038), ('points', 0.037), ('bike', 0.037), ('pji', 0.037), ('columns', 0.037), ('leading', 0.036), ('viewpoints', 0.035), ('geometrical', 0.035), ('uj', 0.035), ('rotation', 0.035), ('object', 0.035), ('frame', 0.035), ('correspondence', 0.034), ('spectra', 0.034), ('microscopy', 0.034), ('objects', 0.033), ('xm', 0.033), ('recover', 0.033), ('scene', 0.032), ('entries', 0.032), ('cmu', 0.031), ('bijection', 0.031), ('semide', 0.031), ('red', 0.031), ('globally', 0.031), ('relaxation', 0.031), ('matrices', 0.03), ('normalized', 0.03), ('alignment', 0.03), ('nding', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="300-tfidf-1" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>Author: Deepti Pachauri, Risi Kondor, Vikas Singh</p><p>Abstract: The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including ﬁnding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, Permutation Synchronization, which ﬁnds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efﬁcient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods. 1</p><p>2 0.19734547 <a title="300-tfidf-2" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vamsi Ithapu, Qinyuan Sun, Sterling C. Johnson, Vikas Singh</p><p>Abstract: Multiple hypothesis testing is a signiﬁcant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given α-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub–sampled — on the order of 0.5% — matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacriﬁcing the ﬁdelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly 50× can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated α-threshold is also recovered faithfully, and is stable. 1</p><p>3 0.18589784 <a title="300-tfidf-3" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>Author: Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Muse, Guillermo Sapiro</p><p>Abstract: Graph matching is a challenging problem with very important applications in a wide range of ﬁelds, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsityrelated techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efﬁciently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available. 1</p><p>4 0.14163652 <a title="300-tfidf-4" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>Author: Fajwel Fogel, Rodolphe Jenatton, Francis Bach, Alexandre D'Aspremont</p><p>Abstract: Seriation seeks to reconstruct a linear order between variables using unsorted similarity information. It has direct applications in archeology and shotgun gene sequencing for example. We prove the equivalence between the seriation and the combinatorial 2-SUM problem (a quadratic minimization problem over permutations) over a class of similarity matrices. The seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we produce a convex relaxation for the 2-SUM problem to improve the robustness of solutions in a noisy setting. This relaxation also allows us to impose additional structural constraints on the solution, to solve semi-supervised seriation problems. We present numerical experiments on archeological data, Markov chains and gene sequences. 1</p><p>5 0.11115441 <a title="300-tfidf-5" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>Author: Zhengdong Lu, Hang Li</p><p>Abstract: Many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents, etc.). The matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema, although proven successful on a range of matching tasks, is insufﬁcient for capturing the rich structure in the matching process of more complicated objects. In this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More speciﬁcally, we apply this model to matching tasks in natural language, e.g., ﬁnding sensible responses for a tweet, or relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models. 1</p><p>6 0.097466275 <a title="300-tfidf-6" href="./nips-2013-Learning_to_Prune_in_Metric_and_Non-Metric_Spaces.html">169 nips-2013-Learning to Prune in Metric and Non-Metric Spaces</a></p>
<p>7 0.095354952 <a title="300-tfidf-7" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>8 0.091105901 <a title="300-tfidf-8" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>9 0.090989433 <a title="300-tfidf-9" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>10 0.089754805 <a title="300-tfidf-10" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>11 0.079355441 <a title="300-tfidf-11" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>12 0.07533212 <a title="300-tfidf-12" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>13 0.068802953 <a title="300-tfidf-13" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>14 0.066518247 <a title="300-tfidf-14" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>15 0.066116311 <a title="300-tfidf-15" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>16 0.06577304 <a title="300-tfidf-16" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>17 0.063611157 <a title="300-tfidf-17" href="./nips-2013-Visual_Concept_Learning%3A_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies.html">349 nips-2013-Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies</a></p>
<p>18 0.062539309 <a title="300-tfidf-18" href="./nips-2013-Learning_invariant_representations_and_applications_to_face_verification.html">166 nips-2013-Learning invariant representations and applications to face verification</a></p>
<p>19 0.061114777 <a title="300-tfidf-19" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>20 0.059450451 <a title="300-tfidf-20" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, 0.102), (2, -0.007), (3, 0.042), (4, 0.045), (5, -0.012), (6, -0.014), (7, -0.046), (8, -0.094), (9, 0.033), (10, -0.023), (11, -0.011), (12, 0.09), (13, -0.001), (14, -0.02), (15, 0.051), (16, -0.026), (17, -0.071), (18, -0.15), (19, 0.042), (20, 0.025), (21, -0.101), (22, -0.043), (23, 0.024), (24, -0.052), (25, -0.102), (26, -0.019), (27, 0.109), (28, -0.137), (29, 0.056), (30, 0.066), (31, 0.078), (32, -0.015), (33, 0.094), (34, 0.002), (35, 0.057), (36, -0.014), (37, -0.065), (38, 0.091), (39, 0.144), (40, 0.044), (41, 0.001), (42, 0.152), (43, 0.027), (44, 0.066), (45, 0.098), (46, -0.005), (47, 0.041), (48, -0.05), (49, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95001423 <a title="300-lsi-1" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>Author: Deepti Pachauri, Risi Kondor, Vikas Singh</p><p>Abstract: The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including ﬁnding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, Permutation Synchronization, which ﬁnds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efﬁcient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods. 1</p><p>2 0.71426803 <a title="300-lsi-2" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>Author: Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Muse, Guillermo Sapiro</p><p>Abstract: Graph matching is a challenging problem with very important applications in a wide range of ﬁelds, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsityrelated techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efﬁciently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available. 1</p><p>3 0.65412706 <a title="300-lsi-3" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vamsi Ithapu, Qinyuan Sun, Sterling C. Johnson, Vikas Singh</p><p>Abstract: Multiple hypothesis testing is a signiﬁcant problem in nearly all neuroimaging studies. In order to correct for this phenomena, we require a reliable estimate of the Family-Wise Error Rate (FWER). The well known Bonferroni correction method, while simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics. Permutation testing, on the other hand, is an exact, non-parametric method of estimating the FWER for a given α-threshold, but for acceptably low thresholds the computational burden can be prohibitive. In this paper, we show that permutation testing in fact amounts to populating the columns of a very large matrix P. By analyzing the spectrum of this matrix, under certain conditions, we see that P has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub–sampled — on the order of 0.5% — matrix completion methods. Based on this observation, we propose a novel permutation testing methodology which offers a large speedup, without sacriﬁcing the ﬁdelity of the estimated FWER. Our evaluations on four different neuroimaging datasets show that a computational speedup factor of roughly 50× can be achieved while recovering the FWER distribution up to very high accuracy. Further, we show that the estimated α-threshold is also recovered faithfully, and is stable. 1</p><p>4 0.64037043 <a title="300-lsi-4" href="./nips-2013-Convex_Relaxations_for_Permutation_Problems.html">73 nips-2013-Convex Relaxations for Permutation Problems</a></p>
<p>Author: Fajwel Fogel, Rodolphe Jenatton, Francis Bach, Alexandre D'Aspremont</p><p>Abstract: Seriation seeks to reconstruct a linear order between variables using unsorted similarity information. It has direct applications in archeology and shotgun gene sequencing for example. We prove the equivalence between the seriation and the combinatorial 2-SUM problem (a quadratic minimization problem over permutations) over a class of similarity matrices. The seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we produce a convex relaxation for the 2-SUM problem to improve the robustness of solutions in a noisy setting. This relaxation also allows us to impose additional structural constraints on the solution, to solve semi-supervised seriation problems. We present numerical experiments on archeological data, Markov chains and gene sequences. 1</p><p>5 0.5171842 <a title="300-lsi-5" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>Author: Krzysztof M. Choromanski, Tony Jebara, Kui Tang</p><p>Abstract: The adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy. This problem leads to a generalization of k-anonymity to the b-matching setting. Novel algorithms and theory are provided to implement this type of anonymity. The relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual. Empirical results conﬁrm improved utility on benchmark and social data-sets.</p><p>6 0.4989545 <a title="300-lsi-6" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>7 0.49529329 <a title="300-lsi-7" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>8 0.49052674 <a title="300-lsi-8" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>9 0.48643494 <a title="300-lsi-9" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>10 0.47030145 <a title="300-lsi-10" href="./nips-2013-Generalizing_Analytic_Shrinkage_for_Arbitrary_Covariance_Structures.html">130 nips-2013-Generalizing Analytic Shrinkage for Arbitrary Covariance Structures</a></p>
<p>11 0.46971893 <a title="300-lsi-11" href="./nips-2013-Modeling_Clutter_Perception_using_Parametric_Proto-object_Partitioning.html">195 nips-2013-Modeling Clutter Perception using Parametric Proto-object Partitioning</a></p>
<p>12 0.46306449 <a title="300-lsi-12" href="./nips-2013-The_Fast_Convergence_of_Incremental_PCA.html">324 nips-2013-The Fast Convergence of Incremental PCA</a></p>
<p>13 0.46288866 <a title="300-lsi-13" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>14 0.44948852 <a title="300-lsi-14" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>15 0.44457829 <a title="300-lsi-15" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<p>16 0.44272575 <a title="300-lsi-16" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>17 0.43814194 <a title="300-lsi-17" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>18 0.43801779 <a title="300-lsi-18" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>19 0.4354319 <a title="300-lsi-19" href="./nips-2013-Higher_Order_Priors_for_Joint_Intrinsic_Image%2C_Objects%2C_and_Attributes_Estimation.html">138 nips-2013-Higher Order Priors for Joint Intrinsic Image, Objects, and Attributes Estimation</a></p>
<p>20 0.42696649 <a title="300-lsi-20" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.028), (33, 0.183), (34, 0.104), (41, 0.021), (49, 0.042), (56, 0.098), (70, 0.039), (80, 0.226), (85, 0.06), (89, 0.044), (93, 0.071), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84556764 <a title="300-lda-1" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>Author: Deepti Pachauri, Risi Kondor, Vikas Singh</p><p>Abstract: The problem of matching not just two, but m different sets of objects to each other arises in many contexts, including ﬁnding the correspondence between feature points across multiple images in computer vision. At present it is usually solved by matching the sets pairwise, in series. In contrast, we propose a new method, Permutation Synchronization, which ﬁnds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition. The resulting algorithm is both computationally efﬁcient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods. 1</p><p>2 0.81991714 <a title="300-lda-2" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>Author: Xiaoqin Zhang, Di Wang, Zhengyuan Zhou, Yi Ma</p><p>Abstract: In this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors. Since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented Lagrange multiplier method. To improve the computational efﬁciency, we introduce a proximal gradient step to the alternating direction minimization method. We have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm. Both simulations and experiments show that our methods are more efﬁcient and effective than previous work. The proposed method can be easily applied to simultaneously rectify and align multiple images or videos frames. In this context, the state-of-the-art algorithms “RASL” and “TILT” can be viewed as two special cases of our work, and yet each only performs part of the function of our method.</p><p>3 0.74043268 <a title="300-lda-3" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>4 0.74011421 <a title="300-lda-4" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>5 0.7373631 <a title="300-lda-5" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>6 0.73649359 <a title="300-lda-6" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>7 0.73580015 <a title="300-lda-7" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>8 0.73576331 <a title="300-lda-8" href="./nips-2013-Mid-level_Visual_Element_Discovery_as_Discriminative_Mode_Seeking.html">190 nips-2013-Mid-level Visual Element Discovery as Discriminative Mode Seeking</a></p>
<p>9 0.73540294 <a title="300-lda-9" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>10 0.73533767 <a title="300-lda-10" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>11 0.73482478 <a title="300-lda-11" href="./nips-2013-Transfer_Learning_in_a_Transductive_Setting.html">335 nips-2013-Transfer Learning in a Transductive Setting</a></p>
<p>12 0.73453617 <a title="300-lda-12" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>13 0.73344529 <a title="300-lda-13" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>14 0.73343593 <a title="300-lda-14" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>15 0.73311311 <a title="300-lda-15" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>16 0.73236579 <a title="300-lda-16" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>17 0.73176426 <a title="300-lda-17" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>18 0.73156595 <a title="300-lda-18" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>19 0.73123431 <a title="300-lda-19" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>20 0.73109472 <a title="300-lda-20" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
