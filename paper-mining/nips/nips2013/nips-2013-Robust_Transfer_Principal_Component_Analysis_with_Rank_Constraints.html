<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-285" href="#">nips2013-285</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</h1>
<br/><p>Source: <a title="nips-2013-285-pdf" href="http://papers.nips.cc/paper/5130-robust-transfer-principal-component-analysis-with-rank-constraints.pdf">pdf</a></p><p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>Reference: <a title="nips-2013-285-reference" href="../nips2013_reference/nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. [sent-2, score-0.211]
</p><p>2 However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. [sent-3, score-0.15]
</p><p>3 In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. [sent-4, score-0.868]
</p><p>4 Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. [sent-5, score-0.523]
</p><p>5 Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. [sent-6, score-0.765]
</p><p>6 The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. [sent-7, score-0.308]
</p><p>7 We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. [sent-8, score-0.273]
</p><p>8 Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. [sent-9, score-0.699]
</p><p>9 It has been used to discover important latent information about observed data matrices for visualization, feature recovery, embedding and data cleaning. [sent-11, score-0.106]
</p><p>10 The fundamental assumption roots in dimensionality reduction is that the intrinsic structure of high dimensional observation data lies on a low dimensional linear subspace. [sent-12, score-0.221]
</p><p>11 It seeks the best low-rank approximation of the given data matrix under a well understood least-squares reconstruction loss, and projects data onto uncorrelated low dimensional subspace. [sent-14, score-0.334]
</p><p>12 These properties make PCA a well suited reduction method when the observed data is mildly corrupted with small Gaussian noise [12]. [sent-16, score-0.276]
</p><p>13 Even a small fraction of large errors can cause severe degradation in PCA’s estimate of the low rank structure. [sent-18, score-0.303]
</p><p>14 Real-life data, however, is often corrupted with large errors or even missing observations. [sent-19, score-0.27]
</p><p>15 To tackle dimensionality reduction with arbitrarily large errors and outliers, a number of approaches that robustify PCA have been developed in the literature, including ℓ1 -norm regularized robust PCA [14], inﬂuence function techniques [5, 13], and alternating ℓ1 -norm minimization [8]. [sent-20, score-0.374]
</p><p>16 Nevertheless, the 1  capacity of these approaches on recovering the low-rank structure of a corrupted data matrix can still be degraded with the increasing of the fraction of the large errors. [sent-21, score-0.428]
</p><p>17 In this paper, we propose a novel robust transfer principal component analysis method to recover the low rank representation of heavily corrupted data by leveraging related uncorrupted auxiliary data. [sent-22, score-1.269]
</p><p>18 Seeking knowledge transfer from a related auxiliary data source for the target learning problem has been popularly studied in supervised learning. [sent-23, score-0.553]
</p><p>19 It is also known that modeling related data sources together provides rich information for discovering theirs shared subspace representations [4]. [sent-24, score-0.139]
</p><p>20 This robust transfer PCA framework combines aspects of both robust PCA and transfer learning methodologies. [sent-26, score-0.712]
</p><p>21 We expect the critical low rank structure shared between the two data matrices can be effectively transferred from the uncorrupted auxiliary data to recover the low dimensional subspace representation of the heavily corrupted target data in a robust manner. [sent-27, score-1.317]
</p><p>22 We formulate this robust transfer PCA as a joint minimization problem over a convex combination of least squares losses with non-convex matrix rank constraints. [sent-28, score-0.826]
</p><p>23 Though a simple relaxation of the matrix rank constraints into convex nuclear norm constraints can lead to a convex optimization problem, it is very difﬁcult to control the rank of the low-rank representation matrix we aim to recover through the nuclear norm. [sent-29, score-1.181]
</p><p>24 We thus develop a proximal projected gradient descent optimization algorithm to solve the proposed optimization problem with rank constraints, which permits a convenient closed-form solution for each proximal step based on singular value decomposition and converges to a stationary point. [sent-30, score-0.767]
</p><p>25 Our experiments over image denoising tasks show the proposed method can effectively recover images corrupted with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. [sent-31, score-0.892]
</p><p>26 Notations: In this paper, we use In to denote an n × n identify matrix, use On,m to denote an n × m matrix with all 0 values, use · F to denote the matrix Frobenius norm, and use · ∗ to denote the nuclear norm (trace norm). [sent-32, score-0.409]
</p><p>27 2  Preliminaries  Assume we are given an observed data matrix X ∈ Rn×d consisting of n observations of ddimensional feature vectors, which was generated by corrupting some entries of a latent low-rank matrix M ∈ Rn×d with an error matrix E ∈ Rn×d such that X = M + E. [sent-33, score-0.392]
</p><p>28 We aim to to recover the low-rank matrix M by projecting the high dimensional observations X into a low dimensional manifold representation matrix Z ∈ Rn×k over the low dimensional subspace B ∈ Rk×d , such that M = ZB, BB ⊤ = Ik for k < d. [sent-34, score-0.537]
</p><p>29 1  PCA  Given the above setup, standard PCA assumes the error matrix E contains small i. [sent-36, score-0.122]
</p><p>30 Gaussian noises, and seeks optimal low dimensional encoding matrix Z and basis matrix B to reconstruct X by X = ZB + E. [sent-39, score-0.346]
</p><p>31 (1) F Z,B  That is, standard PCA seeks the best rank-k estimate of the latent low-rank matrix M = ZB by solving min X − M 2 s. [sent-43, score-0.183]
</p><p>32 With the convenient solution, standard PCA has been widely used for modern data analysis and serves as an efﬁcient and effective dimensionality reduction procedure when the error E is small and i. [sent-47, score-0.137]
</p><p>33 2  Robust PCA  The validity of standard PCA however breaks down when corrupted errors in the observed data matrix are large. [sent-52, score-0.418]
</p><p>34 Note that even a single grossly corrupted entry in the observation matrix X can render the recovered M ∗ matrix to be shifted away from the true low-rank matrix M . [sent-53, score-0.559]
</p><p>35 To recover the intrinsic low-rank matrix M from the observation matrix X corrupted with sparse large errors E, a polynomial-time robust PCA method has been developed in [14], which induces the following optimization problem min rank(M ) + γ E  0  M,E  s. [sent-54, score-0.837]
</p><p>36 (4)  By relaxing the non-convex rank function and the ℓ0 -norm into their convex envelopes of nuclear norm and ℓ1 -norm respectively, a convex relaxation of the robust PCA can be yielded min M M,E  ∗  +λ E  1  s. [sent-57, score-0.685]
</p><p>37 (5)  With an appropriate choice of λ parameter, one can exactly recover the M, E matrices that generated the observations X by solving this convex program. [sent-60, score-0.196]
</p><p>38 To produce a scalable optimization for robust PCA, a more convenient relaxed formulation has been considered in [14] min M M,E  ∗  +λ E  1  +  α M +E−X 2  2 F  (6)  where the original equality constraint is replaced with a reconstruction loss penalty term. [sent-61, score-0.298]
</p><p>39 This formulation apparently seeks the lowest rank M that can best reconstruct the observation matrix X subjecting to sparse errors E. [sent-62, score-0.456]
</p><p>40 Robust PCA though can effectively recover the low-rank matrix given very sparse large errors in the observed data, its performance can be degraded when the observation data is heavily corrupted with dense large errors. [sent-63, score-0.593]
</p><p>41 In this work, we propose to tackle this problem by exploiting information from related uncorrupted auxiliary data. [sent-64, score-0.254]
</p><p>42 3  Robust Transfer PCA  Exploring labeled information in a related auxiliary data set to assist the learning problem on a target data set has been widely studied in supervised learning scenarios within the context of transfer learning, domain adaptation and multi-task learning [10]. [sent-65, score-0.44]
</p><p>43 Moreover, it has also been shown that modeling related data sources together can provide useful information for discovering their shared subspace representations in an unsupervised manner [4]. [sent-66, score-0.139]
</p><p>44 The principle behind these knowledge transfer learning approaches is that related data sets can complement each other on identifying the intrinsic latent structure shared between them. [sent-67, score-0.324]
</p><p>45 Following this transfer learning scheme, we present a robust transfer PCA method for recovering low-rank matrix from a heavily corrupted observation matrix. [sent-68, score-0.946]
</p><p>46 Assume we are given a target data matrix Xt ∈ Rnt ×d corrupted with errors of large magnitude, and a related source data matrix Xs ∈ Rns ×d . [sent-69, score-0.75]
</p><p>47 Given constant matrices As = [Ins , Ons ,nt ] and At = [Ont ,ns , Int ], we can re-express Ns and Nt in term of the uniﬁed matrix Zc such that Ns = As Zc and Nt = At Zc . [sent-72, score-0.176]
</p><p>48 The learning problem of robust transfer PCA can then be formulated as the following joint minimiza3  tion problem min  Zc ,Zs ,Zt ,Bc ,Bs ,Bt ,Es ,Et  s. [sent-73, score-0.401]
</p><p>49 αs As Zc Bc + Zs Bs + Es − Xs 2 + F 2 αt At Zc Bc + Zt Bt + Et − Xt 2 + βs Es F 2 ⊤ ⊤ ⊤ B c B c = I kc , B s B s = I ks , B t B t = I kt  (9) 1  + β t Et  1  which minimizes the least squares reconstruction losses on both data matrices with ℓ1 -norm regularizers over the additive error matrices. [sent-75, score-0.869]
</p><p>50 rank(Mc ) ≤ kc , rank(Ms ) ≤ ks , rank(Mt ) ≤ kt  2 F  (11)  which has a ℓ1 -norm regularized convex objective function, but is subjecting to non-convex inequality rank constraints. [sent-80, score-0.959]
</p><p>51 A standard convexiﬁcation of the rank constraints is to replace rank functions with their convex envelopes, nuclear norms [3, 14, 1, 6, 15]. [sent-81, score-0.561]
</p><p>52 However, though the nuclear norm is a convex envelope of the rank function, it is not always a high-quality approximation of the rank function [11]. [sent-83, score-0.61]
</p><p>53 Moreover, it is very difﬁcult to select the appropriate trade-off parameters λs , λt for the nuclear norm regularizers in (12) to recover the low-rank matrix solutions in the original optimization in (11). [sent-84, score-0.447]
</p><p>54 In principal component analysis problems it is much more convenient to have explicit control on the rank of the low-rank solution matrices. [sent-85, score-0.38]
</p><p>55 Therefore instead of solving the nuclear norm based convex optimization problem (12), we develop a scalable and efﬁcient proximal gradient algorithm to solve the rank constraint based minimization problem (11) directly, which is shown to converge to a stationary point. [sent-86, score-0.661]
</p><p>56 After solving the optimization problem (11), the low-rank approximation of the corrupted matrix Xt ˆ can be obtained as Xt = At Mc + Mt . [sent-87, score-0.373]
</p><p>57 4  Proximal Projected Gradient Descent Algorithm  Proximal gradient methods have been popularly used for unconstrained convex optimization problems with continuous but non-smooth regularizers [2]. [sent-88, score-0.236]
</p><p>58 In this work, we develop a proximal projected gradient algorithm to solve the non-convex optimization problem with matrix rank constraints in (11). [sent-89, score-0.583]
</p><p>59 End While Here f (Θ) is a convex and continuously differentiable function while g(Θ) is a convex but nonsmooth function. [sent-96, score-0.126]
</p><p>60 Let C = {Θ : rank(Mc ) ≤ kc , rank(Ms ) ≤ ks , rank(Mt ) ≤ kt }. [sent-98, score-0.655]
</p><p>61 Our proximal projected gradient algorithm is an iterative procedure. [sent-100, score-0.212]
</p><p>62 5  Experiments  We evaluate the proposed approach using image denoising tasks constructed on the Yale Face Database, which contains 165 grayscale images of 15 individuals. [sent-114, score-0.256]
</p><p>63 Our goal is to investigate the performance of the proposed approach on recovering data corrupted 0 with large and dense errors. [sent-116, score-0.255]
</p><p>64 Let Xt denote a target image matrix from one subject, which has values between 0 and 255. [sent-118, score-0.277]
</p><p>65 We randomly select a fraction of its pixels to add large errors to reach value 255, where the fraction of noisy pixels is controlled using a noise level parameter σ. [sent-119, score-0.176]
</p><p>66 We then 0 use an uncorrupted image matrix Xs from the same or different subject as the source matrix to help ˆ the image denoising of Xt by recovering its low-rank approximation matrix Xt . [sent-121, score-0.922]
</p><p>67 In the experiments, we compared the performance of the following methods on image denoising with large errors: • R-T-PCA: This is the proposed robust transfer PCA method. [sent-122, score-0.536]
</p><p>68 • R-S-PCA: This is a robust shared PCA method that applies a rank-constrained version of 0 the robust PCA in [14] on the concatenated matrix [Xs ; Xt ] to recover a low-rank approxˆ t with rank kc + kt . [sent-125, score-1.279]
</p><p>69 imation matrix X • R-PCA: This is a robust PCA method that applies a rank-constrained version of the robust ˆ PCA in [14] on Xt to recover a low-rank approximation matrix Xt with rank kc + kt . [sent-126, score-1.321]
</p><p>70 0 • S-PCA: This is a shared PCA method that applies PCA on concatenated matrix [Xs ; Xt ] to ˆ recover a low-rank approximation matrix Xt with rank kc + kt . [sent-127, score-1.115]
</p><p>71 • PCA: This method applies PCA on the noisy target matrix Xt to recover a low-rank apˆ proximation matrix Xt with rank kc + kt . [sent-128, score-1.109]
</p><p>72 • R-2Step-PCA: This method exploits the auxiliary source matrix by ﬁrst performing robust 0 PCA over the concatenated matrix [Xs ; Xt ] to produce a shared matrix Mc with rank kc , and then performing robust PCA over the residue matrix (Xt − At Mc ) to produce a matrix ˆ Mt with rank kt . [sent-129, score-2.062]
</p><p>73 All the methods are evaluated using the root mean square error (RMSE) between the true target 0 ˆ image matrix Xt and the low-rank approximation matrix Xt recovered from the noisy image matrix. [sent-131, score-0.518]
</p><p>74 Unless speciﬁed otherwise, we used kc = 8, ks = 3, kt = 3 in all experiments. [sent-132, score-0.655]
</p><p>75 1  Intra-Subject Experiments  We ﬁrst conducted experiments by constructing 15 transfer tasks for the 15 subjects. [sent-134, score-0.274]
</p><p>76 Speciﬁcally, for each subject, we used the ﬁrst image matrix as the target matrix and used each of the remaining 10 image matrices as the source matrix each time. [sent-135, score-0.738]
</p><p>77 For each source matrix, we repeated the experiments 5 times by randomly generating noisy target matrix using the procedure described above. [sent-136, score-0.336]
</p><p>78 The average denoising results in terms of root mean square error (RMSE) with noise level σ = 5% are reported in Table 1. [sent-138, score-0.138]
</p><p>79 We can see that the proposed method R-T-PCA outperforms all other methods 6  Table 1: The average denoising results in terms of RMSE at noise level σ = 5%. [sent-143, score-0.138]
</p><p>80 The comparison between the two groups of methods, {R-T-PCA, R-S-PCA, S-PCA} and {R-PCA, PCA}, shows that a related source matrix is indeed useful for denoising the target matrix. [sent-236, score-0.419]
</p><p>81 The superior performance of R-T-PCA over R-S-PCA and S-PCA demonstrates the efﬁcacy of our transfer PCA framework in exploiting the auxiliary source matrix over methods that simply concatenate the auxiliary source matrix and target matrix. [sent-238, score-0.922]
</p><p>82 2  Cross-Subject Experiments  Next, we conducted transfer experiments using source matrix and target matrix from different subjects. [sent-240, score-0.668]
</p><p>83 We randomly constructed 5 transfer tasks, Task-6-1, Task-8-2, Task-9-4, Task-12-8 and Task14-11, where the ﬁrst number in the task name denotes the source subject index and second number denotes the target subject index. [sent-241, score-0.444]
</p><p>84 For example, to construct Task-6-1, we used the ﬁrst image matrix from subject-6 as the source matrix and used the ﬁrst image matrix from subject-1 as the target matrix. [sent-242, score-0.684]
</p><p>85 We repeated each experiment 10 times using randomly generated noisy target matrix. [sent-244, score-0.118]
</p><p>86 These results also suggest that even a remotely related source image can be useful. [sent-249, score-0.163]
</p><p>87 All these experiments demonstrate the efﬁcacy of the proposed method in exploiting uncorrupted auxiliary data matrix for denoising target images corrupted with large errors. [sent-250, score-0.808]
</p><p>88 7  Table 2: The average denoising results in terms of RMSE. [sent-251, score-0.113]
</p><p>89 3  Parameter Analysis  The optimization problem (11) for the proposed R-T-PCA method has a number of parameters to be set: αs , αt , βs , βt , kc , ks and kt . [sent-326, score-0.691]
</p><p>90 Given that the source and target matrices are similar in size, in these experiments we set αs = αt = 1, βs = βt and ks = kt . [sent-328, score-0.594]
</p><p>91 In the ﬁrst experiment, we set (kc , ks , kt ) = (8, 3, 3) and study the performance of R-T-PCA with different βs = βt = β values, for β ∈ {0. [sent-329, score-0.356]
</p><p>92 We can see that R-T-PCA is quite robust to β within the range of values, {0. [sent-337, score-0.154]
</p><p>93 1 and compared R-T-PCA with other methods across a few different settings of (kc , ks , kt ), with (kc , ks , kt ) ∈ {(6, 3, 3), (8, 3, 3), (8, 5, 5), (10, 3, 3), (10, 5, 5)}. [sent-343, score-0.712]
</p><p>94 6  Conclusion  In this paper, we developed a novel robust transfer principal component analysis method to recover the low-rank representation of corrupted data by leveraging related uncorrupted auxiliary data. [sent-346, score-1.028]
</p><p>95 This robust transfer PCA framework combines aspects of both robust PCA and transfer learning methodologies. [sent-347, score-0.712]
</p><p>96 Our experiments over image denoising tasks demonstrated the proposed method can effectively exploit auxiliary uncorrupted image to recover images corrupted with random large errors and signiﬁcantly outperform a number of comparison methods. [sent-349, score-0.926]
</p><p>97 Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming. [sent-388, score-0.149]
</p><p>98 Fixed point and bregman iterative methods for matrix rank minimization. [sent-395, score-0.325]
</p><p>99 Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization. [sent-406, score-0.467]
</p><p>100 Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization. [sent-424, score-0.49]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pca', 0.458), ('mc', 0.337), ('kc', 0.299), ('transfer', 0.202), ('corrupted', 0.193), ('kt', 0.189), ('rank', 0.18), ('ks', 0.167), ('mt', 0.161), ('robust', 0.154), ('ms', 0.146), ('xt', 0.133), ('uncorrupted', 0.126), ('zc', 0.124), ('matrix', 0.122), ('denoising', 0.113), ('proximal', 0.112), ('principal', 0.106), ('nuclear', 0.105), ('rmse', 0.103), ('auxiliary', 0.098), ('xs', 0.097), ('source', 0.096), ('target', 0.088), ('rns', 0.087), ('rnt', 0.087), ('recover', 0.079), ('errors', 0.077), ('image', 0.067), ('shared', 0.064), ('convex', 0.063), ('zb', 0.06), ('ns', 0.06), ('norm', 0.06), ('bc', 0.058), ('singular', 0.056), ('matrices', 0.054), ('projected', 0.051), ('convenient', 0.05), ('subspace', 0.049), ('gradient', 0.049), ('et', 0.046), ('regularizers', 0.045), ('nt', 0.045), ('component', 0.044), ('pes', 0.043), ('pmc', 0.043), ('pms', 0.043), ('pmt', 0.043), ('popularly', 0.043), ('svd', 0.043), ('bs', 0.042), ('images', 0.042), ('dimensional', 0.039), ('seeks', 0.039), ('yuhong', 0.038), ('envelopes', 0.038), ('pet', 0.038), ('subjecting', 0.038), ('conducted', 0.038), ('concatenated', 0.038), ('es', 0.037), ('heavily', 0.037), ('optimization', 0.036), ('reconstruction', 0.036), ('recovering', 0.036), ('tasks', 0.034), ('permits', 0.033), ('constraints', 0.033), ('reduction', 0.032), ('intrinsic', 0.032), ('noises', 0.03), ('noisy', 0.03), ('effectively', 0.03), ('recovery', 0.03), ('vk', 0.03), ('tackle', 0.03), ('dimensionality', 0.029), ('subject', 0.029), ('minimization', 0.029), ('losses', 0.029), ('degraded', 0.029), ('zs', 0.027), ('lipschitz', 0.027), ('stationary', 0.027), ('data', 0.026), ('bt', 0.026), ('noise', 0.025), ('descent', 0.025), ('low', 0.024), ('squares', 0.024), ('cacy', 0.023), ('joint', 0.023), ('regularized', 0.023), ('trace', 0.023), ('bb', 0.023), ('bregman', 0.023), ('fraction', 0.022), ('approximation', 0.022), ('min', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="285-tfidf-1" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>2 0.23166892 <a title="285-tfidf-2" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shuicheng Yan</p><p>Abstract: Robust PCA methods are typically based on batch optimization and have to load all the samples into memory during optimization. This prevents them from efﬁciently processing big data. In this paper, we develop an Online Robust PCA (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the number of samples, signiﬁcantly enhancing the computation and storage efﬁciency. The proposed OR-PCA is based on stochastic optimization of an equivalent reformulation of the batch RPCA. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efﬁciency advantages of the OR-PCA over online PCA and batch RPCA methods. 1</p><p>3 0.21550415 <a title="285-tfidf-3" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan</p><p>Abstract: We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the ﬁnal result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efﬁcient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data. 1</p><p>4 0.20442697 <a title="285-tfidf-4" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><p>5 0.1792589 <a title="285-tfidf-5" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>Author: Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain</p><p>Abstract: We consider streaming, one-pass principal component analysis (PCA), in the highdimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p2 ) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p2 ). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p log p) samplecomplexity – the ﬁrst algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data. 1</p><p>6 0.1553628 <a title="285-tfidf-6" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>7 0.14574605 <a title="285-tfidf-7" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>8 0.13359459 <a title="285-tfidf-8" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<p>9 0.12652503 <a title="285-tfidf-9" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>10 0.11881668 <a title="285-tfidf-10" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>11 0.11192288 <a title="285-tfidf-11" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>12 0.10773309 <a title="285-tfidf-12" href="./nips-2013-Computing_the_Stationary_Distribution_Locally.html">66 nips-2013-Computing the Stationary Distribution Locally</a></p>
<p>13 0.10381906 <a title="285-tfidf-13" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>14 0.10298202 <a title="285-tfidf-14" href="./nips-2013-Matrix_Completion_From_any_Given_Set_of_Observations.html">185 nips-2013-Matrix Completion From any Given Set of Observations</a></p>
<p>15 0.10205952 <a title="285-tfidf-15" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>16 0.10000037 <a title="285-tfidf-16" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>17 0.098955214 <a title="285-tfidf-17" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>18 0.09851186 <a title="285-tfidf-18" href="./nips-2013-Bayesian_inference_for_low_rank_spatiotemporal_neural_receptive_fields.html">53 nips-2013-Bayesian inference for low rank spatiotemporal neural receptive fields</a></p>
<p>19 0.098197147 <a title="285-tfidf-19" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>20 0.097287036 <a title="285-tfidf-20" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.235), (1, 0.092), (2, 0.133), (3, 0.099), (4, -0.05), (5, -0.038), (6, -0.094), (7, 0.169), (8, -0.137), (9, -0.15), (10, -0.167), (11, 0.08), (12, 0.083), (13, 0.183), (14, -0.037), (15, 0.019), (16, -0.008), (17, 0.052), (18, 0.083), (19, 0.024), (20, 0.084), (21, 0.004), (22, 0.068), (23, -0.058), (24, 0.078), (25, 0.025), (26, 0.091), (27, 0.032), (28, 0.027), (29, 0.073), (30, -0.014), (31, -0.013), (32, -0.078), (33, 0.012), (34, 0.027), (35, -0.164), (36, 0.017), (37, -0.004), (38, -0.16), (39, 0.036), (40, -0.052), (41, -0.035), (42, 0.05), (43, -0.047), (44, -0.028), (45, -0.018), (46, -0.012), (47, 0.039), (48, -0.016), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95692551 <a title="285-lsi-1" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>2 0.80965638 <a title="285-lsi-2" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shuicheng Yan</p><p>Abstract: Robust PCA methods are typically based on batch optimization and have to load all the samples into memory during optimization. This prevents them from efﬁciently processing big data. In this paper, we develop an Online Robust PCA (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the number of samples, signiﬁcantly enhancing the computation and storage efﬁciency. The proposed OR-PCA is based on stochastic optimization of an equivalent reformulation of the batch RPCA. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efﬁciency advantages of the OR-PCA over online PCA and batch RPCA methods. 1</p><p>3 0.79411674 <a title="285-lsi-3" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>Author: Raman Arora, Andy Cotter, Nati Srebro</p><p>Abstract: We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as “Matrix Stochastic Gradient” (MSG), as well as a practical variant, Capped MSG. We study the method both theoretically and empirically. 1</p><p>4 0.78603858 <a title="285-lsi-4" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>Author: Jiashi Feng, Huan Xu, Shie Mannor, Shuicheng Yan</p><p>Abstract: We consider the online Principal Component Analysis (PCA) where contaminated samples (containing outliers) are revealed sequentially to the Principal Components (PCs) estimator. Due to their sensitiveness to outliers, previous online PCA algorithms fail in this case and their results can be arbitrarily skewed by the outliers. Here we propose the online robust PCA algorithm, which is able to improve the PCs estimation upon an initial one steadily, even when faced with a constant fraction of outliers. We show that the ﬁnal result of the proposed online RPCA has an acceptable degradation from the optimum. Actually, under mild conditions, online RPCA achieves the maximal robustness with a 50% breakdown point. Moreover, online RPCA is shown to be efﬁcient for both storage and computation, since it need not re-explore the previous samples as in traditional robust PCA algorithms. This endows online RPCA with scalability for large scale data. 1</p><p>5 0.69526964 <a title="285-lsi-5" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>Author: Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain</p><p>Abstract: We consider streaming, one-pass principal component analysis (PCA), in the highdimensional regime, with limited memory. Here, p-dimensional samples are presented sequentially, and the goal is to produce the k-dimensional subspace that best approximates these points. Standard algorithms require O(p2 ) memory; meanwhile no algorithm can do better than O(kp) memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the spiked covariance model, where p-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, n, scales proportionally with the dimension, p. Yet, all algorithms that provably achieve this, have memory complexity O(p2 ). Meanwhile, algorithms with memory-complexity O(kp) do not have provable bounds on sample complexity comparable to p. We present an algorithm that achieves both: it uses O(kp) memory (meaning storage of any kind) and is able to compute the k-dimensional spike with O(p log p) samplecomplexity – the ﬁrst algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data. 1</p><p>6 0.65739483 <a title="285-lsi-6" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>7 0.62012631 <a title="285-lsi-7" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<p>8 0.5615924 <a title="285-lsi-8" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>9 0.52513915 <a title="285-lsi-9" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>10 0.49282491 <a title="285-lsi-10" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>11 0.49255884 <a title="285-lsi-11" href="./nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</a></p>
<p>12 0.47632828 <a title="285-lsi-12" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>13 0.45953161 <a title="285-lsi-13" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>14 0.45007452 <a title="285-lsi-14" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>15 0.44300917 <a title="285-lsi-15" href="./nips-2013-Phase_Retrieval_using_Alternating_Minimization.html">247 nips-2013-Phase Retrieval using Alternating Minimization</a></p>
<p>16 0.4426522 <a title="285-lsi-16" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>17 0.43658558 <a title="285-lsi-17" href="./nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</a></p>
<p>18 0.4322817 <a title="285-lsi-18" href="./nips-2013-Unsupervised_Spectral_Learning_of_Finite_State_Transducers.html">342 nips-2013-Unsupervised Spectral Learning of Finite State Transducers</a></p>
<p>19 0.42904788 <a title="285-lsi-19" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>20 0.42603478 <a title="285-lsi-20" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.021), (33, 0.169), (34, 0.116), (40, 0.149), (41, 0.028), (49, 0.029), (56, 0.099), (70, 0.032), (85, 0.135), (89, 0.043), (93, 0.063), (95, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8944543 <a title="285-lda-1" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Principal component analysis (PCA), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small Gaussian noises presented in the data. However, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors. In this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method. Our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix. Speciﬁcally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with common principal components shared across matrices and individual principal components speciﬁc to each data matrix. The formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints. We develop an efﬁcient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees. Our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signiﬁcantly outperform both standard PCA and robust PCA with rank constraints. 1</p><p>2 0.86578125 <a title="285-lda-2" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>Author: Nicolas Heess, Daniel Tarlow, John Winn</p><p>Abstract: Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex nonGaussian factors, there is still a signiﬁcant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difﬁcult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising. 1</p><p>3 0.84470105 <a title="285-lda-3" href="./nips-2013-A_Gang_of_Bandits.html">7 nips-2013-A Gang of Bandits</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Giovanni Zappella</p><p>Abstract: Multi-armed bandit problems formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems. In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, content may be served to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More speciﬁcally, we design and analyze a global recommendation strategy which allocates a bandit algorithm to each network node (user) and allows it to “share” signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a consistent increase in prediction performance obtained by exploiting the network structure. 1</p><p>4 0.84309167 <a title="285-lda-4" href="./nips-2013-Tracking_Time-varying_Graphical_Structure.html">332 nips-2013-Tracking Time-varying Graphical Structure</a></p>
<p>Author: Erich Kummerfeld, David Danks</p><p>Abstract: Structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary. In real-world environments, however, such changes often occur without warning or signal. Real-world data often come from generating models that are only locally stationary. In this paper, we present LoSST, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner. We show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and signiﬁcantly better when it is only locally stationary. 1</p><p>5 0.83562654 <a title="285-lda-5" href="./nips-2013-Regularized_Spectral_Clustering_under_the_Degree-Corrected_Stochastic_Blockmodel.html">272 nips-2013-Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel</a></p>
<p>Author: Tai Qin, Karl Rohe</p><p>Abstract: Spectral clustering is a fast and popular algorithm for ﬁnding clusters in networks. Recently, Chaudhuri et al. [1] and Amini et al. [2] proposed inspired variations on the algorithm that artiﬁcially inﬂate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the “star shape” in the eigenvectors–a common feature of empirical networks–can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justiﬁes several of the variations of the spectral clustering algorithm in terms of these models. 1</p><p>6 0.83465195 <a title="285-lda-6" href="./nips-2013-Modeling_Overlapping_Communities_with_Node_Popularities.html">196 nips-2013-Modeling Overlapping Communities with Node Popularities</a></p>
<p>7 0.8289066 <a title="285-lda-7" href="./nips-2013-Efficient_Online_Inference_for_Bayesian_Nonparametric_Relational_Models.html">104 nips-2013-Efficient Online Inference for Bayesian Nonparametric Relational Models</a></p>
<p>8 0.8266291 <a title="285-lda-8" href="./nips-2013-Near-Optimal_Entrywise_Sampling_for_Data_Matrices.html">206 nips-2013-Near-Optimal Entrywise Sampling for Data Matrices</a></p>
<p>9 0.82183707 <a title="285-lda-9" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>10 0.81677032 <a title="285-lda-10" href="./nips-2013-Analyzing_the_Harmonic_Structure_in_Graph-Based_Learning.html">35 nips-2013-Analyzing the Harmonic Structure in Graph-Based Learning</a></p>
<p>11 0.81641138 <a title="285-lda-11" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>12 0.81448293 <a title="285-lda-12" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>13 0.81262499 <a title="285-lda-13" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>14 0.80994356 <a title="285-lda-14" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>15 0.80940276 <a title="285-lda-15" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>16 0.80860645 <a title="285-lda-16" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>17 0.80824536 <a title="285-lda-17" href="./nips-2013-Scalable_Influence_Estimation_in_Continuous-Time_Diffusion_Networks.html">288 nips-2013-Scalable Influence Estimation in Continuous-Time Diffusion Networks</a></p>
<p>18 0.80707294 <a title="285-lda-18" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>19 0.80701774 <a title="285-lda-19" href="./nips-2013-Stochastic_blockmodel_approximation_of_a_graphon%3A_Theory_and_consistent_estimation.html">316 nips-2013-Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</a></p>
<p>20 0.80664772 <a title="285-lda-20" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
