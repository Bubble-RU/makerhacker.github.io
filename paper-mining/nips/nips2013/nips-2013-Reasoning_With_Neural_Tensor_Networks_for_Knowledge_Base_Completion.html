<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-263" href="#">nips2013-263</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</h1>
<br/><p>Source: <a title="nips-2013-263-pdf" href="http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.pdf">pdf</a></p><p>Author: Richard Socher, Danqi Chen, Christopher D. Manning, Andrew Ng</p><p>Abstract: Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the “Sumatran tiger” and “Bengal tiger.” Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively. 1</p><p>Reference: <a title="nips-2013-263-reference" href="../nips2013_reference/nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. [sent-7, score-0.39]
</p><p>2 In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. [sent-8, score-0.525]
</p><p>3 Previous work represented entities as either discrete atomic units or with a single entity vector representation. [sent-9, score-0.812]
</p><p>4 We show that performance can be improved when entities are represented as an average of their constituting word vectors. [sent-10, score-0.557]
</p><p>5 ” Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. [sent-12, score-0.463]
</p><p>6 We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. [sent-13, score-0.464]
</p><p>7 For instance, when told that a new species of monkeys has been discovered, a person does not need to ﬁnd textual evidence to know that this new monkey, too, will have legs (a meronymic relationship inferred due to a hyponymic relation to monkeys in general). [sent-23, score-0.302]
</p><p>8 These vectors can capture facts about that entity and how probable it is part of a certain relation. [sent-28, score-0.662]
</p><p>9 Each relation is deﬁned through the parameters of a novel neural tensor network which can explicitly relate two entity vectors. [sent-29, score-1.014]
</p><p>10 The ﬁrst contribution of this paper is the new neural tensor network (NTN), which generalizes several previous neural network models and provides a more powerful way to model relational information than a standard neural network layer. [sent-30, score-0.586]
</p><p>11 The second contribution is to introduce a new way to represent entities in knowledge bases. [sent-31, score-0.358]
</p><p>12 Previous work [8, 9, 10] represents each entity with one vector. [sent-32, score-0.51]
</p><p>13 R  tiger  Relation: instance of  Bengal  tiger …  Neural Tensor Network  e1  India  Bengal tiger …. [sent-38, score-0.304]
</p><p>14 Figure 1: Overview of our model which learns vector representations for entries in a knowledge base in order to predict new relationship triples. [sent-40, score-0.248]
</p><p>15 If combined with word representations, the relationships can be predicted with higher accuracy and for entities that were not in the original knowledge base. [sent-41, score-0.732]
</p><p>16 Instead, we represent each entity as the average of its word vectors, allowing the sharing of statistical strength between the words describing each entity e. [sent-43, score-1.307]
</p><p>17 The third contribution is the incorporation of word vectors which are trained on large unlabeled text. [sent-46, score-0.327]
</p><p>18 We also show that both our and their model can beneﬁt from initialization with unsupervised word vectors. [sent-60, score-0.318]
</p><p>19 [11] who use tensor factorization and Bayesian clustering for learning relational structures. [sent-62, score-0.313]
</p><p>20 Instead of clustering the entities in a nonparametric Bayesian framework we rely purely on learned entity vectors. [sent-63, score-0.812]
</p><p>21 Instead, we consider the subunits (space separated words) of entity names. [sent-67, score-0.51]
</p><p>22 [13] introduce a model with tensor layers for speech recognition. [sent-71, score-0.246]
</p><p>23 We learn to modify word representations via grounding in world knowledge. [sent-82, score-0.305]
</p><p>24 This essentially allows us to analyze word embeddings and query them for speciﬁc relations. [sent-83, score-0.255]
</p><p>25 Furthermore, the resulting vectors could be used in other tasks such as named entity recognition [18] or relation classiﬁcation in natural language [19]. [sent-84, score-0.749]
</p><p>26 3  Neural Models for Reasoning over Relations  This section introduces the neural tensor network that reasons over database entries by learning vector representations for them. [sent-85, score-0.422]
</p><p>27 1, each relation triple is described by a neural network and pairs of database entities which are given as input to that relation’s model. [sent-87, score-0.595]
</p><p>28 We ﬁrst describe our neural tensor model and then show that many previous models are special cases of it. [sent-90, score-0.286]
</p><p>29 Another way to describe the goal is link prediction in an existing network of relationships between entity nodes. [sent-93, score-0.629]
</p><p>30 The goal of our approach is to be able to state whether two entities (e1 , e2 ) are in a certain relationship R. [sent-94, score-0.368]
</p><p>31 The Neural Tensor Network (NTN) replaces a standard linear neural network layer with a bilinear tensor layer that directly relates the two entity vectors across multiple dimensions. [sent-99, score-1.184]
</p><p>32 The main advantage is that it can relate the two inputs multiplicatively instead of only implicitly through the nonlinearity as with standard neural networks where the entity vectors are simply concatenated. [sent-107, score-0.622]
</p><p>33 Intuitively, we can see each slice of the tensor as being responsible for one type of entity pair or instantiation of a relation. [sent-108, score-0.796]
</p><p>34 For instance, the model could learn that both animals and mechanical entities such as cars can have parts (i. [sent-109, score-0.302]
</p><p>35 , (car, has part, x)) from different parts of the semantic word vector space. [sent-111, score-0.299]
</p><p>36 Another way to interpret each tensor slice is that it mediates the relationship between the two entity vectors differently. [sent-113, score-0.934]
</p><p>37 Each model assigns a score to a triplet using a function g measuring how likely the triplet is correct. [sent-118, score-0.259]
</p><p>38 [8] scores relationships by mapping the left and right entities to a common space using a relationship speciﬁc mapping matrix and measuring the L1 distance between the two. [sent-122, score-0.464]
</p><p>39 The scoring function for each triplet has the following form: g(e1 , R, e2 ) = WR,1 e1 − WR,2 e2 1 , where WR,1 , WR,2 ∈ Rd×d are the parameters of relation R’s classiﬁer. [sent-123, score-0.339]
</p><p>40 This similarity-based model scores correct triplets lower (entities most certainly in a relation have 0 distance). [sent-124, score-0.289]
</p><p>41 The main problem with this model is that the parameters of the two entity vectors do not interact with each other, they are independently mapped to a common space. [sent-126, score-0.582]
</p><p>42 The second model tries to alleviate the problems of the distance model by connecting the entity vectors implicitly through the nonlinearity of a standard, single layer neural network. [sent-128, score-0.697]
</p><p>43 The scoring function has the following form: g(e1 , R, e2 ) = uT f (WR,1 e1 + WR,2 e2 ) = uT f R R  [WR,1 WR,2 ]  e1 e2  ,  where f = tanh, WR,1 , WR,2 ∈ Rk×d and uR ∈ Rk×1 are the parameters of relation R’s scoring function. [sent-129, score-0.281]
</p><p>44 While this is an improvement over the distance model, the non-linearity only provides a weak interaction between the two entity vectors at the expense of a harder optimization problem. [sent-130, score-0.582]
</p><p>45 Collobert and Weston [20] trained a similar model to learn word vector representations using words in their context. [sent-131, score-0.305]
</p><p>46 This model is a special case of the tensor neural network if the tensor is set to 0. [sent-132, score-0.583]
</p><p>47 [10] and tackles the issue of weak entity vector interaction through multiple matrix products followed by Hadamard products. [sent-135, score-0.51]
</p><p>48 It is different to the other models in our comparison in that it represents each relation simply as a single vector that interacts with the entity vectors through several linear products all of which are parameterized by the same parameters. [sent-136, score-0.749]
</p><p>49 While this allows the model to treat relational words and entity words the same way, we show in our experiments that giving each relationship its own matrix operators results in improved performance. [sent-139, score-0.643]
</p><p>50 However, the bilinear form between entity vectors is by itself desirable. [sent-140, score-0.697]
</p><p>51 The fourth model [11, 9] ﬁxes the issue of weak entity vector interaction through a relation-speciﬁc bilinear form. [sent-142, score-0.625]
</p><p>52 The scoring function is as follows: g(e1 , R, e2 ) = eT WR e2 , where 1 WR ∈ Rd×d are the only parameters of relation R’s scoring function. [sent-143, score-0.281]
</p><p>53 This is a big improvement over the two previous models as it incorporates the interaction of two entity vectors in a simple and efﬁcient way. [sent-144, score-0.582]
</p><p>54 However, the model is now restricted in terms of expressive power and number of parameters by the word vectors. [sent-145, score-0.282]
</p><p>55 In comparison to bilinear models, the neural tensor has much more expressive power which will be useful especially for larger databases. [sent-148, score-0.428]
</p><p>56 The main idea is that each (i) (i) triplet in the training set T (i) = (e1 , R(i) , e2 ) should receive a higher score than a triplet in which one of the entities is replaced with a random entity. [sent-152, score-0.59]
</p><p>57 Each relation has its associated neural tensor net parameters. [sent-154, score-0.453]
</p><p>58 We call the triplet 4  (i)  (i)  with a random entity corrupted and denote the corrupted triplet as Tc = (e1 , R(i) , ec ), where we sampled entity ec randomly from the set of all entities that can appear at that position in that relation. [sent-155, score-1.606]
</p><p>59 We minimize the following objective: N  C (i) max 0, 1 − g T (i) + g Tc  J(Ω) =  + λ Ω 2, 2  i=1 c=1  where N is the number of training triplets and we score the correct relation triplet higher than its corrupted one up to a margin of 1. [sent-157, score-0.461]
</p><p>60 4 Entity Representations Revisited All the above models work well with randomly initialized entity vectors. [sent-166, score-0.545]
</p><p>61 In this section we introduce two further improvements: representing entities by their word vectors and initializing word vectors with pre-trained vectors. [sent-167, score-0.956]
</p><p>62 Previous work [8, 9, 10] assigned a single vector representation to each entity of the knowledge base, which does not allow the sharing of statistical strength between the words describing each entity. [sent-168, score-0.598]
</p><p>63 Instead, we model each word as a d-dimensional vector ∈ Rd and compute an entity vector as the composition of its word vectors. [sent-169, score-1.02]
</p><p>64 Hence, for a total number of NE entities consisting of NW many unique words, if we train on the word level (the training error derivatives are also back-propagated to these word vectors), and represent entities by word vectors, the full embedding matrix has dimensionality E ∈ Rd×NW . [sent-171, score-1.398]
</p><p>65 Otherwise we represent each entity as an atomic single vector and train the entity embedding matrix E ∈ Rd×NE . [sent-172, score-1.02]
</p><p>66 We represent the entity vector by averaging its word vectors. [sent-173, score-0.765]
</p><p>67 In the WordNet subset over 60% of the entities have only a single word and over 90% have less or equal to 2 words. [sent-177, score-0.557]
</p><p>68 Furthermore, most of the entities do not exhibit a clear compositional structure, e. [sent-178, score-0.302]
</p><p>69 Training word vectors has the additional advantage that we can beneﬁt from pre-trained unsupervised word vectors, which in general capture some distributional syntactic and semantic information. [sent-182, score-0.655]
</p><p>70 One possible future extension is to incorporate the idea of multiple word vectors per word as in Huang et al. [sent-188, score-0.582]
</p><p>71 Our models can obtain such knowledge (with varying degrees of accuracy) by jointly learning relationship classiﬁers and entity representations. [sent-194, score-0.632]
</p><p>72 We ﬁrst describe the datasets, then compare the above models and conclude with several analyses of important modeling decisions, such as whether to use entity vectors or word vectors. [sent-195, score-0.837]
</p><p>73 In total, there are 38,696 unique entities in 11 different relations. [sent-202, score-0.302]
</p><p>74 We ﬁlter out tuples from the testing set if either or both of their two entities also appear in the training set in a different relation or order. [sent-204, score-0.529]
</p><p>75 We use the development set to ﬁnd a threshold TR for each relation such that if g(e1 , R, e2 ) ≥ TR , the relation (e1 , R, e2 ) holds, otherwise it does not hold. [sent-221, score-0.334]
</p><p>76 In order to create a testing set for classiﬁcation, we randomly switch entities from correct testing triplets resulting in a total of 2×#Test triplets with equal number of positive and negative examples. [sent-222, score-0.552]
</p><p>77 In particular, we constrain the entities from the possible answer set for Freebase by only allowing entities in a position if they appeared in that position in the dataset. [sent-223, score-0.604]
</p><p>78 Reasoning about some relations is more difﬁcult than others, for instance, the relation (dramatic art, domain region, closed circuit television) is much more vague than the relation (missouri, subordinate instance of, river). [sent-265, score-0.524]
</p><p>79 We can see that the two easiest relations for reasoning are gender and nationality, and the two most difﬁcult ones are institution and cause of death. [sent-269, score-0.335]
</p><p>80 Intuitively, we can infer the gender and nationality from the name, location, or profession of a person, but we hardly infer a person’s cause of death from all other information. [sent-270, score-0.358]
</p><p>81 We now analyze the choice of entity representations and also the inﬂuence of word initializations. [sent-271, score-0.815]
</p><p>82 4, we compare training entity vectors (E ∈ Rd×NE ) and training word vectors (E ∈ Rd×NW ), where an entity vector is computed as the average of word vectors. [sent-274, score-1.732]
</p><p>83 Furthermore, we compare random initialization and unsupervised initialization for training word vectors. [sent-275, score-0.381]
</p><p>84 In summary, we explore three options: (i) entity vectors (EV); (ii) randomly initialized word vectors (WV); (iii) word vectors initialized with unsupervised word vectors (WV-init). [sent-276, score-1.662]
</p><p>85 We observe that word vectors consistently and signiﬁcantly outperform entity vectors on WordNet and this also holds in most cases on FreeBase. [sent-279, score-0.909]
</p><p>86 It might be because the entities in WordNet share more common words. [sent-280, score-0.302]
</p><p>87 Furthermore, we can see that most of the models have improved accuracy with initialization from unsupervised word vectors. [sent-281, score-0.369]
</p><p>88 Even with random initialization, our NTN model with training word vectors can reach high classiﬁcation accuracy: 84. [sent-282, score-0.356]
</p><p>89 WV-init: word vectors initialized with unsupervised semantic word vectors. [sent-294, score-0.69]
</p><p>90 The ranking is based on the scores that the neural tensor network assigns to each triplet. [sent-296, score-0.365]
</p><p>91 Given place male historian gender of birth is Florence and profession is historian, our gender model can accurately predict that Francesco Guicprofession Francesco ciardini’s gender is male and his nationality is Italy. [sent-301, score-0.717]
</p><p>92 Francesco Patrizi These might be infered from two pieces of com- place of birth Guicciardini mon knowledge: (i) Florence is a city of Italy; (ii) nationality nationality Francesco is a common name among males in Italy. [sent-302, score-0.423]
</p><p>93 For the ﬁrst fact, some relations Rosselli such as Matteo Rosselli has location Florence and nationality Italy exist in the knowledge base, which Figure 5: A reasoning example in Freemight imply the connection between Florence and Base. [sent-304, score-0.449]
</p><p>94 The dashed line denotes ian or male in the FreeBase, which might imply that word vector sharing. [sent-309, score-0.317]
</p><p>95 5 shows the beneﬁts from the sharing via word representations. [sent-312, score-0.287]
</p><p>96 Unlike previous models for predicting relationships using entities in knowledge bases, our model allows mediated interaction of entity vectors via a tensor. [sent-314, score-1.008]
</p><p>97 The model obtains the highest accuracy in terms of predicting unseen relationships between entities through reasoning inside a given knowledge base. [sent-315, score-0.608]
</p><p>98 We further show that by representing entities through their constituent words and initializing these word representations using readily available word vectors, performance of all models improves substantially. [sent-317, score-0.862]
</p><p>99 Potential path for future work include scaling the number of slices based on available training data for each relation and extending these ideas to reasoning over free text. [sent-318, score-0.328]
</p><p>100 Large vocabulary speech recognition using deep tensor neural networks. [sent-405, score-0.316]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('entity', 0.51), ('entities', 0.302), ('freebase', 0.257), ('word', 0.255), ('tensor', 0.246), ('wordnet', 0.214), ('nationality', 0.194), ('relation', 0.167), ('ntn', 0.146), ('bilinear', 0.115), ('triplet', 0.115), ('relations', 0.106), ('bengal', 0.097), ('triplets', 0.094), ('reasoning', 0.093), ('tiger', 0.09), ('bordes', 0.09), ('gender', 0.087), ('wv', 0.086), ('facts', 0.08), ('francesco', 0.079), ('layer', 0.075), ('vectors', 0.072), ('florence', 0.071), ('relationships', 0.068), ('relational', 0.067), ('wr', 0.066), ('relationship', 0.066), ('male', 0.062), ('bases', 0.059), ('scoring', 0.057), ('knowledge', 0.056), ('network', 0.051), ('tail', 0.051), ('accuracy', 0.051), ('representations', 0.05), ('subordinate', 0.05), ('dog', 0.049), ('institution', 0.049), ('leg', 0.049), ('patrizi', 0.049), ('picaso', 0.049), ('vhomo', 0.049), ('base', 0.048), ('socher', 0.048), ('semantic', 0.044), ('ev', 0.043), ('military', 0.043), ('profession', 0.043), ('hadamard', 0.042), ('manning', 0.041), ('slice', 0.04), ('neural', 0.04), ('rd', 0.04), ('slices', 0.039), ('person', 0.038), ('unseen', 0.038), ('nw', 0.037), ('pablo', 0.037), ('initialized', 0.035), ('database', 0.035), ('birth', 0.035), ('initialization', 0.034), ('weston', 0.034), ('instance', 0.034), ('death', 0.034), ('coreference', 0.032), ('erectus', 0.032), ('guicciardini', 0.032), ('historian', 0.032), ('rosselli', 0.032), ('sapiens', 0.032), ('somebody', 0.032), ('spouse', 0.032), ('vsapiens', 0.032), ('sharing', 0.032), ('collobert', 0.031), ('textual', 0.031), ('testing', 0.031), ('deep', 0.03), ('recursive', 0.03), ('training', 0.029), ('unsupervised', 0.029), ('score', 0.029), ('homo', 0.029), ('incompleteness', 0.029), ('nickel', 0.029), ('rnns', 0.029), ('yago', 0.029), ('br', 0.028), ('acl', 0.028), ('italy', 0.028), ('emnlp', 0.028), ('scores', 0.028), ('ut', 0.028), ('predict', 0.028), ('expressive', 0.027), ('forces', 0.027), ('corrupted', 0.027), ('matteo', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="263-tfidf-1" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>Author: Richard Socher, Danqi Chen, Christopher D. Manning, Andrew Ng</p><p>Abstract: Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the “Sumatran tiger” and “Bengal tiger.” Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively. 1</p><p>2 0.38354272 <a title="263-tfidf-2" href="./nips-2013-Translating_Embeddings_for_Modeling_Multi-relational_Data.html">336 nips-2013-Translating Embeddings for Modeling Multi-relational Data</a></p>
<p>Author: Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko</p><p>Abstract: We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples. 1</p><p>3 0.18183185 <a title="263-tfidf-3" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>Author: Andriy Mnih, Koray Kavukcuoglu</p><p>Abstract: Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and ﬁnd that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones. 1</p><p>4 0.17736188 <a title="263-tfidf-4" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>Author: Bernardino Romera-Paredes, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves signiﬁcantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable. 1</p><p>5 0.16079763 <a title="263-tfidf-5" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>Author: Richard Socher, Milind Ganjoo, Christopher D. Manning, Andrew Ng</p><p>Abstract: This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually deﬁned semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the ﬁrst gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes’ accuracy high. 1</p><p>6 0.15314306 <a title="263-tfidf-6" href="./nips-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.html">96 nips-2013-Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>7 0.15258385 <a title="263-tfidf-7" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>8 0.13511132 <a title="263-tfidf-8" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>9 0.12290819 <a title="263-tfidf-9" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>10 0.11406036 <a title="263-tfidf-10" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>11 0.10954326 <a title="263-tfidf-11" href="./nips-2013-Low-Rank_Matrix_and_Tensor_Completion_via_Adaptive_Sampling.html">179 nips-2013-Low-Rank Matrix and Tensor Completion via Adaptive Sampling</a></p>
<p>12 0.10196126 <a title="263-tfidf-12" href="./nips-2013-Multilinear_Dynamical_Systems_for_Tensor_Time_Series.html">203 nips-2013-Multilinear Dynamical Systems for Tensor Time Series</a></p>
<p>13 0.087888815 <a title="263-tfidf-13" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>14 0.084413283 <a title="263-tfidf-14" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>15 0.078791425 <a title="263-tfidf-15" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>16 0.078484878 <a title="263-tfidf-16" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>17 0.075301446 <a title="263-tfidf-17" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>18 0.073992223 <a title="263-tfidf-18" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>19 0.06487871 <a title="263-tfidf-19" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>20 0.064050853 <a title="263-tfidf-20" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.099), (2, -0.056), (3, 0.089), (4, 0.098), (5, -0.265), (6, 0.041), (7, -0.02), (8, 0.115), (9, 0.033), (10, -0.023), (11, -0.048), (12, -0.055), (13, -0.019), (14, 0.016), (15, -0.05), (16, 0.166), (17, 0.114), (18, 0.03), (19, 0.145), (20, -0.009), (21, -0.097), (22, -0.08), (23, 0.062), (24, 0.081), (25, 0.191), (26, -0.055), (27, 0.144), (28, 0.033), (29, 0.081), (30, -0.07), (31, -0.165), (32, 0.149), (33, 0.091), (34, -0.054), (35, -0.016), (36, -0.087), (37, 0.033), (38, 0.049), (39, 0.048), (40, -0.048), (41, -0.086), (42, -0.096), (43, 0.059), (44, -0.056), (45, -0.065), (46, -0.091), (47, 0.066), (48, 0.065), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96202987 <a title="263-lsi-1" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>Author: Richard Socher, Danqi Chen, Christopher D. Manning, Andrew Ng</p><p>Abstract: Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the “Sumatran tiger” and “Bengal tiger.” Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively. 1</p><p>2 0.87149602 <a title="263-lsi-2" href="./nips-2013-Translating_Embeddings_for_Modeling_Multi-relational_Data.html">336 nips-2013-Translating Embeddings for Modeling Multi-relational Data</a></p>
<p>Author: Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko</p><p>Abstract: We consider the problem of embedding entities and relationships of multirelational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples. 1</p><p>3 0.76980585 <a title="263-lsi-3" href="./nips-2013-Learning_word_embeddings_efficiently_with_noise-contrastive_estimation.html">172 nips-2013-Learning word embeddings efficiently with noise-contrastive estimation</a></p>
<p>Author: Andriy Mnih, Koray Kavukcuoglu</p><p>Abstract: Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-theart method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and ﬁnd that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones. 1</p><p>4 0.72609007 <a title="263-lsi-4" href="./nips-2013-Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality.html">96 nips-2013-Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>Author: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, Jeff Dean</p><p>Abstract: The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for ﬁnding phrases in text, and show that learning good vector representations for millions of phrases is possible.</p><p>5 0.52347392 <a title="263-lsi-5" href="./nips-2013-Learning_and_using_language_via_recursive_pragmatic_reasoning_about_other_agents.html">164 nips-2013-Learning and using language via recursive pragmatic reasoning about other agents</a></p>
<p>Author: Nathaniel J. Smith, Noah Goodman, Michael Frank</p><p>Abstract: Language users are remarkably good at making inferences about speakers’ intentions in context, and children learning their native language also display substantial skill in acquiring the meanings of unknown words. These two cases are deeply related: Language users invent new terms in conversation, and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used. While pragmatic inference and word learning have both been independently characterized in probabilistic terms, no current work uniﬁes these two. We describe a model in which language learners assume that they jointly approximate a shared, external lexicon and reason recursively about the goals of others in using this lexicon. This model captures phenomena in word learning and pragmatic inference; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings. 1</p><p>6 0.47420883 <a title="263-lsi-6" href="./nips-2013-Multilinear_Dynamical_Systems_for_Tensor_Time_Series.html">203 nips-2013-Multilinear Dynamical Systems for Tensor Time Series</a></p>
<p>7 0.4697777 <a title="263-lsi-7" href="./nips-2013-Zero-Shot_Learning_Through_Cross-Modal_Transfer.html">356 nips-2013-Zero-Shot Learning Through Cross-Modal Transfer</a></p>
<p>8 0.46226102 <a title="263-lsi-8" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>9 0.40647638 <a title="263-lsi-9" href="./nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</a></p>
<p>10 0.39545247 <a title="263-lsi-10" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>11 0.36822969 <a title="263-lsi-11" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>12 0.36625487 <a title="263-lsi-12" href="./nips-2013-Learning_Hidden_Markov_Models_from_Non-sequence_Data_via_Tensor_Decomposition.html">155 nips-2013-Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition</a></p>
<p>13 0.36405057 <a title="263-lsi-13" href="./nips-2013-A_New_Convex_Relaxation_for_Tensor_Completion.html">11 nips-2013-A New Convex Relaxation for Tensor Completion</a></p>
<p>14 0.35669333 <a title="263-lsi-14" href="./nips-2013-A_Novel_Two-Step_Method_for_Cross_Language_Representation_Learning.html">12 nips-2013-A Novel Two-Step Method for Cross Language Representation Learning</a></p>
<p>15 0.33410245 <a title="263-lsi-15" href="./nips-2013-Documents_as_multiple_overlapping_windows_into_grids_of_counts.html">98 nips-2013-Documents as multiple overlapping windows into grids of counts</a></p>
<p>16 0.33137059 <a title="263-lsi-16" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>17 0.32896015 <a title="263-lsi-17" href="./nips-2013-Unsupervised_Structure_Learning_of_Stochastic_And-Or_Grammars.html">343 nips-2013-Unsupervised Structure Learning of Stochastic And-Or Grammars</a></p>
<p>18 0.3123191 <a title="263-lsi-18" href="./nips-2013-Exact_and_Stable_Recovery_of_Pairwise_Interaction_Tensors.html">113 nips-2013-Exact and Stable Recovery of Pairwise Interaction Tensors</a></p>
<p>19 0.30040386 <a title="263-lsi-19" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>20 0.28721303 <a title="263-lsi-20" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.021), (33, 0.131), (34, 0.077), (41, 0.021), (43, 0.011), (49, 0.048), (56, 0.073), (70, 0.037), (72, 0.295), (74, 0.011), (85, 0.049), (89, 0.027), (93, 0.082), (95, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77425539 <a title="263-lda-1" href="./nips-2013-Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion.html">263 nips-2013-Reasoning With Neural Tensor Networks for Knowledge Base Completion</a></p>
<p>Author: Richard Socher, Danqi Chen, Christopher D. Manning, Andrew Ng</p><p>Abstract: Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the “Sumatran tiger” and “Bengal tiger.” Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively. 1</p><p>2 0.74960679 <a title="263-lda-2" href="./nips-2013-Gaussian_Process_Conditional_Copulas_with_Applications_to_Financial_Time_Series.html">126 nips-2013-Gaussian Process Conditional Copulas with Applications to Financial Time Series</a></p>
<p>Author: José Miguel Hernández-Lobato, James R. Lloyd, Daniel Hernández-Lobato</p><p>Abstract: The estimation of dependencies between multiple variables is a central problem in the analysis of ﬁnancial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be inaccurate when there are covariates that could have a large inﬂuence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other timevarying copula methods. 1</p><p>3 0.6930306 <a title="263-lda-3" href="./nips-2013-Parametric_Task_Learning.html">244 nips-2013-Parametric Task Learning</a></p>
<p>Author: Ichiro Takeuchi, Tatsuya Hongo, Masashi Sugiyama, Shinichi Nakajima</p><p>Abstract: We introduce an extended formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle inﬁnitely many tasks parameterized by a continuous parameter. Our key ﬁnding is that, for a certain class of PTL problems, the path of the optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression. We demonstrate the advantage of our approach in these scenarios.</p><p>4 0.66342342 <a title="263-lda-4" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>Author: Dan Rosenbaum, Daniel Zoran, Yair Weiss</p><p>Abstract: Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical ﬂow to learn the local statistics of optical ﬂow and compare the learned models to prior models assumed by computer vision researchers. We ﬁnd that a Gaussian mixture model (GMM) with 64 components provides a signiﬁcantly better model for local ﬂow statistics when compared to commonly used models. We investigate the source of the GMM’s success and show it is related to an explicit representation of ﬂow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical ﬂow. In accordance with the assumptions often made in computer vision, the model learns that ﬂow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the beneﬁt of conditioning ﬂow estimation on the local intensity pattern is marginal. 1</p><p>5 0.64395708 <a title="263-lda-5" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>Author: David Carlson, Vinayak Rao, Joshua T. Vogelstein, Lawrence Carin</p><p>Abstract: With simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons. In electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons). We extend previous Bayesian nonparametric models of neural spiking to jointly detect and cluster neurons using a Gamma process model. Importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-theart. Via exploratory data analysis—using data with partial ground truth as well as two novel data sets—we ﬁnd several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) detecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using multiple channels. We hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain. 1</p><p>6 0.61723739 <a title="263-lda-6" href="./nips-2013-Translating_Embeddings_for_Modeling_Multi-relational_Data.html">336 nips-2013-Translating Embeddings for Modeling Multi-relational Data</a></p>
<p>7 0.55260855 <a title="263-lda-7" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>8 0.54824376 <a title="263-lda-8" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>9 0.54792279 <a title="263-lda-9" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>10 0.54768354 <a title="263-lda-10" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>11 0.54648304 <a title="263-lda-11" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>12 0.54566604 <a title="263-lda-12" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>13 0.54480034 <a title="263-lda-13" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>14 0.54459119 <a title="263-lda-14" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>15 0.54321688 <a title="263-lda-15" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>16 0.54316062 <a title="263-lda-16" href="./nips-2013-Decision_Jungles%3A_Compact_and_Rich_Models_for_Classification.html">82 nips-2013-Decision Jungles: Compact and Rich Models for Classification</a></p>
<p>17 0.54154062 <a title="263-lda-17" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>18 0.54071784 <a title="263-lda-18" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<p>19 0.54062045 <a title="263-lda-19" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>20 0.53953224 <a title="263-lda-20" href="./nips-2013-Analyzing_the_Harmonic_Structure_in_Graph-Based_Learning.html">35 nips-2013-Analyzing the Harmonic Structure in Graph-Based Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
