<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-303" href="#">nips2013-303</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</h1>
<br/><p>Source: <a title="nips-2013-303-pdf" href="http://papers.nips.cc/paper/4891-sparse-overlapping-sets-lasso-for-multitask-learning-and-its-application-to-fmri-analysis.pdf">pdf</a></p><p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>Reference: <a title="nips-2013-303-reference" href="../nips2013_reference/nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. [sent-14, score-0.248]
</p><p>2 Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. [sent-15, score-0.432]
</p><p>3 1  Introduction  Multitask learning exploits the relationships between several learning tasks in order to improve performance, which is especially useful if a common subset of features are useful for all tasks at hand. [sent-16, score-0.158]
</p><p>4 The group lasso (Glasso) [19, 8] is naturally suited for this situation: if a feature is selected for one task, then it is selected for all tasks. [sent-17, score-0.492]
</p><p>5 Suppose that the available features can be organized into overlapping subsets according to a notion of similarity, and that the features useful in one task are similar, but not necessarily identical, to those best suited for other tasks. [sent-19, score-0.287]
</p><p>6 In other words, a feature that is useful for one task suggests that the subset it belongs to may contain the features useful in other tasks (Figure 1). [sent-20, score-0.159]
</p><p>7 In this paper, we introduce the sparse overlapping sets lasso (SOSlasso), a convex program to recover the sparsity patterns corresponding to the situations explained above. [sent-21, score-0.586]
</p><p>8 SOSlasso generalizes lasso [16] and Glasso, effectively spanning the range between these two well-known procedures. [sent-22, score-0.274]
</p><p>9 SOSlasso is capable of exploiting the similarities between useful features across tasks, but unlike Glasso it does not force different tasks to use exactly the same features. [sent-23, score-0.146]
</p><p>10 It produces sparse solutions, but unlike lasso it encourages similar patterns of sparsity across tasks. [sent-24, score-0.51]
</p><p>11 Sparse group lasso [14] is a special case of SOSlasso that only applies to disjoint sets, a signiﬁcant limitation when features cannot be easily partitioned, as is the case of our motivating example in fMRI. [sent-25, score-0.476]
</p><p>12 The main contribution of this paper is a theoretical analysis of SOSlasso, which also covers sparse group lasso as a special case (further differentiating us from [14]). [sent-26, score-0.499]
</p><p>13 The performance of SOSlasso is analyzed, error 1  bounds are derived for general loss functions, and its consistency is shown for squared error loss. [sent-27, score-0.124]
</p><p>14 Experiments with real and synthetic data demonstrate the advantages of SOSlasso relative to lasso and Glasso. [sent-28, score-0.274]
</p><p>15 1  Sparse Overlapping Sets  SOSlasso encourages sparsity patterns that are similar, but not identical, across tasks. [sent-30, score-0.169]
</p><p>16 This is accomplished by decomposing the features of each task into groups G1 . [sent-31, score-0.2]
</p><p>17 Conceptually, SOSlasso ﬁrst selects subsets that are most useful for all tasks, and then identiﬁes a unique sparse solution for each task drawing only from features in the selected subsets. [sent-35, score-0.189]
</p><p>18 Figure 1 shows an example of the patterns that typically arise in sparse multitask learning applications, where rows indicate features and columns correspond to tasks. [sent-37, score-0.28]
</p><p>19 Past work has focused on recovering variables that exhibit within and across group sparsity, when the groups do not overlap [14], ﬁnding application in genetics, handwritten character recognition [15] and climate and oceanography [2]. [sent-38, score-0.355]
</p><p>20 Along related lines, the exclusive lasso [21] can be used when it is explicitly known that variables in certain sets are negatively correlated. [sent-39, score-0.293]
</p><p>21 (a) Sparse  (b) Group sparse  (c) Group sparse plus sparse  (d) Group sparse and sparse  Figure 1: A comparison of different sparsity patterns. [sent-40, score-0.395]
</p><p>22 An example of group sparse patterns promoted by Glasso [19] is shown in (b). [sent-42, score-0.289]
</p><p>23 Cognitive Neuroscientists are interested in identifying the patterns of activity associated with different cognitive states, and construct a model of the activity that accurately predicts the cognitive state evoked on novel trials. [sent-47, score-0.162]
</p><p>24 However, the speciﬁc patterns of activity in these regions will vary, both because neural codes can vary by participant [4] and because brains vary in size and shape, rendering neuroanatomy only an approximate guide to the location of relevant information across individuals. [sent-49, score-0.229]
</p><p>25 In short, a voxel useful for prediction in one participant suggests the general anatomical neighborhood where useful voxels may be found, but not the precise voxel. [sent-50, score-0.362]
</p><p>26 While logistic Glasso [17], lasso [13], and the elastic net penalty [12] have been applied to neuroimaging data, these methods do not exclusively take into account both the common macrostructure and the differences in microstructure across brains. [sent-51, score-0.347]
</p><p>27 In Section 4, we specialize the problem to the multitask linear regression setting (2), and derive consistency rates for the same, leveraging ideas from [9]. [sent-57, score-0.164]
</p><p>28 We assume there exists a vector xt ∈ Rp such that measurements obtained are of the form yt = Φt xt + ηt ηt ∼ N (0, σ 2 I). [sent-66, score-0.142]
</p><p>29 Suppose we are given M (possibly overlapping) groups ˜ ˜ ˜ ˜ ˜ G = {G1 , G2 , . [sent-71, score-0.129]
</p><p>30 These groups contain sets of “similar” features, the notion of similarity being application dependent. [sent-78, score-0.155]
</p><p>31 We assume that all but k M groups are identically zero. [sent-79, score-0.129]
</p><p>32 Among the active groups, we further assume that at most only a fraction α ∈ (0, 1) of the coefﬁcients per group are non zero. [sent-80, score-0.232]
</p><p>33 , GM } to be the ˜ set of groups deﬁned on RT p formed by aggregating the rows of X that were originally in G, so that x is composed of groups G ∈ G. [sent-96, score-0.258]
</p><p>34 We next deﬁne a regularizer h that promotes sparsity both within and across overlapping sets of similar features: h(x) = inf W  (αG wG  2  + wG 1 ) s. [sent-97, score-0.291]
</p><p>35 G∈G  wG = x  (3)  G∈G  where the αG > 0 are constants that balance the tradeoff between the group norms and the 1 norm. [sent-99, score-0.158]
</p><p>36 Each wG has the same size as x, with support restricted to the variables indexed by group G. [sent-100, score-0.181]
</p><p>37 W is a set of vectors, where each vector has a support restricted to one of the groups G ∈ G: W = {wG ∈ RT p | [wG ]i = 0 if i ∈ G} / where [wG ]i is the ith coefﬁcient of wG . [sent-101, score-0.129]
</p><p>38 As the αG → ∞ the 1 term becomes redundant, reducing h(x) to the overlapping group lasso penalty introduced in [5], and studied in [10, 11]. [sent-106, score-0.553]
</p><p>39 When the αG → 0, the overlapping group lasso term vanishes and h(x) reduces to the lasso penalty. [sent-107, score-0.827]
</p><p>40 The example in Table 1 gives an insight into the kind of sparsity patterns preferred by the function h(x). [sent-115, score-0.124]
</p><p>41 3  Consider 3 instances of x ∈ R10 , and the corresponding group lasso, 1 , and h(x) function values. [sent-117, score-0.158]
</p><p>42 h(x) is smallest when the support set is sparse within groups, and also when only one of the two groups is selected. [sent-119, score-0.196]
</p><p>43 The 1 norm does not take into account sparsity across groups, while the group lasso norm does not take into account sparsity within groups. [sent-120, score-0.693]
</p><p>44 To solve (1) and (2) with the regularizer proposed in (3), we use the covariate duplication method of [5], to reduce the problem to a non overlapping sparse group lasso problem. [sent-121, score-0.715]
</p><p>45 2 A norm h(·) is decomposable with respect to the subspace pair sA ⊂ sB if h(a + b) = h(a) + h(b) ∀a ∈ sA, b ∈ sB ⊥ . [sent-141, score-0.118]
</p><p>46 3 Let x ∈ Rp be a vector that can be decomposed into (overlapping) groups with withingroup sparsity. [sent-143, score-0.147]
</p><p>47 4 Given a subspace sB, the subspace compatibility constant with respect to a norm is given by h(x) ∀x ∈ sB\{0} Ψ(B) = sup x Lemma 3. [sent-150, score-0.122]
</p><p>48 Suppose the maximum group size is B, and also assume that a fraction α ∈ (0, 1) of the coordinates in each active group is non zero. [sent-152, score-0.39]
</p><p>49 5) gives an upper bound on the subspace compatibility constant with respect to the 2 norm for the subspace indexed by the support of the vector, which is contained in the span of the union of groups in G . [sent-155, score-0.292]
</p><p>50 7 (Corollary 1 in [9]) Consider a convex and differentiable loss function such that RSC holds with constants κ and τ = 0 over (5), and a norm h(·) decomposable over sets sA and sB. [sent-161, score-0.118]
</p><p>51 For the optimization program in (1), using the parameter λn ≥ 2h∗ ( LΦ (x )), any optimal solution ˆ xλn to (1) satisﬁes 9λ2 xλn − x 2 ≤ n Ψ2 (sB) κ The result above shows a general bound on the error using the lasso with sparse overlapping sets. [sent-162, score-0.497]
</p><p>52 4  Consistency of SOSlasso with Squared Error Loss  We ﬁrst need to bound the dual norm of the gradient of the loss function, so as to bound λn . [sent-166, score-0.15]
</p><p>53 Our goal now is to ∗ ﬁnd an upper bound on the quantity h ( L), which from (4) is 1 1 max LG 2 = max ΦT η 2 G 2 G∈G 2n G∈G where ΦG is the matrix Φ restricted to the columns indexed by the group G. [sent-172, score-0.241]
</p><p>54 2 Consider the loss function L := 2n t=1 yt − Φt xt 2 = 2n y − Φx 2 , with the Φt s deterministic and the measurements corrupted with AWGN of variance σ 2 . [sent-189, score-0.135]
</p><p>55 For the regularizer in (3), the dual norm of the gradient of the loss function is bounded as 2 σ 2 σm (log(M ) + T B) 4 n with probability at least 1 − c1 exp(−c2 n), for c1 , c2 > 0, and where σm = maxG∈G σmG  h∗ ( L)2 ≤  Proof Let γ ∼ χ2 |G| . [sent-190, score-0.161]
</p><p>56 We begin with the upper bound obtained for the dual norm of the regularizer T in (4): (i)  h∗ ( L)2 ≤  1 T 1 max Φ η 4 G∈G n G  2  ≤ 2  σ2 σ2 γ max mG 4 G∈G n2  2 2 σ 2 σm γ (iii) σ 2 σm 2 (cn − 1)2 T B ≤ max 2 ≤ c T B w. [sent-191, score-0.205]
</p><p>57 3 Suppose we obtain linear measurements of a sparse overlapping grouped matrix X ∈ Rp×T , corrupted by AWGN of variance σ 2 . [sent-200, score-0.236]
</p><p>58 Suppose the matrix X can be decomposed into M possible overlapping groups of maximum size B, out of which k are active. [sent-201, score-0.268]
</p><p>59 Consider the following vectorized SOSlasso multitask regression problem (2): x = arg min x  h(x) = inf W  1 y − Φx 2n  ( wG  2  2 2  + λn h(x) ,  + wG 1 ) s. [sent-203, score-0.13]
</p><p>60 From [9], we see that the convergence rate matches that of the group lasso, with an additional multiplicative factor α. [sent-210, score-0.158]
</p><p>61 This stems from the fact that the signal has a sparse structure “embedded” within a group sparse structure. [sent-211, score-0.292]
</p><p>62 Visualizing the optimization problem as that of solving a lasso within a group lasso framework lends some intuition into this result. [sent-212, score-0.727]
</p><p>63 Note that since α < 1, this bound is much smaller than that of the standard group lasso. [sent-213, score-0.176]
</p><p>64 1  Synthetic data, Gaussian Linear Regression  For T = 20 tasks, we deﬁne a N = 2002 element vector divided into M = 500 groups of size B = 6. [sent-215, score-0.129]
</p><p>65 Each group overlaps with its neighboring groups (G1 = {1, 2, . [sent-216, score-0.307]
</p><p>66 20 of these groups were activated uniformly at random, and populated from a uniform [−1, 1] distribution. [sent-229, score-0.129]
</p><p>67 The results of applying lasso, standard latent group lasso [5, 10], and our SOSlasso to these data are plotted in Figures 2(a), varying σ, α = 0. [sent-234, score-0.432]
</p><p>68 8  1  1−α  (b) Varying α  (c) Sample pattern  Figure 2: As the noise is increased (a), our proposed penalty function (SOSlasso) allows us to recover the true coefﬁcients more accurately than the group lasso (Glasso). [sent-253, score-0.432]
</p><p>69 Also, when alpha is large, the active groups are not sparse, and the standard overlapping group lasso outperforms the other methods. [sent-254, score-0.708]
</p><p>70 However, as α reduces, the method we propose outperforms the group lasso (b). [sent-255, score-0.432]
</p><p>71 (c) shows a toy sparsity pattern, with different colors denoting different overlapping groups 5. [sent-256, score-0.31]
</p><p>72 6 subjects made judgements that involved processing 40 sentences and 40 pictures while their brains were scanned in half second intervals using fMRI1 . [sent-258, score-0.14]
</p><p>73 We assessed whether SOSlasso could leverage this cross-individual consistency to aid in the discovery of predictive voxels without requiring expert pre-selection of ROIs, or data reduction, or any alignment of voxels beyond that existing in the raw data. [sent-263, score-0.429]
</p><p>74 Rather, we aim to discover a group sparsity pattern that suggests a similar set of voxels in all subjects, before optimizing a separate solution for each individual. [sent-265, score-0.415]
</p><p>75 If SOSlasso can exploit cross-individual anatomical similarity from this raw, coarsely-aligned data, it should show reduced cross-validation error relative to the lasso applied separately to each individual. [sent-266, score-0.363]
</p><p>76 If the solution is sparse within groups and highly variable across individuals, SOSlasso should show reduced cross-validation error relative to Glasso. [sent-267, score-0.258]
</p><p>77 (a) Aggregated sparsity patterns for a single brain slice. [sent-275, score-0.152]
</p><p>78 24  lasso  Glasso SOSlasso  SOSlasso  (b)  (a) Picture only  Sentence only  Picture and Sentence  (c)  Method % ROI t(5) , p lasso 46. [sent-286, score-0.548]
</p><p>79 31 Table 2: Proportion of selected voxels in the 7 relevant ROIS aggregated over subjects, and corresponding two-tailed signiﬁcance levels for the contrast of lasso and Glasso to SOSlasso. [sent-293, score-0.516]
</p><p>80 We trained 3 classiﬁers using 4-fold cross validation to select the regularization parameter, considering all available voxels without preselection. [sent-294, score-0.197]
</p><p>81 We group regions of 5 × 5 × 1 voxels and considered overlapping groups “shifted” by 2 voxels in the ﬁrst 2 dimensions. [sent-295, score-0.834]
</p><p>82 2 Figure 3(b) shows the individual error rates across the 6 subjects for the three methods. [sent-296, score-0.129]
</p><p>83 004 two-tailed), showing that the method can exploit anatomical similarity across subjects to learn a better classiﬁer for each. [sent-301, score-0.167]
</p><p>84 SOSlasso also showed signiﬁcantly lower error rates than glasso (31. [sent-302, score-0.247]
</p><p>85 03 two-tailed), suggesting that the signal is sparse within selected regions and variable across subjects. [sent-305, score-0.164]
</p><p>86 Figure 3(a) presents a sample of the the sparsity patterns obtained from the different methods, aggregated over all subjects. [sent-306, score-0.149]
</p><p>87 Red points indicate voxels that contributed positively to picture classiﬁcation in at least one subject, but never to sentences; Blue points have the opposite interpretation. [sent-307, score-0.282]
</p><p>88 Purple points indicate voxels that contributed positively to picture and sentence classiﬁcation in different subjects. [sent-308, score-0.332]
</p><p>89 First, the Glasso solution is fairly dense, with many voxels signaling both picture and sentence across subjects. [sent-311, score-0.34]
</p><p>90 Second, the lasso solution is less sparse than the SOSlasso because it allows any task-correlated voxel to be selected. [sent-314, score-0.373]
</p><p>91 It leads to a higher cross-validation error, indicating that the ungrouped voxels are inferior predictors (Figure 3(b)). [sent-315, score-0.197]
</p><p>92 To assess how well these clusters align with the anatomical regions thought a-priori to be involved in sentence and picture representation, we calculated the proportion of selected voxels falling within the 7 ROIs identiﬁed by [18] as relevant to the classiﬁcation task (Table 2). [sent-317, score-0.463]
</p><p>93 For SOSlasso an average of 70% of identiﬁed voxels fell within these ROIs, signiﬁcantly more than for lasso or Glasso. [sent-318, score-0.471]
</p><p>94 6  Conclusions and Extensions  We have introduced SOSlasso, a function that recovers sparsity patterns that are a hybrid of overlapping group sparse and sparse patterns when used as a regularizer in convex programs, and proved its theoretical convergence rates when minimizing least squares. [sent-319, score-0.665]
</p><p>95 The SOSlasso succeeds in a multitask fMRI analysis, where it both makes better inferences and discovers more theoretically plausible brain regions that lasso and Glasso. [sent-320, score-0.421]
</p><p>96 Future work involves experimenting with different parameters for the group and l1 penalties, and using other similarity groupings, such as functional connectivity in fMRI. [sent-321, score-0.184]
</p><p>97 2 The irregular group size compensates for voxels being larger and scanner coverage being smaller in the z-dimension (only 8 slices relative to 64 in the x- and y-dimensions). [sent-322, score-0.375]
</p><p>98 Sparse group lasso for regression on land climate variables. [sent-331, score-0.48]
</p><p>99 Group lasso with overlaps: The latent group lasso approach. [sent-388, score-0.706]
</p><p>100 Sparse logistic regression for whole brain classiﬁcation of fmri data. [sent-401, score-0.178]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('soslasso', 0.687), ('wg', 0.359), ('lasso', 0.274), ('glasso', 0.213), ('voxels', 0.197), ('group', 0.158), ('groups', 0.129), ('overlapping', 0.121), ('sb', 0.102), ('ug', 0.101), ('rois', 0.101), ('fmri', 0.097), ('multitask', 0.087), ('sa', 0.079), ('sparse', 0.067), ('patterns', 0.064), ('mg', 0.063), ('sparsity', 0.06), ('maxg', 0.053), ('subjects', 0.05), ('sentence', 0.05), ('non', 0.048), ('norm', 0.048), ('picture', 0.048), ('regularizer', 0.047), ('anatomical', 0.046), ('across', 0.045), ('xt', 0.044), ('features', 0.044), ('awgn', 0.043), ('brains', 0.04), ('rsc', 0.038), ('subspace', 0.037), ('loss', 0.037), ('xg', 0.036), ('consistency', 0.035), ('decomposable', 0.033), ('malsar', 0.033), ('regions', 0.032), ('voxel', 0.032), ('mse', 0.032), ('useful', 0.031), ('gm', 0.031), ('measurements', 0.03), ('scanned', 0.029), ('sos', 0.029), ('dual', 0.029), ('lemma', 0.029), ('rp', 0.028), ('rt', 0.028), ('brain', 0.028), ('obozinski', 0.028), ('logistic', 0.028), ('squares', 0.027), ('roi', 0.027), ('task', 0.027), ('arxiv', 0.026), ('similarity', 0.026), ('active', 0.026), ('cognitive', 0.026), ('tasks', 0.026), ('aggregated', 0.025), ('regression', 0.025), ('participant', 0.025), ('yt', 0.024), ('align', 0.024), ('activity', 0.023), ('indexed', 0.023), ('climate', 0.023), ('purple', 0.023), ('vg', 0.023), ('retained', 0.021), ('max', 0.021), ('notations', 0.021), ('sentences', 0.021), ('lends', 0.021), ('preprint', 0.02), ('selected', 0.02), ('overlaps', 0.02), ('slices', 0.02), ('nowak', 0.02), ('suited', 0.02), ('neuroimage', 0.019), ('exclusive', 0.019), ('coef', 0.019), ('positively', 0.019), ('supp', 0.019), ('cients', 0.019), ('convexity', 0.019), ('proportion', 0.019), ('bound', 0.018), ('decomposed', 0.018), ('indicate', 0.018), ('squared', 0.018), ('inf', 0.018), ('rao', 0.018), ('grouped', 0.018), ('error', 0.017), ('rates', 0.017), ('jacob', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="303-tfidf-1" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>2 0.17750818 <a title="303-tfidf-2" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>3 0.15702257 <a title="303-tfidf-3" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>Author: Mohsen Bayati, Murat A. Erdogdu, Andrea Montanari</p><p>Abstract: We study the fundamental problems of variance and risk estimation in high dimensional statistical modeling. In particular, we consider the problem of learning a coefﬁcient vector θ0 ∈ Rp from noisy linear observations y = Xθ0 + w ∈ Rn (p > n) and the popular estimation procedure of solving the 1 -penalized least squares objective known as the LASSO or Basis Pursuit DeNoising (BPDN). In this context, we develop new estimators for the 2 estimation risk θ − θ0 2 and the variance of the noise when distributions of θ0 and w are unknown. These can be used to select the regularization parameter optimally. Our approach combines Stein’s unbiased risk estimate [Ste81] and the recent results of [BM12a][BM12b] on the analysis of approximate message passing and the risk of LASSO. We establish high-dimensional consistency of our estimators for sequences of matrices X of increasing dimensions, with independent Gaussian entries. We establish validity for a broader class of Gaussian designs, conditional on a certain conjecture from statistical physics. To the best of our knowledge, this result is the ﬁrst that provides an asymptotically consistent risk estimator for the LASSO solely based on data. In addition, we demonstrate through simulations that our variance estimation outperforms several existing methods in the literature. 1</p><p>4 0.12735887 <a title="303-tfidf-4" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>5 0.1135815 <a title="303-tfidf-5" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>6 0.11185187 <a title="303-tfidf-6" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>7 0.10323607 <a title="303-tfidf-7" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>8 0.10261957 <a title="303-tfidf-8" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>9 0.096176125 <a title="303-tfidf-9" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>10 0.09354452 <a title="303-tfidf-10" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>11 0.092732966 <a title="303-tfidf-11" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>12 0.087811992 <a title="303-tfidf-12" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>13 0.073193595 <a title="303-tfidf-13" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>14 0.068575643 <a title="303-tfidf-14" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>15 0.065479271 <a title="303-tfidf-15" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>16 0.063203506 <a title="303-tfidf-16" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>17 0.055521775 <a title="303-tfidf-17" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>18 0.051859975 <a title="303-tfidf-18" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>19 0.051584139 <a title="303-tfidf-19" href="./nips-2013-Speeding_up_Permutation_Testing_in_Neuroimaging.html">306 nips-2013-Speeding up Permutation Testing in Neuroimaging</a></p>
<p>20 0.050401602 <a title="303-tfidf-20" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.144), (1, 0.057), (2, 0.055), (3, 0.042), (4, -0.027), (5, 0.034), (6, -0.064), (7, 0.036), (8, -0.13), (9, 0.001), (10, 0.063), (11, -0.061), (12, -0.048), (13, -0.175), (14, -0.165), (15, -0.093), (16, 0.051), (17, -0.045), (18, 0.017), (19, -0.054), (20, 0.06), (21, 0.037), (22, -0.033), (23, 0.059), (24, 0.093), (25, 0.013), (26, 0.048), (27, 0.017), (28, -0.045), (29, 0.069), (30, -0.029), (31, -0.003), (32, 0.084), (33, -0.077), (34, -0.039), (35, 0.063), (36, -0.042), (37, 0.005), (38, -0.023), (39, -0.034), (40, -0.051), (41, 0.086), (42, 0.022), (43, 0.024), (44, -0.013), (45, -0.046), (46, 0.074), (47, 0.048), (48, 0.051), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93759435 <a title="303-lsi-1" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>Author: Nikhil Rao, Christopher Cox, Rob Nowak, Timothy T. Rogers</p><p>Abstract: Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classiﬁed using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso. 1</p><p>2 0.87085658 <a title="303-lsi-2" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><p>3 0.76136661 <a title="303-lsi-3" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><p>4 0.7290746 <a title="303-lsi-4" href="./nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</a></p>
<p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><p>5 0.64767718 <a title="303-lsi-5" href="./nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</a></p>
<p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><p>6 0.64700168 <a title="303-lsi-6" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>7 0.63997018 <a title="303-lsi-7" href="./nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</a></p>
<p>8 0.59942108 <a title="303-lsi-8" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>9 0.57920605 <a title="303-lsi-9" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>10 0.53026634 <a title="303-lsi-10" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>11 0.52711201 <a title="303-lsi-11" href="./nips-2013-Lasso_Screening_Rules_via_Dual_Polytope_Projection.html">147 nips-2013-Lasso Screening Rules via Dual Polytope Projection</a></p>
<p>12 0.49860713 <a title="303-lsi-12" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>13 0.43359435 <a title="303-lsi-13" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>14 0.40280527 <a title="303-lsi-14" href="./nips-2013-Nonparametric_Multi-group_Membership_Model_for_Dynamic_Networks.html">213 nips-2013-Nonparametric Multi-group Membership Model for Dynamic Networks</a></p>
<p>15 0.40250918 <a title="303-lsi-15" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>16 0.38729131 <a title="303-lsi-16" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>17 0.37599406 <a title="303-lsi-17" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>18 0.37271145 <a title="303-lsi-18" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>19 0.36888671 <a title="303-lsi-19" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>20 0.36011234 <a title="303-lsi-20" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.039), (33, 0.149), (34, 0.12), (41, 0.017), (49, 0.342), (56, 0.086), (70, 0.024), (85, 0.036), (89, 0.039), (93, 0.047), (95, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97183573 <a title="303-lda-1" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>Author: Jasper Snoek, Richard Zemel, Ryan P. Adams</p><p>Abstract: Point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials. However, the most common neural point process models, the Poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons. We develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction. We show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons. The model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena. Applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data. 1</p><p>2 0.9661144 <a title="303-lda-2" href="./nips-2013-Synthesizing_Robust_Plans_under_Incomplete_Domain_Models.html">323 nips-2013-Synthesizing Robust Plans under Incomplete Domain Models</a></p>
<p>Author: Tuan A. Nguyen, Subbarao Kambhampati, Minh Do</p><p>Abstract: Most current planners assume complete domain models and focus on generating correct plans. Unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models. While domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete. In such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain. In this paper, we ﬁrst introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model. We then show an approach to compiling the problem of ﬁnding robust plans to the conformant probabilistic planning problem, and present experimental results with Probabilistic-FF planner. 1</p><p>3 0.94507819 <a title="303-lda-3" href="./nips-2013-Relevance_Topic_Model_for_Unstructured_Social_Group_Activity_Recognition.html">274 nips-2013-Relevance Topic Model for Unstructured Social Group Activity Recognition</a></p>
<p>Author: Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan</p><p>Abstract: Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a “relevance topic model” for jointly learning meaningful mid-level representations upon bagof-words (BoW) video representations and a classiﬁer with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectiﬁed linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efﬁcient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classiﬁcation accuracy, particularly in the case of a very small number of labeled training videos. 1</p><p>4 0.9282465 <a title="303-lda-4" href="./nips-2013-Recurrent_linear_models_of_simultaneously-recorded_neural___populations.html">266 nips-2013-Recurrent linear models of simultaneously-recorded neural   populations</a></p>
<p>Author: Marius Pachitariu, Biljana Petreska, Maneesh Sahani</p><p>Abstract: Population neural recordings with long-range temporal structure are often best understood in terms of a common underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever-larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. We describe a new, scalable approach to discovering low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. We formulate the Recurrent Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood calculation for latent linear dynamical systems to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded generalised-linear model (CGLM) to capture low-dimensional instantaneous correlations in neural populations. The CGLM describes the cortical recordings better than either Ising or Gaussian models and, like the RLM, can be ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a lowrank Gaussian model, in this case factor analysis. The computational tractability of the RLM and CGLM allow both to scale to very high-dimensional neural data. 1</p><p>5 0.88826269 <a title="303-lda-5" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>6 0.87201101 <a title="303-lda-6" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>7 0.85761756 <a title="303-lda-7" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>same-paper 8 0.82513005 <a title="303-lda-8" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>9 0.80462605 <a title="303-lda-9" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>10 0.78276294 <a title="303-lda-10" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>11 0.7532075 <a title="303-lda-11" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>12 0.70516998 <a title="303-lda-12" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>13 0.70236027 <a title="303-lda-13" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>14 0.67599642 <a title="303-lda-14" href="./nips-2013-Perfect_Associative_Learning_with_Spike-Timing-Dependent_Plasticity.html">246 nips-2013-Perfect Associative Learning with Spike-Timing-Dependent Plasticity</a></p>
<p>15 0.6726771 <a title="303-lda-15" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>16 0.67235327 <a title="303-lda-16" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>17 0.67179745 <a title="303-lda-17" href="./nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</a></p>
<p>18 0.67122984 <a title="303-lda-18" href="./nips-2013-Neural_representation_of_action_sequences%3A_how_far_can_a_simple_snippet-matching_model_take_us%3F.html">208 nips-2013-Neural representation of action sequences: how far can a simple snippet-matching model take us?</a></p>
<p>19 0.66849983 <a title="303-lda-19" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>20 0.66830802 <a title="303-lda-20" href="./nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
