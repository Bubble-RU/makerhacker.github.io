<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-8" href="#">nips2013-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</h1>
<br/><p>Source: <a title="nips-2013-8-pdf" href="http://papers.nips.cc/paper/4949-a-graphical-transformation-for-belief-propagation-maximum-weight-matchings-and-odd-sized-cycles.pdf">pdf</a></p><p>Author: Jinwoo Shin, Andrew E. Gelfand, Misha Chertkov</p><p>Abstract: Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efﬁcient BP-based heuristic for the MWM problem, which consists of making sequential, “cutting plane”, modiﬁcations to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems. 1</p><p>Reference: <a title="nips-2013-8-reference" href="../nips2013_reference/nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 gov  Abstract Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). [sent-7, score-0.097]
</p><p>2 It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). [sent-8, score-0.288]
</p><p>3 Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. [sent-9, score-0.144]
</p><p>4 The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. [sent-10, score-0.086]
</p><p>5 We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. [sent-12, score-0.069]
</p><p>6 The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. [sent-13, score-0.075]
</p><p>7 Such models use a graph structure to encode the joint probability distribution, where vertices correspond to random variables and edges (or lack of thereof) specify conditional dependencies. [sent-17, score-0.059]
</p><p>8 An important inference task in many applications involving GMs is to ﬁnd the most likely assignment to the variables in a GM - the maximum a posteriori (MAP) conﬁguration. [sent-18, score-0.074]
</p><p>9 The convergence and correctness of BP was recently established for a certain class of loopy GM formulations of several classic combinatorial optimization problems, including matchings [7, 8, 9], perfect matchings [10], independent sets [11] and network ﬂows [12]. [sent-27, score-0.251]
</p><p>10 1  feature of these instances is that BP converges to a correct MAP assignment when the Linear Programming (LP) relaxation of the MAP inference problem is tight, i. [sent-29, score-0.203]
</p><p>11 While this demonstrates that LP tightness is necessary for the convergence and correctness of BP, it is unfortunately not sufﬁcient in general. [sent-32, score-0.088]
</p><p>12 In other words, BP may not work even when the corresponding LP relaxation to the MAP inference problem is tight. [sent-33, score-0.056]
</p><p>13 Utilizing successive LP relaxations to solve the MWM problem is an example of the popular cutting plane method for solving combinatorial optimization problems [14]. [sent-37, score-0.072]
</p><p>14 The authors showed that max-product BP converges to the correct, MAP solution if the base LP relaxation with no blossom - referred to herein as MWM-LP - is tight. [sent-43, score-0.16]
</p><p>15 Unfortunately, the tightness is not guaranteed in general, and the convergence and correctness for max-product BP do not readily extend to a GM with blossom constraints. [sent-44, score-0.149]
</p><p>16 To resolve this issue, we propose a novel GM formulation of the MWM problem and show that maxproduct BP on this new GM converges to the MWM assignment as long as the MWM-LP relaxation with blossom constraints is tight. [sent-45, score-0.242]
</p><p>17 The only restriction placed on our GM construction is that the set of blossom constraints added to the base MWM-LP be non-intersecting (in edges). [sent-46, score-0.085]
</p><p>18 Our GM construction is motivated by the so-called ‘degree-two’ (DT) condition, which requires that every variable in the GM be associated to at most two factor functions. [sent-47, score-0.089]
</p><p>19 Note, that the DT condition is not satisﬁed by the standard MWM GM formulation, and hence, we design a new GM that satisﬁes the DT condition via a clever graphical transformation namely, collapsing odd-sized cycles and deﬁning new weights on the contracted graph. [sent-49, score-0.209]
</p><p>20 In particular, we examine the BP solution to identify odd-sized cycle constraints - “cuts” - to add to the MWM-LP relaxation; then construct a new GM using our graphical transformation, run BP and repeat. [sent-52, score-0.12]
</p><p>21 To our knowledge, our result is the ﬁrst to suggest how to “ﬁx” BP via a graph transformation so that it works properly, i. [sent-56, score-0.075]
</p><p>22 We believe that our success in crafting a graphical transformation will offer useful insight into the design and analysis of BP algorithms for a wider class of problems. [sent-59, score-0.074]
</p><p>23 The MAP assignment z ∗ is deﬁned as z ∗ = arg max Pr[Z = z]. [sent-69, score-0.074]
</p><p>24 A matching is a set of edges without common vertices. [sent-72, score-0.057]
</p><p>25 The weight of a matching is the sum of corresponding edge weights. [sent-73, score-0.109]
</p><p>26 Here C is a set of odd-sized cycles C ⊂ 2V , δ(i) = {(i, j) ∈ E} and E(C) = {(i, j) ∈ E : i, j ∈ C}. [sent-76, score-0.07]
</p><p>27 Throughout the manuscript, we assume that cycles are non-intersecting in edges, i. [sent-77, score-0.07]
</p><p>28 It is easy to see that a MAP assignment x∗ for the GM (2) induces a MWM in G. [sent-80, score-0.074]
</p><p>29 We also assume that the MAP assignment is unique. [sent-81, score-0.074]
</p><p>30 2  Belief Propagation and Linear Programming for Maximum Weight Matchings  In this section, we introduce max-product Belief Propagation (BP) and the Linear Programming (LP) relaxation to computing the MAP assignment in (2). [sent-83, score-0.13]
</p><p>31 The BP algorithm updates the set of 2|Ω| messages {mt (zi ), mt (zi ) : zi ∈ Ω} between every variable i and its associated factors α→i i→α α ∈ Fi = {α ∈ F : i ∈ α, |α| ≥ 2} using the following update rules: mt+1 (zi ) = α→i  mt (zj ) j→α  ψα (z )  mt+1 (zi ) = ψi (zi ) i→α  and  →i (zi ). [sent-85, score-0.207]
</p><p>32 For the GM (2), we let nt (·) to denote the BP belief on edge e ∈ E at time t. [sent-88, score-0.23]
</p><p>33 , 1] , using the using the beliefs and the rule: e  1 if nt (0) < nt (1)  e e BP xe (t) = ? [sent-90, score-0.426]
</p><p>34 e ij  0 if nt (0) > nt (1) e e  The LP relaxation to the MAP problem for the GM (2) is: C-LP :  max  we xe e∈E  xe ≤ 1,  s. [sent-92, score-0.696]
</p><p>35 e∈δ(i)  ∀i ∈ V,  xe ≤ e∈E(C)  3  |C| − 1 , 2  ∀C ∈ C,  xe ∈ [0, 1]. [sent-94, score-0.356]
</p><p>36 If C = ∅ and the solution of C-LP is integral and unique, then xBP (t) under the GM (2) converges to the MWM assignment x∗ . [sent-102, score-0.152]
</p><p>37 1 extends to a non-empty C since adding more cycles can help to reduce the integrality gap of C-LP. [sent-106, score-0.106]
</p><p>38 For example, BP does not converge for a triangle graph with edge weights {2, 1, 1} and C consisting of the only cycle. [sent-108, score-0.081]
</p><p>39 3  A Graphical Transformation for Convergent & Correct BP  The loss of convergence and correctness of BP when the MWM LP is tight (and unique) but C = ∅ motivates the work in this section. [sent-110, score-0.119]
</p><p>40 We resolve the issue by designing a new GM, equivalent to the original GM, such that when BP is run on this new GM it converges to the MAP/MWM assignment whenever the LP relaxation is tight and unique - even if C = ∅. [sent-111, score-0.219]
</p><p>41 The new GM is deﬁned on an auxiliary graph G = (V , E ) with new weights {we : e ∈ E }, as follows: V = V ∪ {iC : C ∈ C}, we =  1 2  E = E ∪ {(iC , j) : j ∈ V (C), C ∈ C} \ {e : e ∈ ∪C∈C E(C)} dC (j,e )  e ∈E(C) (−1)  we  we  for some C ∈ C  if e = (iC , j) otherwise  . [sent-112, score-0.066]
</p><p>42 Here dC (j, e) is the graph distance of j and e in cycle C = (j1 , j2 , . [sent-113, score-0.102]
</p><p>43 Figure 1: Example of original graph G (left) and new graph G (right) after collapsing cycle C = (1, 2, 3, 4, 5). [sent-119, score-0.156]
</p><p>44 In the new graph G , edge weight w1C = 1/2(w12 − w23 + w34 − w45 + w15 ). [sent-120, score-0.111]
</p><p>45 If the solution of C-LP is integral and unique, then the BP-MAP estimate y BP (t) under the GM (3) converges to the corresponding MAP assignment y ∗ . [sent-126, score-0.152]
</p><p>46 Furthermore, the MWM assignment x∗ is reconstructible from y ∗ as: x∗ = e  1 2  dC (j,e) ∗ yiC ,j j∈V (C) (−1) ∗ ye  4  if e ∈ C∈C E(C) . [sent-127, score-0.517]
</p><p>47 We stress that the new GM (3) is designed so that each variable is associated to at most two factor nodes. [sent-132, score-0.065]
</p><p>48 We further remark that even under the DT condition and given tightness/uniqueness of the LP, proving correctness and convergence of BP is still highly non trivial. [sent-136, score-0.075]
</p><p>49 ∀i ∈ V,  ye ∈ [0, 1],  ∀e ∈ E ,  (5)  e∈δ(i)  (−1)dC (j,e) yiC ,j ∈ [0, 2],  ∀e ∈ E(C),  j∈V (C)  ye ≤ |C| − 1,  ∀C ∈ C. [sent-143, score-0.886]
</p><p>50 (6)  e∈δ(iC )  Consider the following one-to-one linear mapping between x = [xe : e ∈ E] and y = [ye : e ∈ E ]: ye =  e ∈E(C)∩δ(i)  xe  xe  if e = (i, iC ) otherwise  1 2  xe =  j∈V (C) (−1)  ye  dC (j,e)  yiC ,j  if e ∈ C∈C E(C) . [sent-144, score-1.451]
</p><p>51 otherwise  Under the mapping, one can check that C-LP = C-LP and if the solution xC-LP of C-LP is unique and integral, the solution y C-LP of C-LP is as well, i. [sent-145, score-0.092]
</p><p>52 If the solution y C-LP of C-LP is integral and unique, i. [sent-156, score-0.054]
</p><p>53 , y C-LP = y ∗ , then ∗ • If ye = 1, nt [1] > nt [0] for all t > e e  6wmax c  + 6,  ∗ • If ye = 0, nt [1] < nt [0] for all t > e e  6wmax c  + 6,  where nt [·] denotes the BP belief of edge e at time t under the GM (3) and wmax = maxe∈E |we |. [sent-158, score-1.612]
</p><p>54 We focus here on the case of ye = 1, while ∗ translation of the result to the opposite case of ye = 0 is straightforward. [sent-163, score-0.886]
</p><p>55 To derive a contradiction, assume that nt [1] ≤ nt [0] and construct a tree-structured GM Te (t) of depth t + 1, also known as e e the computational tree, using the following scheme:  1. [sent-164, score-0.248]
</p><p>56 Add a copy of Ye ∈ {0, 1} as the (root) variable (with variable function ewe Ye ). [sent-165, score-0.093]
</p><p>57 For each i ∈ V such that e ∈ δ(i) and ψi is not associated to Ye of the current model, add ψi as a factor (function) with copies of {Ye ∈ {0, 1} : e ∈ δ(i) \ e} as child variables (with corresponding variable functions, i. [sent-169, score-0.137]
</p><p>58 For each C ∈ C such that e ∈ δ(iC ) and ψC is not associated to Ye of the current model, add ψC as a factor (function) with copies of {Ye ∈ {0, 1} : e ∈ δ(iC ) \ e} as child variables (with corresponding variable functions, i. [sent-173, score-0.137]
</p><p>59 5  TMAP It is known from [17] that there exists a MAP conﬁguration y TMAP on Te (t) with ye = 0 at the root variable. [sent-176, score-0.443]
</p><p>60 Now we construct a new assignment y NEW on the computational tree Te (t) as follows. [sent-177, score-0.096]
</p><p>61 For each child factor ψ, which is equal to ψi (i. [sent-183, score-0.068]
</p><p>62 NEW ∗ (b) Else if there exists a e’s child e through factor ψ such that ye = ye and ψ is satisﬁed by FLIPe (y NEW ) and FLIPe (FLIPe (y ∗ )), then go to the step 2 with e ← e . [sent-190, score-0.954]
</p><p>63 We note that there exists exactly one child factor ψ in step 3 and we only choose one child e in step (b) (even though there are many possible candidates). [sent-196, score-0.11]
</p><p>64 1 Now we state the following key lemma for the above construction of y NEW . [sent-198, score-0.075]
</p><p>65 , ye = 0), where the proof for the case 0 → 1 follows in a similar manner. [sent-206, score-0.465]
</p><p>66 First, one can observe that y satisﬁes ψC if and only if y NEW is the 0-1 indicator vector of a union of disjoint even paths in the cycle C. [sent-207, score-0.119]
</p><p>67 Since ye is ﬂipped as 1 → 0, the even path including e is broken into an even (possibly, empty) path and an odd (always, NEW non-empty) path. [sent-208, score-0.541]
</p><p>68 We consider two cases: (a) there exists e within the odd path (i. [sent-209, score-0.07]
</p><p>69 , ye = 1) ∗ NEW such that ye = 0 and ﬂipping ye as 1 → 0 broke the odd path into two even (disjoint) paths; (b) there exists no such e within the odd path. [sent-211, score-1.441]
</p><p>70 For the ﬁrst case (a), it is easy to see that we can maintain the structure of disjoint even paths in NEW y NEW after ﬂipping ye as 1 → 0, i. [sent-212, score-0.495]
</p><p>71 Then, ye = 1 since y ∗ satisﬁes factor ψC and induces a union of disjoint even paths in NEW the cycle C. [sent-218, score-0.588]
</p><p>72 Therefore, if we ﬂip ye as 0 → 1, then we can still maintain the structure of disjoint even paths in y NEW , ψ is satisﬁed by FLIPe (y NEW ). [sent-219, score-0.495]
</p><p>73 To this end, for (i, j) ∈ E , let n0→1 and n1→0 be the ij ij number of ﬂip operations 0 → 1 and 1 → 0 for copies of (i, j) in the step 2 of the construction of Te (t). [sent-227, score-0.126]
</p><p>74 We consider two cases: (i) the path P does not arrive ij ij at a leave variable of Te (t), and (ii) otherwise. [sent-229, score-0.145]
</p><p>75 In this case, we deﬁne yij := yij + ε(n1→0 − n0→1 ), and establish the following lemma. [sent-232, score-0.066]
</p><p>76 To this end, for given C ∈ C, 1  NEW NEW P may not have an alternating structure since both ye and its child ye can be ﬂipped in a same way. [sent-239, score-0.928]
</p><p>77 Since the path P does not hit a leave variable of Te (t), we have  1 N  N i=1  ∗ ∗ yC (i) = yC +  1 n1→0 − n0→1 , C N C N  1 ∗ where N is the number of copies of ψC in P ∩ Te (t). [sent-244, score-0.103]
</p><p>78 We consider the case when only one end of P hits a leave variable Ye of Te (t), where the proof of the other case follows in a similar manner. [sent-251, score-0.067]
</p><p>79 In this case, we deﬁne ‡ ∗ yij := yij + ε(m1→0 − m0→1 ), where m1→0 = [m1→0 ] and m0→1 = [m0→1 ] is conij ij ij ij structed as follows: 1. [sent-252, score-0.174]
</p><p>80 If ye is ﬂipped as 1 → 0 and it is associated to a cycle parent factor ψC for some C ∈ C, then decrease 1→0 me by 1 and NEW is ﬂipped from 1 → 0, then decrease m1→0 by 1. [sent-255, score-0.643]
</p><p>81 2-1 If the parent ye e ∗ 2-2 Else if there exists a ‘brother’ edge e ∈ δ(iC ) of e such that ye ∗ 0→1 FLIPe (FLIPe (y )), then increase me by 1. [sent-256, score-0.979]
</p><p>82 If ye is ﬂipped as 1 → 0 and it is associated to a vertex parent factor ψi for some i ∈ V , then decrease m1→0 by 1. [sent-259, score-0.556]
</p><p>83 e ∗ 4-1-2 Else if there exists a ‘brother’ edge e ∈ δ(iC ) of e such that ye = 1 and ψC is satisﬁed by ∗ 0→1 FLIPe (FLIPe (y )), then increase me by 1. [sent-262, score-0.489]
</p><p>84 4  Cutting-Plane Algorithm using Belief Propagation  In the previous section we established that BP on a carefully designed GM using non-intersecting odd-sized cycles solves the MWM problem when the corresponding MWM-LP relaxation is tight. [sent-283, score-0.126]
</p><p>85 However, ﬁnding a collection of odd-sized cycles to ensure tightness of the MWM-LP is a challenging task. [sent-284, score-0.106]
</p><p>86 For each edge e ∈ E, set ye = 0 if nT [1] < nT [0] and nT −1 [1] < nT −1 [0] . [sent-291, score-0.489]
</p><p>87 Otherwise, add a non-intersecting odd-sized cycle of edges {e : xe = 1/2} to C and go to step 2; or terminate if no such cycle exists. [sent-296, score-0.364]
</p><p>88 The primary reason why we design CP-BP to terminate when x ∈ {0, 1/2, 1}|E| is because the solution x of / C-LP is always half integral 2 . [sent-299, score-0.082]
</p><p>89 The sparse graph instances were generated by forming a complete graph on |V | = {50, 100} nodes and independently eliminating edges with probability p = {0. [sent-303, score-0.118]
</p><p>90 The triangulation instances were generated by randomly placing |V | = {100, 200} points in the 220 × 220 square and computing a Delaunay triangulation on this set of points. [sent-307, score-0.162]
</p><p>91 The results are summarized in Table 1 and show that: 1) CP-BP is almost as good as CP-LP for solving the MWM problem; and 2) our graphical transformation allows BP to solve signiﬁcantly more MWM problems than are solvable by BP run on the ‘bare’ LP without odd-sized cycles. [sent-310, score-0.074]
</p><p>92 Columns # CP-BP and # CP-LP indicate the percentage of instances in which the cutting plane methods found a MWM. [sent-324, score-0.096]
</p><p>93 # Correct and # Converged indicate the number of correct matchings and number of instances in which CP-BP converged upon termination, but we failed to ﬁnd a non-intersecting odd-sized cycle. [sent-328, score-0.157]
</p><p>94 Weiss, “Constructing free-energy approximations and generalized belief propagation algorithms,” IEEE Transactions on Information Theory, vol. [sent-334, score-0.12]
</p><p>95 “Residual splash for optimally parallelizing belief propagation,” in International Conference on Artiﬁcial Intelligence and Statistics, 2009. [sent-362, score-0.06]
</p><p>96 Sharma, “Max-product for maximum weight matching: Convergence, correctness, and lp duality,” IEEE Transactions on Information Theory, vol. [sent-374, score-0.22]
</p><p>97 Jebara, “Loopy belief propagation for bipartite maximum weight b-matching,” in Artiﬁcial Intelligence and Statistics (AISTATS), 2007. [sent-383, score-0.15]
</p><p>98 Wei, “Belief propagation for min-cost network ﬂow: convergence & correctness,” in SODA, pp. [sent-398, score-0.06]
</p><p>99 “The cutting plane method is polynomial for perfect matchings,” in Foundations of Computer Science (FOCS), 2012 [16] R. [sent-416, score-0.072]
</p><p>100 Weiss, “Belief propagation and revision in networks with loops,” MIT AI Laboratory, Technical Report 1616, 1997. [sent-420, score-0.06]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ye', 0.443), ('bp', 0.424), ('mwm', 0.398), ('gm', 0.318), ('flipe', 0.233), ('lp', 0.19), ('xe', 0.178), ('te', 0.147), ('nt', 0.124), ('ic', 0.103), ('tmap', 0.096), ('yic', 0.096), ('yc', 0.086), ('gms', 0.085), ('ipped', 0.084), ('matchings', 0.084), ('assignment', 0.074), ('zi', 0.071), ('cycles', 0.07), ('triangulation', 0.069), ('cycle', 0.067), ('dc', 0.061), ('blossom', 0.061), ('belief', 0.06), ('propagation', 0.06), ('relaxation', 0.056), ('ewe', 0.055), ('map', 0.054), ('correctness', 0.052), ('lemma', 0.051), ('lps', 0.05), ('parent', 0.047), ('edge', 0.046), ('child', 0.042), ('tight', 0.042), ('dt', 0.042), ('odd', 0.042), ('alamos', 0.041), ('chertkov', 0.041), ('xbp', 0.041), ('cutting', 0.041), ('ipping', 0.04), ('transformation', 0.04), ('willsky', 0.038), ('integrality', 0.036), ('ij', 0.036), ('tightness', 0.036), ('integral', 0.035), ('graph', 0.035), ('graphical', 0.034), ('mt', 0.034), ('pc', 0.034), ('malioutov', 0.034), ('matching', 0.033), ('yij', 0.033), ('otherwise', 0.031), ('loopy', 0.031), ('sanghavi', 0.031), ('plane', 0.031), ('weight', 0.03), ('copies', 0.03), ('messages', 0.029), ('paths', 0.028), ('path', 0.028), ('shah', 0.028), ('terminate', 0.028), ('brother', 0.027), ('maxproduct', 0.027), ('programming', 0.027), ('leave', 0.026), ('solver', 0.026), ('factor', 0.026), ('correct', 0.025), ('motivates', 0.025), ('construction', 0.024), ('converges', 0.024), ('edges', 0.024), ('gelfand', 0.024), ('shin', 0.024), ('converged', 0.024), ('disjoint', 0.024), ('instances', 0.024), ('satis', 0.024), ('condition', 0.023), ('unique', 0.023), ('los', 0.023), ('heuristic', 0.023), ('bayati', 0.022), ('tree', 0.022), ('proof', 0.022), ('completes', 0.022), ('pr', 0.021), ('decrease', 0.02), ('associated', 0.02), ('korea', 0.02), ('weiss', 0.02), ('collapsing', 0.019), ('ip', 0.019), ('solution', 0.019), ('variable', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="8-tfidf-1" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>Author: Jinwoo Shin, Andrew E. Gelfand, Misha Chertkov</p><p>Abstract: Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efﬁcient BP-based heuristic for the MWM problem, which consists of making sequential, “cutting plane”, modiﬁcations to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems. 1</p><p>2 0.1402148 <a title="8-tfidf-2" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>Author: Srikrishna Sridhar, Stephen Wright, Christopher Re, Ji Liu, Victor Bittorf, Ce Zhang</p><p>Abstract: Many problems in machine learning can be solved by rounding the solution of an appropriate linear program (LP). This paper shows that we can recover solutions of comparable quality by rounding an approximate LP solution instead of the exact one. These approximate LP solutions can be computed efﬁciently by applying a parallel stochastic-coordinate-descent method to a quadratic-penalty formulation of the LP. We derive worst-case runtime and solution quality guarantees of this scheme using novel perturbation and convergence analysis. Our experiments demonstrate that on such combinatorial problems as vertex cover, independent set and multiway-cut, our approximate rounding scheme is up to an order of magnitude faster than Cplex (a commercial LP solver) while producing solutions of similar quality. 1</p><p>3 0.10300399 <a title="8-tfidf-3" href="./nips-2013-Understanding_Dropout.html">339 nips-2013-Understanding Dropout</a></p>
<p>Author: Pierre Baldi, Peter J. Sadowski</p><p>Abstract: Dropout is a relatively new algorithm for training neural networks which relies on stochastically “dropping out” neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function. 1</p><p>4 0.08473888 <a title="8-tfidf-4" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>Author: Maria-Florina Balcan, Steven Ehrlich, Yingyu Liang</p><p>Abstract: This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of ﬁnding a clustering with low cost to the problem of ﬁnding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms. 1</p><p>5 0.081511661 <a title="8-tfidf-5" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>Author: Arash Amini, Xuanlong Nguyen</p><p>Abstract: We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as speciﬁc instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.</p><p>6 0.075488567 <a title="8-tfidf-6" href="./nips-2013-Error-Minimizing_Estimates_and_Universal_Entry-Wise_Error_Bounds_for_Low-Rank_Matrix_Completion.html">108 nips-2013-Error-Minimizing Estimates and Universal Entry-Wise Error Bounds for Low-Rank Matrix Completion</a></p>
<p>7 0.070002005 <a title="8-tfidf-7" href="./nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</a></p>
<p>8 0.063001916 <a title="8-tfidf-8" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>9 0.057631586 <a title="8-tfidf-9" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>10 0.049872462 <a title="8-tfidf-10" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>11 0.046865519 <a title="8-tfidf-11" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>12 0.045622014 <a title="8-tfidf-12" href="./nips-2013-Projecting_Ising_Model_Parameters_for_Fast_Mixing.html">258 nips-2013-Projecting Ising Model Parameters for Fast Mixing</a></p>
<p>13 0.045427781 <a title="8-tfidf-13" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>14 0.045346837 <a title="8-tfidf-14" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>15 0.045071352 <a title="8-tfidf-15" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>16 0.041645907 <a title="8-tfidf-16" href="./nips-2013-Solving_the_multi-way_matching_problem_by_permutation_synchronization.html">300 nips-2013-Solving the multi-way matching problem by permutation synchronization</a></p>
<p>17 0.040646877 <a title="8-tfidf-17" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>18 0.038347062 <a title="8-tfidf-18" href="./nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">168 nips-2013-Learning to Pass Expectation Propagation Messages</a></p>
<p>19 0.037564427 <a title="8-tfidf-19" href="./nips-2013-Blind_Calibration_in_Compressed_Sensing_using_Message_Passing_Algorithms.html">59 nips-2013-Blind Calibration in Compressed Sensing using Message Passing Algorithms</a></p>
<p>20 0.036112092 <a title="8-tfidf-20" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.103), (1, 0.024), (2, 0.009), (3, 0.009), (4, 0.037), (5, 0.046), (6, -0.006), (7, -0.039), (8, 0.001), (9, 0.003), (10, 0.102), (11, -0.057), (12, 0.097), (13, -0.006), (14, -0.031), (15, -0.0), (16, -0.051), (17, 0.023), (18, -0.007), (19, 0.029), (20, -0.069), (21, 0.003), (22, 0.039), (23, -0.05), (24, -0.076), (25, -0.027), (26, 0.052), (27, 0.053), (28, 0.023), (29, 0.061), (30, -0.045), (31, -0.034), (32, 0.041), (33, 0.092), (34, 0.062), (35, -0.005), (36, -0.053), (37, -0.037), (38, -0.085), (39, 0.007), (40, 0.008), (41, -0.082), (42, -0.113), (43, -0.052), (44, 0.048), (45, 0.073), (46, -0.003), (47, 0.077), (48, -0.028), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94813973 <a title="8-lsi-1" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>Author: Jinwoo Shin, Andrew E. Gelfand, Misha Chertkov</p><p>Abstract: Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efﬁcient BP-based heuristic for the MWM problem, which consists of making sequential, “cutting plane”, modiﬁcations to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems. 1</p><p>2 0.79523331 <a title="8-lsi-2" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>Author: Srikrishna Sridhar, Stephen Wright, Christopher Re, Ji Liu, Victor Bittorf, Ce Zhang</p><p>Abstract: Many problems in machine learning can be solved by rounding the solution of an appropriate linear program (LP). This paper shows that we can recover solutions of comparable quality by rounding an approximate LP solution instead of the exact one. These approximate LP solutions can be computed efﬁciently by applying a parallel stochastic-coordinate-descent method to a quadratic-penalty formulation of the LP. We derive worst-case runtime and solution quality guarantees of this scheme using novel perturbation and convergence analysis. Our experiments demonstrate that on such combinatorial problems as vertex cover, independent set and multiway-cut, our approximate rounding scheme is up to an order of magnitude faster than Cplex (a commercial LP solver) while producing solutions of similar quality. 1</p><p>3 0.75187486 <a title="8-lsi-3" href="./nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</a></p>
<p>Author: Bogdan Savchynskyy, Jörg Hendrik Kappes, Paul Swoboda, Christoph Schnörr</p><p>Abstract: We consider energy minimization for undirected graphical models, also known as the MAP-inference problem for Markov random ﬁelds. Although combinatorial methods, which return a provably optimal integral solution of the problem, made a signiﬁcant progress in the past decade, they are still typically unable to cope with large-scale datasets. On the other hand, large scale datasets are often deﬁned on sparse graphs and convex relaxation methods, such as linear programming relaxations then provide good approximations to integral solutions. We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation, our method conﬁnes application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve much larger problems. We demonstrate the efﬁcacy of our approach on a computer vision energy minimization benchmark. 1</p><p>4 0.72030354 <a title="8-lsi-4" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>Author: Tim Roughgarden, Michael Kearns</p><p>Abstract: We consider a number of classical and new computational problems regarding marginal distributions, and inference in models specifying a full joint distribution. We prove general and efﬁcient reductions between a number of these problems, which demonstrate that algorithmic progress in inference automatically yields progress for “pure data” problems. Our main technique involves formulating the problems as linear programs, and proving that the dual separation oracle required by the ellipsoid method is provided by the target problem. This technique may be of independent interest in probabilistic inference. 1</p><p>5 0.55693656 <a title="8-lsi-5" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>Author: Daniel S. Levine, Jonathan P. How</p><p>Abstract: We consider the sensor selection problem on multivariate Gaussian distributions where only a subset of latent variables is of inferential interest. For pairs of vertices connected by a unique path in the graph, we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efﬁciently from the output of message passing algorithms. We integrate these decompositions into a computationally efﬁcient greedy selector where the computational expense of quantiﬁcation can be distributed across nodes in the network. Experimental results demonstrate the comparative efﬁciency of our algorithms for sensor selection in high-dimensional distributions. We additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that, when such a valid augmentation exists, is applicable for any distribution with nuisances. 1</p><p>6 0.54653722 <a title="8-lsi-6" href="./nips-2013-Message_Passing_Inference_with_Chemical_Reaction_Networks.html">189 nips-2013-Message Passing Inference with Chemical Reaction Networks</a></p>
<p>7 0.51131809 <a title="8-lsi-7" href="./nips-2013-Near-optimal_Anomaly_Detection_in_Graphs_using_Lovasz_Extended_Scan_Statistic.html">207 nips-2013-Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic</a></p>
<p>8 0.4905422 <a title="8-lsi-8" href="./nips-2013-Adaptive_Anonymity_via_%24b%24-Matching.html">25 nips-2013-Adaptive Anonymity via $b$-Matching</a></p>
<p>9 0.47415683 <a title="8-lsi-9" href="./nips-2013-Manifold-based_Similarity_Adaptation_for_Label_Propagation.html">182 nips-2013-Manifold-based Similarity Adaptation for Label Propagation</a></p>
<p>10 0.47001845 <a title="8-lsi-10" href="./nips-2013-Learning_Chordal_Markov_Networks_by_Constraint_Satisfaction.html">151 nips-2013-Learning Chordal Markov Networks by Constraint Satisfaction</a></p>
<p>11 0.46068141 <a title="8-lsi-11" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>12 0.45683777 <a title="8-lsi-12" href="./nips-2013-Robust_Multimodal_Graph_Matching%3A_Sparse_Coding_Meets_Graph_Matching.html">282 nips-2013-Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching</a></p>
<p>13 0.44279349 <a title="8-lsi-13" href="./nips-2013-Data-driven_Distributionally_Robust_Polynomial_Optimization.html">80 nips-2013-Data-driven Distributionally Robust Polynomial Optimization</a></p>
<p>14 0.44097933 <a title="8-lsi-14" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>15 0.4323847 <a title="8-lsi-15" href="./nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</a></p>
<p>16 0.43126592 <a title="8-lsi-16" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>17 0.42780983 <a title="8-lsi-17" href="./nips-2013-Bayesian_inference_as_iterated_random_functions_with__applications_to_sequential_inference_in_graphical_models.html">52 nips-2013-Bayesian inference as iterated random functions with  applications to sequential inference in graphical models</a></p>
<p>18 0.42597023 <a title="8-lsi-18" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>19 0.41913182 <a title="8-lsi-19" href="./nips-2013-Wavelets_on_Graphs_via_Deep_Learning.html">350 nips-2013-Wavelets on Graphs via Deep Learning</a></p>
<p>20 0.41906816 <a title="8-lsi-20" href="./nips-2013-Density_estimation_from_unweighted_k-nearest_neighbor_graphs%3A_a_roadmap.html">87 nips-2013-Density estimation from unweighted k-nearest neighbor graphs: a roadmap</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.023), (3, 0.316), (16, 0.026), (33, 0.073), (34, 0.1), (41, 0.032), (49, 0.021), (56, 0.127), (70, 0.04), (85, 0.04), (89, 0.033), (93, 0.045), (95, 0.026), (99, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75071597 <a title="8-lda-1" href="./nips-2013-A_Graphical_Transformation_for_Belief_Propagation%3A_Maximum_Weight_Matchings_and_Odd-Sized_Cycles.html">8 nips-2013-A Graphical Transformation for Belief Propagation: Maximum Weight Matchings and Odd-Sized Cycles</a></p>
<p>Author: Jinwoo Shin, Andrew E. Gelfand, Misha Chertkov</p><p>Abstract: Max-product ‘belief propagation’ (BP) is a popular distributed heuristic for ﬁnding the Maximum A Posteriori (MAP) assignment in a joint probability distribution represented by a Graphical Model (GM). It was recently shown that BP converges to the correct MAP assignment for a class of loopy GMs with the following common feature: the Linear Programming (LP) relaxation to the MAP problem is tight (has no integrality gap). Unfortunately, tightness of the LP relaxation does not, in general, guarantee convergence and correctness of the BP algorithm. The failure of BP in such cases motivates reverse engineering a solution – namely, given a tight LP, can we design a ‘good’ BP algorithm. In this paper, we design a BP algorithm for the Maximum Weight Matching (MWM) problem over general graphs. We prove that the algorithm converges to the correct optimum if the respective LP relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight. The most signiﬁcant part of our approach is the introduction of a novel graph transformation designed to force convergence of BP. Our theoretical result suggests an efﬁcient BP-based heuristic for the MWM problem, which consists of making sequential, “cutting plane”, modiﬁcations to the underlying GM. Our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using LP solvers on MWM problems. 1</p><p>2 0.67064679 <a title="8-lda-2" href="./nips-2013-Summary_Statistics_for_Partitionings_and_Feature_Allocations.html">320 nips-2013-Summary Statistics for Partitionings and Feature Allocations</a></p>
<p>Author: Isik B. Fidaner, Taylan Cemgil</p><p>Abstract: Inﬁnite mixture models are commonly used for clustering. One can sample from the posterior of mixture assignments by Monte Carlo methods or ﬁnd its maximum a posteriori solution by optimization. However, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings. In this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations. We develop an element-based deﬁnition of entropy to quantify segmentation among their elements. Then we propose a simple algorithm called entropy agglomeration (EA) to summarize and visualize this information. Experiments on various inﬁnite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.</p><p>3 0.65477657 <a title="8-lda-3" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>Author: Ian Osband, Dan Russo, Benjamin Van Roy</p><p>Abstract: Most provably-eﬃcient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for eﬃcient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally eﬃcient and allows an √ agent to encode prior knowledge ˜ in a natural way. We establish an O(τ S AT ) bound on expected regret, where T is time, τ is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the ﬁrst for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL signiﬁcantly outperforms existing algorithms with similar regret bounds. 1</p><p>4 0.60102719 <a title="8-lda-4" href="./nips-2013-Restricting_exchangeable_nonparametric_distributions.html">277 nips-2013-Restricting exchangeable nonparametric distributions</a></p>
<p>Author: Sinead A. Williamson, Steve N. MacEachern, Eric Xing</p><p>Abstract: Distributions over matrices with exchangeable rows and inﬁnitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution. 1</p><p>5 0.51406682 <a title="8-lda-5" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>Author: Adhiraj Somani, Nan Ye, David Hsu, Wee Sun Lee</p><p>Abstract: POMDPs provide a principled framework for planning under uncertainty, but are computationally intractable, due to the “curse of dimensionality” and the “curse of history”. This paper presents an online POMDP algorithm that alleviates these difﬁculties by focusing the search on a set of randomly sampled scenarios. A Determinized Sparse Partially Observable Tree (DESPOT) compactly captures the execution of all policies on these scenarios. Our Regularized DESPOT (R-DESPOT) algorithm searches the DESPOT for a policy, while optimally balancing the size of the policy and its estimated value obtained under the sampled scenarios. We give an output-sensitive performance bound for all policies derived from a DESPOT, and show that R-DESPOT works well if a small optimal policy exists. We also give an anytime algorithm that approximates R-DESPOT. Experiments show strong results, compared with two of the fastest online POMDP algorithms. Source code along with experimental settings are available at http://bigbird.comp. nus.edu.sg/pmwiki/farm/appl/. 1</p><p>6 0.51358974 <a title="8-lda-6" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>7 0.50954121 <a title="8-lda-7" href="./nips-2013-Marginals-to-Models_Reducibility.html">184 nips-2013-Marginals-to-Models Reducibility</a></p>
<p>8 0.50869632 <a title="8-lda-8" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>9 0.50805449 <a title="8-lda-9" href="./nips-2013-Online_Learning_of_Dynamic_Parameters_in_Social_Networks.html">228 nips-2013-Online Learning of Dynamic Parameters in Social Networks</a></p>
<p>10 0.50753921 <a title="8-lda-10" href="./nips-2013-Submodular_Optimization_with_Submodular_Cover_and_Submodular_Knapsack_Constraints.html">319 nips-2013-Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints</a></p>
<p>11 0.50739145 <a title="8-lda-11" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>12 0.50615472 <a title="8-lda-12" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>13 0.50506681 <a title="8-lda-13" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>14 0.50493848 <a title="8-lda-14" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>15 0.50444639 <a title="8-lda-15" href="./nips-2013-k-Prototype_Learning_for_3D_Rigid_Structures.html">357 nips-2013-k-Prototype Learning for 3D Rigid Structures</a></p>
<p>16 0.50315243 <a title="8-lda-16" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>17 0.50313401 <a title="8-lda-17" href="./nips-2013-Predictive_PAC_Learning_and_Process_Decompositions.html">252 nips-2013-Predictive PAC Learning and Process Decompositions</a></p>
<p>18 0.50250423 <a title="8-lda-18" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>19 0.50225693 <a title="8-lda-19" href="./nips-2013-Efficient_Algorithm_for_Privately_Releasing_Smooth_Queries.html">102 nips-2013-Efficient Algorithm for Privately Releasing Smooth Queries</a></p>
<p>20 0.50186181 <a title="8-lda-20" href="./nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
