<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 nips-2013-Correlated random features for fast semi-supervised learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-76" href="#">nips2013-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 nips-2013-Correlated random features for fast semi-supervised learning</h1>
<br/><p>Source: <a title="nips-2013-76-pdf" href="http://papers.nips.cc/paper/5000-correlated-random-features-for-fast-semi-supervised-learning.pdf">pdf</a></p><p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><p>Reference: <a title="nips-2013-76-reference" href="../nips2013_reference/nips-2013-Correlated_random_features_for_fast_semi-supervised_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Correlated random features for fast semi-supervised learning Brian McWilliams ETH Z¨ rich, Switzerland u brian. [sent-1, score-0.077]
</p><p>2 ch  Abstract This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. [sent-10, score-0.058]
</p><p>3 First, it generates two views consisting of computationally inexpensive random features. [sent-12, score-0.212]
</p><p>4 Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. [sent-13, score-0.156]
</p><p>5 It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. [sent-14, score-0.307]
</p><p>6 Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. [sent-15, score-0.2]
</p><p>7 We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. [sent-16, score-0.071]
</p><p>8 For learning non-linear relationships, kernel methods achieve excellent performance but na¨vely require operations cubic in ı the number of training points. [sent-18, score-0.092]
</p><p>9 Random features have been introduced to approximate kernel machines when the number of training examples is very large, rendering exact kernel computation intractable. [sent-20, score-0.234]
</p><p>10 Among several different approaches, the Nystr¨ m method for low-rank kernel approximation [1] exhibits good theoretical o properties and empirical performance [3–5]. [sent-21, score-0.102]
</p><p>11 Semi-supervised learning aims to improve prediction by extracting useful structure from the unlabeled data points and using this in conjunction with a function learned on a small number of labeled points. [sent-23, score-0.212]
</p><p>12 We investigate two ways of doing so: one based on the Nystr¨ m method and another based on random o Fourier features (so-called kitchen sinks) [2, 6]. [sent-28, score-0.11]
</p><p>13 It turns out that the Nystr¨ m method almost always o outperforms Fourier features by a quite large margin, so we only report these results in the main text. [sent-29, score-0.11]
</p><p>14 The second step, following [7], uses Canonical Correlation Analysis (CCA, [8, 9]) to bias the optimization procedure towards features that are correlated across the views. [sent-30, score-0.148]
</p><p>15 Intuitively, if both views contain accurate estimators, then penalizing uncorrelated features reduces variance without increasing the bias by much. [sent-31, score-0.411]
</p><p>16 Recent theoretical work by Bach [5] shows that Nystr¨ m views can be exo pected to contain accurate estimators. [sent-32, score-0.211]
</p><p>17 We ﬁnd that XNV outperforms SSSL by around 10-15% on average, depending on the number of labeled points available, see §3. [sent-34, score-0.127]
</p><p>18 We also ﬁnd that the performance of XNV exhibits dramatically less variability than SSSL, with a typical reduction of 30%. [sent-35, score-0.06]
</p><p>19 However, since SSSL does not scale up to large sets of unlabeled data, we modify SSSL by introducing a Nystr¨ m approximation to improve runtime performance. [sent-37, score-0.122]
</p><p>20 Our approximate version of SSSL outperforms kernel ridge regression (KRR) by > 50% on the 18 datasets on average, in line with the results reported in [10], suggesting that we lose little by replacing the exact SSSL with our approximate implementation. [sent-39, score-0.254]
</p><p>21 Surprisingly, despite guaranteeing improved prediction performance under a relatively weak assumption on the views, CCA regression has not been widely used since its proposal – to the best of our knowledge this is ﬁrst empirical evaluation of multi-view regression’s performance. [sent-43, score-0.108]
</p><p>22 A possible reason for this is the difﬁculty in obtaining naturally occurring data equipped with multiple views that can be shown to satisfy the multi-view assumption. [sent-44, score-0.195]
</p><p>23 We overcome this problem by constructing random views that satisfy the assumption by design. [sent-45, score-0.243]
</p><p>24 First, given two equally useful but sufﬁciently different views on a dataset, penalizing regression using the canonical norm (computed via CCA), can substantially improve performance [7]. [sent-48, score-0.46]
</p><p>25 The second is the Nystr¨ m method for constructing random features [1], which we use to o construct the views. [sent-49, score-0.107]
</p><p>26 , (xn , yn ) for xi 2 RD and yi 2 R, sampled according to joint distribution P (x, y). [sent-54, score-0.096]
</p><p>27 Further suppose we have two views on the data z(⌫) : RD ! [sent-55, score-0.195]
</p><p>28 Further let L(Z) denote the space of linear maps from a linear space Z to the reals, and deﬁne: f (⌫) := argmin loss(g) for ⌫ 2 {1, 2} and f := argmin loss(g). [sent-61, score-0.064]
</p><p>29 CCA ﬁnds bases for the two sets of variables such that the correlation between projections onto the bases are maximized. [sent-66, score-0.126]
</p><p>30 ⇣ ⌘ (1) (2) The ﬁrst pair of canonical basis vectors, b1 , b1 is found by solving: ⇣ ⌘ argmax corr b(1)> z(1) , b(2)> z(2) . [sent-67, score-0.152]
</p><p>31 Orthogonality: ET zj zk ] = jk , where jk is the Kronecker delta, and ⇥ (1)> (2) ⇤ ¯ ¯ 2. [sent-73, score-0.091]
</p><p>32 is referred to as the j th canonical correlation coefﬁcient. [sent-79, score-0.195]
</p><p>33 Given vector z(⌫) in the canonical basis, deﬁne its canonical norm as v uD ⇣ ⌘2 uX 1 j (⌫) (⌫) k¯ kCCA := t z zj ¯ . [sent-81, score-0.315]
</p><p>34 Assume we observe n pairs of views coupled with real valued labels n on h i> (⌫) (1) (2) (⌫) (⌫) zi , zi , yi , canonical ridge regression ﬁnds coefﬁcients b = b1 , . [sent-83, score-0.561]
</p><p>35 , bM such that i=1  X⇣ b (⌫) := argmin 1 yi n i=1 n  (⌫) > (⌫) ¯ zi  ⌘2  +k  (⌫) 2 kCCA . [sent-86, score-0.098]
</p><p>36 (3)  The resulting estimator, referred to as the canonical shrinkage estimator, is b(⌫) = j  j  n  n X  (⌫)  (4)  ¯ zi,j yi . [sent-87, score-0.197]
</p><p>37 i=1  Penalizing with the canonical norm biases the optimization towards features that are highly correlated across the views. [sent-88, score-0.282]
</p><p>38 Good regressors exist in both views by Assumption 1. [sent-89, score-0.215]
</p><p>39 Thus, intuitively, penalizing uncorrelated features signiﬁcantly reduces variance, without increasing the bias by much. [sent-90, score-0.184]
</p><p>40 Let (⌫) f b denote the estimator constructed with the canonical shrinkage estimator, Eq. [sent-93, score-0.179]
</p><p>41 P 2 1 The ﬁrst term, 5✏, bounds the bias of the canonical estimator, whereas the second, n j bounds P 2 the variance. [sent-96, score-0.159]
</p><p>42 The j can be thought of as a measure of the “intrinsic dimensionality” of the unlabeled data, which controls the rate of convergence. [sent-97, score-0.072]
</p><p>43 If the canonical correlation coefﬁcients decay sufﬁciently rapidly, then the increase in bias is more than made up for by the decrease in variance. [sent-98, score-0.217]
</p><p>44 2  Constructing random views  We construct two views satisfying Assumption 1 in expectation, see Theorem 3 below. [sent-100, score-0.39]
</p><p>45 To ensure our method scales to large sets of unlabeled data, we use random features generated using the Nystr¨ m o method [1]. [sent-101, score-0.149]
</p><p>46 Where here, (x) deﬁnes a mapping from RD to a high dimensional feature space and (·, ·) is a positive semi-deﬁnite kernel function. [sent-104, score-0.065]
</p><p>47 The idea behind random features is to instead deﬁne a lower-dimensional mapping, z(xi ) : RD ! [sent-105, score-0.077]
</p><p>48 Vectors of random features can be constructed as b z(xi ) = D  1/2  > b ˆ ˆ V> [(xi , x1 ), . [sent-119, score-0.077]
</p><p>49 Constructing features in this way reduces the time complexity of learning a non-linear prediction function from O(N 3 ) to O(N ) [15]. [sent-123, score-0.127]
</p><p>50 , 'r } where r is the rank of K and the 'i are the ﬁrst ˆ ˆ ˆ r eigenfunctions of LM . [sent-128, score-0.074]
</p><p>51 H spanned by the eigenfunctions of linear operator LM in Eq. [sent-130, score-0.074]
</p><p>52 Solving o minr  w2R  is equivalent to solving  N 1 X `(w> z(xi ), yi ) + kwk2 2 N i=1 2  N 1 X `(f (xi ), yi ) + kf k2  . [sent-132, score-0.08]
</p><p>53 3  (7)  (8)  The proposed algorithm: Correlated Nystr¨ m Views (XNV) o  Algorithm 1 details our approach to semi-supervised learning based on generating two views consisting of Nystr¨ m random features and penalizing features which are weakly correlated across views. [sent-134, score-0.449]
</p><p>54 o The setting is that we have labeled data {xi , yi }n and a large amount of unlabeled data {xi }N i=1 i=n+1 . [sent-135, score-0.193]
</p><p>55 The next two steps implement multi-view regression using the randomly generated views z(1) (x) and z(2) (x). [sent-137, score-0.253]
</p><p>56 o n Input: Labeled data: {xi , yi }i=1 and unlabeled data: {xi }N i=n+1 ˆ ˆ 1: Generate features. [sent-140, score-0.112]
</p><p>57 , x2M uniformly from the dataset, compute the eigendecomˆ ˆ positions of the sub-sampled kernel matrices K(1) and K(2) which are constructed from the samples 1, . [sent-144, score-0.065]
</p><p>58 Compute CCA bases B(1) , B(2) and canonical correlations  ¯ two views and set zi 3: Labeled data. [sent-155, score-0.413]
</p><p>59 M  for the  (9)  features are heavily downweighted in the CCA basis without introducing an additional tuning parameter. [sent-161, score-0.092]
</p><p>60 The further penalty on the `2 norm (in the CCA basis) is introduced as a practical measure to control the variance of the estimator b which can become large if there are many highly correlated 1 features (i. [sent-162, score-0.183]
</p><p>61 Nystr¨ m sampling, step 1, reduces the O(N 3 ) o operations required for kernel learning to O(N ). [sent-170, score-0.083]
</p><p>62 The quality of the kernel approximation in (5) has been the subject of detailed study in recent years leading to a number of strong empirical and theoretical results [3–5, 15]. [sent-175, score-0.083]
</p><p>63 , yN ] , and deﬁne smoothed estimate ykernel := (K + 1 ˜ ˜ ˆ N I) K(y + ⇠) and smoothed Nystr¨ m estimate yNystr¨ m := (K + N I) 1 K(y + ⇠), both o o computed by minimizing the MSE with ridge penalty . [sent-181, score-0.171]
</p><p>64 For sufﬁciently large M (depending on ⌘, see [5]), we have ⇥ ⇤ ⇥ ⇤ ˆ ˆ EM E⇠ ky yNystr¨ m k2  (1 + 4⌘) · E⇠ ky ykernel k2 o 2 2 ˜ where EM refers to the expectation over subsampled columns used to construct K. [sent-183, score-0.08]
</p><p>65 In short, the best smoothed estimators in the Nystr¨ m views are close to the optimal smoothed o estimator. [sent-184, score-0.255]
</p><p>66 Since the kernel estimate is consistent, loss(f ) ! [sent-185, score-0.065]
</p><p>67 An alternative approach to constructing random views is to use Fourier features instead of Nystr¨ m features in Step 1. [sent-190, score-0.379]
</p><p>68 We therefore do not discuss Fourier features in the main text, see §SI. [sent-193, score-0.077]
</p><p>69 (6) and then solves  argmin w2Rs  n X i=1  0 s X @ wj j=1  k (xi )  12  yi A ,  (10)  where s is set by the user. [sent-199, score-0.072]
</p><p>70 First, instead of constructing the full Gram matrix, we construct a Nystr¨ m approximation by sampling M points from the labeled and o unlabeled training set. [sent-205, score-0.255]
</p><p>71 Second, instead of thresholding eigenfunctions, we use the easier to tune ridge penalty which penalizes directions proportional to the inverse square of their eigenvalues [18]. [sent-206, score-0.079]
</p><p>72 As justiﬁcation, note that Proposition 2 states that the Nystr¨ m approximation to kernel regression o ˆ actually solves a ridge regression problem in the span of the eigenfunctions of LM . [sent-207, score-0.37]
</p><p>73 We will also refer to the Nystr¨ m approximation to the span of L o SSSL using 2M features as SSSL2M . [sent-209, score-0.113]
</p><p>74 The datasets cover a variety of regression (denoted by R) and two-class classiﬁcation (C) problems. [sent-213, score-0.091]
</p><p>75 The sarcos dataset involves predicting the joint position of a robot arm; following convention we report results on the 1st, 5th and 7th joint positions. [sent-214, score-0.08]
</p><p>76 The SSSL algorithm was shown to exhibit state-of-the-art performance over fully and semisupervised methods in scenarios where few labeled training examples are available [10]. [sent-215, score-0.108]
</p><p>77 We set the kernel width, and the `2 regularisation strength, , for each method using 5-fold cross validation with 1000 labeled training examples. [sent-219, score-0.173]
</p><p>78 We trained all methods using a squared error loss function, `(f (xi ), yi ) = (f (xi ) yi )2 , with M = 200 random features, and n = 100, 150, 200, . [sent-220, score-0.116]
</p><p>79 runtimes bank8 cal housing sylva SSSL 72s 2300s SSSL2M 0. [sent-243, score-0.108]
</p><p>80 3s 26s  For the cal housing dataset, XNV exhibits an almost 1800⇥ speed up over SSSL. [sent-247, score-0.095]
</p><p>81 For regression tasks we report on the mean squared error (MSE) on the testing set normalized by the variance of the test output. [sent-252, score-0.104]
</p><p>82 XNV vs SSSLM/2M Avg reduction in error Avg reduction in std err  n = 100 11% 15%  n = 200 16% 30%  n = 300 15% 31%  n = 400 12% 33%  n = 500 9% 30%  The reduced variability is to be expected from Theorem 1. [sent-256, score-0.149]
</p><p>83 4  100  200  300 400 500 600 700 800 number of labeled training points  900  100  1000  (a) adult  300 400 500 600 700 800 number of labeled training points  0  1000  100  200  XNV  XNV 0. [sent-279, score-0.27]
</p><p>84 5  300 400 500 600 700 800 number of labeled training points  (c) census  0. [sent-288, score-0.135]
</p><p>85 2  SSSLM  prediction error  900  (b) cal housing  0. [sent-289, score-0.124]
</p><p>86 The plots in Figure 1 shows a representative comparison of mean prediction errors for several datasets when n = 100, . [sent-306, score-0.065]
</p><p>87 Observe that XNV almost always improves prediction accuracy and reduces variance compared with SSSLM and SSSL2M when the labeled training set contains between 100 and 500 labeled points. [sent-311, score-0.255]
</p><p>88 This suggests that when there are few labeled points, obtaining a 6  Computed in Matlab 7. [sent-316, score-0.081]
</p><p>89 7  more accurate estimate of the eigenfunctions of the kernel does not necessarily improve predictive performance. [sent-318, score-0.155]
</p><p>90 Indeed, when more random features are added, stronger regularization is required to reduce the inﬂuence of uninformative features, this also has the effect of downweighting informative features. [sent-319, score-0.077]
</p><p>91 2 compares the performance of SSSLM and XNV to fully supervised kernel ridge regression (KRR). [sent-322, score-0.202]
</p><p>92 Nystr¨ m features signiﬁcantly outperform Fourier features, in line o with observations in [3]. [sent-325, score-0.077]
</p><p>93 The table below shows the relative improvement of XNV over XKS: XNV vs XKS Avg reduction in error Avg reduction in std err  n = 100 30% 36%  n = 200 28% 44%  n = 300 26% 34%  n = 400 25% 37%  n = 500 24% 36%  Further results and discussion for XKS are included in the supplementary material. [sent-326, score-0.129]
</p><p>94 By combining two randomly generated views of Nystr¨ m features via an efﬁcient implementation of CCA, XNV outperforms the o prior state-of-the-art, SSSL, by 10-15% (depending on the number of labeled points) on average over 18 datasets. [sent-546, score-0.372]
</p><p>95 An interesting research direction is to investigate using the recently developed deep CCA algorithm, which extracts higher order correlations between views [19], as a preprocessing step. [sent-548, score-0.216]
</p><p>96 Since CCA gives us a criterion by which to measure the important of random features, in the future we aim to investigate active sampling schemes based on canonical correlations which may yield better performance by selecting the most informative indices to sample. [sent-550, score-0.158]
</p><p>97 o 8  References [1] Williams C, Seeger M: Using the Nystr¨ m method to speed up kernel machines. [sent-553, score-0.065]
</p><p>98 [5] Bach F: Sharp analysis of low-rank kernel approximations. [sent-561, score-0.065]
</p><p>99 [11] Belkin M, Niyogi P, Sindhwani V: Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. [sent-573, score-0.153]
</p><p>100 [12] Blum A, Mitchell T: Combining labeled and unlabeled data with co-training. [sent-575, score-0.153]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xnv', 0.583), ('nystr', 0.417), ('sssl', 0.405), ('ssslm', 0.275), ('cca', 0.203), ('views', 0.195), ('canonical', 0.137), ('xks', 0.081), ('labeled', 0.081), ('ridge', 0.079), ('features', 0.077), ('eigenfunctions', 0.074), ('unlabeled', 0.072), ('sarcos', 0.066), ('kernel', 0.065), ('gram', 0.061), ('fourier', 0.059), ('correlation', 0.058), ('regression', 0.058), ('xi', 0.056), ('penalizing', 0.051), ('correlated', 0.049), ('avg', 0.045), ('sinks', 0.043), ('lm', 0.042), ('cal', 0.041), ('yi', 0.04), ('housing', 0.035), ('bases', 0.034), ('datasets', 0.033), ('kitchen', 0.033), ('runtime', 0.032), ('ibn', 0.032), ('krr', 0.032), ('mcwilliams', 0.032), ('sylva', 0.032), ('ykernel', 0.032), ('ynystr', 0.032), ('prediction', 0.032), ('argmin', 0.032), ('eth', 0.031), ('switzerland', 0.031), ('smoothed', 0.03), ('constructing', 0.03), ('xm', 0.029), ('kakade', 0.029), ('training', 0.027), ('points', 0.027), ('mw', 0.026), ('adv', 0.026), ('multiview', 0.026), ('zi', 0.026), ('jk', 0.026), ('vs', 0.026), ('avron', 0.025), ('livescu', 0.025), ('ky', 0.024), ('kcca', 0.024), ('std', 0.024), ('rahimi', 0.024), ('colt', 0.023), ('bm', 0.023), ('estimator', 0.022), ('zj', 0.022), ('bias', 0.022), ('generalization', 0.022), ('correlations', 0.021), ('err', 0.021), ('reduction', 0.021), ('shrinkage', 0.02), ('loss', 0.02), ('mahoney', 0.02), ('variability', 0.02), ('regressors', 0.02), ('foster', 0.019), ('norm', 0.019), ('bach', 0.019), ('exhibits', 0.019), ('sm', 0.019), ('ln', 0.019), ('outperforms', 0.019), ('assumption', 0.018), ('span', 0.018), ('approximation', 0.018), ('reduces', 0.018), ('zk', 0.017), ('jin', 0.017), ('computationally', 0.017), ('randomization', 0.016), ('uncorrelated', 0.016), ('accurate', 0.016), ('error', 0.016), ('variance', 0.016), ('jmlr', 0.016), ('mse', 0.016), ('basis', 0.015), ('laplacian', 0.015), ('nds', 0.015), ('rich', 0.015), ('report', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="76-tfidf-1" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><p>2 0.13474727 <a title="76-tfidf-2" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><p>3 0.06581638 <a title="76-tfidf-3" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>Author: Carlos J. Becker, Christos M. Christoudias, Pascal Fua</p><p>Abstract: A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-speciﬁc decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for speciﬁc a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a signiﬁcant improvement over the state of the art. 1</p><p>4 0.061252389 <a title="76-tfidf-4" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>Author: David Lopez-Paz, Philipp Hennig, Bernhard Schölkopf</p><p>Abstract: We introduce the Randomized Dependence Coefﬁcient (RDC), a measure of nonlinear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R´ nyi Maximum Correlation Coefﬁcient. RDC is deﬁned in e terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just ﬁve lines of R code, included at the end of the paper. 1</p><p>5 0.053313736 <a title="76-tfidf-5" href="./nips-2013-Latent_Structured_Active_Learning.html">149 nips-2013-Latent Structured Active Learning</a></p>
<p>Author: Wenjie Luo, Alex Schwing, Raquel Urtasun</p><p>Abstract: In this paper we present active learning algorithms in the context of structured prediction problems. To reduce the amount of labeling necessary to learn good models, our algorithms operate with weakly labeled data and we query additional examples based on entropies of local marginals, which are a good surrogate for uncertainty. We demonstrate the effectiveness of our approach in the task of 3D layout prediction from single images, and show that good models are learned when labeling only a handful of random variables. In particular, the same performance as using the full training set can be obtained while only labeling ∼10% of the random variables. 1</p><p>6 0.050998785 <a title="76-tfidf-6" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>7 0.04739387 <a title="76-tfidf-7" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>8 0.046749063 <a title="76-tfidf-8" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>9 0.045941953 <a title="76-tfidf-9" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>10 0.045780495 <a title="76-tfidf-10" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>11 0.04309094 <a title="76-tfidf-11" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>12 0.041894335 <a title="76-tfidf-12" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<p>13 0.041837625 <a title="76-tfidf-13" href="./nips-2013-Efficient_Optimization_for_Sparse_Gaussian_Process_Regression.html">105 nips-2013-Efficient Optimization for Sparse Gaussian Process Regression</a></p>
<p>14 0.041129187 <a title="76-tfidf-14" href="./nips-2013-Robust_Low_Rank_Kernel_Embeddings_of_Multivariate_Distributions.html">281 nips-2013-Robust Low Rank Kernel Embeddings of Multivariate Distributions</a></p>
<p>15 0.040202651 <a title="76-tfidf-15" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>16 0.039847519 <a title="76-tfidf-16" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>17 0.039593104 <a title="76-tfidf-17" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<p>18 0.039506756 <a title="76-tfidf-18" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>19 0.038465902 <a title="76-tfidf-19" href="./nips-2013-Least_Informative_Dimensions.html">173 nips-2013-Least Informative Dimensions</a></p>
<p>20 0.037765581 <a title="76-tfidf-20" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.11), (1, 0.044), (2, 0.007), (3, 0.004), (4, 0.029), (5, 0.027), (6, -0.005), (7, 0.028), (8, -0.056), (9, 0.024), (10, -0.017), (11, -0.016), (12, -0.064), (13, -0.05), (14, 0.109), (15, 0.033), (16, -0.047), (17, 0.034), (18, 0.062), (19, -0.029), (20, -0.02), (21, -0.022), (22, -0.019), (23, 0.044), (24, -0.011), (25, 0.016), (26, 0.04), (27, -0.023), (28, 0.031), (29, -0.073), (30, -0.007), (31, 0.005), (32, -0.017), (33, 0.007), (34, 0.011), (35, -0.017), (36, -0.026), (37, -0.071), (38, 0.036), (39, 0.029), (40, 0.001), (41, -0.085), (42, 0.003), (43, 0.003), (44, -0.016), (45, 0.015), (46, 0.015), (47, 0.001), (48, 0.024), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91045743 <a title="76-lsi-1" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><p>2 0.73320663 <a title="76-lsi-2" href="./nips-2013-Learning_Kernels_Using_Local_Rademacher_Complexity.html">156 nips-2013-Learning Kernels Using Local Rademacher Complexity</a></p>
<p>Author: Corinna Cortes, Marius Kloft, Mehryar Mohri</p><p>Abstract: We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby beneﬁt from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efﬁcient solution using existing learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also report the results of experiments with both algorithms in both binary and multi-class classiﬁcation tasks. 1</p><p>3 0.69286937 <a title="76-lsi-3" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>Author: Qichao Que, Mikhail Belkin</p><p>Abstract: q We address the problem of estimating the ratio p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration often referred to as importance sampling in statistical inference. It is also closely related to the problem of covariate shift in transfer learning. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, known as the Fredholm problem of the ﬁrst kind. This formulation, combined with the techniques of regularization leads to a principled framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is ﬂexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities deﬁned on Rd and smooth d-dimensional sub-manifolds of the Euclidean space. Model selection for unsupervised or semi-supervised inference is generally a difﬁcult problem. It turns out that in the density ratio estimation setting, when samples from both distributions are available, simple completely unsupervised model selection methods are available. We call this mechanism CD-CV for Cross-Density Cross-Validation. We show encouraging experimental results including applications to classiﬁcation within the covariate shift framework. 1</p><p>4 0.64513141 <a title="76-lsi-4" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><p>5 0.64406282 <a title="76-lsi-5" href="./nips-2013-The_Randomized_Dependence_Coefficient.html">327 nips-2013-The Randomized Dependence Coefficient</a></p>
<p>Author: David Lopez-Paz, Philipp Hennig, Bernhard Schölkopf</p><p>Abstract: We introduce the Randomized Dependence Coefﬁcient (RDC), a measure of nonlinear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R´ nyi Maximum Correlation Coefﬁcient. RDC is deﬁned in e terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just ﬁve lines of R code, included at the end of the paper. 1</p><p>6 0.63651472 <a title="76-lsi-6" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>7 0.60564852 <a title="76-lsi-7" href="./nips-2013-Heterogeneous-Neighborhood-based_Multi-Task_Local_Learning_Algorithms.html">135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</a></p>
<p>8 0.5920819 <a title="76-lsi-8" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>9 0.58635306 <a title="76-lsi-9" href="./nips-2013-Parametric_Task_Learning.html">244 nips-2013-Parametric Task Learning</a></p>
<p>10 0.57630074 <a title="76-lsi-10" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>11 0.55319202 <a title="76-lsi-11" href="./nips-2013-B-test%3A_A_Non-parametric%2C_Low_Variance_Kernel_Two-sample_Test.html">44 nips-2013-B-test: A Non-parametric, Low Variance Kernel Two-sample Test</a></p>
<p>12 0.55108935 <a title="76-lsi-12" href="./nips-2013-Learning_with_Noisy_Labels.html">171 nips-2013-Learning with Noisy Labels</a></p>
<p>13 0.54500753 <a title="76-lsi-13" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>14 0.53714502 <a title="76-lsi-14" href="./nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">211 nips-2013-Non-Linear Domain Adaptation with Boosting</a></p>
<p>15 0.53306997 <a title="76-lsi-15" href="./nips-2013-Adaptivity_to_Local_Smoothness_and_Dimension_in_Kernel_Regression.html">31 nips-2013-Adaptivity to Local Smoothness and Dimension in Kernel Regression</a></p>
<p>16 0.52404058 <a title="76-lsi-16" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>17 0.51908463 <a title="76-lsi-17" href="./nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</a></p>
<p>18 0.51764017 <a title="76-lsi-18" href="./nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</a></p>
<p>19 0.51506591 <a title="76-lsi-19" href="./nips-2013-Compressive_Feature_Learning.html">65 nips-2013-Compressive Feature Learning</a></p>
<p>20 0.51146907 <a title="76-lsi-20" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (16, 0.047), (33, 0.105), (34, 0.09), (36, 0.016), (39, 0.315), (41, 0.024), (49, 0.022), (56, 0.1), (70, 0.021), (85, 0.026), (89, 0.022), (93, 0.086), (95, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76951802 <a title="76-lda-1" href="./nips-2013-A_Novel_Two-Step_Method_for_Cross_Language_Representation_Learning.html">12 nips-2013-A Novel Two-Step Method for Cross Language Representation Learning</a></p>
<p>Author: Min Xiao, Yuhong Guo</p><p>Abstract: Cross language text classiﬁcation is an important learning task in natural language processing. A critical challenge of cross language learning arises from the fact that words of different languages are in disjoint feature spaces. In this paper, we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents. Speciﬁcally, we ﬁrst formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages, and then induce a low dimensional cross-lingual document representation by applying latent semantic indexing on the obtained matrix. We use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees. The proposed method is evaluated by conducting a set of experiments with cross language sentiment classiﬁcation tasks on Amazon product reviews. The experimental results demonstrate that the proposed learning method outperforms a number of other cross language representation learning methods, especially when the number of parallel bilingual documents is small. 1</p><p>same-paper 2 0.70775229 <a title="76-lda-2" href="./nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">76 nips-2013-Correlated random features for fast semi-supervised learning</a></p>
<p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><p>3 0.65342063 <a title="76-lda-3" href="./nips-2013-Fast_Determinantal_Point_Process_Sampling_with_Application_to_Clustering.html">118 nips-2013-Fast Determinantal Point Process Sampling with Application to Clustering</a></p>
<p>Author: Byungkon Kang</p><p>Abstract: Determinantal Point Process (DPP) has gained much popularity for modeling sets of diverse items. The gist of DPP is that the probability of choosing a particular set of items is proportional to the determinant of a positive deﬁnite matrix that deﬁnes the similarity of those items. However, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets. In this paper, we address this problem by constructing a rapidly mixing Markov chain, from which we can acquire a sample from the given DPP in sub-cubic time. In addition, we show that this framework can be extended to sampling from cardinalityconstrained DPPs. As an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.</p><p>4 0.60235673 <a title="76-lda-4" href="./nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</a></p>
<p>Author: Il M. Park, Evan W. Archer, Kenneth Latimer, Jonathan W. Pillow</p><p>Abstract: Probabilistic models for binary spike patterns provide a powerful tool for understanding the statistical dependencies in large-scale neural recordings. Maximum entropy (or “maxent”) models, which seek to explain dependencies in terms of low-order interactions between neurons, have enjoyed remarkable success in modeling such patterns, particularly for small groups of neurons. However, these models are computationally intractable for large populations, and low-order maxent models have been shown to be inadequate for some datasets. To overcome these limitations, we propose a family of “universal” models for binary spike patterns, where universality refers to the ability to model arbitrary distributions over all 2m binary patterns. We construct universal models using a Dirichlet process centered on a well-behaved parametric base measure, which naturally combines the ﬂexibility of a histogram and the parsimony of a parametric model. We derive computationally efﬁcient inference methods using Bernoulli and cascaded logistic base measures, which scale tractably to large populations. We also establish a condition for equivalence between the cascaded logistic and the 2nd-order maxent or “Ising” model, making cascaded logistic a reasonable choice for base measure in a universal model. We illustrate the performance of these models using neural data. 1</p><p>5 0.52870721 <a title="76-lda-5" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>6 0.5281418 <a title="76-lda-6" href="./nips-2013-Dropout_Training_as_Adaptive_Regularization.html">99 nips-2013-Dropout Training as Adaptive Regularization</a></p>
<p>7 0.52393889 <a title="76-lda-7" href="./nips-2013-Distributed_%24k%24-means_and_%24k%24-median_Clustering_on_General_Topologies.html">94 nips-2013-Distributed $k$-means and $k$-median Clustering on General Topologies</a></p>
<p>8 0.52241695 <a title="76-lda-8" href="./nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">40 nips-2013-Approximate Inference in Continuous Determinantal Processes</a></p>
<p>9 0.52087218 <a title="76-lda-9" href="./nips-2013-Regression-tree_Tuning_in_a_Streaming_Setting.html">269 nips-2013-Regression-tree Tuning in a Streaming Setting</a></p>
<p>10 0.52007598 <a title="76-lda-10" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>11 0.51872075 <a title="76-lda-11" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>12 0.51673454 <a title="76-lda-12" href="./nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</a></p>
<p>13 0.51610619 <a title="76-lda-13" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>14 0.51584411 <a title="76-lda-14" href="./nips-2013-Optimistic_Concurrency_Control_for_Distributed_Unsupervised_Learning.html">238 nips-2013-Optimistic Concurrency Control for Distributed Unsupervised Learning</a></p>
<p>15 0.51551813 <a title="76-lda-15" href="./nips-2013-Context-sensitive_active_sensing_in_humans.html">69 nips-2013-Context-sensitive active sensing in humans</a></p>
<p>16 0.5146113 <a title="76-lda-16" href="./nips-2013-Multi-Task_Bayesian_Optimization.html">201 nips-2013-Multi-Task Bayesian Optimization</a></p>
<p>17 0.51359421 <a title="76-lda-17" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>18 0.5127793 <a title="76-lda-18" href="./nips-2013-Structured_Learning_via_Logistic_Regression.html">318 nips-2013-Structured Learning via Logistic Regression</a></p>
<p>19 0.51183528 <a title="76-lda-19" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>20 0.51042348 <a title="76-lda-20" href="./nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
