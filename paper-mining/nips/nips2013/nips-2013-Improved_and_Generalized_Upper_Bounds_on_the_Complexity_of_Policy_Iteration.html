<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-140" href="#">nips2013-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</h1>
<br/><p>Source: <a title="nips-2013-140-pdf" href="http://papers.nips.cc/paper/4971-improved-and-generalized-upper-bounds-on-the-complexity-of-policy-iteration.pdf">pdf</a></p><p>Author: Bruno Scherrer</p><p>Abstract: Given a Markov Decision Process (MDP) with n states and m actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal “-discounted optimal policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal Ï advantage. We show that Howard’s PI terminates 1 2Ì 1 1 22 1 1 nm 1 after at most n(m ≠ 1) 1≠“ log 1≠“ = O 1≠“ log 1≠“ iterations, improving by a factor O(log 1 a result by [3], while Simplex-PI terminates n) 1 22 1 2 1 22 2 1 1 2 after at most n (m ≠ 1) 1 + 1≠“ log 1≠“ = O n m log 1≠“ 1≠“ iterations, improving by a factor O(log n) a result by [11]. Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor “: given a measure of the maximal transient time ·t and the maximal time ·r to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most n2 (m≠ # $ 1) (Á·r log(n·r )Ë + Á·r log(n·t )Ë) (m ≠ 1)Án·t log(n·t )Ë + Án·t log(n2 ·t )Ë = !</p><p>Reference: <a title="nips-2013-140-reference" href="../nips2013_reference/nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('howard', 0.686), ('pi', 0.37), ('policy', 0.312), ('mdp', 0.227), ('transy', 0.19), ('recur', 0.18), ('termin', 0.143), ('discount', 0.112), ('vmax', 0.101), ('mention', 0.091), ('lemm', 0.088), ('contract', 0.087), ('cyc', 0.086), ('nm', 0.08), ('switch', 0.073), ('yk', 0.072), ('lemma', 0.071), ('it', 0.07), ('mdps', 0.068), ('deduc', 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="140-tfidf-1" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>2 0.28199503 <a title="140-tfidf-2" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>3 0.27832857 <a title="140-tfidf-3" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>4 0.20842499 <a title="140-tfidf-4" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>5 0.20619917 <a title="140-tfidf-5" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>6 0.19866551 <a title="140-tfidf-6" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>7 0.19482403 <a title="140-tfidf-7" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>8 0.19197586 <a title="140-tfidf-8" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>9 0.18464845 <a title="140-tfidf-9" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>10 0.18203974 <a title="140-tfidf-10" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>11 0.16122288 <a title="140-tfidf-11" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>12 0.15352859 <a title="140-tfidf-12" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>13 0.14813221 <a title="140-tfidf-13" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>14 0.14478439 <a title="140-tfidf-14" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>15 0.13236617 <a title="140-tfidf-15" href="./nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</a></p>
<p>16 0.12725857 <a title="140-tfidf-16" href="./nips-2013-Adaptive_Submodular_Maximization_in_Bandit_Setting.html">29 nips-2013-Adaptive Submodular Maximization in Bandit Setting</a></p>
<p>17 0.12670682 <a title="140-tfidf-17" href="./nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
<p>18 0.12405147 <a title="140-tfidf-18" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>19 0.1158497 <a title="140-tfidf-19" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<p>20 0.1104616 <a title="140-tfidf-20" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, -0.286), (2, -0.109), (3, -0.144), (4, 0.039), (5, 0.062), (6, -0.016), (7, -0.07), (8, 0.018), (9, 0.061), (10, 0.045), (11, 0.029), (12, 0.075), (13, 0.021), (14, 0.06), (15, -0.027), (16, 0.07), (17, 0.022), (18, 0.026), (19, 0.003), (20, 0.009), (21, -0.03), (22, 0.037), (23, -0.041), (24, -0.022), (25, 0.066), (26, 0.041), (27, 0.036), (28, -0.02), (29, -0.034), (30, 0.028), (31, 0.021), (32, 0.015), (33, -0.032), (34, 0.061), (35, 0.025), (36, -0.011), (37, 0.046), (38, -0.017), (39, 0.002), (40, 0.013), (41, -0.044), (42, 0.046), (43, -0.015), (44, 0.011), (45, 0.03), (46, 0.008), (47, -0.003), (48, 0.014), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94138873 <a title="140-lsi-1" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>2 0.87693894 <a title="140-lsi-2" href="./nips-2013-Adaptive_Step-Size_for_Policy_Gradient_Methods.html">28 nips-2013-Adaptive Step-Size for Policy Gradient Methods</a></p>
<p>3 0.87276876 <a title="140-lsi-3" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>4 0.86011875 <a title="140-lsi-4" href="./nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</a></p>
<p>5 0.84585124 <a title="140-lsi-5" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>6 0.81687003 <a title="140-lsi-6" href="./nips-2013-Optimizing_Instructional_Policies.html">241 nips-2013-Optimizing Instructional Policies</a></p>
<p>7 0.74592334 <a title="140-lsi-7" href="./nips-2013-Variational_Policy_Search_via_Trajectory_Optimization.html">348 nips-2013-Variational Policy Search via Trajectory Optimization</a></p>
<p>8 0.74511868 <a title="140-lsi-8" href="./nips-2013-DESPOT%3A_Online_POMDP_Planning_with_Regularization.html">79 nips-2013-DESPOT: Online POMDP Planning with Regularization</a></p>
<p>9 0.73049879 <a title="140-lsi-9" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>10 0.72906113 <a title="140-lsi-10" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>11 0.6835708 <a title="140-lsi-11" href="./nips-2013-Learning_from_Limited_Demonstrations.html">165 nips-2013-Learning from Limited Demonstrations</a></p>
<p>12 0.68060064 <a title="140-lsi-12" href="./nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</a></p>
<p>13 0.6675052 <a title="140-lsi-13" href="./nips-2013-Reinforcement_Learning_in_Robust_Markov_Decision_Processes.html">273 nips-2013-Reinforcement Learning in Robust Markov Decision Processes</a></p>
<p>14 0.62960637 <a title="140-lsi-14" href="./nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</a></p>
<p>15 0.60016018 <a title="140-lsi-15" href="./nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</a></p>
<p>16 0.59979475 <a title="140-lsi-16" href="./nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</a></p>
<p>17 0.56470251 <a title="140-lsi-17" href="./nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</a></p>
<p>18 0.55177599 <a title="140-lsi-18" href="./nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</a></p>
<p>19 0.5350911 <a title="140-lsi-19" href="./nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</a></p>
<p>20 0.51831579 <a title="140-lsi-20" href="./nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(20, 0.272), (25, 0.043), (37, 0.055), (47, 0.173), (70, 0.027), (80, 0.145), (86, 0.124), (87, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96767282 <a title="140-lda-1" href="./nips-2013-Gaussian_Process_Conditional_Copulas_with_Applications_to_Financial_Time_Series.html">126 nips-2013-Gaussian Process Conditional Copulas with Applications to Financial Time Series</a></p>
<p>same-paper 2 0.90817648 <a title="140-lda-2" href="./nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</a></p>
<p>3 0.90703398 <a title="140-lda-3" href="./nips-2013-Beyond_Pairwise%3A_Provably_Fast_Algorithms_for_Approximate_%24k%24-Way__Similarity_Search.html">57 nips-2013-Beyond Pairwise: Provably Fast Algorithms for Approximate $k$-Way  Similarity Search</a></p>
<p>4 0.89930403 <a title="140-lda-4" href="./nips-2013-Sensor_Selection_in_High-Dimensional_Gaussian_Trees_with_Nuisances.html">291 nips-2013-Sensor Selection in High-Dimensional Gaussian Trees with Nuisances</a></p>
<p>5 0.8885656 <a title="140-lda-5" href="./nips-2013-Solving_inverse_problem_of_Markov_chain_with_partial_observations.html">299 nips-2013-Solving inverse problem of Markov chain with partial observations</a></p>
<p>6 0.86979717 <a title="140-lda-6" href="./nips-2013-Learning_Feature_Selection_Dependencies_in_Multi-task_Learning.html">153 nips-2013-Learning Feature Selection Dependencies in Multi-task Learning</a></p>
<p>7 0.86958927 <a title="140-lda-7" href="./nips-2013-Pass-efficient_unsupervised_feature_selection.html">245 nips-2013-Pass-efficient unsupervised feature selection</a></p>
<p>8 0.86958379 <a title="140-lda-8" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>9 0.86796385 <a title="140-lda-9" href="./nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</a></p>
<p>10 0.8669908 <a title="140-lda-10" href="./nips-2013-Stochastic_Optimization_of_PCA_with_Capped_MSG.html">314 nips-2013-Stochastic Optimization of PCA with Capped MSG</a></p>
<p>11 0.86398894 <a title="140-lda-11" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>12 0.86389071 <a title="140-lda-12" href="./nips-2013-An_Approximate%2C_Efficient_LP_Solver_for_LP_Rounding.html">33 nips-2013-An Approximate, Efficient LP Solver for LP Rounding</a></p>
<p>13 0.86243713 <a title="140-lda-13" href="./nips-2013-A_Determinantal_Point_Process_Latent_Variable_Model_for_Inhibition_in_Neural_Spiking_Data.html">6 nips-2013-A Determinantal Point Process Latent Variable Model for Inhibition in Neural Spiking Data</a></p>
<p>14 0.86112136 <a title="140-lda-14" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>15 0.86004448 <a title="140-lda-15" href="./nips-2013-Low-rank_matrix_reconstruction_and_clustering_via_approximate_message_passing.html">180 nips-2013-Low-rank matrix reconstruction and clustering via approximate message passing</a></p>
<p>16 0.85732907 <a title="140-lda-16" href="./nips-2013-Online_Learning_of_Nonparametric_Mixture_Models_via_Sequential_Variational_Approximation.html">229 nips-2013-Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation</a></p>
<p>17 0.85515523 <a title="140-lda-17" href="./nips-2013-Third-Order_Edge_Statistics%3A_Contour_Continuation%2C_Curvature%2C_and_Cortical_Connections.html">329 nips-2013-Third-Order Edge Statistics: Contour Continuation, Curvature, and Cortical Connections</a></p>
<p>18 0.85495979 <a title="140-lda-18" href="./nips-2013-Contrastive_Learning_Using_Spectral_Methods.html">70 nips-2013-Contrastive Learning Using Spectral Methods</a></p>
<p>19 0.85478181 <a title="140-lda-19" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>20 0.8543033 <a title="140-lda-20" href="./nips-2013-Learning_Multiple_Models_via_Regularized_Weighting.html">158 nips-2013-Learning Multiple Models via Regularized Weighting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
