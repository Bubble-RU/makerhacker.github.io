<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-56" href="#">nips2013-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</h1>
<br/><p>Source: <a title="nips-2013-56-pdf" href="http://papers.nips.cc/paper/4934-better-approximation-and-faster-algorithm-using-the-proximal-average.pdf">pdf</a></p><p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>Reference: <a title="nips-2013-56-reference" href="../nips2013_reference/nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. [sent-4, score-0.37]
</p><p>2 We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. [sent-5, score-1.023]
</p><p>3 The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. [sent-6, score-1.418]
</p><p>4 Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. [sent-7, score-0.382]
</p><p>5 1  Introduction  In many scientiﬁc areas, an important methodology that has withstood the test of time is the approximation of “complicated” functions by those that are easier to handle. [sent-8, score-0.093]
</p><p>6 And one would probably also agree that a speciﬁc form of approximation should be favored if it well suits our ultimate goal. [sent-15, score-0.1]
</p><p>7 Despite of all these common-sense, in optimization algorithms, the smooth approximations are still dominating, bypassing some recent advances on optimizing nonsmooth functions [2, 3]. [sent-16, score-0.407]
</p><p>8 We consider the composite minimization problem where the objective consists of a smooth loss function and a sum of nonsmooth functions. [sent-18, score-0.453]
</p><p>9 Such problems have received increasing attention due to the arise of structured sparsity [4], notably the overlapping group lasso [5], the graph-guided fused lasso [6] and some others. [sent-19, score-0.504]
</p><p>10 Popular gradient-type algorithms dealing with such composite problems include the generic subgradient method [7], (accelerated) proximal gradient (APG) [2, 3], and the smoothed accelerated proximal gradient (S-APG) [8]. [sent-21, score-1.568]
</p><p>11 The subgradient method is applicable to any nonsmooth function, although the convergence rate is rather slow. [sent-22, score-0.409]
</p><p>12 APG, being a recent advance, can handle simple functions [9] but for more complicated structured regularizers, an inner iterative procedure is needed, resulting in an overall convergence rate that could be as slow as the subgradient method [10]. [sent-23, score-0.239]
</p><p>13 Lastly, S-APG simply runs APG on a smooth approximation of the original objective, resulting in a much improved convergence rate. [sent-24, score-0.187]
</p><p>14 Our work is inspired by the recent advance on nonsmooth optimization [2, 3], of which the building block is the proximal map of the nonsmooth function. [sent-25, score-1.249]
</p><p>15 This proximal map is available in closed-form 1  for simple functions but can be quite expensive for more complicated functions such as a sum of nonsmooth functions we consider here. [sent-26, score-1.129]
</p><p>16 A key observation we make is that oftentimes the proximal map for each individual summand can be easily computed, therefore a bold idea is to simply use the sum of proximal maps, pretending that the proximal map is a linear operator. [sent-27, score-2.05]
</p><p>17 Somewhat surprisingly, this naive choice, when combined with APG, results in a novel proximal algorithm that is strictly better than S-APG, while keeping per-step complexity unchanged. [sent-28, score-0.67]
</p><p>18 We justify our method via a new tool from convex analysis—the proximal average [11]. [sent-29, score-0.746]
</p><p>19 In essence, instead of smoothing the nonsmooth function, we use a nonsmooth approximation whose proximal map is cheap to evaluate, after all this is all we need to run APG. [sent-30, score-1.379]
</p><p>20 After recalling the relevant tools from convex analysis in Section 3 we provide the theoretical justiﬁcation of our method in Section 4. [sent-32, score-0.092]
</p><p>21 2  Problem Formulation  We are interested in solving the following composite minimization problem: K  ¯ min (x) + f (x),  x∈Rd  where  ¯ f (x) =  αk fk (x). [sent-35, score-0.257]
</p><p>22 (1)  k=1  Here is convex with L0 -Lipschitz continuous gradient w. [sent-36, score-0.147]
</p><p>23 Each fk is convex and Mk -Lipschitz continuous w. [sent-42, score-0.267]
</p><p>24 We are interested in the general case where the functions fk need not be differentiable. [sent-47, score-0.211]
</p><p>25 As mentioned in the introduction, a generic scheme that solves (1) is the subgradient method [7], of which each step requires merely an arbitrary subgradient of the objective. [sent-48, score-0.228]
</p><p>26 With a suitable stepsize, the subgradient method converges1 in at most O(1/ 2 ) steps where > 0 is the desired accuracy. [sent-49, score-0.114]
</p><p>27 Although being general, the subgradient method is exceedingly slow, making it unsuitable for many practical applications. [sent-50, score-0.114]
</p><p>28 Another recent algorithm for solving (1) is the (accelerated) proximal gradient (APG) [2, 3], of ¯ which each iteration needs to compute the proximal map of the nonsmooth part f in (1): 1/L0  Pf ¯  (x) = argmin L0 x − y 2  2  ¯ + f (y). [sent-51, score-1.659]
</p><p>29 y  (Recall that L0 is the Lipschitz constant of the gradient of the smooth part in (1). [sent-52, score-0.157]
</p><p>30 ) Provided that the proximal map can be computed in constant time, it can be shown that APG converges within √ O(1/ ) complexity, signiﬁcantly better than the subgradient method. [sent-53, score-0.829]
</p><p>31 For some simple functions, the proximal map indeed is available in closed-form, see [9] for a nice survey. [sent-54, score-0.715]
</p><p>32 However, for more complicated functions such as the one we consider here, the proximal map itself is expensive to compute and an inner iterative subroutine is required. [sent-55, score-0.788]
</p><p>33 Somewhat disappointingly, recent analysis has shown that such a two-loop procedure can be as slow as the subgradient method [10]. [sent-56, score-0.114]
</p><p>34 Yet another approach, popularized by Nesterov [8], is to approximate each nonsmooth component fk with a smooth function and then run APG. [sent-57, score-0.575]
</p><p>35 By carefully balancing the approximation and the convergence requirement of APG, the smoothed accelerated proximal gradient (S-APG) proposed in [8] converges in at most O( 1/ 2 + 1/ ) steps, again much better than the subgradient method. [sent-58, score-0.94]
</p><p>36 Each proximal map Pµk can be computed “easily” for any µ > 0. [sent-61, score-0.715]
</p><p>37 do 3: zt = yt − µ (yt ), 4: xt = k αk · Pµk (zt ), √ f2 1+ 1+4ηt 5: ηt+1 = , 2 ηt −1 6: yt+1 = xt + ηt+1 (xt − xt−1 ). [sent-68, score-0.14]
</p><p>38 do 3: 4:  zt = xt−1 − µ xt =  k  αk ·  (xt−1 ),  Pµk (zt ). [sent-74, score-0.076]
</p><p>39 f  5: end for  We prefer to leave the exact meaning of “easily” unspeciﬁed, but roughly speaking, the proximal map should be no more expensive than computing the gradient of the smooth part so that it does not become the bottleneck. [sent-75, score-0.872]
</p><p>40 Unfortunately, in general, there is no known efﬁcient way that reduces the proximal map of the ¯ average f to the proximal maps of its individual components fk , therefore the fast scheme APG is not readily applicable. [sent-78, score-1.544]
</p><p>41 The main difﬁculty, of course, is due to the nonlinearity of the proximal map Pµ , when treated as an operator on the function f . [sent-79, score-0.715]
</p><p>42 Despite of this fact, we will “naively” pretend f that the proximal map is linear and use ? [sent-80, score-0.715]
</p><p>43 In this example, fk (x) = xgk where gk is a group (subset) of variables and xg denotes a copy of x with all variables not contained in the group g set to 0. [sent-93, score-0.388]
</p><p>44 This group regularizer has been proven quite useful in high-dimensional statistics with the capability of selecting meaningful groups of features [5]. [sent-94, score-0.157]
</p><p>45 ¯ f Clearly each fk is convex and 1-Lipschitz continuous w. [sent-96, score-0.267]
</p><p>46 Moreover, the proximal map Pµk is simply a re-scaling of the variables in group gk , that is f [Pµk (x)]j = f  xj , j ∈ gk , (1 − µ/ xgk )+ xj , j ∈ gk  (3)  where (λ)+ = max{λ, 0}. [sent-102, score-1.028]
</p><p>47 This example is an enhanced version of the fused lasso [12], with some graph structure exploited to improve feature selection in biostatistic applications [6]. [sent-105, score-0.263]
</p><p>48 Speciﬁcally, given some graph whose nodes correspond to the feature variables, we let fij (x) = |xi − xj | for every edge (i, j) ∈ E. [sent-106, score-0.093]
</p><p>49 For a general graph, the proximal map of the regularizer ¯ f = (i,j)∈E αij fij , with αij ≥ 0, (i,j)∈E αij = 1, is not easily computable. [sent-107, score-0.822]
</p><p>50 Similar as above, each fij is 1-Lipschitz continuous w. [sent-108, score-0.084]
</p><p>51 Moreover, the proximal map Pµij is easy to compute: f xs , xs − sign(xi − xj ) min{µ, |xi − xj |/2}, Again, both our assumptions are satisﬁed. [sent-112, score-0.821]
</p><p>52 s ∈ {i, j}  (4)  Note that in both examples we could have incorporated weights into the component functions fk or fij , which amounts to changing αk or αij accordingly. [sent-114, score-0.302]
</p><p>53 3  Technical Tools  To justify our new algorithm, we need a few technical tools from convex analysis [14]. [sent-117, score-0.117]
</p><p>54 Denote Γ0 as the set of all lower semicontinuous proper convex functions f : H → R ∪ {∞}. [sent-119, score-0.106]
</p><p>55 For any f ∈ Γ0 , we deﬁne its Moreau envelop (with parameter µ > 0) [14, 15] 1 Mµ (x) = min 2µ x − y f  2  y  + f (y),  (5)  2  (6)  and correspondingly the proximal map 1 Pµ (x) = argmin 2µ x − y f  + f (y). [sent-127, score-0.833]
</p><p>56 y  Since f is closed convex and · 2 is strongly convex, the proximal map is well-deﬁned and singlevalued. [sent-128, score-0.784]
</p><p>57 As mentioned before, the proximal map is the key component of fast schemes such as APG. [sent-129, score-0.77]
</p><p>58 We summarize some nice properties of the Moreau envelop and the proximal map as: Proposition 1. [sent-130, score-0.807]
</p><p>59 ii), albeit being trivial, is the driving force behind the proximal point algorithm [16]. [sent-139, score-0.597]
</p><p>60 iii) justiﬁes the “niceness” of the Moreau envelop and connects it with the proximal map. [sent-140, score-0.689]
</p><p>61 And lastly vi), known as Moreau’s identity [15], plays an important role in the early development of convex analysis. [sent-142, score-0.1]
</p><p>62 Let SCµ ⊆ Γ0 denote the class of µ-strongly convex functions, that is, functions f such that f − µq is convex. [sent-145, score-0.106]
</p><p>63 Similarly, let SSµ ⊆ Γ0 denote the class of ﬁnite-valued functions whose gradient is µ-Lipschitz continuous (w. [sent-146, score-0.115]
</p><p>64 A well-known duality between strong convexity and smoothness is that for f ∈ Γ0 , we have f ∈ SCµ iff f ∗ ∈ SS1/µ , cf. [sent-150, score-0.101]
</p><p>65 The Moreau envelop map Mµ : Γ0 → SS1/µ that sends f ∈ Γ0 to Mµ is f bijective, increasing, and concave on any convex subset of Γ0 (under the pointwise order). [sent-156, score-0.324]
</p><p>66 4  It is clear that SS1/µ is a convex subset of Γ0 , which motivates the deﬁnition of the proximal K ¯ average—the key object to us. [sent-157, score-0.666]
</p><p>67 Recall that f = k αk fk ¯ is the convex combination of the component functions {fk } under the with each fk ∈ Γ0 , i. [sent-159, score-0.485]
</p><p>68 The µ µ proximal average Af ,α , or simply A when the component functions and weights are clear from K k=1  context, is the unique function h ∈ Γ0 such that Mµ = h  αk Mµk . [sent-171, score-0.699]
</p><p>69 f  Indeed, the existence of the proximal average follows from the surjectivity of Mµ while the uniqueness follows from the injectivity of Mµ , both proven in Proposition 2. [sent-172, score-0.631]
</p><p>70 The main property of the proximal average, as seen from its deﬁnition, is that its Moreau envelop is the convex combination of the Moreau envelops of the component functions. [sent-173, score-0.789]
</p><p>71 It is well-known that as µ → 0, Mµ → f pointwise [14], which, under the Lipschitz assumption, f can be strengthened to uniform convergence (Proof in Appendix B): 2 ¯ Proposition 3. [sent-180, score-0.107]
</p><p>72 A  2  ¯ For the proximal average, [11] showed that Aµ → f pointwise, which again can be strengthened to uniform convergence (proof follows from (10) and Proposition 3 since Aµ ≥ Mµµ ): A ¯ Proposition 4. [sent-182, score-0.659]
</p><p>73 ¯ As it turns out, S-APG approximates the nonsmooth function f with the smooth function Mµµ while A µ our algorithm operates on the nonsmooth approximation A (note that it can be shown that Aµ is smooth iff some component fi is smooth). [sent-184, score-0.867]
</p><p>74 Observe that the proximal A average Aµ remains nondifferentiable at 0 while Mµµ is smooth everywhere. [sent-190, score-0.734]
</p><p>75 For x ≥ 0, f1 = f2 = A ¯ f = Aµ (the red circled line), thus the proximal average Aµ is a strictly tighter approximation than ¯ smoothing. [sent-191, score-0.732]
</p><p>76 A ¯ meaning that the proximal average Aµ is a better under-approximation of f than Mµµ . [sent-193, score-0.631]
</p><p>77 A Let us compare the proximal average Aµ with the smooth approximation Mµµ on a 1-D example. [sent-194, score-0.79]
</p><p>78 Our ﬁx is almost trivial: If necessary, we use a bigger Lipschitz constant L0 = 1/µ so that we can compute the proximal map easily. [sent-207, score-0.715]
</p><p>79 1/L  Note that if we could reduce PAµ 0 efﬁciently to Pµµ , we would end up with the optimal (overall) A ¯ rate O( 1/ ), even though we approximate the nonsmooth function f by the proximal average µ A . [sent-214, score-0.898]
</p><p>80 It is our incapability to (efﬁciently) relate proximal maps that leads to the sacriﬁce in convergence rates. [sent-216, score-0.625]
</p><p>81 5  Discussions  To ease our discussion with related works, let us ﬁrst point out a fact that is not always explicitly ¯ recognized, that is, S-APG essentially relies on approximating the nonsmooth function f with Mµµ . [sent-217, score-0.267]
</p><p>82 The smoothing idea introduced in [8] purports the superﬁcial max-structure assumption, that is, f (x) = maxy∈C x, y − h(y) where C is some bounded convex set and h ∈ Γ0 . [sent-219, score-0.143]
</p><p>83 the norm · ) iff dom f ∗ ⊆ B · (0, M ), the ball centered at the origin with radius M . [sent-223, score-0.089]
</p><p>84 Finally for the general case where f is an average of K nonsmooth functions, the ¯ smoothing technique is applied in a component by component way, i. [sent-230, score-0.437]
</p><p>85 A For comparison, let us recall that S-APG ﬁnds a 2 O( L0 +  M 2 /(2  accurate solution in at most  ) 1/ ) steps since the Lipschitz constant of the gradient of  + Mµµ is upA  per bounded by L0 + M 2 /(2 ) (under the choice of µ in Theorem 1). [sent-233, score-0.081]
</p><p>86 In some sense it is quite remarkable that the seemingly “naive” approximation that pretends the linearity of the proximal map not only can be justiﬁed but also leads to a strictly better result. [sent-237, score-0.944]
</p><p>87 As mentioned, S-APG approximates f with the smooth function Mµµ . [sent-239, score-0.103]
</p><p>88 This smooth approximation is beneﬁcial if our capability is limited to A smooth functions. [sent-240, score-0.298]
</p><p>89 Put differently, S-APG implicitly treats applying the fast gradient algorithms as the ultimate goal. [sent-241, score-0.122]
</p><p>90 However, the recent advances on nonsmooth optimization have broadened the range of fast schemes: It is not smoothness but the proximal map that allows fast convergence. [sent-242, score-1.055]
</p><p>91 Just as how APG improves upon the subgradient method, our approach, with the ultimate goal to enable efﬁcient computation of the proximal map, improves upon S-APG. [sent-243, score-0.755]
</p><p>92 To summarize, smoothing is not free and it should be used when truly needed. [sent-245, score-0.074]
</p><p>93 6  Experiments  We compare the proposed algorithm with S-APG on two important problems: overlapping group lasso and graph-guided fused lasso. [sent-248, score-0.382]
</p><p>94 See Example 1 and Example 2 for details about the nonsmooth ¯ function f . [sent-249, score-0.267]
</p><p>95 7  Conclusions  We have considered the composite minimization problem which consists of a smooth loss and a sum of nonsmooth regularizers. [sent-288, score-0.453]
</p><p>96 Different from smoothing, we considered a seemingly naive nonsmooth approximation which simply pretends the linearity of the proximal map. [sent-289, score-1.076]
</p><p>97 Based on the proximal average, a new tool from convex analysis, we proved that the new approximation leads to a novel algorithm that strictly improves the state-of-the-art. [sent-290, score-0.788]
</p><p>98 Experiments on both overlapping group lasso and graph-guided fused lasso veriﬁed the superiority of the proposed method. [sent-291, score-0.48]
</p><p>99 Smoothing proximal gradient method for general structured sparse regression. [sent-339, score-0.675]
</p><p>100 Dual averaging and proximal gradient descent for online alternating direction multiplier method. [sent-361, score-0.673]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proximal', 0.597), ('apg', 0.43), ('nonsmooth', 0.267), ('pg', 0.181), ('fk', 0.174), ('fused', 0.165), ('moreau', 0.139), ('map', 0.118), ('pa', 0.118), ('subgradient', 0.114), ('smooth', 0.103), ('lasso', 0.098), ('envelop', 0.092), ('mf', 0.082), ('smoothing', 0.074), ('accelerated', 0.07), ('proposition', 0.07), ('convex', 0.069), ('pretends', 0.068), ('lipschitz', 0.065), ('composite', 0.061), ('group', 0.061), ('fij', 0.06), ('overlapping', 0.058), ('approximation', 0.056), ('pf', 0.055), ('gradient', 0.054), ('fenchel', 0.052), ('yaoliang', 0.052), ('gk', 0.047), ('pointwise', 0.045), ('xgk', 0.045), ('strictly', 0.045), ('ultimate', 0.044), ('assumption', 0.04), ('yurii', 0.04), ('heinz', 0.04), ('lucet', 0.04), ('yves', 0.04), ('iff', 0.04), ('xt', 0.039), ('zt', 0.037), ('functions', 0.037), ('fix', 0.037), ('seyoung', 0.037), ('ralph', 0.037), ('complicated', 0.036), ('justi', 0.036), ('duality', 0.036), ('capability', 0.036), ('groups', 0.036), ('ij', 0.035), ('mk', 0.035), ('linearity', 0.035), ('bauschke', 0.034), ('strengthened', 0.034), ('average', 0.034), ('xj', 0.033), ('iv', 0.032), ('component', 0.031), ('alberta', 0.031), ('lastly', 0.031), ('stepsize', 0.03), ('argminx', 0.03), ('maxy', 0.03), ('naive', 0.028), ('convergence', 0.028), ('dom', 0.027), ('id', 0.027), ('recall', 0.027), ('argmin', 0.026), ('sc', 0.025), ('justify', 0.025), ('smoothness', 0.025), ('seemingly', 0.025), ('yt', 0.025), ('structured', 0.024), ('regularizer', 0.024), ('fast', 0.024), ('continuous', 0.024), ('regularizers', 0.023), ('monotone', 0.023), ('patrick', 0.023), ('tools', 0.023), ('eric', 0.023), ('easily', 0.023), ('clearly', 0.023), ('ax', 0.022), ('norm', 0.022), ('author', 0.022), ('differently', 0.022), ('minimization', 0.022), ('af', 0.022), ('alternating', 0.022), ('smoothed', 0.021), ('tool', 0.021), ('xs', 0.02), ('contend', 0.02), ('legitimate', 0.02), ('keith', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="56-tfidf-1" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>2 0.28971586 <a title="56-tfidf-2" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>3 0.22810081 <a title="56-tfidf-3" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>4 0.18076621 <a title="56-tfidf-4" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>Author: Stefanie Jegelka, Francis Bach, Suvrit Sra</p><p>Abstract: Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efﬁcient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reﬂections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the beneﬁts of our method on two image segmentation tasks. 1</p><p>5 0.16299047 <a title="56-tfidf-5" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>6 0.13150673 <a title="56-tfidf-6" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>7 0.10381906 <a title="56-tfidf-7" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>8 0.099706061 <a title="56-tfidf-8" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>9 0.096338421 <a title="56-tfidf-9" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>10 0.082434863 <a title="56-tfidf-10" href="./nips-2013-Mixed_Optimization_for_Smooth_Functions.html">193 nips-2013-Mixed Optimization for Smooth Functions</a></p>
<p>11 0.078935817 <a title="56-tfidf-11" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>12 0.07720273 <a title="56-tfidf-12" href="./nips-2013-Simultaneous_Rectification_and_Alignment_via_Robust_Recovery_of_Low-rank_Tensors.html">295 nips-2013-Simultaneous Rectification and Alignment via Robust Recovery of Low-rank Tensors</a></p>
<p>13 0.075473465 <a title="56-tfidf-13" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>14 0.073193595 <a title="56-tfidf-14" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>15 0.06567324 <a title="56-tfidf-15" href="./nips-2013-Estimating_LASSO_Risk_and_Noise_Level.html">109 nips-2013-Estimating LASSO Risk and Noise Level</a></p>
<p>16 0.061534759 <a title="56-tfidf-16" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>17 0.061324697 <a title="56-tfidf-17" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>18 0.060846549 <a title="56-tfidf-18" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<p>19 0.053746007 <a title="56-tfidf-19" href="./nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</a></p>
<p>20 0.053283297 <a title="56-tfidf-20" href="./nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, 0.036), (2, 0.088), (3, 0.061), (4, 0.008), (5, 0.079), (6, -0.139), (7, -0.003), (8, 0.035), (9, -0.044), (10, 0.016), (11, 0.029), (12, -0.013), (13, -0.142), (14, -0.121), (15, 0.003), (16, 0.037), (17, -0.0), (18, 0.048), (19, 0.07), (20, 0.013), (21, 0.107), (22, 0.134), (23, 0.126), (24, -0.008), (25, -0.1), (26, 0.096), (27, -0.006), (28, 0.104), (29, 0.305), (30, -0.203), (31, 0.107), (32, -0.018), (33, -0.081), (34, -0.167), (35, -0.102), (36, -0.017), (37, 0.196), (38, -0.009), (39, -0.004), (40, 0.04), (41, 0.003), (42, 0.034), (43, -0.016), (44, -0.032), (45, 0.063), (46, 0.03), (47, -0.156), (48, -0.098), (49, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95904547 <a title="56-lsi-1" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><p>2 0.9442274 <a title="56-lsi-2" href="./nips-2013-On_Decomposing_the_Proximal_Map.html">215 nips-2013-On Decomposing the Proximal Map</a></p>
<p>Author: Yao-Liang Yu</p><p>Abstract: The proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems. For simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial. Motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands. We not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory. 1</p><p>3 0.77721024 <a title="56-lsi-3" href="./nips-2013-Polar_Operators_for_Structured_Sparse_Estimation.html">249 nips-2013-Polar Operators for Structured Sparse Estimation</a></p>
<p>Author: Xinhua Zhang, Yao-Liang Yu, Dale Schuurmans</p><p>Abstract: Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately, these estimators normally create computational difﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efﬁciently. With such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efﬁcient latent fused lasso. 1</p><p>4 0.54966313 <a title="56-lsi-4" href="./nips-2013-Stochastic_Majorization-Minimization_Algorithms_for_Large-Scale_Optimization.html">313 nips-2013-Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization</a></p>
<p>Author: Julien Mairal</p><p>Abstract: Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with largescale or possibly inﬁnite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence √ rate of O(1/ n) after n iterations, and of O(1/n) for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efﬁcient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale ℓ1 logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems. 1</p><p>5 0.54860461 <a title="56-lsi-5" href="./nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</a></p>
<p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><p>6 0.51719296 <a title="56-lsi-6" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>7 0.47257197 <a title="56-lsi-7" href="./nips-2013-The_Total_Variation_on_Hypergraphs_-_Learning_on_Hypergraphs_Revisited.html">328 nips-2013-The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited</a></p>
<p>8 0.46514609 <a title="56-lsi-8" href="./nips-2013-Reflection_methods_for_user-friendly_submodular_optimization.html">268 nips-2013-Reflection methods for user-friendly submodular optimization</a></p>
<p>9 0.40160203 <a title="56-lsi-9" href="./nips-2013-Convex_Calibrated_Surrogates_for_Low-Rank_Loss_Matrices_with_Applications_to_Subset_Ranking_Losses.html">72 nips-2013-Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses</a></p>
<p>10 0.37844583 <a title="56-lsi-10" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>11 0.36578718 <a title="56-lsi-11" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>12 0.36194795 <a title="56-lsi-12" href="./nips-2013-Generalized_Method-of-Moments_for_Rank_Aggregation.html">128 nips-2013-Generalized Method-of-Moments for Rank Aggregation</a></p>
<p>13 0.33497092 <a title="56-lsi-13" href="./nips-2013-Projected_Natural_Actor-Critic.html">257 nips-2013-Projected Natural Actor-Critic</a></p>
<p>14 0.32981485 <a title="56-lsi-14" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>15 0.32461777 <a title="56-lsi-15" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>16 0.32277852 <a title="56-lsi-16" href="./nips-2013-Reconciling_%22priors%22_%26_%22priors%22_without_prejudice%3F.html">265 nips-2013-Reconciling "priors" & "priors" without prejudice?</a></p>
<p>17 0.31908575 <a title="56-lsi-17" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<p>18 0.31854761 <a title="56-lsi-18" href="./nips-2013-On_Algorithms_for_Sparse_Multi-factor_NMF.html">214 nips-2013-On Algorithms for Sparse Multi-factor NMF</a></p>
<p>19 0.31329513 <a title="56-lsi-19" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>20 0.30787838 <a title="56-lsi-20" href="./nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.039), (33, 0.124), (34, 0.094), (41, 0.033), (49, 0.028), (56, 0.088), (70, 0.36), (85, 0.035), (89, 0.021), (93, 0.066), (95, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.957753 <a title="56-lda-1" href="./nips-2013-Recurrent_networks_of_coupled_Winner-Take-All_oscillators_for_solving_constraint_satisfaction_problems.html">267 nips-2013-Recurrent networks of coupled Winner-Take-All oscillators for solving constraint satisfaction problems</a></p>
<p>Author: Hesham Mostafa, Lorenz. K. Mueller, Giacomo Indiveri</p><p>Abstract: We present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems. Discrete variables are represented by coupled Winner-Take-All (WTA) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks. Constraints over the variables are encoded in the network connectivity. Although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations. If there is no solution that satisﬁes all constraints, the network state changes in a seemingly random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisﬁed by this assignment. External evidence, or input to the network, can force variables to speciﬁc values. When new inputs are applied, the network re-evaluates the entire set of variables in its search for states that satisfy the maximum number of constraints, while being consistent with the external input. Our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions. The network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits. 1</p><p>2 0.90351194 <a title="56-lda-2" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>Author: Christian Szegedy, Alexander Toshev, Dumitru Erhan</p><p>Abstract: Deep Neural Networks (DNNs) have recently shown outstanding performance on image classiﬁcation tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We deﬁne a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC. 1</p><p>3 0.89415199 <a title="56-lda-3" href="./nips-2013-Learning_Multi-level_Sparse_Representations.html">157 nips-2013-Learning Multi-level Sparse Representations</a></p>
<p>Author: Ferran Diego Andilla, Fred A. Hamprecht</p><p>Abstract: Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reﬂected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron → assembly that should ﬁnd their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the ﬁrst formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difﬁcult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data. 1</p><p>4 0.89159292 <a title="56-lda-4" href="./nips-2013-A_message-passing_algorithm_for_multi-agent_trajectory_planning.html">16 nips-2013-A message-passing algorithm for multi-agent trajectory planning</a></p>
<p>Author: Jose Bento, Nate Derbinsky, Javier Alonso-Mora, Jonathan S. Yedidia</p><p>Abstract: We describe a novel approach for computing collision-free global trajectories for p agents with speciﬁed initial and ﬁnal conﬁgurations, based on an improved version of the alternating direction method of multipliers (ADMM). Compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments. We apply our method to classical challenging instances and observe that its computational requirements scale well with p for several cost functionals. We also show that a specialization of our algorithm can be used for local motion planning by solving the problem of joint optimization in velocity space. 1</p><p>5 0.87586945 <a title="56-lda-5" href="./nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</a></p>
<p>Author: Shaodan Zhai, Tian Xia, Ming Tan, Shaojun Wang</p><p>Abstract: We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classiﬁer of weak classiﬁers through directly minimizing empirical classiﬁcation error over labeled training examples; once the training classiﬁcation error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classiﬁers to maximize any targeted arbitrarily deﬁned margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n′ th order bottom sample margin. 1</p><p>same-paper 6 0.83264691 <a title="56-lda-6" href="./nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</a></p>
<p>7 0.80140692 <a title="56-lda-7" href="./nips-2013-A_memory_frontier_for_complex_synapses.html">15 nips-2013-A memory frontier for complex synapses</a></p>
<p>8 0.70114613 <a title="56-lda-8" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>9 0.69573492 <a title="56-lda-9" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>10 0.685193 <a title="56-lda-10" href="./nips-2013-Correlations_strike_back_%28again%29%3A_the_case_of_associative_memory_retrieval.html">77 nips-2013-Correlations strike back (again): the case of associative memory retrieval</a></p>
<p>11 0.66800374 <a title="56-lda-11" href="./nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</a></p>
<p>12 0.64668679 <a title="56-lda-12" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>13 0.63446379 <a title="56-lda-13" href="./nips-2013-Hierarchical_Modular_Optimization_of_Convolutional_Networks_Achieves_Representations_Similar_to_Macaque_IT_and_Human_Ventral_Stream.html">136 nips-2013-Hierarchical Modular Optimization of Convolutional Networks Achieves Representations Similar to Macaque IT and Human Ventral Stream</a></p>
<p>14 0.62867498 <a title="56-lda-14" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>15 0.61797208 <a title="56-lda-15" href="./nips-2013-Reciprocally_Coupled_Local_Estimators_Implement_Bayesian_Information_Integration_Distributively.html">264 nips-2013-Reciprocally Coupled Local Estimators Implement Bayesian Information Integration Distributively</a></p>
<p>16 0.61090803 <a title="56-lda-16" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>17 0.60933244 <a title="56-lda-17" href="./nips-2013-Probabilistic_Movement_Primitives.html">255 nips-2013-Probabilistic Movement Primitives</a></p>
<p>18 0.60192305 <a title="56-lda-18" href="./nips-2013-Reservoir_Boosting_%3A_Between_Online_and_Offline_Ensemble_Learning.html">275 nips-2013-Reservoir Boosting : Between Online and Offline Ensemble Learning</a></p>
<p>19 0.59796274 <a title="56-lda-19" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>20 0.59588754 <a title="56-lda-20" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
