<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>256 nips-2013-Probabilistic Principal Geodesic Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-256" href="#">nips2013-256</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>256 nips-2013-Probabilistic Principal Geodesic Analysis</h1>
<br/><p>Source: <a title="nips-2013-256-pdf" href="http://papers.nips.cc/paper/5133-probabilistic-principal-geodesic-analysis.pdf">pdf</a></p><p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>Reference: <a title="nips-2013-256-reference" href="../nips2013_reference/nips-2013-Probabilistic_Principal_Geodesic_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. [sent-6, score-0.601]
</p><p>2 Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. [sent-8, score-0.177]
</p><p>3 We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. [sent-10, score-0.633]
</p><p>4 Other examples of latent variable models include probabilistic canonical correlation analysis (CCA) [1] and Gaussian process latent variable models [15]. [sent-16, score-0.228]
</p><p>5 Another important example of manifold data is in shape analysis, where the deﬁnition of the shape of an object should not depend on its position, orientation, or scale. [sent-22, score-0.462]
</p><p>6 The result is a manifold representation of shape, or shape space. [sent-24, score-0.328]
</p><p>7 Linear operations violate the natural constraints of manifold data, e. [sent-25, score-0.194]
</p><p>8 , a linear average of data on a sphere results in a vector that does not have unit length. [sent-27, score-0.169]
</p><p>9 As shown recently [5], using the kernel trick with a Gaussian kernel maps data onto a Hilbert sphere, and utilizing Riemannian distances on this sphere rather than Euclidean distances improves clustering and classiﬁcation performance. [sent-28, score-0.249]
</p><p>10 Other examples of manifold data include geometric transformations, such as rotations and afﬁne transforms, symmetric positive-deﬁnite tensors [9, 24], Grassmannian manifolds (the set of m-dimensional linear subspaces of Rn ), and Stiefel manifolds (the set of orthonormal m-frames in Rn ) [23]. [sent-29, score-0.737]
</p><p>11 It’s important to note 1  the distinction between manifold data, where the manifold representation is known a priori, versus manifold learning and nonlinear component analysis [15, 20], where the data lies in Euclidean space on some unknown, lower-dimensional manifold that must be learned. [sent-36, score-0.816]
</p><p>12 Principal geodesic analysis (PGA) [10] generalizes PCA to nonlinear manifolds. [sent-37, score-0.406]
</p><p>13 It describes the geometric variability of manifold data by ﬁnding lower-dimensional geodesic subspaces that minimize the residual sum-of-squared geodesic distances to the data. [sent-38, score-1.096]
</p><p>14 Related work on manifold component analysis has introduced variants of PGA. [sent-40, score-0.234]
</p><p>15 This includes relaxing the constraint that geodesics pass through the mean of the data [11] and, for spherical data, replacing geodesic subspaces with nested spheres of arbitrary radius [13]. [sent-41, score-0.677]
</p><p>16 , they ﬁnd subspaces that minimize the sum-of-squared geodesic distances to the data. [sent-44, score-0.468]
</p><p>17 Much like the original formulation of PCA, current component analysis methods on manifolds lack a probabilistic interpretation. [sent-45, score-0.267]
</p><p>18 However, due to the lack of an explicit formulation for the normalizing constant, our estimation is limited to symmetric spaces, which include many common manifolds such as Euclidean space, spheres, Kendall shape spaces, Grassman/Stiefel manifolds, and more. [sent-48, score-0.441]
</p><p>19 Recall that a Riemannian manifold is a differentiable manifold M equipped with a metric g, which is a smoothly varying inner product on the tangent spaces of M . [sent-51, score-0.59]
</p><p>20 Given two vector ﬁelds v, w on M , the covariant derivative v w gives the change of the vector ﬁeld w in the v direction. [sent-52, score-0.177]
</p><p>21 The covariant derivative is a generalization of the Euclidean directional derivative to the manifold setting. [sent-53, score-0.515]
</p><p>22 Given a vector ﬁeld ˙ V (t) deﬁned along γ, we can deﬁne the covariant derivative of V to be DV = γ V . [sent-55, score-0.177]
</p><p>23 A vector ﬁeld ˙ dt is called parallel if the covariant derivative along the curve γ is zero. [sent-56, score-0.177]
</p><p>24 A curve γ is geodesic if it satisﬁes the equation γ γ = 0. [sent-57, score-0.406]
</p><p>25 In other words, geodesics are curves with zero acceleration. [sent-58, score-0.176]
</p><p>26 ˙ ˙ Recall that for any point p ∈ M and tangent vector v ∈ Tp M , the tangent space of M at p, there is a unique geodesic curve γ, with initial conditions γ(0) = p and γ(0) = v. [sent-59, score-0.706]
</p><p>27 This geodesic is only ˙ guaranteed to exist locally. [sent-60, score-0.406]
</p><p>28 When γ is deﬁned over the interval [0, 1], the Riemannian exponential map at p is deﬁned as Expp (v) = γ(1). [sent-61, score-0.137]
</p><p>29 In other words, the exponential map takes a position and velocity as input and returns the point at time 1 along the geodesic with these initial conditions. [sent-62, score-0.599]
</p><p>30 The exponential map is locally diffeomorphic onto a neighbourhood of p. [sent-63, score-0.198]
</p><p>31 Then within V (p) the exponential map has an inverse, the Riemannian log map, Logp : V (p) → Tp M . [sent-65, score-0.168]
</p><p>32 3  Probabilistic Principal Geodesic Analysis  Before introducing our PPGA model for manifold data, we ﬁrst review PPCA. [sent-71, score-0.194]
</p><p>33 This removes the rotation ambiguity of the latent factors and makes them analogous to the eigenvectors and eigenvalues of standard PCA (there is still of course an ambiguity of the ordering of the factors). [sent-74, score-0.15]
</p><p>34 1  Probability Model  Following [8, 17], we use a generalization of the normal distribution for a Riemannian manifold as our noise model. [sent-77, score-0.235]
</p><p>35 Consider a random variable y taking values on a Riemannian manifold M , deﬁned by the probability density function (pdf) 1 τ exp − d(µ, y)2 , C(µ, τ ) 2 τ C(µ, τ ) = exp − d(µ, y)2 dy. [sent-78, score-0.417]
</p><p>36 We note that this noise model could be replaced with a different distribution, perhaps speciﬁc to the type of manifold or application, and the inference procedure presented in the next section could be modiﬁed accordingly. [sent-82, score-0.194]
</p><p>37 In this model, a linear combination of W Λ and the latent variables x forms a new tangent vector z ∈ Tµ M . [sent-84, score-0.216]
</p><p>38 Next, the exponential map shoots the base point µ by z to generate the location parameter of a Riemannian normal distribution, from which the data point y is drawn. [sent-85, score-0.178]
</p><p>39 Note that in Euclidean space, the exponential map is an addition operation, Exp(µ, z) = µ + z. [sent-86, score-0.137]
</p><p>40 , yN } on M , with associated latent variable xi ∈ Rq , and zi = W Λxi , we formulate an expectation maximization (EM) algorithm. [sent-93, score-0.224]
</p><p>41 Denote xij as the jth sample for xi , the Monte Carlo approximation of the Q function is given by N  Q(θ|θk ) = Exi |yi ;θk  log p(xi |yi ; θk ) ≈ i=1  1 S  S  N  log p(xij |yi ; θk ). [sent-98, score-0.14]
</p><p>42 The computation of the gradient term xi U (xi ) requires we compute dv Exp(p, v), i. [sent-102, score-0.169]
</p><p>43 , the derivative operator (Jacobian matrix) of the exponential map with respect to the initial velocity v. [sent-104, score-0.29]
</p><p>44 To derive this, consider a variation of geodesics c(s, t) = Exp(p, su + tv), where u ∈ Tp M . [sent-105, score-0.248]
</p><p>45 The variation c produces a “fan” of geodesics; this is illustrated for a sphere on the left side of Figure 1. [sent-106, score-0.209]
</p><p>46 Finally, this gives an expression for the exponential map derivative as dv Exp(p, v)u = Jv (1). [sent-108, score-0.3]
</p><p>47 However, Jacobi ﬁelds can be evaluated in closed-form for the class of manifolds known as symmetric spaces. [sent-110, score-0.231]
</p><p>48 For the sphere and Kendall shape space examples, we provide explicit formulas for these computations in Section 4. [sent-111, score-0.303]
</p><p>49 Now, the gradient with respect to each xi is xi U  = xi − τ ΛW T {dzi Exp(µ, zi )† Log(Exp(µ, zi ), yi )},  (7)  where † represents the adjoint of a linear operator, i. [sent-113, score-0.383]
</p><p>50 Gradient for τ : The gradient of the Q function with respect to τ requires evaulation of the derivative of the normalizing constant in the Riemannian normal distribution (2). [sent-120, score-0.241]
</p><p>51 Thus, the normalizing constant can be written as C(τ ) =  τ exp − d(µ, y)2 dy. [sent-122, score-0.146]
</p><p>52 2 M  We can rewrite this integral in normal coordinates, which can be thought of as a polar coordinate system in the tangent space, Tµ M . [sent-123, score-0.223]
</p><p>53 Now the integral for the normalizing constant becomes R(v)  C(τ ) = S n−1  0  τ exp − r2 | det(dφ(rv))|dr dv, 2  (8)  where R(v) is the maximum distance that φ(rv) is deﬁned. [sent-128, score-0.178]
</p><p>54 , un , with u1 = v, such that n √ 1 | det(dφ(rv))| = (9) √ fk ( κk r), κk k=2  4  where κk = K(u1 , uk ) denotes the sectional curvature, and fk is deﬁned as  if κk > 0, sin(x) fk (x) = sinh(x) if κk < 0,  x if κk = 0. [sent-135, score-0.225]
</p><p>55 Also, if M is simply connected, then R(v) = R does not depend on the direction v, and we can write the normalizing constant as R  C(τ ) = An−1 0  τ exp − r2 2  n −1/2  κk  √ fk ( κk r)dr,  k=2  where An−1 is the surface area of the n − 1 hypersphere, S n−1 . [sent-137, score-0.201]
</p><p>56 While this formula works only for simply connected symmetric spaces, other symmetric spaces could be handled by lifting to the universal cover, which is simply connected, or by restricting the deﬁnition of the Riemannian normal pdf in (2) to have support only up to the injectivity radius, i. [sent-139, score-0.185]
</p><p>57 The gradient term for estimating τ is N τQ  S  = i=1 j=1  1 An−1 C(τ )  R 0  n  τ r2 exp − r2 2 2  −1/2  κk k=2  √ 1 fk ( κk r)dr− d(Exp(µ, zij ), yi )2 dr. [sent-142, score-0.343]
</p><p>58 2  Gradient for µ: From (4) and (5), the gradient term for updating µ is µQ =  1 S  S  N  τ dµ Exp(µ, zij )† Log (Exp(µ, zij ), yi ). [sent-143, score-0.275]
</p><p>59 Similar to before (6), this derivative can be derived from a variation of geodesics: c(s, t) = Exp(Exp(µ, su), tv(s)), where v(s) comes from parallel translating v along the geodesic Exp(µ, su). [sent-145, score-0.543]
</p><p>60 Again, the derivative of the exponential map is given by a Jacobi ﬁeld satisfying Jµ (t) = dc/ds(0, t), and we have dµ Exp(µ, v) = Jµ (1). [sent-146, score-0.234]
</p><p>61 each ath diagonal element Λa as ∂Q 1 = ∂Λa S  N  S  τ (W a xa )T {dzij Exp(µ, zij )† Log(Exp(µ, zij ), yi )}, ij i=1 j=1  a  where W denotes the ath column of W , and xa is the ath component of xij . [sent-150, score-0.437]
</p><p>62 W is WQ =  1 S  N  S  τ dzij Exp(µ, zij )† Log(Exp(µ, zij ), yi )xT Λ. [sent-154, score-0.26]
</p><p>63 ij  (10)  i=1 j=1  To preserve the mutual orthogonality constraint on the columns of W , we represent W as a point on the Stiefel manifold Vq (Tµ M ), i. [sent-155, score-0.194]
</p><p>64 We project the gradient in (10) onto the tangent space TW Vq (Tµ M ), and then update W by taking a small step along the geodesic in the projected gradient direction. [sent-158, score-0.692]
</p><p>65 For details on the geodesic computations for Stiefel manifolds, see [7]. [sent-159, score-0.406]
</p><p>66 The MCEM algorithm for PPGA is an iterative procedure for ﬁnding the subspace spanned by q principal components, shown in Algorithm 1. [sent-160, score-0.155]
</p><p>67 until convergence  4  Experiments  In this section, we demonstrate the effectiveness of PPGA and our ML estimation using both simulated data on the 2D sphere and a real corpus callosum data set. [sent-169, score-0.458]
</p><p>68 Before presenting the experiments of PPGA, we brieﬂy review the necessary computations for the speciﬁc types of manifolds used, including, the Riemannian exponential map, log map, and Jacobi ﬁelds. [sent-170, score-0.271]
</p><p>69 1  Simulated Sphere Data  Sphere geometry overview: Let p be a point on an n-dimensional sphere embedded in Rn+1 , and let v be a tangent at p. [sent-172, score-0.358]
</p><p>70 The exponential map is given by a 2D rotation of p by an angle given by the norm of the tangent, i. [sent-174, score-0.195]
</p><p>71 (11) θ The log map between two points p, q on the sphere can be computed by ﬁnding the initial velocity of the rotation between the two points. [sent-177, score-0.396]
</p><p>72 The adjoint derivatives of the exponential map are given by dp Exp(p, v)† w = cos( v )w⊥ + w ,  dv Exp(p, v)† w =  sin( v ) ⊥ w +w , v  where w⊥ , w denote the components of w that are orthogonal and tangent to v, respectively. [sent-181, score-0.423]
</p><p>73 An illustration of geodesics and the Jacobi ﬁelds that give rise to the exponential map derivatives is shown in Figure 1. [sent-182, score-0.313]
</p><p>74 Parameter estimation on the sphere: Using our generative model for PGA (3), we forward simulated a random sample of 100 data points on the unit sphere S 2 , with known parameters θ = (µ, W, Λ, τ ), shown in Table 1. [sent-183, score-0.225]
</p><p>75 Figure 1 compares the ground truth principal geodesics and MLE principal geodesic analysis using our algorithm. [sent-187, score-0.961]
</p><p>76 A good overlap between the ﬁrst principal geodesic shows that PPGA recovers the model parameters. [sent-188, score-0.561]
</p><p>77 One advantage that our PPGA model has over the least-squares PGA formulation is that the mean point is estimated jointly with the principal geodesics. [sent-189, score-0.155]
</p><p>78 In the standard PGA algorithm, the mean is estimated ﬁrst (using geodesic least-squares), then the principal geodesics are estimated second. [sent-190, score-0.737]
</p><p>79 The estimation error of principal geodesics turned to be larger in PGA compared to our model. [sent-193, score-0.359]
</p><p>80 v  p  J(x)  M  Figure 1: Left: Jacobi ﬁelds; Right: the principal geodesic of random generated data on unit sphere. [sent-226, score-0.561]
</p><p>81 Next, points in this subspace are deemed equivalent if they are a rotation and scaling of each other, which can be represented as multiplication by a complex number, ρeiθ , where ρ is the scaling factor and θ is the rotation angle. [sent-233, score-0.147]
</p><p>82 We think of a centered shape p ∈ V as representing the complex line Lp = {z · p : z ∈ C\{0} }, i. [sent-235, score-0.165]
</p><p>83 A tangent vector at Lp ∈ V is a complex vector, v ∈ V , such that p, v = 0. [sent-238, score-0.181]
</p><p>84 The exponential map is given by rotating (within V ) the complex line Lp by the initial velocity v, that is, p sin θ · v, θ = v . [sent-239, score-0.278]
</p><p>85 (13) θ Likewise, the log map between two shapes p, q ∈ V is given by ﬁnding the initial velocity of the rotation between the two complex lines Lp and Lq . [sent-240, score-0.258]
</p><p>86 Then the log map is given by Exp(p, v) = cos θ · p +  Log(p, q) =  θ · (q − πp (q)) , q − πp (q)  θ = arccos  | p, q | . [sent-242, score-0.19]
</p><p>87 The adjoint derivatives of the exponential map are given by ⊥ ⊥ dp Exp(p, v)† w = cos( v )w1 + cos(2 v )w2 + w , sin( v ) ⊥ sin(2 v ) dv Exp(p, v)† w = w1 + + w2 , v 2 v ⊥ ⊥ where w1 denotes the component of w parallel to u1 , i. [sent-249, score-0.287]
</p><p>88 , w1 = w, u1 u1 , u2 denotes the remaining orthogonal component of w, and w denotes the component tangent to v. [sent-251, score-0.256]
</p><p>89 7  Shape variability of corpus callosum data: As a demonstration of PPGA on Kendall shape space, we applied it to corpus callosum shape data derived from the OASIS database (www. [sent-252, score-0.734]
</p><p>90 The corpus callosum was segmented in a midsagittal slice using the ITK SNAP program (www. [sent-256, score-0.261]
</p><p>91 An example of a segmented corpus callosum in an MRI is shown in Figure 2. [sent-259, score-0.261]
</p><p>92 Figure 2 displays the ﬁrst two modes of corpus callosum shape variation, generated from the as points along the estimated principal geodesics: Exp(µ, αi wi ), where αi = −3λi , −1. [sent-266, score-0.522]
</p><p>93 5λ2 3λ2  Figure 2: Left: example corpus callosum segmentation from an MRI slice. [sent-273, score-0.233]
</p><p>94 Middle to right: ﬁrst and second PGA mode of shape variation with −3, −1. [sent-274, score-0.174]
</p><p>95 Nonparametric bayesian density estimation on manifolds with applications to planar shapes. [sent-293, score-0.213]
</p><p>96 Principal geodesic analysis on symmetric spaces: statistics of diffusion tensors. [sent-332, score-0.452]
</p><p>97 Statistics of shape via principal geodesic analysis on Lie groups. [sent-339, score-0.695]
</p><p>98 Principal component analysis for Riemannian manifolds, with an application to triangular shape spaces. [sent-344, score-0.174]
</p><p>99 Manifold valued statistics, exact principal geodesic analysis and the effect of linear approximations. [sent-402, score-0.561]
</p><p>100 Statistical computations on Grassmann and Stiefel manifolds for image and video-based recognition. [sent-416, score-0.185]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('geodesic', 0.406), ('pga', 0.36), ('riemannian', 0.346), ('ppga', 0.24), ('manifold', 0.194), ('manifolds', 0.185), ('geodesics', 0.176), ('sphere', 0.169), ('jacobi', 0.159), ('callosum', 0.159), ('principal', 0.155), ('tangent', 0.15), ('shape', 0.134), ('exp', 0.098), ('derivative', 0.097), ('pca', 0.09), ('zij', 0.085), ('map', 0.082), ('ppca', 0.081), ('covariant', 0.08), ('hmc', 0.076), ('corpus', 0.074), ('euclidean', 0.07), ('hamiltonian', 0.069), ('dv', 0.066), ('latent', 0.066), ('stiefel', 0.061), ('fletcher', 0.061), ('rv', 0.061), ('mcem', 0.06), ('sectional', 0.06), ('spheres', 0.06), ('rotation', 0.058), ('velocity', 0.056), ('carlo', 0.056), ('monte', 0.056), ('exponential', 0.055), ('fk', 0.055), ('gradient', 0.055), ('sin', 0.054), ('kendall', 0.053), ('dzi', 0.053), ('jv', 0.053), ('logp', 0.053), ('spaces', 0.052), ('yi', 0.05), ('ath', 0.049), ('normalizing', 0.048), ('xi', 0.048), ('directional', 0.047), ('symmetric', 0.046), ('zi', 0.045), ('lp', 0.045), ('cos', 0.045), ('adjoint', 0.044), ('probabilistic', 0.042), ('projective', 0.042), ('normal', 0.041), ('component', 0.04), ('variation', 0.04), ('jacobian', 0.04), ('tp', 0.04), ('courty', 0.04), ('dzij', 0.04), ('expp', 0.04), ('miaomiao', 0.04), ('mri', 0.039), ('geometry', 0.039), ('maximization', 0.038), ('orthonormal', 0.038), ('truth', 0.036), ('vq', 0.035), ('diffeomorphic', 0.035), ('curvatures', 0.035), ('utah', 0.035), ('subspaces', 0.035), ('eld', 0.035), ('dr', 0.033), ('ground', 0.033), ('arccos', 0.032), ('su', 0.032), ('integral', 0.032), ('log', 0.031), ('complex', 0.031), ('salt', 0.031), ('ascent', 0.03), ('xij', 0.03), ('simulated', 0.028), ('estimation', 0.028), ('segmented', 0.028), ('geometric', 0.028), ('variable', 0.027), ('distances', 0.027), ('tipping', 0.027), ('orthogonal', 0.026), ('factors', 0.026), ('rotations', 0.026), ('onto', 0.026), ('ck', 0.025), ('rn', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="256-tfidf-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.17702644 <a title="256-tfidf-2" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>3 0.14600147 <a title="256-tfidf-3" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>Author: Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman</p><p>Abstract: unkown-abstract</p><p>4 0.12096794 <a title="256-tfidf-4" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><p>5 0.10519239 <a title="256-tfidf-5" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>Author: Sam Patterson, Yee Whye Teh</p><p>Abstract: In this paper we investigate the use of Langevin Monte Carlo methods on the probability simplex and propose a new method, Stochastic gradient Riemannian Langevin dynamics, which is simple to implement and can be applied to large scale data. We apply this method to latent Dirichlet allocation in an online minibatch setting, and demonstrate that it achieves substantial performance improvements over the state of the art online variational Bayesian methods. 1</p><p>6 0.099257931 <a title="256-tfidf-6" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>7 0.093412422 <a title="256-tfidf-7" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<p>8 0.082037434 <a title="256-tfidf-8" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<p>9 0.076178342 <a title="256-tfidf-9" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>10 0.075296484 <a title="256-tfidf-10" href="./nips-2013-Memory_Limited%2C_Streaming_PCA.html">188 nips-2013-Memory Limited, Streaming PCA</a></p>
<p>11 0.066800892 <a title="256-tfidf-11" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>12 0.066253386 <a title="256-tfidf-12" href="./nips-2013-Online_PCA_for_Contaminated_Data.html">232 nips-2013-Online PCA for Contaminated Data</a></p>
<p>13 0.064599782 <a title="256-tfidf-13" href="./nips-2013-Robust_learning_of_low-dimensional_dynamics_from_large_neural_ensembles.html">286 nips-2013-Robust learning of low-dimensional dynamics from large neural ensembles</a></p>
<p>14 0.06455496 <a title="256-tfidf-14" href="./nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</a></p>
<p>15 0.0584356 <a title="256-tfidf-15" href="./nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</a></p>
<p>16 0.055933475 <a title="256-tfidf-16" href="./nips-2013-Dirty_Statistical_Models.html">91 nips-2013-Dirty Statistical Models</a></p>
<p>17 0.055000842 <a title="256-tfidf-17" href="./nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</a></p>
<p>18 0.0545814 <a title="256-tfidf-18" href="./nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">233 nips-2013-Online Robust PCA via Stochastic Optimization</a></p>
<p>19 0.054074697 <a title="256-tfidf-19" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>20 0.052253038 <a title="256-tfidf-20" href="./nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, 0.068), (2, 0.022), (3, 0.029), (4, -0.002), (5, 0.089), (6, 0.016), (7, 0.043), (8, -0.035), (9, -0.032), (10, -0.032), (11, 0.064), (12, 0.013), (13, 0.079), (14, 0.048), (15, 0.054), (16, -0.007), (17, -0.058), (18, -0.036), (19, 0.067), (20, 0.009), (21, 0.066), (22, 0.025), (23, 0.021), (24, 0.059), (25, -0.049), (26, 0.105), (27, 0.032), (28, 0.011), (29, 0.034), (30, -0.028), (31, -0.037), (32, -0.011), (33, -0.01), (34, -0.001), (35, -0.02), (36, 0.029), (37, -0.011), (38, 0.047), (39, 0.028), (40, 0.089), (41, 0.005), (42, -0.122), (43, -0.035), (44, -0.075), (45, -0.021), (46, -0.088), (47, 0.042), (48, 0.036), (49, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91887707 <a title="256-lsi-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.68578982 <a title="256-lsi-2" href="./nips-2013-Geometric_optimisation_on_positive_definite_matrices_for_elliptically_contoured_distributions.html">131 nips-2013-Geometric optimisation on positive definite matrices for elliptically contoured distributions</a></p>
<p>Author: Suvrit Sra, Reshad Hosseini</p><p>Abstract: Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning, statistics, and optimisation. This paper develops (conic) geometric optimisation on the cone of hpd matrices, which allows us to globally optimise a large class of nonconvex functions of hpd matrices. Speciﬁcally, we ﬁrst use the Riemannian manifold structure of the hpd cone for studying functions that are nonconvex in the Euclidean sense but are geodesically convex (g-convex), hence globally optimisable. We then go beyond g-convexity, and exploit the conic geometry of hpd matrices to identify another class of functions that remain amenable to global optimisation without requiring g-convexity. We present key results that help recognise g-convexity and also the additional structure alluded to above. We illustrate our ideas by applying them to likelihood maximisation for a broad family of elliptically contoured distributions: for this maximisation, we derive novel, parameter free ﬁxed-point algorithms. To our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far. Experiments show that advantages of using our ﬁxed-point algorithms. 1</p><p>3 0.57853758 <a title="256-lsi-3" href="./nips-2013-Learning_the_Local_Statistics_of_Optical_Flow.html">167 nips-2013-Learning the Local Statistics of Optical Flow</a></p>
<p>Author: Dan Rosenbaum, Daniel Zoran, Yair Weiss</p><p>Abstract: Motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical ﬂow to learn the local statistics of optical ﬂow and compare the learned models to prior models assumed by computer vision researchers. We ﬁnd that a Gaussian mixture model (GMM) with 64 components provides a signiﬁcantly better model for local ﬂow statistics when compared to commonly used models. We investigate the source of the GMM’s success and show it is related to an explicit representation of ﬂow boundaries. We also learn a model that jointly models the local intensity pattern and the local optical ﬂow. In accordance with the assumptions often made in computer vision, the model learns that ﬂow boundaries are more likely at intensity boundaries. However, when evaluated on a large dataset, this dependency is very weak and the beneﬁt of conditioning ﬂow estimation on the local intensity pattern is marginal. 1</p><p>4 0.55543631 <a title="256-lsi-4" href="./nips-2013-Multiscale_Dictionary_Learning_for_Estimating_Conditional_Distributions.html">204 nips-2013-Multiscale Dictionary Learning for Estimating Conditional Distributions</a></p>
<p>Author: Francesca Petralia, Joshua T. Vogelstein, David Dunson</p><p>Abstract: Nonparametric estimation of the conditional distribution of a response given highdimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change ﬂexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efﬁciently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features. 1</p><p>5 0.54958791 <a title="256-lsi-5" href="./nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</a></p>
<p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><p>6 0.5476141 <a title="256-lsi-6" href="./nips-2013-Cluster_Trees_on_Manifolds.html">63 nips-2013-Cluster Trees on Manifolds</a></p>
<p>7 0.54237628 <a title="256-lsi-7" href="./nips-2013-On_the_Sample_Complexity_of_Subspace_Learning.html">224 nips-2013-On the Sample Complexity of Subspace Learning</a></p>
<p>8 0.50454593 <a title="256-lsi-8" href="./nips-2013-Sparse_Inverse_Covariance_Estimation_with_Calibration.html">302 nips-2013-Sparse Inverse Covariance Estimation with Calibration</a></p>
<p>9 0.49753365 <a title="256-lsi-9" href="./nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</a></p>
<p>10 0.4925459 <a title="256-lsi-10" href="./nips-2013-One-shot_learning_and_big_data_with_n%3D2.html">225 nips-2013-One-shot learning and big data with n=2</a></p>
<p>11 0.49173424 <a title="256-lsi-11" href="./nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</a></p>
<p>12 0.49093384 <a title="256-lsi-12" href="./nips-2013-Flexible_sampling_of_discrete_data_correlations_without_the_marginal_distributions.html">123 nips-2013-Flexible sampling of discrete data correlations without the marginal distributions</a></p>
<p>13 0.49053308 <a title="256-lsi-13" href="./nips-2013-Non-Uniform_Camera_Shake_Removal_Using_a_Spatially-Adaptive_Sparse_Penalty.html">212 nips-2013-Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty</a></p>
<p>14 0.48294297 <a title="256-lsi-14" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>15 0.48083177 <a title="256-lsi-15" href="./nips-2013-Robust_Spatial_Filtering_with_Beta_Divergence.html">284 nips-2013-Robust Spatial Filtering with Beta Divergence</a></p>
<p>16 0.47016492 <a title="256-lsi-16" href="./nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</a></p>
<p>17 0.46795306 <a title="256-lsi-17" href="./nips-2013-Matrix_factorization_with_binary_components.html">186 nips-2013-Matrix factorization with binary components</a></p>
<p>18 0.46120545 <a title="256-lsi-18" href="./nips-2013-Fast_Algorithms_for_Gaussian_Noise_Invariant_Independent_Component_Analysis.html">117 nips-2013-Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis</a></p>
<p>19 0.45344839 <a title="256-lsi-19" href="./nips-2013-Sketching_Structured_Matrices_for_Faster_Nonlinear_Regression.html">297 nips-2013-Sketching Structured Matrices for Faster Nonlinear Regression</a></p>
<p>20 0.45038992 <a title="256-lsi-20" href="./nips-2013-Robust_Transfer_Principal_Component_Analysis_with_Rank_Constraints.html">285 nips-2013-Robust Transfer Principal Component Analysis with Rank Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.015), (16, 0.032), (33, 0.093), (34, 0.582), (41, 0.028), (49, 0.014), (56, 0.054), (70, 0.026), (85, 0.025), (89, 0.019), (93, 0.023), (95, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96427184 <a title="256-lda-1" href="./nips-2013-Probabilistic_Principal_Geodesic_Analysis.html">256 nips-2013-Probabilistic Principal Geodesic Analysis</a></p>
<p>Author: Miaomiao Zhang, P.T. Fletcher</p><p>Abstract: Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is deﬁned as a geometric ﬁt to the data, rather than as a probabilistic model. Inspired by probabilistic PCA, we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model, we develop a Monte Carlo Expectation Maximization algorithm, where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images. 1</p><p>2 0.95737809 <a title="256-lda-2" href="./nips-2013-What_Are_the_Invariant_Occlusive_Components_of_Image_Patches%3F_A_Probabilistic_Generative_Approach.html">351 nips-2013-What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach</a></p>
<p>Author: Zhenwen Dai, Georgios Exarchakis, Jörg Lücke</p><p>Abstract: We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the ﬁrst time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We ﬁrst investigated encodings learned by the model using artiﬁcial data with mutually occluding components. We ﬁnd that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive ﬁelds associated with the model’s hidden units. We ﬁnd many Gabor-like or globular receptive ﬁelds as well as ﬁelds sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efﬁciently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex. 1</p><p>3 0.94858712 <a title="256-lda-3" href="./nips-2013-Noise-Enhanced_Associative_Memories.html">210 nips-2013-Noise-Enhanced Associative Memories</a></p>
<p>Author: Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav R. Varshney</p><p>Abstract: Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms allow reliable learning and recall of exponential numbers of patterns. Though these designs correct external errors in recall, they assume neurons compute noiselessly, in contrast to highly variable neurons in hippocampus and olfactory cortex. Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as internal noise is less than a speciﬁed threshold, error probability in the recall phase can be made exceedingly small. More surprisingly, we show internal noise actually improves performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional beneﬁt to noisy neurons in biological neuronal networks. 1</p><p>4 0.94282067 <a title="256-lda-4" href="./nips-2013-Generalized_Random_Utility_Models_with_Multiple_Types.html">129 nips-2013-Generalized Random Utility Models with Multiple Types</a></p>
<p>Author: Hossein Azari Soufiani, Hansheng Diao, Zhenyu Lai, David C. Parkes</p><p>Abstract: We propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents’ types. Our model extends the popular setup in Berry, Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’ types using agent-level data. We focus on applications involving data on agents’ ranking over alternatives, and present theoretical conditions that establish the identiﬁability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach. 1</p><p>5 0.93731683 <a title="256-lda-5" href="./nips-2013-Multiclass_Total_Variation_Clustering.html">202 nips-2013-Multiclass Total Variation Clustering</a></p>
<p>Author: Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht</p><p>Abstract: Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches. 1</p><p>6 0.92958087 <a title="256-lda-6" href="./nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</a></p>
<p>7 0.91798282 <a title="256-lda-7" href="./nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</a></p>
<p>8 0.91794568 <a title="256-lda-8" href="./nips-2013-First-order_Decomposition_Trees.html">122 nips-2013-First-order Decomposition Trees</a></p>
<p>9 0.89662623 <a title="256-lda-9" href="./nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</a></p>
<p>10 0.89225143 <a title="256-lda-10" href="./nips-2013-Integrated_Non-Factorized_Variational_Inference.html">143 nips-2013-Integrated Non-Factorized Variational Inference</a></p>
<p>11 0.77676815 <a title="256-lda-11" href="./nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</a></p>
<p>12 0.7709462 <a title="256-lda-12" href="./nips-2013-Variational_Planning_for_Graph-based_MDPs.html">347 nips-2013-Variational Planning for Graph-based MDPs</a></p>
<p>13 0.7682929 <a title="256-lda-13" href="./nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</a></p>
<p>14 0.7399025 <a title="256-lda-14" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>15 0.73282933 <a title="256-lda-15" href="./nips-2013-Stochastic_Gradient_Riemannian_Langevin_Dynamics_on_the_Probability_Simplex.html">312 nips-2013-Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</a></p>
<p>16 0.72823119 <a title="256-lda-16" href="./nips-2013-Latent_Maximum_Margin_Clustering.html">148 nips-2013-Latent Maximum Margin Clustering</a></p>
<p>17 0.72692615 <a title="256-lda-17" href="./nips-2013-Reward_Mapping_for_Transfer_in_Long-Lived_Agents.html">278 nips-2013-Reward Mapping for Transfer in Long-Lived Agents</a></p>
<p>18 0.72124326 <a title="256-lda-18" href="./nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</a></p>
<p>19 0.71900433 <a title="256-lda-19" href="./nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</a></p>
<p>20 0.71788764 <a title="256-lda-20" href="./nips-2013-Streaming_Variational_Bayes.html">317 nips-2013-Streaming Variational Bayes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
