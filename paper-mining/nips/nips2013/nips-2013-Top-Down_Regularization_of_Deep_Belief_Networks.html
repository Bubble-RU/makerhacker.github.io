<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>331 nips-2013-Top-Down Regularization of Deep Belief Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-331" href="#">nips2013-331</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>331 nips-2013-Top-Down Regularization of Deep Belief Networks</h1>
<br/><p>Source: <a title="nips-2013-331-pdf" href="http://papers.nips.cc/paper/5031-top-down-regularization-of-deep-belief-networks.pdf">pdf</a></p><p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>Reference: <a title="nips-2013-331-reference" href="../nips2013_reference/nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract Designing a principled and effective algorithm for learning deep architectures is a challenging problem. [sent-6, score-0.2]
</p><p>2 The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. [sent-7, score-0.201]
</p><p>3 We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. [sent-8, score-0.2]
</p><p>4 We propose to implement the scheme using a method to regularize deep belief networks with top-down information. [sent-9, score-0.363]
</p><p>5 The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. [sent-10, score-0.213]
</p><p>6 A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. [sent-11, score-0.203]
</p><p>7 Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. [sent-12, score-0.28]
</p><p>8 However, when the architecture is deep, it is challenging to train the entire network through supervised learning due to the large number of parameters, the non-convex optimization problem and the dilution of the error signal through the layers. [sent-17, score-0.208]
</p><p>9 Recent developments in unsupervised feature learning and deep learning algorithms have made it possible to learn deep feature hierarchies. [sent-19, score-0.49]
</p><p>10 The ﬁrst phase greedily learns unsupervised modules layer-by-layer from the bottom-up [1, 5]. [sent-21, score-0.322]
</p><p>11 This is subsequently followed by a supervised phase that ﬁne-tunes the network using a supervised, usually discriminative algorithm, such as supervised error backpropagation. [sent-23, score-0.592]
</p><p>12 The unsupervised learning phase initializes the parameters without taking into account the ultimate task of interest, such as classiﬁcation. [sent-24, score-0.32]
</p><p>13 The second phase assumes the entire burden of modifying the model to ﬁt the task. [sent-25, score-0.202]
</p><p>14 This is done by adding an intermediate training phase between the two existing deep learning phases, which enhances the unsupervised representation by incorporating top-down information. [sent-27, score-0.575]
</p><p>15 1  that regularizes the deep belief network (DBN) from the top-down. [sent-30, score-0.364]
</p><p>16 The new regularization method and deep learning strategy are applied to handwritten digit recognition and dictionary learning for object recognition, with competitive empirical results. [sent-32, score-0.466]
</p><p>17 A restricted Boltzmann machine (RBM) [8] is a bipartite Markov random ﬁeld with an input layer x ∈ RI and a latent layer z ∈ RJ (see Figure 1). [sent-37, score-0.645]
</p><p>18 The units in a layer are conditionally independent with distributions given by logistic functions: P (z|x) = j  P (x|z) = i  P (zj |x),  P (zj |x) = 1/(1 + exp(−wj x − bj )),  (3)  P (xi |z),  P (xi |z) = 1/(1 + exp(−wi z − ci )). [sent-48, score-0.338]
</p><p>19 The ﬁrst term samples the data distribution at t = 0, while the second term approximates the equilibrium distribution at t = ∞ using the contrastive divergence method [9] by using a small and ﬁnite number of sampling steps N to obtain a distribution of reconstructed states at t = N . [sent-51, score-0.189]
</p><p>20 E(x, y, z) = −z Wx − z Vy − b z − c x − d y (6) The conditional distribution of the concatenated vector is now: P (x, y|z) = P (x|z)P (y|z) = i  P (xi |z)  c  P (yc |z), (7)  where P (xi |z) is given in Equation 4 and the outputs yc may either be logistic units or the softmax units. [sent-53, score-0.22]
</p><p>21 The RBM may again be trained using contrastive divergence algorithm [9] to approximate the maximum likelihood of joint distribution. [sent-54, score-0.184]
</p><p>22 However, if the objective is to train a deep network, then with ever new layer, the previous V has to be discarded and retrained. [sent-80, score-0.2]
</p><p>23 It may also not be desirable to use a discriminative criterion directly from the outputs, especially in the initial layers of the network. [sent-81, score-0.177]
</p><p>24 Deep belief networks (DBN) [1] are probabilistic graphical models made up of a hierarchy of stochastic latent variables. [sent-83, score-0.188]
</p><p>25 It follows a two-phase training strategy of unsupervised greedy pre-training followed by supervised ﬁne-tuning. [sent-85, score-0.295]
</p><p>26 For unsupervised pre-training, a stack of RBMs is trained greedily from the bottom-up, with the latent activations of each layer used as the inputs for the next RBM. [sent-86, score-0.665]
</p><p>27 Each new layer RBM models the data distribution P (x), such that when higher-level layers are sufﬁciently large, the variational bound on the likelihood always improves [1]. [sent-87, score-0.391]
</p><p>28 An alternative supervised method is a generative model that implements a supervised RBM (Figure 2) that models P (x, y) at the top layer. [sent-90, score-0.285]
</p><p>29 First, a stochastic bottom-up pass is performed and the generative weights are adjusted to be good at reconstructing the layer below. [sent-93, score-0.384]
</p><p>30 Next, a few iterations of alternating sampling using the respective conditional probabilities are done at the top-level supervised RBM between the concatenated vector and the latent layer. [sent-94, score-0.338]
</p><p>31 Using contrastive divergence the RBM is updated by ﬁtting to its posterior distribution. [sent-95, score-0.155]
</p><p>32 Finally, a stochastic top-down pass adjusts bottom-up recognition weights to reconstruct the activations of the layer above. [sent-96, score-0.559]
</p><p>33 In this work, we extend the existing DBN training strategy by having an additional supervised training phase before the discriminative error backpropagation. [sent-97, score-0.49]
</p><p>34 The aim is to construct a top-down regularized building block for deep networks, instead of combining the optimization criteria directly [12], which is done for the supervised RBM model (Figure 2). [sent-103, score-0.441]
</p><p>35 To give control over individual elements in the latent vector, one way to manipulate the representations is to point-wise bias the activations for each latent variable j [11]. [sent-104, score-0.353]
</p><p>36 z  (8)  The update rule of the cross-entropy-regularized RBM can be modiﬁed to: ∆wij ∝ xi sj 0 − xi zj N , (9) where sj = (1 − λ) zj + λˆj z (10) is the merger of the latent and target activations used to update the parameters. [sent-106, score-0.391]
</p><p>37 Here, the inﬂuences of zj and zj are regulated by parameter λ. [sent-107, score-0.176]
</p><p>38 zj = zj ), ˆ ˆ then the parameter update is exactly that the original contrastive divergence learning algorithm. [sent-110, score-0.331]
</p><p>39 The same principle of regularizing the latent activations can be used to combine signals from the bottom-up and top-down. [sent-112, score-0.215]
</p><p>40 The basic building block is a three-layer structure consisting of three consecutive layers: the previous zl−1 ∈ RI , current zl ∈ RJ and next zl+1 ∈ RH layers. [sent-114, score-0.502]
</p><p>41 The layers are connected by two sets of weight parameters Wl−1 and Wl to the previous and next layers respectively. [sent-115, score-0.238]
</p><p>42 Meanwhile, sampling from the next layer zl+1 via weights Wl drives the top-down representations zl,l+1 : P (zl,l+1,j | zl+1 ; Wl ) = 1/(1 + exp(−wl,j zl+1 − cl,j )). [sent-117, score-0.376]
</p><p>43 (12)  The objective is to learn the RBM parameters Wl−1 that map from the previous layer zl−1 to the current latent layer zl,l−1 , by maximizing the likelihood of the previous layer P (zl−1 ) while considering the top-down samples zl,l+1 from the next layer zl+1 as target representations. [sent-118, score-1.156]
</p><p>44 Additionally, the alternating Gibbs sampling, necessary for the contrastive divergence updates, is performed from the unbiased bottom-up samples using Equation 11 and a symmetric decoder: P (zl−1,l,j = 1 | zl,l−1 ; Wl−1 ) = 1/(1 + exp(−wl−1,i zl,l−1 − cl−1,j )). [sent-122, score-0.199]
</p><p>45 Figure 3: The basic building block learns a bottom-up latent representation regularized by topdown signals. [sent-130, score-0.254]
</p><p>46 Bottom-up zl,l−1 and top-down zl,l+1 latent activations are sampled from zl−1 and zl+1 respectively. [sent-131, score-0.215]
</p><p>47 They are merged to get the modiﬁed activations sl used for parameter updates. [sent-132, score-0.286]
</p><p>48 In the DBN, RBMs are stacked from the bottom-up in a greedy layer-wise manner, with each new layer modeling the posterior distribution of the previous layer. [sent-135, score-0.371]
</p><p>49 Similarly, regularized building blocks can also be used to construct the regularized DBN (Figure 4). [sent-136, score-0.18]
</p><p>50 The network can be trained with a forward and backward strategy (Figure 4(b)). [sent-138, score-0.244]
</p><p>51 It integrates top-down regularization with contrastive divergence learning, which is given by alternating Gibbs sampling between the layers (Figure 4(c)). [sent-139, score-0.386]
</p><p>52 s2  s3  s4  s5  z2,1  x  z5,4  z4,3  z3,2 z2,3  z1,2  z3,4  z4,5  z2,1 z2,3  z3,2 z3,4  z4,3 z4,5  y  z5,4  (a) Top-down regularized deep belief network. [sent-145, score-0.343]
</p><p>53 z1,2  (c) Alternating Gibbs sampling chains for contrastive divergence learning. [sent-151, score-0.189]
</p><p>54 Figure 4: Constructing a top-down regularized deep belief network (DBN). [sent-152, score-0.427]
</p><p>55 All the restricted Boltzmann machines (RBM) that make up the network are concurrently optimized. [sent-153, score-0.159]
</p><p>56 Both bottom-up and top-down activations are used for training the network. [sent-155, score-0.2]
</p><p>57 (b) Activations for the top-down regularization are obtained by sampling and merging the forward pass and the backward pass. [sent-156, score-0.274]
</p><p>58 (c) From the activations of the forward pass, the reconstructions can be obtained by performing alternating Gibbs sampling with the previous layer. [sent-157, score-0.306]
</p><p>59 In the forward pass, given the input features, each layer zl is sampled from the bottom-up, based on the representation of the previous layer zl−1 (Equation 11). [sent-158, score-1.045]
</p><p>60 Upon reaching the output layer, the backward pass begins. [sent-160, score-0.181]
</p><p>61 This is repeated until the second layer is reached (l = 2) and s2 is computed. [sent-162, score-0.272]
</p><p>62 All other backward activations from this point onwards are based on the merged representation from instance- and class-based representations. [sent-169, score-0.293]
</p><p>63 This suggest that the network can adopt a three-phase strategy for training, whereby the parameters learned in one phase initializes the next, as follows: • Phase 1 – Unsupervised Greedy. [sent-173, score-0.314]
</p><p>64 The network is constructed by greedily learning a new unsupervised RBM on top of the existing network. [sent-174, score-0.204]
</p><p>65 The stacking process is repeated for L − 2 RBMs, until layer L − 1 is added to the network. [sent-176, score-0.272]
</p><p>66 This phase begins by connecting the L − 1 to a ﬁnal layer, which is activated by the softmax activation function for a classiﬁcation problem. [sent-178, score-0.296]
</p><p>67 Using the one-hot coded output vector y ∈ RC as its target activations and setting λL to 1, the RBM is learned as an associative memory with the following update: ∆wL−1,ic ∝ zL−1,L−2,i yc  0  − zL−1,L,i zL,L−1,c  N. [sent-179, score-0.229]
</p><p>68 This phase is used to ﬁne-tune the network using generative learning, and binds the layers together by aligning all the parameters of the network with the outputs. [sent-181, score-0.526]
</p><p>69 Finally, the supervised error backpropagation algorithm is used to improve class discrimination in the representations. [sent-183, score-0.24]
</p><p>70 In the forward pass, each layer is activated from the bottom-up to obtain the class predictions. [sent-185, score-0.359]
</p><p>71 The classiﬁcation error is then computed based on the groundtruth and the backward pass performs gradient descent on the parameters by backpropagating the errors through the layers from the top-down. [sent-186, score-0.306]
</p><p>72 Essentially, the two phases are performing a variant of the contrastive divergence algorithm. [sent-189, score-0.246]
</p><p>73 5  Empirical Evaluation  In this work, the proposed deep learning strategy and top-down regularization method were evaluated and analyzed using the MNIST handwritten digit dataset [16] and the Caltech-101 object recognition dataset [17]. [sent-191, score-0.434]
</p><p>74 [1] by initially using 44, 000 training and 10, 000 validation images to train the network before retraining it with the full training set. [sent-207, score-0.218]
</p><p>75 In phase 3, sets of 50, 000 and 10, 000 images were used as the initial training and validation sets. [sent-208, score-0.283]
</p><p>76 To simplify the parameterization for the forward-backward learning in phase 2, the top-down modulation parameter λl across the layers were controlled by a single parameter γ using the function: λl = |l − 1|γ /(|l − 1|γ − |L − l|γ ). [sent-210, score-0.321]
</p><p>77 The top-down inﬂuence for a layer l is also dependent on its relative position in the network. [sent-212, score-0.272]
</p><p>78 The function assigns λl such that the layers nearer to the input will have stronger inﬂuences from the input, while the layers near the output will be biased towards the output. [sent-213, score-0.266]
</p><p>79 For each setup, the intermediate results for each training phase are reported in Table 1. [sent-224, score-0.285]
</p><p>80 The deep convex net [19], which utilized more complex convex-optimized modules as building blocks but did not perform ﬁne-tuning on a global network level, got a score of 0. [sent-228, score-0.37]
</p><p>81 23% and used a heavy architecture of a committee of 35 deep convolutional neural nets with elastic distortions and image normalization [20]. [sent-231, score-0.3]
</p><p>82 Setup / Learning algorithm* Classiﬁcation error rate Phase 1 Phase 2 Phase 1 Phase 2 Phase 3 Deep belief network (reported in [1]) 1. [sent-239, score-0.164]
</p><p>83 Additionally, SIFT descriptors from a spatial neighborhood of 2 × 2 were concatenated to form a macrofeature [22]. [sent-263, score-0.156]
</p><p>84 Two layers of RBMs were stacked to model the macrofeatures. [sent-265, score-0.19]
</p><p>85 The resulting representations of the ﬁrst RBM were then concatenated within each spatial neighborhood of 2 × 2. [sent-269, score-0.172]
</p><p>86 For each experimental trial, a set of 30 training examples per class (totaling to 3060) was randomly selected for supervised learning. [sent-273, score-0.177]
</p><p>87 The results demonstrate a consistent improvement moving from Phase 1 to phase 3. [sent-280, score-0.202]
</p><p>88 Method / Training phase Accuracy Proposed top-down regularized DBN Phase 1: Unsupervised stacking Phase 2: Top-down regularization Phase 3: Error backpropagation  72. [sent-287, score-0.415]
</p><p>89 9%  Conclusion  We proposed the notion of deep learning by gradually transitioning from being fully unsupervised to strongly discriminative. [sent-295, score-0.29]
</p><p>90 This is achieved through the introduction of an intermediate phase between the unsupervised and supervised learning phases. [sent-296, score-0.446]
</p><p>91 The method is easily integrated into the intermediate learning phase based on simple building blocks. [sent-298, score-0.286]
</p><p>92 It can be performed to complement greedy layer-wise unsupervised learning and discriminative optimization using error backpropagation. [sent-299, score-0.176]
</p><p>93 Empirical evaluation show that the method leads to competitive results for handwritten digit recognition and object recognition datasets. [sent-300, score-0.265]
</p><p>94 Teh, “A fast learning algorithm for deep belief networks,” Neural Computation, vol. [sent-306, score-0.28]
</p><p>95 Bengio, “Learning deep architectures for AI,” Foundations and Trends in Machine Learning, vol. [sent-322, score-0.2]
</p><p>96 Larochelle, “Greedy layer-wise training of deep networks,” in NIPS, 2006. [sent-330, score-0.253]
</p><p>97 Ng, “Sparse deep belief net model for visual area V2,” in NIPS, 2008. [sent-355, score-0.28]
</p><p>98 Bengio, “Representational power of restricted Boltzmann machines and deep belief networks,” Neural Computation, vol. [sent-365, score-0.355]
</p><p>99 Schmidhuber, “Multi-column deep neural networks for image classiﬁca¸ tion,” in CVPR, 2012. [sent-400, score-0.279]
</p><p>100 Lim, “Unsupervised and supervised visual codes with restricted Boltzmann machines,” in ECCV, 2012. [sent-413, score-0.157]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zl', 0.448), ('rbm', 0.338), ('layer', 0.272), ('wl', 0.255), ('rbms', 0.22), ('phase', 0.202), ('deep', 0.2), ('dbn', 0.199), ('activations', 0.147), ('supervised', 0.124), ('layers', 0.119), ('backpropagation', 0.116), ('dtrain', 0.103), ('contrastive', 0.101), ('thome', 0.098), ('phases', 0.091), ('unsupervised', 0.09), ('zj', 0.088), ('network', 0.084), ('belief', 0.08), ('boltzmann', 0.079), ('goh', 0.078), ('backward', 0.078), ('pass', 0.075), ('cord', 0.075), ('stacked', 0.071), ('sl', 0.071), ('representations', 0.07), ('topdown', 0.069), ('latent', 0.068), ('concatenated', 0.068), ('merged', 0.068), ('units', 0.066), ('recognition', 0.065), ('regularized', 0.063), ('discriminative', 0.058), ('hinton', 0.055), ('divergence', 0.054), ('yc', 0.054), ('building', 0.054), ('descriptors', 0.054), ('forward', 0.053), ('training', 0.053), ('bengio', 0.052), ('mnist', 0.049), ('object', 0.047), ('digit', 0.045), ('singapore', 0.044), ('alternating', 0.044), ('regularize', 0.043), ('handwritten', 0.043), ('machines', 0.042), ('larochelle', 0.042), ('gibbs', 0.041), ('networks', 0.04), ('hanlin', 0.039), ('macrofeatures', 0.039), ('umi', 0.039), ('image', 0.039), ('generative', 0.037), ('lecun', 0.036), ('classi', 0.035), ('regularization', 0.034), ('sohn', 0.034), ('gated', 0.034), ('backpropagating', 0.034), ('infocomm', 0.034), ('convolutional', 0.034), ('spatial', 0.034), ('sampling', 0.034), ('rc', 0.034), ('activated', 0.034), ('sift', 0.033), ('restricted', 0.033), ('uences', 0.032), ('wrongly', 0.032), ('got', 0.032), ('dictionary', 0.032), ('softmax', 0.032), ('greedily', 0.03), ('intermediate', 0.03), ('pervasive', 0.03), ('pooling', 0.03), ('trained', 0.029), ('inputs', 0.029), ('initializes', 0.028), ('wx', 0.028), ('reconstructions', 0.028), ('hmax', 0.028), ('cvpr', 0.028), ('activation', 0.028), ('images', 0.028), ('output', 0.028), ('greedy', 0.028), ('setup', 0.028), ('ponce', 0.027), ('gradual', 0.027), ('pyramid', 0.027), ('cnrs', 0.027), ('distortions', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="331-tfidf-1" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>2 0.28165838 <a title="331-tfidf-2" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>Author: James Martens, Arkadev Chattopadhya, Toni Pitassi, Richard Zemel</p><p>Abstract: This paper examines the question: What kinds of distributions can be efﬁciently represented by Restricted Boltzmann Machines (RBMs)? We characterize the RBM’s unnormalized log-likelihood function as a type of neural network, and through a series of simulation results relate these networks to ones whose representational properties are better understood. We show the surprising result that RBMs can efﬁciently capture any distribution whose density depends on the number of 1’s in their input. We also provide the ﬁrst known example of a particular type of distribution that provably cannot be efﬁciently represented by an RBM, assuming a realistic exponential upper bound on the weights. By formally demonstrating that a relatively simple distribution cannot be represented efﬁciently by an RBM our results provide a new rigorous justiﬁcation for the use of potentially more expressive generative models, such as deeper ones. 1</p><p>3 0.25816742 <a title="331-tfidf-3" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>4 0.25302657 <a title="331-tfidf-4" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>Author: Özlem Aslan, Hao Cheng, Xinhua Zhang, Dale Schuurmans</p><p>Abstract: Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction. Unfortunately, such models are difﬁcult to train because inference over latent variables must be performed concurrently with parameter optimization—creating a highly non-convex problem. Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training. Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer. The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics. 1</p><p>5 0.23008844 <a title="331-tfidf-5" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>Author: Michiel Hermans, Benjamin Schrauwen</p><p>Abstract: Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hierarchical processing on difﬁcult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent time scales. 1</p><p>6 0.21237992 <a title="331-tfidf-6" href="./nips-2013-Stochastic_Ratio_Matching_of_RBMs_for_Sparse_High-Dimensional_Inputs.html">315 nips-2013-Stochastic Ratio Matching of RBMs for Sparse High-Dimensional Inputs</a></p>
<p>7 0.20089671 <a title="331-tfidf-7" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>8 0.16598836 <a title="331-tfidf-8" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>9 0.15702772 <a title="331-tfidf-9" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>10 0.14239295 <a title="331-tfidf-10" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>11 0.13271904 <a title="331-tfidf-11" href="./nips-2013-Annealing_between_distributions_by_averaging_moments.html">36 nips-2013-Annealing between distributions by averaging moments</a></p>
<p>12 0.12680694 <a title="331-tfidf-12" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>13 0.11636496 <a title="331-tfidf-13" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<p>14 0.10728402 <a title="331-tfidf-14" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>15 0.10567956 <a title="331-tfidf-15" href="./nips-2013-Discriminative_Transfer_Learning_with_Tree-based_Priors.html">93 nips-2013-Discriminative Transfer Learning with Tree-based Priors</a></p>
<p>16 0.095733471 <a title="331-tfidf-16" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>17 0.093624093 <a title="331-tfidf-17" href="./nips-2013-RNADE%3A_The_real-valued_neural_autoregressive_density-estimator.html">260 nips-2013-RNADE: The real-valued neural autoregressive density-estimator</a></p>
<p>18 0.09309613 <a title="331-tfidf-18" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>19 0.09253414 <a title="331-tfidf-19" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>20 0.089011148 <a title="331-tfidf-20" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.199), (1, 0.115), (2, -0.202), (3, -0.137), (4, 0.134), (5, -0.155), (6, -0.081), (7, 0.09), (8, 0.046), (9, -0.219), (10, 0.243), (11, 0.018), (12, -0.056), (13, 0.05), (14, 0.071), (15, 0.088), (16, 0.0), (17, -0.068), (18, -0.041), (19, -0.018), (20, 0.036), (21, -0.036), (22, 0.183), (23, 0.017), (24, 0.058), (25, -0.01), (26, -0.252), (27, -0.01), (28, 0.076), (29, 0.039), (30, 0.042), (31, 0.015), (32, -0.063), (33, -0.084), (34, 0.096), (35, 0.016), (36, 0.005), (37, 0.034), (38, -0.066), (39, 0.003), (40, 0.028), (41, 0.044), (42, 0.051), (43, -0.0), (44, 0.029), (45, -0.053), (46, 0.038), (47, 0.028), (48, 0.039), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.961496 <a title="331-lsi-1" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>2 0.82100177 <a title="331-lsi-2" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>Author: Michiel Hermans, Benjamin Schrauwen</p><p>Abstract: Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hierarchical processing on difﬁcult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent time scales. 1</p><p>3 0.78536129 <a title="331-lsi-3" href="./nips-2013-On_the_Expressive_Power_of_Restricted_Boltzmann_Machines.html">221 nips-2013-On the Expressive Power of Restricted Boltzmann Machines</a></p>
<p>Author: James Martens, Arkadev Chattopadhya, Toni Pitassi, Richard Zemel</p><p>Abstract: This paper examines the question: What kinds of distributions can be efﬁciently represented by Restricted Boltzmann Machines (RBMs)? We characterize the RBM’s unnormalized log-likelihood function as a type of neural network, and through a series of simulation results relate these networks to ones whose representational properties are better understood. We show the surprising result that RBMs can efﬁciently capture any distribution whose density depends on the number of 1’s in their input. We also provide the ﬁrst known example of a particular type of distribution that provably cannot be efﬁciently represented by an RBM, assuming a realistic exponential upper bound on the weights. By formally demonstrating that a relatively simple distribution cannot be represented efﬁciently by an RBM our results provide a new rigorous justiﬁcation for the use of potentially more expressive generative models, such as deeper ones. 1</p><p>4 0.74720627 <a title="331-lsi-4" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>Author: Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas</p><p>Abstract: We demonstrate that there is signiﬁcant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy. 1</p><p>5 0.74411517 <a title="331-lsi-5" href="./nips-2013-Stochastic_Ratio_Matching_of_RBMs_for_Sparse_High-Dimensional_Inputs.html">315 nips-2013-Stochastic Ratio Matching of RBMs for Sparse High-Dimensional Inputs</a></p>
<p>Author: Yann Dauphin, Yoshua Bengio</p><p>Abstract: Sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised. Unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and RBMs, because they involve a reconstruction step where the whole input vector is predicted from the current feature values. An algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling scheme stochastically selecting which input elements to actually reconstruct during training for each particular example. To generalize this idea to RBMs, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme. We show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classiﬁcation benchmarks (20 Newsgroups and RCV1), while keeping computational cost linear in the number of non-zeros. 1</p><p>6 0.73324311 <a title="331-lsi-6" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<p>7 0.66692597 <a title="331-lsi-7" href="./nips-2013-Deep_Fisher_Networks_for_Large-Scale_Image_Classification.html">83 nips-2013-Deep Fisher Networks for Large-Scale Image Classification</a></p>
<p>8 0.61868376 <a title="331-lsi-8" href="./nips-2013-Convex_Two-Layer_Modeling.html">75 nips-2013-Convex Two-Layer Modeling</a></p>
<p>9 0.61390346 <a title="331-lsi-9" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>10 0.60085142 <a title="331-lsi-10" href="./nips-2013-A_Deep_Architecture_for_Matching_Short_Texts.html">5 nips-2013-A Deep Architecture for Matching Short Texts</a></p>
<p>11 0.58937174 <a title="331-lsi-11" href="./nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">160 nips-2013-Learning Stochastic Feedforward Neural Networks</a></p>
<p>12 0.57220066 <a title="331-lsi-12" href="./nips-2013-Adaptive_Multi-Column_Deep_Neural_Networks_with_Application_to_Robust_Image_Denoising.html">27 nips-2013-Adaptive Multi-Column Deep Neural Networks with Application to Robust Image Denoising</a></p>
<p>13 0.56300205 <a title="331-lsi-13" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>14 0.53229189 <a title="331-lsi-14" href="./nips-2013-Annealing_between_distributions_by_averaging_moments.html">36 nips-2013-Annealing between distributions by averaging moments</a></p>
<p>15 0.49453935 <a title="331-lsi-15" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>16 0.49290478 <a title="331-lsi-16" href="./nips-2013-RNADE%3A_The_real-valued_neural_autoregressive_density-estimator.html">260 nips-2013-RNADE: The real-valued neural autoregressive density-estimator</a></p>
<p>17 0.46303231 <a title="331-lsi-17" href="./nips-2013-Deep_Neural_Networks_for_Object_Detection.html">84 nips-2013-Deep Neural Networks for Object Detection</a></p>
<p>18 0.45628944 <a title="331-lsi-18" href="./nips-2013-Generalized_Denoising_Auto-Encoders_as_Generative_Models.html">127 nips-2013-Generalized Denoising Auto-Encoders as Generative Models</a></p>
<p>19 0.42784867 <a title="331-lsi-19" href="./nips-2013-Supervised_Sparse_Analysis_and_Synthesis_Operators.html">321 nips-2013-Supervised Sparse Analysis and Synthesis Operators</a></p>
<p>20 0.42257085 <a title="331-lsi-20" href="./nips-2013-DeViSE%3A_A_Deep_Visual-Semantic_Embedding_Model.html">81 nips-2013-DeViSE: A Deep Visual-Semantic Embedding Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.017), (16, 0.035), (33, 0.191), (34, 0.091), (41, 0.023), (49, 0.097), (51, 0.147), (56, 0.073), (70, 0.066), (85, 0.027), (89, 0.049), (93, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89137316 <a title="331-lda-1" href="./nips-2013-Top-Down_Regularization_of_Deep_Belief_Networks.html">331 nips-2013-Top-Down Regularization of Deep Belief Networks</a></p>
<p>Author: Hanlin Goh, Nicolas Thome, Matthieu Cord, Joo-Hwee Lim</p><p>Abstract: Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results. 1</p><p>2 0.87725163 <a title="331-lda-2" href="./nips-2013-Demixing_odors_-_fast_inference_in_olfaction.html">86 nips-2013-Demixing odors - fast inference in olfaction</a></p>
<p>Author: Agnieszka Grabska-Barwinska, Jeff Beck, Alexandre Pouget, Peter Latham</p><p>Abstract: The olfactory system faces a difﬁcult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons. Here we derive neural implementations of two approximate inference algorithms that could be used by the brain. One is a variational algorithm (which builds on the work of Beck. et al., 2012), the other is based on sampling. Importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a “spike and slab” prior, for which most odors have zero concentration. After mapping the two algorithms onto neural dynamics, we ﬁnd that both can infer correct odors in less than 100 ms. Thus, at the behavioral level, the two algorithms make very similar predictions. However, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity. Thus, they should be distinguishable experimentally. If so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities. 1</p><p>3 0.84055889 <a title="331-lda-3" href="./nips-2013-Compete_to_Compute.html">64 nips-2013-Compete to Compute</a></p>
<p>Author: Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, Jürgen Schmidhuber</p><p>Abstract: Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artiﬁcial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time. 1</p><p>4 0.83881795 <a title="331-lda-4" href="./nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</a></p>
<p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><p>5 0.8343395 <a title="331-lda-5" href="./nips-2013-Firing_rate_predictions_in_optimal_balanced_networks.html">121 nips-2013-Firing rate predictions in optimal balanced networks</a></p>
<p>Author: David G. Barrett, Sophie Denève, Christian K. Machens</p><p>Abstract: How are ﬁring rates in a spiking network related to neural input, connectivity and network function? This is an important problem because ﬁring rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difﬁcult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating ﬁring rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate ﬁring rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate ﬁring rates by ﬁnding the solution to the algorithm. Our ﬁring rate calculation relates network ﬁring rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems. 1</p><p>6 0.82805163 <a title="331-lda-6" href="./nips-2013-Sparse_Overlapping_Sets_Lasso_for_Multitask_Learning_and_its_Application_to_fMRI_Analysis.html">303 nips-2013-Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis</a></p>
<p>7 0.82313585 <a title="331-lda-7" href="./nips-2013-Sparse_Additive_Text_Models_with_Low_Rank_Background.html">301 nips-2013-Sparse Additive Text Models with Low Rank Background</a></p>
<p>8 0.82303023 <a title="331-lda-8" href="./nips-2013-Mapping_paradigm_ontologies_to_and_from_the_brain.html">183 nips-2013-Mapping paradigm ontologies to and from the brain</a></p>
<p>9 0.82083035 <a title="331-lda-9" href="./nips-2013-Sparse_nonnegative_deconvolution_for_compressive_calcium_imaging%3A_algorithms_and_phase_transitions.html">304 nips-2013-Sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</a></p>
<p>10 0.82077444 <a title="331-lda-10" href="./nips-2013-Extracting_regions_of_interest_from_biological_images_with_convolutional_sparse_block_coding.html">114 nips-2013-Extracting regions of interest from biological images with convolutional sparse block coding</a></p>
<p>11 0.8207702 <a title="331-lda-11" href="./nips-2013-Adaptive_dropout_for_training_deep_neural_networks.html">30 nips-2013-Adaptive dropout for training deep neural networks</a></p>
<p>12 0.82073146 <a title="331-lda-12" href="./nips-2013-Variance_Reduction_for_Stochastic_Gradient_Optimization.html">345 nips-2013-Variance Reduction for Stochastic Gradient Optimization</a></p>
<p>13 0.82044744 <a title="331-lda-13" href="./nips-2013-Training_and_Analysing_Deep_Recurrent_Neural_Networks.html">334 nips-2013-Training and Analysing Deep Recurrent Neural Networks</a></p>
<p>14 0.82019496 <a title="331-lda-14" href="./nips-2013-Reshaping_Visual_Datasets_for_Domain_Adaptation.html">276 nips-2013-Reshaping Visual Datasets for Domain Adaptation</a></p>
<p>15 0.82003421 <a title="331-lda-15" href="./nips-2013-Inferring_neural_population_dynamics_from_multiple_partial_recordings_of_the_same_neural_circuit.html">141 nips-2013-Inferring neural population dynamics from multiple partial recordings of the same neural circuit</a></p>
<p>16 0.81935281 <a title="331-lda-16" href="./nips-2013-Optimal_Neural_Population_Codes_for_High-dimensional_Stimulus_Variables.html">236 nips-2013-Optimal Neural Population Codes for High-dimensional Stimulus Variables</a></p>
<p>17 0.81871504 <a title="331-lda-17" href="./nips-2013-Real-Time_Inference_for_a_Gamma_Process_Model_of_Neural_Spiking.html">262 nips-2013-Real-Time Inference for a Gamma Process Model of Neural Spiking</a></p>
<p>18 0.81640714 <a title="331-lda-18" href="./nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</a></p>
<p>19 0.81496322 <a title="331-lda-19" href="./nips-2013-Predicting_Parameters_in_Deep_Learning.html">251 nips-2013-Predicting Parameters in Deep Learning</a></p>
<p>20 0.81412995 <a title="331-lda-20" href="./nips-2013-Multi-Prediction_Deep_Boltzmann_Machines.html">200 nips-2013-Multi-Prediction Deep Boltzmann Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
