<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-155" href="../nips2007/nips-2007-Predicting_human_gaze_using_low-level_saliency_combined_with_face_detection.html">nips2007-155</a> <a title="nips-2007-155-reference" href="#">nips2007-155-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>155 nips-2007-Predicting human gaze using low-level saliency combined with face detection</h1>
<br/><p>Source: <a title="nips-2007-155-pdf" href="http://papers.nips.cc/paper/3169-predicting-human-gaze-using-low-level-saliency-combined-with-face-detection.pdf">pdf</a></p><p>Author: Moran Cerf, Jonathan Harel, Wolfgang Einhaeuser, Christof Koch</p><p>Abstract: Under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input. Many computational models try to predict such voluntary eye and attentional shifts. Although the important role of high level stimulus properties (e.g., semantic information) in search stands undisputed, most models are based on low-level image properties. We here demonstrate that a combined model of face detection and low-level saliency signiﬁcantly outperforms a low-level model in predicting locations humans ﬁxate on, based on eye-movement recordings of humans observing photographs of natural scenes, most of which contained at least one person. Observers, even when not instructed to look for anything particular, ﬁxate on a face with a probability of over 80% within their ﬁrst two ﬁxations; furthermore, they exhibit more similar scanpaths when faces are present. Remarkably, our model’s predictive performance in images that do not contain faces is not impaired, and is even improved in some cases by spurious face detector responses. 1</p><br/>
<h2>reference text</h2><p>[1] G. Rizzolatti, L. Riggio, I. Dascola, and C. Umilta. Reorienting attention across the horizontal and vertical meridians: evidence in favor of a premotor theory of attention. Neuropsychologia, 25(1A):31–40, 1987.</p>
<p>[2] G.T. Buswell. How People Look at Pictures: A Study of the Psychology of Perception in Art. The University of Chicago press, 1935.</p>
<p>[3] M. Cerf, D. R. Cleary, R. J. Peters, and C. Koch. Observers are consistent when rating image conspicuity. Vis Res, 47(25):3017–3027, 2007.</p>
<p>[4] S.J. Dickinson, H.I. Christensen, J. Tsotsos, and G. Olofsson. Active object recognition integrating attention and viewpoint control. Computer Vision and Image Understanding, 67(3):239–260, 1997.</p>
<p>[5] L. Itti, C. Koch, E. Niebur, et al. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, 1998.</p>
<p>[6] A.L. Yarbus. Eye Movements and Vision. Plenum Press New York, 1967.  7</p>
<p>[7] J.M. Henderson, J.R. Brockmole, M.S. Castelhano, and M. Mack. Visual Saliency Does Not Account for Eye Movements during Visual Search in Real-World Scenes. Eye Movement Research: Insights into Mind and Brain, R. van Gompel, M. Fischer, W. Murray, and R. Hill, Eds., 1997.</p>
<p>[8] Gregory Zelinsky, Wei Zhang, Bing Yu, Xin Chen, and Dimitris Samaras. The role of top-down and bottom-up processes in guiding eye movements during visual search. In Y. Weiss, B. Sch¨ lkopf, and o J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 1569–1576. MIT Press, Cambridge, MA, 2006.</p>
<p>[9] A. Torralba, A. Oliva, M.S. Castelhano, and J.M. Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psych Rev, 113(4):766–786, 2006.</p>
<p>[10] W. Einh¨ user and P. K¨ nig. Does luminance-contrast contribute to a saliency map for overt visual attena o tion? Eur. J Neurosci, 17(5):1089–1097, 2003.</p>
<p>[11] O. Hershler and S. Hochstein. At ﬁrst sight: a high-level pop out effect for faces. Vision Res, 45(13):1707– 24, 2005.</p>
<p>[12] R. Vanrullen. On second glance: Still no high-level pop-out effect for faces. Vision Res, 46(18):3017– 3027, 2006.</p>
<p>[13] C. Simion and S. Shimojo. Early interactions between orienting, visual sampling and decision making in facial preference. Vision Res, 46(20):3331–3335, 2006.</p>
<p>[14] R. Adolphs. Neural systems for recognizing emotion. Curr. Op. Neurobiol., 12(2):169–177, 2002.</p>
<p>[15] A. Klin, W. Jones, R. Schultz, F. Volkmar, and D. Cohen. Visual Fixation Patterns During Viewing of Naturalistic Social Situations as Predictors of Social Competence in Individuals With Autism, 2002.</p>
<p>[16] JJ Barton. Disorders of face perception and recognition. Neurol Clin, 21(2):521–48, 2003.</p>
<p>[17] K.K. Sung and T. Poggio. Example-based learning for view-based human face detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):39–51, 1998.</p>
<p>[18] H.A. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(1):23–38, 1998.</p>
<p>[19] H. Schneiderman and T. Kanade. Statistical method for 3 D object detection applied to faces and cars. Computer Vision and Pattern Recognition, 1:746–751, 2000.</p>
<p>[20] D. Roth, M. Yang, and N. Ahuja. A snow-based face detection. In S. A. Solla, T. K. Leen, and K. R. Muller, editors, Advances in Neural Information Processing Systems 13, pages 855–861. MIT Press, Cambridge, MA, 2000.</p>
<p>[21] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. Computer Vision and Pattern Recognition, 1:511–518, 2001.</p>
<p>[22] D. Walther. Interactions of visual attention and object recognition: computational modeling, algorithms, and psychophysics. PhD thesis, California Institute of Technology, 2006.</p>
<p>[23] C. Breazeal and B. Scassellati. A context-dependent attention system for a social robot. 1999 International Joint Conference on Artiﬁcial Intelligence, pages 1254–1259, 1999.</p>
<p>[24] V. Navalpakkam and L. Itti. Search Goal Tunes Visual Features Optimally. Neuron, 53(4):605–617, 2007.</p>
<p>[25] D.H. Brainard. The psychophysics toolbox. Spat Vis, 10(4):433–436, 1997.</p>
<p>[26] F.W. Cornelissen, E.M. Peters, and J. Palmer. The Eyelink Toolbox: Eye tracking with MATLAB and the Psychophysics Toolbox. Behav Res Meth Instr Comput, 34(4):613–617, 2002.</p>
<p>[27] J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. In B. Sch¨ lkopf, J. Platt, and T. Hoffman, o editors, Advances in Neural Information Processing Systems 19, pages 545–552. MIT Press, Cambridge, MA, 2007.</p>
<p>[28] G. Bradski, A. Kaehler, and V. Pisarevsky. Learning-based computer vision with Intels open source computer vision library. Intel Technology Journal, 9(1), 2005.</p>
<p>[29] B.W. Tatler, R.J. Baddeley, and I.D. Gilchrist. Visual correlates of ﬁxation selection: effects of scale and time. Vision Res, 45(5):643–59, 2005.</p>
<p>[30] V. Navalpakkam and L. Itti. Search goal tunes visual features optimally. Neuron, 53(4):605–617, 2007.</p>
<p>[31] R.J. Peters, A. Iyer, L. Itti, and C. Koch. Components of bottom-up gaze allocation in natural images. Vision Res, 45(18):2397–2416, 2005.</p>
<p>[32] O. Hershler and S. Hochstein. With a careful look: Still no low-level confound to face pop-out Authors’ reply. Vis Res, 46(18):3028–3035, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
