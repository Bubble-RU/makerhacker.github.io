<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2007-How SVMs can estimate quantiles and the median</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-101" href="../nips2007/nips-2007-How_SVMs_can_estimate_quantiles_and_the_median.html">nips2007-101</a> <a title="nips-2007-101-reference" href="#">nips2007-101-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 nips-2007-How SVMs can estimate quantiles and the median</h1>
<br/><p>Source: <a title="nips-2007-101-pdf" href="http://papers.nips.cc/paper/3180-how-svms-can-estimate-quantiles-and-the-median.pdf">pdf</a></p><p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: We investigate quantile regression based on the pinball loss and the ǫ-insensitive loss. For the pinball loss a condition on the data-generating distribution P is given that ensures that the conditional quantiles are approximated with respect to · 1 . This result is then used to derive an oracle inequality for an SVM based on the pinball loss. Moreover, we show that SVMs based on the ǫ-insensitive loss estimate the conditional median only under certain conditions on P . 1</p><br/>
<h2>reference text</h2><p>[1] H. Bauer. Measure and Integration Theory. De Gruyter, Berlin, 2001.</p>
<p>[2] A. Christmann and I. Steinwart. Consistency and robustness of kernel based regression. Bernoulli, 15:799–819, 2007.</p>
<p>[3] D.E. Edmunds and H. Triebel. Function Spaces, Entropy Numbers, Differential Operators. Cambridge University Press, 1996.</p>
<p>[4] C. Hwang and J. Shim. A simple quantile regression via support vector machine. In Advances in Natural Computation: First International Conference (ICNC), pages 512 –520. Springer, 2005.</p>
<p>[5] R. Koenker. Quantile Regression. Cambridge University Press, 2005.</p>
<p>[6] B. Sch¨ lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New support vector algorithms. o Neural Computation, 12:1207–1245, 2000.</p>
<p>[7] I. Steinwart. How to compare different loss functions. Constr. Approx., 26:225–287, 2007.</p>
<p>[8] I. Steinwart, D. Hush, and C. Scovel. Function classes that approximate the Bayes risk. In Proceedings of the 19th Annual Conference on Learning Theory, COLT 2006, pages 79–93. Springer, 2006.</p>
<p>[9] I. Steinwart, D. Hush, and C. Scovel. An oracle inequality for clipped regularized risk minimizers. In Advances in Neural Information Processing Systems 19, pages 1321–1328, 2007.</p>
<p>[10] I. Takeuchi, Q.V. Le, T.D. Sears, and A.J. Smola. Nonparametric quantile estimation. J. Mach. Learn. Res., 7:1231–1264, 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
