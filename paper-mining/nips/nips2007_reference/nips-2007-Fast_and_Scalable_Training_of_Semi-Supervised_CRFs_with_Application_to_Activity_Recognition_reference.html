<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-88" href="../nips2007/nips-2007-Fast_and_Scalable_Training_of_Semi-Supervised_CRFs_with_Application_to_Activity_Recognition.html">nips2007-88</a> <a title="nips-2007-88-reference" href="#">nips2007-88-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 nips-2007-Fast and Scalable Training of Semi-Supervised CRFs with Application to Activity Recognition</h1>
<br/><p>Source: <a title="nips-2007-88-pdf" href="http://papers.nips.cc/paper/3348-fast-and-scalable-training-of-semi-supervised-crfs-with-application-to-activity-recognition.pdf">pdf</a></p><p>Author: Maryam Mahdaviani, Tanzeem Choudhury</p><p>Abstract: We present a new and efﬁcient semi-supervised training method for parameter estimation and feature selection in conditional random ﬁelds (CRFs). In real-world applications such as activity recognition, unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect. Furthermore, the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy. In this paper, we introduce the semi-supervised virtual evidence boosting (sVEB) algorithm for training CRFs – a semi-supervised extension to the recently developed virtual evidence boosting (VEB) method for feature selection and parameter learning. The objective function of sVEB combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood. It reduces the overall system cost as well as the human labeling cost required during training, which are both important considerations in building real-world inference systems. Experiments on synthetic data and real activity traces collected from wearable sensors, illustrate that sVEB beneﬁts from both the use of unlabeled data and automatic feature selection, and outperforms other semi-supervised approaches. 1</p><br/>
<h2>reference text</h2><p>[1] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proc. of the International Conference on Machine Learning (ICML), 2001. 2  The experiments were run in Matlab environment and as a result they took longer.  7</p>
<p>[2] Andrew McCallum. Efﬁciently inducing features or conditional random ﬁelds. In Proc. of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2003.</p>
<p>[3] T. Dietterich, A. Ashenfelter, and Y. Bulatov. Training conditional random ﬁelds via gradient tree boosting. In Proc. of the International Conference on Machine Learning (ICML), 2004.</p>
<p>[4] A. Torralba, K. P. Murphy, and W. T. Freeman. Contextual models for object detection using boosted random ﬁelds. In Advances in Neural Information Processing Systems (NIPS), 2004.</p>
<p>[5] L. Liao, T. Choudhury, D. Fox, and H Kautz. Training conditional random ﬁelds using virtual evidence boosting. In Proc. of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2007.</p>
<p>[6] K. Nigam, A. McCallum, A. Thrun, and T. Mitchell. Text classiﬁcation from labeled and unlabeled documents using em. Machine learning, 2000.</p>
<p>[7] A. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proc. of the International Conference on Machine Learning (ICML), 2003.</p>
<p>[8] W. Li and M. Andrew. Semi-supervised sequence modeling with syntactic topic models. In Proc. of the National Conference on Artiﬁcial Intelligence (AAAI), 2005.</p>
<p>[9] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems (NIPS), 2004.</p>
<p>[10] F. Jiao, W. Wang, C. H. Lee, R. Greiner, and D. Schuurmans. Semi-supervised conditional random ﬁelds for improved sequence segmentation and labeling. In International Committee on Computational Linguistics and the Association for Computational Linguistics, 2006.</p>
<p>[11] C. Lee, S. Wang, F. Jiao, Schuurmans D., and R. Greiner. Learning to Model Spatial Dependency: SemiSupervised Discriminative Random Fields. In NIPS, 2006.</p>
<p>[12] J.S. Yedidia, W.T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51(7):2282–2312, 2005.</p>
<p>[13] Y. Weiss. Comparing mean ﬁeld method and belief propagation for approximate inference in mrfs. 2001.</p>
<p>[14] J. Besag. Statistical analysis of non-lattice data. The Statistician, 24, 1975.</p>
<p>[15] C. J. Geyer and E. A. Thompson. Constrained Monte Carlo Maximum Likelihood for dependent data. Journal of Royal Statistical Society, 1992.</p>
<p>[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical view of boosting. The Annals of Statistics, 38(2):337–374, 2000.</p>
<p>[17] G. Mann and A. McCullum. Efﬁcient computation of entropy gradient for semi-supervised conditional random ﬁelds. In Human Language Technologies, 2007.  6  Appendix  In this section, we show how we derived the equations for wi and zi (eq. 8): LF = LsV EB = LV EB − αHemp =  N P  log p(yi |vei ) + α  i=1  M P  P  i=N +1 y i  p(yi |vei ) log p(yi |vei )  As in LogitBoost, the likelihood function LF is maximized by learning an ensemble of weak learners. We start with an empty ensemble F = 0 and iteratively add the next best weak learner, ft , by computing the Newton s update H , where s and H are the ﬁrst and second derivative respectively of LF with respect to f (vei , yi ). F (vei , yi )) ← F (vei , yi ) − s=  N P  s , H  2(2yi − 1)(1 − p(yi |vei )) + α  i=1  H=−  where s =  N P  ∂LF +f |f =0 ∂f  M P  P  i=N +1 y i  and H =  ∂ 2 LF +f |f =0 ∂f 2  [2(2yi − 1)(1 − p(yi |vei ))p(yi |vei )(1 − log p(yi |vei ))]  4p(yi |vei )(1 − p(yi |vei ))(2yi − 1)2 + α2  i=1  M P  P  i=N +1 y i  4(2yi − 1)2 (1 − p(yi |vei ))[p(yi |vei )(1 −  p(yi |vei )) + log p(yi |vei )] N P  F ← F+  i=1  zi wi +  N P i=1    and  wi =  M P  P  i=N +1 y  wi +  M P  P  i=N +1 y  (  zi wi  i  where zi = wi  yi −0.5 p(yi |vei ) (yi −0.5)p(yi |vei )(1−log p(yi |vei )) α[p(yi |vei )(1−p(yi |vei ))+log p(yi |vei )]  if 1 ≤ i ≤ N  eq. (4)  if N < i ≤ M  eq. (8)  i  p(yi |vei )(1 − p(yi |vei )) α2 (1 − p(yi |vei ))[p(yi |vei )(1 − p(yi |vei )) + log p(yi |vei )]  if 1 ≤ i ≤ N if N < i ≤ M  At iteration t we get the best weak learner, ft , by solving the WLSE problem in eq. 7.  8  eq. (4) eq. (8)</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
