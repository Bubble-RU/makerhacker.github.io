<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-65" href="../nips2007/nips-2007-DIFFRAC%3A_a_discriminative_and_flexible_framework_for_clustering.html">nips2007-65</a> <a title="nips-2007-65-reference" href="#">nips2007-65-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>65 nips-2007-DIFFRAC: a discriminative and flexible framework for clustering</h1>
<br/><p>Source: <a title="nips-2007-65-pdf" href="http://papers.nips.cc/paper/3269-diffrac-a-discriminative-and-flexible-framework-for-clustering.pdf">pdf</a></p><p>Author: Francis R. Bach, Zaïd Harchaoui</p><p>Abstract: We present a novel linear clustering framework (D IFFRAC) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem. The large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions. This framework has several attractive properties: (1) although apparently similar to K-means, it exhibits superior clustering performance than K-means, in particular in terms of robustness to noise. (2) It can be readily extended to non linear clustering if the discriminative cost function is based on positive deﬁnite kernels, and can then be seen as an alternative to spectral clustering. (3) Prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classiﬁcation. We present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.</p><br/>
<h2>reference text</h2><p>[1] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. Maximum margin clustering. In Adv. NIPS, 2004.</p>
<p>[2] T. De Bie and N. Cristianini. Fast SDP relaxations of graph cut clustering, transduction, and other combinatorial problems. J. Mac. Learn. Res., 7:1409–1436, 2006.</p>
<p>[3] K. Zhang, I. W. Tsang, and J. T. Kwok. Maximum margin clustering made practical. In Proc. ICML, 2007.</p>
<p>[4] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer-Verlag, 2001.</p>
<p>[5] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Camb. Univ. Press, 2004.</p>
<p>[6] A. Frieze and M. Jerrum. Improved approximation algorithms for MAX k-CUT and MAX BISECTION. In Integer Programming and Combinatorial Optimization, volume 920, pages 1–13. Springer, 1995.</p>
<p>[7] C. Swamy. Correlation clustering: maximizing agreements via semideﬁnite programming. In ACM-SIAM Symp. Discrete algorithms, 2004.</p>
<p>[8] A. S. Lewis and H. S. Sendov. Twice differentiable spectral functions. SIAM J. Mat. Anal. App., 23(2):368–386, 2002.</p>
<p>[9] F R. Bach and M I. Jordan. Learning spectral clustering, with application to speech separation. J. Mac. Learn. Res., 7:1963–2001, 2006.</p>
<p>[10] B. Sch¨ lkopf, A. J. Smola, and K.-R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Comp., 10(3):1299–1319, 1998.</p>
<p>[11] N. Srebro, G. Shakhnarovich, and S. Roweis. An investigation of computational and informational limits in gaussian mixture clustering. In Proc. ICML, 2006.</p>
<p>[12] S. Boyd and L. Vandenberghe. Convex Optimization. Camb. Univ. Press, 2003.</p>
<p>[13] J. F. Bonnans, J. C. Gilbert, C. Lemar´ chal, and C. A. Sagastizbal. Numerical Optimization Theoretical e and Practical Aspects. Springer, 2003.</p>
<p>[14] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. In Adv. NIPS, 2002.</p>
<p>[15] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector machines. In Proc. AAAI, 2005.</p>
<p>[16] M. Heiler, J. Keuchel, and C. Schn¨ rr. Semideﬁnite clustering for image segmentation with a-priori o knowledge. In Pattern Recognition, Proc. DAGM, 2005.</p>
<p>[17] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr¨ dl. Constrained K-means clustering with background o knowledge. In Proc. ICML, 2001.</p>
<p>[18] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning distance functions using equivalence relations. In Proc. ICML, 2003.</p>
<p>[19] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Proc. AISTATS, 2004.</p>
<p>[20] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In Proc. ICML, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
