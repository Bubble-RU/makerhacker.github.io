<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-10" href="../nips2007/nips-2007-A_Randomized_Algorithm_for_Large_Scale_Support_Vector_Learning.html">nips2007-10</a> <a title="nips-2007-10-reference" href="#">nips2007-10-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>10 nips-2007-A Randomized Algorithm for Large Scale Support Vector Learning</h1>
<br/><p>Source: <a title="nips-2007-10-pdf" href="http://papers.nips.cc/paper/3352-a-randomized-algorithm-for-large-scale-support-vector-learning.pdf">pdf</a></p><p>Author: Krishnan Kumar, Chiru Bhattacharya, Ramesh Hariharan</p><p>Abstract: This paper investigates the application of randomized algorithms for large scale SVM learning. The key contribution of the paper is to show that, by using ideas random projections, the minimal number of support vectors required to solve almost separable classiﬁcation problems, such that the solution obtained is near optimal with a very high probability, is given by O(log n); if on removal of properly chosen O(log n) points the data becomes linearly separable then it is called almost separable. The second contribution is a sampling based algorithm, motivated from randomized algorithms, which solves a SVM problem by considering subsets of the dataset which are greater in size than the number of support vectors for the problem. These two ideas are combined to obtain an algorithm for SVM classiﬁcation problems which performs the learning by considering only O(log n) points at a time. Experiments done on synthetic and real life datasets show that the algorithm does scale up state of the art SVM solvers in terms of memory required and execution time without loss in accuracy. It is to be noted that the algorithm presented here nicely complements existing large scale SVM learning approaches as it can be used to scale up any SVM solver. 1</p><br/>
<h2>reference text</h2><p>[1] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.</p>
<p>[2] Bernd Gartner. A subexponential algorithm for abstract optimization problems. In Proceedings 33rd Symposium on Foundations of Computer Science, IEEE CS Press, 1992.</p>
<p>[3] Jose L. Balcazar, Yang Dai, and Osamu Watanabe. A random sampling technique for training support vector machines. In ALT. Springer, 2001.</p>
<p>[4] Jose L. Balcazar, Yang Dai, and Osamu Watanabe. Provably fast training algorithms for support vector machines. In ICDM, pages 43–50, 2001.</p>
<p>[5] K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classiﬁers. In P. Langley, editor, ICML, pages 57–64, San Francisco, California, 2000.</p>
<p>[6] W. Johnson and J. Lindenstauss. Extensions of lipschitz maps into a hilbert space. Contemporary Mathematics, 1984.</p>
<p>[7] R. I. Arriaga and S. Vempala. An algorithmic theory of learning: Random concepts and random projections. In Proceedings of the 40th Foundations of Computer Science, 1999.</p>
<p>[8] Kenneth L. Clarkson. Las vegas algorithms for linear and integer programming when the dimension is small. Journal of the ACM, 42(2):488–499, 1995.</p>
<p>[9] B. Gartner and E. Welzl. A simple sampling lemma: analysis and application in geometric optimization. In Proceedings of the 16th annual ACM symposium on Computational Geometry, 2000.</p>
<p>[10] M. Pellegrini. Randomizing combinatorial algorithms for linear programming when the dimension is moderately high. In SODA ’01, pages 101–108, Philadelphia, PA, USA, 2001.</p>
<p>[11] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. On kernels, margins and lowdimensional mappings. In Proc. of the 15th Conf. Algorithmic Learning Theory, 2004.</p>
<p>[12] T. Joachims. Training linear svms in linear time. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
