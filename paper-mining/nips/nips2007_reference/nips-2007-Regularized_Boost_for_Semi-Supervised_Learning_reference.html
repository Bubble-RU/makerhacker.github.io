<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2007-Regularized Boost for Semi-Supervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-166" href="../nips2007/nips-2007-Regularized_Boost_for_Semi-Supervised_Learning.html">nips2007-166</a> <a title="nips-2007-166-reference" href="#">nips2007-166-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 nips-2007-Regularized Boost for Semi-Supervised Learning</h1>
<br/><p>Source: <a title="nips-2007-166-pdf" href="http://papers.nips.cc/paper/3167-regularized-boost-for-semi-supervised-learning.pdf">pdf</a></p><p>Author: Ke Chen, Shihai Wang</p><p>Abstract: Semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data. Several boosting algorithms have been extended to semi-supervised learning with various strategies. To our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning. In this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals. Our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training. Comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer. We discuss relevant issues and relate our regularizer to previous work. 1</p><br/>
<h2>reference text</h2><p>[1] Chapelle, O., Sch¨ lkopf, B., & Zien, A. (2006) Semi-Supervised Learning. Cambridge, MA: MIT Press. o</p>
<p>[2] Zhu, X. (2006) Semi-supervised learning literature survey. Computer Science TR-1530, University of Wisconsin - Madison, U.S.A.</p>
<p>[3] Bousquet, O., Chapelle, O., & Hein, M. (2004) Measure based regularization. In Advances in Neural Information Processing Systems 16. Cambridge, MA: MIT Press.</p>
<p>[4] Belkin, M., Niyogi, P., & Sindhwani, V. (2004) Manifold regularization: a geometric framework for learning from examples. Technical Report, University of Michigan, U.S.A.</p>
<p>[5] Szummer, M., & Jaakkola, T. (2003) Information regularization with partially labeled data. In Advances in Neural Information Processing Systems 15. Cambridge, MA: MIT Press.</p>
<p>[6] Grandvalet, Y., & Begio, Y. (2005) Semi-supervised learning by entropy minimization. In Advances in Neural Information Processing Systems 17. Cambridge, MA: MIT Press.</p>
<p>[7] Zhu, X., & Lafferty, J. (2005) Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning. In Proc. Int. Conf. Machine Learning, pp. 1052-1059.</p>
<p>[8] Zhou, D., Bousquet, O., Lal, T., Weston, J., & Sch¨ lkopf, B. (2004) Learning with local and global consistency. In Advances in Neural Information Processing Systems 16. Cambridge, MA: MIT Press.</p>
<p>[9] Mason, L., Bartlett, P., Baxter, J., & Frean, M. (2000) Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classiﬁers. Cambridge, MA: MIT Press.</p>
<p>[10] d’Alch´ -Buc, F., Grandvalet, Y., & Ambroise, C. (2002) Semi-supervised MarginBoost. In Advances in e Neural Information Processing Systems 14. Cambridge, MA: MIT Press.</p>
<p>[11] Bennett, K., Demiriz, A., & Maclin, R. (2002) Expoliting unlabeled data in ensemble methods. In Proc. ACM Int. Conf. Knowledge Discovery and Data Mining, pp. 289-296.</p>
<p>[12] Collins, M., & Singer, Y. (1999) Unsupervised models for the named entity classiﬁcation. In Proc. SIGDAT Conf. Empirical Methods in Natural Language Processing and Very Large Corpora.</p>
<p>[13] Leskes, B. (2005) The value of agreement, a new boosting algorithm. In Proc. Int. Conf. Algorithmic Learning Theory (LNAI 3559), pp. 95-110, Berlin: Springer-Verlag.</p>
<p>[14] G¨ nther, E., & Pfeiffer, K.P. (2005) Multiclass boosting for weak classiﬁers. Journal of Machine Learning u Research 6:189-210.</p>
<p>[15] Nigam, K., McCallum, A., Thrum, S., & Mitchell, T. (2000) Using EM to classify text from labeled and unlabeled documents. Machine Learning 39:103-134.</p>
<p>[16] Blake, C., Keogh, E., & Merz, C.J. (1998) UCI repository of machine learning databases. University of California, Irvine. [on-line] http://www.ics.uci.edu/ mlearn/MLRepository.html</p>
<p>[17] The JAFFE Database. [Online] http://www.kasrl.org/jaffe.html</p>
<p>[18] Fasel, B. (2002) Robust face analysis using convolutional neural networks. In Proc. Int. Conf. Pattern Recognition, vol. 2, pp. 40-43.</p>
<p>[19] K´ gl, B., & Wang, L. (2004) Boosting on manifolds: adaptive regularization of base classiﬁer. In Advances e in Neural Information Processing Systems 16. Cambridge, MA: MIT Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
