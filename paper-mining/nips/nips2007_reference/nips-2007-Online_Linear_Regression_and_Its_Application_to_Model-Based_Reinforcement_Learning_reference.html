<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-148" href="../nips2007/nips-2007-Online_Linear_Regression_and_Its_Application_to_Model-Based_Reinforcement_Learning.html">nips2007-148</a> <a title="nips-2007-148-reference" href="#">nips2007-148-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>148 nips-2007-Online Linear Regression and Its Application to Model-Based Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2007-148-pdf" href="http://papers.nips.cc/paper/3197-online-linear-regression-and-its-application-to-model-based-reinforcement-learning.pdf">pdf</a></p><p>Author: Alexander L. Strehl, Michael L. Littman</p><p>Abstract: We provide a provably efﬁcient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Speciﬁcally, we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh’s work that provides a provably efﬁcient algorithm for ﬁnite state MDPs. Our approach is not restricted to the linear setting, and is applicable to other classes of continuous MDPs.</p><br/>
<h2>reference text</h2><p>Abbeel, P., & Ng, A. Y. (2005). Exploration and apprenticeship learning in reinforcement learning. ICML ’05: Proceedings of the 22nd international conference on Machine learning (pp. 1–8). New York, NY, USA: ACM Press. Auer, P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3, 397–422. Brafman, R. I., & Tennenholtz, M. (2002). R-MAX—a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3, 213–231. Chow, C.-S., & Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithmfor discrete time stochastic control. IEEE Transactions on Automatic Control, 36, 898–914. Fiechter, C.-N. (1997). PAC adaptive control of linear systems. Tenth Annual Conference on Computational Learning Theory (COLT) (pp. 72–80). Golub, G. H., & Van Loan, C. F. (1996). Matrix computations. Baltimore, Maryland: The Johns Hopkins University Press. 3rd edition. Kakade, S. M. (2003). On the sample complexity of reinforcement learning. Doctoral dissertation, Gatsby Computational Neuroscience Unit, University College London. Kakade, S. M. K., Kearns, M. J., & Langford, J. C. (2003). Exploration in metric state spaces. Proceedings of the 20th International Conference on Machine Learning (ICML-03). Kearns, M., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for near-optimal planning in large Markov decision processes. Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI-99) (pp. 1324–1331). Kearns, M. J., & Singh, S. P. (2002). Near-optimal reinforcement learning in polynomial time. Machine Learning, 49, 209–232. Ng, A. Y., Kim, H. J., Jordan, M. I., & Sastry, S. (2003). Autonomous helicopter ﬂight via reinforcement learning. Advances in Neural Information Processing Systems 16 (NIPS-03). Seber, G. A. F., & Lee, A. J. (2003). Linear regression analysis. Wiley-Interscience. Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An introduction. The MIT Press. Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6, 215–219.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
