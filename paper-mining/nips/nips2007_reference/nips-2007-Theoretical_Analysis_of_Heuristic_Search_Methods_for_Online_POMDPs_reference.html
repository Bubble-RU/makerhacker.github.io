<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-204" href="../nips2007/nips-2007-Theoretical_Analysis_of_Heuristic_Search_Methods_for_Online_POMDPs.html">nips2007-204</a> <a title="nips-2007-204-reference" href="#">nips2007-204-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>204 nips-2007-Theoretical Analysis of Heuristic Search Methods for Online POMDPs</h1>
<br/><p>Source: <a title="nips-2007-204-pdf" href="http://papers.nips.cc/paper/3250-theoretical-analysis-of-heuristic-search-methods-for-online-pomdps.pdf">pdf</a></p><p>Author: Stephane Ross, Joelle Pineau, Brahim Chaib-draa</p><p>Abstract: Planning in partially observable environments remains a challenging problem, despite signiﬁcant recent advances in ofﬂine approximation techniques. A few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their ofﬂine counterparts. Thus it seems natural to try to unify ofﬂine and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter. In this paper, we provide theoretical guarantees on an anytime algorithm for POMDPs which aims to reduce the error made by approximate ofﬂine value iteration algorithms through the use of an efﬁcient online searching procedure. The algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error. We provide a general theorem showing that these search heuristics are admissible, and lead to complete and ǫ-optimal algorithms. This is, to the best of our knowledge, the strongest theoretical result available for online POMDP solution methods. We also provide empirical evidence showing that our approach is also practical, and can ﬁnd (provably) near-optimal solutions in reasonable time. 1</p><br/>
<h2>reference text</h2><p>[1] J. Pineau. Tractable planning under uncertainty: exploiting structure. PhD thesis, Carnegie Mellon University, Pittsburgh, PA, 2004.</p>
<p>[2] P. Poupart. Exploiting structure to efﬁciently solve large scale partially observable Markov decision processes. PhD thesis, University of Toronto, 2005.</p>
<p>[3] T. Smith and R. Simmons. Point-based POMDP algorithms: improved analysis and implementation. In UAI, 2005.</p>
<p>[4] M. T. J. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for POMDPs. JAIR, 24:195–220, 2005.</p>
<p>[5] N. Roy and G. Gordon. Exponential family PCA for belief compression in POMDPs. In NIPS, 2003.</p>
<p>[6] P. Poupart and C. Boutilier. Value-directed compression of POMDPs. In NIPS, 2003.</p>
<p>[7] J. K. Satia and R. E. Lave. Markovian decision processes with probabilistic observation of states. Management Science, 20(1):1–13, 1973.</p>
<p>[8] R. Washington. BI-POMDP: bounded, incremental partially observable Markov model planning. In 4th Eur. Conf. on Planning, pages 440–451, 1997.</p>
<p>[9] D. McAllester and S. Singh. Approximate Planning for Factored POMDPs using Belief State Simpliﬁcation. In UAI, 1999.</p>
<p>[10] S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for complex multiagent environments. In AAMAS, 2005.</p>
<p>[11] S. Ross and B. Chaib-draa. AEMS: an anytime online search algorithm for approximate policy reﬁnement in large POMDPs. In IJCAI, 2007.</p>
<p>[12] E. A. Hansen and S. Zilberstein. LAO * : A heuristic search algorithm that ﬁnds solutions with loops. Artiﬁcial Intelligence, 129(1-2):35–62, 2001.</p>
<p>[13] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY, USA, 1994.</p>
<p>[14] N.J. Nilsson. Principles of Artiﬁcial Intelligence. Tioga Publishing, 1980.</p>
<p>[15] M. Hauskrecht. Value-function approximations for POMDPs. JAIR, 13:33–94, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
