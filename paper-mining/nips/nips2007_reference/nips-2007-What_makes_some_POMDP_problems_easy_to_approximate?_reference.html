<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>215 nips-2007-What makes some POMDP problems easy to approximate?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-215" href="../nips2007/nips-2007-What_makes_some_POMDP_problems_easy_to_approximate%3F.html">nips2007-215</a> <a title="nips-2007-215-reference" href="#">nips2007-215-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>215 nips-2007-What makes some POMDP problems easy to approximate?</h1>
<br/><p>Source: <a title="nips-2007-215-pdf" href="http://papers.nips.cc/paper/3291-what-makes-some-pomdp-problems-easy-to-approximate.pdf">pdf</a></p><p>Author: Wee S. Lee, Nan Rong, Daniel J. Hsu</p><p>Abstract: Point-based algorithms have been surprisingly successful in computing approximately optimal solutions for partially observable Markov decision processes (POMDPs) in high dimensional belief spaces. In this work, we seek to understand the belief-space properties that allow some POMDP problems to be approximated efﬁciently and thus help to explain the point-based algorithms’ success often observed in the experiments. We show that an approximately optimal POMDP solution can be computed in time polynomial in the covering number of a reachable belief space, which is the subset of the belief space reachable from a given belief point. We also show that under the weaker condition of having a small covering number for an optimal reachable space, which is the subset of the belief space reachable under an optimal policy, computing an approximately optimal solution is NP-hard. However, given a suitable set of points that “cover” an optimal reachable space well, an approximate solution can be computed in polynomial time. The covering number highlights several interesting properties that reduce the complexity of POMDP planning in practice, e.g., fully observed state variables, beliefs with sparse support, smooth beliefs, and circulant state-transition matrices. 1</p><br/>
<h2>reference text</h2><p>[1] R.M. Gray. Toeplitz and Circulant Matrices: A Review. Now Publishers Inc, 2006.</p>
<p>[2] M. Hauskrecht. Value-function approximations for partially observable Markov decision processes. J. Artiﬁcial Intelligence Research, 13:33–94, 2000.</p>
<p>[3] J. Hoey, A. von Bertoldi, P. Poupart, and A. Mihailidis. Assisting persons with dementia during handwashing using a partially observable Markov decision process. In Proc. Int. Conf. on Vision Systems, 2007.</p>
<p>[4] D. Hsu, W.S. Lee, and N. Rong. Accelerating point-based POMDP algorithms through successive approximations of the optimal reachable space. Technical Report TRA4/07, National University of Singapore. School of Computing, April 2007.</p>
<p>[5] L.P. Kaelbling, M.L. Littman, and A.R. Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101(1–2):99–134, 1998.</p>
<p>[6] M. Kearns, Y. Mansour, and A.Y. Ng. A sparse sampling algorithm for near optimal planning in large Markov decision processes. Machine Learning, 49(2-3):193–208, 2002.</p>
<p>[7] M.L. Littman. Algorithms for sequential decision making. PhD thesis, Dept. of Computer Science, Brown University, 1996.</p>
<p>[8] C. Lusena, J. Goldsmith, and M. Mundhenk. Nonapproximability results for partially observable Markov decision processes. J. Artiﬁcial Intelligence Research, 14:83–103, 2002.</p>
<p>[9] O. Madani, S. Hanks, and A. Condon. On the undecidability of probabilistic planning and inﬁnite-horizon partially observable Markov decision problems. In Proc. Nat. Conf. on Artiﬁcial Intelligence, pages 541– 548, 1999.</p>
<p>[10] C. Papadimitriou and J.N. Tsisiklis. The complexity of Markov decision processes. Mathematics of Operations Research, 12(3):441–450, 1987.</p>
<p>[11] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for POMDPs. In Proc. Int. Jnt. Conf. on Artiﬁcial Intelligence, pages 477–484, 2003.</p>
<p>[12] S.P. Singh and R.C. Yee. An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16(3):227–233, 1994.</p>
<p>[13] R.D. Smallwood and E.J. Sondik. The optimal control of partially observable Markov processes over a ﬁnite horizon. Operations Research, 21:1071–1088, 1973.</p>
<p>[14] T. Smith and R. Simmons. Point-based POMDP algorithms: Improved analysis and implementation. In Proc. Uncertainty in Artiﬁcial Intelligence, 2005.</p>
<p>[15] M.T.J. Spaan and N. Vlassis. A point-based POMDP algorithm for robot planning. In Proc. IEEE Int. Conf. on Robotics & Automation, 2004.</p>
<p>[16] N.L. Zhang and W. Zhang. Speeding up the convergence of value iteration in partially observable Markov decision processes. Journal of Artiﬁcial Intelligence Research, 14:29–51, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
