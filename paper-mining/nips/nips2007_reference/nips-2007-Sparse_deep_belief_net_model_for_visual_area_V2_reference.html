<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 nips-2007-Sparse deep belief net model for visual area V2</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-182" href="../nips2007/nips-2007-Sparse_deep_belief_net_model_for_visual_area_V2.html">nips2007-182</a> <a title="nips-2007-182-reference" href="#">nips2007-182-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>182 nips-2007-Sparse deep belief net model for visual area V2</h1>
<br/><p>Source: <a title="nips-2007-182-pdf" href="http://papers.nips.cc/paper/3313-sparse-deep-belief-net-model-for-visual-area-v2.pdf">pdf</a></p><p>Author: Honglak Lee, Chaitanya Ekanadham, Andrew Y. Ng</p><p>Abstract: Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito & Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features. 1</p><br/>
<h2>reference text</h2><p>[1] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527–1554, 2006.</p>
<p>[2] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient learning of sparse representations with an energy-based model. In NIPS, 2006.</p>
<p>[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In NIPS, 2006.</p>
<p>[4] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML, 2007.</p>
<p>[5] G. E. Hinton, S. Osindero, and K. Bao. Learning causally linked MRFs. In AISTATS, 2005.</p>
<p>[6] S. Osindero, M. Welling, and G. E. Hinton. Topographic product models applied to natural scene statistics. Neural Computation, 18:381–344, 2006.</p>
<p>[7] M. Ito and H. Komatsu. Representation of angles embedded within contour stimuli in area v2 of macaque monkeys. The Journal of Neuroscience, 24(13):3313–3324, 2004.</p>
<p>[8] J. H. van Hateren and A. van der Schaaf. Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. Proc.R.Soc.Lond. B, 265:359–366, 1998.</p>
<p>[9] A. J. Bell and T. J. Sejnowski. The ‘independent components’ of natural scenes are edge ﬁlters. Vision Research, 37(23):3327–3338, 1997.</p>
<p>[10] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381:607–609, 1996.</p>
<p>[11] H. Lee, , A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse coding algorithms. In NIPS, 2007.</p>
<p>[12] D. Hubel and T. Wiesel. Receptive ﬁelds and functional architecture of monkey striate cortex. Journal of Physiology, 195:215–243, 1968.</p>
<p>[13] R. L. DeValois, E. W. Yund, and N. Hepler. The orientation and direction selectivity of cells in macaque visual cortex. Vision Res., 22:531–544, 1982a.</p>
<p>[14] H. B. Barlow. The coding of sensory messages. Current Problems in Animal Behavior, 1961.</p>
<p>[15] P. O. Hoyer and A. Hyvarinen. A multi-layer sparse coding network learns contour coding from natural images. Vision Research, 42(12):1593–1605, 2002.</p>
<p>[16] Y. Karklin and M. S. Lewicki. A hierarchical bayesian model for learning non-linear statistical regularities in non-stationary natural signals. Neural Computation, 17(2):397–423, 2005.</p>
<p>[17] A. Hyvarinen and P. O. Hoyer. Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces. Neural Computation, 12(7):1705–1720, 2000.</p>
<p>[18] A. Hyv¨ rinen, P. O. Hoyer, and M. O. Inki. Topographic independent component analysis. Neural a Computation, 13(7):1527–1558, 2001.</p>
<p>[19] A. Hyvarinen, M. Gutmann, and P. O. Hoyer. Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in v2. BMC Neuroscience, 6:12, 2005.</p>
<p>[20] L. Wiskott and T. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 14(4):715–770, 2002.</p>
<p>[21] G. Boynton and J. Hegde. Visual cortex: The continuing puzzle of area v2. Current Biology, 14(13):R523–R524, 2004.</p>
<p>[22] J. B. Levitt, D. C. Kiper, and J. A. Movshon. Receptive ﬁelds and functional architecture of macaque v2. Journal of Neurophysiology, 71(6):2517–2542, 1994.</p>
<p>[23] J. Hegde and D.C. Van Essen. Selectivity for complex shapes in primate visual area v2. Journal of Neuroscience, 20:RC61–66, 2000.</p>
<p>[24] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.</p>
<p>[25] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800, 2002.</p>
<p>[26] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning: Transfer learning from unlabeled data. In ICML, 2007.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
