<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2007-A general agnostic active learning algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-15" href="../nips2007/nips-2007-A_general_agnostic_active_learning_algorithm.html">nips2007-15</a> <a title="nips-2007-15-reference" href="#">nips2007-15-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 nips-2007-A general agnostic active learning algorithm</h1>
<br/><p>Source: <a title="nips-2007-15-pdf" href="http://papers.nips.cc/paper/3325-a-general-agnostic-active-learning-algorithm.pdf">pdf</a></p><p>Author: Sanjoy Dasgupta, Claire Monteleoni, Daniel J. Hsu</p><p>Abstract: We present an agnostic active learning algorithm for any hypothesis class of bounded VC dimension under arbitrary data distributions. Most previous work on active learning either makes strong distributional assumptions, or else is computationally prohibitive. Our algorithm extends the simple scheme of Cohn, Atlas, and Ladner [1] to the agnostic setting, using reductions to supervised learning that harness generalization bounds in a simple but subtle manner. We provide a fall-back guarantee that bounds the algorithm’s label complexity by the agnostic PAC sample complexity. Our analysis yields asymptotic label complexity improvements for certain hypothesis classes and distributions. We also demonstrate improvements experimentally. 1</p><br/>
<h2>reference text</h2><p>[1] D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, 15(2):201–221, 1994.</p>
<p>[2] Y. Freund, H. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28(2):133–168, 1997.</p>
<p>[3] S. Dasgupta, A. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. In COLT, 2005.</p>
<p>[4] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, 2005.</p>
<p>[5] S. Hanneke. Teaching dimension and the complexity of active learning. In COLT, 2007.</p>
<p>[6] M.-F. Balcan, A. Broder, and T. Zhang. Margin based active learning. In COLT, 2007.</p>
<p>[7] R. Castro and R. Nowak. Upper and lower bounds for active learning. In Allerton Conference on Communication, Control and Computing, 2006.</p>
<p>[8] R. Castro and R. Nowak. Minimax bounds for active learning. In COLT, 2007.</p>
<p>[9] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, 2006.</p>
<p>[10] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. UCSD Technical Report CS2007-0898, http://www.cse.ucsd.edu/∼djhsu/papers/cal.pdf, 2007.</p>
<p>[11] M. K¨¨ri¨inen. Active learning in the non-realizable case. In ALT, 2006. aa a</p>
<p>[12] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, 2007.</p>
<p>[13] C. Monteleoni. Learning with online constraints: shifting concepts and active learning. PhD Thesis, MIT Computer Science and Artiﬁcial Intelligence Laboratory, 2006.</p>
<p>[14] O. Bousquet, S. Boucheron, and G. Lugosi. Introduction to statistical learning theory. Lecture Notes in Artiﬁcial Intelligence, 3176:169–207, 2004.</p>
<p>[15] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16:264–280, 1971.</p>
<p>[16] C. Monteleoni. Eﬃcient algorithms for general active learning. In COLT. Open problem, 2006.</p>
<p>[17] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In FOCS, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
