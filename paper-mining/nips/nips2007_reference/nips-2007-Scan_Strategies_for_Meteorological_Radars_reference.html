<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2007-Scan Strategies for Meteorological Radars</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-171" href="../nips2007/nips-2007-Scan_Strategies_for_Meteorological_Radars.html">nips2007-171</a> <a title="nips-2007-171-reference" href="#">nips2007-171-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>171 nips-2007-Scan Strategies for Meteorological Radars</h1>
<br/><p>Source: <a title="nips-2007-171-pdf" href="http://papers.nips.cc/paper/3199-scan-strategies-for-meteorological-radars.pdf">pdf</a></p><p>Author: Victoria Manfredi, Jim Kurose</p><p>Abstract: We address the problem of adaptive sensor control in dynamic resourceconstrained sensor networks. We focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360◦ . We compare three sector scanning strategies. The sit-and-spin strategy always scans 360◦ . The limited lookahead strategy additionally uses the expected environmental state K decision epochs in the future, as predicted from Kalman ﬁlters, in its decision-making. The full lookahead strategy uses all expected future states by casting the problem as a Markov decision process and using reinforcement learning to estimate the optimal scan strategy. We show that the main beneﬁts of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufﬁciently smaller than the radius of the radars. We also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned. 1</p><br/>
<h2>reference text</h2><p>[1] D. Cox and V. Isham. A simple spatial-temporal model of rainfall. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 415:1849:317–328, 1988.</p>
<p>[2] B. Donovan and D. J. McLaughlin. Improved radar sensitivity through limited sector scanning: The DCAS approach. In Proceedings of AMS Radar Meteorology, 2005.</p>
<p>[3] M. Huber and R. Grupen. A feedback control structure for on-line learning tasks. Robotics and Autonomous Systems, 22(3-4):303–315, 1997.</p>
<p>[4] C. Kreucher and A. O. H. III. Non-myopic approaches to scheduling agile sensors for multistage detection, tracking and identiﬁcation. In Proceedings of ICASSP, pages 885–888, 2005.</p>
<p>[5] J. Kurose, E. Lyons, D. McLaughlin, D. Pepyne, B. Phillips, D. Westbrook, and M. Zink. An end-user-responsive sensor network architecture for hazardous weather detection, prediction and response. AINTEC, 2006.</p>
<p>[6] C. Kwok and D. Fox. Reinforcement learning for sensing strategies. In IROS, 2004.</p>
<p>[7] V. Manfredi and J. Kurose. Comparison of myopic and lookahead scan strategies for meteorological radars. Technical Report U of Massachusetts Amherst, 2006-62, 2006.</p>
<p>[8] V. Manfredi, S. Mahadevan, and J. Kurose. Switching kalman ﬁlters for prediction and tracking in an adaptive meteorological sensing network. In IEEE SECON, 2005.</p>
<p>[9] K. Murphy. A survey of POMDP solution techniques. Technical Report U.C. Berkeley, 2000.</p>
<p>[10] A. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang. Inverted autonomous helicopter ﬂight via reinforcement learning. In International Symposium on Experimental Robotics, 2004.</p>
<p>[11] I. Rodrigues-Iturbe and P. Eagleson. Mathematical models of rainstorm events in space and time. Water Resources Research, 23:1:181– 190, 1987.</p>
<p>[12] P. Stone, R. Sutton, and G. Kuhlmann. Reinforcement learning for robocup-soccer keepaway. Adaptive Behavior, 3, 2005.</p>
<p>[13] R. Sutton. Tile coding software. http://rlai.cs.ualberta.ca/RLAI/RLtoolkit/tiles.html.</p>
<p>[14] R. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In NIPS, 1996.</p>
<p>[15] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Massachusetts, 1998.</p>
<p>[16] S. Suvorova, D. Musicki, B. Moran, S. Howard, and B. L. Scala. Multi step ahead beam and waveform scheduling for tracking of manoeuvering targets in clutter. In Proceedings of ICASSP, 2005.</p>
<p>[17] J. M. Trabal, B. C. Donovan, M. Vega, V. Marrero, D. J. McLaughlin, and J. G. Colom. Puerto Rico student test bed applications and system requirements document development. In Proceedings of the 9th International Conference on Engineering Education, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
