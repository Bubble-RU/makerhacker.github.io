<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-124" href="../nips2007/nips-2007-Managing_Power_Consumption_and_Performance_of_Computing_Systems_Using_Reinforcement_Learning.html">nips2007-124</a> <a title="nips-2007-124-reference" href="#">nips2007-124-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>124 nips-2007-Managing Power Consumption and Performance of Computing Systems Using Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2007-124-pdf" href="http://papers.nips.cc/paper/3251-managing-power-consumption-and-performance-of-computing-systems-using-reinforcement-learning.pdf">pdf</a></p><p>Author: Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart, David Levine, Freeman Rawson, Charles Lefurgy</p><p>Abstract: Electrical power management in large-scale IT systems such as commercial datacenters is an application area of rapidly growing interest from both an economic and ecological perspective, with billions of dollars and millions of metric tons of CO2 emissions at stake annually. Businesses want to save power without sacriﬁcing performance. This paper presents a reinforcement learning approach to simultaneous online management of both performance and power consumption. We apply RL in a realistic laboratory testbed using a Blade cluster and dynamically varying HTTP workload running on a commercial web applications middleware platform. We embed a CPU frequency controller in the Blade servers’ ﬁrmware, and we train policies for this controller using a multi-criteria reward signal depending on both application performance and CPU power consumption. Our testbed scenario posed a number of challenges to successful use of RL, including multiple disparate reward functions, limited decision sampling rates, and pathologies arising when using multiple sensor readings as state variables. We describe innovative practical solutions to these challenges, and demonstrate clear performance improvements over both hand-designed policies as well as obvious “cookbook” RL implementations. 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng. An application of reinforcement learning to aerobatic helicopter ﬂight. In Proc. of NIPS-06, 2006.</p>
<p>[2] P. Abbeel and A. Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In Proc. of ICML-05, 2005.</p>
<p>[3] A. Antos, C. Szepesvari, and R. Munos. Learning near-optimal policies with bellman-residual minimization based ﬁtted policy iteration and a single sample path. In Proc. of COLT-06, 2006.</p>
<p>[4] L. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proc. of ICML95, 1995.</p>
<p>[5] Y. Chen et al. Managing server energy and operational costs in hosting centers. In Proc. of SIGMETRICS, 2005.</p>
<p>[6] M. Femal and V. Freeh. Boosting data center performance through non-uniform power allocation. In Second Intl. Conf. on Autonomic Computing, 2005.</p>
<p>[7] Green Grid Consortium. Green grid. http://www.thegreengrid.org, 2006.</p>
<p>[8] J. Chen et al. Datacenter power modeling and prediction. UC Berkeley RAD Lab presentation, 2007.</p>
<p>[9] J. O. Kephart, H. Chan, R. Das, D. Levine, G. Tesauro, F. Rawson, and C. Lefurgy. Coordinating multiple autonomic managers to achieve speciﬁed power-performance tradeoffs. In Proc. of ICAC-07, 2007.</p>
<p>[10] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. J. of Machine Learning Research, 4:1107– 1149, 2003.</p>
<p>[11] C. Lefurgy, X. Wang, and M. Ware. Server-level power control. In Proc. of ICAC-07, 2007.</p>
<p>[12] D. Menasce and V. A. F. Almeida. Capacity Planning for Web Performance: Metrics, Models, and Methods. Prentice Hall, 1998.</p>
<p>[13] S. Russell and A. L. Zimdars. Q-decomposition for reinforcement learning agents. In Proc. of ICML-03, pages 656–663, 2003.</p>
<p>[14] M. S. Squillante, D. D. Yao, and L. Zhang. Internet trafﬁc: Periodicity, tail behavior and performance implications. In System Performance Evaluation: Methodologies and Applications, 1999.</p>
<p>[15] G. Tesauro, N. K. Jong, R. Das, and M. N. Bennani. A hybrid reinforcement learning approach to autonomic resource allocation. In Proc. of ICAC-06, pages 65–73, 2006.</p>
<p>[16] United States Environmental Protection Agency. Letter to Enterprise Server Manufacturers and Other Stakeholders. http://www.energystar.gov, 2006.</p>
<p>[17] M. Wang et al. Adaptive Performance Control of Computing Systems via Distributed Cooperative Control: Application to Power Management in Computer Clusters. In Proc. of ICAC-06, 2006.</p>
<p>[18] WebSphere Extended Deployment. http://www.ibm.com/software/webservers/appserv/extend/, 2007.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
