<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 nips-2007-A probabilistic model for generating realistic lip movements from speech</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-18" href="../nips2007/nips-2007-A_probabilistic_model_for_generating_realistic_lip_movements_from_speech.html">nips2007-18</a> <a title="nips-2007-18-reference" href="#">nips2007-18-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 nips-2007-A probabilistic model for generating realistic lip movements from speech</h1>
<br/><p>Source: <a title="nips-2007-18-pdf" href="http://papers.nips.cc/paper/3287-a-probabilistic-model-for-generating-realistic-lip-movements-from-speech.pdf">pdf</a></p><p>Author: Gwenn Englebienne, Tim Cootes, Magnus Rattray</p><p>Abstract: The present work aims to model the correspondence between facial motion and speech. The face and sound are modelled separately, with phonemes being the link between both. We propose a sequential model and evaluate its suitability for the generation of the facial animation from a sequence of phonemes, which we obtain from speech. We evaluate the results both by computing the error between generated sequences and real video, as well as with a rigorous double-blind test with human subjects. Experiments show that our model compares favourably to other existing methods and that the sequences generated are comparable to real video sequences. 1</p><br/>
<h2>reference text</h2><p>[1] David Barber. Expectation correction for smoothed inference in switching linear dynamical systems. Journal of Machine Learning Research, 7:2515–2540, 2006.</p>
<p>[2] V. Blanz, C. Basso, T. Poggio, and T. Vetter. Reanimating faces in images and video. In Proceedings of ACM SIGGRAPH, Annual Conference Series, 2003.</p>
<p>[3] M. Brand. Voice puppetry. In SIGGRAPH ’99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 21–28, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co.</p>
<p>[4] C. Bregler, H. Hild, and S. Manke. Improving letter recognition by lipreading. In Proceedings of ICASSP, 1993.</p>
<p>[5] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham. Active shape models, their training and application. Comput. Vis. Image Underst., 61(1):38–59, 1995.</p>
<p>[6] T.F. Cootes, G.J. Edwards, and C.J. Taylor. Active appearance models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):681–685, 2001.</p>
<p>[7] P. Duchnowski, U. Meier, and A. Weibel. See me, hear me: Integrating automatic speech recognition and lipreading. In Proc. ICSLP 94, 1994.</p>
<p>[8] G. Edwards, C. Taylor, and T. Cootes. Interpreting face images using active appearance models, 1998.</p>
<p>[9] T. F. Ezzat, G. Geiger, and T. Poggio. Trainable videorealistic speech animation. In SIGGRAPH ’02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques, pages 388–398, New York, NY, USA, 2002. ACM Press.</p>
<p>[10] H. McGurk and J. MacDonald. Hearing lips and seeing voices. Nature, pages 746 – 748, December 1976.</p>
<p>[11] Alan B. Poritz. Linear predictive hidden markov models and the speech signal. Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, 7:1291–1294, May 1982.</p>
<p>[12] L. R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. In Readings in speech recognition, pages 267–296. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1990.</p>
<p>[13] B. Theobald, G. Cawley, I. Matthews, J. Glauert, and J. Bangham. 2.5D visual speech synthesis using appearance models. Proceedings of the British Machine Vision Conference, 2003.</p>
<p>[14] M. A. Turk and A. P. Pentland. Face recognition using eigenfaces. Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 586–591, 1991.</p>
<p>[15] Mike West and Jeﬀ Harrison. Bayesian Forecasting and Dynamic Models. Springer, 1999.</p>
<p>[16] S. Young. The HTK hidden markov model toolkit: Design and philosophy, 1993.  7  A  Parameter estimation in DPDS  The log-likelihood of a sequence is given by eq. 1, which is a multiplicative function of A s s s (x1 = f (Aπ1 ), x2 = f (Aπ2 Aπ1 ), etc.). Applying the chain rule repeatedly gives us, for diagonal matrices and using Lt to denote the log-likelihood of a single observation at time s t, that ∂L1 ∂An = 0 and ∂Lt ∂An = R−1 (yt − xs )(∂xs ∂An ) for 2 t T , where s t t πt ∂xs ∂xs t s s s s = xs δnπt + Aπt t−1 , and δnπt = 1 iﬀ n = πt t ∂An ∂An  (2)  There we give the gradients for diagonal matrices for simplicity of notation and because we used diagonal matrices for this work, but the same principle applies to full matrices. The S Ts gradient of the likelihood is then ∂L ∂An = s=1 t=2 ∂Ls,t ∂An . In general the same is done for the other parameters of the model, however when the covariance is shared by all states, the value of the other parameters can be maximised exactly as described below. In the following, superscripts diﬀerentiate between variables by indicating what the variable is S Ts S s s a coeﬃcient to. The covariance R = s=1 t=2 (yt − xs )(yt − xs )⊤ t t s=1 (Ts − 1) where s s s s s s x1 = µπ1 , xt = Aπt xt−1 + ν πt , while µπt and ν πt are found by solving the system of linear s s equations (3) for which the coeﬃcients D and b are computed by Algorithm 1, which takes {π}, {y} and the current values of A1...Π as input:     X1,1 · · · X1,Π µν µµ µ   µΠ×1 diagΠ×Π (Dn ) DΠ×Π bΠ×1 .  (3) ..   where XΠ×Π  . = .  .  . ν νµ . . ν Π×1 bΠ×1 DΠ×Π Dνν Π×Π X1,Π · · · XΠ,Π Algorithm 1 Maximisation of L with respect to µ and ν for n ∈ {1 . . . Π} do bµ ← 0, bν ← 0, Dµµ ← 0 n n n ∀m ∈ {1 . . . Π}: Dµν , Dνν , Dνµ ← 0 n,m n,m n s for s ∈ {s|π0 = n} do ⊲ Compute coeﬃcients Dµµ ,Dµν ,bµ to µn n nx n µµ µµ µ µ µ s Dn ← Dn + I, D ← I, bn ← bn + yt ∀m ∈ {1 . . . Π}: Cµ ← 0 ⊲ Cµ and Dµ below are temporary variables m m for t ∈ {2 . . . Ts } do s s Dµ ← Dµ + Aπt Dµ , Dµµ ← Dµµ + Dµ Dµ , bµ ← bµ + Dµ yt n n n n µ µ µν µν µ µ s ∀m ∈ {1 . . . Π}: Cm ← Aπt Cm , Dn,m ← Dn,m + D Cm Cµt ← Cµt + I s s π π end for end for for s ∈ {1 . . . S} do ⊲ Compute coeﬃcients Dνµ ,Dνν ,bν to ν n nx nx n ∀m ∈ {1 . . . Π}: Cν ← 0 m Dν ← 0, Cµ ← I ⊲ Cν , Dν , Cµ are temporary variables m m for t ∈ {2 . . . Ts } do s Dν ← Aπt Dν , s if πt = n then Dν ← Dν + I end if s ∀m ∈ {1 . . . Π}: Cν ← Aπt Cν , Dνν ← Dνν + Dν Cν m m n,m n,m m ν ν µ s s Cπt ← Cπt + I, C ← Aπt Cµ , Dνµ ← Dνµ + Dν Cµ , bν ← bν + Dν yt s s s s n n π1 π1 end for end for end for  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
