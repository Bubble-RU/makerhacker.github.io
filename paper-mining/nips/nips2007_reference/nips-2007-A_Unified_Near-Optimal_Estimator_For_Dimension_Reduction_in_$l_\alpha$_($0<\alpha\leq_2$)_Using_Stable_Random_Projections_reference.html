<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-13" href="../nips2007/nips-2007-A_Unified_Near-Optimal_Estimator_For_Dimension_Reduction_in_%24l_%5Calpha%24_%28%240%3C%5Calpha%5Cleq_2%24%29_Using_Stable_Random_Projections.html">nips2007-13</a> <a title="nips-2007-13-reference" href="#">nips2007-13-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 nips-2007-A Unified Near-Optimal Estimator For Dimension Reduction in $l \alpha$ ($0<\alpha\leq 2$) Using Stable Random Projections</h1>
<br/><p>Source: <a title="nips-2007-13-pdf" href="http://papers.nips.cc/paper/3225-a-unified-near-optimal-estimator-for-dimension-reduction-in-l_alpha-0alphaleq-2-using-stable-random-projections.pdf">pdf</a></p><p>Author: Ping Li, Trevor J. Hastie</p><p>Abstract: Many tasks (e.g., clustering) in machine learning only require the lα distances instead of the original data. For dimension reductions in the lα norm (0 < α ≤ 2), the method of stable random projections can efﬁciently compute the lα distances in massive datasets (e.g., the Web or massive data streams) in one pass of the data. The estimation task for stable random projections has been an interesting topic. We propose a simple estimator based on the fractional power of the samples (projected data), which is surprisingly near-optimal in terms of the asymptotic variance. In fact, it achieves the Cram´ r-Rao bound when α = 2 and α = 0+. This e new result will be useful when applying stable random projections to distancebased clustering, classiﬁcations, kernels, massive data streams etc.</p><br/>
<h2>reference text</h2><p>[1] C. Aggarwal, editor. Data Streams: Models and Algorithms. Springer, New York, NY, 2007.</p>
<p>[2] B. Babcock, S. Babu, M. Datar, R. Motwani, and J. Widom. Models and issues in data stream systems. In PODS, 1–16, 2002.</p>
<p>[3] B. Brinkman and M. Charikar. On the impossibility of dimension reduction in l1 . Journal of ACM, 52(2):766–788, 2005.</p>
<p>[4] O. Chapelle, P. Haffner, and V. Vapnik. Support vector machines for histogram-based image classiﬁcation. IEEE Trans. Neural Networks, 10(5):1055–1064, 1999.</p>
<p>[5] G. Cormode, M. Datar, P. Indyk, and S. Muthukrishnan. Comparing data streams using hamming norms (how to zero in). In VLDB, 335–345, 2002.</p>
<p>[6] D. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289–1306, 2006.</p>
<p>[7] E. Fama and R. Roll. Parameter estimates for symmetric stable distributions. JASA, 66(334):331–338, 1971.</p>
<p>[8] I. Gradshteyn and I. Ryzhik. Table of Integrals, Series, and Products. Academic Press, New York, ﬁfth edition, 1994.</p>
<p>[9] P. Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. Journal of ACM, 53(3):307–323, 2006.</p>
<p>[10] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mapping into Hilbert space. Contemporary Mathematics, 26:189–206, 1984.</p>
<p>[11] E. Lehmann and G. Casella. Theory of Point Estimation. Springer, New York, NY, second edition, 1998.</p>
<p>[12] E. Leopold and J. Kindermann. Text categorization with support vector machines. how to represent texts in input space? Machine Learning, 46(1-3):423–444, 2002.</p>
<p>[13] P. Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In SODA, 2008.</p>
<p>[14] P. Li and K. Church. Using sketches to estimate associations. In HLT/EMNLP, 708–715, 2005.</p>
<p>[15] P. Li and K. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, 33(3):305–354, 2007.</p>
<p>[16] P. Li, K. Church, and T. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS, 873–880, 2007.</p>
<p>[17] P. Li, T. Hastie, and K. Church. Improving random projections using marginal information. In COLT, 635–649, 2006.</p>
<p>[18] P. Li, T. Hastie, and K. Church. Nonlinear estimators and tail bounds for dimensional reduction in l1 using cauchy random projections. Journal of Machine Learning Research (To appear) .</p>
<p>[19] M. Matsui and A. Takemura. Some improvements in numerical evaluation of symmetric stable density and its derivatives. Communications on Statistics-Theory and Methods, 35(1):149–172, 2006.</p>
<p>[20] J. McCulloch. Simple consistent estimators of stable distribution parameters. Communications on Statistics-Simulation, 15(4):1109– 1136, 1986.</p>
<p>[21] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling the poor assumptions of naive Bayes text classiﬁers. In ICML, 616–623, 2003.</p>
<p>[22] S. Vempala. The Random Projection Method. American Mathematical Society, Providence, RI, 2004.</p>
<p>[23] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. In NIPS, Vancouver, 2003.</p>
<p>[24] V. M. Zolotarev. One-dimensional Stable Distributions. American Mathematical Society, Providence, RI, 1986.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
