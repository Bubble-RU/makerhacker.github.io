<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2007-Expectation Maximization and Posterior Constraints</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-84" href="../nips2007/nips-2007-Expectation_Maximization_and_Posterior_Constraints.html">nips2007-84</a> <a title="nips-2007-84-reference" href="#">nips2007-84-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>84 nips-2007-Expectation Maximization and Posterior Constraints</h1>
<br/><p>Source: <a title="nips-2007-84-pdf" href="http://papers.nips.cc/paper/3170-expectation-maximization-and-posterior-constraints.pdf">pdf</a></p><p>Author: Kuzman Ganchev, Ben Taskar, João Gama</p><p>Abstract: The expectation maximization (EM) algorithm is a widely used maximum likelihood estimation procedure for statistical models when the values of some of the variables in the model are not observed. Very often, however, our aim is primarily to ﬁnd a model that assigns values to the latent variables that have intended meaning for our data and maximizing expected likelihood only sometimes accomplishes this. Unfortunately, it is typically difﬁcult to add even simple a-priori information about latent variables in graphical models without making the models overly complex or intractable. In this paper, we present an efﬁcient, principled way to inject rich constraints on the posteriors of latent variables into the EM algorithm. Our method can be used to learn tractable graphical models that satisfy additional, otherwise intractable constraints. Focusing on clustering and the alignment problem for statistical machine translation, we show that simple, intuitive posterior constraints can greatly improve the performance over standard baselines and be competitive with more complex, intractable models. 1</p><br/>
<h2>reference text</h2><p>[1] D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, 1999.</p>
<p>[2] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J. Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty. But dictionaries are data too. In Proc. HLT, 1993.</p>
<p>[3] Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, 1994.</p>
<p>[4] O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, o Cambridge, MA, 2006.</p>
<p>[5] I. Csiszar. I-divergence geometry of probability distributions and minimization problems. The Annals of Probability, 3, 1975.</p>
<p>[6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38, 1977.</p>
<p>[7] Alexander Fraser and Daniel Marcu. Measuring word alignment quality for statistical machine translation. Comput. Linguist., 33(3):293–303, 2007.</p>
<p>[8] Nir Friedman, Ori Mosenzon, Noam Slonim, and Naftali Tishby. Multivariate information bottleneck. In UAI, 2001.</p>
<p>[9] Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.</p>
<p>[10] Philipp Koehn. Europarl: A multilingual corpus for evaluation of machine translation, 2002.</p>
<p>[11] P. Lambert, A.De Gispert, R. Banchs, and J. B. Mari˜ o. Guidelines for word alignment evaln uation and manual alignment. In Language Resources and Evaluation, Volume 39, Number 4, pages 267–285, 2005.</p>
<p>[12] Percy Liang, Ben Taskar, and Dan Klein. Alignment by agreement. In Proc. HLT-NAACL, 2006.</p>
<p>[13] Gideon S. Mann and Andrew McCallum. Simple, robust, scalable semi-supervised learning via expectation regularization. In Proc. ICML, 2007.</p>
<p>[14] R. M. Neal and G. E. Hinton. A new view of the EM algorithm that justiﬁes incremental, sparse and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355– 368. Kluwer, 1998.</p>
<p>[15] Franz Josef Och and Hermann Ney. Improved statistical alignment models. In ACL, 2000.</p>
<p>[16] Franz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, 2003.</p>
<p>[17] Noah A. Smith and Jason Eisner. Annealing structural bias in multilingual weighted grammar induction. In Proc. ACL, pages 569–576, 2006.</p>
<p>[18] Martin Szummer and Tommi Jaakkola. Information regularization with partially labeled data. In Proc. NIPS, pages 1025–1032, 2003.</p>
<p>[19] L. G. Valiant. The complexity of computing the permanent. Theoretical Computer Science, 8:189–201, 1979.</p>
<p>[20] Stephan Vogel, Hermann Ney, and Christoph Tillmann. Hmm-based word alignment in statistical translation. In Proc. COLING, 1996.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
