<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-82" href="../nips2007/nips-2007-Estimating_divergence_functionals_and_the_likelihood_ratio_by_penalized_convex_risk_minimization.html">nips2007-82</a> <a title="nips-2007-82-reference" href="#">nips2007-82-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>82 nips-2007-Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization</h1>
<br/><p>Source: <a title="nips-2007-82-pdf" href="http://papers.nips.cc/paper/3193-estimating-divergence-functionals-and-the-likelihood-ratio-by-penalized-convex-risk-minimization.pdf">pdf</a></p><p>Author: Xuanlong Nguyen, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We develop and analyze an algorithm for nonparametric estimation of divergence functionals and the density ratio of two probability distributions. Our method is based on a variational characterization of f -divergences, which turns the estimation into a penalized convex risk minimization problem. We present a derivation of our kernel-based estimation algorithm and an analysis of convergence rates for the estimator. Our simulation results demonstrate the convergence behavior of the method, which compares favorably with existing methods in the literature. 1</p><br/>
<h2>reference text</h2><p>[1] S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from another. J. Royal Stat. Soc. Series B, 28:131–142, 1966.</p>
<p>[2] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation, and risk bounds. Journal of the American Statistical Association, 101:138–156, 2006.</p>
<p>[3] P. Bickel and Y. Ritov. Estimating integrated squared density derivatives: Sharp best order of convergence estimates. Sankhy¯ Ser. A, 50:381–393, 1988. a</p>
<p>[4] L. Birg´ and P. Massart. Estimation of integral functionals of a density. Ann. Statist., 23(1):11–29, 1995. e</p>
<p>[5] M. Broniatowski and A. Keziou. Parametric estimation and tests through divergences. Technical report, LSTA, Universit´ Pierre et Marie Curie, 2004. e</p>
<p>[6] I. Csisz´ r. Information-type measures of difference of probability distributions and indirect observation. a Studia Sci. Math. Hungar, 2:299–318, 1967.</p>
<p>[7] L. Gyorﬁ and E.C. van der Meulen. Density-free convergence properties of various estimators of entropy. Computational Statistics and Data Analysis, 5:425–436, 1987.</p>
<p>[8] P. Hall and S. Morton. On estimation of entropy. Ann. Inst. Statist. Math., 45(1):69–88, 1993.</p>
<p>[9] I. A. Ibragimov and R. Z. Khasminskii. On the nonparametric estimation of functionals. In Symposium in Asymptotic Statistics, pages 41–52, 1978.</p>
<p>[10] H. Joe. Estimation of entropy and other functionals of a multivariate density. Ann. Inst. Statist. Math., 41:683–697, 1989.</p>
<p>[11] A. Keziou. Dual representation of φ-divergences and applications. C. R. Acad. Sci. Paris, Ser. I 336, pages 857–862, 2003.</p>
<p>[12] B. Laurent. Efﬁcient estimation of integral functionals of a density. Ann. Statist., 24(2):659–681, 1996.</p>
<p>[13] B. Ya. Levit. Asymptotically efﬁcient estimation of nonlinear functionals. Problems Inform. Transmission, 14:204–209, 1978.</p>
<p>[14] X. Nguyen, M. J. Wainwright, and M. I. Jordan. On divergences, surrogate losses and decentralized detection. Technical Report 695, Dept of Statistics, UC Berkeley, October 2005.</p>
<p>[15] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Nonparametric estimation of the likelihood ratio and divergence functionals. In International Symposium on Information Theory (ISIT), 2007.</p>
<p>[16] G. Rockafellar. Convex Analysis. Princeton University Press, Princeton, 1970.</p>
<p>[17] S. Saitoh. Theory of Reproducing Kernels and its Applications. Longman, Harlow, UK, 1988.</p>
<p>[18] B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o</p>
<p>[19] F. Topsoe. Some inequalities for information divergence and related measures of discrimination. IEEE Transactions on Information Theory, 46:1602–1609, 2000.</p>
<p>[20] S. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.</p>
<p>[21] A. W. van der Vaart and J. Wellner. Weak Convergence and Empirical Processes. Springer-Verlag, New York, NY, 1996.</p>
<p>[22] Q. Wang, S. R. Kulkarni, and S. Verd´ . Divergence estimation of continuous distributions based on u data-dependent partitions. IEEE Transactions on Information Theory, 51(9):3064–3074, 2005.</p>
<p>[23] D. X. Zhou. The covering number in learning theory. Journal of Complexity, 18:739–767, 2002.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
