<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-151" href="../nips2007/nips-2007-Optimistic_Linear_Programming_gives_Logarithmic_Regret_for_Irreducible_MDPs.html">nips2007-151</a> <a title="nips-2007-151-reference" href="#">nips2007-151-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>151 nips-2007-Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs</h1>
<br/><p>Source: <a title="nips-2007-151-pdf" href="http://papers.nips.cc/paper/3329-optimistic-linear-programming-gives-logarithmic-regret-for-irreducible-mdps.pdf">pdf</a></p><p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates, a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time T is within C(P ) log T of the reward obtained by the optimal policy, where C(P ) is an explicit, MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler, it does not require knowledge of the supports of transition probabilities, the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in ﬂavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP. 1</p><br/>
<h2>reference text</h2><p>[1] Burnetas, A.N. & Katehakis, M.N. (1997) Optimal adaptive policies for Markov decision processes. Mathematics of Operations Research 22(1):222–255</p>
<p>[2] Auer, P. & Ortner, R. (2007) Logarithmic online regret bounds for undiscounted reinforcement learning. Advances in Neural Information Processing Systems 19. Cambridge, MA: MIT Press.</p>
<p>[3] Lai, T.L. & Robbins, H. (1985) Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics 6(1):4–22.</p>
<p>[4] Brafman, R.I. & Tennenholtz, M. (2002) R-MAX - a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research 3:213–231.</p>
<p>[5] Auer, P. (2002) Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research 3:397–422.</p>
<p>[6] Auer, P., Cesa-Bianchi, N. & and Fischer, P. (2002) Finite-time analysis of the multiarmed bandit problem. Machine Learning 47(2-3):235-256.</p>
<p>[7] Strehl, A.L. & Littman, M. (2005) A theoretical analysis of model-based interval estimation. In Proceedings of the Twenty-Second International Conference on Machine Learning, pp. 857-864. ACM Press.</p>
<p>[8] Tewari, A. (2007) Reinforcement Learning in Large or Unknown MDPs. PhD thesis, Department of Electrical Engineering and Computer Sciences, University of California at Berkeley.</p>
<p>[9] Puterman, M.L. (1994) Markov Decision Processes: Discrete Stochastic Dynamic Programming. New York: John Wiley and Sons.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
