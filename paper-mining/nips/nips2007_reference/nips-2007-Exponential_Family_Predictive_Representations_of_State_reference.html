<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 nips-2007-Exponential Family Predictive Representations of State</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-86" href="../nips2007/nips-2007-Exponential_Family_Predictive_Representations_of_State.html">nips2007-86</a> <a title="nips-2007-86-reference" href="#">nips2007-86-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>86 nips-2007-Exponential Family Predictive Representations of State</h1>
<br/><p>Source: <a title="nips-2007-86-pdf" href="http://papers.nips.cc/paper/3177-exponential-family-predictive-representations-of-state.pdf">pdf</a></p><p>Author: David Wingate, Satinder S. Baveja</p><p>Abstract: In order to represent state in controlled, partially observable, stochastic dynamical systems, some sort of sufﬁcient statistic for history is necessary. Predictive representations of state (PSRs) capture state as statistics of the future. We introduce a new model of such systems called the “Exponential family PSR,” which deﬁnes as state the time-varying parameters of an exponential family distribution which models n sequential observations in the future. This choice of state representation explicitly connects PSRs to state-of-the-art probabilistic modeling, which allows us to take advantage of current efforts in high-dimensional density estimation, and in particular, graphical models and maximum entropy models. We present a parameter learning algorithm based on maximum likelihood, and we show how a variety of current approximate inference methods apply. We evaluate the quality of our model with reinforcement learning by directly evaluating the control performance of the model. 1</p><br/>
<h2>reference text</h2><p>[1] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002.</p>
<p>[2] E. T. Jaynes. Notes on present status and future prospects. In W. Grandy and L. Schick, editors, Maximum Entropy and Bayesian Methods, pages 1–13, 1991.</p>
<p>[3] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML), 2001.</p>
<p>[4] M. L. Littman, R. S. Sutton, and S. Singh. Predictive representations of state. In Neural Information Processing Systems (NIPS), pages 1555–1561, 2002.</p>
<p>[5] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction and segmentation. In International Conference on Machine Learning (ICML), pages 591–598, 2000.</p>
<p>[6] J. Peters, S. Vijayakumar, and S. Schaal. Natural actor-critic. In European Conference on Machine Learning (ECML), pages 280–291, 2005.</p>
<p>[7] S. D. Pietra, V. D. Pietra, and J. Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997.</p>
<p>[8] M. Rudary, S. Singh, and D. Wingate. Predictive linear-Gaussian models of stochastic dynamical systems. In Uncertainty in Artiﬁcial Intelligence (UAI), pages 501–508, 2005.</p>
<p>[9] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical Report 649, UC Berkeley, 2003.</p>
<p>[10] M. J. Wainwright and M. I. Jordan. Log-determinant relaxation for approximate inference in discrete Markov random ﬁelds. IEEE Transactions on Signal Processing, 54(6):2099–2109, 2006.</p>
<p>[11] D. Wingate. Exponential Family Predictive Representations of State. PhD thesis, University of Michigan, 2008.</p>
<p>[12] J. S. Yedida, W. T. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. Technical Report TR-2001-22, Mitsubishi Electric Research Laboratories, 2001.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
