<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-209" href="../nips2007/nips-2007-Ultrafast_Monte_Carlo_for_Statistical_Summations.html">nips2007-209</a> <a title="nips-2007-209-reference" href="#">nips2007-209-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>209 nips-2007-Ultrafast Monte Carlo for Statistical Summations</h1>
<br/><p>Source: <a title="nips-2007-209-pdf" href="http://papers.nips.cc/paper/3166-ultrafast-monte-carlo-for-statistical-summations.pdf">pdf</a></p><p>Author: Charles L. Isbell, Michael P. Holmes, Alexander G. Gray</p><p>Abstract: Machine learning contains many computational bottlenecks in the form of nested summations over datasets. Kernel estimators and other methods are burdened by these expensive computations. Exact evaluation is typically O(n2 ) or higher, which severely limits application to large datasets. We present a multi-stage stratiﬁed Monte Carlo method for approximating such summations with probabilistic relative error control. The essential idea is fast approximation by sampling in trees. This method differs from many previous scalability techniques (such as standard multi-tree methods) in that its error is stochastic, but we derive conditions for error control and demonstrate that they work. Further, we give a theoretical sample complexity for the method that is independent of dataset size, and show that this appears to hold in experiments, where speedups reach as high as 1014 , many orders of magnitude beyond the previous state of the art. 1</p><br/>
<h2>reference text</h2><p>[1] Nicol N. Schraudolph and Thore Graepel. Combining conjugate direction methods with stochastic approximation of gradients. In Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), 2003.</p>
<p>[2] Alexander G. Gray and Andrew W. Moore. N-body problems in statistical learning. In Advances in Neural Information Processing Systems (NIPS) 13, 2000.</p>
<p>[3] Mike Klaas, Mark Briers, Nando de Freitas, and Arnaud Doucet. Fast particle smoothing: If I had a million particles. In International Conference on Machine Learning (ICML), 2006.</p>
<p>[4] Ping Wang, Dongryeol Lee, Alexander Gray, and James M. Rehg. Fast mean shift with accurate and stable convergence. In Workshop on Artiﬁcial Intelligence and Statistics (AISTATS), 2007.</p>
<p>[5] Michael P. Holmes, Alexander G. Gray, and Charles Lee Isbell Jr. Fast nonparametric conditional density estimation. In Uncertainty in Artiﬁcial Intelligence (UAI), 2007.</p>
<p>[6] Reuven Y. Rubinstein. Simulation and the Monte Carlo Method. John Wiley & Sons, 1981.</p>
<p>[7] Paul Glasserman. Monte Carlo methods in ﬁnancial engineering. Springer-Verlag, 2004.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
