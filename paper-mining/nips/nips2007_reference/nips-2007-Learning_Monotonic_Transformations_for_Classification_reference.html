<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 nips-2007-Learning Monotonic Transformations for Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-112" href="../nips2007/nips-2007-Learning_Monotonic_Transformations_for_Classification.html">nips2007-112</a> <a title="nips-2007-112-reference" href="#">nips2007-112-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 nips-2007-Learning Monotonic Transformations for Classification</h1>
<br/><p>Source: <a title="nips-2007-112-pdf" href="http://papers.nips.cc/paper/3245-learning-monotonic-transformations-for-classification.pdf">pdf</a></p><p>Author: Andrew Howard, Tony Jebara</p><p>Abstract: A discriminative method is proposed for learning monotonic transformations of the training data while jointly estimating a large-margin classiﬁer. In many domains such as document classiﬁcation, image histogram classiﬁcation and gene microarray experiments, ﬁxed monotonic transformations can be useful as a preprocessing step. However, most classiﬁers only explore these transformations through manual trial and error or via prior domain knowledge. The proposed method learns monotonic transformations automatically while training a large-margin classiﬁer without any prior knowledge of the domain. A monotonic piecewise linear function is learned which transforms data for subsequent processing by a linear hyperplane classiﬁer. Two algorithmic implementations of the method are formalized. The ﬁrst solves a convergent alternating sequence of quadratic and linear programs until it obtains a locally optimal solution. An improved algorithm is then derived using a convex semideﬁnite relaxation that overcomes initialization issues in the greedy optimization problem. The eﬀectiveness of these learned transformations on synthetic problems, text data and image data is demonstrated. 1</p><br/>
<h2>reference text</h2><p>[1] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[2] M. Brown, W. Grundy, D. Lin, N. Christianini, C. Sugnet, M. Jr, and D. Haussler. Support vector machine classiﬁcation of microarray gene expression data, 1999.</p>
<p>[3] O. Chapelle, P. Hafner, and V.N. Vapnik. Support vector machines for histogram-based classiﬁcation. Neural Networks, IEEE Transactions on, 10:1055–1064, 1999.</p>
<p>[4] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.</p>
<p>[5] M. Hein and O. Bousquet. Hilbertian metrics and positive deﬁnite kernels on probability measures. In Proceedings of Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[6] T. Jebara, R. Kondor, and A. Howard. Probability product kernels. Journal of Machine Learning Research, 5:819–844, 2004.</p>
<p>[7] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004.</p>
<p>[8] J.B. Lasserre. Convergent LMI relaxations for nonconvex quadratic programs. In Proceedings of 39th IEEE Conference on Decision and Control, 2000.</p>
<p>[9] B. Moghaddam and M.H. Yang. Sex with support vector machines. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing 13, pages 960–966. MIT Press, 2000.</p>
<p>[10] T. Robertson, F.T. Wright, and R.L. Dykstra. Order Restricted Statistical Inference. Wiley, 1988.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
