<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2007-Learning Bounds for Domain Adaptation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-110" href="../nips2007/nips-2007-Learning_Bounds_for_Domain_Adaptation.html">nips2007-110</a> <a title="nips-2007-110-reference" href="#">nips2007-110-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 nips-2007-Learning Bounds for Domain Adaptation</h1>
<br/><p>Source: <a title="nips-2007-110-pdf" href="http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation.pdf">pdf</a></p><p>Author: John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman</p><p>Abstract: Empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain. In the real world, though, we often wish to adapt a classiﬁer from a source domain with a large amount of training data to different target domain with very little training data. In this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk. The bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set. Our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization. 1</p><br/>
<h2>reference text</h2><p>[1] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, 1999.</p>
<p>[2] P. Barlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 3:463–482, 2002.</p>
<p>[3] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation. In NIPS, 2007.</p>
<p>[4] S. Ben-David, J. Gehrke, and D. Kifer. Detecting change in data streams. In VLDB, 2004.</p>
<p>[5] S. Bickel, M. Br¨ ckner, and T. Scheffer. Discriminative learning for differing training and test distribuu tions. In ICML, 2007.</p>
<p>[6] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classiﬁcation. In ACL, 2007.</p>
<p>[7] C. Chelba and A. Acero. Empirical methods in natural language processing. In EMNLP, 2004.</p>
<p>[8] K. Crammer, M. Kearns, and J. Wortman. Learning from multiple sources. In NIPS, 2007.</p>
<p>[9] W. Dai, Q. Yang, G. Xue, and Y. Yu. Boosting for transfer learning. In ICML, 2007.</p>
<p>[10] J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Schoelkopf. Correcting sample selection bias by unlabeled data. In NIPS, 2007.</p>
<p>[11] J. Jiang and C. Zhai. Instance weighting for domain adaptation. In ACL, 2007.</p>
<p>[12] C. Legetter and P. Woodland. Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language, 9:171–185, 1995.</p>
<p>[13] X. Li and J. Bilmes. A bayesian divergence prior for classiﬁcation adaptation. In AISTATS, 2007.</p>
<p>[14] A. Martinez. Recognition of partially occluded and/or imprecisely localized faces using a probabilistic approach. In CVPR, 2007.</p>
<p>[15] D. McAllester. Simpliﬁed PAC-Bayesian margin bounds. In COLT, 2003.</p>
<p>[16] V. Vapnik. Statistical Learning Theory. John Wiley, New York, 1998.</p>
<p>[17] P. Wu and T. Dietterich. Improving svm accuracy by training on auxiliary data sources. In ICML, 2004.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
