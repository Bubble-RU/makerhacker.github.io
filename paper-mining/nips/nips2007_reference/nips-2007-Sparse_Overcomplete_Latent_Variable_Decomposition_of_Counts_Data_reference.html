<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-181" href="../nips2007/nips-2007-Sparse_Overcomplete_Latent_Variable_Decomposition_of_Counts_Data.html">nips2007-181</a> <a title="nips-2007-181-reference" href="#">nips2007-181-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>181 nips-2007-Sparse Overcomplete Latent Variable Decomposition of Counts Data</h1>
<br/><p>Source: <a title="nips-2007-181-pdf" href="http://papers.nips.cc/paper/3235-sparse-overcomplete-latent-variable-decomposition-of-counts-data.pdf">pdf</a></p><p>Author: Madhusudana Shashanka, Bhiksha Raj, Paris Smaragdis</p><p>Abstract: An important problem in many ﬁelds is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the “expressiveness” of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.</p><br/>
<h2>reference text</h2><p>[1] DM Blei and JD Lafferty. Correlated Topic Models. In NIPS, 2006.</p>
<p>[2] DM Blei, AY Ng, and MI Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[3] ME Brand. Pattern Discovery via Entropy Minimization. In Uncertainty 99: AISTATS 99, 1999.</p>
<p>[4] RM Corless, GH Gonnet, DEG Hare, DJ Jeffrey, and DE Knuth. On the Lambert W Function. Advances in Computational mathematics, 1996.</p>
<p>[5] DJ Field. What is the Goal of Sensory Coding? Neural Computation, 1994.</p>
<p>[6] T Hofmann. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Machine Learning, 42:177–196, 2001.</p>
<p>[7] PO Hoyer. Non-negative Matrix Factorization with Sparseness Constraints. Journal of Machine Learning Research, 5, 2004.</p>
<p>[8] DD Lee and HS Seung. Algorithms for Non-negative Matrix Factorization. In NIPS, 2001.</p>
<p>[9] J Skilling. Classic Maximum Entropy. In J Skilling, editor, Maximum Entropy and Bayesian Methods. Kluwer Academic, 1989.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
