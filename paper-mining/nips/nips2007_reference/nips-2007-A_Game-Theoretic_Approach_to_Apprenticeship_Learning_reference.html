<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-5" href="../nips2007/nips-2007-A_Game-Theoretic_Approach_to_Apprenticeship_Learning.html">nips2007-5</a> <a title="nips-2007-5-reference" href="#">nips2007-5-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 nips-2007-A Game-Theoretic Approach to Apprenticeship Learning</h1>
<br/><p>Source: <a title="nips-2007-5-pdf" href="http://papers.nips.cc/paper/3293-a-game-theoretic-approach-to-apprenticeship-learning.pdf">pdf</a></p><p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We study the problem of an apprentice learning to behave in an environment with an unknown reward function by observing the behavior of an expert. We follow on the work of Abbeel and Ng [1] who considered a framework in which the true reward function is assumed to be a linear combination of a set of known and observable features. We give a new algorithm that, like theirs, is guaranteed to learn a policy that is nearly as good as the expert’s, given enough examples. However, unlike their algorithm, we show that ours may produce a policy that is substantially better than the expert’s. Moreover, our algorithm is computationally faster, is easier to implement, and can be applied even in the absence of an expert. The method is based on a game-theoretic view of the problem, which leads naturally to a direct application of the multiplicative-weights algorithm of Freund and Schapire [2] for playing repeated matrix games. In addition to our formal presentation and analysis of the new algorithm, we sketch how the method can be applied when the transition function itself is unknown, and we provide an experimental demonstration of the algorithm on a toy video-game environment. 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel, A. Ng (2004). Apprenticeship Learning via Inverse Reinforcement Learning. ICML 21.</p>
<p>[2] Y. Freund, R. E. Schapire (1999). Adaptive Game Playing Using Multiplicative Weights. Games and Economic Behavior 29, 79–103.</p>
<p>[3] N. Ratliff, J. Bagnell, M. Zinkevich (2006). Maximum Margin Planning. ICML 23.</p>
<p>[4] U. Syed, R. E. Schapire (2007). “A Game-Theoretic Approach to Apprenticeship Learning — Supplement”. http://www.cs.princeton.edu/˜usyed/nips2007/.</p>
<p>[5] M. Kearns, S. Singh (2002). Near-Optimal Reinforcement Learning in Polynomial Time. Machine Learning 49, 209–232.</p>
<p>[6] P. Abbeel, A. Ng (2005). Exploration and Apprenticeship Learning in Reinforcement Learning. ICML 22. (Long version; available at http://www.cs.stanford.edu/˜pabbeel/)  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
