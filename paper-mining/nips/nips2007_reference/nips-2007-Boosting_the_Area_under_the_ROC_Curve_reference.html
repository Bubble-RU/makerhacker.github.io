<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2007-Boosting the Area under the ROC Curve</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-39" href="../nips2007/nips-2007-Boosting_the_Area_under_the_ROC_Curve.html">nips2007-39</a> <a title="nips-2007-39-reference" href="#">nips2007-39-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>39 nips-2007-Boosting the Area under the ROC Curve</h1>
<br/><p>Source: <a title="nips-2007-39-pdf" href="http://papers.nips.cc/paper/3247-boosting-the-area-under-the-roc-curve.pdf">pdf</a></p><p>Author: Phil Long, Rocco Servedio</p><p>Abstract: We show that any weak ranker that can achieve an area under the ROC curve slightly better than 1/2 (which can be achieved by random guessing) can be efﬁciently boosted to achieve an area under the ROC curve arbitrarily close to 1. We further show that this boosting can be performed even in the presence of independent misclassiﬁcation noise, given access to a noise-tolerant weak ranker.</p><br/>
<h2>reference text</h2><p>[1] A. P. Bradley. Use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition, 30:1145–1159, 1997.</p>
<p>[2] C. Cortes and M. Mohri. AUC optimization vs. error rate minimzation. In NIPS 2003, 2003.</p>
<p>[3] T. Fawcett. ROC graphs: Notes and practical considerations for researchers. Technical Report HPL-2003-4, HP, 2003.</p>
<p>[4] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4(6):933–970, 2004.</p>
<p>[5] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.</p>
<p>[6] J. Hanley and B. McNeil. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1):29–36, 1982.</p>
<p>[7] A. Kalai and R. Servedio. Boosting in the presence of noise. Journal of Computer & System Sciences, 71(3):266–290, 2005. Preliminary version in Proc. STOC’03.</p>
<p>[8] M. Kearns and U. Vazirani. An introduction to computational learning theory. MIT Press, Cambridge, MA, 1994.</p>
<p>[9] P. Long and R. Servedio. Martingale boosting. In Proceedings of the Eighteenth Annual Conference on Computational Learning Theory (COLT), pages 79–94, 2005.</p>
<p>[10] F. Provost, T. Fawcett, and Ron Kohavi. The case against accuracy estimation for comparing induction algorithms. ICML, 1998.</p>
<p>[11] C. Rudin, C. Cortes, M. Mohri, and R. E. Schapire. Margin-based ranking meets boosting in the middle. COLT, 2005.</p>
<p>[12] J. A. Swets. Signal detection theory and ROC analysis in psychology and diagnostics: Collected papers. Lawrence Erlbaum Associates, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
