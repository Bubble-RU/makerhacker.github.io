<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-44" href="../nips2007/nips-2007-Catching_Up_Faster_in_Bayesian_Model_Selection_and_Model_Averaging.html">nips2007-44</a> <a title="nips-2007-44-reference" href="#">nips2007-44-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 nips-2007-Catching Up Faster in Bayesian Model Selection and Model Averaging</h1>
<br/><p>Source: <a title="nips-2007-44-pdf" href="http://papers.nips.cc/paper/3277-catching-up-faster-in-bayesian-model-selection-and-model-averaging.pdf">pdf</a></p><p>Author: Tim V. Erven, Steven D. Rooij, Peter Grünwald</p><p>Abstract: Bayesian model averaging, model selection and their approximations such as BIC are generally statistically consistent, but sometimes achieve slower rates of convergence than other methods such as AIC and leave-one-out cross-validation. On the other hand, these other methods can be inconsistent. We identify the catch-up phenomenon as a novel explanation for the slow convergence of Bayesian methods. Based on this analysis we deﬁne the switch-distribution, a modiﬁcation of the Bayesian model averaging distribution. We prove that in many situations model selection and prediction based on the switch-distribution is both consistent and achieves optimal convergence rates, thereby resolving the AIC-BIC dilemma. The method is practical; we give an efﬁcient algorithm. 1</p><br/>
<h2>reference text</h2><p>[1] H. Akaike. A new look at statistical model identiﬁcation. IEEE T. Automat. Contr., 19(6):716–723, 1974.</p>
<p>[2] A. Barron. Logically Smooth Density Estimation. PhD thesis, Stanford University, Stanford, CA, 1985.</p>
<p>[3] A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE T. Inform. Theory, 44(6):2743–2760, 1998.</p>
<p>[4] A. R. Barron. Information-theoretic characterization of Bayes performance and the choice of priors in parametric and nonparametric problems. In Bayesian Statistics 6, pages 27–52, 1998.</p>
<p>[5] A. P. Dawid. Statistical theory: The prequential approach. J. Roy. Stat. Soc. A, 147, Part 2:278–292, 1984.</p>
<p>[6] P. D. Gr¨ nwald. The Minimum Description Length Principle. The MIT Press, 2007. u</p>
<p>[7] M. Herbster and M. K. Warmuth. Tracking the best expert. Machine Learning, 32:151–178, 1998.</p>
<p>[8] R. E. Kass and A. E. Raftery. Bayes factors. J. Am. Stat. Assoc., 90(430):773–795, 1995.</p>
<p>[9] K. Li. Asymptotic optimality of cp , cl , cross-validation and generalized cross-validation: Discrete index set. Ann. Stat., 15:958–975, 1987.</p>
<p>[10] C. Monteleoni and T. Jaakkola. Online learning of non-stationary sequences. In Advances in Neural Information Processing Systems, volume 16, Cambridge, MA, 2004. MIT Press.</p>
<p>[11] J. Rissanen. Universal coding, information, prediction, and estimation. IEEE T. Inform. Theory, IT-30(4): 629–636, 1984.</p>
<p>[12] J. Rissanen. Stochastic Complexity in Statistical Inquiry. World Scientiﬁc, 1989.</p>
<p>[13] J. Rissanen, T. P. Speed, and B. Yu. Density estimation by stochastic complexity. IEEE T. Inform. Theory, 38(2):315–323, 1992.</p>
<p>[14] G. Schwarz. Estimating the dimension of a model. Ann. Stat., 6(2):461–464, 1978.</p>
<p>[15] R. Shibata. Asymptotic mean efﬁciency of a selection of regression variables. Ann. I. Stat. Math., 35: 415–423, 1983.</p>
<p>[16] M. Stone. An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. J. Roy. Stat. Soc. B, 39:44–47, 1977.</p>
<p>[17] P. Volf and F. Willems. Switching between two universal source coding algorithms. In Proceedings of the Data Compression Conference, Snowbird, Utah, pages 491–500, 1998.</p>
<p>[18] V. Vovk. Derandomizing stochastic prediction strategies. Machine Learning, 35:247–282, 1999.</p>
<p>[19] Y. Yang. Can the strengths of AIC and BIC be shared? Biometrica, 92(4):937–950, 2005.</p>
<p>[20] Y. Yang. Model selection for nonparametric regression. Statistica Sinica, 9:475–499, 1999.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
