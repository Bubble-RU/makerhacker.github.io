<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 nips-2007-Bundle Methods for Machine Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-40" href="../nips2007/nips-2007-Bundle_Methods_for_Machine_Learning.html">nips2007-40</a> <a title="nips-2007-40-reference" href="#">nips2007-40-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 nips-2007-Bundle Methods for Machine Learning</h1>
<br/><p>Source: <a title="nips-2007-40-pdf" href="http://papers.nips.cc/paper/3230-bundle-methods-for-machine-learning.pdf">pdf</a></p><p>Author: Quoc V. Le, Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ ) steps to precision for general convex problems and in O(log(1/ )) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Altun, A. J. Smola, and T. Hofmann. Exponential families for conditional random ﬁelds. In UAI, pages 2–9, 2004.</p>
<p>[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[3] J. Hiriart-Urruty and C. Lemar´ chal. Convex Analysis and Minimization Algorithms. 1993. e</p>
<p>[4] T. Joachims. A support vector method for multivariate performance measures. In ICML, pages 377–384, 2005.</p>
<p>[5] T. Joachims. Training linear SVMs in linear time. In KDD, 2006.</p>
<p>[6] S.-I. Lee, V. Ganapathi, and D. Koller. Efﬁcient structure learning of Markov networks using L1regularization. In NIPS, pages 817–824, 2007.</p>
<p>[7] A. Nedich and D. P. Bertsekas. Convergence rate of incremental subgradient algorithms. In Stochastic Optimization: Algorithms and Applications, pages 263–304. 2000.</p>
<p>[8] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.</p>
<p>[9] N. Ratliff, J. Bagnell, and M. Zinkevich. (online) subgradient methods for structured prediction. In Proc. of AIStats, 2007.</p>
<p>[10] R. T. Rockafellar. Convex Analysis. Princeton University Press, 1970.</p>
<p>[11] S. Shalev-Shwartz and Y. Singer. Online learning meets optimization in the dual. In COLT, 2006.</p>
<p>[12] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML, 2007.</p>
<p>[13] A. J. Smola, S. V. N. Vishwanathan, and Q. V. Le. Bundle methods for machine learning. JMLR, 2008. in preparation.</p>
<p>[14] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS, pages 25–32, 2004.</p>
<p>[15] C. H. Teo, Q. Le, A. Smola, and S. V. N. Vishwanathan. A scalable modular convex solver for regularized risk minimization. In KDD, 2007.</p>
<p>[16] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. JMLR, 6:1453–1484, 2005.</p>
<p>[17] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.</p>
<p>[18] T. Zhang. Sequential greedy approximation for certain convex optimization problems. IEEE Trans. Information Theory, 49(3):682–691, 2003.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
