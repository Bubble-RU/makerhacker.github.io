<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-66" href="../nips2007/nips-2007-Density_Estimation_under_Independent_Similarly_Distributed_Sampling_Assumptions.html">nips2007-66</a> <a title="nips-2007-66-reference" href="#">nips2007-66-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>66 nips-2007-Density Estimation under Independent Similarly Distributed Sampling Assumptions</h1>
<br/><p>Source: <a title="nips-2007-66-pdf" href="http://papers.nips.cc/paper/3288-density-estimation-under-independent-similarly-distributed-sampling-assumptions.pdf">pdf</a></p><p>Author: Tony Jebara, Yingbo Song, Kapil Thadani</p><p>Abstract: A method is proposed for semiparametric estimation where parametric and nonparametric criteria are exploited in density estimation and unsupervised learning. This is accomplished by making sampling assumptions on a dataset that smoothly interpolate between the extreme of independently distributed (or id) sample data (as in nonparametric kernel density estimators) to the extreme of independent identically distributed (or iid) sample data. This article makes independent similarly distributed (or isd) sampling assumptions and interpolates between these two using a scalar parameter. The parameter controls a Bhattacharyya afﬁnity penalty between pairs of distributions on samples. Surprisingly, the isd method maintains certain consistency and unimodality properties akin to maximum likelihood estimation. The proposed isd scheme is an alternative for handling nonstationarity in data without making drastic hidden variable assumptions which often make estimation difﬁcult and laden with local optima. Experiments in density estimation on a variety of datasets conﬁrm the value of isd over iid estimation, id estimation and mixture modeling.</p><br/>
<h2>reference text</h2><p>Bengio, Y., Larochelle, H., & Vincent, P. (2005). Non-local manifold Parzen windows. Neural Information Processing Systems. Bhattacharyya, A. (1943). On a measure of divergence between two statistical populations deﬁned by their probability distributions. Bull. Calcutta Math Soc. Collins, M., Dasgupta, S., & Schapire, R. (2002). A generalization of principal components analysis to the exponential family. NIPS. Devroye, L., & Gyorﬁ, L. (1985). Nonparametric density estimation: The l1 view. John Wiley. Efron, B., & Tibshirani, R. (1996). Using specially designed exponential families for density estimation. The Annals of Statistics, 24, 2431–2461. Hjort, N., & Glad, I. (1995). Nonparametric density estimation with a parametric start. The Annals of Statistics, 23, 882–904. Jebara, T., Kondor, R., & Howard, A. (2004). Probability product kernels. Journal of Machine Learning Research, 5, 819–844. Naito, K. (2004). Semiparametric density estimation by local l2 -ﬁtting. The Annals of Statistics, 32, 1162– 1192. Olking, I., & Spiegelman, C. (1987). A semiparametric approach to density estimation. Journal of the American Statistcal Association, 82, 858–865. Prekopa, A. (1973). On logarithmic concave measures and functions. Acta. Sci. Math., 34, 335–343. Rasmussen, C. (1999). The inﬁnite Gaussian mixture model. NIPS. Silverman, B. (1986). Density estimation for statistics and data analysis. Chapman and Hall: London. Teh, Y., Jordan, M., Beal, M., & Blei, D. (2004). Hierarchical Dirichlet processes. NIPS. Topsoe, F. (1999). Some inequalities for information divergence and related measures of discrimination. Journal of Inequalities in Pure and Applied Mathematics, 2. Wand, M., & Jones, M. (1995). Kernel smoothing. CRC Press. 2  Work supported in part by NSF Award IIS-0347499 and ONR Award N000140710507.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
