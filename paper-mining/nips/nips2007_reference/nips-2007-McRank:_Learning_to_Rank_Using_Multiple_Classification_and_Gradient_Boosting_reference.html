<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-126" href="../nips2007/nips-2007-McRank%3A_Learning_to_Rank_Using_Multiple_Classification_and_Gradient_Boosting.html">nips2007-126</a> <a title="nips-2007-126-reference" href="#">nips2007-126-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2007-McRank: Learning to Rank Using Multiple Classification and Gradient Boosting</h1>
<br/><p>Source: <a title="nips-2007-126-pdf" href="http://papers.nips.cc/paper/3270-mcrank-learning-to-rank-using-multiple-classification-and-gradient-boosting.pdf">pdf</a></p><p>Author: Ping Li, Qiang Wu, Christopher J. Burges</p><p>Abstract: We cast the ranking problem as (1) multiple classiﬁcation (“Mc”) (2) multiple ordinal classiﬁcation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the DCG criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classiﬁcations result in perfect DCG scores and the DCG errors are bounded by classiﬁcation errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve LambdaRank [5] and the regressions-based ranker [6], in terms of the (normalized) DCG scores. An efﬁcient implementation of the boosting tree algorithm is also presented.</p><br/>
<h2>reference text</h2><p>[1] A. Agresti. Categorical Data Analysis. John Wiley & Sons, Inc., Hoboken, NJ, second edition, 2002.</p>
<p>[2] L. Brieman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. 1983.</p>
<p>[3] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In WWW, pages 107–117, 1998.</p>
<p>[4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML, pages 89–96, 2005.</p>
<p>[5] C. Burges, R. Ragno, and Q. Le. Learning to rank with nonsmooth cost functions. In NIPS, pages 193–200, 2007.</p>
<p>[6] D. Cossock and T. Zhang. Subset ranking using regression. In COLT, pages 605–619, 2006.</p>
<p>[7] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003.</p>
<p>[8] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. The Annals of Statistics, 28(2):337– 407, 2000.</p>
<p>[9] J. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5):1189–1232, 2001.</p>
<p>[10] K. J¨ rvelin and J. Kek¨ l¨ inen. IR evaluation methods for retrieving highly relevant documents. In SIGIR, pages 41–48, 2000. a aa</p>
<p>[11] T. Joachims. Optimizing search engines using clickthrough data. In KDD, pages 133–142, 2002.</p>
<p>[12] J. Kleinberg. Authoritative sources in a hyperlinked environment. In SODA, pages 668–677, 1998.</p>
<p>[13] M. Tsai, T. Liu, T. Qin, H. Chen, and W. Ma. Frank: a ranking method with ﬁdelity loss. In SIGIR, pages 383–390, 2007.</p>
<p>[14] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression framework for learning ranking functions using relative relevance judgments. In SIGIR, pages 287-294, 2007.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
