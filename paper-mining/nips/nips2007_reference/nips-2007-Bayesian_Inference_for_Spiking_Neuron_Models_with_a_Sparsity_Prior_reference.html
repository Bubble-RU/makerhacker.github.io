<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-33" href="../nips2007/nips-2007-Bayesian_Inference_for_Spiking_Neuron_Models_with_a_Sparsity_Prior.html">nips2007-33</a> <a title="nips-2007-33-reference" href="#">nips2007-33-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>33 nips-2007-Bayesian Inference for Spiking Neuron Models with a Sparsity Prior</h1>
<br/><p>Source: <a title="nips-2007-33-pdf" href="http://papers.nips.cc/paper/3300-bayesian-inference-for-spiking-neuron-models-with-a-sparsity-prior.pdf">pdf</a></p><p>Author: Sebastian Gerwinn, Matthias Bethge, Jakob H. Macke, Matthias Seeger</p><p>Abstract: Generalized linear models are the most commonly used tools to describe the stimulus selectivity of sensory neurons. Here we present a Bayesian treatment of such models. Using the expectation propagation algorithm, we are able to approximate the full posterior distribution over all weights. In addition, we use a Laplacian prior to favor sparse solutions. Therefore, stimulus features that do not critically inﬂuence neural activity will be assigned zero weights and thus be effectively excluded by the model. This feature selection mechanism facilitates both the interpretation of the neuron model as well as its predictive abilities. The posterior distribution can be used to obtain conﬁdence intervals which makes it possible to assess the statistical signiﬁcance of the solution. In neural data analysis, the available amount of experimental measurements is often limited whereas the parameter space is large. In such a situation, both regularization by a sparsity prior and uncertainty estimates for the model parameters are essential. We apply our method to multi-electrode recordings of retinal ganglion cells and use our uncertainty estimate to test the statistical signiﬁcance of functional couplings between neurons. Furthermore we used the sparsity of the Laplace prior to select those ﬁlters from a spike-triggered covariance analysis that are most informative about the neural response. 1</p><br/>
<h2>reference text</h2><p>Chornoboy, E., Schramm, L., & Karr, A. (1988). Maximum likelihood identiﬁcation of neural point process systems. Biological Cybernetics, 59, 265-275. Harris, K., Csicsvari, J., Hirase, H., Dragoi, G., & Buzsaki, G. (2003). Organization of cell assemblies in the hippocampus. Nature, 424(6948), 552–6. Minka, T. (2001). Expectation propagation for approximate Bayesian inference. Uncertainty in Artiﬁcial Intelligence, 17, 362–369. Okatan, M., Wilson, M. A., & Brown, E. N. (2005). Analyzing functional connectivity using a network likelihood model of ensemble neural spiking activity. Neural Computation, 17, 19271961. Opper, M., & Winther, O. (2000). Gaussian Processes for Classiﬁcation: Mean-Field Algorithms. Neural Computation, 12(11), 2655-2684. Paninski, L. (2004). Maximum likelihood estimation of cascade point-process neural encoding models. Network, 15(4), 243–262. Pillow, J. W., Paninski, L., Uzzell, V. J., Simoncelli, E. P., & Chichilnisky, E. J. (2005). Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model. J Neurosci, 25(47), 11003–11013. Rasmussen, C., & Williams, C. (2006). Gaussian processes for machine learning. Springer. Rust, N., Schwartz, O., Movshon, J., & Simoncelli, E.(2005). Spatiotemporal Elements of Macaque V1 Receptive Fields. Neuron, 46(6), 945–956. Seeger, M. (2005). Expectation propagation for exponential families (Tech. Rep.). University of California at Berkeley. (See www.kyb.tuebingen.mpg.de/bs/people/seeger.) Seeger, M., Steinke, F., & Tsuda, K. (2007). Bayesian inference and optimal design in the sparse linear model. AI and Statistics. Simoncelli, E., Paninski, L., Pillow, J., & Schwartz, O. (2004). Characterization of neural responses with stochastic stimuli. In M. Gazzaniga (Ed.), (Vol. 3, pp. 327–338). MIT Press. Snyder, D., & Miller, M. (1991). Random point processes in time and space. Springer Texts in Electrical Engineering. Steveninck, R., & Bialek, W. (1988). Real-Time Performance of a Movement-Sensitive Neuron in the Blowﬂy Visual System: Coding and Information Transfer in Short Spike Sequences. Proceedings of the Royal Society of London. Series B, Biological Sciences, 234(1277), 379–414. Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Touryan, J., Lau, B., & Dan, Y. (2002). Isolation of Relevant Visual Features from Random Stimuli for Cortical Complex Cells. Journal of Neuroscience, 22(24), 10811. Zeck, G. M., Xiao, Q., & Masland, R. H. (2005). The spatial ﬁltering properties of local edge detectors and brisk-sustained retinal ganglion cells. Eur J Neurosci, 22(8), 2016-26.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
