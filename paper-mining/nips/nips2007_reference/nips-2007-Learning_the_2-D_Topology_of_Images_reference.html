<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 nips-2007-Learning the 2-D Topology of Images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-115" href="../nips2007/nips-2007-Learning_the_2-D_Topology_of_Images.html">nips2007-115</a> <a title="nips-2007-115-reference" href="#">nips2007-115-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>115 nips-2007-Learning the 2-D Topology of Images</h1>
<br/><p>Source: <a title="nips-2007-115-pdf" href="http://papers.nips.cc/paper/3206-learning-the-2-d-topology-of-images.pdf">pdf</a></p><p>Author: Nicolas L. Roux, Yoshua Bengio, Pascal Lamblin, Marc Joliveau, Balázs Kégl</p><p>Abstract: We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a ﬁxed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topologyextraction approaches and show how having the two-dimensional topology can be exploited.</p><br/>
<h2>reference text</h2><p>[1] S. Abdallah and M. Plumbley. Geometry dependency analysis. Technical Report C4DM-TR06-05, Center for Digital Music, Queen Mary, University of London, 2006.</p>
<p>[2] Y. Bengio and Y. Le Cun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines. MIT Press, 2007.</p>
<p>[3] G. Hinton, M. Welling, Y. Teh, and S. Osindero. A new view of ica. In Proceedings of ICA-2001, San Diego, CA, 2001.</p>
<p>[4] A. Hyv¨ rinen, P. O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Compua tation, 13(7):1527–1558, 2001.</p>
<p>[5] K. J. Lang and G. E. Hinton. The development of the time-delay neural network architecture for speech recognition. Technical Report CMU-CS-88-152, Carnegie-Mellon University, 1988.</p>
<p>[6] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.</p>
<p>[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998.</p>
<p>[8] Y. LeCun and J. S. Denker. Natural versus universal probability complexity, and entropy. In IEEE Workshop on the Physics of Computation, pages 122–127. IEEE, 1992.</p>
<p>[9] T.-W. Lee and M. S. Lewicki. Unsupervised classiﬁcation segmentation and enhancement of images using ica mixture models. IEEE Trans. Image Proc., 11(3):270–279, 2002.</p>
<p>[10] D. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004.</p>
<p>[11] S. Osindero, M. Welling, and G. Hinton. Topographic product models applied to natural scene statistics. Neural Computation, 18:381–344, 2005.</p>
<p>[12] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, Dec. 2000.</p>
<p>[13] J. Tenenbaum, V. de Silva, and J. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, Dec. 2000.</p>
<p>[14] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of audio signals. IEEE Transactions on Speech and Audio Processing, 10(5):293–302, Jul 2002.</p>
<p>[15] V. Vapnik. Estimation of Dependences Based on Empirical Data. Springer-Verlag, Berlin, 1982.</p>
<p>[16] A. Waibel. Modular construction of time-delay neural networks for speech recognition. Neural Computation, 1:39–46, 1989.</p>
<p>[17] K. Q. Weinberger and L. K. Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI), Boston, MA, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
