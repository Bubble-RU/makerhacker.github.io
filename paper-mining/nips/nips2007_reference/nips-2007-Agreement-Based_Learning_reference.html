<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2007-Agreement-Based Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-22" href="../nips2007/nips-2007-Agreement-Based_Learning.html">nips2007-22</a> <a title="nips-2007-22-reference" href="#">nips2007-22-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2007-Agreement-Based Learning</h1>
<br/><p>Source: <a title="nips-2007-22-pdf" href="http://papers.nips.cc/paper/3246-agreement-based-learning.pdf">pdf</a></p><p>Author: Percy Liang, Dan Klein, Michael I. Jordan</p><p>Abstract: The learning of probabilistic models with many hidden variables and nondecomposable dependencies is an important and challenging problem. In contrast to traditional approaches based on approximate inference in a single intractable model, our approach is to train a set of tractable submodels by encouraging them to agree on the hidden variables. This allows us to capture non-decomposable aspects of the data while still maintaining tractability. We propose an objective function for our approach, derive EM-style algorithms for parameter estimation, and demonstrate their effectiveness on three challenging real-world learning tasks. 1</p><br/>
<h2>reference text</h2><p>[1] J. Besag. The analysis of non-lattice data. The Statistician, 24:179–195, 1975.</p>
<p>[2] P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311, 1993.</p>
<p>[3] G. Hinton. Products of experts. In International Conference on Artiﬁcial Neural Networks, 1999.</p>
<p>[4] V. Jojic, N. Jojic, C. Meek, D. Geiger, A. Siepel, D. Haussler, and D. Heckerman. Efﬁcient approximations for learning phylogenetic HMM models from data. Bioinformatics, 20:161–168, 2004.</p>
<p>[5] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Association for Computational Linguistics (ACL), 2004.</p>
<p>[6] P. Liang, B. Taskar, and D. Klein. Alignment by agreement. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), 2006.</p>
<p>[7] B. Lindsay. Composite likelihood methods. Contemporary Mathematics, 80:221–239, 1988.</p>
<p>[8] H. Ney and S. Vogel. HMM-based word alignment in statistical translation. In International Conference on Computational Linguistics (COLING), 1996.</p>
<p>[9] A. Siepel and D. Haussler. Combining phylogenetic and hidden Markov models in biosequence analysis. Journal of Computational Biology, 11:413–428, 2004.</p>
<p>[10] C. Sutton and A. McCallum. Piecewise training of undirected models. In Uncertainty in Artiﬁcial Intelligence (UAI), 2005.</p>
<p>[11] C. Sutton and A. McCallum. Piecewise pseudolikelihood for efﬁcient CRF training. In International Conference on Machine Learning (ICML), 2007.</p>
<p>[12] M. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Technical report, Department of Statistics, University of California at Berkeley, 2003.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
