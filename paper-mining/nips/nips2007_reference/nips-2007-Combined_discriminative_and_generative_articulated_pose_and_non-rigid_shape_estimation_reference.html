<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2007-Combined discriminative and generative articulated pose and non-rigid shape estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-50" href="../nips2007/nips-2007-Combined_discriminative_and_generative_articulated_pose_and_non-rigid_shape_estimation.html">nips2007-50</a> <a title="nips-2007-50-reference" href="#">nips2007-50-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>50 nips-2007-Combined discriminative and generative articulated pose and non-rigid shape estimation</h1>
<br/><p>Source: <a title="nips-2007-50-pdf" href="http://papers.nips.cc/paper/3271-combined-discriminative-and-generative-articulated-pose-and-non-rigid-shape-estimation.pdf">pdf</a></p><p>Author: Leonid Sigal, Alexandru Balan, Michael J. Black</p><p>Abstract: Estimation of three-dimensional articulated human pose and motion from images is a central problem in computer vision. Much of the previous work has been limited by the use of crude generative models of humans represented as articulated collections of simple parts such as cylinders. Automatic initialization of such models has proved difﬁcult and most approaches assume that the size and shape of the body parts are known a priori. In this paper we propose a method for automatically recovering a detailed parametric model of non-rigid body shape and pose from monocular imagery. Speciﬁcally, we represent the body using a parameterized triangulated mesh model that is learned from a database of human range scans. We demonstrate a discriminative method to directly recover the model parameters from monocular images using a conditional mixture of kernel regressors. This predicted pose and shape are used to initialize a generative model for more detailed pose and shape estimation. The resulting approach allows fully automatic pose and shape recovery from monocular and multi-camera imagery. Experimental results show that our method is capable of robustly recovering articulated pose, shape and biometric measurements (e.g. height, weight, etc.) in both calibrated and uncalibrated camera environments. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal and B. Triggs. Recovering 3D human pose from monocular images, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 28, No. 1, pp. 44–58, 2006.</p>
<p>[2] A. Agarwal and B. Triggs. Monocular human motion capture with a mixture of regressors, IEEE Workshop on Vision for Human-Computer Interaction, 2005.</p>
<p>[3] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers and J.Davis. SCAPE: Shape Completion and Animation of PEople, ACM Transactions on Graphics (SIGGRAPH), Vol. 24(3), pp. 408–416, 2005.</p>
<p>[4] A. Balan, M. J. Black, H. Haussecker and L. Sigal. Shining a light on human pose: On shadows, shading and the estimation of pose and shape, International Conference on Computer Vision (ICCV), 2007.</p>
<p>[5] A. Balan, L. Sigal, M. Black, J. Davis and H. Haussecker. Detailed human shape and pose from images, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007.</p>
<p>[6] S. Belongie, J. Malik and J. Puzicha. Matching shapes, ICCV, pp. 454–461, 2001. 3  Arm span is deﬁned as the distance between knuckles of left and right arm fully extended in ‘T’-pose [5].  7  Subject A Subject B Figure 4: Visualizing pose and shape estimation. Examples of simultaneous pose and shape estimation for subjects A and B are shown on top and bottom respectively. Results are obtained by discriminatively estimating the distribution over the initial state and then reﬁning this distribution via generative local stochastic search. Left column illustrates projection of the estimated model into all 4 views. Middle column shows the projection of the model onto image silhouettes, where light blue denotes image silhouette, dark red projection of the model and orange non-silhouette regions that overlap with the projection. On the right are the two views of the estimated 3D model.</p>
<p>[7] K. M. Cheung, S. Baker and T. Kanade. Shape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture, CVPR, Vol. 1, pp. 77–84, 2003.</p>
<p>[8] J. Deutscher, A. Blake and I. Reid. Articulated body motion capture by annealed particle ﬁltering, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Vol. 2, pp. 126–133, 2000.</p>
<p>[9] K. Grauman, G. Shakhnarovich, T. Darrell. Inferring 3D structure with a statistical image-based shape model, IEEE International Conference on Computer Vision (ICCV), pp. 641–648, 2003.</p>
<p>[10] A. Kanaujia, C. Sminchisescu and D. Metaxas. Semi-supervised Hierarchical Models for 3D Human Pose Reconstruction, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007.</p>
<p>[11] L. Muendermann, S. Corazza and T. Andriacchi. Accurately measuring human movement using articulated ICP with soft-joint constraints and a repository of articulated models, CVPR, 2007.</p>
<p>[12] R. Plankers and P. Fua. Articulated soft objects for video-based body modeling, ICCV, 2001.</p>
<p>[13] R. W. Poppe and M. Poel. Comparison of silhouette shape descriptors for example-based human pose recovery, IEEE Conference on Automatic Face and Gesture Recognition (FG 2006), pp. 541–546, 2006.</p>
<p>[14] R. Rosales and S. Sclaroff. Learning Body Pose Via Specialized Maps, NIPS, 2002.</p>
<p>[15] L. Sigal, S. Bhatia, S. Roth, M. J. Black and M. Isard Tracking Loose-limbed People, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Vol. 1, pp. 421–428, 2004.</p>
<p>[16] C. Sminchisescu, A. Kanajujia and D. Metaxas. Learning Joint Top-Down and Bottom-up Processes for 3D Visual Inference, CVPR, Vol. 2, pp. 1743–1752, 2006.</p>
<p>[17] C. Sminchisescu, A. Kanaujia, Z. Li and D. Metaxas. Discriminative density propagation for 3D human motion estimation, CVPR, Vol. 1, pp. 390–397, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
