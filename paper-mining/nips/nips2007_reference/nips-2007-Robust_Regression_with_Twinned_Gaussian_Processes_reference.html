<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 nips-2007-Robust Regression with Twinned Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-170" href="../nips2007/nips-2007-Robust_Regression_with_Twinned_Gaussian_Processes.html">nips2007-170</a> <a title="nips-2007-170-reference" href="#">nips2007-170-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>170 nips-2007-Robust Regression with Twinned Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2007-170-pdf" href="http://papers.nips.cc/paper/3346-robust-regression-with-twinned-gaussian-processes.pdf">pdf</a></p><p>Author: Andrew Naish-guzman, Sean Holden</p><p>Abstract: We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression, and a specialization of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahramani [2]. The value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture. An additional beneﬁt over the latter method lies in our ability to incorporate knowledge of the noise domain to inﬂuence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods, but yields more conﬁdent predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. [3].</p><br/>
<h2>reference text</h2><p>[1] Volker Tresp. Mixtures of Gaussian processes. In Advances in Neural Information Processing Systems, pages 654–660, 2000.</p>
<p>[2] Carl Edward Rasmussen and Zoubin Ghahramani. Inﬁnite mixtures of gaussian process experts. In Advances in Neural Information Processing Systems, 2002.</p>
<p>[3] Paul Goldberg, Christopher Williams, and Christopher Bishop. Regression with input-dependent noise: a Gaussian process treatment. In Advances in Neural Information Processing Systems. MIT Press, 1998.</p>
<p>[4] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18. MIT Press, 2005.</p>
<p>[5] Thomas Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of Technology, 2001.</p>
<p>[6] Carl Rasmussen and Christopher Williams. Gaussian processes for machine learning. MIT Press, 2006.</p>
<p>[7] Malte Kuss. Gaussian process models for robust regression, classiﬁcation and reinforcement learning. PhD thesis, Technische Universit¨ t Darmstadt, 2006. a</p>
<p>[8] Matthias Seeger. Expectation propagation for exponential families, 2005. Available from http://www.cs.berkeley.edu/˜mseeger/papers/epexpfam.ps.gz.</p>
<p>[9] J. H. Friedman. Multivariate adaptive regression splines. Annals of Statistics, 19(1):1–67, 1991.</p>
<p>[10] B.W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting. Journal of the Royal Statistical Society B, 47:1–52, 1985.</p>
<p>[11] Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian processes. In Advances in Neural Information Processing Systems 16, 2003.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
