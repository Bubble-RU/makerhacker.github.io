<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2007-The Infinite Markov Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-197" href="../nips2007/nips-2007-The_Infinite_Markov_Model.html">nips2007-197</a> <a title="nips-2007-197-reference" href="#">nips2007-197-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 nips-2007-The Infinite Markov Model</h1>
<br/><p>Source: <a title="nips-2007-197-pdf" href="http://papers.nips.cc/paper/3281-the-infinite-markov-model.pdf">pdf</a></p><p>Author: Daichi Mochihashi, Eiichiro Sumita</p><p>Abstract: We present a nonparametric Bayesian method of estimating variable order Markov processes up to a theoretically inﬁnite order. By extending a stick-breaking prior, which is usually deﬁned on a unit interval, “vertically” to the trees of inﬁnite depth associated with a hierarchical Chinese restaurant process, our model directly infers the hidden orders of Markov dependencies from which each symbol originated. Experiments on character and word sequences in natural language showed that the model has a comparative performance with an exponentially large full-order model, while computationally much efﬁcient in both time and space. We expect that this basic model will also extend to the variable order hierarchical clustering of general data. 1</p><br/>
<h2>reference text</h2><p>[1] C. E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27:379–423, 623–656, 1948.</p>
<p>[2] Alberto Apostolico and Gill Bejerano. Optimal amnesic probabilistic automata, or, how to learn and classify proteins in linear time and space. Journal of Computational Biology, 7:381–393, 2000.</p>
<p>[3] F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. The Context-Tree Weighting Method: Basic Properties. IEEE Trans. on Information Theory, 41:653–664, 1995.</p>
<p>[4] Frederick Jelinek. Statistical Methods for Speech Recognition. Language, Speech, and Communication Series. MIT Press, 1998.</p>
<p>[5] Peter Buhlmann and Abraham J. Wyner. Variable Length Markov Chains. The Annals of Statistics, 27(2):480–513, 1999.</p>
<p>[6] Fernando Pereira, Yoram Singer, and Naftali Tishby. Beyond Word N-grams. In Proc. of the Third Workshop on Very Large Corpora, pages 95–106, 1995.</p>
<p>[7] Dana Ron, Yoram Singer, and Naftali Tishby. The Power of Amnesia. In Advances in Neural Information Processing Systems, volume 6, pages 176–183, 1994.</p>
<p>[8] Andreas Stolcke. Entropy-based Pruning of Backoff Language Models. In Proc. of DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274, 1998.</p>
<p>[9] Yee Whye Teh. A Bayesian Interpretation of Interpolated Kneser-Ney. Technical Report TRA2/06, School of Computing, NUS, 2006.</p>
<p>[10] Sharon Goldwater, Thomas L. Grifﬁths, and Mark Johnson. Interpolating Between Types and Tokens by Estimating Power-Law Generators. In NIPS 2005, 2005.</p>
<p>[11] Daniel Sleator and Robert Tarjan. Self-Adjusting Binary Search Trees. JACM, 32(3):652–686, 1985.</p>
<p>[12] Joshua T. Goodman. A Bit of Progress in Language Modeling, Extended Version. Technical Report MSR–TR–2001–72, Microsoft Research, 2001.</p>
<p>[13] Thomas P. Minka. Estimating a Dirichlet distribution, 2000. http://research.microsoft.com/˜minka/papers/ dirichlet/.</p>
<p>[14] Mark Girolami and Ata Kab´ n. Simplicial Mixtures of Markov Chains: Distributed Modelling of Dya namic User Proﬁles. In NIPS 2003. 2003.</p>
<p>[15] R. Daniel Mauldin, William D. Sudderth, and S. C. Williams. Polya Trees and Random Distributions. Annals of Statistics, 20(3):1203–1221, 1992.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
