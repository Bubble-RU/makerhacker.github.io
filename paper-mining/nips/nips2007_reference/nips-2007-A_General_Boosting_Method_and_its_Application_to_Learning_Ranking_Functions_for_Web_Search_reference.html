<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-6" href="../nips2007/nips-2007-A_General_Boosting_Method_and_its_Application_to_Learning_Ranking_Functions_for_Web_Search.html">nips2007-6</a> <a title="nips-2007-6-reference" href="#">nips2007-6-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>6 nips-2007-A General Boosting Method and its Application to Learning Ranking Functions for Web Search</h1>
<br/><p>Source: <a title="nips-2007-6-pdf" href="http://papers.nips.cc/paper/3305-a-general-boosting-method-and-its-application-to-learning-ranking-functions-for-web-search.pdf">pdf</a></p><p>Author: Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, Gordon Sun</p><p>Abstract: We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly, this general framework enables us to use a standard regression base learner such as single regression tree for £tting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show signi£cant improvements of our proposed methods over some existing methods. 1</p><br/>
<h2>reference text</h2><p>[1] BALCAN N., B EYGELZIMER A., L ANGFORD J., AND S ORKIN G. Robust Reductions from Ranking to Classi£cation, manuscript, 2007.</p>
<p>[2] B ERTSEKAS D. Nonlinear programming. Athena Scienti£c, second edition, 1999.</p>
<p>[3] B URGES , C., S HAKED , T., R ENSHAW, E., L AZIER , A., D EEDS , M., H AMILTON , N., AND H ULLEN DER , G. Learning to rank using gradient descent. Proc. of Intl. Conf. on Machine Learning (ICML) (2005).</p>
<p>[4] D IETTERICH , T. G., A SHENFELTER , A., B ULATOV, Y. Training Conditional Random Fields via Gradient Tree Boosting Proc. of Intl. Conf. on Machine Learning (ICML) (2004).</p>
<p>[5] C LEMENCON S., L UGOSI G., AND VAYATIS N. Ranking and scoring using empirical risk minimization. Proc. of COLT (2005).</p>
<p>[6] C OHEN , W. W., S CHAPIRE , R. E., AND S INGER , Y. Learning to order things. Journal of Arti£cial Intelligence Research, Neural Computation, 13, 14431472 (1999).</p>
<p>[7] F REUND , Y., I YER , R., S CHAPIRE , R. E., AND S INGER , Y. An ef£cient boosting algorithm for combining preferences. Journal of Machine Learning Research 4 (2003), 933–969.</p>
<p>[8] F RIEDMAN , J. H. Greedy function approximation: A gradient boosting machine. Annals of Statistics 29, 5 (2001), 1189–1232.</p>
<p>[9] H ERBRICH , R., G RAEPEL , T., AND O BERMAYER , K. Large margin rank boundaries for ordinal regression. 115–132.</p>
<p>[10] JARVELIN , K., AND K EKALAINEN , J. Ir evaluation methods for retrieving highly relevant documents. Proc. of ACM SIGIR Conference (2000).</p>
<p>[11] J OACHIMS , T. Optimizing search engines using clickthrough data. Proc. of ACM SIGKDD Conference (2002).</p>
<p>[12] J OACHIMS , T., G RANKA , L., PAN , B., AND G AY, G. Accurately interpreting clickthough data as implicit feedback. Proc. of ACM SIGIR Conference (2005).</p>
<p>[13] T SOCHANTARIDIS , I., J OACHIMS , T., H OFMANN , T., AND A LTUN , Y. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
