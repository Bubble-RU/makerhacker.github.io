<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>207 nips-2007-Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-207" href="../nips2007/nips-2007-Transfer_Learning_using_Kolmogorov_Complexity%3A_Basic_Theory_and_Empirical_Evaluations.html">nips2007-207</a> <a title="nips-2007-207-reference" href="#">nips2007-207-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>207 nips-2007-Transfer Learning using Kolmogorov Complexity: Basic Theory and Empirical Evaluations</h1>
<br/><p>Source: <a title="nips-2007-207-pdf" href="http://papers.nips.cc/paper/3228-transfer-learning-using-kolmogorov-complexity-basic-theory-and-empirical-evaluations.pdf">pdf</a></p><p>Author: M. M. Mahmud, Sylvian Ray</p><p>Abstract: In transfer learning we aim to solve new problems using fewer examples using information gained from solving related problems. Transfer learning has been successful in practice, and extensive PAC analysis of these methods has been developed. However it is not yet clear how to deﬁne relatedness between tasks. This is considered as a major problem as it is conceptually troubling and it makes it unclear how much information to transfer and when and how to transfer it. In this paper we propose to measure the amount of information one task contains about another using conditional Kolmogorov complexity between the tasks. We show how existing theory neatly solves the problem of measuring relatedness and transferring the ‘right’ amount of information in sequential transfer learning in a Bayesian setting. The theory also suggests that, in a very formal and precise sense, no other reasonable transfer method can do much better than our Kolmogorov Complexity theoretic transfer method, and that sequential transfer is always justiﬁed. We also develop a practical approximation to the method and use it to transfer information between 8 arbitrarily chosen databases from the UCI ML repository. 1</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  Rich Caruana. Multitask learning. Machine Learning, 28:41–75, 1997. Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research, 12:149– 198, March 2000. Shai Ben-David and Reba Schuller. Exploiting task relatedness for learning multiple tasks. In Proceedings of the 16th Annual Conference on Learning Theory, 2003. Brendan Juba. Estimating relatedness via data compression. In Proceedings of the 23rd International Conference on Machine Learning, 2006. Ming Li and Paul Vitanyi. An Introduction to Kolmogorov Complexity and its Applications. SpringerVerlag, New York, 2nd edition, 1997. M. M. Hassan Mahmud. On universal transfer learning. In Proceedings of the 18th International Conference on Algorithmic Learning Theory, 2007. M. M. Hassan Mahmud. On universal transfer learning (Under Review). 2008. R. Cilibrasi and P. Vitanyi. Clustering by compression. IEEE Transactions on Information theory, 51(4):1523–1545, 2004. D.J. Newman, S. Hettich, C.L. Blake, and C.J. Merz. UCI repository of ML databases, 1998. Radford M. Neal. Bayesian methods for machine learning, NIPS tutorial, 2004. Marcus Hutter. Optimality of Bayesian universal prediction for general loss and alphabet. Journal of Machine Learning Research, 4:971–1000, 2003. Marcus Hutter. On universal prediction and bayesian conﬁrmation. Theoretical Computer Science (in press), 2007. Samarth Swarup and Sylvian R. Ray. Cross domain knowledge transfer using structured representations. In Proceedings of the 21st National Conference on Artiﬁcial Intelligence (AAAI), 2006. R. J. Solomonoff. Complexity-based induction systems: comparisons and convergence theorems. IEEE Transactions on Information Theory, 24(4):422–432, 1978. Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan. An introduction to MCMC for machine learning. Machine Learning, 50(1-2):5–43, 2003. Leo Breiman. Random forests. Machine Learning, 45:5–32, 2001. Lilyana Mihalkova, Tuyen Huynh, and Raymond Mooney. Mapping and revising markov logic networks for transfer learning. In Proceedings of the 22nd National Conference on Artiﬁcial Intelligence (AAAI, 2007. Matthew Taylor and Peter Stone. Cross-domain transfer for reinforcement learning. In Proceedings of the 24th International Conference on Machine Learning, 2007.  3 A ﬂavor of this approach: if the standard compressor is gzip, then the function Cgzip (xy) will give the length of the string xy after compression by gzip. Cgzip (xy) − Cgzip (y) will be the conditional Cgzip (x|y). So Cgzip (h|h′ ) will give the relatedness between tasks.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
