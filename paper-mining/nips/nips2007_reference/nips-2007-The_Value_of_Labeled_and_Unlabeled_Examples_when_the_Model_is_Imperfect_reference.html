<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 nips-2007-The Value of Labeled and Unlabeled Examples when the Model is Imperfect</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-201" href="../nips2007/nips-2007-The_Value_of_Labeled_and_Unlabeled_Examples_when_the_Model_is_Imperfect.html">nips2007-201</a> <a title="nips-2007-201-reference" href="#">nips2007-201-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>201 nips-2007-The Value of Labeled and Unlabeled Examples when the Model is Imperfect</h1>
<br/><p>Source: <a title="nips-2007-201-pdf" href="http://papers.nips.cc/paper/3345-the-value-of-labeled-and-unlabeled-examples-when-the-model-is-imperfect.pdf">pdf</a></p><p>Author: Kaushik Sinha, Mikhail Belkin</p><p>Abstract: Semi-supervised learning, i.e. learning from both labeled and unlabeled data has received signiﬁcant attention in the machine learning literature in recent years. Still our understanding of the theoretical foundations of the usefulness of unlabeled data remains somewhat limited. The simplest and the best understood situation is when the data is described by an identiﬁable mixture model, and where each class comes from a pure component. This natural setup and its implications ware analyzed in [11, 5]. One important result was that in certain regimes, labeled data becomes exponentially more valuable than unlabeled data. However, in most realistic situations, one would not expect that the data comes from a parametric mixture distribution with identiﬁable components. There have been recent efforts to analyze the non-parametric situation, for example, “cluster” and “manifold” assumptions have been suggested as a basis for analysis. Still, a satisfactory and fairly complete theoretical understanding of the nonparametric problem, similar to that in [11, 5] has not yet been developed. In this paper we investigate an intermediate situation, when the data comes from a probability distribution, which can be modeled, but not perfectly, by an identiﬁable mixture distribution. This seems applicable to many situation, when, for example, a mixture of Gaussians is used to model the data. the contribution of this paper is an analysis of the role of labeled and unlabeled data depending on the amount of imperfection in the model.</p><br/>
<h2>reference text</h2><p>[1] J. Y. Audibert and A. Tsybakov. Fast convergence rate for plug-in estimators under margin conditions. In Unpublished manuscript, 2005.</p>
<p>[2] M-F. Balcan and A. Blum. A PAC-style model for learning from labeled and unlabeled data. In 18th Annual Conference on Learning Theory, 2005.</p>
<p>[3] M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. Machine Learning, 56, Invited, Special Issue on Clustering:209–239, 2004.</p>
<p>[4] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In 11th Annual Conference on Learning Theory, 1998.</p>
<p>[5] V. Castelli and T. M. Cover. The relative values of labeled and unlabeld samples in pattern recognition with an unknown mixing parameters. IEEE Trans. Information Theory, 42((6):2102–2117, 1996.</p>
<p>[6] O. Chapelle, J. Weston, and B. Scholkopf. Cluster kernels for semi-supervised learning. NIPS, 15, 2002.</p>
<p>[7] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In 10th International Workshop on Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[8] S. Dasgupta, M. L. Littman, and D. McAllester. PAC generalization bounds for co-training. NIPS, 14, 2001.</p>
<p>[9] L. Devroye, L. Gyorﬁ, and G. Lugosi. A probabilistic theory of pattern recognition. Springer, New York, Berlin, Heidelberg, 1996.</p>
<p>[10] J. Ratsaby. The complexity of learning from a mixture of labeled and unlabeled examples. In Phd Thesis, 1994.</p>
<p>[11] J. Ratsaby and S. S. Venkatesh. Learning from a mixture of labeled and unlabeled examples with parametric side information. In 8th Annual Conference on Learning Theory, 1995.</p>
<p>[12] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist., 32(1):135–166, 1996.</p>
<p>[13] V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.</p>
<p>[14] Y. Yang. Minimax nonparametric classiﬁcation- part I: Rates of convergence, part II: Model selection for adaptation. IEEE Trans. Inf. Theory, 45:2271–2292, 1999.</p>
<p>[15] X. Zhu. Semi-supervised literature survey. Technical Report 1530, Department of Computer Science, University of Wisconsin Madison, December 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
