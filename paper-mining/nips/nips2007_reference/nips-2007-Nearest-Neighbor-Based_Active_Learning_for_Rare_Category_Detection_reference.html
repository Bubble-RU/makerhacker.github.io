<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-139" href="../nips2007/nips-2007-Nearest-Neighbor-Based_Active_Learning_for_Rare_Category_Detection.html">nips2007-139</a> <a title="nips-2007-139-reference" href="#">nips2007-139-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>139 nips-2007-Nearest-Neighbor-Based Active Learning for Rare Category Detection</h1>
<br/><p>Source: <a title="nips-2007-139-pdf" href="http://papers.nips.cc/paper/3344-nearest-neighbor-based-active-learning-for-rare-category-detection.pdf">pdf</a></p><p>Author: Jingrui He, Jaime G. Carbonell</p><p>Abstract: Rare category detection is an open challenge for active learning, especially in the de-novo case (no labeled examples), but of signiﬁcant practical importance for data mining - e.g. detecting new ﬁnancial transaction fraud patterns, where normal legitimate transactions dominate. This paper develops a new method for detecting an instance of each minority class via an unsupervised local-density-differential sampling strategy. Essentially a variable-scale nearest neighbor process is used to optimize the probability of sampling tightly-grouped minority classes, subject to a local smoothness assumption of the majority class. Results on both synthetic and real data sets are very positive, detecting each minority class with only a fraction of the actively sampled points required by random sampling and by Pelleg’s Interleave method, the prior best technique in the sparse literature on this topic. 1</p><br/>
<h2>reference text</h2><p>[1] M. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proc. of the 23rd Int. Conf. on Machine Learning, pages 65–72, 2006.</p>
<p>[2] S. Bay, K. Kumaraswamy, M. Anderle, R. Kumar, and D. Steier. Large scale detection of irregularities in accounting data. In Proc. of the 6th Int. Conf. on Data Mining, pages 75–86, 2006.</p>
<p>[3] C. Blake and C. Merz. Uci repository of machine learning databases. http://www.ics.uci.edu/ machine/MLRepository.html, 1998.</p>
<p>[4] P. Brazdil and J. Gama. Statlog repository. http://www.niaad.liacc.up.pt/old/statlog/datasets/shuttle/shuttle.doc.html, 1991.  In In</p>
<p>[5] S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Information Processing Systems 19, 2005.</p>
<p>[6] S. Fine and Y. Mansour. Active sampling for multiple output identiﬁcation. In The 19th Annual Conf. on Learning Theory, pages 620–634, 2006.</p>
<p>[7] A. Moore. A tutorial on kd-trees. Technical report, University of Cambridge Computer Laboratory, 1991.</p>
<p>[8] D. Pelleg and A. Moore. Active learning for anomaly and rare-category detection. In Advances in Neural Information Processing Systems 18, 2004.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
