<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2007-Heterogeneous Component Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-96" href="../nips2007/nips-2007-Heterogeneous_Component_Analysis.html">nips2007-96</a> <a title="nips-2007-96-reference" href="#">nips2007-96-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2007-Heterogeneous Component Analysis</h1>
<br/><p>Source: <a title="nips-2007-96-pdf" href="http://papers.nips.cc/paper/3222-heterogeneous-component-analysis.pdf">pdf</a></p><p>Author: Shigeyuki Oba, Motoaki Kawanabe, Klaus-Robert Müller, Shin Ishii</p><p>Abstract: In bioinformatics it is often desirable to combine data from various measurement sources and thus structured feature vectors are to be analyzed that possess different intrinsic blocking characteristics (e.g., different patterns of missing values, observation noise levels, effective intrinsic dimensionalities). We propose a new machine learning tool, heterogeneous component analysis (HCA), for feature extraction in order to better understand the factors that underlie such complex structured heterogeneous data. HCA is a linear block-wise sparse Bayesian PCA based not only on a probabilistic model with block-wise residual variance terms but also on a Bayesian treatment of a block-wise sparse factor-loading matrix. We study various algorithms that implement our HCA concept extracting sparse heterogeneous structure by obtaining common components for the blocks and speciﬁc components within each block. Simulations on toy and bioinformatics data underline the usefulness of the proposed structured matrix factorization concept. 1</p><br/>
<h2>reference text</h2><p>[1] I. Nabney and Christopher Bishop. Netlab: Netlab neural network software. http://www.ncrg.aston.ac.uk/netlab/, 1995.</p>
<p>[2] C.M. Bishop. Bayesian PCA. In Proceedings of 11th conference on Advances in neural information processing systems, pages 382–388. MIT Press Cambridge, MA, USA, 1999.</p>
<p>[3] N. Srebro and T. Jaakkola. Weighted low rank matrix approximations. In Proceedings of 20th International Conference on Machine Learning, pages 720–727, 2003.</p>
<p>[4] A. d’Aspremont, F. R. Bach, and L. El Ghaoui. Full regularization path for sparse principal component analysis. In Proceedings of the 24th International Conference on Machine Learning, 2007.</p>
<p>[5] S. Oba, M. Sato, I. Takemasa, M. Monden, K. Matsubara, and S. Ishii. A Bayesian missing value estimation method for gene expression proﬁle data. Bioinformatics, 19(16):2088–2096, 2003.</p>
<p>[6] M. Ohira, S. Oba, Y. Nakamura, E. Isogai, S. Kaneko, A. Nakagawa, T. Hirata, H. Kubo, T. Goto, S. Yamada, Y. Yoshida, M. Fuchioka, S. Ishii, and A. Nakagawara. Expression proﬁling using a tumor-speciﬁc cDNA microarray predicts the prognosis of intermediate risk neuroblastomas. Cancer Cell, 7(4):337–350, Apr 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
