<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-45" href="../nips2007/nips-2007-Classification_via_Minimum_Incremental_Coding_Length_%28MICL%29.html">nips2007-45</a> <a title="nips-2007-45-reference" href="#">nips2007-45-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 nips-2007-Classification via Minimum Incremental Coding Length (MICL)</h1>
<br/><p>Source: <a title="nips-2007-45-pdf" href="http://papers.nips.cc/paper/3314-classification-via-minimum-incremental-coding-length-micl.pdf">pdf</a></p><p>Author: John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, Heung-yeung Shum</p><p>Abstract: We present a simple new criterion for classiﬁcation, based on principles from lossy data compression. The criterion assigns a test sample to the class that uses the minimum number of additional bits to code the test sample, subject to an allowable distortion. We prove asymptotic optimality of this criterion for Gaussian data and analyze its relationships to classical classiﬁers. Theoretical results provide new insights into relationships among popular classiﬁers such as MAP and RDA, as well as unsupervised clustering methods based on lossy compression [13]. Minimizing the lossy coding length induces a regularization effect which stabilizes the (implicit) density estimate in a small-sample setting. Compression also provides a uniform means of handling classes of varying dimension. This simple classiﬁcation criterion and its kernel and local versions perform competitively against existing classiﬁers on both synthetic examples and real imagery data such as handwritten digits and human faces, without requiring domain-speciﬁc information. 1</p><br/>
<h2>reference text</h2><p>[1] A. Barron, J. Rissanen, and B. Yu. The minimum description length principle in coding and modeling. IEEE Transactions on Information Theory, 44(6):2743–2760, 1998.</p>
<p>[2] R. Basri and D. Jacobs. Lambertian reﬂection and linear subspaces. PAMI, 25(2):218– 233, 2003.</p>
<p>[3] P. Bickel and B. Li. Regularization in statistics. TEST, 15(2):271–344, 2006.</p>
<p>[4] C. Chang and C. Lin. LIBSVM: a library for support vector machines, 2001.</p>
<p>[5] T. Cover and J. Thomas. Elements of Information Theory. Wiley Series in Telecommunications, 1991.</p>
<p>[6] J. Friedman. Regularized discriminant analysis. JASA, 84:165–175, 1989.</p>
<p>[7] A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. PAMI, 23(6):643–660, 2001.</p>
<p>[8] P. Grunwald and J. Langford. Suboptimal behaviour of Bayes and MDL in classiﬁcation under misspeciﬁcation. In Proceedings of Conference on Learning Theory, 2004.</p>
<p>[9] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001.</p>
<p>[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</p>
<p>[11] K. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under variable lighting. PAMI, 27(5):684–698, 2005.</p>
<p>[12] J. Li. A source coding approach to classiﬁcation by vector quantization and the principle of minimum description length. In IEEE DCC, pages 382–391, 2002.</p>
<p>[13] Y. Ma, H. Derksen, W. Hong, and J. Wright. Segmentation of multivariate mixed data via lossy data coding and compression. PAMI, 29(9):1546–1562, 2007.</p>
<p>[14] D. MacKay. Developments in probabilistic modelling with neural networks – ensemble learning. In Proc. 3rd Annual Symposium on Neural Networks, pages 191–198, 1995.</p>
<p>[15] M. Madiman, M. Harrison, and I. Kontoyiannis. Minimum description length vs. maximum likelihood in lossy data compression. In IEEE International Symposium on Information Theory, 2004.</p>
<p>[16] J. Rissanen. Modeling by shortest data description. Automatica, 14:465–471, 1978.</p>
<p>[17] P. Simard, Y. LeCun, and J. Denker. Efﬁcient pattern recognition using a new transformation distance. In Proceedings of NIPS, volume 5, 1993.</p>
<p>[18] P. Simard, D. Steinkraus, and J. Platt. Best practice for convolutional neural networks applied to visual document analysis. In ICDAR, pages 958–962, 2003.</p>
<p>[19] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Allerton, 1999.</p>
<p>[20] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 2000.</p>
<p>[21] J. Wright, Y. Tao, Z. Lin, Y. Ma, and H. Shum. Classiﬁcation via minimum incremental coding length (MICL). Technical report, UILU-ENG-07-2201, http://perception.csl.uiuc.edu/coding, 2007.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
