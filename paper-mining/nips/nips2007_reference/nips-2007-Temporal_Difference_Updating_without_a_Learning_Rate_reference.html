<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2007-Temporal Difference Updating without a Learning Rate</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-191" href="../nips2007/nips-2007-Temporal_Difference_Updating_without_a_Learning_Rate.html">nips2007-191</a> <a title="nips-2007-191-reference" href="#">nips2007-191-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 nips-2007-Temporal Difference Updating without a Learning Rate</h1>
<br/><p>Source: <a title="nips-2007-191-pdf" href="http://papers.nips.cc/paper/3290-temporal-difference-updating-without-a-learning-rate.pdf">pdf</a></p><p>Author: Marcus Hutter, Shane Legg</p><p>Abstract: We derive an equation for temporal difference learning from statistical principles. Speciﬁcally, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(λ), however it lacks the parameter α that speciﬁes the learning rate. In the place of this free parameter there is now an equation for the learning rate that is speciﬁc to each state transition. We experimentally test this new learning rule against TD(λ) and ﬁnd that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins’ Q(λ) and Sarsa(λ) and ﬁnd that it again offers superior performance without a learning rate parameter. 1</p><br/>
<h2>reference text</h2><p>[1] A. P. George and W. B. Powell. Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming. Journal of Machine Learning, 65(1):167–198, 2006.</p>
<p>[2] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003.</p>
<p>[3] J. Peng and R. J. Williams. Increamental multi-step Q-learning. Machine Learning, 22:283–290, 1996.</p>
<p>[4] G. A. Rummery. Problem solving with reinforcement learning. PhD thesis, Cambridge University, 1995.</p>
<p>[5] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technial Report CUED/F-INFENG/TR 166, Engineering Department, Cambridge University, 1994.</p>
<p>[6] R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge, MA, MIT Press, 1998.</p>
<p>[7] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9–44, 1988.</p>
<p>[8] C.J.C.H Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Oxford, 1989.</p>
<p>[9] I. H. Witten. An adaptive optimal controller for discrete-time markov environments. Information and Control, 34:286–295, 1977.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
