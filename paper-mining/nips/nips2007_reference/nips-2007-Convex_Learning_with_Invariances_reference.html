<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2007-Convex Learning with Invariances</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-62" href="../nips2007/nips-2007-Convex_Learning_with_Invariances.html">nips2007-62</a> <a title="nips-2007-62-reference" href="#">nips2007-62-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2007-Convex Learning with Invariances</h1>
<br/><p>Source: <a title="nips-2007-62-pdf" href="http://papers.nips.cc/paper/3218-convex-learning-with-invariances.pdf">pdf</a></p><p>Author: Choon H. Teo, Amir Globerson, Sam T. Roweis, Alex J. Smola</p><p>Abstract: Incorporating invariances into a learning algorithm is a common problem in machine learning. We provide a convex formulation which can deal with arbitrary loss functions and arbitrary losses. In addition, it is a drop-in replacement for most optimization algorithms for kernels, including solvers of the SVMStruct family. The advantage of our setting is that it relies on column generation instead of modifying the underlying optimization problem directly. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Abu-Mostafa. A method for learning from hints. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, NIPS 5, 1992.</p>
<p>[2] C. Bhattacharyya, K. S. Pannagadatta, and A. J. Smola. A second order cone programming formulation for classifying missing data. In L. K. Saul, Y. Weiss, and L. Bottou, editors, NIPS 17, 2005.</p>
<p>[3] C. J. C. Burges. Geometry and invariance in kernel based methods. In B. Sch¨ lkopf, C. J. C. Burges, and o A. J. Smola, editors, Advances in Kernel Methods — Support Vector Learning, pages 89–116, Cambridge, MA, 1999. MIT Press.</p>
<p>[4] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial classiﬁcation. In KDD, 2004.</p>
<p>[5] D. DeCoste and B. Sch¨ lkopf. Training invariant support vector machines. Machine Learning, 46:161– o 190, 2002.</p>
<p>[6] M. Ferraro and T. M. Caelli. Lie transformation groups, integral transforms, and invariant pattern recognition. Spatial Vision, 8:33–44, 1994.</p>
<p>[7] A. Globerson and S. Roweis. Nightmare at test time: Robust learning by feature deletion. In ICML, 2006.</p>
<p>[8] T. Graepel and R. Herbrich. Invariant pattern recognition by semideﬁnite programming machines. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, NIPS 16, 2004. o</p>
<p>[9] G. E. Hinton. Learning translation invariant recognition in massively parallel networks. In Proceedings Conference on Parallel Architectures and Laguages Europe, pages 1–13. Springer, 1987.</p>
<p>[10] T. Joachims. Training linear SVMs in linear time. In KDD, 2006.</p>
<p>[11] Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, U. A. M¨ ller, E. S¨ ckinger, P. Simard, and V. Vapnik. Comparison of learning algorithms for handwritten digit u a recognition. In F. Fogelman-Souli´ and P. Gallinari, editors, ICANN, 1995. e</p>
<p>[12] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML, 2007.</p>
<p>[13] P. Simard, Y. LeCun, and J. Denker. Efﬁcient pattern recognition using a new transformation distance. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, NIPS 5, 1993.</p>
<p>[14] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, NIPS 16, 2004. o</p>
<p>[15] C.H. Teo, Q. Le, A.J. Smola, and S.V.N. Vishwanathan. A scalable modular convex solver for regularized risk minimization. In KDD, 2007.</p>
<p>[16] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453–1484, 2005.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
