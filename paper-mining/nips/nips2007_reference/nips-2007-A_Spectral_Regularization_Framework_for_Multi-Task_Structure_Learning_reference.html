<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2007-A Spectral Regularization Framework for Multi-Task Structure Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-12" href="../nips2007/nips-2007-A_Spectral_Regularization_Framework_for_Multi-Task_Structure_Learning.html">nips2007-12</a> <a title="nips-2007-12-reference" href="#">nips2007-12-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>12 nips-2007-A Spectral Regularization Framework for Multi-Task Structure Learning</h1>
<br/><p>Source: <a title="nips-2007-12-pdf" href="http://papers.nips.cc/paper/3187-a-spectral-regularization-framework-for-multi-task-structure-learning.pdf">pdf</a></p><p>Author: Andreas Argyriou, Massimiliano Pontil, Yiming Ying, Charles A. Micchelli</p><p>Abstract: Learning the common structure shared by a set of supervised tasks is an important practical and theoretical problem. Knowledge of this structure may lead to better generalization performance on the tasks and may also facilitate learning new tasks. We propose a framework for solving this problem, which is based on regularization with spectral functions of matrices. This class of regularization problems exhibits appealing computational properties and can be optimized efﬁciently by an alternating minimization algorithm. In addition, we provide a necessary and sufﬁcient condition for convexity of the regularizer. We analyze concrete examples of the framework, which are equivalent to regularization with Lp matrix norms. Experiments on two real data sets indicate that the algorithm scales well with the number of tasks and improves on state of the art statistical performance. 1</p><br/>
<h2>reference text</h2><p>[1] J. Abernethy, F. Bach, T. Evgeniou, and J-P. Vert. Low-rank matrix factorization with attributes. Technical Report N24/06/MM, Ecole des Mines de Paris, 2006.</p>
<p>[2] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853, 2005.</p>
<p>[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 2007. In press.</p>
<p>[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems 19, pages 41–48. 2007.</p>
<p>[5] B. Bakker and T. Heskes. Task clustering and gating for bayesian multi–task learning. Journal of Machine Learning Research, 4:83–99, 2003.</p>
<p>[6] J. Baxter. A model for inductive bias learning. J. of Artiﬁcial Intelligence Research, 12:149–198, 2000.</p>
<p>[7] R. Bhatia. Matrix Analysis. Graduate texts in Mathematics. Springer, 1997.</p>
<p>[8] R. Chari, W.W. Lockwood, and B.P. Coe et al. Sigma: a system for integrative genomic microarray analysis of cancer genomes. BMC Genomics, 7:324, 2006.</p>
<p>[9] J.-B. Hiriart-Urruty and C. Lemar´ chal. Convex Analysis and Minimization Algorithms. Springer, 1996. e</p>
<p>[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.</p>
<p>[11] G.R.G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M.I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2005.</p>
<p>[12] P. J. Lenk, W. S. DeSarbo, P. E. Green, and M. R. Young. Hierarchical Bayes conjoint analysis: recovery of partworth heterogeneity from reduced experimental designs. Marketing Science, 15(2):173–191, 1996.</p>
<p>[13] A. W. Marshall and I. Olkin. Inequalities: Theory of Majorization and its Applications. Academic Press, 1979.</p>
<p>[14] A. Maurer. Bounds for linear multi-task learning. J. of Machine Learning Research, 7:117–139, 2006.</p>
<p>[15] C.A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine Learning Research, 6:1099–1125, 2005.</p>
<p>[16] R. Raina, A. Y. Ng, and D. Koller. Constructing informative priors using transfer learning. In Proceedings of the 23rd International Conference on Machine Learning, 2006.</p>
<p>[17] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 1329–1336. 2005.</p>
<p>[18] A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing features: efﬁcient boosting procedures for multiclass object detection. In Proc. of Conf. on Computer Vision and Pattern Recognition. 2:762-769, 2004.</p>
<p>[19] J. Zhang, Z. Ghahramani, and Y. Yang. Learning multiple related tasks using latent independent component analysis. In Advances in Neural Information Processing Systems 18, pages 1585–1592. 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
