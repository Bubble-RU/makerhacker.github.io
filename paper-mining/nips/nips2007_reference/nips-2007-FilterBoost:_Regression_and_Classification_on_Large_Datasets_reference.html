<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-90" href="../nips2007/nips-2007-FilterBoost%3A_Regression_and_Classification_on_Large_Datasets.html">nips2007-90</a> <a title="nips-2007-90-reference" href="#">nips2007-90-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>90 nips-2007-FilterBoost: Regression and Classification on Large Datasets</h1>
<br/><p>Source: <a title="nips-2007-90-pdf" href="http://papers.nips.cc/paper/3321-filterboost-regression-and-classification-on-large-datasets.pdf">pdf</a></p><p>Author: Joseph K. Bradley, Robert E. Schapire</p><p>Abstract: We study boosting in the ﬁltering setting, where the booster draws examples from an oracle instead of using a ﬁxed training set and so may train efﬁciently on very large datasets. Our algorithm, which is based on a logistic regression technique proposed by Collins, Schapire, & Singer, requires fewer assumptions to achieve bounds equivalent to or better than previous work. Moreover, we give the ﬁrst proof that the algorithm of Collins et al. is a strong PAC learner, albeit within the ﬁltering setting. Our proofs demonstrate the algorithm’s strong theoretical properties for both classiﬁcation and conditional probability estimation, and we validate these results through extensive experiments. Empirically, our algorithm proves more robust to noise and overﬁtting than batch boosters in conditional probability estimation and proves competitive in classiﬁcation. 1</p><br/>
<h2>reference text</h2><p>[1] Freund, Y., & Schapire, R. E. (1997) A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55, 119-139.</p>
<p>[2] Schapire., R. E. (1990) The strength of weak learnability. Machine Learning, 5(2), pp. 197-227.</p>
<p>[3] Freund, Y. (1995) Boosting a weak learning algorithm by majority. Information and Computation, 121, pp. 256-285.</p>
<p>[4] Freund, Y. (1992) An improved boosting algorithm and its implications on learning complexity. 5th Annual Conference on Computational Learning Theory, pp. 391-398.</p>
<p>[5] Domingo, C., & Watanabe, O. (2000) MadaBoost: a modiﬁcation of AdaBoost. 13th Annual Conference on Computational Learning Theory, pp. 180-189.</p>
<p>[6] Bshouty, N. H., & Gavinsky, D. (2002) On boosting with polynomially bounded distributions. Journal of Machine Learning Research, 3, pp. 483-506.</p>
<p>[7] Gavinsky, D. (2003) Optimally-smooth adaptive boosting and application to agnostic learning. Journal of Machine Learning Research, 4, pp. 101-117.</p>
<p>[8] Hatano, K. (2006) Smooth boosting using an information-based criterion. 17th International Conference on Algorithmic Learning Theory, pp. 304-319.</p>
<p>[9] Schapire, R. E., & Singer, Y. (1999) Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning, 37, 297-336.</p>
<p>[10] Collins, M., Schapire, R. E., & Singer, Y. (2002) Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48, pp. 253-285.</p>
<p>[11] Watanabe, O. (2000) Simple sampling techniques for discovery science. IEICE Trans. Information and Systems, E83-D(1), 19-26.</p>
<p>[12] Domingo, C., Galvad` , R., & Watanabe, O. (2002) Adaptive sampling methods for scaling up knowledge a discovery algorithms. Data Mining and Knowledge Discovery, 6, pp. 131-152.</p>
<p>[13] Friedman, J., Hastie, T., & Tibshirani, R. (2000) Additive logistic regression: a statistical view of boosting. The Annals of Statistics, 28, 337-407.</p>
<p>[14] Shalev-Shwartz, S., & Singer, Y. (2006) Convex repeated games and Fenchel duality. Advances in Neural Information Processing Systems 20.</p>
<p>[15] Breiman, L. (1998) Arcing classiﬁers. The Annals of Statistics, 26, pp. 801-849.</p>
<p>[16] Newman, D. J., Hettich, S., Blake, C. L., & Merz, C. J. (1998) UCI Repository of machine learning databases [http://www.ics.uci.edu/∼mlearn/MLRepository.html]. Irvine, CA: U. of California, Dept. of Information & Computer Science.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
