<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-57" href="../nips2007/nips-2007-Congruence_between_model_and_human_attention_reveals_unique_signatures_of_critical_visual_events.html">nips2007-57</a> <a title="nips-2007-57-reference" href="#">nips2007-57-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2007-Congruence between model and human attention reveals unique signatures of critical visual events</h1>
<br/><p>Source: <a title="nips-2007-57-pdf" href="http://papers.nips.cc/paper/3360-congruence-between-model-and-human-attention-reveals-unique-signatures-of-critical-visual-events.pdf">pdf</a></p><p>Author: Robert Peters, Laurent Itti</p><p>Abstract: Current computational models of bottom-up and top-down components of attention are predictive of eye movements across a range of stimuli and of simple, ﬁxed visual tasks (such as visual search for a target among distractors). However, to date there exists no computational framework which can reliably mimic human gaze behavior in more complex environments and tasks, such as driving a vehicle through traﬃc. Here, we develop a hybrid computational/behavioral framework, combining simple models for bottom-up salience and top-down relevance, and looking for changes in the predictive power of these components at diﬀerent critical event times during 4.7 hours (500,000 video frames) of observers playing car racing and ﬂight combat video games. This approach is motivated by our observation that the predictive strengths of the salience and relevance models exhibit reliable temporal signatures during critical event windows in the task sequence—for example, when the game player directly engages an enemy plane in a ﬂight combat game, the predictive strength of the salience model increases signiﬁcantly, while that of the relevance model decreases signiﬁcantly. Our new framework combines these temporal signatures to implement several event detectors. Critically, we ﬁnd that an event detector based on fused behavioral and stimulus information (in the form of the model’s predictive strength) is much stronger than detectors based on behavioral information alone (eye position) or image information alone (model prediction maps). This approach to event detection, based on eye tracking combined with computational models applied to the visual input, may have useful applications as a less-invasive alternative to other event detection approaches based on neural signatures derived from EEG or fMRI recordings. 1</p><br/>
<h2>reference text</h2><p>[1] L. Itti and P. Baldi. A principled approach to detecting surprising events in video. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 631–637, San Siego, CA, Jun 2005.</p>
<p>[2] V. Navalpakkam and L. Itti. Modeling the inﬂuence of task on attention. Vision Research, 45(2):205–231, January 2005.</p>
<p>[3] A. Torralba, A. Oliva, M.S. Castelhano, and J.M. Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psychological Review, 113(4):766–786, October 2006.</p>
<p>[4] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, November 1998.</p>
<p>[5] D. Parkhurst, K. Law, and E. Niebur. Modeling the role of salience in the allocation of overt visual attention. Vision Research, 42(1):107–123, 2002.</p>
<p>[6] R.J. Peters, A. Iyer, L. Itti, and C. Koch. Components of bottom-up gaze allocation in natural images. Vision Research, 45(18):2397–2416, 2005.</p>
<p>[7] R. Carmi and L. Itti. Visual causes versus correlates of attentional selection in dynamic scenes. Vision Research, 46(26):4333–4345, Dec 2006.</p>
<p>[8] R.J. Peters and L. Itti. Computational mechanisms for gaze direction in interactive visual environments. In Proc. ACM Eye Tracking Research and Applications, pages 27–32, Mar 2006.</p>
<p>[9] R.J. Peters and L. Itti. Beyond bottom-up: Incorporating task-dependent inﬂuences into a computational model of spatial attention. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2007), Minneapolis, MN, Jun 2007.</p>
<p>[10] M. Hayhoe and D. Ballard. Eye movements in natural behavior. Trends in Cognitive Sciences, 9(4):188– 194, April 2005.</p>
<p>[11] J. Theeuwes, A.F. Kramer, S. Hahn, D.E. Irwin, and G.J. Zelinsky. Inﬂuence of attentional capture on oculomotor control. Journal of Experimental Psychology—Human Perception and Performance, 25(6):1595– 1608, December 1999.</p>
<p>[12] W. Einhauser, W. Kruse, K.P. Hoﬀmann, and P. Konig. Diﬀerences of monkey and human overt attention under natural conditions. Vision Research, 46(8-9):1194–1209, April 2006.</p>
<p>[13] A.D. Gerson, L.C. Parra, and P. Sajda. Cortically coupled computer vision for rapid image search. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 14(2):174–179, June 2006.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
