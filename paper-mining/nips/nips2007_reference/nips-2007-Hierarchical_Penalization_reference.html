<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>99 nips-2007-Hierarchical Penalization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-99" href="../nips2007/nips-2007-Hierarchical_Penalization.html">nips2007-99</a> <a title="nips-2007-99-reference" href="#">nips2007-99-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>99 nips-2007-Hierarchical Penalization</h1>
<br/><p>Source: <a title="nips-2007-99-pdf" href="http://papers.nips.cc/paper/3338-hierarchical-penalization.pdf">pdf</a></p><p>Author: Marie Szafranski, Yves Grandvalet, Pierre Morizet-mahoudeaux</p><p>Abstract: Hierarchical penalization is a generic framework for incorporating prior information in the ﬁtting of statistical models, when the explicative variables are organized in a hierarchical structure. The penalizer is a convex functional that performs soft selection at the group level, and shrinks variables within each group. This favors solutions with few leading terms in the ﬁnal combination. The framework, originally derived for taking prior knowledge into account, is shown to be useful in linear regression, when several parameters are used to model the inﬂuence of one feature, or in kernel regression, for learning multiple kernels. Keywords – Optimization: constrained and convex optimization. Supervised learning: regression, kernel methods, sparsity and feature selection. 1</p><br/>
<h2>reference text</h2><p>[1] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B, 58(1):267–288, 1996.</p>
<p>[2] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004.</p>
<p>[3] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society. Series B, 68(1):49–67, 2006. 7  combined h1 =10−1 h3 =1 1  h4 =10 2  h5 =10  ν=10  ν=25  ν=50  Figure 3: Hierarchical penalization applied to kernel smoothing on the motorcycle data. Combined: the points represent data and the solid line the function of estimated responses. Isolated bandwidths: the points represent partial residuals and the solid line represents the contribution of the bandwidth to the model.</p>
<p>[4] Y. Grandvalet and S. Canu. Adaptive scaling for feature selection in SVMs. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2003.</p>
<p>[5] M. R. Osborne, B. Presnell, and B. A. Turlach. On the lasso and its dual. Journal of Computational and Graphical Statistics, 9(2):319–337, June 2000.</p>
<p>[6] C.L. Blake D.J. Newman, S. Hettich and C.J. Merz. UCI repository of machine learning databases, 1998. URL http://www.ics.uci.edu/˜mlearn/MLRepository.html.</p>
<p>[7] Delve: Data for evaluating learning in valid experiments. URL http://www.cs.toronto. edu/˜delve/.</p>
<p>[8] G. Lanckriet, T. De Bie, N. Cristianini, M. Jordan, and W. Noble. A statistical framework for genomic data fusion. Bioinformatics, 20:2626–2635, 2004.</p>
<p>[9] W. H¨ rdle. Applied Nonparametric Regression, volume 19. Economic Society Monographs, a 1990.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
