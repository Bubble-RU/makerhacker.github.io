<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2007-Stable Dual Dynamic Programming</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-185" href="../nips2007/nips-2007-Stable_Dual_Dynamic_Programming.html">nips2007-185</a> <a title="nips-2007-185-reference" href="#">nips2007-185-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2007-Stable Dual Dynamic Programming</h1>
<br/><p>Source: <a title="nips-2007-185-pdf" href="http://papers.nips.cc/paper/3179-stable-dual-dynamic-programming.pdf">pdf</a></p><p>Author: Tao Wang, Michael Bowling, Dale Schuurmans, Daniel J. Lizotte</p><p>Abstract: Recently, we have introduced a novel approach to dynamic programming and reinforcement learning that is based on maintaining explicit representations of stationary distributions instead of value functions. In this paper, we investigate the convergence properties of these dual algorithms both theoretically and empirically, and show how they can be scaled up by incorporating function approximation. 1</p><br/>
<h2>reference text</h2><p>[1] M. Puterman. Markov Decision Processes: Discrete Dynamic Programming. Wiley, 1994.</p>
<p>[2] D. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, 1995.</p>
<p>[3] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[4] T. Wang, M. Bowling, and D. Schuurmans. Dual representations for dynamic programming and reinforcement learning. In Proceeding of the IEEE International Symposium on ADPRL, pages 44–51, 2007.</p>
<p>[5] L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In International Conference on Machine Learning, pages 30–37, 1995.</p>
<p>[6] R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.</p>
<p>[7] J. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Trans. Automat. Control, 42(5):674–690, 1997.</p>
<p>[8] D. de Farias and B. Van Roy. On the existence of ﬁxed points for approximate value iteration and temporal-difference learning. J. Optimization Theory and Applic., 105(3):589–608, 2000.</p>
<p>[9] J. A. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the value function. In NIPS 7, pages 369–376, 1995.</p>
<p>[10] R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In Advances in Neural Information Processing Systems, pages 1038–1044, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
