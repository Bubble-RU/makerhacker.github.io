<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-168" href="../nips2007/nips-2007-Reinforcement_Learning_in_Continuous_Action_Spaces_through_Sequential_Monte_Carlo_Methods.html">nips2007-168</a> <a title="nips-2007-168-reference" href="#">nips2007-168-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>168 nips-2007-Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods</h1>
<br/><p>Source: <a title="nips-2007-168-pdf" href="http://papers.nips.cc/paper/3318-reinforcement-learning-in-continuous-action-spaces-through-sequential-monte-carlo-methods.pdf">pdf</a></p><p>Author: Alessandro Lazaric, Marcello Restelli, Andrea Bonarini</p><p>Abstract: Learning in real-world domains often requires to deal with continuous state and action spaces. Although many solutions have been proposed to apply Reinforcement Learning algorithms to continuous state problems, the same techniques can be hardly extended to continuous action spaces, where, besides the computation of a good approximation of the value function, a fast method for the identiﬁcation of the highest-valued action is needed. In this paper, we propose a novel actor-critic approach in which the policy of the actor is estimated through sequential Monte Carlo methods. The importance sampling step is performed on the basis of the values learned by the critic, while the resampling step modiﬁes the actor’s policy. The proposed approach has been empirically compared to other learning algorithms into several domains; in this paper, we report results obtained in a control problem consisting of steering a boat across a river. 1</p><br/>
<h2>reference text</h2><p>[1] M. Sanjeev Arulampalam, Simon Maskell, Neil Gordon, and Tim Clapp. A tutorial on particle ﬁlters for online nonlinear/non-gaussian bayesian tracking. IEEE Trans. on Signal Processing, 50(2):174–188, 2002.</p>
<p>[2] Leemon C. Baird and A. Harry Klopf. Reinforcement learning with high-dimensional, continuous actions. Technical Report WL-TR-93-117, Wright-Patterson Air Force Base Ohio: Wright Laboratory, 1993.</p>
<p>[3] D.P. Bertsekas and J.N. Tsitsiklis. Neural Dynamic Programming. Athena Scientiﬁc, Belmont, MA, 1996.</p>
<p>[4] Chris Gaskett, David Wettergreen, and Alexander Zelinsky. Q-learning in continuous state and action spaces. In Australian Joint Conference on Artiﬁcial Intelligence, pages 417–428, 2003.</p>
<p>[5] L. Jouffe. Fuzzy inference system learning by reinforcement methods. IEEE Trans. on Systems, Man, and Cybernetics-PART C, 28(3):338–355, 1998.</p>
<p>[6] H. Kimura and S. Kobayashi. Reinforcement learning for continuous action using stochastic gradient ascent. In 5th Intl. Conf. on Intelligent Autonomous Systems, pages 288–295, 1998.</p>
<p>[7] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. SIAM Journal on Control and Optimization, 42(4):1143–1166, 2003.</p>
<p>[8] J. S. Liu and E. Chen. Sequential monte carlo methods for dynamical systems. Journal of American Statistical Association, 93:1032–1044, 1998.</p>
<p>[9] Jose Del R. Millan, Daniele Posenato, and Eric Dedieu. Continuous-action q-learning. Machine Learning, 49:247–265, 2002.</p>
<p>[10] Jan Peters and Stefen Schaal. Policy gradient methods for robotics. In Proceedings of the IEEE International Conference on Intelligent Robotics Systems (IROS), pages 2219–2225, 2006.</p>
<p>[11] J. C. Santamaria, R. S: Sutton, and A. Ram. Experiments with reinforcement learning in problems with continuous state and action spaces. Adaptive Behavior, 6:163–217, 1998.</p>
<p>[12] Alexander A. Sherstov and Peter Stone. Function approximation via tile coding: Automating parameter choice. In SARA 2005, LNAI, pages 194–205. Springer Verlag, 2005.</p>
<p>[13] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[14] Hado van Hasselt and Marco Wiering. Reinforcement learning in continuous action spaces. In 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning, pages 272–279, 2007.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
