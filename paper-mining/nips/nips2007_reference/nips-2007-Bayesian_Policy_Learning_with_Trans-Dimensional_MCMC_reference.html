<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-34" href="../nips2007/nips-2007-Bayesian_Policy_Learning_with_Trans-Dimensional_MCMC.html">nips2007-34</a> <a title="nips-2007-34-reference" href="#">nips2007-34-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>34 nips-2007-Bayesian Policy Learning with Trans-Dimensional MCMC</h1>
<br/><p>Source: <a title="nips-2007-34-pdf" href="http://papers.nips.cc/paper/3343-bayesian-policy-learning-with-trans-dimensional-mcmc.pdf">pdf</a></p><p>Author: Matthew Hoffman, Arnaud Doucet, Nando D. Freitas, Ajay Jasra</p><p>Abstract: A recently proposed formulation of the stochastic planning and control problem as one of parameter estimation for suitable artiﬁcial statistical models has led to the adoption of inference algorithms for this notoriously hard problem. At the algorithmic level, the focus has been on developing Expectation-Maximization (EM) algorithms. In this paper, we begin by making the crucial observation that the stochastic control problem can be reinterpreted as one of trans-dimensional inference. With this new interpretation, we are able to propose a novel reversible jump Markov chain Monte Carlo (MCMC) algorithm that is more efﬁcient than its EM counterparts. Moreover, it enables us to implement full Bayesian policy search, without the need for gradients and with one single Markov chain. The new approach involves sampling directly from a distribution that is proportional to the reward and, consequently, performs better than classic simulations methods in situations where the reward is a rare event.</p><br/>
<h2>reference text</h2><p>[1] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan. An introduction to MCMC for machine learning. Machine Learning, 50:5–43, 2003.</p>
<p>[2] H. Attias. Planning by probabilistic inference. In Uncertainty in Artiﬁcial Intelligence, 2003.</p>
<p>[3] J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001.</p>
<p>[4] D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc, 1995.</p>
<p>[5] P. Dayan and G. E. Hinton. Using EM for reinforcement learning. Neural Computation, 9:271–278, 1997.  7  Evolution of policy parameters against transition-model samples  policy parameter (theta)  0.8 0.7  rjmdp pegasus optimal  0.6 0.5 0.4 0.3 0.20  1000  2000 3000 4000 5000 6000 7000 8000 number of samples taken from transition-model  9000  Figure 4: Convergence of PEGASUS and our Bayesian policy search algorithm when started from θ = 0 and converging to the optimum of θ∗ = π/4. The plots are averaged over 10 runs. For our algorithm we plot samples taken directly from the MCMC algorithm itself: plotting the empirical average would produce an estimate whose convergence is almost immediate, but we also wanted to show the “burn-in” period. For both algorithms lines denoting one standard deviation are shown and performance is plotted against the number of samples taken from the transition model.</p>
<p>[6] A. Doucet and V. B. Tadic. On solving integral equations using Markov chain Monte Carlo methods. Technical Report CUED-F-INFENG 444, Cambridge University Engineering Department, 2004.</p>
<p>[7] P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, 82:711–732, 1995.</p>
<p>[8] P. J. Green. Trans-dimensional Markov chain Monte Carlo. In Highly Structured Stochastic Systems, 2003.</p>
<p>[9] M. Klaas, M. Briers, N. de Freitas, A. Doucet, and S. Maskell. Fast particle smoothing: If i had a million particles. In International Conference on Machine Learning, 2006.</p>
<p>[10] G. Lawrence, N. Cowan, and S. Russell. Efﬁcient gradient estimation for motor control learning. In Uncertainty in Artiﬁcial Intelligence, pages 354–36, 2003.</p>
<p>[11] P. M¨ ller. Simulation based optimal design. Bayesian Statistics, 6, 1999. u</p>
<p>[12] P. M¨ ller, B. Sans´ , and M. De Iorio. Optimal Bayesian design by inhomogeneous Markov chain simuu o lation. J. American Stat. Assoc., 99:788–798, 2004.</p>
<p>[13] A. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger, and E. Liang. Inverted autonomous helicopter ﬂight via reinforcement learning. In International Symposium on Experimental Robotics, 2004.</p>
<p>[14] A. Y. Ng and M. I. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs. In Uncertainty in Artiﬁcial Intelligence, 2000.</p>
<p>[15] J. Peters and S. Schaal. Policy gradient methods for robotics. In IEEE International Conference on Intelligent Robotics Systems, 2006.</p>
<p>[16] M. Porta, N. Vlassis, M. T. J. Spaan, and P. Poupart. Point-based value iteration for continuous POMDPs. Journal of Machine Learning Research, 7:2329–2367, 2006.</p>
<p>[17] S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number of components. Journal of the Royal Statistical Society B, 59(4):731–792, 1997.</p>
<p>[18] S. Thrun. Monte Carlo POMDPs. In S. Solla, T. Leen, and K.-R. M¨ ller, editors, Neural Information u Processing Systems, pages 1064–1070. MIT Press, 2000.</p>
<p>[19] M. Toussaint, S. Harmeling, and A. Storkey. Probabilistic inference for solving (PO)MDPs. Technical Report EDI-INF-RR-0934, University of Edinburgh, School of Informatics, 2006.</p>
<p>[20] M. Toussaint and A. Storkey. Probabilistic inference for solving discrete and continuous state Markov decision processes. In International Conference on Machine Learning, 2006.</p>
<p>[21] D. Verma and R. P. N. Rao. Planning and acting in uncertain environments using probabilistic inference. In IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, 2006.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
