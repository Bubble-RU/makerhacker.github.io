<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2007-Infinite State Bayes-Nets for Structured Domains</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2007" href="../home/nips2007_home.html">nips2007</a> <a title="nips-2007-105" href="../nips2007/nips-2007-Infinite_State_Bayes-Nets_for_Structured_Domains.html">nips2007-105</a> <a title="nips-2007-105-reference" href="#">nips2007-105-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2007-Infinite State Bayes-Nets for Structured Domains</h1>
<br/><p>Source: <a title="nips-2007-105-pdf" href="http://papers.nips.cc/paper/3310-infinite-state-bayes-nets-for-structured-domains.pdf">pdf</a></p><p>Author: Max Welling, Ian Porteous, Evgeniy Bart</p><p>Abstract: A general modeling framework is proposed that uniﬁes nonparametric-Bayesian models, topic-models and Bayesian networks. This class of inﬁnite state Bayes nets (ISBN) can be viewed as directed networks of ‘hierarchical Dirichlet processes’ (HDPs) where the domain of the variables can be structured (e.g. words in documents or features in images). We show that collapsed Gibbs sampling can be done efﬁciently in these models by leveraging the structure of the Bayes net and using the forward-ﬁltering-backward-sampling algorithm for junction trees. Existing models, such as nested-DP, Pachinko allocation, mixed membership stochastic block models as well as a number of new models are described as ISBNs. Two experiments have been performed to illustrate these ideas. 1</p><br/>
<h2>reference text</h2><p>[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[2] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. To appear in Journal of the American Statistical Association, 2006.</p>
<p>[3] S. L. Scott. Bayesian methods for hidden Markov models, recursive computing in the 21st century. volume 97, pages 337–351, 2002.</p>
<p>[4] T. Minka. Estimating a dirichlet distribution. Technical report, 2000.</p>
<p>[5] M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90:577–588, 1995.</p>
<p>[6] T.L. Grifﬁths and M. Steyvers. A probabilistic approach to semantic representation. In Proceedings of the 24th Annual Conference of the Cognitive Science Society, 2002.</p>
<p>[7] Y.W. Teh, D. Newman, and M. Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. In NIPS, volume 19, 2006.</p>
<p>[8] C.E. Antoniak. Mixtures of Dirichlet processes with applications to bayesian nonparametric problems. The Annals of Statistics, 2:1152–1174, 1974.</p>
<p>[9] B. Bidyuk and R. Dechter. Cycle-cutset sampling for Bayesian networks. In Sixteenth Canadian Conf. on AI, 2003.</p>
<p>[10] David Blei, Thomas L. Grifﬁths, Michael I. Jordan, and Joshua B. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In Neural Information Processing Systems 16, 2004.</p>
<p>[11] B. Marlin. Modeling user rating proﬁles for collaborative ﬁltering. In Advances in Neural Information Processing Systems 16. 2004.</p>
<p>[12] S. Kim and P. Smyth. Hierarchical dirichlet processes with random effects. In NIPS, volume 19, 2006.</p>
<p>[13] W. Li and A. McCallum. Pachinko allocation: Dag-structured mixture models of topic correlations. In Proceedings of the 23rd international conference on Machine learning, pages 577–584, 2006.</p>
<p>[14] W. Li, A. McCallum, and D. Blei. Nonparametric bayes pachinko allocation. In UAI, 2007.</p>
<p>[15] D. Larlus and F. Jurie. Latent mixture vocabularies for object categorization. In British Machine Vision Conference, 2006.</p>
<p>[16] E. Airoldi, D. Blei, E. Xing, and S. Fienberg. A latent mixed membership model for relational data. In LinkKDD ’05: Proceedings of the 3rd international workshop on Link discovery, pages 82–89, 2005.</p>
<p>[17] R. Agrawal, T. Imielinski, and A. Swami. Mining associations between sets of items in massive databases. In Proc. of the ACM-SIGMOD 1993 Intl Conf on Management of Data, 1993.</p>
<p>[18] M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. The inﬁnite hidden markov model. In NIPS, pages 577–584, 2001.</p>
<p>[19] Y. W. Teh, D. G¨ r¨ r, and Z. Ghahramani. Stick-breaking construction for the Indian buffet process. In ou Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics, volume 11, 2007.</p>
<p>[20] W. Li D. Mimno and A. McCallum. Mixtures of hierarchical topics with pachinko allocation. In Proceedings of the 21st International Conference on Machine Learning, 2007.  8</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
