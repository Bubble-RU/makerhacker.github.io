<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-7" href="../nips2004/nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">nips2004-7</a> <a title="nips-2004-7-reference" href="#">nips2004-7-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</h1>
<br/><p>Source: <a title="nips-2004-7-pdf" href="http://papers.nips.cc/paper/2544-a-large-deviation-bound-for-the-area-under-the-roc-curve.pdf">pdf</a></p><p>Author: Shivani Agarwal, Thore Graepel, Ralf Herbrich, Dan Roth</p><p>Abstract: The area under the ROC curve (AUC) has been advocated as an evaluation criterion for the bipartite ranking problem. We study large deviation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an independent test sequence. A comparison of our result with a corresponding large deviation result for the classiﬁcation error rate suggests that the test sample size required to obtain an -accurate estimate of the expected accuracy of a ranking function with δ-conﬁdence is larger than that required to obtain an -accurate estimate of the expected error rate of a classiﬁcation function with the same conﬁdence. A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from ﬁnite function classes. 1</p><br/>
<h2>reference text</h2><p>[1] S. Agarwal, S. Har-Peled, and D. Roth. A uniform convergence bound for the area under the ROC curve. In Proceedings of the 10th International Workshop on Artiﬁcial Intelligence and Statistics, 2005.</p>
<p>[2] W. W. Cohen, R. E. Schapire, and Y. Singer. Learning to order things. Journal of Artiﬁcial Intelligence Research, 10:243–270, 1999.</p>
<p>[3] C. Cortes and M. Mohri. AUC optimization vs. error rate minimization. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing Systems 16, 2004. o</p>
<p>[4] K. Crammer and Y. Singer. Pranking with ranking. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, 2002.</p>
<p>[5] V. H. de la P˜ na and E. Gin´ . Decoupling: From Dependence to Independence. Springer-Verlag, e e New York, 1999.</p>
<p>[6] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springero Verlag, New York, 1996.</p>
<p>[7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003.</p>
<p>[8] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. Advances in Large Margin Classiﬁers, pages 115–132, 2000.</p>
<p>[9] S. I. Hill, H. Zaragoza, R. Herbrich, and P. J. W. Rayner. Average precision and the problem of generalisation. In Proceedings of the ACM SIGIR Workshop on Mathematical and Formal Methods in Information Retrieval, 2002.</p>
<p>[10] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics 1989, pages 148–188. Cambridge University Press, 1989.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
