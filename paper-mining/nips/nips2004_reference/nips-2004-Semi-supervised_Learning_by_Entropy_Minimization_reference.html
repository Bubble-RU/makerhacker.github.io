<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 nips-2004-Semi-supervised Learning by Entropy Minimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-164" href="../nips2004/nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">nips2004-164</a> <a title="nips-2004-164-reference" href="#">nips2004-164-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 nips-2004-Semi-supervised Learning by Entropy Minimization</h1>
<br/><p>Source: <a title="nips-2004-164-pdf" href="http://papers.nips.cc/paper/2740-semi-supervised-learning-by-entropy-minimization.pdf">pdf</a></p><p>Author: Yves Grandvalet, Yoshua Bengio</p><p>Abstract: We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution beneﬁts from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are deﬁnitely in favor of minimum entropy regularization when generative models are misspeciﬁed, and the weighting of unlabeled data provides robustness to the violation of the “cluster assumption”. Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces. 1</p><br/>
<h2>reference text</h2><p>[1] M. R. Amini and P. Gallinari. Semi-supervised logistic regression. In 15th European Conference on Artiﬁcial Intelligence, pages 390–394. IOS Press, 2002.</p>
<p>[2] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, New York, 2 edition, 1985.</p>
<p>[3] M. Brand. Structure learning in conditional probability models via an entropic prior and parameter extinction. Neural Computation, 11(5):1155–1182, 1999.</p>
<p>[4] V. Castelli and T. M. Cover. The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter. IEEE Trans. on Information Theory, 42(6):2102–2117, 1996.</p>
<p>[5] Y. Grandvalet. Logistic regression for partial labels. In 9th Information Processing and Management of Uncertainty in Knowledge-based Systems – IPMU’02, pages 1935–1941, 2002.</p>
<p>[6] G. J. McLachlan. Discriminant analysis and statistical pattern recognition. Wiley, 1992.</p>
<p>[7] K. Nigam and R. Ghani. Analyzing the effectiveness and applicability of co-training. In Ninth International Conference on Information and Knowledge Management, pages 86–93, 2000.</p>
<p>[8] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text classiﬁcation from labeled and unlabeled documents using EM. Machine learning, 39(2/3):135–167, 2000.</p>
<p>[9] T. J. O’Neill. Normal discrimination with unclassiﬁed observations. Journal of the American Statistical Association, 73(364):821–826, 1978.</p>
<p>[10] M. Seeger. Learning with labeled and unlabeled data. Technical report, Institute for Adaptive and Neural Computation, University of Edinburgh, 2002.</p>
<p>[11] M. Szummer and T. S. Jaakkola. Information regularization with partially labeled data. In Advances in Neural Information Processing Systems 15. MIT Press, 2003.</p>
<p>[12] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, and B. Sch¨ lkopf. Learning with local and o global consistency. In Advances in Neural Information Processing Systems 16, 2004.</p>
<p>[13] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In 20th Int. Conf. on Machine Learning, pages 912–919, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
