<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 nips-2004-Maximum Margin Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-115" href="../nips2004/nips-2004-Maximum_Margin_Clustering.html">nips2004-115</a> <a title="nips-2004-115-reference" href="#">nips2004-115-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>115 nips-2004-Maximum Margin Clustering</h1>
<br/><p>Source: <a title="nips-2004-115-pdf" href="http://papers.nips.cc/paper/2602-maximum-margin-clustering.pdf">pdf</a></p><p>Author: Linli Xu, James Neufeld, Bryce Larson, Dale Schuurmans</p><p>Abstract: We propose a new method for clustering based on ﬁnding maximum margin hyperplanes through data. By reformulating the problem in terms of the implied equivalence relation matrix, we can pose the problem as a convex integer program. Although this still yields a difﬁcult computational problem, the hard-clustering constraints can be relaxed to a soft-clustering formulation which can be feasibly solved with a semidefinite program. Since our clustering technique only depends on the data through the kernel matrix, we can easily achieve nonlinear clusterings in the same manner as spectral clustering. Experimental results show that our maximum margin clustering technique often obtains more accurate results than conventional clustering methods. The real beneﬁt of our approach, however, is that it leads naturally to a semi-supervised training method for support vector machines. By maximizing the margin simultaneously on labeled and unlabeled training data, we achieve state of the art performance by using a single, integrated learning principle. 1</p><br/>
<h2>reference text</h2><p>[1] A. Ben-Hur, D. Horn, H. Siegelman, and V. Vapnik. Support vector clustering. In Journal of Machine Learning Research 2 (2001), 2001.</p>
<p>[2] K. Bennett and A. Demiriz. Semi-supervised support vector machines. In Advances in Neural Information Processing Systems 11 (NIPS-98), 1998.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge U. Press, 2004.</p>
<p>[4] Chakra Chennubhotla and Allan Jepson. Eigencuts: Half-lives of eigenﬂows for spectral clustering. In In Advances in Neural Information Processing Systems, 2002, 2002.</p>
<p>[5] K. Crammer and Y. Singer. On the algorithmic interpretation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2, 2001.</p>
<p>[6] T. De Bie and N. Cristianini. Convex methods for transduction. In Advances in Neural Information Processing Systems 16 (NIPS-03), 2003.</p>
<p>[7] C. Helmberg. Semideﬁnite programming for combinatorial optimization. Technical Report ZIB-Report ZR-00-34, Konrad-Zuse-Zentrum Berlin, 2000.</p>
<p>[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In International Conference on Machine Learning (ICML-99), 1999.</p>
<p>[9] Y. Kluger, R. Basri, J. Chang, and M. Gerstein. Spectral biclustering of microarray cancer data: co-clustering genes and conditions. Genome Research, 13, 2003.</p>
<p>[10] G. Lanckriet, N. Cristianini, P. Bartlett, L Ghaoui, and M. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research, 5, 2004.</p>
<p>[11] M. Laurent and S. Poljak. On a positive semideﬁnite relaxation of the cut polytope. Linear Algebra and its Applications, 223/224, 1995.</p>
<p>[12] S. Chawla N. Bansal, A. Blum. Correlation clustering. In Conference on Foundations of Computer Science (FOCS-02), 2002.</p>
<p>[13] J. Kandola N. Cristianini, J. Shawe-Taylor. Spectral kernel methods for clustering. In In Advances in Neural Information Processing System, 2001, 2001.</p>
<p>[14] A. Ng, M. Jordan, and Y Weiss. On spectral clustering: analysis and an algorithm. In Advances in Neural Information Processing Systems 14 (NIPS-01), 2001.</p>
<p>[15] B. Schoelkopf and A. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2002.</p>
<p>[16] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans PAMI, 22(8), 2000.</p>
<p>[17] Y. Weiss. Segmentation using eigenvectors: a unifying view. In International Conference on Computer Vision (ICCV-99), 1999.</p>
<p>[18] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In International Conference on Machine Learning (ICML-03), 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
