<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-188" href="../nips2004/nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">nips2004-188</a> <a title="nips-2004-188-reference" href="#">nips2004-188-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</h1>
<br/><p>Source: <a title="nips-2004-188-pdf" href="http://papers.nips.cc/paper/2685-the-laplacian-pdf-distance-a-cost-function-for-clustering-in-a-kernel-feature-space.pdf">pdf</a></p><p>Author: Robert Jenssen, Deniz Erdogmus, Jose Principe, Torbjørn Eltoft</p><p>Abstract: A new distance measure between probability density functions (pdfs) is introduced, which we refer to as the Laplacian pdf distance. The Laplacian pdf distance exhibits a remarkable connection to Mercer kernel based learning theory via the Parzen window technique for density estimation. In a kernel feature space deﬁned by the eigenspectrum of the Laplacian data matrix, this pdf distance is shown to measure the cosine of the angle between cluster mean vectors. The Laplacian data matrix, and hence its eigenspectrum, can be obtained automatically based on the data at hand, by optimal Parzen window selection. We show that the Laplacian pdf distance has an interesting interpretation as a risk function connected to the probability of error. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Weiss, “Segmentation Using Eigenvectors: A Unifying View,” in International Conference on Computer Vision, 1999, pp. 975–982.</p>
<p>[2] A. Y. Ng, M. Jordan, and Y. Weiss, “On Spectral Clustering: Analysis and an Algorithm,” in Advances in Neural Information Processing Systems, 14, 2001, vol. 2, pp. 849–856.</p>
<p>[3] K. R. M¨ller, S. Mika, G. R¨tsch, K. Tsuda, and B. Sch¨lkopf, “An Introduction u a o to Kernel-Based Learning Algorithms,” IEEE Transactions on Neural Networks, vol. 12, no. 2, pp. 181–201, 2001.</p>
<p>[4] J. Mercer, “Functions of Positive and Negative Type and their Connection with the Theory of Integral Equations,” Philos. Trans. Roy. Soc. London, vol. A, pp. 415–446, 1909.</p>
<p>[5] C. Williams and M. Seeger, “Using the Nystr¨m Method to Speed Up Kernel o Machines,” in Advances in Neural Information Processing Systems 13, Vancouver, Canada, USA, 2001, pp. 682–688.</p>
<p>[6] M. Brand and K. Huang, “A Unifying Theorem for Spectral Embedding and Clustering,” in Ninth Int’l Workshop on Artiﬁcial Intelligence and Statistics, Key West, Florida, USA, 2003.</p>
<p>[7] E. Parzen, “On the Estimation of a Probability Density Function and the Mode,” Ann. Math. Stat., vol. 32, pp. 1065–1076, 1962.</p>
<p>[8] B. W. Silverman, Density Estimation for Statistics and Data Analysis, Chapman and Hall, London, 1986.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
