<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2004-Neighbourhood Components Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-127" href="../nips2004/nips-2004-Neighbourhood_Components_Analysis.html">nips2004-127</a> <a title="nips-2004-127-reference" href="#">nips2004-127-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 nips-2004-Neighbourhood Components Analysis</h1>
<br/><p>Source: <a title="nips-2004-127-pdf" href="http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf">pdf</a></p><p>Author: Jacob Goldberger, Geoffrey E. Hinton, Sam T. Roweis, Ruslan Salakhutdinov</p><p>Abstract: In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classiﬁcation algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classiﬁcation. Unlike other methods, our classiﬁcation model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction. 1</p><br/>
<h2>reference text</h2><p>[1] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learning distance functions using equivalence relation. In International Conference on Machine Learning, 2003.</p>
<p>[2] L. Chen, H. Liao, M. Ko, J. Lin, and G. Yu. A new lda-based face recognition system which can solve the small sample size problem. In Pattern Recognition, pages 1713–1726, 2000.</p>
<p>[3] R. A. Fisher. The use of multiple measurements in taxonomic problems. In Annual of Eugenic, pages 179–188, 1936.</p>
<p>[4] J. Friedman, J.bentley, and R. Finkel. An algorithm for ﬁnding best matches in logarithmic expected time. In ACM, 1977.</p>
<p>[5] Y. Koren and L. Carmel. Robust linear dimensionality reduction. In IEEE Trans. Vis. and Comp. Graph., pages 459–470, 2004.</p>
<p>[6] D. Lowe. Similarity metric learning for a variable kernel classiﬁer. In Neural Computation, pages 72–85, 1995.</p>
<p>[7] E.P. Xing, A. Y. Ng, M.I. Jordan, and S. Russell. Distance learning metric. In Proc. of Neural Information Processing Systems, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
