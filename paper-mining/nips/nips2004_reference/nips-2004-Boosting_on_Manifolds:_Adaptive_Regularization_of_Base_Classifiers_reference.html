<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 nips-2004-Boosting on Manifolds: Adaptive Regularization of Base Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-32" href="../nips2004/nips-2004-Boosting_on_Manifolds%3A_Adaptive_Regularization_of_Base_Classifiers.html">nips2004-32</a> <a title="nips-2004-32-reference" href="#">nips2004-32-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>32 nips-2004-Boosting on Manifolds: Adaptive Regularization of Base Classifiers</h1>
<br/><p>Source: <a title="nips-2004-32-pdf" href="http://papers.nips.cc/paper/2613-boosting-on-manifolds-adaptive-regularization-of-base-classifiers.pdf">pdf</a></p><p>Author: Ligen Wang, Balázs Kégl</p><p>Abstract: In this paper we propose to combine two powerful ideas, boosting and manifold learning. On the one hand, we improve A DA B OOST by incorporating knowledge on the structure of the data into base classiﬁer design and selection. On the other hand, we use A DA B OOST’s efﬁcient learning mechanism to signiﬁcantly improve supervised and semi-supervised algorithms proposed in the context of manifold learning. Beside the speciﬁc manifold-based penalization, the resulting algorithm also accommodates the boosting of a large family of regularized learning algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line learning and an application to boosting,” Journal of Computer and System Sciences, vol. 55, pp. 119–139, 1997.</p>
<p>[2] L. Mason, P. Bartlett, J. Baxter, and M. Frean, “Boosting algorithms as gradient descent,” in Advances in Neural Information Processing Systems. 2000, vol. 12, pp. 512–518, The MIT Press.</p>
<p>[3] G. R¨ tsch, T. Onoda, and K.-R. M¨ ller, “Soft margins for AdaBoost,” Machine Learning, vol. a u 42, no. 3, pp. 287–320, 2001.</p>
<p>[4] M. Belkin and P. Niyogi, “Semi-supervised learning on Riemannian manifolds,” Machine Learning, to appear, 2004.</p>
<p>[5] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on Pettern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888–905, 2000.</p>
<p>[6] G. R¨ tsch and M. K. Warmuth, “Maximizing the margin with boosting,” in Proceedings of the a 15th Conference on Computational Learning Theory, 2002.</p>
<p>[7] L. Breiman, “Prediction games and arcing classiﬁers,” Neural Computation, vol. 11, pp. 1493– 1518, 1999.</p>
<p>[8] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee, “Boosting the margin: a new explanation for the effectiveness of voting methods,” Annals of Statistics, vol. 26, no. 5, pp. 1651–1686, 1998.</p>
<p>[9] A. Antos, B. K´ gl, T. Linder, and G. Lugosi, “Data-dependent margin-based generalization e bounds for classiﬁcation,” Journal of Machine Learning Research, pp. 73–98, 2002.</p>
<p>[10] R. E. Schapire and Y. Singer, “Improved boosting algorithms using conﬁdence-rated predictions,” Machine Learning, vol. 37, no. 3, pp. 297–336, 1999.</p>
<p>[11] B. K´ gl, “Robust regression by boosting the median,” in Proceedings of the 16th Conference e on Computational Learning Theory, Washington, D.C., 2003, pp. 258–272.</p>
<p>[12] M. Belkin, I. Matveeva, and P. Niyogi, “Regression and regularization on large graphs,” in Proceedings of the 17th Conference on Computational Learning Theory, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
