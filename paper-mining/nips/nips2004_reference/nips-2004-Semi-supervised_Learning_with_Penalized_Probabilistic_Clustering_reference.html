<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-167" href="../nips2004/nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">nips2004-167</a> <a title="nips-2004-167-reference" href="#">nips2004-167-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</h1>
<br/><p>Source: <a title="nips-2004-167-pdf" href="http://papers.nips.cc/paper/2610-semi-supervised-learning-with-penalized-probabilistic-clustering.pdf">pdf</a></p><p>Author: Zhengdong Lu, Todd K. Leen</p><p>Abstract: While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. We express clustering preferences in the prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. We ﬁt the model parameters with EM. Experiments on a variety of data sets show that PPC can consistently improve clustering results.</p><br/>
<h2>reference text</h2><p>[1] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. Constrained K-means clustering with background knowledge. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 577–584, 2001.</p>
<p>[2] S. Basu, A. Bannerjee, and R. Mooney. Semi-supervised clustering by seeding. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 19–26, 2002.</p>
<p>[3] D. Klein, S. Kamvar, and C. Manning. From instance Level to space-level constraints: making the most of prior knowledge in data clustering. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 307–313, 2002.</p>
<p>[4] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. Computing Gaussian mixture models with EM using equivalence constraints. In Advances in Neural Information Processing System, volume 15, 2003.</p>
<p>[5] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.</p>
<p>[6] R. Neal. Probabilistic inference using Markov Chain Monte Carlo methods. Technical Report CRG-TR-93-1, Computer Science Department, Toronto University, 1993.</p>
<p>[7] C. Bouman and M. Shapiro. A multiscale random ﬁeld model for Bayesian image segmentation. IEEE Trans. Image Processing, 3:162–177, March 1994.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
