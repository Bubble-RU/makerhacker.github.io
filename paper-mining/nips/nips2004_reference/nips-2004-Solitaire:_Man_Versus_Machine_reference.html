<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2004-Solitaire: Man Versus Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-171" href="../nips2004/nips-2004-Solitaire%3A_Man_Versus_Machine.html">nips2004-171</a> <a title="nips-2004-171-reference" href="#">nips2004-171-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>171 nips-2004-Solitaire: Man Versus Machine</h1>
<br/><p>Source: <a title="nips-2004-171-pdf" href="http://papers.nips.cc/paper/2568-solitaire-man-versus-machine.pdf">pdf</a></p><p>Author: Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy</p><p>Abstract: In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 1</p><br/>
<h2>reference text</h2><p>[1] R. Bellman. Applied Dynamic Programming. Princeton University Press, 1957.</p>
<p>[2] D. Bertsekas and J.N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[3] D. P. Bertsekas, J. N. Tsitsiklis, and C. Wu, Rollout Algorithms for Combinatorial Optimization. Journal of Heuristics, 3:245-262, 1997.</p>
<p>[4] D. P. Bertsekas and D. A. Casta˜ on. Rollout Algorithms for Stochastic Scheduling n Problems. Journal of Heuristics, 5:89-108, 1999.</p>
<p>[5] D. Bertsimas and R. Demir. An Approximate Dynamic Programming Approach to Multi-dimensional Knapsack Problems. Management Science, 4:550-565, 2002.</p>
<p>[6] D. Bertsimas and I. Popescu. Revenue Management in a Dynamic Network Environment. Transportation Science, 37:257-277, 2003.</p>
<p>[7] R. Howard. Dynamic Programming and Markov Processes. MIT Press, 1960.</p>
<p>[8] A. McGovern, E. Moss, and A. Barto. Building a Basic Block Instruction Scheduler Using Reinforcement Learning and Rollouts. Machine Learning, 49:141-160, 2002.</p>
<p>[9] Y. Mansour and S. Singh. On the Complexity of Policy Iteration. In Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, 1999.</p>
<p>[10] D. Parlett. A History of Card Games. Oxford University Press, 1991.</p>
<p>[11] N. Secomandi. Analysis of a Rollout Approach to Sequencing Problems with Stochastic Routing Applications. Journal of Heuristics, 9:321-352, 2003.</p>
<p>[12] N. Secomandi. A Rollout Policy for the Vehicle Routing Problem with Stochastic Demands. Operations Research, 49:796-802, 2001.</p>
<p>[13] G. Tesauro and G. Galperin. On-line Policy Improvement Using Monte-Carlo Search. In Advances in Neural Information Processing Systems, 9:1068-1074, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
