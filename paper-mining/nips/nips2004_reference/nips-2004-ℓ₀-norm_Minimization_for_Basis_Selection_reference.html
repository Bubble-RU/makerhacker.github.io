<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>207 nips-2004-ℓ₀-norm Minimization for Basis Selection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-207" href="../nips2004/nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">nips2004-207</a> <a title="nips-2004-207-reference" href="#">nips2004-207-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>207 nips-2004-ℓ₀-norm Minimization for Basis Selection</h1>
<br/><p>Source: <a title="nips-2004-207-pdf" href="http://papers.nips.cc/paper/2726-l0-norm-minimization-for-basis-selection.pdf">pdf</a></p><p>Author: David P. Wipf, Bhaskar D. Rao</p><p>Abstract: Finding the sparsest, or minimum ℓ0 -norm, representation of a signal given an overcomplete dictionary of basis vectors is an important problem in many application domains. Unfortunately, the required optimization problem is often intractable because there is a combinatorial increase in the number of local minima as the number of candidate basis vectors increases. This deﬁciency has prompted most researchers to instead minimize surrogate measures, such as the ℓ1 -norm, that lead to more tractable computational methods. The downside of this procedure is that we have now introduced a mismatch between our ultimate goal and our objective function. In this paper, we demonstrate a sparse Bayesian learning-based method of minimizing the ℓ0 -norm while reducing the number of troublesome local minima. Moreover, we derive necessary conditions for local minima to occur via this approach and empirically demonstrate that there are typically many fewer for general problems of interest. 1</p><br/>
<h2>reference text</h2><p>[1] S.S. Chen, D.L. Donoho, and M.A. Saunders, “Atomic decomposition by basis pursuit,” SIAM Journal on Scientiﬁc Computing, vol. 20, no. 1, pp. 33–61, 1999.</p>
<p>[2] B.D. Rao and K. Kreutz-Delgado, “An afﬁne scaling methodology for best basis selection,” IEEE Transactions on Signal Processing, vol. 47, no. 1, pp. 187–200, January 1999.</p>
<p>[3] R.M. Leahy and B.D. Jeffs, “On the design of maximally sparse beamforming arrays,” IEEE Transactions on Antennas and Propagation, vol. 39, no. 8, pp. 1178–1187, Aug. 1991.</p>
<p>[4] I. F. Gorodnitsky and B. D. Rao, “Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm,” IEEE Transactions on Signal Processing, vol. 45, no. 3, pp. 600–616, March 1997.</p>
<p>[5] M.A.T. Figueiredo, “Adaptive sparseness using Jeffreys prior,” Neural Information Processing Systems, vol. 14, pp. 697–704, 2002.</p>
<p>[6] D.P. Wipf and B.D. Rao, “Sparse Bayesian learning for basis selection,” IEEE Transactions on Signal Processing, vol. 52, no. 8, pp. 2153–2164, 2004.</p>
<p>[7] D.L. Donoho and M. Elad, “Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization,” Proc. National Academy of Sciences, vol. 100, no. 5, pp. 2197–2202, March 2003.</p>
<p>[8] M.E. Tipping, “Sparse Bayesian learning and the relevance vector machine,” Journal of Machine Learning Research, vol. 1, pp. 211–244, 2001.</p>
<p>[9] D.P. Wipf and B.D. Rao, “Some results on sparse Bayesian learning,” ECE Department Technical Report, University of California, San Diego, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
