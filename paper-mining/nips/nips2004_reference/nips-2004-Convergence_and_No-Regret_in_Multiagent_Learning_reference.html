<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2004-Convergence and No-Regret in Multiagent Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-48" href="../nips2004/nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">nips2004-48</a> <a title="nips-2004-48-reference" href="#">nips2004-48-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>48 nips-2004-Convergence and No-Regret in Multiagent Learning</h1>
<br/><p>Source: <a title="nips-2004-48-pdf" href="http://papers.nips.cc/paper/2673-convergence-and-no-regret-in-multiagent-learning.pdf">pdf</a></p><p>Author: Michael Bowling</p><p>Abstract: Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identiﬁable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). 1</p><br/>
<h2>reference text</h2><p>[1] Michael Bowling. Convergence and no-regret in multiagent learning. Technical Report TR04-11, Department of Computing Science, University of Alberta, 2004.</p>
<p>[2] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the Eleventh International Conference on Machine Learning, pages 157–163, 1994.</p>
<p>[3] Junling Hu and Michael P. Wellman. Multiagent reinforcement learning: Theoretical framework and an algorithm. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 242–250, 1998.</p>
<p>[4] Amy Greenwald and Keith Hall. Correlated Q-learning. In Proceedings of the AAAI Spring Symposium Workshop on Collaborative Learning Agents, 2002.</p>
<p>[5] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. In Proceedings of the Fifteenth National Conference on Artiﬁcial Intelligence, pages 746–752, 1998.</p>
<p>[6] Satinder Singh, Michael Kearns, and Yishay Mansour. Nash convergence of gradient dynamics in general-sum games. In Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 541–548, 2000.</p>
<p>[7] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artiﬁcial Intelligence, 136:215–250, 2002.</p>
<p>[8] Yu-Han Chang and Leslie Pack Kaelbling. Playing is believing: the role of beliefs in multi-agent learning. In Advances in Neural Information Processing Systems 14, 2001.</p>
<p>[9] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68:1127–1150, 2000.</p>
<p>[10] Peter Auer, Nicol` Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. Gambling in o a rigged casino: The adversarial multi-arm bandit problem. In 36th Annual Symposium on Foundations of Computer Science, pages 322–331, 1995.</p>
<p>[11] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning, pages 928–925, 2003.</p>
<p>[12] Amir Jafari, Amy Greenwald, David Gondek, and Gunes Ercal. On no-regret learning, ﬁctitious play, and nash equilibrium. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 226–223, 2001.</p>
<p>[13] G. M. Korpelevich. The extragradient method for ﬁnding saddle points and other problems. Matecon, 12:747–756, 1976.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
