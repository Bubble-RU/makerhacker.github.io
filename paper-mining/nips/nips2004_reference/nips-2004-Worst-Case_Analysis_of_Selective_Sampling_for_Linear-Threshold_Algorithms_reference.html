<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-206" href="../nips2004/nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">nips2004-206</a> <a title="nips-2004-206-reference" href="#">nips2004-206-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</h1>
<br/><p>Source: <a title="nips-2004-206-pdf" href="http://papers.nips.cc/paper/2584-worst-case-analysis-of-selective-sampling-for-linear-threshold-algorithms.pdf">pdf</a></p><p>Author: Nicolò Cesa-bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: We provide a worst-case analysis of selective sampling algorithms for learning linear threshold functions. The algorithms considered in this paper are Perceptron-like algorithms, i.e., algorithms which can be efﬁciently run in any reproducing kernel Hilbert space. Our algorithms exploit a simple margin-based randomized rule to decide whether to query the current label. We obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic counterparts, but using much fewer labels. We complement our theoretical ﬁndings with an empirical comparison on two text categorization tasks. The outcome of these experiments is largely predicted by our theoretical results: Our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classiﬁcation, while observing in practice substantially fewer labels. 1</p><br/>
<h2>reference text</h2><p>[1] The OHSUMED test collection. URL: medir.ohsu.edu/pub/ohsumed/.</p>
<p>[2] Reuters corpus volume 1. URL: about.reuters.com/researchandstandards/corpus/.</p>
<p>[3] Atlas, L., Cohn, R., and Ladner, R. (1990). Training connectionist networks with queries and selective sampling. In NIPS 2. MIT Press.</p>
<p>[4] Azoury, K.S., and Warmuth, M.K. (2001). Relative loss bounds for on-line density estimation with the exponential familiy of distributions. Machine Learning, 43(3):211–246, 2001.</p>
<p>[5] Cesa-Bianchi, N., Conconi, A., and Gentile, C. (2002). A second-order Perceptron algorithm. In Proc. 15th COLT, pp. 121–137. LNAI 2375, Springer.</p>
<p>[6] Cesa-Bianchi, N. Lugosi, G., and Stoltz, G. (2004). Minimizing Regret with Label Efﬁcient Prediction In Proc. 17th COLT, to appear.</p>
<p>[7] Cesa-Bianchi, N., Conconi, A., and Gentile, C. (2003). Learning probabilistic linear-threshold classiﬁers via selective sampling. In Proc. 16th COLT, pp. 373–386. LNAI 2777, Springer.</p>
<p>[8] Campbell, C., Cristianini, N., and Smola, A. (2000). Query learning with large margin classiﬁers. In Proc. 17th ICML, pp. 111–118. Morgan Kaufmann.</p>
<p>[9] Cristianini, N., and Shawe-Taylor, J. (2001). An Introduction to Support Vector Machines. Cambridge University Press.</p>
<p>[10] Forster, J. On relative loss bounds in generalized linear regression. (1999). In Proc. 12th Int. Symp. FCT, pp. 269–280, Springer.</p>
<p>[11] Freund, Y., and Schapire, R. E. (1999). Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3), 277–296.</p>
<p>[12] Freund, Y., Seung, S., Shamir, E., and Tishby, N. (1997). Selective sampling using the query by committee algorithm. Machine Learning, 28(2/3):133–168.</p>
<p>[13] Gentile, C. & Warmuth, M. (1998). Linear hinge loss and average margin. In NIPS 10, MIT Press, pp. 225–231.</p>
<p>[14] Gentile, C. (2003). The robustness of the p-norm algorithms. Machine Learning, 53(3), 265– 299.</p>
<p>[15] Grove, A.J., Littlestone, N., & Schuurmans, D. (2001). General convergence results for linear discriminant updates. Machine Learning, 43(3), 173–210.</p>
<p>[16] Helmbold, D.P., Littlestone, N. and Long, P.M. (2000). Apple tasting. Information and Computation, 161(2), 85–139.</p>
<p>[17] Helmbold, D.P., and Panizza, S. (1997). Some label efﬁcient learning results. In Proc. 10th COLT, pp. 218–230. ACM Press.</p>
<p>[18] Littlestone, N. (1988). Learning quickly when irrelevant attributes abound: a new linearthreshold algorithm. Machine Learning, 2(4), 285–318.</p>
<p>[19] Littlestone, N., and Warmuth, M.K. (1994). The weighted majority algorithm. Information and Computation, 108(2), 212–261.</p>
<p>[20] F. Rosenblatt. (1958). The Perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Review, 65, 386–408.</p>
<p>[21] Sch¨ lkopf, B., and Smola, A. (2002). Learning with kernels. MIT Press, 2002. o</p>
<p>[22] Tong, S., and Koller, D. (2000). Support vector machine active learning with applications to text classiﬁcation. In Proc. 17th ICML. Morgan Kaufmann.</p>
<p>[23] Vapnik, V.N. (1998). Statistical Learning Theory. Wiley.</p>
<p>[24] Vovk, V. (1990). Aggregating strategies. Proc. 3rd COLT, pp. 371–383. Morgan Kaufmann.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
