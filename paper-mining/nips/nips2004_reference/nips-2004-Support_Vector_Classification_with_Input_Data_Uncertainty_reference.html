<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2004-Support Vector Classification with Input Data Uncertainty</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-178" href="../nips2004/nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">nips2004-178</a> <a title="nips-2004-178-reference" href="#">nips2004-178-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>178 nips-2004-Support Vector Classification with Input Data Uncertainty</h1>
<br/><p>Source: <a title="nips-2004-178-pdf" href="http://papers.nips.cc/paper/2743-support-vector-classification-with-input-data-uncertainty.pdf">pdf</a></p><p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><br/>
<h2>reference text</h2><p>[1] J. Bezdek and R. Hathaway. Convergence of alternating optimization. Neural, Parallel Sci. Comput., 11:351–368, 2003.</p>
<p>[2] C. Bhattacharyya, K.S. Pannagadatta, and A. J. Smola. A second order cone programming formulation for classifying missing data. In NIPS, Vol 17, 2005.</p>
<p>[3] J. Bi and V. N. Vapnik. Learning with rigorous support vector machines. In M. Warmuth and B. Sch¨ lkopf, editors, Proceedings of the 16th Annual Conference on Learning Theory, pages o 35–42, Menlo Park, CA, 2003. AAAI Press.</p>
<p>[4] L. El Ghaoui and H. Lebret. Robust solutions to least-squares problems with uncertain data. SIAM Journal on Matrix Analysis and Applications, 18:1035–1064, 1997.</p>
<p>[5] G. H. Golub, P. C. Hansen, and D. P. O’Leary. Tikhonov regularization and total least squares. SIAM Journal on Numerical Analysis, 30:185–194, 1999.</p>
<p>[6] G. H. Golub and C. F. Van Loan. An analysis of the total least squares problem. SIAM Journal on Numerical Analysis, 17:883–893, 1980.</p>
<p>[7] S. Van Huffel and J. Vandewalle. The Total Least Squares Problem: Computational Aspects and Analysis, in Frontiers in Applied Mathematics 9. SIAM Press, Philadelphia, PA, 1991.</p>
<p>[8] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, Inc., New York, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
