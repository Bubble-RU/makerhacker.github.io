<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-60" href="../nips2004/nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">nips2004-60</a> <a title="nips-2004-60-reference" href="#">nips2004-60-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</h1>
<br/><p>Source: <a title="nips-2004-60-pdf" href="http://papers.nips.cc/paper/2550-efficient-kernel-machines-using-the-improved-fast-gauss-transform.pdf">pdf</a></p><p>Author: Changjiang Yang, Ramani Duraiswami, Larry S. Davis</p><p>Abstract: The computation and memory required for kernel machines with N training samples is at least O(N 2 ). Such a complexity is signiﬁcant even for moderate size problems and is prohibitive for large datasets. We present an approximation technique based on the improved fast Gauss transform to reduce the computation to O(N ). We also give an error bound for the approximation, and provide experimental results on the UCI datasets. 1</p><br/>
<h2>reference text</h2><p>[1] M. Bern and D. Eppstein. Approximation algorithms for geometric problems. In D. Hochbaum, editor, Approximation Algorithms for NP-Hard Problems, chapter 8, pages 296–345. PWS Publishing Company, Boston, 1997.</p>
<p>[2] T. Feder and D. Greene. Optimal algorithms for approximate clustering. In Proc. 20th ACM Symp. Theory of computing, pages 434–444, Chicago, Illinois, 1988.</p>
<p>[3] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of Machine Learning Research, 2:243–264, Dec. 2001.</p>
<p>[4] G. Fung and O. L. Mangasarian. Proximal support vector machine classiﬁers. In Proceedings KDD-2001: Knowledge Discovery and Data Mining, pages 77–86, San Francisco, CA, 2001.</p>
<p>[5] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural Computation, 7(2):219–269, 1995.</p>
<p>[6] T. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Computer Science, 38:293–306, 1985.</p>
<p>[7] L. Greengard and V. Rokhlin. A fast algorithm for particle simulations. J. Comput. Phys., 73(2):325–348, 1987.</p>
<p>[8] L. Greengard and J. Strain. The fast Gauss transform. SIAM J. Sci. Statist. Comput., 12(1):79– 94, 1991.</p>
<p>[9] Y.-J. Lee and O. Mangasarian. RSVM: Reduced support vector machines. In First SIAM International Conference on Data Mining, Chicago, 2001.</p>
<p>[10] T. Poggio and S. Smale. The mathematics of learning: Dealing with data. Notices of the American Mathematical Society (AMS), 50(5):537–544, 2003.</p>
<p>[11] R. Rifkin. Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning. PhD thesis, MIT, Cambridge, MA, 2002.</p>
<p>[12] A. Smola and P. Bartlett. Sparse greedy gaussian process regression. In Advances in Neural Information Processing Systems, pages 619–625. MIT Press, 2001.</p>
<p>[13] A. Smola and B. Sch¨lkopf. Sparse greedy matrix approximation for machine learning. In o Proc. Int’l Conf. Machine Learning, pages 911–918. Morgan Kaufmann, 2000.</p>
<p>[14] X. Sun and N. P. Pitsianis. A matrix version of the fast multipole method. SIAM Review, 43(2):289–300, 2001.</p>
<p>[15] J. A. K. Suykens and J. Vandewalle. Least squares support vector machine classiﬁers. Neural Processing Letters, 9(3):293–300, 1999.</p>
<p>[16] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.</p>
<p>[17] G. Wahba. Spline Models for Observational Data. SIAM, Philadelphia, PA, 1990.</p>
<p>[18] C. K. Williams and D. Barber. Bayesian classiﬁcation with gaussian processes. IEEE Trans. Pattern Anal. Mach. Intell., 20(12):1342–1351, Dec. 1998. ¨</p>
<p>[19] C. K. I. Williams and M. Seeger. Using the Nystrom method to speed up kernel machines. In Advances in Neural Information Processing Systems, pages 682–688. MIT Press, 2001.</p>
<p>[20] C. Yang, R. Duraiswami, N. Gumerov, and L. Davis. Improved fast Gauss transform and efﬁcient kernel density estimation. In Proc. ICCV 2003, pages 464–471, 2003.</p>
<p>[21] C. Yang, R. Duraiswami, and N. A. Gumerov. Improved fast gauss transform. Technical Report CS-TR-4495, UMIACS, Univ. of Maryland, College Park, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
