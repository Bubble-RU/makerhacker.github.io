<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-31" href="../nips2004/nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">nips2004-31</a> <a title="nips-2004-31-reference" href="#">nips2004-31-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</h1>
<br/><p>Source: <a title="nips-2004-31-pdf" href="http://papers.nips.cc/paper/2572-blind-one-microphone-speech-separation-a-spectral-learning-approach.pdf">pdf</a></p><p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm to perform blind, one-microphone speech separation. Our algorithm separates mixtures of speech without modeling individual speakers. Instead, we formulate the problem of speech separation as a problem in segmenting the spectrogram of the signal into two or more disjoint sets. We build feature sets for our segmenter using classical cues from speech psychophysics. We then combine these features into parameterized afﬁnity matrices. We also take advantage of the fact that we can generate training examples for segmentation by artiﬁcially superposing separately-recorded signals. Thus the parameters of the afﬁnity matrices can be tuned using recent work on learning spectral clustering [1]. This yields an adaptive, speech-speciﬁc segmentation algorithm that can successfully separate one-microphone speech mixtures. 1</p><br/>
<h2>reference text</h2><p>[1] F. R. Bach and M. I. Jordan. Learning spectral clustering. In NIPS 16, 2004.</p>
<p>[2] A. Hyv¨ rinen, J. Karhunen, and E. Oja. Independent Component Analysis. John a Wiley & Sons, 2001.</p>
<p>[3] M. Zibulevsky, P. Kisilev, Y. Y. Zeevi, and B. A. Pearlmutter. Blind source separation via multinode sparse representation. In NIPS 14, 2002.</p>
<p>[4] O. Yilmaz and S. Rickard. Blind separation of speech mixtures via time-frequency masking. IEEE Trans. Sig. Proc., 52(7):1830–1847, 2004.</p>
<p>[5] S. T. Roweis. One microphone source separation. In NIPS 13, 2001.</p>
<p>[6] G.-J. Jang and T.-W. Lee. A probabilistic approach to single channel source separation. In NIPS 15, 2003.</p>
<p>[7] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE PAMI, 22(8):888–905, 2000.</p>
<p>[8] A. S. Bregman. Auditory Scene Analysis: The Perceptual Organization of Sound. MIT Press, 1990.</p>
<p>[9] G. J. Brown and M. P. Cooke. Computational auditory scene analysis. Computer Speech and Language, 8:297–333, 1994.</p>
<p>[10] S. Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1998.</p>
<p>[11] M. Cooke and D. P. W. Ellis. The auditory organization of speech and other sources in listeners and computational models. Speech Communication, 35(3-4):141–177, 2001.</p>
<p>[12] B. Gold and N. Morgan. Speech and Audio Signal Processing: Processing and Perception of Speech and Music. Wiley Press, 1999.</p>
<p>[13] S. Belongie, C. Fowlkes, F. Chung, and J. Malik. Spectral partitioning with indeﬁnite kernels using the Nystr¨ m extension. In ECCV, 2002. o</p>
<p>[14] G. Wahba. Spline Models for Observational Data. SIAM, 1990.</p>
<p>[15] F. R. Bach and M. I. Jordan. Discriminative training of hidden Markov models for multiple pitch tracking. In ICASSP, 2005.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
