<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2004-Euclidean Embedding of Co-Occurrence Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-62" href="../nips2004/nips-2004-Euclidean_Embedding_of_Co-Occurrence_Data.html">nips2004-62</a> <a title="nips-2004-62-reference" href="#">nips2004-62-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>62 nips-2004-Euclidean Embedding of Co-Occurrence Data</h1>
<br/><p>Source: <a title="nips-2004-62-pdf" href="http://papers.nips.cc/paper/2733-euclidean-embedding-of-co-occurrence-data.pdf">pdf</a></p><p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for low dimensional structure in complex data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of our embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text datasets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling and correspondence analysis. 1</p><br/>
<h2>reference text</h2><p>[1] E.D. Bolker. and B. Roth. When is a bipartite graph a rigid framework? 90:27–44, 1980.  Paciﬁc J. Math.,</p>
<p>[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2004.</p>
<p>[3] G. Chechik and N. Tishby. Extracting relevant structures with side information. In S. Becker, S. Thrun, and K. Obermayer, editors, NIPS 15, 2002.</p>
<p>[4] T. Cox and M. Cox. Multidimensional Scaling. Chapman and Hall, London, 1984.</p>
<p>[5] M. Fazel, H. Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order system approximation. In Proc. of the American Control Conference, 2001.</p>
<p>[6] R.A. Fisher. The percision of discriminant functions. Ann. Eugen. Lond., 10:422–429, 1940.</p>
<p>[7] A. Globerson and N. Tishby. Sufﬁcient dimensionality reduction. Journal of Machine Learning Research, 3:1307–1331, 2003.</p>
<p>[8] M.J. Greenacre. Theory and applications of correspondence analysis. Academic Press, 1984.</p>
<p>[9] G. Hinton and S.T. Roweis. Stochastic neighbor embedding. In NIPS 15, 2002.</p>
<p>[10] H. Hotelling. The most predictable criterion. Journal of Educational Psych., 26:139–142, 1935.</p>
<p>[11] T. Iwata, K. Saito, N. Ueda, S. Stromsten, T. Grifﬁths, and J. Tenenbaum. Parametric embedding for class visualization. In NIPS 18, 2004.</p>
<p>[12] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000.</p>
<p>[13] J.B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000.</p>
<p>[14] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semideﬁnite programming. In CVPR, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
