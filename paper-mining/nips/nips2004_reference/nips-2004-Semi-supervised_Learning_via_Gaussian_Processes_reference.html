<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2004-Semi-supervised Learning via Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-166" href="../nips2004/nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">nips2004-166</a> <a title="nips-2004-166-reference" href="#">nips2004-166-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>166 nips-2004-Semi-supervised Learning via Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2004-166-pdf" href="http://papers.nips.cc/paper/2605-semi-supervised-learning-via-gaussian-processes.pdf">pdf</a></p><p>Author: Neil D. Lawrence, Michael I. Jordan</p><p>Abstract: We present a probabilistic approach to learning a Gaussian Process classiﬁer in the presence of unlabeled data. Our approach involves a “null category noise model” (NCNM) inspired by ordered categorical noise models. The noise model reﬂects an assumption that the data density is lower between the class-conditional densities. We illustrate our approach on a toy problem and present comparative results for the semi-supervised classiﬁcation of handwritten digits. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agresti. Categorical Data Analysis. John Wiley and Sons, 2002.</p>
<p>[2] O. Chapelle, J. Weston, and B. Sch¨lkopf. Cluster kernels for semi-supervised learno ing. In Advances in Neural Information Processing Systems, Cambridge, MA, 2002. MIT Press.</p>
<p>[3] L. Csat´. Gaussian Processes — Iterative Sparse Approximations. PhD thesis, Aston o University, 2002.</p>
<p>[4] T. Joachims. Making large-scale SVM learning practical. In Advances in Kernel Methods: Support Vector Learning, Cambridge, MA, 1998. MIT Press.</p>
<p>[5] N. D. Lawrence and B. Sch¨lkopf. Estimating a kernel Fisher discriminant in the o presence of label noise. In Proceedings of the International Conference in Machine Learning, San Francisco, CA, 2001. Morgan Kaufmann.</p>
<p>[6] N. D. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In Advances in Neural Information Processing Systems, Cambridge, MA, 2003. MIT Press.</p>
<p>[7] T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of Technology, 2001.</p>
<p>[8] M. Seeger. Covariance kernels from Bayesian generative models. In Advances in Neural Information Processing Systems, Cambridge, MA, 2002. MIT Press.</p>
<p>[9] P. Sollich. Probabilistic interpretation and Bayesian methods for support vector machines. In Proceedings 1999 International Conference on Artiﬁcial Neural Networks, ICANN’99, pages 91–96, 1999.</p>
<p>[10] V. N. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.</p>
<p>[11] C. K. I. Williams. Prediction with Gaussian processes: From linear regression to linear prediction and beyond. In Learning in Graphical Models, Cambridge, MA, 1999. MIT Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
