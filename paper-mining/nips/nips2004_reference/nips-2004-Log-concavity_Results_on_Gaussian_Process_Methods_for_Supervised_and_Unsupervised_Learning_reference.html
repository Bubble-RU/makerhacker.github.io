<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2004-Log-concavity Results on Gaussian Process Methods for Supervised and Unsupervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-105" href="../nips2004/nips-2004-Log-concavity_Results_on_Gaussian_Process_Methods_for_Supervised_and_Unsupervised_Learning.html">nips2004-105</a> <a title="nips-2004-105-reference" href="#">nips2004-105-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>105 nips-2004-Log-concavity Results on Gaussian Process Methods for Supervised and Unsupervised Learning</h1>
<br/><p>Source: <a title="nips-2004-105-pdf" href="http://papers.nips.cc/paper/2590-log-concavity-results-on-gaussian-process-methods-for-supervised-and-unsupervised-learning.pdf">pdf</a></p><p>Author: Liam Paninski</p><p>Abstract: Log-concavity is an important property in the context of optimization, Laplace approximation, and sampling; Bayesian methods based on Gaussian process priors have become quite popular recently for classiﬁcation, regression, density estimation, and point process intensity estimation. Here we prove that the predictive densities corresponding to each of these applications are log-concave, given any observed data. We also prove that the likelihood is log-concave in the hyperparameters controlling the mean function of the Gaussian prior in the density and point process intensity estimation cases, and the mean, covariance, and observation noise parameters in the classiﬁcation and regression cases; this result leads to a useful parameterization of these hyperparameters, indicating a suitably large class of priors for which the corresponding maximum a posteriori problem is log-concave.</p><br/>
<h2>reference text</h2><p>1. M. Seeger, International Journal of Neural Systems 14, 1 (2004). 2. P. Sollich, A. Halees, Neural Computation 14, 1393 (2002). 3. C. Williams, D. Barber, IEEE PAMI 20, 1342 (1998). 4. M. Gibbs, D. MacKay, IEEE Transactions on Neural Networks 11, 1458 (2000). 5. L. Csato, Gaussian processes - iterative sparse approximations, Ph.D. thesis, Aston U. (2002). 6. T. Minka, A family of algorithms for approximate bayesian inference, Ph.D. thesis, MIT (2001). 7. W. Gilks, P. Wild, Applied Statistics 41, 337 (1992). 8. R. Neal, Annals of Statistics 31, 705 (2003). 9. L. Lovasz, S. Vempala, The geometry of logconcave functions and an O∗ (n3 ) sampling algorithm, Tech. Rep. 2003-04, Microsoft Research (2003). 10. A. Prekopa, Acad Sci. Math. 34, 335 (1973). 11. Y. Rinott, Annals of Probability 4, 1020 (1976). 12. P. McCullagh, J. Nelder, Generalized linear models (Chapman and Hall, London, 1989). 13. J. Oakley, A. O’Hagan, Biometrika under review (2003). 14. I. Good, R. Gaskins, Biometrika 58, 255 (1971). 15. W. Bialek, C. Callan, S. Strong, Physical Review Letters 77, 4693 (1996). 16. D. Snyder, M. Miller, Random Point Processes in Time and Space (Springer-Verlag, 1991). 17. J. Moller, A. Syversveen, R. Waagepetersen, Scandinavian Journal of Statistics 25, 451 (1998). 18. I. DiMatteo, C. Genovese, R. Kass, Biometrika 88, 1055 (2001). 19. R. Neal, Monte Carlo implementation of Gaussian process models for Bayesian regression and classiﬁcation, Tech. Rep. 9702, University of Toronto (1997). 20. L. Paninski, Network: Computation in Neural Systems 15, 243 (2004). 21. J. Pillow, L. Paninski, E. Simoncelli, NIPS 17 (2003). 22. L. Paninski, J. Pillow, E. Simoncelli, Neural Computation 16, 2533 (2004). 23. H. Dym, H. McKean, Fourier Series and Integrals (Academic Press, New York, 1972).</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
