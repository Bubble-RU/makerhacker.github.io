<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2004-Experts in a Markov Decision Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-64" href="../nips2004/nips-2004-Experts_in_a_Markov_Decision_Process.html">nips2004-64</a> <a title="nips-2004-64-reference" href="#">nips2004-64-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 nips-2004-Experts in a Markov Decision Process</h1>
<br/><p>Source: <a title="nips-2004-64-pdf" href="http://papers.nips.cc/paper/2730-experts-in-a-markov-decision-process.pdf">pdf</a></p><p>Author: Eyal Even-dar, Sham M. Kakade, Yishay Mansour</p><p>Abstract: We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain ﬁxed. Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time. We provide efﬁcient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions. We also show that in the case that the dynamics change over time, the problem becomes computationally hard. 1</p><br/>
<h2>reference text</h2><p>Y. Freund and R. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29:79–103, 1999. J. Hastad. Some optimal inapproximability results. J. ACM, 48(4):798–859, 2001. S. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003. A. Kalai and S. Vempala. Efﬁcient algorithms for on-line optimization. Proceedings of COLT, 2003. M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Proceedings of ICML, 1998. H. McMahan, G. Gordon, and A. Blum. Planning in the presence of cost functions controlled by an adversary. In In the 20th ICML, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
