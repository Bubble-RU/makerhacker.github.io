<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-15" href="../nips2004/nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">nips2004-15</a> <a title="nips-2004-15-reference" href="#">nips2004-15-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</h1>
<br/><p>Source: <a title="nips-2004-15-pdf" href="http://papers.nips.cc/paper/2554-active-learning-for-anomaly-and-rare-category-detection.pdf">pdf</a></p><p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We introduce a novel active-learning scenario in which a user wants to work with a learning algorithm to identify useful anomalies. These are distinguished from the traditional statistical deﬁnition of anomalies as outliers or merely ill-modeled points. Our distinction is that the usefulness of anomalies is categorized subjectively by the user. We make two additional assumptions. First, there exist extremely few useful anomalies to be hunted down within a massive dataset. Second, both useful and useless anomalies may sometimes exist within tiny classes of similar anomalies. The challenge is thus to identify “rare category” records in an unlabeled noisy set with help (in the form of class labels) from a human expert who has a small budget of datapoints that they are prepared to categorize. We propose a technique to meet this challenge, which assumes a mixture model ﬁt to the data, but otherwise makes no assumptions on the particular form of the mixture components. This property promises wide applicability in real-life scenarios and for various statistical models. We give an overview of several alternative methods, highlighting their strengths and weaknesses, and conclude with a detailed empirical analysis. We show that our method can quickly zoom in on an anomaly set containing a few tens of points in a dataset of hundreds of thousands. 1</p><br/>
<h2>reference text</h2><p>[1] Sugato Basu, Arindam Banerjee, and Raymond J. Mooney. Active semi-supervision for pairwise constrained clustering. Submitted for publication, February, 2003.</p>
<p>[2] M. Seeger. Learning with labeled and unlabeled data. Technical report, Institue for Adaptive and Neural Computation, Universiy of Edinburgh, 2000.</p>
<p>[3] Klaus Brinker. Incorporating diversity in active learning with support vector machines. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.</p>
<p>[4] David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. Active learning with statistical models. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information Processing Systems, volume 7, pages 705–712. The MIT Press, 1995.</p>
<p>[5] Nirmalie Wiratunga, Susan Craw, and Stewart Massie. Index driven selective sampling for CBR, 2003. To appear in Proceedings of the Fifth International Conference on Case-Based Reasoning, Springer-Verlag, Trondheim, Norway, 23-26 June 2003.</p>
<p>[6] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine Learning, 15(2):201–221, 1994.</p>
<p>[7] Mark Plutowski and Halbert White. Selecting concise training sets from clean data. IEEE Transactions on Neural Networks, 4(2):305–318, March 1993.</p>
<p>[8] Shahshashani and Landgrebe. The effect of unlabeled examples in reducing the small sample size problem. IEEE Trans Geoscience and Remote Sensing, 32(5):1087–1095, 1994.</p>
<p>[9] Miller and Uyar. A mixture of experts classiﬁer with learning based on both labeled and unlabelled data. In NIPS-9, 1997.</p>
<p>[10] H. S. Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Computational Learning Theory, pages 287–294, 1992.</p>
<p>[11] David D. Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In William W. Cohen and Haym Hirsh, editors, Proceedings of ICML-94, 11th International Conference on Machine Learning, pages 148–156, New Brunswick, US, 1994. Morgan Kaufmann Publishers, San Francisco, US.</p>
<p>[12] P.Brazdil and J.Gama. StatLog, 1991. http://www.liacc.up.pt/ML/statlog.</p>
<p>[13] C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1998. http:// www.ics.uci.edu/∼mlearn/MLRepository.html.</p>
<p>[14] R. C. Nichol, C. A. Collins, and S. L. Lumsden. The Edinburgh/Durham southern galaxy catalogue — IX. Submitted to the Astrophysical Journal, 2000.</p>
<p>[15] SDSS. The Sloan Digital Sky Survey, 1998. www.sdss.org.</p>
<p>[16] Dan Pelleg. Scalable and Practical Probability Density Estimators for Scientiﬁc Anomaly Detection. PhD thesis, Carnegie-Mellon University, 2004. Tech Report CMU-CS-04-134.</p>
<p>[17] David MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):590–604, 1992.</p>
<p>[18] Fabio Gagliardi Cozman, Ira Cohen, and Marclo Cesar Cirelo. Semi-supervised learning of mixture models and bayesian networks. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.</p>
<p>[19] Yoram Baram, Ran El-Yaniv, and Kobi Luz. Online choice of active learning algorithms. In Proceedings of the Twentieth International Conference on Machine Learning, 2003.</p>
<p>[20] Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural Information Processing Systems 18, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
