<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 nips-2004-Fast Rates to Bayes for Kernel Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-69" href="../nips2004/nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">nips2004-69</a> <a title="nips-2004-69-reference" href="#">nips2004-69-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>69 nips-2004-Fast Rates to Bayes for Kernel Machines</h1>
<br/><p>Source: <a title="nips-2004-69-pdf" href="http://papers.nips.cc/paper/2630-fast-rates-to-bayes-for-kernel-machines.pdf">pdf</a></p><p>Author: Ingo Steinwart, Clint Scovel</p><p>Abstract: We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels. Finally, we compare our methods with a recent paper of G. Blanchard et al.. 1</p><br/>
<h2>reference text</h2><p>[1] I. Steinwart. Support vector machines are universally consistent. J. Complexity, 18:768–791, 2002.</p>
<p>[2] T. Zhang. Statistical behaviour and consistency of classiﬁcation methods based on convex risk minimization. Ann. Statist., 32:56–134, 2004.</p>
<p>[3] I. Steinwart. Consistency of support vector machines and other regularized kernel machines. IEEE Trans. Inform. Theory, to appear, 2005.</p>
<p>[4] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. o Springer, New York, 1996.</p>
<p>[5] A.B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist., 32:135–166, 2004.</p>
<p>[6] Y. Yang. Minimax nonparametric classiﬁcation—part I and II. IEEE Trans. Inform. Theory, 45:2271–2292, 1999.</p>
<p>[7] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000.</p>
<p>[8] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. J. Mach. Learn. Res., 2:67–93, 2001.</p>
<p>[9] Q. Wu and D.-X. Zhou. Analysis of support vector machine classiﬁcation. Tech. Report, City University of Hong Kong, 2003.</p>
<p>[10] C. Scovel and I. Steinwart. Fast rates for support vector machines. Ann. Statist., submitted, 2003. http://www.c3.lanl.gov/˜ingo/publications/ ann-03.ps.</p>
<p>[11] G. Blanchard, O. Bousquet, and P. Massart. Statistical performance of support vector machines. Ann. Statist., submitted, 2004.</p>
<p>[12] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Anal. Appl., 1:17–41, 2003.</p>
<p>[13] I. Steinwart. Sparseness of support vector machines. J. Mach. Learn. Res., 4:1071– 1105, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
