<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 nips-2004-Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-169" href="../nips2004/nips-2004-Sharing_Clusters_among_Related_Groups%3A_Hierarchical_Dirichlet_Processes.html">nips2004-169</a> <a title="nips-2004-169-reference" href="#">nips2004-169-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>169 nips-2004-Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes</h1>
<br/><p>Source: <a title="nips-2004-169-pdf" href="http://papers.nips.cc/paper/2698-sharing-clusters-among-related-groups-hierarchical-dirichlet-processes.pdf">pdf</a></p><p>Author: Yee W. Teh, Michael I. Jordan, Matthew J. Beal, David M. Blei</p><p>Abstract: We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.</p><br/>
<h2>reference text</h2><p>[1] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. JMLR, 3:993–1022, 2003.</p>
<p>[2] M.D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90:577–588, 1995.</p>
<p>[3] S.N. MacEachern and P. M¨ ller. Estimating mixture of Dirichlet process models. Journal of u Computational and Graphical Statistics, 7:223–238, 1998.</p>
<p>[4] T.S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1(2):209–230, 1973. ´</p>
<p>[5] D. Aldous. Exchangeability and related topics. In Ecole d’´ t´ de probabilit´ s de Saint-Flour ee e XIII–1983, pages 1–198. Springer, Berlin, 1985.</p>
<p>[6] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650, 1994.</p>
<p>[7] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249–265, 2000.</p>
<p>[8] C.E. Rasmussen. The inﬁnite Gaussian mixture model. In NIPS, volume 12, 2000.</p>
<p>[9] D.M. Blei, T.L. Grifﬁths, M.I. Jordan, and J.B. Tenenbaum. Hierarchical topic models and the nested Chinese restaurant process. NIPS, 2004.</p>
<p>[10] Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. Hierarchical dirichlet processes. Technical Report 653, Department of Statistics, University of California at Berkeley, 2004.</p>
<p>[11] M.J. Beal, Z. Ghahramani, and C.E. Rasmussen. The inﬁnite hidden Markov model. In NIPS, volume 14, 2002.</p>
<p>[12] M.J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Unit, University College London, 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
