<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2004-Online Bounds for Bayesian Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-138" href="../nips2004/nips-2004-Online_Bounds_for_Bayesian_Algorithms.html">nips2004-138</a> <a title="nips-2004-138-reference" href="#">nips2004-138-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2004-Online Bounds for Bayesian Algorithms</h1>
<br/><p>Source: <a title="nips-2004-138-pdf" href="http://papers.nips.cc/paper/2637-online-bounds-for-bayesian-algorithms.pdf">pdf</a></p><p>Author: Sham M. Kakade, Andrew Y. Ng</p><p>Abstract: We present a competitive analysis of Bayesian learning algorithms in the online learning setting and show that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) perform favorably when compared, in retrospect, to the single best model in the model class. The analysis does not assume that the Bayesian algorithms’ modeling assumptions are “correct,” and our bounds hold even if the data is adversarially chosen. For Gaussian linear regression (using logloss), our error bounds are comparable to the best bounds in the online learning literature, and we also provide a lower bound showing that Gaussian linear regression is optimal in a certain worst case sense. We also give bounds for some widely used maximum a posteriori (MAP) estimation algorithms, including regularized logistic regression. 1</p><br/>
<h2>reference text</h2><p>Azoury, K. S. and Warmuth, M. (2001). Relative loss bounds for on-line density estimation with the exponential family of distributions. Machine Learning, 43(3). Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold, D., Schapire, R., and Warmuth, M. (1997). How to use expert advice. J. ACM, 44. Cesa-Bianchi, N., Helmbold, D., and Panizza, S. (1998). On Bayes methods for on-line boolean prediction. Algorithmica, 22. Dawid, A. (1984). Statistical theory: The prequential approach. J. Royal Statistical Society. Foster, D. P. (1991). Prediction in the worst case. Annals of Statistics, 19. Freund, Y. and Schapire, R. (1999). Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29:79–103. Freund, Y., Schapire, R., Singer, Y., and Warmuth, M. (1997). Using and combining predictors that specialize. In STOC. Grunwald, P. (2005). A tutorial introduction to the minimum description length principle. McCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models (2nd ed.). Chapman and Hall. Ng, A. Y. and Jordan, M. (2001). Convergence rates of the voting Gibbs classiﬁer, with application to Bayesian feature selection. In Proceedings of the 18th Int’l Conference on Machine Learning. Vovk, V. (2001). Competitive on-line statistics. International Statistical Review, 69.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
