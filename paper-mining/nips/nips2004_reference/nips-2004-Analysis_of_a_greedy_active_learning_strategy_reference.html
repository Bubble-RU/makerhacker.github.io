<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 nips-2004-Analysis of a greedy active learning strategy</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-23" href="../nips2004/nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">nips2004-23</a> <a title="nips-2004-23-reference" href="#">nips2004-23-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 nips-2004-Analysis of a greedy active learning strategy</h1>
<br/><p>Source: <a title="nips-2004-23-pdf" href="http://papers.nips.cc/paper/2636-analysis-of-a-greedy-active-learning-strategy.pdf">pdf</a></p><p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><br/>
<h2>reference text</h2><p>[1] D. Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988.</p>
<p>[2] D. Angluin. Queries revisited. Proceedings of the Twelfth International Conference on Algorithmic Learning Theory, pages 12–31, 2001.</p>
<p>[3] E.B. Baum and K. Lang. Query learning can work poorly when a human oracle is used. International Joint Conference on Neural Networks, 1992.</p>
<p>[4] A. Bjorner, M. Las Vergnas, B. Sturmfels, N. White, and G. Ziegler. Oriented matroids. Cambridge University Press, 1999.</p>
<p>[5] D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical models. Journal of Artiﬁcial Intelligence Research, 4:129–145, 1996.</p>
<p>[6] S. Dasgupta, P.M. Long, and W.S. Lee. A theoretical analysis of query selection for collaborative ﬁltering. Machine Learning, 51:283–298, 2003.</p>
<p>[7] Y. Freund, S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee algorithm. Machine Learning, 28:133–168, 1997.</p>
<p>[8] D.S. Johnson. Approximation algorithms for combinatorial problems. Journal of Computer and System Sciences, 9:256–278, 1974.</p>
<p>[9] M.J. Kearns and U.V. Vazirani. An introduction to computational learning theory. MIT Press, 1993.</p>
<p>[10] A. McCallum and K. Nigam. Employing em and pool-based active learning for text classiﬁcation. Fifteenth International Conference on Machine Learning, 1998.</p>
<p>[11] N. Roy and A. McCallum. Toward optimal active learning through sampling of error reduction. Twentieth International Conference on Machine Learning, 2003.</p>
<p>[12] J. Shawe-Taylor, P. Bartlett, R. Williamson, and M. Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 1998.</p>
<p>[13] S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. Journal of Machine Learning Research, 2001.</p>
<p>[14] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining active learning and semi-supervised learning using gaussian ﬁelds and harmonic functions. ICML workshop, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
