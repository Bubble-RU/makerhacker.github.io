<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-38" href="../nips2004/nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">nips2004-38</a> <a title="nips-2004-38-reference" href="#">nips2004-38-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</h1>
<br/><p>Source: <a title="nips-2004-38-pdf" href="http://papers.nips.cc/paper/2603-co-validation-using-model-disagreement-on-unlabeled-data-to-validate-classification-algorithms.pdf">pdf</a></p><p>Author: Omid Madani, David M. Pennock, Gary W. Flake</p><p>Abstract: In the context of binary classiﬁcation, we deﬁne disagreement as a measure of how often two independently-trained models differ in their classiﬁcation of unlabeled data. We explore the use of disagreement for error estimation and model selection. We call the procedure co-validation, since the two models effectively (in)validate one another by comparing results on unlabeled data, which we assume is relatively cheap and plentiful compared to labeled data. We show that per-instance disagreement is an unbiased estimate of the variance of error for that instance. We also show that disagreement provides a lower bound on the prediction (generalization) error, and a tight upper bound on the “variance of prediction error”, or the variance of the average error across instances, where variance is measured across training sets. We present experimental results on several data sets exploring co-validation for error estimation and model selection. The procedure is especially effective in active learning settings, where training sets are not drawn at random and cross validation overestimates error. 1</p><br/>
<h2>reference text</h2><p>[BC03] Y. Bengio and N. Chapados. Extensions to metric-based model selection. Journal of Machine Learning Research, 2003. [BG03] Y. Bengio and Y. Granvalet. No unbiased estimator of the variance of k-fold cross-validation. In NIPS, 2003. [BKM98] C.L. Blake, E. Keogh, and C.J. Merz. UCI repository of machine learning databases, 1998. [CL01] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm. [KN02] S. Kutin and P. Niyogi. Almost-everywhere algorithmic stability and generalization error. In UAI, 2002. [Kut02] S. Kutin. Algorithmic stability and ensemble-based learning. PhD thesis, University of Chicago, 2002. [KV95] A. Krogh and J. Vedelsby. Neural network ensembles, cross validation, and active learning. In NIPS, 1995. [LBRB02] T. Lange, M. Braun, V. Roth, and J. Buhmann. Stability-based model selection. In NIPS, 2002. [LRBB04] T. Lange, V. Roth, M. Braun, and J. Buhmann. Stability based validation of clustering algorithms. Neural Computation, 16, 2004. [McC96] A. K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classiﬁcation and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996. [Sch97] D. Schuurmans. A new metric-based approach to model selection. In AAAI, 1997. [SS02] D. Schuurmans and F. Southey. Metric-based methods for adaptive model selection and regularization. Machine Learning, pages 51–84, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
