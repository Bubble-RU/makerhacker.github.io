<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-11" href="../nips2004/nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">nips2004-11</a> <a title="nips-2004-11-reference" href="#">nips2004-11-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</h1>
<br/><p>Source: <a title="nips-2004-11-pdf" href="http://papers.nips.cc/paper/2670-a-second-order-cone-programming-formulation-for-classifying-missing-data.pdf">pdf</a></p><p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><br/>
<h2>reference text</h2><p>[1] J. Bi and T. Zhang. Support vector classiﬁcation with input data uncertainty. In Advances in Neural Information Processing Systems. MIT Press, 2004.</p>
<p>[2] C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998.</p>
<p>[3] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273–297, 1995.</p>
<p>[4] G. Fung, O. L. Mangasarian, and Jude Shavlik. Knowledge-based support vector machine classiﬁers. In Advances in Neural Information Processing Systems. MIT Press, 2002.</p>
<p>[5] Thore Graepel and Ralf Herbrich. Invariant pattern recognition by semideﬁnite programming machines. In Advances in Neural Information Processing Systems 16, Cambridge, MA, 2003. MIT Press.</p>
<p>[6] M.S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order cone programming. Linear Algebra and its Applications, 284(1–3):193–228, 1998.</p>
<p>[7] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate Analysis. Academic Press, 1979.</p>
<p>[8] Y. Nesterov and A. Nemirovskii. Interior Point Algorithms in Convex Programming. Number 13 in Studies in Applied Mathematics. SIAM, Philadelphia, 1993.</p>
<p>[9] V. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
