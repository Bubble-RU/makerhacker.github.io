<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-154" href="../nips2004/nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">nips2004-154</a> <a title="nips-2004-154-reference" href="#">nips2004-154-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</h1>
<br/><p>Source: <a title="nips-2004-154-pdf" href="http://papers.nips.cc/paper/2723-resolving-perceptual-aliasing-in-the-presence-of-noisy-sensors.pdf">pdf</a></p><p>Author: Guy Shani, Ronen I. Brafman</p><p>Abstract: Agents learning to act in a partially observable domain may need to overcome the problem of perceptual aliasing Ă˘&euro;&ldquo; i.e., different states that appear similar but require different responses. This problem is exacerbated when the agentĂ˘&euro;&trade;s sensors are noisy, i.e., sensors may produce different observations in the same state. We show that many well-known reinforcement learning methods designed to deal with perceptual aliasing, such as Utile SufÄ?Ĺš x Memory, Ä?Ĺš nite size history windows, eligibility traces, and memory bits, do not handle noisy sensors well. We suggest a new algorithm, Noisy Utile SufÄ?Ĺš x Memory (NUSM), based on USM, that uses a weighted classiÄ?Ĺš cation of observed trajectories. We compare NUSM to the above methods and show it to be more robust to noise.</p><br/>
<h2>reference text</h2><p>[1] A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. Acting optimally in partially observable stochastic domains. In AAAIĂ˘&euro;&trade;94, pages 1023Ă˘&euro;&ldquo;1028, 1994.</p>
<p>[2] L. Chrisman. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In AAAIĂ˘&euro;&trade;02, pages 183Ă˘&euro;&ldquo;188, 1992.</p>
<p>[3] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735Ă˘&euro;&ldquo; 1780, 1997.</p>
<p>[4] T. Jaakkola, S. P. Singh, and M. I. Jordan. Reinforcement learning algorithm for partially observable Markov decision problems. In NIPSĂ˘&euro;&trade;95, volume 7, pages 345Ă˘&euro;&ldquo;352, 1995.</p>
<p>[5] L.-J. Lin and T. M. Mitchell. Memory approaches to reinforcement learning in non-markovian domains. Technical Report CMU-CS-92-138, 1992.</p>
<p>[6] J. Loch and S. Singh. Using eligibility traces to Ä?Ĺš  the best memoryless policy in partially nd observable Markov decision processes. In ICMLĂ˘&euro;&trade;98, pages 323Ă˘&euro;&ldquo;331, 1998.</p>
<p>[7] A. K. McCallum. Reinforcement Learning with Selective Perception and Hidden State. PhD thesis, University of Rochester, 1996.</p>
<p>[8] N. Meuleau, L. Peshkin, K. Kim, and L. P. Kaelbling. Learning Ä?Ĺš  nite-state controllers for partially observable environments. In UAIĂ˘&euro;&trade;99, pages 427Ă˘&euro;&ldquo;436, 1999.</p>
<p>[9] D. Nikovski. State-Aggregation Algorithms for Learning Probabilistic Models for Robot Control. PhD thesis, Carnegie Mellon University, 2002.</p>
<p>[10] L. Peshkin, N. Meuleau, and L. P. Kaelbling. Learning policies with external memory. In ICMLĂ˘&euro;&trade;99, pages 307Ă˘&euro;&ldquo;314, 1999.</p>
<p>[11] S. Singh, M. L. Littman, and R. S. Sutton. Predictive representations of state. In NIPS 2001, pages 1555Ă˘&euro;&ldquo;1561, December 2001.</p>
<p>[12] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.</p>
<p>[13] J. K. Williams and S. Singh. Experimental results on learning stochastic memoryless policies for partially observable markov decision processes. In NIPS, 1998.</p>
<p>[14] A. Yeh. More accurate tests for the statistical signiÄ?Ĺš  cance of result differences. In 18th Int. Conf. on Computational Linguistics, pages 947Ă˘&euro;&ldquo;953, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
