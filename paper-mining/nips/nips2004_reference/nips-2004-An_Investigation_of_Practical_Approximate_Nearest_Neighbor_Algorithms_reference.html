<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-22" href="../nips2004/nips-2004-An_Investigation_of_Practical_Approximate_Nearest_Neighbor_Algorithms.html">nips2004-22</a> <a title="nips-2004-22-reference" href="#">nips2004-22-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>22 nips-2004-An Investigation of Practical Approximate Nearest Neighbor Algorithms</h1>
<br/><p>Source: <a title="nips-2004-22-pdf" href="http://papers.nips.cc/paper/2666-an-investigation-of-practical-approximate-nearest-neighbor-algorithms.pdf">pdf</a></p><p>Author: Ting Liu, Andrew W. Moore, Ke Yang, Alexander G. Gray</p><p>Abstract: This paper concerns approximate nearest neighbor searching algorithms, which have become increasingly important, especially in high dimensional perception areas such as computer vision, with dozens of publications in recent years. Much of this enthusiasm is due to a successful new approximate nearest neighbor approach called Locality Sensitive Hashing (LSH). In this paper we ask the question: can earlier spatial data structure approaches to exact nearest neighbor, such as metric trees, be altered to provide approximate answers to proximity queries and if so, how? We introduce a new kind of metric tree that allows overlap: certain datapoints may appear in both the children of a parent. We also introduce new approximate k-NN search algorithms on this structure. We show why these structures should be able to exploit the same randomprojection-based approximations that LSH enjoys, but with a simpler algorithm and perhaps with greater efﬁciency. We then provide a detailed empirical evaluation on ﬁve large, high dimensional datasets which show up to 31-fold accelerations over LSH. This result holds true throughout the spectrum of approximation levels. 1</p><br/>
<h2>reference text</h2><p>[1] http://kdd.ics.uci.edu/databases/CorelFeatures/CorelFeatures.data.html.</p>
<p>[2] http://www.autonlab.org/autonweb/showsoftware/154/.</p>
<p>[3] S. Arya, D. Mount, N. Netanyahu, R. Silverman, and A. Wu. An optimal algorithm for approximate nearest neighbor searching ﬁxed dimensions. Journal of the ACM, 45(6):891–923, 1998.</p>
<p>[4] Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When is “nearest neighbor” meaningful? Lecture Notes in Computer Science, 1540:217–235, 1999.</p>
<p>[5] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An efﬁcient access method for similarity search in metric spaces. In Proceedings of the 23rd VLDB International Conference, September 1997. 3 The comparison in [9] is on disk access while we compare CPU time. So strictly speaking, these results are not comparable. Nonetheless we expect them to be more or less consistent.</p>
<p>[6] K. Clarkson. Nearest Neighbor Searching in Metric Spaces: Experimental Results for sb(S). , 2002.</p>
<p>[7] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. John Wiley & Sons, 1973.</p>
<p>[8] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for ﬁnding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3):209–226, September 1977.</p>
<p>[9] A. Gionis, P. Indyk, and R. Motwani. Similarity Search in High Dimensions via Hashing. In Proc 25th VLDB Conference, 1999.</p>
<p>[10] J. Goldstein and R. Ramakrishnan. Constrast Polots and P-Sphere Trees: Speace vs. Time in Nearest Neighbor Searches. In Proc. 26th VLDB conference, 2000.</p>
<p>[11] A. Guttman. R-trees: A dynamic index structure for spatial searching. In Proceedings of the Third ACM SIGACT-SIGMOD Symposium on Principles of Database Systems. Assn for Computing Machinery, April 1984.</p>
<p>[12] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In STOC, pages 604–613, 1998.</p>
<p>[13] Piotr Indyk. High Dimensional Computational Geometry. PhD. Thesis, 2000.</p>
<p>[14] Piotr Indyk. On approximate nearest neighbors under l∞ norm. J. Comput. Syst. sci., 63(4), 2001.</p>
<p>[15] W. Johnson and J. Lindenstrauss. Extensions of lipschitz maps into a hilbert space. Contemp. Math., 26:189–206, 1984.</p>
<p>[16] Norio Katayama and Shin’ichi Satoh. The SR-tree: an index structure for high-dimensional nearest neighbor queries. pages 369–380, 1997.</p>
<p>[17] J. Kleinberg. Two Algorithms for Nearest Neighbor Search in High Dimension. In Proceedings of the Twenty-ninth Annual ACM Symposium on the Theory of Computing, pages 599–608, 1997.</p>
<p>[18] E. Kushilevitz, R. Ostrovsky, and Y. Rabani. Efﬁcient Search for Approximate Nearest Neighbors in High Dimensional Spaces. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing, 1998.</p>
<p>[19] T. Liu, A. W. Moore, A. Gray, and Ke. Yang. An investigation of practical approximate nearest neighbor algorithms (full version). Manuscript in preparation.</p>
<p>[20] B. S. Manjunath. Airphoto dataset, http://vivaldi.ece.ucsb.edu/Manjunath/research.htm.</p>
<p>[21] B. S. Manjunath and W. Y. Ma. Texture features for browsing and retrieval of large image data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(8):837–842, 1996.</p>
<p>[22] A. W. Moore. The Anchors Hierarchy: Using the Triangle Inequality to Survive HighDimensional Data. In Twelfth Conference on Uncertainty in Artiﬁcial Intelligence. AAAI Press, 2000.</p>
<p>[23] G. Mori, S. Belongie, and J. Malik. Shape contexts enable efﬁcient retrieval of similar shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2001.</p>
<p>[24] S. M. Omohundro. Efﬁcient Algorithms with Neural Network Behaviour. Journal of Complex Systems, 1(2):273–347, 1987.</p>
<p>[25] S. M. Omohundro. Bumptrees for Efﬁcient Function, Constraint, and Classiﬁcation Learning. In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information Processing Systems 3. Morgan Kaufmann, 1991.</p>
<p>[26] F. P. Preparata and M. Shamos. Computational Geometry. Springer-Verlag, 1985.</p>
<p>[27] Y. Rubnet, C. Tomasi, and L. J. Guibas. The earth mover’s distance as a metric for image retrieval. International Journal of Computer Vision, 40(2):99–121, 2000.</p>
<p>[28] Gregory Shakhnarovich, Paul Viola, and Trevor Darrell. Fast pose estimation with parameter sensitive hashing. In Proceedings of the International Conference on Computer Vision, 2003.</p>
<p>[29] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Information Processing Letters, 40:175–179, 1991.</p>
<p>[30] P. Yianilos. Excluded middle vantage point forests for nearest neighbor search. In DIMACS Implementation Challenge, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
