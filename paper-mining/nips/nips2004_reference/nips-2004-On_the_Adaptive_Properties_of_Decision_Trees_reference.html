<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>137 nips-2004-On the Adaptive Properties of Decision Trees</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-137" href="../nips2004/nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">nips2004-137</a> <a title="nips-2004-137-reference" href="#">nips2004-137-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>137 nips-2004-On the Adaptive Properties of Decision Trees</h1>
<br/><p>Source: <a title="nips-2004-137-pdf" href="http://papers.nips.cc/paper/2625-on-the-adaptive-properties-of-decision-trees.pdf">pdf</a></p><p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: Decision trees are surprisingly adaptive in three important respects: They automatically (1) adapt to favorable conditions near the Bayes decision boundary; (2) focus on data distributed on lower dimensional manifolds; (3) reject irrelevant features. In this paper we examine a decision tree based on dyadic splits that adapts to each of these conditions to achieve minimax optimal rates of convergence. The proposed classiﬁer is the ﬁrst known to achieve these optimal rates while being practical and implementable. 1</p><br/>
<h2>reference text</h2><p>[1] A. B. Tsybakov, “Optimal aggregation of classiﬁers in statistical learning,” Ann. Stat., vol. 32, no. 1, pp. 135–166, 2004.</p>
<p>[2] Y. Mansour and D. McAllester, “Generalization bounds for decision trees,” in Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, N. Cesa-Bianchi and S. Goldman, Eds., Palo Alto, CA, 2000, pp. 69–74.</p>
<p>[3] Y. Yang, “Minimax nonparametric classiﬁcation–Part I: Rates of convergence,” IEEE Trans. Inform. Theory, vol. 45, no. 7, pp. 2271–2284, 1999.</p>
<p>[4] E. Mammen and A. B. Tsybakov, “Smooth discrimination analysis,” Ann. Stat., vol. 27, pp. 1808–1829, 1999.</p>
<p>[5] A. B. Tsybakov and S. A. van de Geer, “Square root penalty: adaptation to the margin in classiﬁcation and in edge estimation,” 2004, preprint.</p>
<p>[6] P. Bartlett, M. Jordan, and J. McAuliffe, “Convexity, classiﬁcation, and risk bounds,” Department of Statistics, U.C. Berkeley, Tech. Rep. 638, 2003, to appear in Journal of the American Statistical Association.</p>
<p>[7] G. Blanchard, G. Lugosi, and N. Vayatis, “On the rate of convergence of regularized boosting classiﬁers,” J. Machine Learning Research, vol. 4, pp. 861–894, 2003.</p>
<p>[8] J. C. Scovel and I. Steinwart, “Fast rates for support vector machines,” Los Alamos National Laboratory, Tech. Rep. LA-UR 03-9117, 2004.</p>
<p>[9] G. Blanchard, C. Sch¨ fer, and Y. Rozenholc, “Oracle bounds and exact algorithm for dyadic a classiﬁcation trees,” in Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, J. Shawe-Taylor and Y. Singer, Eds. Heidelberg: Springer-Verlag, 2004, pp. 378–392.</p>
<p>[10] C. Scott and R. Nowak, “Near-minimax optimal classiﬁcation with dyadic classiﬁcation trees,” in Advances in Neural Information Processing Systems 16, S. Thrun, L. Saul, and B. Sch¨ lkopf, o Eds. Cambridge, MA: MIT Press, 2004.</p>
<p>[11] ——, “Minimax optimal classiﬁcation with dyadic decision trees,” Rice University, Tech. Rep. TREE0403, 2004. [Online]. Available: http://www.stat.rice.edu/∼cscott</p>
<p>[12] A. Nobel, “Analysis of a complexity based pruning scheme for classiﬁcation trees,” IEEE Trans. Inform. Theory, vol. 48, no. 8, pp. 2362–2368, 2002.</p>
<p>[13] J.-Y. Audibert, “PAC-Bayesian statistical learning theory,” Ph.D. dissertation, Universit´ Paris e 6, June 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
