<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-93" href="../nips2004/nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">nips2004-93</a> <a title="nips-2004-93-reference" href="#">nips2004-93-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</h1>
<br/><p>Source: <a title="nips-2004-93-pdf" href="http://papers.nips.cc/paper/2580-kernel-projection-machine-a-new-tool-for-pattern-recognition.pdf">pdf</a></p><p>Author: Laurent Zwald, Gilles Blanchard, Pascal Massart, Régis Vert</p><p>Abstract: This paper investigates the effect of Kernel Principal Component Analysis (KPCA) within the classiﬁcation framework, essentially the regularization properties of this dimensionality reduction method. KPCA has been previously used as a pre-processing step before applying an SVM but we point out that this method is somewhat redundant from a regularization point of view and we propose a new algorithm called Kernel Projection Machine to avoid this redundancy, based on an analogy with the statistical framework of regression for a Gaussian white noise model. Preliminary experimental results show that this algorithm reaches the same performances as an SVM. 1</p><br/>
<h2>reference text</h2><p>[1] P. Massart A. Barron, L. Birg´ . Risk bounds for model selection via penalization. e Proba.Theory Relat.Fields, 113:301–413, 1999.</p>
<p>[2] Baker. The numerical treatment of integral equations. Oxford:Clarendon Press, 1977.</p>
<p>[3] http://ida.ﬁrst.gmd.de/˜raetsch/data/benchmarks.htm. Benchmark repository used in several Boosting, KFD and SVM papers.</p>
<p>[4] G. Blanchard, O. Bousquet, and P.Massart. Statistical performance of support vector machines. Manuscript, 2004.</p>
<p>[5] D.L. Donoho, R.C. Liu, and B. MacGibbon. Minimax risk over hyperrectangles, and implications. Ann. Statist. 18,1416-1437, 1990.</p>
<p>[6] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. In A. J. Smola, P. L. Bartlett, B. Sch¨ lkopf, and D. Schuurmans, editors, o Advances in Large Margin Classiﬁers, pages 171–203, Cambridge, MA, 2000. MIT Press.</p>
<p>[7] V. Koltchinskii. Asymptotics of spectral projections of some random matrices approximating integral operators. Progress in Probability, 43:191–227, 1998.</p>
<p>[8] B. Sch¨ lkopf, A. J. Smola, and K.-R. M¨ ller. Nonlinear component analysis as a o u kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998.</p>
<p>[9] A. J. Smola and B. Sch¨ lkopf. On a kernel-based method for pattern recognition, o regression, approximation and operator inversion. Algorithmica, 22:211–231, 1998.</p>
<p>[10] G. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, 1990.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
