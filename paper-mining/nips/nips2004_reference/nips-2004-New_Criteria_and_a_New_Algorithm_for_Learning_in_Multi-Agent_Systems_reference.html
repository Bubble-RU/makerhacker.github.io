<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-129" href="../nips2004/nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">nips2004-129</a> <a title="nips-2004-129-reference" href="#">nips2004-129-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</h1>
<br/><p>Source: <a title="nips-2004-129-pdf" href="http://papers.nips.cc/paper/2680-new-criteria-and-a-new-algorithm-for-learning-in-multi-agent-systems.pdf">pdf</a></p><p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><br/>
<h2>reference text</h2><p>[1] Bowling, M. & Veloso, M. (2002). Multiagent learning using a variable learning rate. In Artiﬁcial Intelligence, 136, pp. 215-250.</p>
<p>[2] Brafman, R. & Tennenholtz, M. (2002). Efﬁcient Learning Equilibrium. In Advances in Neural Information Processing Systems 15.</p>
<p>[3] Brown, G. (1951). Iterative Solution of Games by Fictitious Play. In Activity Analysis of Production and Allocation. New York: John Wiley and Sons.</p>
<p>[4] Claus, C. & Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative multiagent systems. In Proceedings of the National Conference on Artiﬁcial Intelligence , pp. 746-752.</p>
<p>[5] Conitzer, V. & Sandholm, T. (2003). AWESOME: A General Multiagent Learning Algorithm that Converges in Self-Play and Learns a Best Response Against Stationary Opponents. In Proceedings of the 20th International Conference on Machine Learning, pp. 83-90, Washington, DC.</p>
<p>[6] Foster, D. & Vohra, R. (1999). Regret in the on-line decision problem. ”Games and Economic Behavior” 29:7-36.</p>
<p>[7] Fudenberg, D. & Levine, D. (1995) Universal consistency and cautious ﬁctitious play. Journal of Economics Dynamics and Control 19:1065-1089.</p>
<p>[8] Fudenberg, D. & Levine, D. (1998). The theory of learning in games. MIT Press.</p>
<p>[9] Hannan, J. (1957) Approximation to Bayes risk in repeated plays. Contributions to the Theory of Games 3:97-139.</p>
<p>[10] Hart, S. & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium. In Econometrica, Vol. 68, No. 5, pages 1127-1150.</p>
<p>[11] Hoeffding, W. (1956). On the distribution of the number of successes in independent trials. Annals of Mathematical Statistics 27:713-721.</p>
<p>[12] Littman, M. & Stone, P. (2001). Implicit Negotiation in Repeated Games. In Proceedings of the Eighth International Workshop on Agent Theories, Architectures, and Languages, pp. 393-404.</p>
<p>[13] Nudelman, E., Wortman, J., Leyton-Brown, K., & Shoham, Y. (2004). Run the GAMUT: A Comprehensive Approach to Evaluating Game-Theoretic Algorithms. AAMAS-2004. To Appear.</p>
<p>[14] Sen, S. & Weiss, G. (1998). Learning in multiagent systems. In Multiagent systems: A modern introduction to distributed artiﬁcial intelligence, chapter 6, pp. 259-298, MIT Press.</p>
<p>[15] Shoham, Y., Powers, R., & Grenager, T. (2003). Multi-Agent Reinforcement Learning: a critical survey. Technical Report.</p>
<p>[16] Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence of gradient dynamics in generalsum games. In Proceedings of UAI-2000, pp. 541-548, Morgan Kaufman.</p>
<p>[17] Stone, P. & Veloso, M. (2000). Multiagent systems: A survey from a machine learning perspective. Autonomous Robots, 8(3).</p>
<p>[18] Tesauro, G. (2004). Extending Q-Learning to General Adaptive Multi-Agent Systems. In Advances in Neural Information Processing Systems 16.</p>
<p>[19] Watkins, C. & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8(3):279-292.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
