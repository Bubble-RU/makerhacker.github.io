<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 nips-2004-The Rescorla-Wagner Algorithm and Maximum Likelihood Estimation of Causal Parameters</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-190" href="../nips2004/nips-2004-The_Rescorla-Wagner_Algorithm_and_Maximum_Likelihood_Estimation_of_Causal_Parameters.html">nips2004-190</a> <a title="nips-2004-190-reference" href="#">nips2004-190-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 nips-2004-The Rescorla-Wagner Algorithm and Maximum Likelihood Estimation of Causal Parameters</h1>
<br/><p>Source: <a title="nips-2004-190-pdf" href="http://papers.nips.cc/paper/2574-the-rescorla-wagner-algorithm-and-maximum-likelihood-estimation-of-causal-parameters.pdf">pdf</a></p><p>Author: Alan L. Yuille</p><p>Abstract: This paper analyzes generalization of the classic Rescorla-Wagner (RW) learning algorithm and studies their relationship to Maximum Likelihood estimation of causal parameters. We prove that the parameters of two popular causal models, ∆P and P C, can be learnt by the same generalized linear Rescorla-Wagner (GLRW) algorithm provided genericity conditions apply. We characterize the ﬁxed points of these GLRW algorithms and calculate the ﬂuctuations about them, assuming that the input is a set of i.i.d. samples from a ﬁxed (unknown) distribution. We describe how to determine convergence conditions and calculate convergence rates for the GLRW algorithms under these conditions.</p><br/>
<h2>reference text</h2><p>[1]. B. A. Spellman. “Conditioning Causality”. In D.R. Shanks, K.J. Holyoak, and D.L. Medin, (eds). Causal Learning: The Psychology of Learning and Motivation, Vol. 34. San Diego, California. Academic Press. pp 167-206. 1996.</p>
<p>[2]. P. Cheng. “From Covariance to Causation: A Causal Power Theory”. Psychological Review, 104, pp 367-405. 1997.</p>
<p>[3]. M. Buehner and P. Cheng. “Causal Induction: The power PC theory versus the Rescorla-Wagner theory”. In Proceedings of the 19th Annual Conference of the Cognitive Science Society”. 1997.</p>
<p>[4]. J.B. Tenenbaum and T.L. Grifﬁths. “Structure Learning in Human Causal Induction”. Advances in Neural Information Processing Systems 12. MIT Press. 2001.</p>
<p>[5]. D. Danks, T.L. Grifﬁths, J.B. Tenenbaum. “Dynamical Causal Learning”. Advances in Neural Information Processing Systems 14. 2003.</p>
<p>[6]. D. Danks. “Equilibria of the Rescorla-Wagner Model”. Journal of Mathematical Psychology. Vol. 47, pp 109-121. 2003.</p>
<p>[7]. R.A. Rescorla and A.R. Wagner. “A Theory of Pavlovian Conditioning: Variations in the Effectiveness of Reinforcement and Nonreinforcement”. In A.H. Black andW.F. Prokasy, eds. Classical Conditioning II: Current Research and Theory. New York. Appleton-Century-Crofts, pp 64-99. 1972.</p>
<p>[8]. H.J. Kushner and D.S. Clark. Stochastic Approximation for Constrained and Unconstrained Systems. New York. Springer-Verlag. 1978.</p>
<p>[9]. P. Dayan and S. Kakade. “Explaining away in weight space”. In Advances in Neural Information Processing Systems 13. 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
