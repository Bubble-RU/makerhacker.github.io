<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2004-Hierarchical Clustering of a Mixture Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-77" href="../nips2004/nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">nips2004-77</a> <a title="nips-2004-77-reference" href="#">nips2004-77-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2004-Hierarchical Clustering of a Mixture Model</h1>
<br/><p>Source: <a title="nips-2004-77-pdf" href="http://papers.nips.cc/paper/2585-hierarchical-clustering-of-a-mixture-model.pdf">pdf</a></p><p>Author: Jacob Goldberger, Sam T. Roweis</p><p>Abstract: In this paper we propose an eﬃcient algorithm for reducing a large mixture of Gaussians into a smaller mixture while still preserving the component structure of the original model; this is achieved by clustering (grouping) the components. The method minimizes a new, easily computed distance measure between two Gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters, avoiding the need for explicit resampling of datapoints. We demonstrate the method by performing hierarchical clustering of scenery images and handwritten digits. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Bar-Shalom and X. Li. Estimation and tracking: principles, techniques and software. Artech House, 1993.</p>
<p>[2] S. Gordon, H. Greenspan, and J. Goldberger. Applying the information bottleneck principle to unsupervised clustering of discrete and continuous image representations. In ICCV, 2003.</p>
<p>[3] U. Lerner, R. Parr, D. Koller, and G. Biswas. Bayesian fault detection and diagnosis in dynamic systems. In AAAI/IAAI, pp. 531–537, 2000.</p>
<p>[4] J. Puzicha, T. Hofmann, and J. Buhmann. Histogram clustering for unsupervised segmentation and image retrieval. Pattern Recognition Letters, 20(9):899–909, 1999.</p>
<p>[5] N. Shental, A. Bar-Hillel, T. Hertz, and D. Weinshall. Computing gaussian mixture models with em using equivalence constraints. In Proc. of Neural Information Processing Systems, 2003.</p>
<p>[6] N. Slonim and Y. Weiss. Maximum likelihood and the information bottleneck. In Proc. of Neural Information Processing Systems, 2003.</p>
<p>[7] E. Sudderth, A. Ihler, W. Freeman, and A. Wilsky. Non-parametric belief propagation. In CVPR, 2003.</p>
<p>[8] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pages 368–377, 1999.</p>
<p>[9] N. Vasconcelos and A. Lippman. Learning mixture hierarchies. In Proc. of Neural Information Processing Systems, 1998.</p>
<p>[10] J. Vermaak, A. A. Doucet, and P. Perez. Maintaining multi-modality through mixture tracking. In Int. Conf. on Computer Vision, 2003.</p>
<p>[11] K. Wagstaﬀ, C. Cardie, S. Rogers, and S. Schroell. Constraind k-means clustering with background knowledge. In Proc. Int. Conf. on Machine Learning, 2001.</p>
<p>[12] E.P. Xing, A. Y. Ng, M.I. Jordan, and S. Russell. Distance learning metric. In Proc. of Neural Information Processing Systems, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
