<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-9" href="../nips2004/nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">nips2004-9</a> <a title="nips-2004-9-reference" href="#">nips2004-9-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</h1>
<br/><p>Source: <a title="nips-2004-9-pdf" href="http://papers.nips.cc/paper/2583-a-method-for-inferring-label-sampling-mechanisms-in-semi-supervised-learning.pdf">pdf</a></p><p>Author: Saharon Rosset, Ji Zhu, Hui Zou, Trevor J. Hastie</p><p>Abstract: We consider the situation in semi-supervised learning, where the “label sampling” mechanism stochastically depends on the true response (as well as potentially on the features). We suggest a method of moments for estimating this stochastic dependence using the unlabeled data. This is potentially useful for two distinct purposes: a. As an input to a supervised learning procedure which can be used to “de-bias” its results using labeled data only and b. As a potentially interesting learning task in itself. We present several examples to illustrate the practical usefulness of our method.</p><br/>
<h2>reference text</h2><p>[1]  Acton, F. (1990) Numerical Methods That Work. Washington: Math. Assoc. of America.</p>
<p>[2]  Dennis, J. & Schnabel, R. (1983) Numerical Methods for Unconstrained Optimization and Nonlinear Equations. New Jersey: Prentice-Hall.</p>
<p>[3]  Heckman, J.I. (1976). The common structure of statistical models for truncation, sample selection and limited dependent variables, and a simple estimator for such models. Annals of Economic and Social Measurement 5:475-492.</p>
<p>[4]  Lee, W.S. & Liu, B. (2003). Learning with Positive and Unlabeled Examples Using Weighted Logistic Regression. ICML-03</p>
<p>[5]  Lin, Y., Lee, Y. & Wahba, G. (2000). Support vector machines for classiﬁcation in nonstandard situations. Machine Learning, 46:191-202.</p>
<p>[6]  Liu, B., Dai, Y., Li, X., Lee, W.S. & Yu, P. (2003). Building Text Classiﬁers Using Positive and Unlabeled Examples. Proceedings ICDM-03</p>
<p>[7]  Little, R. & Rubin, D. (2002). Statistical Analysis with Missing Data, 2nd Ed. . Wiley & Sons.</p>
<p>[8]  Nigam, K., McCallum , A., Thrun, S. & Mitchell, T. (2000) Text Classiﬁcation from Labeled and Unlabeled Documents using EM. Machine Learning 39(2/3):103-134.</p>
<p>[9]  Pace, R.K. & Barry, R. (1997). Sparse Spatial Autoregressions. Stat. & Prob. Let., 33 291-297.</p>
<p>[10] Vardi, Y. (1985). Empirical Distributions in Selection Bias Models. Annals of Statistics, 13.</p>
<p>[11] Zou, H., Zhu, J. & Hastie, T. (2004). Automatic Bayes Carpentary in Semi-Supervised Classiﬁcation. Unpublished.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
