<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 nips-2004-The Entire Regularization Path for the Support Vector Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-187" href="../nips2004/nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">nips2004-187</a> <a title="nips-2004-187-reference" href="#">nips2004-187-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>187 nips-2004-The Entire Regularization Path for the Support Vector Machine</h1>
<br/><p>Source: <a title="nips-2004-187-pdf" href="http://papers.nips.cc/paper/2713-the-entire-regularization-path-for-the-support-vector-machine.pdf">pdf</a></p><p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><br/>
<h2>reference text</h2><p>[1] B. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin classiﬁers. In Proceedings of COLT II, Philadelphia, PA, 1992.</p>
<p>[2] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:1–25, 1995.</p>
<p>[3] Bernard Sch¨ lkopf and Alex Smola. Learning with Kernels: Support Vector Mao chines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning). MIT Press, 2001.</p>
<p>[4] G. Wahba, Y. Lin, and H. Zhang. Gacv for support vector machines. In A.J. Smola, P.L. Bartlett, B. Sch¨ lkopf, and D. Schuurmans, editors, Advances in Large Margin o Classiﬁers, pages 297–311, Cambridge, MA, 2000. MIT Press.</p>
<p>[5] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning; Data mining, Inference and Prediction. Springer Verlag, New York, 2001.</p>
<p>[6] Thorsten Joachims. Practical Advances in Kernel Methods — Support Vector Learning, chapter Making large scale SVM learning practical. MIT Press, 1999. see http://svmlight.joachims.org.</p>
<p>[7] Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, (5):1391–1415, 2004.</p>
<p>[8] B. Efron, T. Hastie, I. Johnstone, and R.. Tibshirani. Least angle regression. Technical report, Stanford University, 2002.</p>
<p>[9] Saharon Rosset and Ji Zhu. Piecewise linear regularized solution paths. Technical report, Stanford University, 2003. http://www-stat.stanford.edu/∼saharon/papers/piecewise.ps.</p>
<p>[10] Ji Zhu, Saharon Rosset, Trevor Hastie, and Robert Tibshirani. L1 norm support vector machines. Technical report, Stanford University, 2003.</p>
<p>[11] Massimiliano Pontil and Alessandro Verri. Properties of support vector machines. Neural Comput., 10(4):955–974, 1998.</p>
<p>[12] Shai Fine and Katya Scheinberg. Incas: An incremental active set method for svm. Technical report, IBM Research Labs, Haifa, 2002.</p>
<p>[13] G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. In Advances in Neural Information Processing Systems (NIPS*2000), volume 13. MIT Press, Cambridge, MA, 2001.</p>
<p>[14] Christopher Diehl and Gert Cauwenberghs. Svm incremental learning, adaptation and optimization. In Proceedings of the 2003 International Joint Conference on Neural Networks, pages 2685–2690, 2003. Special series on Incremental Learning.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
