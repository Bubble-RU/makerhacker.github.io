<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2004-A Machine Learning Approach to Conjoint Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-8" href="../nips2004/nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">nips2004-8</a> <a title="nips-2004-8-reference" href="#">nips2004-8-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>8 nips-2004-A Machine Learning Approach to Conjoint Analysis</h1>
<br/><p>Source: <a title="nips-2004-8-pdf" href="http://papers.nips.cc/paper/2725-a-machine-learning-approach-to-conjoint-analysis.pdf">pdf</a></p><p>Author: Olivier Chapelle, Za\</p><p>Abstract: Choice-based conjoint analysis builds models of consumer preferences over products with answers gathered in questionnaires. Our main goal is to bring tools from the machine learning community to solve this problem more efﬁciently. Thus, we propose two algorithms to quickly and accurately estimate consumer preferences. 1</p><br/>
<h2>reference text</h2><p>[1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proc. 5th Annu. Workshop on Comput. Learning Theory, 1992.</p>
<p>[2] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. Technical report, University College London, 2004.</p>
<p>[3] T. Evgeniou, C. Boussios, and G. Zacharia. Generalized robust conjoint estimation. Marketing Science, 25, 2005.</p>
<p>[4] Z. Harchaoui. Statistical learning approaches to conjoint estimation. Technical report, Max Planck Institute for Biological Cybernetics, to appear.</p>
<p>[5] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classiﬁers. MIT Press, 2000.</p>
<p>[6] J. Huber and K. Zwerina. The importance of utility balance in efﬁcient choice designs. Journal of Marketing Research, 33, 1996.</p>
<p>[7] T. S. Jaakkola and D. Haussler. Probabilistic kernel regression models. In Artiﬁcial Intelligence and Statistics, 1999.</p>
<p>[8] T. S. Jaakkola and M. I. Jordan. Bayesian logistic regression: a variational approach. Statistics and Computing, 10:25–37, 2000.</p>
<p>[9] T. Jebara. Convex invariance learning. In Artiﬁcial Intelligence and Statistics, 2003.</p>
<p>[10] C. A. Micchelli and M. Pontil. Kernels for multi–task learning. In Advances in Neural Information Processing Systems 17, 2005.</p>
<p>[11] Sawtooth Software. Research paper series. Available at www.sawtoothsoftware.com/techpap.shtml#hbrel.</p>
<p>[12] B. Sch¨ lkopf and A. Smola. Learning with kernels. MIT Press, 2002. o</p>
<p>[13] A. Schwaighofer, V. Tresp, and K. Yu. Hierarchical bayesian modelling with gaussian processes. In Advances in Neural Information Processing Systems 17, 2005.</p>
<p>[14] M. Tipping. Bayesian inference: Principles and practice. In Advanced Lectures on Machine Learning. Springer, 2004.</p>
<p>[15] S. Tong and D. Koller. Support vector machine active learning with applications to text classiﬁcation. In Journal of Machine Learning Research, volume 2, 2001.</p>
<p>[16] O. Toubia, J. R. Hauser, and D. I. Simester. Polyhedral methods for adaptive choicebased conjoint analysis. Journal of Marketing Research, 41(1):116–131, 2004.</p>
<p>[17] V. Vapnik and O. Chapelle. Bounds on error expectation for support vector machines. Neural Computation, 12(9), 2000.</p>
<p>[18] C. K. I. Williams and D. Barber. Bayesian classiﬁcation with gaussian processes. IEEE Trans. Pattern Anal. Mach. Intell., 20, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
