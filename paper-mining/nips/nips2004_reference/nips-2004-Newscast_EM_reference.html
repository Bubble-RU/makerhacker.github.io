<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 nips-2004-Newscast EM</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-130" href="../nips2004/nips-2004-Newscast_EM.html">nips2004-130</a> <a title="nips-2004-130-reference" href="#">nips2004-130-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 nips-2004-Newscast EM</h1>
<br/><p>Source: <a title="nips-2004-130-pdf" href="http://papers.nips.cc/paper/2650-newscast-em.pdf">pdf</a></p><p>Author: Wojtek Kowalczyk, Nikos A. Vlassis</p><p>Abstract: We propose a gossip-based distributed algorithm for Gaussian mixture learning, Newscast EM. The algorithm operates on network topologies where each node observes a local quantity and can communicate with other nodes in an arbitrary point-to-point fashion. The main difference between Newscast EM and the standard EM algorithm is that the M-step in our case is implemented in a decentralized manner: (random) pairs of nodes repeatedly exchange their local parameter estimates and combine them by (weighted) averaging. We provide theoretical evidence and demonstrate experimentally that, under this protocol, nodes converge exponentially fast to the correct estimates in each M-step of the EM algorithm. 1</p><br/>
<h2>reference text</h2><p>¨</p>
<p>[1] R. Karp, C. Schindelhauer, S. Shenker, and B. Vocking. Randomized rumour spreading. In Proc. 41th IEEE Symp. on Foundations of Computer Science, Redondo Beach, CA, November 2000.</p>
<p>[2] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate information. In Proc. 44th IEEE Symp. on Foundations of Computer Science, Cambridge, MA, October 2003.</p>
<p>[3] M. Jelasity, W. Kowalczyk, and M. van Steen. Newscast computing. Technical report, Dept. of Computer Science, Vrije Universiteit Amsterdam, 2003. IR-CS-006.</p>
<p>[4] C. C. Moallemi and B. Van Roy. Distributed optimization in adaptive networks. In S. Thrun, L. Saul, and B. Sch¨ lkopf, editors, Advances in Neural Information Processing Systems 16. o MIT Press, Cambridge, MA, 2004.</p>
<p>[5] D. Kempe and F. McSherry. A decentralized algorithm for spectral analysis. In Proc. 36th ACM Symp. on Theory of Computing, Chicago, IL, June 2004.</p>
<p>[6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. J. Roy. Statist. Soc. B, 39:1–38, 1977.</p>
<p>[7] G. Forman and B. Zhang. Distributed data clustering can be efﬁcient and exact. ACM SIGKDD Explorations, 2(2):34–38, 2000.</p>
<p>[8] R. D. Nowak. Distributed EM algorithms for density estimation and clustering in sensor networks. IEEE Trans. on Signal Processing, 51(8):2245–2253, August 2003.</p>
<p>[9] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in graphical models, pages 355–368. Kluwer Academic Publishers, 1998.</p>
<p>[10] S. Voulgaris, D. Gavidia, and M. van Steen. Inexpensive membership management for unstructured P2P overlays. Journal of Network and Systems Management, 2005. To appear.</p>
<p>[11] J. R. J. Nunnink, J. J. Verbeek, and N. Vlassis. Accelerated greedy mixture learning. In Proc. Belgian-Dutch Conference on Machine Learning, Brussels, Belgium, January 2004.</p>
<p>[12] M. A. Paskin and C. E. Guestrin. Robust probabilistic inference in distributed systems. In Proc. 20th Int. Conf. on Uncertainty in Artiﬁcial Intelligence, Banff, Canada, July 2004.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
