<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2004-Kernels for Multi--task Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-94" href="../nips2004/nips-2004-Kernels_for_Multi--task_Learning.html">nips2004-94</a> <a title="nips-2004-94-reference" href="#">nips2004-94-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>94 nips-2004-Kernels for Multi--task Learning</h1>
<br/><p>Source: <a title="nips-2004-94-pdf" href="http://papers.nips.cc/paper/2615-kernels-for-multi-task-learning.pdf">pdf</a></p><p>Author: Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: This paper provides a foundation for multi–task learning using reproducing kernel Hilbert spaces of vector–valued functions. In this setting, the kernel is a matrix–valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix– valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi–task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation. 1</p><br/>
<h2>reference text</h2><p>[1] N.I. Akhiezer and I.M. Glazman. Theory of linear operators in Hilbert spaces, volume I. Dover reprint, 1993.</p>
<p>[2] N. Aronszajn. Theory of reproducing kernels. Trans. AMS, 686:337–404, 1950.</p>
<p>[3] J. Baxter. A Model for Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research, 12, p. 149–198, 2000.</p>
<p>[4] M. Belkin and P. Niyogi. Laplacian Eigenmaps for Dimensionality Reduction and data Representation Neural Computation, 15(6):1373–1396, 2003.</p>
<p>[5] S. Ben-David and R. Schuller. Exploiting Task Relatedness for Multiple Task Learning. Proc. of the 16–th Annual Conference on Learning Theory (COLT’03), 2003.</p>
<p>[6] T. Evgeniou and M.Pontil. Regularized Multitask Learning. Proc. of 17-th SIGKDD Conf. on Knowledge Discovery and Data Mining, 2004.</p>
<p>[7] C.A. Micchelli and M. Pontil. On Learning Vector-Valued Functions. Neural Computation, 2004 (to appear).</p>
<p>[8] C.A. Micchelli and M. Pontil. A function representation for learning in Banach spaces. Proc. of the 17–th Annual Conf. on Learning Theory (COLT’04), 2004.</p>
<p>[9] B. Sch¨ lkopf, R. Herbrich, and A.J. Smola. A Generalized Representer Theorem. Proc. of the o 14-th Annual Conf. on Computational Learning Theory (COLT’01), 2001.</p>
<p>[10] V. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
