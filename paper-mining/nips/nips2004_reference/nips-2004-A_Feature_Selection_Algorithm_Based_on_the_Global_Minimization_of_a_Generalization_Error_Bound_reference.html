<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-3" href="../nips2004/nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">nips2004-3</a> <a title="nips-2004-3-reference" href="#">nips2004-3-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</h1>
<br/><p>Source: <a title="nips-2004-3-pdf" href="http://papers.nips.cc/paper/2629-a-feature-selection-algorithm-based-on-the-global-minimization-of-a-generalization-error-bound.pdf">pdf</a></p><p>Author: Dori Peleg, Ron Meir</p><p>Abstract: A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. Highly competitive numerical results on both artiﬁcial and real-world data sets are reported. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Grandvalet and S. Canu. Adaptive scaling for feature selection in svms. In S. Thrun S. Becker and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 553– 560. MIT Press, 2003.</p>
<p>[2] Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, and Vladimir Vapnik. Feature selection for SVMs. In Advances in Neural Information Processing Systems 13, pages 668–674, 2000.</p>
<p>[3] Alain Rakotomamonjy. Variable selection using svm based criteria. The Journal of Machine Learning Research, 3:1357–1370, 2003.</p>
<p>[4] Jason Weston, Andr´ Elisseeff, Bernhard Sch¨ lkopf, and Mike Tipping. Use of the zero norm e o with linear models and kernel methods. The Journal of Machine Learning Research, 3:1439– 1461, March 2003.</p>
<p>[5] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. http://www.stanford.edu/∼boyd/cvxbook.html.</p>
<p>[6] R. Meir and T. Zhang. Generalization bounds for Bayesian mixture algorithms. Journal of Machine Learning Research, 4:839–860, 2003.</p>
<p>[7] Glenn Fung and O. L. Mangasarian. Data selection for support vector machines classiﬁers. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 64–70, 2000.</p>
<p>[8] Simon Perkins, Kevin Lacker, and James Theiler. Grafting: Fast, incremental feature selection by gradient descent in function space. Journal of Machine Learning Research, 3:1333–1356, March 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
