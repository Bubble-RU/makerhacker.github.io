<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2004-Sampling Methods for Unsupervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-158" href="../nips2004/nips-2004-Sampling_Methods_for_Unsupervised_Learning.html">nips2004-158</a> <a title="nips-2004-158-reference" href="#">nips2004-158-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>158 nips-2004-Sampling Methods for Unsupervised Learning</h1>
<br/><p>Source: <a title="nips-2004-158-pdf" href="http://papers.nips.cc/paper/2553-sampling-methods-for-unsupervised-learning.pdf">pdf</a></p><p>Author: Rob Fergus, Andrew Zisserman, Pietro Perona</p><p>Abstract: We present an algorithm to overcome the local maxima problem in estimating the parameters of mixture models. It combines existing approaches from both EM and a robust ﬁtting algorithm, RANSAC, to give a data-driven stochastic learning scheme. Minimal subsets of data points, sufﬁcient to constrain the parameters of the model, are drawn from proposal densities to discover new regions of high likelihood. The proposal densities are learnt using EM and bias the sampling toward promising solutions. The algorithm is computationally efﬁcient, as well as effective at escaping from local maxima. We compare it with alternative methods, including EM and RANSAC, on both challenging synthetic data and the computer vision problem of alpha-matting. 1</p><br/>
<h2>reference text</h2><p>[1] Ondˇej Chum, Jiˇ´ Matas, and Josef Kittler. Locally optimized ransac. In DAGM r rı 2003: Proceedings of the 25th DAGM Symposium, pages 236–243, 2003.</p>
<p>[2] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39:1–38, 1976.</p>
<p>[3] M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model ﬁtting with applications to image analysis and automated cartography. Comm. ACM, 24(6):381–395, 1981.</p>
<p>[4] S. Richardson and P.J. Green. On bayesian analysis of mixtures with an unknown number of components. Journal of the Royal Statistical Society, 59(4):731–792, 1997.</p>
<p>[5] C.V. Stewart. Robust parameter estimation. SIAM Review, 41(3):513–537, Sept. 1999.</p>
<p>[6] B. Tordoff and D.W. Murray. Guided sampling and consensus for motion estimation. In Proc. ECCV, 2002.</p>
<p>[7] P. H. S. Torr and A. Zisserman. MLESAC: A new robust estimator with application to estimating image geometry. CVIU, 78:138–156, 2000.</p>
<p>[8] N. Ueda and R. Nakano. Deterministic Annealing EM algorithm. Neural Networks, 11(2):271–282, 1998.</p>
<p>[9] N. Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton. SMEM algorithm for mixture models. Neural Computation, 12(9):2109–2128, 2000.</p>
<p>[10] G. Wei and M. Tanner. A Monte Carlo implementation of the EM algorithm. Journal American Statistical Society, 85:699–704, 1990.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
