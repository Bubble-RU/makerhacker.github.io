<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-67" href="../nips2004/nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">nips2004-67</a> <a title="nips-2004-67-reference" href="#">nips2004-67-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</h1>
<br/><p>Source: <a title="nips-2004-67-pdf" href="http://papers.nips.cc/paper/2677-exponentiated-gradient-algorithms-for-large-margin-structured-classification.pdf">pdf</a></p><p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><br/>
<h2>reference text</h2><p>[1] Long version of this paper. Available at http://www.ai.mit.edu/people/mcollins.</p>
<p>[2] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In ICML, 2003.</p>
<p>[3] Michael Collins. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Harry Bunt, John Carroll, and Giorgio Satta, editors, New Developments in Parsing Technology. Kluwer, 2004.</p>
<p>[4] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of Machine Learning Research, 2(5):265–292, 2001.</p>
<p>[5] N. Cristianini, C. Campbell, and J. Shawe-Taylor. Multiplicative updatings for support-vector learning. Technical report, NeuroCOLT2, 1998.</p>
<p>[6] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods. Cambridge University Press, 2000.</p>
<p>[7] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–63, 1997.</p>
<p>[8] J. Kivinen and M. Warmuth. Relative loss bounds for multidimensional regression problems. Journal of Machine Learning Research, 45(3):301–329, 2001.</p>
<p>[9] John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-01, 2001.</p>
<p>[10] D. Pollard. Convergence of Stochastic Processes. Springer-Verlag, 1984.</p>
<p>[11] F. Sha, L. Saul, and D. Lee. Multiplicative updates for large margin classiﬁers. In COLT, 2003.</p>
<p>[12] B. Taskar, C. Guestrin, and D. Koller. Max margin Markov networks. In NIPS, 2003.</p>
<p>[13] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. ICML, 2004 (To appear).</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
