<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-72" href="../nips2004/nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">nips2004-72</a> <a title="nips-2004-72-reference" href="#">nips2004-72-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</h1>
<br/><p>Source: <a title="nips-2004-72-pdf" href="http://papers.nips.cc/paper/2606-generalization-error-and-algorithmic-convergence-of-median-boosting.pdf">pdf</a></p><p>Author: Balázs Kégl</p><p>Abstract: We have recently proposed an extension of A DA B OOST to regression that uses the median of the base regressors as the ﬁnal regressor. In this paper we extend theoretical results obtained for A DA B OOST to median boosting and to its localized variant. First, we extend recent results on efﬁcient margin maximizing to show that the algorithm can converge to the maximum achievable margin within a preset precision in a ﬁnite number of steps. Then we provide conﬁdence-interval-type bounds on the generalization error. 1</p><br/>
<h2>reference text</h2><p>[1] B. K´ gl, “Robust regression by boosting the median,” in Proceedings of the 16th Conference on e Computational Learning Theory, Washington, D.C., 2003, pp. 258–272.</p>
<p>[2] R. E. Schapire and Y. Singer, “Improved boosting algorithms using conﬁdence-rated predictions,” Machine Learning, vol. 37, no. 3, pp. 297–336, 1999.</p>
<p>[3] R. Meir, R. El-Yaniv, and S. Ben-David, “Localized boosting,” in Proceedings of the 13th Annual Conference on Computational Learning Theory, 2000, pp. 190–199.</p>
<p>[4] B. K´ gl, “Conﬁdence-rated regression by boosting the median,” Tech. Rep. 1241, Department e of Computer Science, University of Montreal, 2004.</p>
<p>[5] G. R¨ tsch and M. K. Warmuth, “Efﬁcient margin maximizing with boosting,” Journal of Machine a Learning Research (submitted), 2003.</p>
<p>[6] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee, “Boosting the margin: a new explanation for the effectiveness of voting methods,” Annals of Statistics, vol. 26, no. 5, pp. 1651–1686, 1998.</p>
<p>[7] L. Mason, P. Bartlett, J. Baxter, and M. Frean, “Boosting algorithms as gradient descent,” in Advances in Neural Information Processing Systems. 2000, vol. 12, pp. 512–518, The MIT Press.</p>
<p>[8] J. von Neumann, “Zur Theorie der Gesellschaftsspiele,” Math. Ann., vol. 100, pp. 295–320, 1928.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
