<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 nips-2004-Integrating Topics and Syntax</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-87" href="../nips2004/nips-2004-Integrating_Topics_and_Syntax.html">nips2004-87</a> <a title="nips-2004-87-reference" href="#">nips2004-87-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 nips-2004-Integrating Topics and Syntax</h1>
<br/><p>Source: <a title="nips-2004-87-pdf" href="http://papers.nips.cc/paper/2587-integrating-topics-and-syntax.pdf">pdf</a></p><p>Author: Thomas L. Griffiths, Mark Steyvers, David M. Blei, Joshua B. Tenenbaum</p><p>Abstract: Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously ﬁnd syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classiﬁcation with models that exclusively use short- and long-range dependencies respectively. 1</p><br/>
<h2>reference text</h2><p>[1] H. J. Neville, D. L. Mills, and D. S. Lawson. Fractionating language: Different neural subsytems with different sensitive periods. Cerebral Cortex, 2:244–258, 1992.</p>
<p>[2] R. Brown. A ﬁrst language. Harvard University Press, Cambridge, MA, 1973.</p>
<p>[3] M. Redington, N. Chater, and S. Finch. Distributional information: A powerful cue for acquiring syntactic categories. Cognitive Science, 22:425–469, 1998.</p>
<p>[4] T. K. Landauer and S. T. Dumais. A solution to Plato’s problem: the Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240, 1997.</p>
<p>[5] C. Manning and H. Sch¨ tze. Foundations of statistical natural language processing. MIT Press, u Cambridge, MA, 1999.</p>
<p>[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[7] N. Coccaro and D. Jurafsky. Towards better integration of semantic predictors in statistical language modeling. In Proceedings of ICSLP-98, volume 6, pages 2403–2406, 1998.</p>
<p>[8] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of Science, 101:5228–5235, 2004.</p>
<p>[9] W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo in Practice. Chapman and Hall, Suffolk, 1996.</p>
<p>[10] H. Kucera and W. N. Francis. Computational analysis of present-day American English. Brown University Press, Providence, RI, 1967.</p>
<p>[11] R. E. Kass and A. E. Rafferty. Bayes factors. Journal of the American Statistical Association, 90:773–795, 1995.</p>
<p>[12] L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation, 2:193–218, 1985.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
