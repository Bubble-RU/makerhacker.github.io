<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2004-Computing regularization paths for learning multiple kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-42" href="../nips2004/nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">nips2004-42</a> <a title="nips-2004-42-reference" href="#">nips2004-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 nips-2004-Computing regularization paths for learning multiple kernels</h1>
<br/><p>Source: <a title="nips-2004-42-pdf" href="http://papers.nips.cc/paper/2594-computing-regularization-paths-for-learning-multiple-kernels.pdf">pdf</a></p><p>Author: Francis R. Bach, Romain Thibaux, Michael I. Jordan</p><p>Abstract: The problem of learning a sparse conic combination of kernel functions or kernel matrices for classiﬁcation or regression can be achieved via the regularization by a block 1-norm [1]. In this paper, we present an algorithm that computes the entire regularization path for these problems. The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter. Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 1</p><br/>
<h2>reference text</h2><p>[1] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm. In ICML, 2004.</p>
<p>[2] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. JMLR, 5:27–72, 2004.</p>
<p>[3] C. S. Ong, A. J. Smola, and R. C. Williamson. Hyperkernels. In NIPS 15, 2003.</p>
<p>[4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann. Stat., 32(2):407–499, 2004.</p>
<p>[5] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. In NIPS 17, 2005.</p>
<p>[6] A. Corduneanu and T. Jaakkola. Continuation methods for mixing heterogeneous sources. In UAI, 2002.</p>
<p>[7] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer-Verlag, 2001.</p>
<p>[8] E. L. Allgower and K. Georg. Continuation and path following. Acta Numer., 2:1–64, 1993.</p>
<p>[9] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge Univ. Press, 2003.</p>
<p>[10] J. F. Bonnans, J. C. Gilbert, C. Lemar´ chal, and C. A. Sagastizbal. Numerical Optimization e Theoretical and Practical Aspects. Springer, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
