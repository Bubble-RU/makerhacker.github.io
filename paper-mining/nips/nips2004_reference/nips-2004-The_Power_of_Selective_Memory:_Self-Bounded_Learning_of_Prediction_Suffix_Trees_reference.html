<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-189" href="../nips2004/nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">nips2004-189</a> <a title="nips-2004-189-reference" href="#">nips2004-189-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</h1>
<br/><p>Source: <a title="nips-2004-189-pdf" href="http://papers.nips.cc/paper/2549-the-power-of-selective-memory-self-bounded-learning-of-prediction-suffix-trees.pdf">pdf</a></p><p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><br/>
<h2>reference text</h2><p>[1] G. Bejerano and A. Apostolico. Optimal amnesic probabilistic automata, or, how to learn and classify proteins in linear time and space. Journal of Computational Biology, 7(3/4):381–393, 2000.</p>
<p>[2] P. Buhlmann and A.J. Wyner. Variable length markov chains. The Annals of Statistics, 27(2):480–513, 1999.</p>
<p>[3] K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive algorithms. In Advances in Neural Information Processing Systems 16, 2003.</p>
<p>[4] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000.</p>
<p>[5] O. Dekel, J. Keshet, and Y. Singer. Large margin hierarchical classiﬁcation. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004.</p>
<p>[6] E. Eskin. Sparse Sequence Modeling with Applications to Computational Biology and Intrusion Detection. PhD thesis, Columbia University, 2002.</p>
<p>[7] D.P. Helmbold and R.E. Schapire. Predicting nearly as well as the best pruning of a decision tree. Machine Learning, 27(1):51–68, April 1997.</p>
<p>[8] M. Kearns and Y. Mansour. A fast, bottom-up decision tree pruning algorithm with near-optimal generalization. In Proceedings of the Fourteenth International Conference on Machine Learning, 1996.</p>
<p>[9] P. Auer, N. Cesa-Bianchi and C. Gentile. Adaptive and self-conﬁdent on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48–75, 2002.</p>
<p>[10] F.C. Pereira and Y. Singer. An efﬁcient extension to mixture techniques for prediction and decision trees. Machine Learning, 36(3):183–199, 1999.</p>
<p>[11] D. Ron, Y. Singer, and N. Tishby. The power of amnesia: learning probabilistic automata with variable memory length. Machine Learning, 25(2):117–150, 1996.</p>
<p>[12] V.N. Vapnik. Statistical Learning Theory. Wiley, 1998.</p>
<p>[13] F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. The context tree weighting method: basic properties. IEEE Transactions on Information Theory, 41(3):653–664, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
