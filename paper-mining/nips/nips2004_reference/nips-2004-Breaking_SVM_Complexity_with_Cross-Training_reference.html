<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 nips-2004-Breaking SVM Complexity with Cross-Training</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-34" href="../nips2004/nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">nips2004-34</a> <a title="nips-2004-34-reference" href="#">nips2004-34-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 nips-2004-Breaking SVM Complexity with Cross-Training</h1>
<br/><p>Source: <a title="nips-2004-34-pdf" href="http://papers.nips.cc/paper/2695-breaking-svm-complexity-with-cross-training.pdf">pdf</a></p><p>Author: Léon Bottou, Jason Weston, Gökhan H. Bakir</p><p>Abstract: We propose to selectively remove examples from the training set using probabilistic estimates related to editing algorithms (Devijver and Kittler, 1982). This heuristic procedure aims at creating a separable distribution of training examples with minimal impact on the position of the decision boundary. It breaks the linear dependency between the number of SVs and the number of training examples, and sharply reduces the complexity of SVMs during both the training and prediction stages. 1</p><br/>
<h2>reference text</h2><p>Burges, C. J. C. (1996). Simpliﬁed Support Vector Decision Rules. In Saitta, L., editor, Proceedings of the 13th International Conference on Machine Learning, pages 71–77, San Mateo, CA. Morgan Kaufmann. Burges, C. J. C. (1998). A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2(2):121–167. Chang, C.-C. and Lin, C.-J. (2001). Training ν-Support Vector Classiﬁers: Theory and Algorithms. Neural Computation, 13(9):2119–2147. Devijver, P. and Kittler, J. (1982). Pattern Recogniton, A statistical approach. Prentice Hall, Englewood Cliffs. Downs, T., Gates, K. E., and Masters, A. (2001). Exact Simpliﬁcation of Support Vector Solutions. Journal of Machine Learning Research, 2:293–297. Hart, P. (1968). The condensed nearest neighbor rule. IEEE Transasctions on Information Theory, 14:515–516. ¨ Joachims, T. (1999). Making Large–Scale SVM Learning Practical. In Sch olkopf, B., Burges, C. J. C., and Smola, A. J., editors, Advances in Kernel Methods — Support Vector Learning, pages 169–184, Cambridge, MA. MIT Press. Sch¨ lkopf, B. and Smola, A. J. (2002). Learning with Kernels. MIT Press, Cambridge, MA. o Steinwart, I. (2004). Sparseness of Support Vector Machines—Some Asymptotically Sharp Bounds. ¨ In Thrun, S., Saul, L., and Scholkopf, B., editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA. Vapnik, V. N. (1995). The Nature of Statistical Learning Theory. Springer Verlag, New York. Wilson, D. L. (1972). Asymptotic properties of the nearest neighbor rules using edited data. IEEE Transactions on Systems, Man, and Cybernetics, 2:408–420.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
