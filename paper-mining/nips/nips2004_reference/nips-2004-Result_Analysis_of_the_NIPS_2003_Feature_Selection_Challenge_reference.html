<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-156" href="../nips2004/nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">nips2004-156</a> <a title="nips-2004-156-reference" href="#">nips2004-156-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</h1>
<br/><p>Source: <a title="nips-2004-156-pdf" href="http://papers.nips.cc/paper/2728-result-analysis-of-the-nips-2003-feature-selection-challenge.pdf">pdf</a></p><p>Author: Isabelle Guyon, Steve Gunn, Asa Ben-Hur, Gideon Dror</p><p>Abstract: The NIPS 2003 workshops included a feature selection competition organized by the authors. We provided participants with ﬁve datasets from diﬀerent application domains and called for classiﬁcation results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the ﬁnal competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diﬀusion trees. Other top entries used a variety of methods for feature selection, which combined ﬁlters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classiﬁcation engine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scoring software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research. 1</p><br/>
<h2>reference text</h2><p>[1] A. Blum and P. Langley. Selection of relevant features and examples in machine learning. Artiﬁcial Intelligence, 97(1-2):245–271, December 1997.</p>
<p>[2] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.</p>
<p>[3] Leo Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.</p>
<p>[4] S. Kremer, et al. NIPS 2000 unlabeled data competition. http://q.cis.uoguelph.ca/~skremer/Research/NIPS2000/, 2000.</p>
<p>[5] S. Kremer, et al. NIPS 2001 unlabeled data competition. http://q.cis.uoguelph.ca/~skremer/Research/NIPS2001/, 2001.</p>
<p>[6] I. Guyon. Design of experiments of the NIPS 2003 variable selection benchmark. http://www.nipsfsc.ecs.soton.ac.uk/papers/Datasets.pdf, 2003.</p>
<p>[7] I. Guyon and A. Elisseeﬀ. An introduction to variable and feature selection. JMLR, 3:1157–1182, March 2003.</p>
<p>[8] I. Guyon and S. Gunn. Model selection and ensemble methods challenge in preparation http://clopinet.com/isabelle/projects/modelselect.</p>
<p>[9] I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh, Editors. Feature Extraction, Foundations and Applications. Springer-Verlag, http://clopinet.com/isabelle/Projects/NIPS2003/call-for-papers.html, In preparation. See also on-line supplementary material: http://clopinet.com/isabelle/Projects/NIPS2003/analysis.html.</p>
<p>[10] D. Kazakov, L. Popelinsky, and O. Stepankova. MLnet machine learning network on-line information service. In http://www.mlnet.org.</p>
<p>[11] R. Kohavi and G. John. Wrappers for feature selection. Artiﬁcial Intelligence, 97(1-2):273–324, December 1997.</p>
<p>[12] D. LaLoudouana and M. Bonouliqui Tarare. Data set selection. In NIPS02 http://www.jmlg.org/papers/laloudouana03.pdf, 2002.</p>
<p>[13] P. M. Murphy and D. W. Aha. UCI repository of machine learning databases. In http://www.ics.uci.edu/~mlearn/MLRepository.html, 1994.</p>
<p>[14] R. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in Statistics. Springer-Verlag, New York, 1996.</p>
<p>[15] R. M. Neal. Deﬁning priors for distributions using dirichlet diﬀusion trees. Technical Report 0104, Dept. of Statistics, University of Toronto, March 2001.</p>
<p>[16] B. Schoelkopf and A. Smola. Learning with Kernels – Support Vector Machines, Regularization, Optimization and Beyond. MIT Press, Cambridge MA, 2002.</p>
<p>[17] V. Vapnik. Statistical Learning Theory. John Wiley & Sons, N.Y., 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
