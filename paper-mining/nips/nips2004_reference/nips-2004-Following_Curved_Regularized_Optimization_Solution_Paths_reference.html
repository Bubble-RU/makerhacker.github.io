<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 nips-2004-Following Curved Regularized Optimization Solution Paths</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-70" href="../nips2004/nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">nips2004-70</a> <a title="nips-2004-70-reference" href="#">nips2004-70-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>70 nips-2004-Following Curved Regularized Optimization Solution Paths</h1>
<br/><p>Source: <a title="nips-2004-70-pdf" href="http://papers.nips.cc/paper/2600-following-curved-regularized-optimization-solution-paths.pdf">pdf</a></p><p>Author: Saharon Rosset</p><p>Abstract: Regularization plays a central role in the analysis of modern data, where non-regularized ﬁtting is likely to lead to over-ﬁtted models, useless for both prediction and interpretation. We consider the design of incremental algorithms which follow paths of regularized solutions, as the regularization varies. These approaches often result in methods which are both efﬁcient and highly ﬂexible. We suggest a general path-following algorithm based on second-order approximations, prove that under mild conditions it remains “very close” to the path of optimal solutions and illustrate it with examples.</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]  Allgower, E. L. & Georg, K. (1993). Continuation and path following. Acta Numer., 2:164 Donoho, D., Johnstone, I., Kerkyachairan, G. & Picard, D. (1995). Wavelet shrinkage: Asymptopia? Annals of Statistics Efron, B., Hastie, T., Johnstone, I. & Tibshirani, R.(2004). Least Angle Regression. Annals of Statistics . Hastie, T., Rosset, S., Tibshirani, R. & Zhu, J. (2004). The Entire Regularization Path for the Support Vector Machine. Journal of Machine Learning Research, 5(Oct):1391–1415. Hastie, T., Tibshirani, R. & Friedman, J. (2001). Elements of Stat. Learning. Springer-Verlag Kim, Y & Kim, J. (2004) Gradient LASSO for feature selection. ICML-04, to appear. Rosset, S. (2003). Topics in Regularization and Boosting. PhD thesis, dept. of Statistics, Stanford University. http://www-stat.stanford.edu/˜saharon/papers/PhDThesis.pdf Rosset, S., Zhu, J. & Hastie,T. (2003). Boosting as a regularized path to a maximum margin classiﬁer. Journal of Machine Learning Research, 5(Aug):941-973. Rosset, S. & Zhu, J. (2003). Piecewise linear regularized solution paths. Submitted. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. JRSSB Wahba, G., Gu, C., Wang, Y. & Chappell, R. (1995) Soft Classiﬁcation, a.k.a. Risk Estimation, via Penalized Log Likelihood and Smoothing Spline Analysis of Variance. In D.H. Wolpert, editor, The Mathematics of Generalization. Zhu, J. & Hastie, T. (2003). Classiﬁcation of Gene Microarrays by Penalized Logistic Regression. Biostatistics, to appear. Zhu, J., Hastie, T., Rosset, S. & Tibshirani, R. (2004). 1-norm support vector machines. Neural Information Processing Systems, 16.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
