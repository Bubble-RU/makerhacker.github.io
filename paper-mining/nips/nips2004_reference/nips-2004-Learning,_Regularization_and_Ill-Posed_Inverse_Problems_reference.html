<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-96" href="../nips2004/nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">nips2004-96</a> <a title="nips-2004-96-reference" href="#">nips2004-96-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</h1>
<br/><p>Source: <a title="nips-2004-96-pdf" href="http://papers.nips.cc/paper/2722-learning-regularization-and-ill-posed-inverse-problems.pdf">pdf</a></p><p>Author: Lorenzo Rosasco, Andrea Caponnetto, Ernesto D. Vito, Francesca Odone, Umberto D. Giovannini</p><p>Abstract: Many works have shown that strong connections relate learning from examples to regularization techniques for ill-posed inverse problems. Nevertheless by now there was no formal evidence neither that learning from examples could be seen as an inverse problem nor that theoretical results in learning theory could be independently derived using tools from regularization theory. In this paper we provide a positive answer to both questions. Indeed, considering the square loss, we translate the learning problem in the language of regularization theory and show that consistency results and optimal regularization parameter choice can be derived by the discretization of the corresponding inverse problem. 1</p><br/>
<h2>reference text</h2><p>[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337–404, 1950.</p>
<p>[2] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bull. Amer. Math. Soc. (N.S.), 39(1):1–49 (electronic), 2002.</p>
<p>[3] E. De Vito, A. Caponnetto, and L. Rosasco. Discretization error analysis for Tikhonov regularization. submitted to Inverse Problem, 2004. available http://www.disi.unige.it/person/RosascoL/publications/discre iop.pdf.</p>
<p>[4] E. De Vito, A. Caponnetto, and L. Rosasco. Model selection for regularized leastsquares algorithm in learning theory. to appear on Journal Machine Learning Research, 2004.</p>
<p>[5] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. o Number 31 in Applications of mathematics. Springer, New York, 1996.</p>
<p>[6] Schock E. and Sergei V. Pereverzev. On the adaptive selection of the parameter in regularization of ill-posed problems. Technical report, University of Kaiserslautern, august 200r.</p>
<p>[7] Heinz W. Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems, volume 375 of Mathematics and its Applications. Kluwer Academic Publishers Group, Dordrecht, 1996.</p>
<p>[8] Theodoros Evgeniou, Massimiliano Pontil, and Tomaso Poggio. Regularization networks and support vector machines. Adv. Comput. Math., 13(1):1–50, 2000.</p>
<p>[9] Vera Kurkova. Supervised learning as an inverse problem. Technical Report 960, Institute of Computer Science, Academy of Sciences of the Czech Republic, April 2004.</p>
<p>[10] Colin McDiarmid. On the method of bounded differences. In Surveys in combinatorics, 1989 (Norwich, 1989), volume 141 of London Math. Soc. Lecture Note Ser., pages 148–188. Cambridge Univ. Press, Cambridge, 1989.</p>
<p>[11] S. Mukherjee, T. Niyogi, P.and Poggio, and R. Rifkin. Statistical learning: Stability is sufﬁcient for generalization and necessary and sufﬁcient for consistency of empirical risk minimization. Technical Report CBCL Paper 223, Massachusetts Institute of Technology, january revision 2004.</p>
<p>[12] T. Poggio and Girosi F. Networks for approximation and learning. Proc. IEEE, 78:1481–1497, 1990.</p>
<p>[13] Cynthia Rudin. A different type of convergence for statistical learning algorithms. Technical report, Program in Applied and Computational Mathematics Princeton University, 2004.</p>
<p>[14] I. Steinwart. Consistency of support vector machines and other regularized kernel machines. IEEE Transaction on Information Theory, 2004. (accepted).</p>
<p>[15] Andrey N. Tikhonov and Vasiliy Y. Arsenin. Solutions of ill-posed problems. V. H. Winston & Sons, Washington, D.C.: John Wiley & Sons, New York, 1977. Translated from the Russian, Preface by translation editor Fritz John, Scripta Series in Mathematics.</p>
<p>[16] Vladimir N. Vapnik. Statistical learning theory. Adaptive and Learning Systems for Signal Processing, Communications, and Control. John Wiley & Sons Inc., New York, 1998. A Wiley-Interscience Publication.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
