<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-8" href="#">nips2010-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</h1>
<br/><p>Source: <a title="nips-2010-8-pdf" href="http://papers.nips.cc/paper/3950-a-log-domain-implementation-of-the-diffusion-network-in-very-large-scale-integration.pdf">pdf</a></p><p>Author: Yi-da Wu, Shi-jie Lin, Hsin Chen</p><p>Abstract: The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. 1</p><p>Reference: <a title="nips-2010-8-reference" href="../nips2010_reference/nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 tw  Abstract The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. [sent-4, score-0.143]
</p><p>2 However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. [sent-5, score-0.301]
</p><p>3 This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. [sent-6, score-0.155]
</p><p>4 Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. [sent-7, score-0.535]
</p><p>5 A VLSI chip containing a DN with two stochastic units has been designed and fabricated. [sent-8, score-0.429]
</p><p>6 The design of component circuits will be described, so will the simulation of the full system be presented. [sent-9, score-0.165]
</p><p>7 The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. [sent-10, score-0.196]
</p><p>8 1  Introduction  In many implantable biomedical microsystems [1, 2], an embedded system capable of recognising high-dimensional, time-varying signals have been demanded. [sent-11, score-0.219]
</p><p>9 For example, recognising multichannel neural activity on-line is important for implantable brain-machine interfaces to avoid transmitting all data wirelessly, or to control prosthetic devices and to deliver bio-feedbacks in realtime [3]. [sent-12, score-0.122]
</p><p>10 The Diffusion Network (DN) proposed by Movellan is a stochastic recurrent network whose stochastic dynamics can be trained to model the probability distributions of continuous-time paths by the Monte-Carlo Expectation-Maximisation (EM) algorithm [4, 5]. [sent-13, score-0.289]
</p><p>11 As stochasticity is useful for generalising the natural variability in data [6, 7], the DN is further shown suitable for recognising noisy, continuous-time biomedical data [8]. [sent-14, score-0.118]
</p><p>12 However, the stochastic dynamics of the DN is deﬁned by a set of continuous-time, stochastic differential equations (SDEs). [sent-15, score-0.28]
</p><p>13 The speed of simulating stochastic differential equations in a digital computer is inherently limited by the serial processing and numerical iterations of the computer. [sent-16, score-0.176]
</p><p>14 Translating the DN into analogue circuits is thus of great interests for simulating the DN in real time by exploiting the natural, differential current-voltage (I-V) relationship of capacitors [9]. [sent-17, score-0.349]
</p><p>15 This paper presents the implementation of the DN in analogue Very Large Scale Integration (VLSI). [sent-18, score-0.125]
</p><p>16 To minimise the power consumption, the power supply voltage is only 1. [sent-19, score-0.359]
</p><p>17 As the reduced supply voltage limits directly the dynamic range available for voltages across capacitors, the log-domain representation proposed in [10] is applied to the DN, allowing diffusion processes to be simulated in a limited voltage ranges. [sent-21, score-0.651]
</p><p>18 After a brief 1  introduction to the DN, the following sections will derive the log-domain representation of the DN and describe its corresponding implementation in analogue VLSI. [sent-22, score-0.125]
</p><p>19 1, the DN comprises n continuous-time, continuous-valued stochastic units with fully recurrent connections. [sent-24, score-0.177]
</p><p>20 The state of the j th unit at time t, xj (t), is governed by dB(t) dxj (t) = µj xj (t) + σ · dt dt  (1)  where µj (t) is a deterministic drift term given in (2), σ a constant, and dB(t) the Brownian motion. [sent-25, score-0.34]
</p><p>21 n  µj xj (t) = κj · −ρj xj (t) + ξj +  ωij · ϕ xi (t)  (2)  i=1  ωij deﬁnes the connection weight from unit i to unit j. [sent-27, score-0.286]
</p><p>22 ξj is the input bias, and ϕ is the sigmoid function given as ϕ(xj ; a) = −1 +  2 a = tanh xj −axj 1+e 2  (3)  where a adapts the slope of the sigmoid function. [sent-29, score-0.25]
</p><p>23 The learning of the DN aims to regenerate at visible units the probability distribution of a speciﬁc set of continuous paths. [sent-32, score-0.263]
</p><p>24 The number of visible units thus equals the dimension of the data to be modeled, while the minimum number of hidden units required for modeling data satisfactorily is identiﬁed by experimental trials. [sent-33, score-0.354]
</p><p>25 During training, visible units are “clamped” to the dynamics of the training dataset, and the dynamics of hidden units are Monte-Carlo sampled for estimating optimal parameters (ωij , κj , ρj , ξj ) that maximise the expectation of training data [5]. [sent-34, score-0.52]
</p><p>26 After training, all units are given initial values at t = 0 only to sample the dynamics modeled by the DN. [sent-35, score-0.201]
</p><p>27 The similarity between the dynamics of visible units and those of training data indicate how well the DN models the data. [sent-36, score-0.287]
</p><p>28 1  Log-domain translation  To maximise the dynamic ranges for diffusion processes in VLSI, the stochastic state xj (t) is represented as a current and then logarithmically-compressed into a voltage VXj in VLSI [11]. [sent-38, score-0.613]
</p><p>29 The logarithmic compression allows xj (t) to change over three decades within a limited voltage range for VXj . [sent-39, score-0.299]
</p><p>30 The voltage representation VXj further facilitates the exploitation of the nature, differential (I-V) relationship of a capacitor to simulate SDEs in real-time and in parallel. [sent-40, score-0.382]
</p><p>31 2  The logarithmic relationship between xj (t) and VXj can be realised by the exponential I-V characteristics of a MOS transistor in subthreshold operation [12]. [sent-41, score-0.264]
</p><p>32 To keep xj (t) a non-negative value (current) in VLSI, an offset xof f is added to xj (t), resulting in the following relationship between xj (t) and VXj . [sent-42, score-0.429]
</p><p>33 xj + xof f ≡ IS · eαVXj , dxj = αIS · eαVXj · dVXj  (4)  where Is and α are process-dependent constants extractable from simulated I-V curves of transistors. [sent-43, score-0.32]
</p><p>34 n  CXj ·  dVXj σ dBj (t) −αVXj = ξj + ωij ϕ(xi ) · e−αVXj + ·e + ρj xof f · e−αVXj − ρj IS dt κj dt i=1 (5)  where CXj equals α/κj . [sent-48, score-0.237]
</p><p>35 CXj is a capacitor and VXj the voltage across the capacitor. [sent-52, score-0.303]
</p><p>36 Let (VP − VN ) and IV AR represent the differential input voltage and the input current of an EXP-element, respectively. [sent-55, score-0.347]
</p><p>37 Finally, the sigmoid circuit n transforms xj into ϕ(xj ) and the multipliers output a total current proportional to i=1 ωij · ϕ(xi ). [sent-61, score-0.39]
</p><p>38 7 6 5 4 3  7 6 5 4 3 0  100  200 300 Time samples  400  500  0 100 200 300 400 500 600 700 800 900 1000 Time samples  Figure 3: The stochastic dynamics (gray lines) regenerated by the DN trained on the bifurcating curves (black lines). [sent-62, score-0.363]
</p><p>39 Figure 4: The stochastic dynamics (gray lines) regenerated by the DN trained on the sinusoidal curve (the black line). [sent-63, score-0.339]
</p><p>40 5  6 5 4  0  20  40 Time samples  60  80  Figure 5: The stochastic dynamics (gray lines) regenerated by the DN trained on the QRS segments of electrocardiograms (black lines). [sent-70, score-0.337]
</p><p>41 5  Figure 6: The stochastic dynamics (gray lines) regenerated by the DN trained on the handwritten ρ (the black line). [sent-78, score-0.328]
</p><p>42 Adapting ρj instead of κj  The DN has been shown capable of modeling various distributions of continuous paths by adapting wij , ξj , and κj in [5]. [sent-79, score-0.15]
</p><p>43 An adaptable κj corresponds to an adaptable CXj , but a tunable capacitor with a wide linear range is not easy to implement in VLSI. [sent-80, score-0.092]
</p><p>44 (2) indicates that ρj is complementary 3  to κj in determining the “time constant” of the dynamics of the unit j, the possibility of adapting ρj instead of κj is investigated by Matlab simulation. [sent-82, score-0.178]
</p><p>45 A DN with one visible and one hidden units was proved capable of regenerating the dynamics of bifurcating curves (Fig. [sent-84, score-0.488]
</p><p>46 Moreover, a DN with only two visible units was able to regenerate the handwritten ρ satisfactorily, as illustrated in Fig. [sent-88, score-0.302]
</p><p>47 3  Parameter mappings  Table 1 summarises the parameter mappings between the numerical simulation and the VLSI implementation. [sent-94, score-0.154]
</p><p>48 The unit currents (Iunit ) of xj , ωij , and ξj are deﬁned as 10 nA to match the current scales of transistors in subthreshold operation, as well as to reduce the power consumption. [sent-97, score-0.484]
</p><p>49 Moreover, extensive simulations indicate that the dynamic ranges required for modeling various data are [−3, 5] for xj and [−30, 30] for ωij . [sent-98, score-0.129]
</p><p>50 xof f = 50nA in VLSI, VXj ranges from 773 to 827 mV. [sent-102, score-0.206]
</p><p>51 Finally, the unit capacitance for 1/κj is calculated as Cunit = Iunit · ∆tunit /VXj,unit , equaling 1 pF and resulting in CXj = α · Cunit = 30 pF. [sent-107, score-0.097]
</p><p>52 Table 1: Parameter mappings between numerical simulation and VLSI implementation parameter xj xof f VXj ω, ξ ϕ(xj ) CXj ∆t ρ  3  numeric -3∼5 5 0. [sent-108, score-0.403]
</p><p>53 5∼2  circuit -30∼50 nA 50 nA 773∼827 mV -300∼300 nA -400∼400 nA 30 pF 5 µs 0. [sent-112, score-0.109]
</p><p>54 1 ms  Circuit implementation  A DN with two stochastic units have been designed with the CMOS 0. [sent-115, score-0.241]
</p><p>55 With M1 and M2 operated in the subthreshold region, the output current is given as 1 (VP − VN ) (6) Iout = IB · exp nUT where UT denotes the thermal voltage and n the subthreshold slope factor. [sent-121, score-0.522]
</p><p>56 As the drain current (Id ) of a transistor in subthreshold operation is exponentially proportional to its gate-to-source voltage (VGS ) as Id ∝ eVGS /nUT , α = 1/nUT is extracted to be 30 by plotting log(Id ) versus VGS in SPICE. [sent-125, score-0.444]
</p><p>57 Transistors M3-M5 form an active biasing circuit that sinks IB + Iout . [sent-126, score-0.109]
</p><p>58 By adjusting the gate voltage of M3 through the negative feedback, Iout is allowed to change over several decades. [sent-127, score-0.211]
</p><p>59 In addition, 4  IOU T  VP VN  M7  IOU T  IB  IOU T  IB M5  IV AR  n actually depends on the gate voltage and introduces variability to α [13]. [sent-128, score-0.211]
</p><p>60 To prevent the variable α from introducing simulation errors, all EXP elements of the DN unit are biased with a constant IB = 100 nA. [sent-129, score-0.135]
</p><p>61 7(a), Iout of each element is then re-scaled by the one-quadrant current multiplier basing on translinear loops (Fig. [sent-131, score-0.181]
</p><p>62 IOU T  EXP IOU T  IB  VN  VP M1  IB  IV AR  M2  M3  M4  M1 VS M2  Vbiasn  Vbiasn Vref  M4  M6  M3  (a)  M5  (b)  (c)  Figure 7: The circuit diagram of the EXP element. [sent-136, score-0.181]
</p><p>63 2  Current multipliers  Four-quadrant multipliers basing on translinear loops [13] are employed to calculate Σωij ϕ(xi ) in Eq. [sent-138, score-0.179]
</p><p>64 Both ωij and ϕ(xi ) are represented by differential currents as  ωij = Iω+ − Iω− , ϕ(xi ) = Iϕ+ − Iϕ−  (7)  Let the differential current (IZ+ − IZ− ) represents the multiplier’s output and IU represent a unit current. [sent-140, score-0.351]
</p><p>65 IZ+ · IU − IZ− · IU = (Iω+ · Iϕ+ + Iω− · Iϕ− ) − (Iω+ · Iϕ− + Iω− · Iϕ+ )  (8)  Iϕ+ IU  IZ−  Figure 8: The four-quadrant current multiplier 5  Iω−  IZ+  IU Iω+  IU Iω−  Iω+  IU  Iϕ−  Iϕ+  Iϕ−  Fig. [sent-145, score-0.112]
</p><p>66 9 shows the simulation result of the four-quadrant multiplier, exhibiting satisfactory linearity over the dynamic ranges required in Table 1. [sent-146, score-0.121]
</p><p>67 0  400 Output current in nA  ϕi ϕi ϕi ϕi  200  ϕi = 0 100nA 200nA 300nA 400nA  200 100 0 -100 -200 -300 -400  0  200  -500 -600  400  -400  (Iω+ − Iω− ) in nA  Figure 9: The simulation results of the fourquadrant current multiplier 3. [sent-151, score-0.249]
</p><p>68 3  -200 0 200 Input current in nA  400  600  Figure 10: The simulation result of the sigmoid circuit with different Va  Sigmoid function ϕ(·)  Fig. [sent-152, score-0.327]
</p><p>69 11 shows the block diagram for implementing the sigmoid function in Eq. [sent-153, score-0.153]
</p><p>70 The current IXi representing xi is ﬁrstly converted into a voltage Vi by the the operational ampliﬁer(OPA) with a voltage-controlled active resistor (VCR) proposed in [14]. [sent-155, score-0.268]
</p><p>71 Finally, the 2nd generation current conveyor (CCII) in Fig. [sent-157, score-0.094]
</p><p>72 12 [15] converts the current Is into a pair of differential currents (IOU T N , IOU T P ) ranging between −400 nA and +400 nA. [sent-158, score-0.217]
</p><p>73 The differential currents are then duplicated for the inputs of four-quadrant multipliers of all DN units. [sent-159, score-0.215]
</p><p>74 Va  VCR  IXi  IOU T P OPA Vref  CCII  OTA Vref  IOU T N  Vref  Figure 11: The block diagram of the sigmoid circuit. [sent-160, score-0.153]
</p><p>75 4 Capacitor ampliﬁcation As CXi = 30 pF requires considerable chip area, CXi is implemented by the circuit in Fig. [sent-162, score-0.361]
</p><p>76 VBIAS  CX VREF  X  4/4x16  M1  X  −A  Y  Y 4/4x1  CEQ = CX (1 + A)  M2  Figure 13: The circuit diagram of the capacitor ampliﬁed by the Miller effect. [sent-169, score-0.273]
</p><p>77 3V  Figure 12: The circuit diagram of the single-to-differential current conveyor  Technology  1P6M 0. [sent-174, score-0.275]
</p><p>78 6 kHz  Figure 14: The chip layout and its speciﬁcation. [sent-181, score-0.252]
</p><p>79 3  Time in ms  Figure 16: The electrocardiogram dynamics regenerated by the DN chip in post-layout simulation (10 trials). [sent-190, score-0.593]
</p><p>80 Figure 15: The sinusoidal dynamics regenerated by the DN chip in post-layout simulation (10 trials). [sent-191, score-0.612]
</p><p>81 5  IX1 in µA  Time in ms  Figure 17: The bifurcating dynamics regenerated by the DN chip in post-layout simulation (8 trials). [sent-195, score-0.667]
</p><p>82 4  Figure 18: The handwritten ρ regenerated by the DN chip in post-layout simulation (10 trials). [sent-196, score-0.518]
</p><p>83 14 shows the chip layout of the log-domain implementation of the DN with two stochastic units, so is the speciﬁcation shown. [sent-198, score-0.344]
</p><p>84 The area of the core circuit and the capacitors are 0. [sent-199, score-0.164]
</p><p>85 The total power consumption is merely 345 µW, by the merit of low supply voltage (1. [sent-202, score-0.369]
</p><p>86 The chip has been taped out for fabrication with the CMOS 0. [sent-204, score-0.289]
</p><p>87 With one unit functioning as a visible unit and the other as a hidden unit, the parameters of the DN was programmed to regenerate the one-dimensional paths in Sec. [sent-208, score-0.312]
</p><p>88 The noise current σ · dB was κ dt simulated by a piecewise-linear current source with random amplitudes in the SPICE. [sent-211, score-0.18]
</p><p>89 15-17, the visible unit was capable of regenerating the sinusoidal waves, the electrocardiograms, and the bifurcating curves with negligible differences from Fig. [sent-213, score-0.392]
</p><p>90 Moreover, as both units functioned as visible units, the DN was capable of regenerating the handwritten ρ as Fig. [sent-215, score-0.37]
</p><p>91 These promising results demonstrate the capability of the DN chip to model the distributions of different continuous paths reliably and power-efﬁciently. [sent-217, score-0.309]
</p><p>92 After chip is fabricated in August, the chip will be tested and the measurement results will be presented in the conference. [sent-218, score-0.504]
</p><p>93 5  Conclusion  The log-domain representation of the Diffusion Network has been derived and translated into analogue VLSI circuits. [sent-219, score-0.092]
</p><p>94 Based on well-deﬁned parameter mappings, the DN chip is proved capable of regenerating various types of continuous paths, and the log-domain representation allows the diffusion processes to be simulated in real-time and within a limited dynamic range. [sent-220, score-0.534]
</p><p>95 In other words, analogue VLSI circuits are proved useful for solving (simulating) multiple SDEs in real-time and in a power-efﬁcient manner. [sent-221, score-0.177]
</p><p>96 After verifying the chip functionality, a DN chip with a scalable number of units will be further developed for recognising multi-channel, time-varying biomedical signals in implantable microsystems. [sent-222, score-0.788]
</p><p>97 Williams, “A Monte Carlo EM approach for partially observable diffusion processes: Theory and applications to neural networks,” Neural Computation, vol. [sent-276, score-0.125]
</p><p>98 Chen, “Real-time recognition of continuous-time biomedical signals using the diffusion network,” in Proc. [sent-298, score-0.169]
</p><p>99 Linares-Barranco, “Log-domain implementation of complex dynamics reaction-diffusion neural networks,” IEEE Trans. [sent-315, score-0.116]
</p><p>100 Fellrath, “CMOS analog integrated circuits based on weak inversion operation,” IEEE J. [sent-330, score-0.085]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dn', 0.414), ('vxj', 0.294), ('vlsi', 0.258), ('chip', 0.252), ('voltage', 0.211), ('iou', 0.202), ('xof', 0.165), ('cxj', 0.147), ('regenerated', 0.147), ('vref', 0.147), ('iout', 0.129), ('iu', 0.129), ('diffusion', 0.125), ('ib', 0.119), ('iz', 0.119), ('units', 0.118), ('subthreshold', 0.111), ('circuit', 0.109), ('na', 0.106), ('analogue', 0.092), ('capacitor', 0.092), ('cmos', 0.092), ('xj', 0.088), ('visible', 0.086), ('circuits', 0.085), ('dynamics', 0.083), ('currents', 0.081), ('sigmoid', 0.081), ('simulation', 0.08), ('differential', 0.079), ('ampli', 0.079), ('ij', 0.075), ('supply', 0.074), ('bifurcating', 0.074), ('iunit', 0.074), ('recognising', 0.074), ('regenerating', 0.074), ('diagram', 0.072), ('vp', 0.069), ('pf', 0.063), ('regenerate', 0.059), ('stochastic', 0.059), ('iv', 0.058), ('current', 0.057), ('paths', 0.057), ('ar', 0.056), ('unit', 0.055), ('capacitors', 0.055), ('cunit', 0.055), ('ixi', 0.055), ('opa', 0.055), ('sdes', 0.055), ('transistors', 0.055), ('vbiasn', 0.055), ('vcr', 0.055), ('multiplier', 0.055), ('multipliers', 0.055), ('capable', 0.053), ('vn', 0.05), ('sinusoidal', 0.05), ('cxi', 0.048), ('electrocardiograms', 0.048), ('implantable', 0.048), ('movellan', 0.048), ('cx', 0.048), ('consumption', 0.047), ('july', 0.045), ('biomedical', 0.044), ('db', 0.044), ('capacitance', 0.042), ('ranges', 0.041), ('adapting', 0.04), ('handwritten', 0.039), ('simulating', 0.038), ('va', 0.038), ('power', 0.037), ('mappings', 0.037), ('ccii', 0.037), ('conveyor', 0.037), ('dvxj', 0.037), ('dxj', 0.037), ('fabrication', 0.037), ('schematics', 0.037), ('translinear', 0.037), ('tunit', 0.037), ('vgs', 0.037), ('dt', 0.036), ('implementation', 0.033), ('operation', 0.033), ('basing', 0.032), ('maximise', 0.032), ('operated', 0.032), ('ota', 0.032), ('satisfactorily', 0.032), ('transistor', 0.032), ('network', 0.031), ('ms', 0.031), ('gain', 0.031), ('simulated', 0.03), ('gray', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="8-tfidf-1" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>Author: Yi-da Wu, Shi-jie Lin, Hsin Chen</p><p>Abstract: The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. 1</p><p>2 0.27919328 <a title="8-tfidf-2" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: Divisive normalization (DN) has been advocated as an effective nonlinear efﬁcient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justiﬁes DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm DN as an effective efﬁcient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small. 1</p><p>3 0.16483968 <a title="8-tfidf-3" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>4 0.077655949 <a title="8-tfidf-4" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>Author: Yung-kyun Noh, Byoung-tak Zhang, Daniel D. Lee</p><p>Abstract: We consider the problem of learning a local metric to enhance the performance of nearest neighbor classiﬁcation. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from ﬁnite sampling effects, and ﬁnd an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models. 1</p><p>5 0.06367062 <a title="8-tfidf-5" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>6 0.062157393 <a title="8-tfidf-6" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>7 0.061773192 <a title="8-tfidf-7" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>8 0.058347423 <a title="8-tfidf-8" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>9 0.05086768 <a title="8-tfidf-9" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>10 0.045085061 <a title="8-tfidf-10" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>11 0.041859776 <a title="8-tfidf-11" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>12 0.041738324 <a title="8-tfidf-12" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>13 0.041737564 <a title="8-tfidf-13" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>14 0.038234539 <a title="8-tfidf-14" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>15 0.036403749 <a title="8-tfidf-15" href="./nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">216 nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>16 0.034922369 <a title="8-tfidf-16" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>17 0.034494303 <a title="8-tfidf-17" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>18 0.033962835 <a title="8-tfidf-18" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>19 0.033861954 <a title="8-tfidf-19" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>20 0.033151504 <a title="8-tfidf-20" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, 0.027), (2, -0.077), (3, 0.083), (4, 0.023), (5, 0.054), (6, -0.006), (7, 0.027), (8, -0.008), (9, 0.0), (10, -0.016), (11, -0.027), (12, 0.058), (13, -0.06), (14, -0.076), (15, -0.031), (16, 0.014), (17, -0.032), (18, -0.036), (19, -0.021), (20, -0.032), (21, 0.017), (22, -0.097), (23, 0.074), (24, -0.052), (25, -0.044), (26, 0.139), (27, -0.036), (28, -0.088), (29, -0.156), (30, -0.079), (31, 0.128), (32, 0.023), (33, -0.271), (34, 0.068), (35, 0.15), (36, 0.248), (37, -0.005), (38, -0.039), (39, 0.013), (40, 0.003), (41, -0.031), (42, 0.068), (43, -0.147), (44, -0.076), (45, 0.089), (46, 0.031), (47, 0.121), (48, 0.055), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97243953 <a title="8-lsi-1" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>Author: Yi-da Wu, Shi-jie Lin, Hsin Chen</p><p>Abstract: The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. 1</p><p>2 0.73675185 <a title="8-lsi-2" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: Divisive normalization (DN) has been advocated as an effective nonlinear efﬁcient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justiﬁes DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm DN as an effective efﬁcient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small. 1</p><p>3 0.50902182 <a title="8-lsi-3" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>4 0.44542566 <a title="8-lsi-4" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>Author: Yung-kyun Noh, Byoung-tak Zhang, Daniel D. Lee</p><p>Abstract: We consider the problem of learning a local metric to enhance the performance of nearest neighbor classiﬁcation. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from ﬁnite sampling effects, and ﬁnd an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models. 1</p><p>5 0.41887927 <a title="8-lsi-5" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>6 0.39483345 <a title="8-lsi-6" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>7 0.3576622 <a title="8-lsi-7" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>8 0.29368559 <a title="8-lsi-8" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>9 0.2854943 <a title="8-lsi-9" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>10 0.28452468 <a title="8-lsi-10" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>11 0.28402668 <a title="8-lsi-11" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>12 0.26924163 <a title="8-lsi-12" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>13 0.2678968 <a title="8-lsi-13" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>14 0.26431921 <a title="8-lsi-14" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>15 0.25421506 <a title="8-lsi-15" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>16 0.24770269 <a title="8-lsi-16" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>17 0.22672178 <a title="8-lsi-17" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>18 0.22023799 <a title="8-lsi-18" href="./nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">29 nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<p>19 0.21804836 <a title="8-lsi-19" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>20 0.21235465 <a title="8-lsi-20" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.014), (17, 0.025), (22, 0.44), (27, 0.052), (30, 0.017), (35, 0.02), (45, 0.12), (50, 0.064), (52, 0.025), (60, 0.02), (77, 0.112), (90, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80345029 <a title="8-lda-1" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>Author: Yi-da Wu, Shi-jie Lin, Hsin Chen</p><p>Abstract: The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. 1</p><p>2 0.50284266 <a title="8-lda-2" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>3 0.42149535 <a title="8-lda-3" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>4 0.38856435 <a title="8-lda-4" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>5 0.37023014 <a title="8-lda-5" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>6 0.36987099 <a title="8-lda-6" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>7 0.36966681 <a title="8-lda-7" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>8 0.36596251 <a title="8-lda-8" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>9 0.36589411 <a title="8-lda-9" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>10 0.36018953 <a title="8-lda-10" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>11 0.35821176 <a title="8-lda-11" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>12 0.35516024 <a title="8-lda-12" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>13 0.35343224 <a title="8-lda-13" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>14 0.35305691 <a title="8-lda-14" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>15 0.3420409 <a title="8-lda-15" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>16 0.34145039 <a title="8-lda-16" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>17 0.34142655 <a title="8-lda-17" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>18 0.34114581 <a title="8-lda-18" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>19 0.34006336 <a title="8-lda-19" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>20 0.33812591 <a title="8-lda-20" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
