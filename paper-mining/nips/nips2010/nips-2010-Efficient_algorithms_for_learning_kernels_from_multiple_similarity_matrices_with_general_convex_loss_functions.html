<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-72" href="#">nips2010-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</h1>
<br/><p>Source: <a title="nips-2010-72-pdf" href="http://papers.nips.cc/paper/4131-efficient-algorithms-for-learning-kernels-from-multiple-similarity-matrices-with-general-convex-loss-functions.pdf">pdf</a></p><p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>Reference: <a title="nips-2010-72-reference" href="../nips2010_reference/nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Efﬁcient algorithms for learning kernels from multiple similarity matrices with general convex loss functions Vikram Tankasali Dept. [sent-1, score-0.466]
</p><p>2 il Visiting Professor, CWI, Amsterdam  Abstract In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. [sent-21, score-0.478]
</p><p>3 One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. [sent-25, score-0.086]
</p><p>4 This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. [sent-26, score-0.136]
</p><p>5 Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms. [sent-29, score-0.297]
</p><p>6 1 Introduction Learning procedures based on positive semideﬁnite (psd) kernel functions, like Support vector machines (SVMs), have emerged as powerful tools for several learning tasks with wide applicability [13]. [sent-30, score-0.146]
</p><p>7 In many applications it is relatively straightforward to deﬁne measures of similarity between any pair of examples but extremely difﬁcult to design a kernel function for accurate classiﬁcation. [sent-31, score-0.302]
</p><p>8 For instance, similarity score between two protein sequences given by various measures like BLAST [1], Smith-Waterman [14], etc are not psd, whence cannot be substituted as kernel. [sent-32, score-0.226]
</p><p>9 In this paper, we consider the problem of learning an optimal kernel matrix, from multiple similarity matrices, that yields accurate classiﬁcation. [sent-33, score-0.342]
</p><p>10 Let the set of n × n real symmetric matrices be denoted as Sn and the set of psd matrices as Sn . [sent-34, score-0.206]
</p><p>11 Let y ∈ {+1 , −1}n be the 1  vector of class labels and K ∈ Sn be a kernel matrix. [sent-36, score-0.122]
</p><p>12 1 Background and Related work To the best of our knowledge the problem of handling multiple similarity matrices and arbitrary convex losses has not been studied before. [sent-43, score-0.375]
</p><p>13 Existing literature has focussed on only one similarity matrix and speciﬁc choices of loss function. [sent-44, score-0.265]
</p><p>14 The problem was ﬁrst studied in [8] for a single similarity matrix. [sent-46, score-0.18]
</p><p>15 They introduced the following optimization problem min ω(K) + ρ K − S 2 , (2) F n K∈S+  where S ∈ Sn , is a similarity matrix, whose (i, j)-th element S(i, j) represents the similarity between example pair i, j and ω(K) is deﬁned in (1). [sent-47, score-0.379]
</p><p>16 By interchanging the maximization over α and minimization over K the authors note that the inner minimization admits a closed form solution: K ∗ = S + (4ρ)−1 (Y α)(Y α)⊤  +  ,  (3)  where (X)+ denotes the psd matrix obtained by clipping the negative eigen values of X to zero, i. [sent-48, score-0.283]
</p><p>17 , ⊤ ⊤ if X = n λi vi vi is the eigen decomposition of X ∈ Sn then (X)+ = n max(λi , 0)vi vi . [sent-50, score-0.339]
</p><p>18 The formulation (2) was studied further in [5] where an iterative algorithm based on solving a quadratically constrained linear program (QCLP) was proposed. [sent-52, score-0.075]
</p><p>19 In this paper we generalize F the setting in (2) by providing an algorithm which works for any convex loss function. [sent-56, score-0.116]
</p><p>20 Apart from non-applicability of the existing methods to other loss functions it is not clear how these procedures could be used to handle multiple similarity matrices. [sent-58, score-0.301]
</p><p>21 Contributions: The key contribution of the paper is to design efﬁcient procedures which can learn a kernel matrix, K ∈ Sn , from m(≥ 1) similarity matrices, possibly indeﬁnite, under general con+ vex sub-differentiable loss function by using either SVM or MKL solvers. [sent-59, score-0.383]
</p><p>22 In the ﬁrst setting we consider learning a single kernel matrix from multiple similarity matrices under a general loss function. [sent-61, score-0.498]
</p><p>23 It is a provably convergent algorithm which requires O( log n ) calls to an SVM solver. [sent-63, score-0.063]
</p><p>24 In the second setting we consider learning separate ǫ2 kernel matrix for each of the given similarity matrices and then aggregating them using a Multiple Kernel Learning (MKL) setup. [sent-64, score-0.401]
</p><p>25 We present EMKL which is based on generalizing the existing MD setup to deal with Cartesian product of psd matri2 ces. [sent-66, score-0.104]
</p><p>26 Apart from allowing general loss functions the procedures also opens up new avenues for learning multiple kernels, which could be viable alternatives to the framework proposed in [7]. [sent-70, score-0.121]
</p><p>27 Our main contribution is in section 3, where we develop mirror descent algorithms for learning kernel from multiple indeﬁnite similarity matrices. [sent-72, score-0.487]
</p><p>28 2  2 Problem formulation Given multiple similarity matrices {Si : i = 1, . [sent-75, score-0.345]
</p><p>29 , m} we consider the following formulation m  min n  K∈S+ , tr(K)=τ  f (K) ≡  Li (K − Si )  ω(K) + ρ  ,  (4)  i=1  where ρ ≥ 0 is a trade-off parameter, Li : Sn → R is a convex sub-differentiable loss function operating on K and Si . [sent-78, score-0.189]
</p><p>30 A more naturally suited formulation for handling multiple similarity matrices is to consider learning individual kernel matrix Ki from similarity matrix Si , ∀i and invoke a Multiple Kernel Learning n (MKL) setup to obtain a kernel matrix K ∈ K i βi Ki | Ki ∈ S+ , βi ≥ 0, ∀i . [sent-80, score-0.896]
</p><p>31 , Km ) = min ω(K) , (5) K∈K, tr(K)=τ  where the kernels Ki ’s are ﬁxed and βi ’s are variable. [sent-84, score-0.078]
</p><p>32 Based on MKL we consider the following kernel learning formulation m  min  K1 , . [sent-85, score-0.195]
</p><p>33 , Km ) on the Cartesian product of sets = i=1 { Ki ⊤ µij vij vij | µij ≥ 0, vij is j-th eigen vector of Si }, yields a very interesting alternative to j (6). [sent-107, score-0.678]
</p><p>34 Based on this restriction we formulate the restricted kernel learning problem as m  ℓi (µi , λi ) , min g(µ1 , . [sent-108, score-0.161]
</p><p>35 Ki = j µij vij vij , Ki ∈ Sn , + j=1 µij = τ , i = 1, . [sent-119, score-0.322]
</p><p>36 λin ]⊤ denotes the eigen values of Si and ℓi : Rn × Rn → R is a convex loss function on µi = [µi1 . [sent-125, score-0.289]
</p><p>37 We mention that the formulation (4) generalizes the existing single similarity matrix based formulations. [sent-129, score-0.262]
</p><p>38 Also the SOCP based spectrum modiﬁcation learning formulation [6] proposed in the context of single similarity matrix (m = 1) is a special case of (7). [sent-131, score-0.262]
</p><p>39 1 Entropic single kernel learning (ESKL) algorithm We denote the feasible set of kernels as K = {K ∈ Sn | tr(K) = τ } and its relative interior as + int(K) = {K ∈ Sn | tr(K) = τ, K is positive deﬁnite }. [sent-135, score-0.181]
</p><p>40 Therefore the objective function f in (4) is convex and Lipschitz continuous on K. [sent-140, score-0.085]
</p><p>41 i Thus the convex programming problem (4) satisﬁes the conditions for applying Mirror Descent (MD) [2] scheme. [sent-144, score-0.059]
</p><p>42 To apply MD procedure we require a strongly convex and continuously differentiable function ψ : K → R. [sent-145, score-0.101]
</p><p>43 With the above setup we derive an MD algorithm, named entropic single kernel learning (ESKL) algorithm, similar to the entropic mirror descent algorithm proposed in [2]. [sent-150, score-0.636]
</p><p>44 Algorithm 1 Entropic single kernel learning (ESKL) algorithm Initialization: K (1) ∈ int(K). [sent-151, score-0.122]
</p><p>45 a maximizer of the SVM dual problem (1) for kernel K = K (t) . [sent-156, score-0.143]
</p><p>46 (t) (t) • Compute eigen decomposition f ′ (K (t) ) = V (t) diag([d1 . [sent-160, score-0.198]
</p><p>47 Let f (t) denote the objective function value at t-th iteration and f ∗ be the optimal τ objective value. [sent-174, score-0.077]
</p><p>48 If the ESKL algorithm is initialized with K (1) = n I and the step-sizes are chosen 1 2 log n 2 log n then min f (t) − f ∗ ≤ τ Lip(f ) , where Lip(f ) is a as ηt = 1≤t≤T Lip(f ) t T Lipschitz constant of f such that f ′ (K (t) ) F ≤ Lip(F ) , t = 1, . [sent-175, score-0.059]
</p><p>49 2 Entropic multiple kernel learning (EMKL) algorithm Consider the kernel learning formulation (6) which minimizes the distances of kernels {Ki : i = 1, . [sent-189, score-0.397]
</p><p>50 , m} from the corresponding similarity matrices {Si : i = 1, . [sent-192, score-0.251]
</p><p>51 , m} and simultaneously learns an SVM classiﬁer by performing multiple kernel learning (MKL) on those kernels. [sent-195, score-0.162]
</p><p>52 To learn a non-sparse combination of kernels the following MKL formulation has been proposed in [10]: Ω(K1 , . [sent-196, score-0.113]
</p><p>53 , Km ) = max max γ ∈△m α∈A i 1 1 Fi (Ki ; α, γ) = m 1⊤ α − 2γi tr Ki Y αα⊤ Y  (11) + ρ Li (Ki − Si ) , i = 1, . [sent-209, score-0.06]
</p><p>54 Thus the objective function F in (6) is convex and Lipschitz continuous on Km . [sent-227, score-0.085]
</p><p>55 We develop a novel Mirror Descent procedure for problem (6) by deﬁning a strongly convex and continuously differentiable function Ψ on the product space Km as Ψ(K) =  m i=1  n j=1  λi,j log λi,j , K ∈ Km ,  (14)  where (λi,1 , . [sent-266, score-0.143]
</p><p>56 The resulting algorithm, named entropic multiple kernel learning (EMKL), is given below. [sent-270, score-0.353]
</p><p>57 Algorithm 2 Entropic multiple kernel learning (EMKL) algorithm (1)  Initialization: Ki ∈ int(K), i = 1, . [sent-271, score-0.162]
</p><p>58 i γ (t)  (t+1)  :=  • λi,j  (t+1)  := • Ki end for until Convergence  i  (t)  • Find eigen decomposition ∂Ki Fi (Ki ; α∗ , γ ∗ ) = Vi  (t)  (t)  (t) ⊤  diag([di,1 . [sent-282, score-0.198]
</p><p>59 Let F (t) denote the objective function value at t-th iteration and F ∗ be the optimal (1) τ objective value. [sent-296, score-0.077]
</p><p>60 If the EMKL algorithm is initialized with Ki = n I , ∀ i and the step-sizes are 1 2 log n 2 log n then min F (t) − F ∗ ≤ τ m Lip(F ) , where chosen as ηt = 1≤t≤T Lip(F ) mt T (t) Lip(F ) is a Lipschitz constant of F such that ∂Ki F (Ki ; α, γ) F ≤ Lip(F ) , ∀i, t. [sent-297, score-0.079]
</p><p>61 3 Restricted entropic kernel learning (REKL) algorithm The proposed EMKL algorithm is computationally expensive as it computes eign decomposition of m matrices of dimension n × n at every iteration. [sent-315, score-0.378]
</p><p>62 Here we propose an efﬁcient algorithm by considering the restricted kernel learning formulation (7) where Ω(K1 , . [sent-316, score-0.176]
</p><p>63 We denote the feasible set for µi as X := {µi ∈ Rn | µij ≥ 0, ∀j, j=1 µij = τ }, n which is a convex compact subset of R . [sent-321, score-0.077]
</p><p>64 We note that Ω in (10) when viewed as a function of m µi ’s, is a convex function on the Cartesian product space X m := i=1 X . [sent-322, score-0.081]
</p><p>65 The loss function ℓi is assumed to be a convex function of µi with bounded sub-gradients. [sent-323, score-0.116]
</p><p>66 Hence, the objective function g in (7) is convex and Lipschitz continuous over the compact space X m . [sent-324, score-0.103]
</p><p>67 , ∂µnn Ω ), where ∂µij Ω = − 2 1 ∗ α∗⊤ Y vij vij Y α∗ . [sent-332, score-0.322]
</p><p>68 We derive an MD γi algorithm, named restricted entropic kernel learning (REKL), by extending the entropic mirror descent scheme [2] to deal with Cartesian product of simplices. [sent-333, score-0.64]
</p><p>69 This is achieved by deﬁning a strongly convex and continuously differentiable function ψe : X m → R as m i=1  ψe (µ1 , . [sent-334, score-0.101]
</p><p>70 (16)  Algorithm 3 Restricted entropic kernel learning (REKL) algorithm Find eigen decomposition: Si =  j  ⊤ λij vij vij , i = 1, . [sent-338, score-0.777]
</p><p>71 repeat t= t+1 (t) ⊤ Obtain α∗ , γ ∗ by solving the MKL problem (10) with Ki = j µij vij vij , i = 1, . [sent-347, score-0.322]
</p><p>72 for i = 1 to m do (t) (t) ⊤ • Compute sub-gradient g ′ ij := − 2 1 ∗ α∗⊤ Y vij vij Y α∗ + ∂µij ℓi (µi , λi ). [sent-351, score-0.411]
</p><p>73 γ i  (t)  (t)  (t+1)  • µij  :=  τ µij exp −ηt g ′ ij n l=1  (t)  (t)  , j = 1, . [sent-352, score-0.089]
</p><p>74 Let g (t) denote the objective function value at t-th iteration and g ∗ be the optimal (1) τ objective value. [sent-358, score-0.077]
</p><p>75 If the REKL algorithm is initialized with µi = n 1 , ∀ i and the step-sizes are 1 2 log n 2 log n chosen as ηt = then min g (t) − g ∗ ≤ τ m Lip(g) , where Lip(g) 1≤t≤T Lip(g) mt T (t) is a Lipschitz constant of g such that |g ′ ij | ≤ Lip(g) , i, = 1, . [sent-359, score-0.168]
</p><p>76 4 Discussion The ESKL formulation requires O log n iterations (see Proposition 3. [sent-373, score-0.074]
</p><p>77 4 Experiments and Results In this section we experimentally compare the proposed kernel learning formulations against IndSVM [8] and the eigen transformation methods: Denoise, Flip, Shift [15]. [sent-379, score-0.363]
</p><p>78 Given an indeﬁnite ⊤ similarity matrix S with eigen-decomposition S = j λj vj vj , eigen transformation methods gen⊤ erate kernel matrix as K := j µj vj vj , where choice µj ’s are: (a) Denoise: µj = max(λj , 0), 6  (b) Flip: µj = |λj |, (c) Shift: µj = λj − δ, where δ = min{λ1 , . [sent-380, score-0.679]
</p><p>79 We have generated the indeﬁnite similarity matrices as prescribed in [16] for each of the following data sets: Sonar, Liver disorder, Ionosphere, Diabetes and Heart. [sent-394, score-0.251]
</p><p>80 We have used the same similarity matrices as in [6]:1 for the data sets Amazon, AuralSonar, Yeast-SW-5-7 and Yeast-SW-5-12 . [sent-395, score-0.251]
</p><p>81 To test the proposed multiple similarity based formulations we experimented on a subset of the SCOP database [9] taken from Protein Classiﬁcation Benchmark Collection 2 . [sent-396, score-0.26]
</p><p>82 Considering proteins having < 40% sequence identity, we randomly select 8 super-families which have at least 45 proteins. [sent-397, score-0.069]
</p><p>83 We compute 3 different pairwise similarity measure for proteins: Psi-BLAST [1], Smith-Waterman [14] and Needleman-Wunsch [11]. [sent-398, score-0.18]
</p><p>84 The similarity matrices obtained from these 3 similarity measures are indeﬁnite in general. [sent-399, score-0.431]
</p><p>85 2 Effect of various loss functions We experimentally demonstrate the ability of the proposed ESKL algorithm in handling general convex loss function. [sent-401, score-0.198]
</p><p>86 We observe that on Liver disorder data set L2 loss performs better than L1 , L3 . [sent-403, score-0.098]
</p><p>87 From Table 2 we observe that on AuralSonar data set ESKL formulation works best with L3 loss function. [sent-405, score-0.111]
</p><p>88 3 Combining multiple sequence similarity matrices for Proteins Consider the task of classifying proteins into super-families when multiple sequence similarity measures are available. [sent-481, score-0.58]
</p><p>89 6  originally proposed to handle single similarity matrix, to multiple similarity matrices by averaging over the similarity matrices. [sent-672, score-0.651]
</p><p>90 We also compare with a multiple kernel learning formulation, simple MKL [12]. [sent-674, score-0.162]
</p><p>91 Denoised version of the similarity matrices are given as input to simple MKL. [sent-675, score-0.251]
</p><p>92 In Table 3 the proposed multiple similarity based kernel learning algorithms ESKL / EMKL / REKL are compared with the other methods mentioned above. [sent-676, score-0.342]
</p><p>93 5 Conclusion We have proposed three formulations, (4), (6), (7) for learning kernels from multiple similarity matrices. [sent-679, score-0.279]
</p><p>94 The key advantages of the proposed algorithms over the state of the art are: (i) require only SVM / MKL solvers and does not require any other sophisticated tools; (ii) the algorithms are applicable for a wide choice of loss functions and multiple similarity functions. [sent-680, score-0.298]
</p><p>95 Gapped blast and psiblast: a new generation of protein database search programs. [sent-691, score-0.082]
</p><p>96 Mirror descent and nonlinear projected subgradient methods for convex optimization. [sent-696, score-0.1]
</p><p>97 The ordered subsets mirror descent optimization method with applications to tomography. [sent-702, score-0.145]
</p><p>98 Scop: a structural classiﬁcation of proteins database for the investigation of sequences and structures. [sent-750, score-0.069]
</p><p>99 On the algorithmics and applications of a mixed-norm based kernel learning formulation. [sent-760, score-0.122]
</p><p>100 An analysis of transformation on non-positive semideﬁnite similarity matrix for kernel machines. [sent-791, score-0.358]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ki', 0.501), ('km', 0.333), ('emkl', 0.271), ('eskl', 0.253), ('mkl', 0.246), ('lip', 0.222), ('rekl', 0.217), ('similarity', 0.18), ('eigen', 0.173), ('vij', 0.161), ('entropic', 0.16), ('kernel', 0.122), ('mirror', 0.104), ('inde', 0.1), ('sn', 0.097), ('indsvm', 0.09), ('ij', 0.089), ('fi', 0.089), ('si', 0.081), ('denoise', 0.079), ('md', 0.078), ('svm', 0.073), ('matrices', 0.071), ('proteins', 0.069), ('psd', 0.064), ('flip', 0.064), ('tr', 0.06), ('kernels', 0.059), ('cartesian', 0.059), ('convex', 0.059), ('loss', 0.057), ('auralsonar', 0.054), ('formulation', 0.054), ('vi', 0.047), ('protein', 0.046), ('lipschitz', 0.046), ('molecular', 0.042), ('socp', 0.041), ('disorder', 0.041), ('indian', 0.041), ('descent', 0.041), ('int', 0.041), ('formulations', 0.04), ('multiple', 0.04), ('bhattacharyya', 0.039), ('diabetes', 0.039), ('accp', 0.036), ('achintya', 0.036), ('blast', 0.036), ('schffer', 0.036), ('scop', 0.036), ('vikram', 0.036), ('liver', 0.036), ('odd', 0.036), ('diag', 0.035), ('automation', 0.033), ('chiranjib', 0.032), ('named', 0.031), ('vj', 0.03), ('rows', 0.03), ('shift', 0.029), ('classi', 0.028), ('transformation', 0.028), ('matrix', 0.028), ('li', 0.027), ('objective', 0.026), ('solves', 0.026), ('amazon', 0.026), ('faculty', 0.026), ('sonar', 0.026), ('iteration', 0.025), ('decomposition', 0.025), ('rn', 0.025), ('handling', 0.025), ('continuously', 0.024), ('procedures', 0.024), ('calls', 0.024), ('ionosphere', 0.023), ('libsvm', 0.023), ('biology', 0.023), ('nite', 0.022), ('product', 0.022), ('proposition', 0.021), ('sophisticated', 0.021), ('maximizer', 0.021), ('quadratically', 0.021), ('mt', 0.02), ('log', 0.02), ('semide', 0.02), ('restriction', 0.02), ('convergent', 0.019), ('heart', 0.019), ('min', 0.019), ('compact', 0.018), ('bregman', 0.018), ('setup', 0.018), ('differentiable', 0.018), ('find', 0.018), ('il', 0.018), ('maximization', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="72-tfidf-1" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>2 0.20547181 <a title="72-tfidf-2" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>Author: Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan</p><p>Abstract: Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efﬁciently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on ﬁfty thousand points in less than half an hour on a single core. 1</p><p>3 0.20134841 <a title="72-tfidf-3" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>4 0.1941916 <a title="72-tfidf-4" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>Author: Kun Gai, Guangyun Chen, Chang-shui Zhang</p><p>Abstract: In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of norm constraints on combination coefﬁcients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method signiﬁcantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches. 1</p><p>5 0.10493518 <a title="72-tfidf-5" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>6 0.092675515 <a title="72-tfidf-6" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>7 0.091114238 <a title="72-tfidf-7" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>8 0.08163967 <a title="72-tfidf-8" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>9 0.074344195 <a title="72-tfidf-9" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>10 0.073681213 <a title="72-tfidf-10" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>11 0.072842464 <a title="72-tfidf-11" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>12 0.071525343 <a title="72-tfidf-12" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>13 0.070800021 <a title="72-tfidf-13" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>14 0.067406073 <a title="72-tfidf-14" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>15 0.066493794 <a title="72-tfidf-15" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>16 0.061634056 <a title="72-tfidf-16" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>17 0.056214459 <a title="72-tfidf-17" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>18 0.053905025 <a title="72-tfidf-18" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>19 0.053732499 <a title="72-tfidf-19" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>20 0.051800866 <a title="72-tfidf-20" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.056), (2, 0.079), (3, -0.05), (4, 0.172), (5, 0.064), (6, 0.255), (7, -0.031), (8, 0.077), (9, 0.039), (10, -0.072), (11, 0.012), (12, 0.016), (13, 0.06), (14, 0.003), (15, 0.028), (16, -0.104), (17, 0.086), (18, -0.004), (19, 0.058), (20, 0.066), (21, 0.057), (22, 0.048), (23, 0.017), (24, 0.024), (25, -0.022), (26, -0.008), (27, 0.045), (28, 0.068), (29, 0.119), (30, -0.093), (31, 0.02), (32, -0.018), (33, -0.098), (34, -0.058), (35, -0.026), (36, 0.046), (37, -0.039), (38, -0.049), (39, -0.024), (40, -0.063), (41, 0.033), (42, 0.002), (43, 0.033), (44, -0.044), (45, -0.015), (46, -0.005), (47, 0.085), (48, 0.058), (49, -0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92349124 <a title="72-lsi-1" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>2 0.81886822 <a title="72-lsi-2" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>Author: Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan</p><p>Abstract: Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efﬁciently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on ﬁfty thousand points in less than half an hour on a single core. 1</p><p>3 0.78289527 <a title="72-lsi-3" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>Author: Kun Gai, Guangyun Chen, Chang-shui Zhang</p><p>Abstract: In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of norm constraints on combination coefﬁcients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method signiﬁcantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches. 1</p><p>4 0.75924677 <a title="72-lsi-4" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>5 0.61839056 <a title="72-lsi-5" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>Author: Prateek Jain, Brian Kulis, Inderjit S. Dhillon</p><p>Abstract: In this paper we consider the problem of semi-supervised kernel function learning. We ﬁrst propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. To this end, we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the k-NN classiﬁcation accuracy signiﬁcantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique signiﬁcantly reduces the dimensionality of the feature space while achieving competitive classiﬁcation accuracies. 1</p><p>6 0.55872703 <a title="72-lsi-6" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>7 0.43294755 <a title="72-lsi-7" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>8 0.4161711 <a title="72-lsi-8" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>9 0.39950395 <a title="72-lsi-9" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>10 0.39514339 <a title="72-lsi-10" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>11 0.38347957 <a title="72-lsi-11" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>12 0.37892064 <a title="72-lsi-12" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>13 0.35766363 <a title="72-lsi-13" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>14 0.34812492 <a title="72-lsi-14" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>15 0.31978115 <a title="72-lsi-15" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>16 0.31633291 <a title="72-lsi-16" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>17 0.31322452 <a title="72-lsi-17" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>18 0.3005892 <a title="72-lsi-18" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>19 0.29752901 <a title="72-lsi-19" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>20 0.29502335 <a title="72-lsi-20" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.045), (17, 0.022), (27, 0.023), (30, 0.041), (35, 0.017), (45, 0.195), (50, 0.051), (52, 0.031), (60, 0.041), (77, 0.031), (78, 0.012), (90, 0.04), (93, 0.364)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72083008 <a title="72-lda-1" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>2 0.6382668 <a title="72-lda-2" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of estimating the Fα -measure of a given model as accurately as possible on a ﬁxed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reﬂect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. 1</p><p>3 0.60308492 <a title="72-lda-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.51872408 <a title="72-lda-4" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Many statistical M -estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer. We analyze the convergence rates of ﬁrst-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We deﬁne appropriately restricted versions of these conditions, and show that they are satisﬁed with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov’s ﬁrst-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ∗ and the optimal solution θ. This globally linear rate is substantially faster than previous analyses of global convergence for speciﬁc methods that yielded only sublinear rates. Our analysis applies to a wide range of M -estimators and statistical models, including sparse linear regression using Lasso ( 1 regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational eﬃciency in high-dimensional estimation. 1</p><p>5 0.51832706 <a title="72-lda-5" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>6 0.51685959 <a title="72-lda-6" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>7 0.51670176 <a title="72-lda-7" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>8 0.51647353 <a title="72-lda-8" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>9 0.51619321 <a title="72-lda-9" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>10 0.5160318 <a title="72-lda-10" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>11 0.51579171 <a title="72-lda-11" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>12 0.51543713 <a title="72-lda-12" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>13 0.51543194 <a title="72-lda-13" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>14 0.51522076 <a title="72-lda-14" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>15 0.5150041 <a title="72-lda-15" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>16 0.51496667 <a title="72-lda-16" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>17 0.51486772 <a title="72-lda-17" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>18 0.51428628 <a title="72-lda-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.51407683 <a title="72-lda-19" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>20 0.51391596 <a title="72-lda-20" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
