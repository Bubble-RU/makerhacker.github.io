<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2010-Feature Set Embedding for Incomplete Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-94" href="#">nips2010-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2010-Feature Set Embedding for Incomplete Data</h1>
<br/><p>Source: <a title="nips-2010-94-pdf" href="http://papers.nips.cc/paper/4047-feature-set-embedding-for-incomplete-data.pdf">pdf</a></p><p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>Reference: <a title="nips-2010-94-reference" href="../nips2010_reference/nips-2010-Feature_Set_Embedding_for_Incomplete_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. [sent-3, score-0.801]
</p><p>2 In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. [sent-4, score-0.847]
</p><p>3 In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. [sent-5, score-0.765]
</p><p>4 Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. [sent-7, score-0.463]
</p><p>5 The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. [sent-8, score-0.326]
</p><p>6 This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. [sent-9, score-0.678]
</p><p>7 1  Introduction  Many applications require classiﬁcation techniques dealing with train and/or test instances with missing features: e. [sent-10, score-0.753]
</p><p>8 a churn predictor might deal with incomplete log features for new customers, a spam ﬁlter might be trained from data originating from servers storing different features, a face detector might deal with images for which high resolution cues are corrupted. [sent-12, score-0.345]
</p><p>9 In this work, we address a learning setting in which the missing features are either missing at random [6], i. [sent-13, score-1.329]
</p><p>10 deletion due to corruption or noise, or structurally missing [4], i. [sent-15, score-0.697]
</p><p>11 some features do not make sense for some examples, e. [sent-17, score-0.221]
</p><p>12 We do not consider setups in which the features are maliciously deleted to fool the classiﬁer [5]. [sent-20, score-0.404]
</p><p>13 Techniques for dealing with incomplete data fall mainly into two categories: techniques which impute the missing features and techniques considering an instance-speciﬁc subspace. [sent-21, score-0.901]
</p><p>14 In this case, the data instances are viewed as feature vectors in a high-dimensional space and the classiﬁer is a function from this space into the discrete set of classes. [sent-23, score-0.289]
</p><p>15 Prior to classiﬁcation, the missing vector components need to be imputed. [sent-24, score-0.554]
</p><p>16 Early imputation approaches ﬁll any missing value with a constant, zero or the average of the feature over the observed cases [18]. [sent-25, score-0.887]
</p><p>17 Along this line, more complex strategies based on generative models have been used to ﬁll missing features according to the most likely value given the observed features. [sent-27, score-0.857]
</p><p>18 Building upon this generative model strategy, several approaches have considered integrating out the missing values, either by integrating the loss [2] or the decision function [22]. [sent-29, score-0.629]
</p><p>19 15 Feature B missing Feature C missing Feature D missing  p(F, 0. [sent-34, score-1.662]
</p><p>20 Each pair is mapped into an embedding space, then the embedded vectors are combined into a single vector (either linearly with mean or non-linearly with max). [sent-43, score-0.497]
</p><p>21 Our learning procedure learns both the embedding space and the linear classiﬁer jointly. [sent-45, score-0.326]
</p><p>22 In this work, we propose a novel strategy called feature set embedding. [sent-53, score-0.259]
</p><p>23 Contrary to previous work, we do not consider instances as vectors from a given feature space. [sent-54, score-0.255]
</p><p>24 For that purpose, we introduce a model which maps each (feature, value) pair onto an embedding space and combines the embedded pairs into a single vector before applying a linear classiﬁer, see Figure 1. [sent-56, score-0.463]
</p><p>25 The embedding space mapping and the linear classiﬁer are jointly learned to maximize the conditional probability of the label given the observed input. [sent-57, score-0.326]
</p><p>26 Contrary to previous work, this set embedding framework naturally handles incomplete data without modeling the missing feature distribution, or considering an instance speciﬁc decision function. [sent-58, score-1.172]
</p><p>27 Compared to other work on learning from sets, our approach is original as it proposes to learn to embed set elements and to classify sets as a single optimization problem, while prior strategies learn their decision function considering a ﬁxed mapping from sets into a feature space [12, 3]. [sent-59, score-0.386]
</p><p>28 At the lower level, (feature, value) pairs are |X| individually mapped into an embedding space of dimension m: given an example X = {(fi , vi )}i=1 , m a function p predicts an embedding vector pi = p(fi , vi ) ∈ R for each feature value pair (fi , vi ). [sent-79, score-1.158]
</p><p>29 At the upper level, the embedded vectors are combined to make the class prediction: a function h takes |X| |X| the set of embedded vectors {pi }i=1 and predicts a vector of conﬁdence values h({pi }i=1 ) ∈ Rk in which the correct class should be assigned the highest value. [sent-80, score-0.301]
</p><p>30 1  Feature Embedding  Feature embedding offers great ﬂexibility. [sent-85, score-0.326]
</p><p>31 For discrete features, the simplest embedding strategy learns a distinct parameter vector for each (f, v) pair, i. [sent-87, score-0.457]
</p><p>32 When the feature values are continuous, we adopt a similar strategy and deﬁne (a)  p(f, v) = W  Lf (b) vLf  (a)  (a)  (b)  Lf ∈ Rl and Lf ∈ Rl l(a) + l(b) = l  where  (a)  (b)  (3)  (b)  where Lf informs about the presence of feature f , while vLf informs about its value. [sent-93, score-0.534]
</p><p>33 When the dataset contains a mix of continuous and discrete features, both embedding approaches can be used jointly. [sent-97, score-0.36]
</p><p>34 Feature embedding is hence a versatile strategy; the practitioner deﬁnes the model parameterization according to the nature of the features, and the learned parameters L and W encode the correlation between features. [sent-98, score-0.326]
</p><p>35 2  Classifying from an Embedded Feature Set  The second level of our architecture h considers the set of embedded features and predicts a vector |X| of conﬁdence values. [sent-100, score-0.413]
</p><p>36 Hence, in this case, the dimension of the embedding space m bounds the rank of the matrix V W . [sent-106, score-0.37]
</p><p>37 In the speciﬁc case where features are continuous and no presence information is provided, (b) i. [sent-108, score-0.254]
</p><p>38 Lf,v = vLf , our model is equivalent to a classical linear classiﬁer operating on feature vectors when all features are present, i. [sent-110, score-0.427]
</p><p>39 Intuitively, each dimension in the embedding space provides a meta-feature describing each (feature, value) pair, the max operator then outputs the best meta-feature match over the set of (feature, value) pairs, performing a kind of soft-OR, i. [sent-124, score-0.358]
</p><p>40 3  Experiments  Our experiments consider different setups: features missing at train and test time, features missing only at train time, features missing only at test time. [sent-143, score-2.625]
</p><p>41 1  Missing Features at Train and Test Time  The setup in which features are missing at train and test time is relevant to applications suffering sensor failure or communication errors. [sent-147, score-0.984]
</p><p>42 It is also relevant to applications in which some features are 4  UCI sick pima hepatitis echo hypo MNIST-5-vs-6 Cars USPS Physics Mine MNIST-miss-test† MNIST-full †  Table 1: Dataset Statistics Train set Test set # eval. [sent-148, score-0.453]
</p><p>43 (%) 90 90 90 90 90 25 62 85 85 26 0 to 99† 0 to 87  Continuous or discrete c c c c c d d c c c d d  Features missing only at training time for USPS, Physics and Mine. [sent-151, score-0.662]
</p><p>44 For each dataset, 90% of the features are removed at random. [sent-162, score-0.221]
</p><p>45 Contrary to UCI, the deleted features have some structure; for each example, a square area covering 25% of the image surface is removed at random. [sent-164, score-0.275]
</p><p>46 This task presents a problem where some features are structurally missing. [sent-166, score-0.302]
</p><p>47 For each example, regions of interests corresponding to potential car parts are detected, and features are extracted for each region. [sent-167, score-0.256]
</p><p>48 Hence, at most 1900 = 19 × 10 × 10 features are provided for each image. [sent-171, score-0.221]
</p><p>49 These baselines are all variants of Support Vector Machines (SVMs), suitable for the missing feature problem. [sent-174, score-0.716]
</p><p>50 Flag relies on the Zero imputation but complements the examples with binary features indicating whether each feature was observed or imputed. [sent-176, score-0.595]
</p><p>51 Finally, geom is a subspace-based strategy [4]; for each example, a classiﬁer in the subspace corresponding to the observed features is considered. [sent-177, score-0.427]
</p><p>52 For each experiment, the hyperparameters of our model l, m and the number of training iterations are validated by ﬁrst training the model on 4/5 of the training data and assessing it on the remainder of the training data. [sent-179, score-0.373]
</p><p>53 In the case of structurally missing features, the car experiment shows a substantial advantage for FSE over the second best approach geom, which was speciﬁcally introduced for this kind of setup. [sent-185, score-0.67]
</p><p>54 We therefore solely validate non-linear FSE in the following: For feature embedding of continuous data, feature presence information has proven to be useful in all cases, i. [sent-201, score-0.72]
</p><p>55 For feature embedding of discrete data, sharing parameters across different values of the same feature, i. [sent-205, score-0.574]
</p><p>56 We also relied on sharing parameters across different features with the same value, i. [sent-209, score-0.273]
</p><p>57 gray levels for MNIST and region features for cars. [sent-214, score-0.254]
</p><p>58 For the hyperparameters (l, m) of our model, we observed that the main control on our model capacity is the embedding size m. [sent-215, score-0.395]
</p><p>59 2  Missing Features at Train Time  The setup presenting missing features at training time is relevant to applications which rely on different sources for training. [sent-219, score-0.908]
</p><p>60 At test time however, the feature detector can be designed to collect the complete feature set. [sent-221, score-0.373]
</p><p>61 The training set is degraded and 85% of the features are missing. [sent-225, score-0.328]
</p><p>62 Again, the training set is degraded and 85% of the features are missing. [sent-227, score-0.328]
</p><p>63 The third set considers the problem of detecting land-mines from 4 types of sensors, each sensor provides a different set of features or views. [sent-228, score-0.288]
</p><p>64 In this case, for each instance, whole views are considered missing during training. [sent-229, score-0.554]
</p><p>65 Inﬁnite imputation is a general technique proposed for the case where features are missing at train time. [sent-232, score-1.047]
</p><p>66 Instead of pretraining the distribution governing the missing values with a generative objective, inﬁnite imputations proposes to train the imputation model and the ﬁnal classiﬁer in a joint optimization framework [6]. [sent-233, score-0.92]
</p><p>67 In this context, we consider an SVM with a RBF kernel as the classiﬁer and three alternative imputation models Mean, GMM and MeanFeat which corresponds to mean imputations in the feature space. [sent-234, score-0.428]
</p><p>68 In this case, features are highly correlated and GMM imputation yields a challenging baseline. [sent-239, score-0.392]
</p><p>69 of missing features  750  Figure 2: Results for MNIST-miss-test (12 binary problems with features missing at test time only) error rates for all models. [sent-241, score-1.599]
</p><p>70 In this case, feature correlation is low and GMM imputation is yielding the worse performance, while our model brings a strong improvement. [sent-242, score-0.333]
</p><p>71 3  Missing Features at Test Time  The setup presenting missing features at test time considers applications in which the training data have been produced with more care than the test data. [sent-244, score-1.073]
</p><p>72 Both strategies propose to learn a classiﬁer which avoids assigning high weight to a small subset of features, hence limiting the impact of the deletion of some features at test time. [sent-247, score-0.381]
</p><p>73 Since no features are missing at train time, we adapt our training procedure to take into account the mismatched conditions between train and test. [sent-257, score-1.051]
</p><p>74 Figure 2 plots the error rate as a function of the number of missing features. [sent-260, score-0.554]
</p><p>75 FSE has a clear advantage in most settings: it achieves a lower error rate than Globerson & Roweis [10] in all cases, while it is better than Dekel & Shamir [5], as soon as the number of missing features is above 50, i. [sent-261, score-0.775]
</p><p>76 In fact, we observe that FSE is very robust to feature deletion; its error rate remains below 20% for up to 700 missing features i. [sent-264, score-0.937]
</p><p>77 On the other end, the alternative strategies report performance close to random when the number of missing features reaches 150, i. [sent-267, score-0.858]
</p><p>78 features are intentionally deleted to fool the classiﬁer, that is beyond the scope of this work. [sent-272, score-0.325]
</p><p>79 These setups proposed small training sets motivated by the training cost of the compared alternatives (see Table 1). [sent-275, score-0.321]
</p><p>80 All conditions are considered: features missing at training time, at testing time, and at both times. [sent-277, score-0.849]
</p><p>81 100, 200, 500 and 784 features which approximately correspond to 90, 60, 35 and 0% missing 7  Table 4: Error Rate (%) 10-class MNIST-full Experiments # train f. [sent-280, score-0.876]
</p><p>82 all training features are available but the training procedure randomly hides some features each time it examines an example. [sent-305, score-0.59]
</p><p>83 when facing a test problem with 300 available features, the model trained with 300 features is better than the models trained with 100, 500 or 784 features. [sent-310, score-0.319]
</p><p>84 We also observe that models facing less features at test time than at train time yield poor performance, while the models trained with few features yield satisfying performance when facing more features. [sent-312, score-0.69]
</p><p>85 This seems to suggest that training with missing features yields more robust models as it avoids the decision function to rely solely on few speciﬁc features that might be corrupted. [sent-313, score-1.149]
</p><p>86 In other word, training with missing features seems to achieve a similar goal as L∞ regularization [5]. [sent-314, score-0.849]
</p><p>87 In fact, the results obtained with no missing features (1. [sent-317, score-0.775]
</p><p>88 The regularization effect of missing training features could be related to noise injection techniques for regularization [21, 11]. [sent-322, score-0.887]
</p><p>89 4  Conclusions  This paper introduces Feature Set Embedding for the problem of classiﬁcation with missing features. [sent-323, score-0.554]
</p><p>90 Our approach deviates from the standard classiﬁcation paradigm: instead of considering examples as feature vectors, we consider examples as sets of (feature, value) pairs which handle the missing feature problem more naturally. [sent-324, score-0.973]
</p><p>91 In order to classify sets, we propose a new strategy relying on two levels of modeling. [sent-325, score-0.242]
</p><p>92 At the ﬁrst level, each (feature, value) is mapped onto an embedding space. [sent-326, score-0.365]
</p><p>93 Our training algorithm then relies on stochastic gradient ascent to jointly learn the embedding space and the ﬁnal linear decision function. [sent-328, score-0.518]
</p><p>94 First, sets are conceptually better suited than vectors for dealing with missing values. [sent-330, score-0.644]
</p><p>95 Second, embedding (feature, value) pairs offers a ﬂexible framework which easily allows encoding prior knowledge about the features. [sent-331, score-0.375]
</p><p>96 From a broader perspective, the ﬂexible feature embedding framework could go beyond the missing feature application. [sent-333, score-1.204]
</p><p>97 the embedding vector of the temperature features in a weather prediction system could be computed from the locations of their sensors. [sent-336, score-0.547]
</p><p>98 It also enables the designing of a system in which new sensors are added without requiring full model re-training; in this case, the model could be quickly adapted by only updating embedding vectors corresponding to the new sensors. [sent-337, score-0.402]
</p><p>99 Also, our approach of relying on feature sets offers interesting opportunities for feature selection and adversarial feature deletion. [sent-338, score-0.603]
</p><p>100 A second order cone programming formulation for classifying missing data. [sent-351, score-0.554]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('missing', 0.554), ('embedding', 0.326), ('fse', 0.264), ('features', 0.221), ('imputation', 0.171), ('feature', 0.162), ('gmm', 0.126), ('lf', 0.126), ('train', 0.101), ('lfi', 0.1), ('strategy', 0.097), ('usps', 0.089), ('embedded', 0.088), ('incomplete', 0.088), ('classi', 0.084), ('structurally', 0.081), ('setups', 0.079), ('er', 0.078), ('fi', 0.076), ('geom', 0.075), ('vfi', 0.075), ('vlf', 0.075), ('training', 0.074), ('vi', 0.073), ('rl', 0.072), ('relying', 0.071), ('considers', 0.067), ('physics', 0.066), ('hepatitis', 0.066), ('validation', 0.063), ('deletion', 0.062), ('imputations', 0.061), ('setup', 0.059), ('shamir', 0.057), ('deleted', 0.054), ('sharing', 0.052), ('dekel', 0.052), ('fool', 0.05), ('hypo', 0.05), ('meanfeat', 0.05), ('facing', 0.049), ('knn', 0.049), ('cars', 0.049), ('instances', 0.049), ('mnist', 0.049), ('strategies', 0.049), ('test', 0.049), ('pairs', 0.049), ('alternatives', 0.048), ('discriminating', 0.046), ('sets', 0.046), ('rank', 0.044), ('dtrain', 0.044), ('mines', 0.044), ('dick', 0.044), ('sick', 0.044), ('vectors', 0.044), ('globerson', 0.043), ('decision', 0.042), ('validated', 0.041), ('classify', 0.041), ('uci', 0.041), ('relies', 0.041), ('informs', 0.04), ('orr', 0.04), ('handwritten', 0.04), ('mapped', 0.039), ('injection', 0.038), ('pima', 0.038), ('nec', 0.038), ('lv', 0.038), ('parameterizing', 0.038), ('impute', 0.038), ('predicts', 0.037), ('solely', 0.037), ('hyperparameters', 0.036), ('liao', 0.036), ('heitz', 0.036), ('originating', 0.036), ('elidan', 0.036), ('chechik', 0.036), ('car', 0.035), ('ascent', 0.035), ('reports', 0.035), ('contrary', 0.035), ('iain', 0.034), ('echo', 0.034), ('discrete', 0.034), ('subspace', 0.034), ('alternative', 0.034), ('generative', 0.033), ('levels', 0.033), ('degraded', 0.033), ('customers', 0.033), ('presence', 0.033), ('capacity', 0.033), ('table', 0.032), ('outputs', 0.032), ('sensors', 0.032), ('labs', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="94-tfidf-1" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>2 0.24102053 <a title="94-tfidf-2" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>3 0.19803558 <a title="94-tfidf-3" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>Author: Samy Bengio, Jason Weston, David Grangier</p><p>Abstract: Multi-class classiﬁcation becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classiﬁers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster. 1</p><p>4 0.13333285 <a title="94-tfidf-4" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>5 0.11060884 <a title="94-tfidf-5" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>Author: Yariv Maron, Elie Bienenstock, Michael James</p><p>Abstract: Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a hig hdimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks. 1 In trod u cti on The embedding of objects in a low-dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10, 13]. Most methods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3], embed objects of heterogeneous types, based on their co-occurrence statistics. In this paper we demonstrate that the latter can be successfully applied to unsupervised part-of-speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1, 4, 5, 6, 7]. The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features). The induction task is fully unsupervised, i.e., it uses no annotations. This task has been addressed in the past using a variety of methods. Some approaches, such as [1], combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [7, 2, 5]. Using a latent-descriptor clustering approach, [15] obtain the best results to date for distributional-only unsupervised POS tagging of the widely-used WSJ corpus. Using a heterogeneous-data embedding approach for this task, we define separate embedding functions for the objects</p><p>6 0.10368912 <a title="94-tfidf-6" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>7 0.10070478 <a title="94-tfidf-7" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>8 0.095354363 <a title="94-tfidf-8" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>9 0.094823554 <a title="94-tfidf-9" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>10 0.093620487 <a title="94-tfidf-10" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>11 0.089552522 <a title="94-tfidf-11" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>12 0.085986905 <a title="94-tfidf-12" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>13 0.08477769 <a title="94-tfidf-13" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>14 0.076792046 <a title="94-tfidf-14" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>15 0.076317109 <a title="94-tfidf-15" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>16 0.074721865 <a title="94-tfidf-16" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>17 0.074327119 <a title="94-tfidf-17" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>18 0.07417652 <a title="94-tfidf-18" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>19 0.072981156 <a title="94-tfidf-19" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>20 0.071204573 <a title="94-tfidf-20" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.228), (1, 0.071), (2, -0.006), (3, -0.11), (4, 0.029), (5, -0.002), (6, 0.012), (7, -0.047), (8, -0.102), (9, -0.081), (10, 0.001), (11, 0.054), (12, 0.158), (13, 0.018), (14, 0.09), (15, -0.061), (16, -0.041), (17, -0.015), (18, -0.018), (19, -0.049), (20, -0.084), (21, 0.063), (22, 0.009), (23, -0.235), (24, 0.154), (25, 0.099), (26, 0.054), (27, 0.1), (28, -0.109), (29, -0.037), (30, 0.049), (31, -0.058), (32, -0.136), (33, 0.023), (34, 0.043), (35, -0.065), (36, 0.122), (37, 0.017), (38, 0.118), (39, 0.081), (40, -0.039), (41, -0.025), (42, -0.034), (43, -0.019), (44, 0.041), (45, -0.097), (46, 0.05), (47, -0.035), (48, 0.037), (49, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95380604 <a title="94-lsi-1" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>2 0.82690978 <a title="94-lsi-2" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>3 0.63409865 <a title="94-lsi-3" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>Author: Samy Bengio, Jason Weston, David Grangier</p><p>Abstract: Multi-class classiﬁcation becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classiﬁers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster. 1</p><p>4 0.55569464 <a title="94-lsi-4" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>Author: Umar Syed, Ben Taskar</p><p>Abstract: We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efﬁcient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines. 1</p><p>5 0.53739572 <a title="94-lsi-5" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>Author: Rob Fergus, George Williams, Ian Spiro, Christoph Bregler, Graham W. Taylor</p><p>Abstract: This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that realworld performance can be improved through the use of synthetic data. 1</p><p>6 0.53126574 <a title="94-lsi-6" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>7 0.51651818 <a title="94-lsi-7" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>8 0.51318496 <a title="94-lsi-8" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>9 0.49824932 <a title="94-lsi-9" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>10 0.4872562 <a title="94-lsi-10" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>11 0.48335084 <a title="94-lsi-11" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>12 0.47991613 <a title="94-lsi-12" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>13 0.4774248 <a title="94-lsi-13" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>14 0.47470042 <a title="94-lsi-14" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>15 0.47267693 <a title="94-lsi-15" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>16 0.46903092 <a title="94-lsi-16" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>17 0.46070239 <a title="94-lsi-17" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>18 0.45929748 <a title="94-lsi-18" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>19 0.45577615 <a title="94-lsi-19" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>20 0.43183559 <a title="94-lsi-20" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.113), (17, 0.027), (27, 0.067), (30, 0.049), (35, 0.014), (45, 0.241), (50, 0.06), (52, 0.037), (60, 0.032), (64, 0.179), (77, 0.035), (78, 0.021), (90, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91620189 <a title="94-lda-1" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>Author: Fabian Triefenbach, Azarakhsh Jalalvand, Benjamin Schrauwen, Jean-pierre Martens</p><p>Abstract: Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still signiﬁcant. 1</p><p>same-paper 2 0.85930395 <a title="94-lda-2" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>3 0.83608979 <a title="94-lda-3" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>Author: Mauricio Alvarez, Jan R. Peters, Neil D. Lawrence, Bernhard Schölkopf</p><p>Abstract: Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology. 1</p><p>4 0.83416808 <a title="94-lda-4" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>Author: Armand Joulin, Jean Ponce, Francis R. Bach</p><p>Abstract: Dimensionality reduction is commonly used in the setting of multi-label supervised classiﬁcation to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added beneﬁts in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efﬁcient algorithm based on convex relaxations and lowrank representations of the data, capable of handling large-scale problems. Experiments on text document classiﬁcation show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods. 1</p><p>5 0.83108103 <a title="94-lda-5" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>6 0.82968754 <a title="94-lda-6" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>7 0.82916784 <a title="94-lda-7" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>8 0.82876247 <a title="94-lda-8" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>9 0.82787412 <a title="94-lda-9" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>10 0.82606328 <a title="94-lda-10" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>11 0.82597858 <a title="94-lda-11" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>12 0.82576889 <a title="94-lda-12" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>13 0.82336825 <a title="94-lda-13" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>14 0.82266104 <a title="94-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.82121402 <a title="94-lda-15" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>16 0.82073265 <a title="94-lda-16" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>17 0.82066482 <a title="94-lda-17" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>18 0.82014602 <a title="94-lda-18" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>19 0.8195076 <a title="94-lda-19" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>20 0.81855547 <a title="94-lda-20" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
