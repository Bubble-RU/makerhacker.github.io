<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>286 nips-2010-Word Features for Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-286" href="#">nips2010-286</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>286 nips-2010-Word Features for Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2010-286-pdf" href="http://papers.nips.cc/paper/4094-word-features-for-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>Reference: <a title="nips-2010-286-reference" href="../nips2010_reference/nips-2010-Word_Features_for_Latent_Dirichlet_Allocation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. [sent-4, score-0.591]
</p><p>2 We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. [sent-5, score-0.542]
</p><p>3 Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. [sent-6, score-0.418]
</p><p>4 1  Introduction  Latent Dirichlet Allocation [4] assigns topics to documents and generates topic distributions over words given a collection of texts. [sent-7, score-0.905]
</p><p>5 The inability to deal with word features makes LDA fall short on several aspects. [sent-10, score-0.256]
</p><p>6 The most obvious one is perhaps that the topics estimated for infrequently occurring words are usually unreliable. [sent-11, score-0.34]
</p><p>7 Ideally, for example, we would like the topics associated with synonyms to have a prior tendency of being similar, so that in case one of the words is rare but the other is common, the topic estimates for the rare one can be improved. [sent-12, score-0.777]
</p><p>8 Similarly, we would like to be able to leverage dictionaries in order to boost topic cohesion across languages, a problem that has been researched but is far from being fully solved, especially for non-aligned corpora [6]. [sent-15, score-0.534]
</p><p>9 A possible solution, which we propose in this paper, is to treat word information as features rather than as explicit constraints and to adjust a smoothing prior over topic distributions for words such that correlation is emphasised. [sent-17, score-0.869]
</p><p>10 In the parlance of LDA we do not pick a globally constant β smoother over the word multinomials but rather we adjust it according to word similarity. [sent-18, score-0.561]
</p><p>11 In this way we are capable of learning the prior probability of how words are distributed over various topics based on how similar they are, e. [sent-19, score-0.343]
</p><p>12 in the context of dictionaries, synonym collections, thesauri, edit distances, or distributional word similarity features. [sent-21, score-0.35]
</p><p>13 Instead, we use a hybrid approach where we perform smooth optimisation over the word smoothing coefﬁcients, while retaining a collapsed Gibbs sampler to assign topics for a ﬁxed choice of smoothing coefﬁcients. [sent-23, score-0.741]
</p><p>14 We present experimental results on multi-language topic synchronisation which clearly evidence the ability of the model to incorporate dictionary information successfully. [sent-25, score-0.435]
</p><p>15 Using several different measures of topic alignment, we consistently observe that the proposed model improves substantially on standard LDA, which is unable to leverage this type of information. [sent-26, score-0.389]
</p><p>16 1  for m = 1 to M  wmn  for v = 1 to V  ψkv  for n = 1 to Nm  Related work  βkv  ψkv  for n = 1 to Nm  βkv  φv  for v = 1 to V  Figure m α2: x  for k = to K y Our Extension: Assume 1we observe side zmn θm information φv (i. [sent-29, score-0.526]
</p><p>17 m  Previous work on multilingual topic models requires parallelism at either the sentence level ([20]) or document level ([9], [15]). [sent-36, score-0.551]
</p><p>18 More recent work [13] relaxes that, but still requires that a signiﬁcant fraction (at least 25%) of the documents are paired up. [sent-37, score-0.198]
</p><p>19 Multilingual topic alignment without parallelism was recently proposed by [6]. [sent-38, score-0.441]
</p><p>20 Their model requires a list of matched word pairs m (where each pair has one word in each language) and corresponding matching priors π that encode the prior knowledge on how likely the match is to occur. [sent-39, score-0.556]
</p><p>21 The topics are deﬁned as distributions over word pairs, while the unmatched words come from a unigram distribution speciﬁc to each language. [sent-40, score-0.576]
</p><p>22 Although their model could be in principle extended to more than two languages their experimental section was focused on the bilingual case. [sent-41, score-0.145]
</p><p>23 One of the key differences between [6] and our method is that we do not hardcode word information, but we use it only as a prior – this way our method becomes less sensitive to errors in the word features. [sent-42, score-0.508]
</p><p>24 Furthermore, our model automatically extends to multiple languages without any modiﬁcation, aligning topics even for language pairs for which we have no information, as we show in the experimental section for the Portuguese/French pair. [sent-43, score-0.407]
</p><p>25 It assumes that θm ∼ Dir(α) zmn ∼ Mult(θm )  (1a) (1b)  ψk ∼ Dir(βk |φ, y)  (2a)  ψk ∼ Dir(β) wmn ∼ Multi(ψzmn )  (1c) (1d)  β ∼ Logistic(y; φ). [sent-46, score-0.502]
</p><p>26 (2b)  Nonparametric extensions in terms of the number of topics can be obtained using Dirichlet process models [2] regarding the generation of topics. [sent-47, score-0.204]
</p><p>27 Instead of treating it as a constant for all words we attempt to infer its values for different words and topics respectively. [sent-49, score-0.418]
</p><p>28 The above dependency allows us to incorporate features of words as side information. [sent-52, score-0.149]
</p><p>29 ’politics’ and ’politician’) are very similar then it is plausible to assume that their topic distributions should also be quite similar. [sent-55, score-0.396]
</p><p>30 Conjugate distribution p(θm |α): This is a Dirichlet distribution with parameters α, where αk denotes the smoother for topic k. [sent-62, score-0.436]
</p><p>31 Word distribution p(wmn |zmn , ψ): We assume that given a topic zmn the word wmn is drawn from a multinomial distribution ψwmn ,zmn . [sent-64, score-1.109]
</p><p>32 Collapsed distribution p(w|z, β): Integrating out ψk for all topics k yields the following K  p(w|z, β) = k=1  2. [sent-70, score-0.204]
</p><p>33 2  V v=1  Γ(nKV + βkv ) Γ ( βk 1 ) kv V Γ nK + βk 1 k v=1 Γ(βkv )  Priors  In order to better control the capacity of our model, we impose a prior on naturally related words, e. [sent-71, score-0.43]
</p><p>34 For this purpose we design a similarity graph G(V, E) with words represented as vertices V and similarity edge weights φuv between vertices u, v ∈ V whenever u is related to v. [sent-74, score-0.265]
</p><p>35 In particular, the magnitude of φuv can denote the similarity between words u and v. [sent-75, score-0.165]
</p><p>36 In the following we denote by ykv the topic dependent smoothing coefﬁcients for a given word v and topic k. [sent-76, score-1.216]
</p><p>37 We impose the smoother   −1  2 log βkv = ykv + yv and log p(β) = 2 φv,v (ykv − ykv )2 + yv  2λ v v,v ,k  where log p(β) is given up to an additive constant and yv allows for multiplicative topic-unspeciﬁc corrections. [sent-77, score-0.643]
</p><p>38 A similar model was used by [3] to capture temporal dependence between topic models computed at different time instances, e. [sent-78, score-0.369]
</p><p>39 when dealing with topic drift over several years in a scientiﬁc journal. [sent-80, score-0.369]
</p><p>40 There the vertices are words at a given time and the edges are between smoothers instantiated at subsequent years. [sent-81, score-0.15]
</p><p>41 3  Inference  In analogy to the collapsed sampler of [8] we also represent the model in a collapsed fashion. [sent-82, score-0.225]
</p><p>42 1  Document Likelihood  The likelihood contains two terms: a word-dependent term which can be computed on the ﬂy while resampling data1 , and a model-dependent term involving the topic counts and the word-topic counts which can be computed by one pass through the aggregate tables respectively. [sent-85, score-0.423]
</p><p>43 For the count variables nKM , nKV , nK and nM we denote by the subscript ‘−’ their values after the word wmn and associated topic zmn have been removed from the statistics. [sent-93, score-1.134]
</p><p>44 Standard calculations yield the following topic probability for resampling: βkv + nKV − nKM + αk kvmn km− (6) p(zmn = k|rest) ∝ ¯ nK + βk k− In the appendix we detail how to addapt the sampler of [19] to obtain faster sampling. [sent-94, score-0.438]
</p><p>45 The data-dependent contribution to the negative log-likelihood is K  Lβ = k=1  ¯ ¯ log Γ(βk + nK ) − log Γ(βk ) + k  K k=1 v:nKV =0 kv  log Γ(βkv ) − log Γ(βkv + nKV ) kv  with gradients given by the appropriate derivatives of the Γ function. [sent-98, score-0.796]
</p><p>46 After choosing edges φuv according to these matching words, we obtain an optimisation problem directly in terms of the variables ykv and yv . [sent-101, score-0.306]
</p><p>47 Denote by N (v) the neighbours for word v in G(V, E), and Υ(x) := ∂x log Γ(x) the Digamma function. [sent-102, score-0.238]
</p><p>48 We have 1 ¯ ¯ ∂ykv [Lβ − log p(β)] = 2 φv,v [ykv − ykv ] + βkv Υ(βk + nK ) − Υ(βk ) + k λ v ∈N (v)  + nKV > 0 kv The gradient with respect to yk is analogous. [sent-103, score-0.578]
</p><p>49 2 Here zi denotes the topic of word i, and z¬i the topics of all words in the corpus except for i. [sent-108, score-0.971]
</p><p>50 4  4  Experiments  To demonstrate the usefulness of our model we applied it to a multi-lingual document collection, where we can show a substantial improvement over the standard LDA model on the coordination between topics of different languages. [sent-109, score-0.296]
</p><p>51 1  Dataset  Since our goal is to compare topic distributions on different languages we used a parallel corpus [11] with the proceedings of the European Parliament in 11 languages. [sent-111, score-0.583]
</p><p>52 We randomly sampled 1000 documents from each language, removed infrequent4 and frequent5 words and kept only the documents with at least 20 words. [sent-115, score-0.528]
</p><p>53 Finally, we removed all documents that lost their corresponding translations in this process. [sent-116, score-0.264]
</p><p>54 cz/dict/, augmented with translations from Google Translate for the most frequent words in our dataset. [sent-125, score-0.148]
</p><p>55 As described earlier, each word corresponds to a vertex, with an edge7 whenever two words match in the dictionary. [sent-126, score-0.345]
</p><p>56 In our model β = exp(ykv + yv ), so we want to keep both ykv and yv reasonably low to avoid numerical problems, as a large value of either would lead to overﬂows. [sent-127, score-0.324]
</p><p>57 We did the same for the standard LDA model, where to learn an asymmetric beta we simply removed ykv to get β = exp(yv ). [sent-129, score-0.205]
</p><p>58 4  Methodology  In our experiments we used all the English documents and a subset of the French and Portuguese ones – this is what we have in a real application, when we try to learn a topic model from web pages: the number of pages is English is far greater than in any other language. [sent-131, score-0.567]
</p><p>59 First, we run the standard LDA model with all documents mixed together – this is one of our baselines, which we call STD1. [sent-133, score-0.198]
</p><p>60 8 We need to start with only one language so that an initial topic-word distribution is built; once that is done the priors are learned and can be used to guide the topic-word distributions in other languages. [sent-140, score-0.141]
</p><p>61 5  Finally, as a control experiment we run the standard LDA model in this same setting: ﬁrst English documents, then all languages mixed. [sent-141, score-0.112]
</p><p>62 In all experiments we run the Gibbs sampler for a total of 3000 iterations, with the number of topics ﬁxed to 20, and keep the last sample. [sent-143, score-0.273]
</p><p>63 After a burn-in of 500 iterations, the optimisation over the word smoothing coefﬁcients is done every 100 iterations, using an off-the-shelf L-BFGS [12] optimizer. [sent-144, score-0.33]
</p><p>64 5  Evaluation  Evaluation of topic models is an open problem – recent work [7] suggests that popular measures based on held-out likelihood, such as perplexity, do not capture whether topics are coherent or not. [sent-148, score-0.573]
</p><p>65 our goal – to synchronize topics across different languages – and there’s no reason to believe that likelihood measures would assess that: a model where topics are synchronized across languages is not necessarily more likely than a model that is not synchronized. [sent-152, score-0.668]
</p><p>66 1 Mean number of agreements in top 5 topics: |L1 | d1 ∈L1 ,d2 =F (d1 ) agreements(d1 , d2 ) where agreements(d1 , d2 ) is the cardinality of the intersection of the 5 most likely topics of d1 and d2 . [sent-155, score-0.367]
</p><p>67 In Figure 6 we plot the word smoothing prior for the English word democracy and its French and Portuguese translations, d´ mocratie and democracia, for both the standard LDA model (STD1) and e our model (DC), with 20% of the French and Portuguese documents used in training. [sent-162, score-0.897]
</p><p>68 In STD1 we don’t have topic-speciﬁc priors (hence the horizontal line) and the word democracy has a much higher prior, because it happens more often in the dataset (since we have all English documents and only 20% of the French and Portuguese ones). [sent-163, score-0.541]
</p><p>69 6  To emphasize that we do not need a parallel corpus we ran a second experiment where we selected the same number of documents of each language, but assuring that for each document its corresponding translations are not in the dataset, and trained our model (DC) with 100 topics. [sent-168, score-0.385]
</p><p>70 In this case, however, we cannot compute the distance metrics as before, since we have no information on the actual topic distributions of the documents. [sent-170, score-0.45]
</p><p>71 This is shown in Table 1, for some selected topics, where the synchronization amongst the different languages is clear. [sent-172, score-0.112]
</p><p>72 Mean l2−distance  Mean Hellinger distance  1  % agreements on first topic  2  1. [sent-173, score-0.557]
</p><p>73 5  20  0 0  5 10 15 % of French documents  20  1  0 0  0. [sent-183, score-0.198]
</p><p>74 5 20 0  5 10 15 % of French documents  5 10 15 % of French documents  20  Figure 3: Comparison of topic distributions in English and French documents. [sent-184, score-0.792]
</p><p>75 Mean l2−distance  Mean Hellinger distance  1  % agreements on first topic  2 STD1 STD2 DC  3. [sent-186, score-0.557]
</p><p>76 2  5 10 15 % of Portuguese documents  20  STD1 STD2 DC  0 0  5 10 15 % of Portuguese documents  1. [sent-196, score-0.396]
</p><p>77 5 20 0  5 10 15 % of Portuguese documents  5 10 15 % of Portuguese documents  20  Figure 4: Comparison of topic distributions in English and Portuguese documents. [sent-198, score-0.792]
</p><p>78 5 STD1 STD2 DC  0 0  5 10 15 20 % of Portuguese/French documents  0. [sent-207, score-0.198]
</p><p>79 5  0 0  20  STD1 STD2 DC  1  5 10 15 20 % of Portuguese/French documents  0 0  5 10 15 % of Portuguese/French documents  0. [sent-208, score-0.396]
</p><p>80 5 20 0  5 10 15 20 % of Portuguese/French documents  Figure 5: Comparison of topic distributions in Portuguese and French documents. [sent-209, score-0.594]
</p><p>81 9  −1 0  5  10 topic  15  −5 0  20  5  10 topic  15  20  Figure 6: Word smoothing prior for two words in the standard LDA and in our model. [sent-218, score-0.937]
</p><p>82 7  Table 1: Top 10 words for some of the learned topics (from top to bottom, respectively, topics 8, 17, 20, 32, 49). [sent-223, score-0.515]
</p><p>83 , information is a word in both French and English). [sent-226, score-0.238]
</p><p>84 2 with edges only between words which exceed a level of proximity. [sent-231, score-0.129]
</p><p>85 Lexical Similarity: For interpolation between words one could use a distribution over substrings of a word as the feature map. [sent-232, score-0.345]
</p><p>86 Such lexical similarity makes the sampler less sensitive to issues such as stemming: after all, two words which reduce to the same stem will also have a high lexical similarity score, hence the estimated βkv will yield very similar topic assignments. [sent-234, score-0.845]
</p><p>87 This can be achieved by adding edges between a word and all of its synonyms. [sent-236, score-0.26]
</p><p>88 Since in our framework we only use this information to shape a prior, errors in the synonym list and multiple meanings of a word will not prove fatal. [sent-237, score-0.319]
</p><p>89 2  Multiple Languages  Lexical Similarity: Similar considerations apply for inter-lingual topic models. [sent-239, score-0.369]
</p><p>90 It is reasonable to assume that lexical similarity generally points to similarity in meaning. [sent-240, score-0.208]
</p><p>91 Using such features should allow one to synchronise topics even in the absence of dictionaries. [sent-241, score-0.255]
</p><p>92 However, it is important that similarities are not hardcoded but only imposed as a prior on the topic distribution (e. [sent-242, score-0.401]
</p><p>93 6  Discussion  In this paper we described a simple yet general formalism for incorporating word features into LDA, which among other things allows us to synchronise topics across different languages. [sent-245, score-0.511]
</p><p>94 We performed a number of experiments in the multiple-language setting, in which the goal was to show that our model is able to incorporate dictionary information in order to improve topic alignment across different languages. [sent-246, score-0.463]
</p><p>95 We also showed that the algorithm is quite effective even in the absence of documents that are explicitly denoted as being aligned (see Table 1). [sent-248, score-0.198]
</p><p>96 This sets it apart from [13], which requires that a signiﬁcant fraction (at least 25%) of documents are paired up. [sent-249, score-0.198]
</p><p>97 For instance, noun / verb disambiguation or named entity recognition are all useful in determining the meaning of words and therefore it is quite likely that they will also aid in obtaining an improved topical mixture model. [sent-252, score-0.133]
</p><p>98 Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. [sent-255, score-0.369]
</p><p>99 Lexical triggers and latent semantic analysis for crosslingual language model adaptation. [sent-300, score-0.114]
</p><p>100 Efﬁcient methods for topic model inference on streaming document collections. [sent-354, score-0.44]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kv', 0.398), ('topic', 0.369), ('zmn', 0.287), ('word', 0.238), ('wmn', 0.215), ('topics', 0.204), ('documents', 0.198), ('nkv', 0.18), ('ykv', 0.18), ('french', 0.173), ('lda', 0.172), ('agreements', 0.163), ('portuguese', 0.148), ('dc', 0.136), ('nkm', 0.131), ('english', 0.113), ('languages', 0.112), ('words', 0.107), ('lexical', 0.092), ('language', 0.091), ('nm', 0.083), ('democracy', 0.082), ('multilingual', 0.082), ('collapsed', 0.078), ('yv', 0.072), ('document', 0.071), ('sampler', 0.069), ('km', 0.068), ('smoother', 0.067), ('democracia', 0.065), ('synonyms', 0.065), ('smoothing', 0.06), ('similarity', 0.058), ('hellinger', 0.057), ('dirichlet', 0.056), ('corpus', 0.053), ('david', 0.05), ('cohesion', 0.049), ('mocratie', 0.049), ('thesauri', 0.049), ('nk', 0.049), ('alignment', 0.043), ('translations', 0.041), ('mimno', 0.037), ('dictionaries', 0.036), ('uv', 0.035), ('dictionary', 0.033), ('bilingual', 0.033), ('elections', 0.033), ('politician', 0.033), ('synchronisation', 0.033), ('synchronise', 0.033), ('synonym', 0.033), ('prior', 0.032), ('resampling', 0.032), ('optimisation', 0.032), ('dir', 0.031), ('gibbs', 0.029), ('metrics', 0.029), ('argmaxk', 0.029), ('parallelism', 0.029), ('infrequently', 0.029), ('politics', 0.029), ('distributions', 0.027), ('topical', 0.026), ('blei', 0.026), ('distance', 0.025), ('removed', 0.025), ('list', 0.025), ('xiaojin', 0.025), ('allocation', 0.024), ('side', 0.024), ('grammar', 0.023), ('meanings', 0.023), ('nicta', 0.023), ('latent', 0.023), ('priors', 0.023), ('cients', 0.023), ('coef', 0.022), ('edges', 0.022), ('parallel', 0.022), ('pass', 0.022), ('coordination', 0.021), ('distributional', 0.021), ('corpora', 0.021), ('editors', 0.021), ('entirely', 0.021), ('andrew', 0.021), ('boost', 0.021), ('german', 0.021), ('vertices', 0.021), ('leverage', 0.02), ('zm', 0.02), ('yahoo', 0.019), ('occurred', 0.019), ('across', 0.018), ('features', 0.018), ('australian', 0.018), ('logistic', 0.018), ('adjust', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="286-tfidf-1" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>2 0.28145298 <a title="286-tfidf-2" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>3 0.20540059 <a title="286-tfidf-3" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>4 0.20046912 <a title="286-tfidf-4" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>5 0.19535185 <a title="286-tfidf-5" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>6 0.1932335 <a title="286-tfidf-6" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>7 0.1669261 <a title="286-tfidf-7" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>8 0.15789838 <a title="286-tfidf-8" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>9 0.10857416 <a title="286-tfidf-9" href="./nips-2010-Multitask_Learning_without_Label_Correspondences.html">177 nips-2010-Multitask Learning without Label Correspondences</a></p>
<p>10 0.10685648 <a title="286-tfidf-10" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>11 0.1009852 <a title="286-tfidf-11" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>12 0.086259753 <a title="286-tfidf-12" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>13 0.080531381 <a title="286-tfidf-13" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>14 0.076691292 <a title="286-tfidf-14" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>15 0.068224587 <a title="286-tfidf-15" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>16 0.049070798 <a title="286-tfidf-16" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>17 0.046271861 <a title="286-tfidf-17" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>18 0.046096839 <a title="286-tfidf-18" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>19 0.045500215 <a title="286-tfidf-19" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>20 0.044438828 <a title="286-tfidf-20" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, 0.055), (2, -0.002), (3, -0.022), (4, -0.359), (5, 0.134), (6, 0.247), (7, 0.008), (8, -0.069), (9, 0.008), (10, 0.205), (11, 0.105), (12, -0.02), (13, 0.111), (14, -0.043), (15, 0.051), (16, 0.018), (17, 0.011), (18, -0.112), (19, -0.035), (20, 0.082), (21, -0.08), (22, 0.006), (23, 0.101), (24, 0.123), (25, -0.084), (26, 0.07), (27, -0.027), (28, 0.094), (29, 0.034), (30, 0.055), (31, -0.051), (32, -0.081), (33, 0.059), (34, -0.012), (35, -0.035), (36, 0.033), (37, 0.052), (38, -0.002), (39, -0.019), (40, 0.037), (41, 0.023), (42, -0.021), (43, 0.044), (44, -0.013), (45, 0.028), (46, 0.038), (47, -0.008), (48, -0.005), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97685933 <a title="286-lsi-1" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>2 0.79770881 <a title="286-lsi-2" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>3 0.71419567 <a title="286-lsi-3" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>4 0.67725378 <a title="286-lsi-4" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>Author: Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David B. Dunson</p><p>Abstract: We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here deﬁned by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efﬁcient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. 1</p><p>5 0.66763186 <a title="286-lsi-5" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>6 0.62824506 <a title="286-lsi-6" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>7 0.62310821 <a title="286-lsi-7" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>8 0.49989772 <a title="286-lsi-8" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>9 0.4808417 <a title="286-lsi-9" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>10 0.33467278 <a title="286-lsi-10" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>11 0.33109593 <a title="286-lsi-11" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>12 0.32170197 <a title="286-lsi-12" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>13 0.32167739 <a title="286-lsi-13" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>14 0.31294295 <a title="286-lsi-14" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>15 0.29719558 <a title="286-lsi-15" href="./nips-2010-Multitask_Learning_without_Label_Correspondences.html">177 nips-2010-Multitask Learning without Label Correspondences</a></p>
<p>16 0.29241052 <a title="286-lsi-16" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>17 0.28938147 <a title="286-lsi-17" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>18 0.26617467 <a title="286-lsi-18" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>19 0.26347077 <a title="286-lsi-19" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>20 0.24464101 <a title="286-lsi-20" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.013), (13, 0.036), (17, 0.044), (27, 0.151), (30, 0.098), (35, 0.014), (43, 0.272), (45, 0.143), (50, 0.044), (52, 0.023), (60, 0.011), (77, 0.028), (78, 0.015), (90, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77363628 <a title="286-lda-1" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>2 0.70755124 <a title="286-lda-2" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>Author: Shuang Wu, Xuming He, Hongjing Lu, Alan L. Yuille</p><p>Abstract: The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a uniﬁed framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion ﬁeld at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.</p><p>3 0.63784438 <a title="286-lda-3" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>Author: Hongjing Lu, Tungyou Lin, Alan Lee, Luminita Vese, Alan L. Yuille</p><p>Abstract: It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More speciﬁcally, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, ﬁrst-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our ﬁrst experimental session we estimate the weights of the three terms for each functional form to maximize the ﬁt to human performance. We then measured human performance for motion tasks and found that we obtained better ﬁt for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better ﬁt to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best ﬁt models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction – ranging from 3% to 8% for ﬁve human subjects over ten experimental conditions – and give further support that the human visual system uses an L1-norm (Laplace) prior.</p><p>4 0.62610817 <a title="286-lda-4" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>5 0.61968642 <a title="286-lda-5" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>6 0.61501348 <a title="286-lda-6" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>7 0.614959 <a title="286-lda-7" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>8 0.61397791 <a title="286-lda-8" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>9 0.61212438 <a title="286-lda-9" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>10 0.61055231 <a title="286-lda-10" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>11 0.61046422 <a title="286-lda-11" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>12 0.60887569 <a title="286-lda-12" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>13 0.60695183 <a title="286-lda-13" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>14 0.60530239 <a title="286-lda-14" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>15 0.592776 <a title="286-lda-15" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>16 0.59201449 <a title="286-lda-16" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>17 0.59072083 <a title="286-lda-17" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>18 0.58962274 <a title="286-lda-18" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>19 0.58904874 <a title="286-lda-19" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>20 0.58658415 <a title="286-lda-20" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
