<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-16" href="#">nips2010-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</h1>
<br/><p>Source: <a title="nips-2010-16-pdf" href="http://papers.nips.cc/paper/3995-a-vlsi-implementation-of-the-adaptive-exponential-integrate-and-fire-neuron-model.pdf">pdf</a></p><p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>Reference: <a title="nips-2010-16-reference" href="../nips2010_reference/nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. [sent-3, score-0.882]
</p><p>2 Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. [sent-4, score-0.544]
</p><p>3 The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. [sent-5, score-0.829]
</p><p>4 As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. [sent-6, score-0.349]
</p><p>5 1  Introduction  Since the beginning of neuromorphic engineering [1, 2] designers have had great success in building VLSI1 neurons mimicking the behavior of biological neurons using analog circuits [3–8]. [sent-7, score-0.583]
</p><p>6 It has been argued [4] whether it is best to emulate an established model or to create a new one using analog circuits. [sent-9, score-0.181]
</p><p>7 We approach gaining access to the computational power of neural systems and creating a device being able to emulate biologically relevant spiking neural networks that can be reproduced in a traditional simulation environment for modeling. [sent-12, score-0.469]
</p><p>8 The use of a commonly known model enables modelers to do experiments on neuromorphic hardware and compare them to simulations. [sent-13, score-0.35]
</p><p>9 The software framework PyNN [11, 12] even allows for directly switching between a simulator and the neuromorphic hardware device, allowing modelers to access the hardware on a high level without knowing all implementation details. [sent-15, score-0.593]
</p><p>10 The hardware neuron presented here can emulate the adaptive exponential integrate-and-ﬁre neuron model (AdEx) [13], developed within the FACETS-project [14]. [sent-16, score-0.953]
</p><p>11 The AdEx model can produce complex ﬁring patterns observed in biology [15], like spike-frequency-adaptation, bursting, regular spiking, irregular spiking and transient spiking by tuning a limited number of parameters [16]. [sent-17, score-0.421]
</p><p>12 (2) dt Cm , gl , ge and gi are the membrane capacitance, the leakage conductance and the conductances for excitatory and inhibitory synaptic inputs, where ge and gi depend on time and the inputs from other neurons. [sent-19, score-0.706]
</p><p>13 El , Ei and Ee are the leakage reversal potential and the synaptic reversal potentials. [sent-20, score-0.191]
</p><p>14 The time constant of the adaptation variable is τw and a is called adaptation parameter. [sent-22, score-0.292]
</p><p>15 If the membrane voltage crosses a certain threshold voltage Θ, the neuron is reset: V → Vreset ; w → w + b. [sent-24, score-0.936]
</p><p>16 Due to the sharp rise, created by the exponential term in equation 1, the exact value of Θ is not critical for the determination of the moment of a spike [13]. [sent-26, score-0.213]
</p><p>17 The neuron is integrated on a prototype chip called HICANN2 [17–19] (ﬁgure 2) which has been produced in 2009. [sent-34, score-0.393]
</p><p>18 Each HICANN contains 512 dendrite membrane (DenMem) circuits (ﬁgure 3), each being connected to 224 dynamic input synapses. [sent-35, score-0.308]
</p><p>19 Neurons are built of DenMems by shorting their membrane capacitances gaining up to 14336 input synapses for a single neuron. [sent-36, score-0.259]
</p><p>20 The HICANN is prepared for integration in the FACETS wafer-scale system [17–19] allowing to interconnect 384 HICANNs on an uncut silicon wafer via a high speed bus system, so networks of up to 196 608 neurons can be emulated on a single wafer. [sent-37, score-0.263]
</p><p>21 Another VLSI neuron designed with a time scaling factor is presented in [7]. [sent-41, score-0.293]
</p><p>22 This implementation is capable of reproducing lots of different ﬁring patterns of cortical neurons, but has no direct correspondence to a neuron from the modeling area. [sent-42, score-0.339]
</p><p>23 2  High Input Count Analog Neural Network  2  bus system synapse array neuron block floating gates 10 mm  Figure 2: Photograph of the HICANN-chip  2 2. [sent-43, score-0.405]
</p><p>24 1  Neuron implementation Neuron  The smallest part of a neuron is a DenMem, which implements the terms of the AdEx neuron described above. [sent-44, score-0.586]
</p><p>25 Each term is constructed by a single circuit using operational ampliﬁers (OP) and operational transconductance ampliﬁers (OTA) and can be switched off separately, so less complex models like the leaky integrate-and-ﬁre model implemented in [9] can be emulated. [sent-45, score-0.198]
</p><p>26 A ﬁrst, not completely implemented version of the neuron has been proposed in [17]. [sent-48, score-0.328]
</p><p>27 Some simulation results of the actual neuron can be found in [19]. [sent-49, score-0.332]
</p><p>28 Input  Neighbour-Neurons Leak  SynIn  Exp Spiking/ Connection  Membrane CMembrane Reset SynIn  Spikes  VReset  In/Out Input  STDP/ Network  Adapt  Current-Input Membrane-Output  Figure 3: Schematic diagram of AdEx neuron circuit Figure 3 shows a block diagram of a DenMem. [sent-50, score-0.42]
</p><p>29 During normal operation, the neuron gets rectangular shaped current pulses as input from the synapse array (ﬁgure 2) at one of the two synaptic input circuits. [sent-51, score-0.539]
</p><p>30 Inside these circuits the current is integrated by a leaky integrator OP-circuit resulting in a voltage that is transformed to a current by an OTA. [sent-52, score-0.426]
</p><p>31 Using this current as bias for another OTA, a sharply rising and exponentially decaying synaptic input conductance is created. [sent-53, score-0.241]
</p><p>32 Each DenMem is equipped with two synaptic input circuits, each having its own connection to the synapse array. [sent-54, score-0.17]
</p><p>33 The output of a synapse can be chosen between them, which allows for two independent synaptic channels which could be inhibitory or excitatory. [sent-55, score-0.17]
</p><p>34 The leakage term of equation 1 can be implemented directly using an OTA, building a conductance between the leakage potential El and the membrane voltage V . [sent-56, score-0.744]
</p><p>35 dt 3  (5)  Now the time constant τw shall be created by a capacitance Cadapt and a conductance gadapt and we get: dVadapt −Cadapt = gadapt (Vadapt − V ). [sent-58, score-0.289]
</p><p>36 (6) dt We need to transform b into a voltage using the conductance a and get Cadapt Ib tpulse =  b a  (7)  where the ﬁxed tpulse is the time a current Ib increases Vadapt on Cadapt at each detected spike of a neuron. [sent-59, score-0.6]
</p><p>37 These resulting equations for adaptation can be directly implemented as a circuit. [sent-60, score-0.181]
</p><p>38 To generate the correct gate source voltage, a non inverting ampliﬁer multiplies the difference between the membrane voltage and a voltage Vt by an adjustable factor. [sent-62, score-0.724]
</p><p>39 The gate source voltage of M1 is : R1 (V − Vt ) (8) R2 Deployed in the equation for a MOSFET in sub-threshold mode this results in a current depending exponentially on V following equation 1 where ∆t can be adjusted via the resistors R1 and R2 . [sent-64, score-0.304]
</p><p>40 The factor in front of the exponential gl ∆t and Vt of the model can be changed by moving the circuits Vt . [sent-65, score-0.276]
</p><p>41 VGSM1 =  Figure 4: Simpliﬁed schematic of the exponential circuit Our neuron detects a spike at a directly adjustable threshold voltage Θ - this is especially necessary as the circuit cannot only implement the AdEx model, but also less complex models. [sent-67, score-0.993]
</p><p>42 In a model without a sharp spike, like the one created by the positive feedback of the exponential term, spike timing very much depends on the exact voltage Θ. [sent-68, score-0.442]
</p><p>43 A detected spike triggers reseting of the membrane by a current pulse to a potential Vreset for an adjustable time. [sent-69, score-0.422]
</p><p>44 2  Parameterization  In contrast to most other systems, we are using analog ﬂoating gate memories similar to [20] as storage device for the analog parameters of a neuron. [sent-72, score-0.314]
</p><p>45 This way, matching issues can be counterbalanced, and different types of neurons can be implemented on a single chip enhancing the universality of the wafer-scale system. [sent-74, score-0.218]
</p><p>46 Technical biasing parameters and parameters of the synaptic input circuits are excluded. [sent-76, score-0.258]
</p><p>47 As these switches are parameterized globally, ranges of a parameter of a neuron group(one quarter of a HICANN) need to be in the same order of magnitude. [sent-79, score-0.333]
</p><p>48 3  metal-oxide-semiconductor ﬁeld-effect transistor  4  Table 1: Neuron parameters PARAMETER  SHARING  gl a gadapt Ib tpulse Vreset Vexp treset Cmem Cadapt ∆t Θ  individual individual individual individual ﬁxed global individual global global ﬁxed individual individual  RANGE 34 nS. [sent-80, score-0.271]
</p><p>49 The chosen ranges allow leakage time constants τmem = Cmem /gl at an acceleration factor of 104 between 1 ms and 588 ms and an adaptation time constant τw between 10 ms and 5 s in terms of biological real time. [sent-104, score-0.41]
</p><p>50 As OTAs are used for modeling conductances, and linear operation for this type of devices can only be achieved for smaller voltage differences, it is necessary to limit the operating range of the variables V and Vadapt to some hundreds of millivolts. [sent-107, score-0.229]
</p><p>51 If this area is left, the OTAs will not work as a conductance anymore, but as a constant current, hence there will not be any more spike triggered adaptation for example. [sent-108, score-0.374]
</p><p>52 A neuron can be composed of up to 64 DenMem circuit hence several different adaptation variables with different time constants for each are allowed. [sent-109, score-0.566]
</p><p>53 3  Parameter mapping  For a given set of parameters from the AdEx model, we want to reproduce the exact same behavior with our hardware neuron. [sent-111, score-0.243]
</p><p>54 Therefore, a simple two-steps procedure was developed to translate biological parameters from the AdEx model to hardware parameters. [sent-112, score-0.32]
</p><p>55 The translation procedure is summarized in ﬁgure 5:  Biological AdEx parameters  Scaling  Scaled AdEx parameters  Translation  Hardware parameters  Figure 5: Biology to hardware parameter translation The ﬁrst step is to scale the biological AdEx parameters in terms of time and voltage. [sent-113, score-0.32]
</p><p>56 Then, a voltage scaling factor is deﬁned, by which the biological voltages parameters are multiplied. [sent-115, score-0.306]
</p><p>57 The second step is to translate the parameters from the scaled AdEx model to hardware parameters. [sent-117, score-0.243]
</p><p>58 For this purpose, each part of the DenMem circuit was characterized in transistor-level simulations using a circuit simulator. [sent-118, score-0.308]
</p><p>59 This theoretical characterization was then used to establish mathematical relations between scaled AdEx parameters and hardware parameters. [sent-119, score-0.243]
</p><p>60 4  Measurement capabilities  For neuron measuring purposes, the membrane can be either stimulated by incoming events from the synapse array - as an additional feature a Poisson event source is implemented on the chip - or by a programmable current. [sent-121, score-0.76]
</p><p>61 Four current sources are implemented on the chip allowing to stimulate adjacent neurons individually. [sent-123, score-0.255]
</p><p>62 The membrane voltage and all stored parameters in the ﬂoating gates can directly be measured via one of the two analog outputs of the HICANN chip. [sent-125, score-0.524]
</p><p>63 To characterize the chip, parameters like the membrane capacitance need to be measured indirectly using the OTA, emulating gl , as a current source example. [sent-127, score-0.362]
</p><p>64 3  Results  Different ﬁring patterns have been reproduced using our hardware neuron and the current stimulus in circuit simulation and in silicon, inducing a periodic step current onto the membrane. [sent-128, score-0.917]
</p><p>65 The examined neuron consists of two DenMem circuits with their membrane capacitances switched to 2 pF each. [sent-129, score-0.673]
</p><p>66 Figure 6 shows results of some reproduced patterns according to [23] or [16] neighbored by their phase plane trajectory of V and Vadapt . [sent-130, score-0.172]
</p><p>67 gadapt and gl have been chosen equal in all simulations except tonic spiking to facilitate the nullclines: gl gl (V − El ) + ∆T e a a =V;  Vadapt = − Vadapt  V −VT ∆T  + El +  I a  (9) (10)  As described in [16], the AdEx model allows different types of spike after potentials (SAP). [sent-133, score-0.837]
</p><p>68 Sharp SAPs are reached if the reset after a spike sets the trajectory to a point, below the V-nullcline. [sent-134, score-0.209]
</p><p>69 If reset ends in a point, above the V-nullcline, the membrane voltage will be pulled down below the reset voltage Vreset by the adaptation current. [sent-135, score-0.965]
</p><p>70 Here, a has been set to zero, while gl has been doubled to keep the total conductance at a similar level. [sent-137, score-0.207]
</p><p>71 Parameters between simulation and measurement are only roughly mapped, as the precise mapping algorithm is still in progress - on a real chip there is a variation of transistor parameters which still needs to be counterbalanced by parameter choice. [sent-138, score-0.222]
</p><p>72 As metric, for adaptation [24] and [16] use the accommodation index:  A=  1 N −k−1  N  i=k  ISIi − ISIi−1 ISIi + ISIi−1  (11)  Here k determines the number of ISI excluded from A to exclude transient behavior [15, 24] and can be chosen as one ﬁfth for small numbers of ISIs [24]. [sent-140, score-0.223]
</p><p>73 The metric calculates the average of the difference between two neighbored ISIs weighted by their sum, so it should be zero for ideal tonic spiking. [sent-141, score-0.16]
</p><p>74 15  0  5  10  15 20 Time[µs]  25  30  0  5  10  15 20 Time[µs]  25  30  0  5  10  15 20 Time[µs]  25  30  0  5  10  15 20 Time[µs]  25  30  0  5  10  15 20 Time[µs]  25  30  (a) Tonic spiking 0. [sent-175, score-0.167]
</p><p>75 75 0  5  10  15 20 Time[µs]  25  30  (d) Tonic burst  Figure 6: Phase plane and transient plot from simulations and measurement results of the neuron stimulated by a step current of 600 nA. [sent-272, score-0.554]
</p><p>76 As parameters have been chosen to reproduce the patterns obviously (adaptation is switched of for tonic spiking and strong for spike frequency adaptation) they are a little bit extreme in comparison to the calculated ones in [24] which are 0. [sent-276, score-0.494]
</p><p>77 It is ambiguous to deﬁne a burst looking just at the spike frequency. [sent-281, score-0.215]
</p><p>78 To generate bursting behavior, the reset has to be set to a value above the exponential threshold so that V is pulled upwards by the exponential directly after a spike. [sent-284, score-0.294]
</p><p>79 As can be seen in ﬁgure 1, depending on the sharpness ∆t of the exponential term, the exact reset voltage Vr might be critical in bursting, when reseting above the exponential threshold and the nullcline is already steep at this point. [sent-285, score-0.495]
</p><p>80 The AdEx model is capable of irregular spiking in contrast to the Izhikevich neuron [25] which uses a quadratic term to simulate the rise at a spike. [sent-286, score-0.46]
</p><p>81 The 7  chaotic spiking capability of the AdEx model has been shown in [16]. [sent-287, score-0.167]
</p><p>82 In Hardware, we observe that it is common to reach regimes, where the exact number of spikes in a burst is not constant, thus the distance to the next spike or burst may differ in the next period. [sent-288, score-0.309]
</p><p>83 Another effect is that if the equilibrium potential - the potential, where the nullclines cross - is near Vt , noise may cause the membrane to cross Vt and hence generate a spike (Compare phase planes in ﬁgure 6 c) and d) ). [sent-289, score-0.416]
</p><p>84 In phasic bursting, the nullclines are still crossing in a stable ﬁx point - the resting potential caused by adaptation, leakage and stimulus is below the ﬁring threshold of the exponential. [sent-291, score-0.28]
</p><p>85 Patterns reproduced in experiment and simulations but not shown here are phasic spiking and initial bursting. [sent-292, score-0.35]
</p><p>86 4  Discussion  The main feature of our neuron is the capability of directly reproducing the AdEx model. [sent-293, score-0.293]
</p><p>87 Nevertheless, it is low power in comparison to simulation on a supercomputer (estimated 100 µW in comparison to 370 mW on a Blue Gene/P [26] at an acceleration factor of 104 , computing time of Izhikevich neuron model [23] used as estimate. [sent-295, score-0.385]
</p><p>88 ) and does not consume much chip area in comparison to the synapse array and communication infrastructure on the HICANN (ﬁgure 2). [sent-296, score-0.212]
</p><p>89 Due to the design approach - implementing an established model instead of developing a new model ﬁtting best to hardware devices - we gain a neuron allowing neuroscientist to do experiments without being a hardware specialist. [sent-302, score-0.779]
</p><p>90 5  Outlook  The neuron topology - several DenMems are interconnected to form a neuron - is predestined to be enhanced to a multi-compartment model. [sent-303, score-0.62]
</p><p>91 The simulations and measurements in this work qualitatively reproduce patterns observed in biology and reproduced by the AdEx model in [16]. [sent-305, score-0.158]
</p><p>92 Nested in the FACETS wafer-scale system, our neuron will complete the universality of the system by a versatile core for analog computation. [sent-308, score-0.403]
</p><p>93 Encapsulation of the parameter mapping into low level software and PyNN [12] integration of the system will allow computational neural scientists to do experiments on the hardware and compare them to simulations, or to do large experiments, currently not implementable in a simulation. [sent-309, score-0.243]
</p><p>94 A current-mode conductance-based silicon neuron for address-event neuromorphic systems. [sent-342, score-0.54]
</p><p>95 Compact silicon neuron circuit with spiking and bursting behaviour. [sent-349, score-0.827]
</p><p>96 Implementing synaptic plasticity in a VLSI spiking u neural network model. [sent-357, score-0.264]
</p><p>97 u u Establishing a novel modeling tool: A python-based interface for a neuromorphic hardware system. [sent-373, score-0.35]
</p><p>98 Realizing biological spiking network models in a conﬁgurable wafer-scale hardware system. [sent-419, score-0.487]
</p><p>99 A wafer-scale neuromorphic u u hardware system for large-scale neural modeling. [sent-428, score-0.35]
</p><p>100 A novel multiple objective optimization framework for constraining conductance-based neuron models by experimental data. [sent-450, score-0.293]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adex', 0.373), ('neuron', 0.293), ('vadapt', 0.266), ('hardware', 0.243), ('voltage', 0.229), ('membrane', 0.185), ('spiking', 0.167), ('adaptation', 0.146), ('silicon', 0.14), ('circuit', 0.127), ('denmem', 0.124), ('schemmel', 0.124), ('tonic', 0.124), ('circuits', 0.123), ('spike', 0.121), ('analog', 0.11), ('conductance', 0.107), ('hicann', 0.107), ('neuromorphic', 0.107), ('chip', 0.1), ('bursting', 0.1), ('gl', 0.1), ('synaptic', 0.097), ('burst', 0.094), ('leakage', 0.094), ('cadapt', 0.089), ('reset', 0.088), ('vt', 0.087), ('neurons', 0.083), ('ijcnn', 0.078), ('nullclines', 0.078), ('vreset', 0.078), ('biological', 0.077), ('synapse', 0.073), ('derle', 0.071), ('emulate', 0.071), ('gadapt', 0.071), ('iscas', 0.071), ('isii', 0.071), ('phasic', 0.071), ('ota', 0.062), ('vlsi', 0.062), ('reproduced', 0.058), ('device', 0.056), ('simulations', 0.054), ('meier', 0.054), ('exponential', 0.053), ('el', 0.053), ('otas', 0.053), ('pynn', 0.053), ('tpulse', 0.053), ('acceleration', 0.053), ('transistor', 0.047), ('conductances', 0.047), ('patterns', 0.046), ('gure', 0.043), ('ib', 0.043), ('adjustable', 0.043), ('henry', 0.043), ('markram', 0.043), ('br', 0.041), ('transient', 0.041), ('capacitance', 0.04), ('networks', 0.04), ('ranges', 0.04), ('sharp', 0.039), ('array', 0.039), ('simulation', 0.039), ('ampli', 0.038), ('biasing', 0.038), ('gate', 0.038), ('alain', 0.038), ('facets', 0.038), ('gaining', 0.038), ('ge', 0.038), ('current', 0.037), ('stimulus', 0.037), ('pf', 0.036), ('switched', 0.036), ('accommodation', 0.036), ('capacitances', 0.036), ('cmem', 0.036), ('counterbalanced', 0.036), ('denmems', 0.036), ('dvadapt', 0.036), ('fieres', 0.036), ('isis', 0.036), ('karlheinz', 0.036), ('livi', 0.036), ('mosfet', 0.036), ('neighbored', 0.036), ('nullcline', 0.036), ('reseting', 0.036), ('synin', 0.036), ('nov', 0.035), ('stimulated', 0.035), ('implemented', 0.035), ('enhanced', 0.034), ('ring', 0.033), ('phase', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="16-tfidf-1" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>2 0.26248118 <a title="16-tfidf-2" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>3 0.17429763 <a title="16-tfidf-3" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>4 0.16483968 <a title="16-tfidf-4" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>Author: Yi-da Wu, Shi-jie Lin, Hsin Chen</p><p>Abstract: The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuoustime paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the logdomain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time. 1</p><p>5 0.13872112 <a title="16-tfidf-5" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>6 0.135794 <a title="16-tfidf-6" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>7 0.12982634 <a title="16-tfidf-7" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>8 0.11683626 <a title="16-tfidf-8" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>9 0.10668322 <a title="16-tfidf-9" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>10 0.10110703 <a title="16-tfidf-10" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>11 0.086963408 <a title="16-tfidf-11" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>12 0.084459327 <a title="16-tfidf-12" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>13 0.081029139 <a title="16-tfidf-13" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>14 0.077088721 <a title="16-tfidf-14" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>15 0.067331649 <a title="16-tfidf-15" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>16 0.063673995 <a title="16-tfidf-16" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>17 0.060854483 <a title="16-tfidf-17" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>18 0.059227854 <a title="16-tfidf-18" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>19 0.053138662 <a title="16-tfidf-19" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>20 0.050621834 <a title="16-tfidf-20" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.023), (2, -0.166), (3, 0.206), (4, 0.091), (5, 0.235), (6, -0.067), (7, 0.097), (8, 0.09), (9, -0.032), (10, 0.023), (11, 0.076), (12, 0.05), (13, 0.078), (14, 0.066), (15, 0.016), (16, 0.041), (17, -0.087), (18, -0.018), (19, -0.102), (20, 0.003), (21, 0.065), (22, -0.113), (23, -0.009), (24, -0.043), (25, -0.056), (26, 0.045), (27, -0.075), (28, 0.002), (29, -0.12), (30, 0.047), (31, 0.033), (32, 0.005), (33, -0.13), (34, 0.07), (35, 0.035), (36, 0.099), (37, 0.047), (38, 0.008), (39, 0.02), (40, -0.017), (41, -0.001), (42, 0.104), (43, -0.067), (44, -0.007), (45, 0.032), (46, -0.016), (47, 0.009), (48, 0.117), (49, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97028315 <a title="16-lsi-1" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>2 0.85550416 <a title="16-lsi-2" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>3 0.79789138 <a title="16-lsi-3" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>4 0.76223534 <a title="16-lsi-4" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>5 0.73660392 <a title="16-lsi-5" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>6 0.69346392 <a title="16-lsi-6" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>7 0.65494972 <a title="16-lsi-7" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>8 0.575773 <a title="16-lsi-8" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>9 0.54342216 <a title="16-lsi-9" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>10 0.4661662 <a title="16-lsi-10" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>11 0.42516005 <a title="16-lsi-11" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>12 0.39663231 <a title="16-lsi-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.39561582 <a title="16-lsi-13" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>14 0.39221996 <a title="16-lsi-14" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>15 0.32556814 <a title="16-lsi-15" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>16 0.29791877 <a title="16-lsi-16" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>17 0.29581404 <a title="16-lsi-17" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>18 0.27415791 <a title="16-lsi-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.2433825 <a title="16-lsi-19" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>20 0.22622156 <a title="16-lsi-20" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.026), (22, 0.023), (27, 0.05), (30, 0.027), (45, 0.104), (50, 0.031), (52, 0.027), (60, 0.011), (63, 0.011), (77, 0.584), (90, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91089207 <a title="16-lda-1" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>Author: K. Wong, He Wang, Si Wu, Chi Fung</p><p>Abstract: Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We ﬁnd that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we ﬁnd that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p><p>same-paper 2 0.89043725 <a title="16-lda-2" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>3 0.80408782 <a title="16-lda-3" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>4 0.78341901 <a title="16-lda-4" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>5 0.70514917 <a title="16-lda-5" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>Author: Corinna Cortes, Yishay Mansour, Mehryar Mohri</p><p>Abstract: This paper presents an analysis of importance weighting for learning from ﬁnite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and, more signiﬁcantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the R´ nyi divergence of the training and test distributions. e These results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest. We use these bounds to guide the deﬁnition of an alternative reweighting algorithm and report the results of experiments demonstrating its beneﬁts. Finally, we analyze the properties of normalized importance weights which are also commonly used.</p><p>6 0.64941263 <a title="16-lda-6" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>7 0.5824458 <a title="16-lda-7" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>8 0.53937596 <a title="16-lda-8" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>9 0.53560865 <a title="16-lda-9" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>10 0.5231548 <a title="16-lda-10" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>11 0.52105707 <a title="16-lda-11" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>12 0.51196998 <a title="16-lda-12" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>13 0.48698363 <a title="16-lda-13" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>14 0.48296127 <a title="16-lda-14" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>15 0.45535943 <a title="16-lda-15" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>16 0.41871345 <a title="16-lda-16" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>17 0.41415757 <a title="16-lda-17" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>18 0.40426627 <a title="16-lda-18" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>19 0.40048075 <a title="16-lda-19" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>20 0.3961747 <a title="16-lda-20" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
