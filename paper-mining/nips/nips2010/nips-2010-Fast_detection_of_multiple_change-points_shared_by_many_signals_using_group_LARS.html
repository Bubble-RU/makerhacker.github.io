<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-91" href="#">nips2010-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</h1>
<br/><p>Source: <a title="nips-2010-91-pdf" href="http://papers.nips.cc/paper/4157-fast-detection-of-multiple-change-points-shared-by-many-signals-using-group-lars.pdf">pdf</a></p><p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>Reference: <a title="nips-2010-91-reference" href="../nips2010_reference/nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Fast detection of multiple change-points shared by many signals using group LARS  Jean-Philippe Vert and Kevin Bleakley Mines ParisTech CBIO, Institut Curie, INSERM U900 {firstname. [sent-1, score-0.476]
</p><p>2 fr  Abstract We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. [sent-3, score-0.313]
</p><p>3 We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. [sent-4, score-0.177]
</p><p>4 1  Introduction  Finding the place (or time) where most or all of a set of one-dimensional signals (or proﬁles) jointly change in some speciﬁc way is an important question in several ﬁelds. [sent-5, score-0.15]
</p><p>5 A ﬁrst common situation is when we want to ﬁnd change-points in a multidimensional signal, for instance, we may want to automatically detect changes from human speech to other sound in a movie, based on data representation of features coming from both the audio and visual tracks [1]. [sent-6, score-0.419]
</p><p>6 Another important situation is when we are confronted with several 1-dimensional signals which we believe share common change-points, e. [sent-7, score-0.167]
</p><p>7 The latter application is increasingly important in biology and medicine, in particular for the detection of copy-number variation along the genome, though it is also useful for microarray and genetic linkage studies [2]. [sent-10, score-0.159]
</p><p>8 The common thread in all of these is the search for data patterns shared by a set of patients at precise places on the genome; in particular, sudden changes in measurement. [sent-11, score-0.21]
</p><p>9 As opposed to the segmentation of multi-dimensional signals such as speech, the length of the signal (i. [sent-12, score-0.357]
</p><p>10 , the number of probes along the genome) is ﬁxed for a given technology while the number of signals (i. [sent-14, score-0.272]
</p><p>11 It is therefore of interest to develop method to identify multiple change-points shared by several signals which can beneﬁt from increasing the number of proﬁles. [sent-17, score-0.379]
</p><p>12 There exists a vast literature on the change-point detection problem [3, 4]. [sent-18, score-0.145]
</p><p>13 Here we focus on the problem of approximating a multidimensional signal by a piecewise-constant one, using quadratic error criteria. [sent-19, score-0.245]
</p><p>14 It is well-known that the optimal segmentation of a p-dimensional signal of length n into k segments can be obtained in O(n2 pk) by dynamic programming [5, 6, 7]. [sent-20, score-0.238]
</p><p>15 While such recursive methods can be extremely fast, in the order of O(np log(k)) when the single change-point detector is O(np), quality of segmentation is questionable when compared with global procedures [9]. [sent-23, score-0.116]
</p><p>16 The resulting piecewise-constant approximation of the signal, deﬁned as the global minimum of the objective function, beneﬁts from 1  theoretical guaranties in terms of correctly detecting change-points [12, 13], and can be implemented efﬁciently in O(nk) or O(n log(n)) [14, 12, 15]. [sent-25, score-0.126]
</p><p>17 In this paper we propose an extension of total-variation based methods for single signals to the multidimensional setting, in order to approximate a multidimensional signal by a piecewise constant signal with multiple change-points. [sent-26, score-0.642]
</p><p>18 We deﬁne the approximation as the solution of a convex optimization problem, which involves a quadratic approximation error penalized by the 1 norm of increments of the function. [sent-27, score-0.125]
</p><p>19 The problem can be reformulated as a group LASSO problem, which we propose to solve approximately with a group LARS procedure [16]. [sent-28, score-0.224]
</p><p>20 Using the particular structure of the design matrix, we can ﬁnd the ﬁrst k change-points in O(npk), extending the method of [12] to the multidimensional setting. [sent-29, score-0.202]
</p><p>21 Unlike most previous theoretical investigations of change-point methods, we are not interested in the case where the dimension p is ﬁxed and the length of the proﬁles n increases, but in the opposite situation where n is ﬁxed and p increases. [sent-30, score-0.135]
</p><p>22 Indeed, this corresponds to the case in genomics where, for example, n would be the ﬁxed number of probes used to measure a signal along the genome, and p the number of samples or patients analyzed. [sent-31, score-0.357]
</p><p>23 We want to design a method that beneﬁts from increasing p in order to identify shared change-points, even though the signal-to-noise ratio may be very low within each signal. [sent-32, score-0.291]
</p><p>24 As a ﬁrst step towards this question, we give conditions under which our method is able to consistently identify a single change-point as p increases. [sent-33, score-0.129]
</p><p>25 We also show by simulation that our method is able to consistently identify multiple change-points, as p → +∞, validating its relevance in practical settings. [sent-34, score-0.162]
</p><p>26 To conclude, we present possible applications of the method in the study of copy number variations in cancer. [sent-35, score-0.082]
</p><p>27 3  Formulation  We consider p proﬁles of length n, stored in an n × p matrix Y . [sent-53, score-0.103]
</p><p>28 We assume that each proﬁle is a piecewise-constant signal corrupted by noise, and that change-points locations tend to be shared across proﬁles. [sent-58, score-0.249]
</p><p>29 Our goal is to detect these shared change-points, and beneﬁt from the possibly large number p of proﬁles to increase the statistical power of change-point detection. [sent-59, score-0.179]
</p><p>30 When p = 1 (single proﬁle), a popular method to ﬁnd change-points in a signal is to approximate it by a piecewise constant function using total variation (TV) denoising [10], i. [sent-60, score-0.222]
</p><p>31 Adding penalties proportional to the 1 ot 2 norm of U to (1) does not change the position of the change-points detected [11, 17], and the capacity of TV denoising to correctly identify change-points when n increases has been investigated in [12, 13]. [sent-64, score-0.332]
</p><p>32 i=1  2  (2)  The second term in (2) penalizes the sum of Euclidean norms of the increments of U , seen as a time-dependent multidimensional vector. [sent-66, score-0.21]
</p><p>33 Intuitively, this penalty will enforce many increments Ui+1,• − Ui,• to collapse to 0, just like the total variation in (1). [sent-67, score-0.114]
</p><p>34 In the following, we propose a fast algorithm to approximately solve (2) (Section 4), discuss theoretically whether the solution identiﬁes correctly the change-points (Section 5), and provide an empirical evaluation of the method (Section 6). [sent-69, score-0.225]
</p><p>35 In the single proﬁle case (p = 1), [14] proposed a fast coordinate descent-like method, [12] showed how to ﬁnd the ﬁrst k change-points iteratively in O(nk), and [15] proposed an O(n ln(n)) method to ﬁnd all change-points. [sent-72, score-0.11]
</p><p>36 In order to propose a fast method to solve (2) in the p > 1 setting, let us ﬁrst reformulate it as a group LASSO regression problem [16]. [sent-74, score-0.208]
</p><p>37 In other words βi,j is the jump between the i-th and the (i + 1)-th positions of the j-th proﬁle. [sent-79, score-0.1]
</p><p>38 j=1  This can be rewritten in matrix form as U = 1n,1 γ + Xβ , where X is the n × (n − 1) matrix with entries Xi,j = 1 for i > j. [sent-84, score-0.094]
</p><p>39 Plugging this into (3), we get that the matrix of jumps β is solution of min  ¯ ¯ Y − Xβ  β∈R(n−1)×p  n−1 2  +λ  βi,• ,  (4)  i=1  ¯ ¯ where Y and X are obtained from Y and X by centering each column. [sent-87, score-0.181]
</p><p>40 Equation 4 is a group LASSO problem, with a particular design matrix and particular groups of features. [sent-88, score-0.179]
</p><p>41 More precisely, the group LARS approximates the solution path of (4) with a piecewiseafﬁne set of solutions, and iteratively ﬁnds change-points. [sent-90, score-0.185]
</p><p>42 While the original group LARS method requires storing and manipulation of the design matrix [16], which we can not afford here, we can ¯ extend technical results of [12] to show that the particular structure of the design matrix X allows efﬁcient computation of matrix inverses and products. [sent-91, score-0.335]
</p><p>43 < a|A| ≤ n, the ¯ ¯ matrix X•,A X•,A is invertible, and for any |A| × p matrix R, the matrix ¯ ¯ C = X•,A X•,A  −1  R  can be computed in O(|A|p) time and memory. [sent-101, score-0.141]
</p><p>44 Algorithm 1 describes the fast group LARS method to approximately solve (4). [sent-103, score-0.208]
</p><p>45 At each subsequent iteration to ﬁnd the next change-point, we follow steps 3–8 which have maximum complexity O(np), resulting in O(npk) complexity in time and O(np) in memory to ﬁnd the ﬁrst k changepoints with the fast group LARS algorithm. [sent-104, score-0.152]
</p><p>46 Algorithm 1 Fast group LARS algorithm ¯ Require: centered data Y , number of breakpoints k. [sent-105, score-0.201]
</p><p>47 u ˆ 9: end for  5  Theoretical analysis  In this section, we study theoretically to what extent the estimator (2) recovers correct change-points. [sent-113, score-0.083]
</p><p>48 The vast majority of existing theoretical results for ofﬂine segmentation and change-point detection consider the setting where p is ﬁxed (usually p = 1), and n increases. [sent-114, score-0.253]
</p><p>49 This typically corresponds to a setting where we can sample a continuous signal with increasing density, and wish to locate more precisely the underlying change-points as the density increases. [sent-115, score-0.217]
</p><p>50 Here, the length of proﬁles n is ﬁxed for a given technology, but the number of proﬁles p can increase when more biological samples or patients are analyzed. [sent-117, score-0.117]
</p><p>51 The property we would like to study is then, for a given change-point detection method, whether increasing p for ﬁxed n allows us to locate more precisely the change-points. [sent-118, score-0.227]
</p><p>52 While this simply translates our intuition that increasing the number of proﬁles should increase the statistical power of change-point detection, and while this property was empirically observed in [2], we are not aware of previous theoretical results in this setting. [sent-119, score-0.11]
</p><p>53 In other words, we assume that ¯ ¯ Y = Xβ ∗ + W , ∗ ∗ where β is an (n−1)×p matrix of zeros except for the u-th row βu,• , and W is a noise matrix whose entries are assumed to be independent and identically distributed with respect to a centered Gaussian 4  distribution with variance σ 2 . [sent-122, score-0.198]
</p><p>54 In this section we study the probability that the ﬁrst breakpoint found by our procedure is the correct one, when p increases. [sent-123, score-0.12]
</p><p>55 We therefore consider an inﬁnite sequence of k ∗ ∗ ¯ ¯2 ¯2 jumps βu,i i≥1 , and letting βk = 1/k i=1 (βu,i )2 , we assume that β 2 = limk→∞ βk exists and is ﬁnite. [sent-124, score-0.107]
</p><p>56 σα = nβ 2 ˜2 1 1 α − 2 − 2n  (6)  When σ 2 < σα , the probability that the ﬁrst selected change-point is the correct one tends to 1 as ˜2 p → +∞. [sent-134, score-0.104]
</p><p>57 When σ 2 > σα , it is not the correct one with probability tending to 1. [sent-135, score-0.109]
</p><p>58 • To detect a change-point at position u = αn, the noise level σ 2 must not be larger than the critical value σα given by (7), hence the method is not consistent for all positions. [sent-137, score-0.249]
</p><p>59 σα increases monotonically from α = 1/2 to 1, meaning that change-points near the boundary are more difﬁcult to detect correctly than change-points near the center. [sent-138, score-0.154]
</p><p>60 The most difﬁcult change point is the last one (u = n − 1) which can only be detected consistently if σ 2 is smaller than ¯ 2β 2 σ1−1/n = ¯2 + o(n−1 ). [sent-139, score-0.128]
</p><p>61 n • For a given level of noise σ 2 , change-point detection is asymptotically correct for any α ∈ [ , 1 − ], where satisﬁes σ 2 = σ1− , i. [sent-140, score-0.21]
</p><p>62 ¯ + o(n 2nβ 2  This shows in particular that increasing the proﬁle length n increases the interval where change-points are correctly identiﬁed, and that we can get as close as possible to the boundary for n large enough. [sent-143, score-0.191]
</p><p>63 2 • When σ 2 < σα then the correct change-point is found consistently when p increases, showing the beneﬁt of the accumulation of many proﬁles. [sent-144, score-0.105]
</p><p>64 • It is possible to make the detection of the ﬁrst change-point consistent uniformly over the full signal, by simply subtracting the term pσ 2 i(n−i)/n from ci,• 2 , which is maximized ¯ over i to select the ﬁrst change-point. [sent-145, score-0.115]
</p><p>65 However, this modiﬁcation, easy to do for the ﬁrst change-point, is not obvious to extend to successive change-points detected by group LARS. [sent-149, score-0.172]
</p><p>66 We consider it an interesting future challenge to develop variants of the group LARS iterative segmentation method whose performance does not depend on the position of the change points. [sent-150, score-0.302]
</p><p>67 Assuming that the support of PU is [a, b] with 1 ≤ a ≤ b ≤ n − 1, the following result extends Theorem 4 by showing that, under a condition on the noise level, the ﬁrst change-point discovered is indeed in the support of PU : Theorem 5. [sent-160, score-0.112]
</p><p>68 Let α = U/n be the random position of the change-point on [0, 1] and αm = a/n and αM = b/n the position of the left and right boundaries of the support of PU scaled to [0, 1]. [sent-161, score-0.166]
</p><p>69 σPU = nβ 2 ˜2 (7) 1 1 αm − 2 − 2n If 1/2 ∈ (αm , αM ), then for any σ 2 the probability that the ﬁrst selected change-point is in the support of P tends to 1 as p → +∞. [sent-163, score-0.091]
</p><p>70 If 1/2 < αm , then the probability that the ﬁrst selected change-point is in the support of P tends to 1 when σ 2 < σα , . [sent-164, score-0.091]
</p><p>71 When σ 2 > σα , it is not the correct ˜2 ˜2 one with probability tending to 1. [sent-165, score-0.109]
</p><p>72 This theorem, whose proof is postponed to Supplementary Materials, illustrates the robustness of the method to handle ﬂuctuations in the precise position of the change-point shared between the proﬁles. [sent-166, score-0.296]
</p><p>73 Although this situation rarely occurs when we are considering classical multidimensional signals such as ﬁnancial time series or video signals, it is likely to be the rule when we consider proﬁles coming from different biological samples. [sent-167, score-0.341]
</p><p>74 3  The case of multiple change-points  While the theoretical results presented above focus on the detection of a single change-point, the real interest of the method is to estimate multiple change-points. [sent-170, score-0.24]
</p><p>75 We nevertheless conjecture here that we can consistently estimate multiple change-points under conditions on the level of noise (not too large), the distance between them (not to small), and the correlations between their jumps (not too large). [sent-172, score-0.254]
</p><p>76 ) must hit a hypersphere at each correct change-point, and must c remain strictly within the hypersphere between consecutive change-points. [sent-180, score-0.183]
</p><p>77 This can be ensured if the noise level is not too high (like in the single change-point case), and if the positions corresponding to successive change-points on the hypersphere are far enough from each other. [sent-181, score-0.229]
</p><p>78 In practice this translates to conditions that two successive change-points should not be too close to each other, and that proﬁles should have, if possible, independent jumps (direction, etc. [sent-182, score-0.176]
</p><p>79 We provide experimental results below that conﬁrm that, when the noise is not too large, we can indeed correctly identify several change-points, with a probability of success increasing to 1 as p increases. [sent-184, score-0.188]
</p><p>80 To test Theorem 4, we considered signals of length 100, each with a unique change-point located at position u. [sent-187, score-0.273]
</p><p>81 8; assuming for simplicity that each signal jumps a height of 1 at ¯ the change-point, we get β 2 = 1, and it is then easy to calculate the critical value σα = 10. [sent-189, score-0.245]
</p><p>82 We ˜2 2 set the variance of the centered Gaussian noise added to each signal to σα , and ran 1000 trials for ˜ each u. [sent-191, score-0.238]
</p><p>83 (a) CPU time for ﬁnding 50 change-points when there are 2000 probes and the number of proﬁles varies from 1 to 20. [sent-218, score-0.153]
</p><p>84 (b) CPU time when ﬁnding 50 change-points with the number of proﬁles ﬁxed at 20 and the number of probes varying from 1000 to 10000 in intervals of 1000. [sent-219, score-0.153]
</p><p>85 (c) CPU time for 20 proﬁles and 2000 probes when selecting from 1 to 50 change-points. [sent-220, score-0.153]
</p><p>86 Accuracy as a function of the number of proﬁles p when the change-point is placed in a variety of positions from: u = 50 to u = 90 (left panel), or: u = 50 ± 2 to u = 90 ± 2 (right panel), for a signal of length 100. [sent-230, score-0.229]
</p><p>87 The right-hand-side panel of Figure 2 shows results for the same trials except that change-point locations can vary uniformly in the interval u ± 2. [sent-231, score-0.122]
</p><p>88 To investigate the potential for extending the results of the article to the case of many shared change-points, we further simulated proﬁles of length 100 with a change-point at all of positions 10, 20, . [sent-233, score-0.235]
</p><p>89 The jump at each change-point was drawn from a centered Gaussian with variance 1. [sent-237, score-0.084]
</p><p>90 We then ﬁxed various values of σ 2 and looked at convergence in accuracy as the number of signals increased. [sent-238, score-0.16]
</p><p>91 An interesting application of the fast group LARS method is in the joint segmentation of copynumber proﬁles. [sent-246, score-0.257]
</p><p>92 a type of cancer), we expect there to be regions of the genome which are frequently gained (potentially containing oncogenes) or lost (potentially containing tumor suppressor genes) in many or all of the patients. [sent-249, score-0.089]
</p><p>93 Figure 4 shows Chromosome 8 of three bladder cancer copy-number proﬁles. [sent-251, score-0.118]
</p><p>94 We see that in the region of probe 60, a copy number change occurs on all three proﬁles. [sent-252, score-0.205]
</p><p>95 The bottom right panel shows the smoothed proﬁles superimposed on the same axes. [sent-271, score-0.109]
</p><p>96 A promising use of these smoothed signals, beyond visualization of many proﬁles simultaneously, is to detect regions of frequent gain of loss by testing the average proﬁle values on each segment for signiﬁcant positive (gain) or negative (loss) values. [sent-272, score-0.117]
</p><p>97 5  −1  0  50  100  −1  150  Probe  0  50 Probe  Figure 4: Segmented and smoothed bladder cancer copy-number proﬁles. [sent-282, score-0.167]
</p><p>98 A shared change-point hotspot is found in the region of probe 60. [sent-284, score-0.231]
</p><p>99 We observed both theoretically and empirically that increasing the number of proﬁles is highly beneﬁcial to detect shared change-points Acknowledgements We thank Zaid Harchaoui and Francis Bach for useful discussions. [sent-287, score-0.268]
</p><p>100 Adaptive detection of multiple change-points in asset price volatility. [sent-346, score-0.148]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pro', 0.514), ('les', 0.441), ('lars', 0.22), ('probes', 0.153), ('ui', 0.144), ('multidimensional', 0.14), ('pu', 0.134), ('probe', 0.12), ('signals', 0.119), ('npk', 0.117), ('detection', 0.115), ('shared', 0.111), ('jumps', 0.107), ('signal', 0.105), ('group', 0.098), ('genome', 0.089), ('harchaoui', 0.087), ('teyssi', 0.087), ('le', 0.081), ('ua', 0.077), ('breakpoint', 0.077), ('segmentation', 0.077), ('np', 0.074), ('increments', 0.07), ('hypersphere', 0.07), ('position', 0.068), ('positions', 0.068), ('detect', 0.068), ('tending', 0.066), ('consistently', 0.062), ('patients', 0.061), ('panel', 0.06), ('cancer', 0.06), ('bladder', 0.058), ('lavielle', 0.058), ('length', 0.056), ('fused', 0.055), ('fast', 0.054), ('copy', 0.054), ('lemma', 0.053), ('noise', 0.052), ('centered', 0.052), ('breakpoints', 0.051), ('postponed', 0.051), ('lasso', 0.051), ('increasing', 0.049), ('smoothed', 0.049), ('correctly', 0.048), ('situation', 0.048), ('matrix', 0.047), ('cpu', 0.047), ('xa', 0.047), ('detecting', 0.047), ('uctuations', 0.046), ('tv', 0.046), ('denoising', 0.045), ('chromosome', 0.044), ('variation', 0.044), ('correct', 0.043), ('accuracy', 0.041), ('theoretically', 0.04), ('identify', 0.039), ('successive', 0.039), ('procedures', 0.039), ('au', 0.038), ('genomics', 0.038), ('increases', 0.038), ('precise', 0.038), ('materials', 0.037), ('bene', 0.035), ('detected', 0.035), ('theorem', 0.035), ('audio', 0.035), ('millions', 0.035), ('speech', 0.034), ('selected', 0.034), ('design', 0.034), ('coming', 0.034), ('locations', 0.033), ('multiple', 0.033), ('critical', 0.033), ('path', 0.032), ('precisely', 0.032), ('jump', 0.032), ('theoretical', 0.031), ('change', 0.031), ('locate', 0.031), ('translates', 0.03), ('want', 0.03), ('support', 0.03), ('vast', 0.03), ('located', 0.03), ('trials', 0.029), ('solve', 0.028), ('method', 0.028), ('norm', 0.028), ('location', 0.028), ('iteratively', 0.028), ('solution', 0.027), ('tends', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="91-tfidf-1" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>2 0.29512212 <a title="91-tfidf-2" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>3 0.12640692 <a title="91-tfidf-3" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>4 0.11601491 <a title="91-tfidf-4" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><p>5 0.087846503 <a title="91-tfidf-5" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>6 0.082532644 <a title="91-tfidf-6" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>7 0.074077323 <a title="91-tfidf-7" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>8 0.073991515 <a title="91-tfidf-8" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>9 0.071421683 <a title="91-tfidf-9" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>10 0.071149699 <a title="91-tfidf-10" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>11 0.069372326 <a title="91-tfidf-11" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>12 0.066705465 <a title="91-tfidf-12" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>13 0.065723173 <a title="91-tfidf-13" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>14 0.064083979 <a title="91-tfidf-14" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>15 0.063894324 <a title="91-tfidf-15" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>16 0.063677683 <a title="91-tfidf-16" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>17 0.060198948 <a title="91-tfidf-17" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>18 0.059822701 <a title="91-tfidf-18" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>19 0.059134036 <a title="91-tfidf-19" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>20 0.058499012 <a title="91-tfidf-20" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, 0.043), (2, -0.001), (3, 0.093), (4, 0.006), (5, -0.059), (6, -0.034), (7, 0.067), (8, -0.061), (9, 0.008), (10, 0.027), (11, 0.072), (12, -0.026), (13, 0.02), (14, 0.062), (15, -0.004), (16, 0.014), (17, 0.135), (18, 0.038), (19, -0.049), (20, -0.056), (21, 0.132), (22, 0.124), (23, 0.069), (24, 0.093), (25, -0.095), (26, 0.049), (27, -0.07), (28, -0.108), (29, 0.095), (30, -0.049), (31, 0.059), (32, 0.016), (33, -0.138), (34, 0.065), (35, -0.235), (36, -0.06), (37, -0.0), (38, -0.118), (39, 0.211), (40, 0.024), (41, 0.015), (42, 0.056), (43, 0.112), (44, -0.083), (45, 0.053), (46, -0.099), (47, -0.092), (48, -0.274), (49, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95349592 <a title="91-lsi-1" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>2 0.84913778 <a title="91-lsi-2" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>3 0.46901163 <a title="91-lsi-3" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>4 0.45942447 <a title="91-lsi-4" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><p>5 0.39843366 <a title="91-lsi-5" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>Author: Yu Zhang, Dit-Yan Yeung, Qian Xu</p><p>Abstract: Recently, some variants of the đ?&lsquo;&trade;1 norm, particularly matrix norms such as the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms by considering a family of đ?&lsquo;&trade;1,đ?&lsquo;ž norms for 1 < đ?&lsquo;ž â&permil;¤ â&circ;ž and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the đ?&lsquo;&trade;1,đ?&lsquo;ž norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including đ?&lsquo;ž, automatically. Experiments have been conducted on two cancer classiďŹ cation applications using microarray gene expression data. 1</p><p>6 0.39689901 <a title="91-lsi-6" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>7 0.36342102 <a title="91-lsi-7" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>8 0.35117003 <a title="91-lsi-8" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>9 0.33797249 <a title="91-lsi-9" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>10 0.33030024 <a title="91-lsi-10" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>11 0.31973687 <a title="91-lsi-11" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>12 0.31895882 <a title="91-lsi-12" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>13 0.31479102 <a title="91-lsi-13" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>14 0.30778596 <a title="91-lsi-14" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>15 0.30778351 <a title="91-lsi-15" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>16 0.30192244 <a title="91-lsi-16" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>17 0.2959539 <a title="91-lsi-17" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>18 0.2866514 <a title="91-lsi-18" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>19 0.28546512 <a title="91-lsi-19" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>20 0.27988622 <a title="91-lsi-20" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.039), (17, 0.353), (27, 0.051), (30, 0.059), (35, 0.039), (45, 0.178), (50, 0.042), (52, 0.06), (60, 0.036), (77, 0.044), (90, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80412728 <a title="91-lda-1" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>2 0.79174072 <a title="91-lda-2" href="./nips-2010-Copula_Bayesian_Networks.html">53 nips-2010-Copula Bayesian Networks</a></p>
<p>Author: Gal Elidan</p><p>Abstract: We present the Copula Bayesian Network model for representing multivariate continuous distributions, while taking advantage of the relative ease of estimating univariate distributions. Using a novel copula-based reparameterization of a conditional density, joined with a graph that encodes independencies, our model offers great ﬂexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals. We demonstrate the advantage of our framework for generalization over standard Bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature. 1</p><p>3 0.77949178 <a title="91-lda-3" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>Author: James Petterson, Tibério S. Caetano</p><p>Abstract: Multi-label classiﬁcation is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classiﬁcation and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classiﬁcation admit relaxations that can be efﬁciently optimised. We optimise these relaxations with standard algorithms and compare our results with several stateof-the-art methods, showing excellent performance. 1</p><p>4 0.74464953 <a title="91-lda-4" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>Author: Koray Kavukcuoglu, Pierre Sermanet, Y-lan Boureau, Karol Gregor, Michael Mathieu, Yann L. Cun</p><p>Abstract: We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting ﬁlters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efﬁciency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efﬁcient feed-forward encoder that predicts quasisparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse ﬁlters, including center-surround ﬁlters, corner detectors, cross detectors, and oriented grating detectors. We show that using these ﬁlters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks. 1</p><p>5 0.62494123 <a title="91-lda-5" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>6 0.57586634 <a title="91-lda-6" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>7 0.57417792 <a title="91-lda-7" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>8 0.57339293 <a title="91-lda-8" href="./nips-2010-Copula_Processes.html">54 nips-2010-Copula Processes</a></p>
<p>9 0.56790721 <a title="91-lda-9" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>10 0.56438822 <a title="91-lda-10" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>11 0.55786973 <a title="91-lda-11" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>12 0.55742306 <a title="91-lda-12" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>13 0.55633426 <a title="91-lda-13" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>14 0.55584443 <a title="91-lda-14" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>15 0.55534106 <a title="91-lda-15" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>16 0.55393124 <a title="91-lda-16" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>17 0.55282897 <a title="91-lda-17" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>18 0.55151284 <a title="91-lda-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.54996526 <a title="91-lda-19" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>20 0.54922825 <a title="91-lda-20" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
