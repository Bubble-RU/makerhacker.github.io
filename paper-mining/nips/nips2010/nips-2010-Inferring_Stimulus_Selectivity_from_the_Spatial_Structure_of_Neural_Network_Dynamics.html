<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-127" href="#">nips2010-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</h1>
<br/><p>Source: <a title="nips-2010-127-pdf" href="http://papers.nips.cc/paper/4051-inferring-stimulus-selectivity-from-the-spatial-structure-of-neural-network-dynamics.pdf">pdf</a></p><p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>Reference: <a title="nips-2010-127-reference" href="../nips2010_reference/nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spont', 0.447), ('stimul', 0.331), ('evok', 0.309), ('jij', 0.264), ('traject', 0.211), ('spat', 0.205), ('network', 0.196), ('respons', 0.169), ('ring', 0.137), ('rmax', 0.127), ('princip', 0.115), ('haim', 0.114), ('extern', 0.111), ('rj', 0.108), ('ri', 0.106), ('recur', 0.101), ('neuron', 0.1), ('unit', 0.099), ('phas', 0.099), ('subspac', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="127-tfidf-1" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>2 0.20615348 <a title="127-tfidf-2" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>3 0.19733255 <a title="127-tfidf-3" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>4 0.17700121 <a title="127-tfidf-4" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>5 0.16196521 <a title="127-tfidf-5" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>6 0.15096074 <a title="127-tfidf-6" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>7 0.14302999 <a title="127-tfidf-7" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>8 0.13912086 <a title="127-tfidf-8" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>9 0.13422203 <a title="127-tfidf-9" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>10 0.13344267 <a title="127-tfidf-10" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>11 0.13218702 <a title="127-tfidf-11" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>12 0.12471815 <a title="127-tfidf-12" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>13 0.12339643 <a title="127-tfidf-13" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>14 0.10657982 <a title="127-tfidf-14" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>15 0.095565915 <a title="127-tfidf-15" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>16 0.093262099 <a title="127-tfidf-16" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>17 0.087927483 <a title="127-tfidf-17" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>18 0.08165323 <a title="127-tfidf-18" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>19 0.080818087 <a title="127-tfidf-19" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>20 0.077676497 <a title="127-tfidf-20" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, -0.055), (2, -0.249), (3, 0.032), (4, 0.119), (5, 0.072), (6, -0.023), (7, -0.038), (8, 0.122), (9, -0.037), (10, 0.041), (11, 0.013), (12, 0.12), (13, 0.071), (14, 0.031), (15, 0.268), (16, -0.058), (17, 0.04), (18, -0.076), (19, -0.051), (20, -0.015), (21, 0.012), (22, -0.04), (23, 0.048), (24, -0.06), (25, -0.049), (26, -0.068), (27, 0.001), (28, 0.048), (29, 0.009), (30, -0.02), (31, -0.008), (32, 0.022), (33, -0.011), (34, 0.035), (35, -0.001), (36, -0.073), (37, 0.071), (38, 0.066), (39, -0.004), (40, 0.107), (41, -0.022), (42, -0.002), (43, -0.023), (44, -0.073), (45, -0.016), (46, -0.013), (47, -0.036), (48, 0.086), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94378275 <a title="127-lsi-1" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>2 0.80760151 <a title="127-lsi-2" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>3 0.78536564 <a title="127-lsi-3" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>4 0.77944094 <a title="127-lsi-4" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>5 0.71740425 <a title="127-lsi-5" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>6 0.68161786 <a title="127-lsi-6" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>7 0.62214792 <a title="127-lsi-7" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>8 0.59713668 <a title="127-lsi-8" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>9 0.59226394 <a title="127-lsi-9" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>10 0.5875206 <a title="127-lsi-10" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>11 0.54327309 <a title="127-lsi-11" href="./nips-2010-Decoding_Ipsilateral_Finger_Movements_from_ECoG_Signals_in_Humans.html">57 nips-2010-Decoding Ipsilateral Finger Movements from ECoG Signals in Humans</a></p>
<p>12 0.48957396 <a title="127-lsi-12" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>13 0.48952162 <a title="127-lsi-13" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>14 0.47879493 <a title="127-lsi-14" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>15 0.47239122 <a title="127-lsi-15" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>16 0.46313331 <a title="127-lsi-16" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>17 0.44266087 <a title="127-lsi-17" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>18 0.42257041 <a title="127-lsi-18" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>19 0.41918331 <a title="127-lsi-19" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>20 0.41364539 <a title="127-lsi-20" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.163), (30, 0.072), (32, 0.099), (34, 0.111), (45, 0.017), (52, 0.324), (68, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80103528 <a title="127-lda-1" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>2 0.7495876 <a title="127-lda-2" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>3 0.69023997 <a title="127-lda-3" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>4 0.68850678 <a title="127-lda-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>5 0.64475495 <a title="127-lda-5" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>6 0.64385664 <a title="127-lda-6" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>7 0.64072251 <a title="127-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.63582987 <a title="127-lda-8" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>9 0.62896311 <a title="127-lda-9" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>10 0.62636042 <a title="127-lda-10" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>11 0.62445003 <a title="127-lda-11" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>12 0.6178053 <a title="127-lda-12" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>13 0.61671686 <a title="127-lda-13" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>14 0.60947144 <a title="127-lda-14" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>15 0.60871482 <a title="127-lda-15" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>16 0.60825294 <a title="127-lda-16" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>17 0.60752964 <a title="127-lda-17" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>18 0.60737371 <a title="127-lda-18" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>19 0.6054101 <a title="127-lda-19" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>20 0.6047563 <a title="127-lda-20" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
