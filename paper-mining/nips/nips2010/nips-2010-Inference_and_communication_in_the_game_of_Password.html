<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>125 nips-2010-Inference and communication in the game of Password</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-125" href="#">nips2010-125</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>125 nips-2010-Inference and communication in the game of Password</h1>
<br/><p>Source: <a title="nips-2010-125-pdf" href="http://papers.nips.cc/paper/3951-inference-and-communication-in-the-game-of-password.pdf">pdf</a></p><p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>Reference: <a title="nips-2010-125-reference" href="../nips2010_reference/nips-2010-Inference_and_communication_in_the_game_of_Password_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu}  Abstract Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. [sent-4, score-0.974]
</p><p>2 We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. [sent-5, score-0.613]
</p><p>3 Under normal circumstances, a hearer will infer that not all of Joan’s pets are dogs on the grounds that Joan would have expressed herself differently if all of her pets were dogs [1]. [sent-13, score-0.712]
</p><p>4 The hearer should therefore allow for inferences on the part of the speaker (“did she think that saying X would lead me to infer Y? [sent-24, score-0.916]
</p><p>5 In this game, a speaker is supplied with a single, secret word (the password) and must communicate this word to a hearer by choosing a single one-word clue. [sent-29, score-1.059]
</p><p>6 For example, if the password is “mend”, then the speaker might choose “sew” as the clue, and the hearer might guess “stitch” in response. [sent-30, score-1.181]
</p><p>7 Given each password, the top row plots the forward (Sf : password → clue) and backward (Sb : password ← clue) strengths for several potential clues. [sent-32, score-1.039]
</p><p>8 Given this clue, the bottom row plots the forward (Hf : clue → guess) and backward (Hb : clue ← guess) strengths for several potential guesses. [sent-34, score-1.077]
</p><p>9 The guess chosen by the hearer is circled and the password is indicated by an arrow. [sent-35, score-0.874]
</p><p>10 The ﬁrst two columns represent two normal rounds, and the ﬁnal column is a lightning round where speakers and hearers are placed under time pressure. [sent-36, score-0.647]
</p><p>11 The gray dots in each plot show words that are associated with the password (top row) or clue (bottom row) in the University of Southern Florida word association database. [sent-37, score-0.557]
</p><p>12 At ﬁrst sight the optimal strategies for speaker and hearer may seem obvious: the speaker should generate the clue that is associated most strongly with the password, and the hearer should guess the word that is associated most strongly with the clue. [sent-42, score-2.197]
</p><p>13 Given a pair of words such as “shovel” and “snow”, the forward association (shovel → snow) may be strong but the backward association (shovel ← snow) may be weak. [sent-44, score-0.6]
</p><p>14 The third example in Figure 1 shows a case where communication fails because the speaker chooses a clue with a strong forward association but a weak backward association. [sent-45, score-1.148]
</p><p>15 We test this hypothesis by exploring whether speakers and hearers tend to take backward associations into account when generating their clues and guesses. [sent-47, score-0.848]
</p><p>16 Our second hypothesis is that speaker and hearer are calibrated: in other words, that both make accurate assumptions about the strategy used by the other. [sent-48, score-0.92]
</p><p>17 Suppose, for example, that the speaker attempts to make the hearer’s task as easy as possible, and considers only backward associations when choosing his clue. [sent-50, score-0.693]
</p><p>18 This strategy will work best if the hearer considers only forward associates of the clue, but suppose that the hearer considers only backward associations, on the theory that the speaker probably generated his clue by choosing a forward associate. [sent-51, score-2.536]
</p><p>19 In operationalizing this hypothesis we assume that forward associates are easier for people to generate than backward associates. [sent-54, score-0.614]
</p><p>20 A pair of strategies can be calibrated but not cooperative: for example, the speaker and hearer will be calibrated if both agree that the speaker will consider only forward associates, and the hearer will consider only backward associates. [sent-55, score-2.41]
</p><p>21 We ﬁrst present evidence that speakers and hearers are considerate and take both forward and backward associations into account. [sent-58, score-1.042]
</p><p>22 We then develop simple models of the speaker and hearer, and use these models to explore the extent to which speakers and hearers weight forward and backward associations. [sent-59, score-1.218]
</p><p>23 Our results suggest that speakers and hearers are both calibrated and collaborative under normal conditions, but that calibration and collaboration tend to break down under time pressure. [sent-60, score-0.583]
</p><p>24 With each team taking turns, the speaker gives a one-word clue to the hearer and the hearer makes a one-word guess in return. [sent-64, score-1.782]
</p><p>25 The responses of speakers and hearers are likely to depend heavily on word associations, and we can therefore use word association data to model both speakers and hearers. [sent-69, score-0.773]
</p><p>26 The backward strength (wi ← wj ) is proportional to the forward strength (wj → wi ) but is normalized with respect to all forward strengths to wi : (wj → wi ) . [sent-75, score-1.039]
</p><p>27 k (wk → wi )  (wi ← wj ) =  (1)  Note that this normalization ensures that both forward and backward strengths can be treated as probabilities. [sent-76, score-0.709]
</p><p>28 The correlation between forward strengths and backward strengths is positive but low (r = 0. [sent-77, score-0.771]
</p><p>29 32), suggesting that our game show analyses may be able to differentiate the inﬂuence of forward and backward associations. [sent-78, score-0.696]
</p><p>30 Some of the rounds in our game show data include passwords, clues or guesses that do not appear in this lexicon, and we removed these rounds, leaving 68 password-clue and 68 clue-guess pairs in the normal rounds and 86 password-clue pairs and 80 clue-guess pairs in the lightning rounds. [sent-80, score-0.7]
</p><p>31 5  0  0  Bb EbWb Cf  Bf Ef Wf Cb  Bb EbWb Cf  Bf Ef Wf Cb  Bb EbWb Cf  Bf Ef Wf Cb  0  Bb EbWb Cf  Bf Ef Wf Cb  Figure 2: (a) Analyses of the speaker and hearer data (SN and HN ) from the normal rounds. [sent-106, score-0.933]
</p><p>32 Ranks are shown along three dimensions: forward strength (f ), backward strength (b) and combined forward and backward strengths. [sent-108, score-1.135]
</p><p>33 (ii) Ranks of the human responses along the forward and backward dimensions. [sent-111, score-0.607]
</p><p>34 3 Speakers and hearers are considerate A speaker should ﬁnd it easy to generate clues that are strong forward associates of a password, and a hearer should likewise ﬁnd it easy to generate guesses that are strong forward associates of a clue. [sent-118, score-1.863]
</p><p>35 A considerate speaker, however, may attempt to generate strong backward associates, which will make it easier for the hearer to successfully guess the password. [sent-119, score-1.055]
</p><p>36 Similarly, a hearer who considers the task faced by the speaker should also take backward associates into account. [sent-120, score-1.299]
</p><p>37 i compares forward and backward strengths as predictors of the responses chosen by speakers and hearers. [sent-123, score-0.857]
</p><p>38 i, Sf and Sb represent forward (password → clue) and backward (password ← clue) strengths for the speaker, and Hf and Hb represent forward (clue → guess) and 4  backward (clue ← guess) strengths for the hearer. [sent-127, score-1.29]
</p><p>39 In addition to forward and backward strengths, we also considered word frequency as a predictor. [sent-128, score-0.604]
</p><p>40 Across both normal (SN and HN ) and lightning (SL and HL ) rounds, the ranks along the forward and backward dimensions are substantially better than ranks along the frequency dimension (p < 0. [sent-129, score-1.02]
</p><p>41 01 in pairwise t-tests), and we therefore focus on forward and backward strengths for the rest of our analyses. [sent-130, score-0.645]
</p><p>42 For data set SN the mean ranks suggest that forward and backward strengths appear to predict choices about equally well. [sent-131, score-0.721]
</p><p>43 For data set SN , the mean rank based on the Sf + Sb dimension is lower than that for Sf alone, suggesting that backward strengths make a predictive contribution that goes beyond the information present in the forward associations. [sent-134, score-0.756]
</p><p>44 i provides little evidence that backward strengths make a contribution that goes beyond the forward strengths. [sent-137, score-0.682]
</p><p>45 ii plots the rank of each guess along the dimensions of forward and backward strength. [sent-139, score-0.699]
</p><p>46 As a result, the hearer data set HN may offer little opportunity to explore whether backward and forward associations both contribute to people’s responses. [sent-141, score-1.195]
</p><p>47 For example, if the backward dimension matters, then the actual words should tend to be better along the b dimension than words that are matched along the f dimension. [sent-147, score-0.585]
</p><p>48 iii shows the proportion of actual words that are better (Bb), equivalent (Eb) or worse (W b) along the backward dimension than matches along the forward dimension. [sent-149, score-0.689]
</p><p>49 The Bb bar is higher than the others, suggesting that the backward dimension does indeed make a contribution that goes beyond the forward dimension. [sent-150, score-0.626]
</p><p>50 The fourth bar (Cf , for champion along the forward dimension) includes all cases where a word is ranked best along the forward dimension, which means that no match can be found. [sent-152, score-0.608]
</p><p>51 The Bf bar for the speaker data is also high, suggesting that the forward dimension makes a contribution that goes beyond the backward dimension. [sent-156, score-0.933]
</p><p>52 The results for the hearer data HN provide additional support for the idea that neither dimension predicts hearer guesses better than the other. [sent-159, score-1.259]
</p><p>53 iii suggests that the forward dimension is not predictive once the backward dimension is taken into account (Bf is smaller than W f ). [sent-161, score-0.619]
</p><p>54 This result is consistent with our previous ﬁnding that forward and backward strengths are highly correlated in the case of the hearer, and that neither dimension makes a contribution after controlling for the other. [sent-162, score-0.686]
</p><p>55 Our analyses so far suggest that forward and backward strengths both make independent contributions to the choices made by speakers, but that the hearer data do not allow us to discriminate between these dimensions. [sent-163, score-1.305]
</p><p>56 The most notable change is that backward strengths appear to play a much smaller role when speakers are placed under time pressure. [sent-165, score-0.641]
</p><p>57 i suggests that backward strengths are now worse than forward strengths at predicting the clues chosen by speakers. [sent-167, score-0.86]
</p><p>58 This result provides further evidence that speakers tend to rely more heavily on forward associations than backward associations when placed under time pressure. [sent-172, score-0.92]
</p><p>59 In each case we assume that the speaker and hearer sample words from distributions pS (c|w) and pH (w|c) based on the expressions shown. [sent-180, score-0.926]
</p><p>60 At level 0, both speaker and hearer rely entirely on forward associates, and at level 1, both parties rely entirely on backward associates. [sent-181, score-1.5]
</p><p>61 Our previous analyses found little evidence that forward and backward strengths make separate contributions in the case of the hearer, but the lightning data HL suggest that these dimensions may indeed make separate contributions. [sent-183, score-0.979]
</p><p>62 01), suggesting that the hearer (like the speaker) tends to rely on forward strengths rather than backward strengths in the lightning rounds. [sent-187, score-1.609]
</p><p>63 Taken together, the full set of results in Figure 2 suggests that the responses of speakers and hearers are both shaped by backward associates—in other words, that both parties are considerate of the other person’s situation. [sent-188, score-0.854]
</p><p>64 The evidence in the case of the speaker is relatively strong and all of the analyses we considered suggest that backward associations play a role. [sent-189, score-0.791]
</p><p>65 The evidence is weaker in the case of the hearer, and only the comparison between normal and lightning rounds suggests that backward associations play some role. [sent-190, score-0.793]
</p><p>66 4 Efﬁcient communication: calibration and collaboration Our analyses so far provide some initial evidence that speakers and hearers are both inﬂuenced by forward and backward associations. [sent-191, score-1.002]
</p><p>67 Given this result, we now consider a model that explores how forward and backward associations are combined in generating a response. [sent-192, score-0.59]
</p><p>68 The corresponding hearer model assumes that guess w given clue c is sampled from the mixture distribution pH (w|c) = αH (c → w) + βH (c ← w). [sent-195, score-0.942]
</p><p>69 (3)  Several possible mixture distributions for speaker and hearer are shown in Table 1. [sent-196, score-0.938]
</p><p>70 For example, the level 0 distributions assume that speaker and hearer both rely entirely on forward associates, and the level 1 distributions assume that both rely entirely on backward associates. [sent-197, score-1.442]
</p><p>71 By ﬁtting mixture weights to the game show data we can explore the extent to which speaker and hearer rely on forward and backward associations. [sent-198, score-1.618]
</p><p>72 The mixture models in Equations 2 and 3 can be derived by assuming that the hearer relies on Bayesian inference. [sent-199, score-0.631]
</p><p>73 Using Bayes’ rule, the hearer distribution pH (w|c) can be expressed as pH (w|c) ∝ pS (c|w)p(w). [sent-200, score-0.582]
</p><p>74 In other words, we assume that the hearer samples a guess w from the distribution pH (w|c) in Equation 4, and that the speaker samples a clue from a distribution pS (c|w) ∝ pH (w|c). [sent-204, score-1.2]
</p><p>75 For example, if the speaker uses strategy S0 and samples a clue c from the distribution pS (c|w) = w → c, then Equation 4 suggests that the hearer should sample a guess w from the distribution pH (c|w) ∝ (w → c) = (c ← w). [sent-210, score-1.249]
</p><p>76 Similarly, if the speaker uses the strategy S1 and samples a clue c from the distribution pS (c|w) = (w ← c), then Equation 4 suggests that the hearer should sample a guess w from the distribution pH (c|w) ∝ (w ← c) = (c → w). [sent-211, score-1.249]
</p><p>77 Suppose now that the hearer is uncertain about the strategy used by the speaker. [sent-212, score-0.613]
</p><p>78 A level 2 hearer (2) assumes that the speaker could use strategy S0 or strategy S1 and assigns prior probabilities of βH (2) and αH to these speaker strategies. [sent-213, score-1.258]
</p><p>79 Since H1 is the appropriate response to S0 and H0 is the appropriate response to S1 , the level 2 hearer should sample from the distribution pH (w|c)  =  p(S1 )pH (w|c, S1 ) + p(S0 )pH (w|c, S0 )  =  αH (c → w) + βH (c ← w). [sent-214, score-0.614]
</p><p>80 (2)  (2)  (5)  More generally, suppose that a level n hearer assumes that the speaker uses a strategy from the set {S0 , S1 , . [sent-215, score-0.92]
</p><p>81 Some pairs of mixture models are calibrated in the sense that the hearer model is the best choice given the speaker model and vice versa. [sent-222, score-0.983]
</p><p>82 Equation 4 implies that calibration is achieved when the forward weight for the speaker matches the backward weight for the hearer (αS = βH ) and the backward weight for the speaker matches the forward weight for the hearer (βS = αH ). [sent-223, score-2.897]
</p><p>83 For example, calibration is achieved if the speaker uses strategy S0 and the hearer uses strategy H1 . [sent-226, score-0.988]
</p><p>84 If generating backward associates is more difﬁcult than thinking about forward associates, this solution seems unbalanced since the hearer alone is required to think about backward associates. [sent-227, score-1.511]
</p><p>85 Consistent with the principle of least collaborative effort, we make a second prediction that speaker and hearer will collaborate and share the communicative burden equally. [sent-228, score-0.953]
</p><p>86 2 Fitting forward and backward mixture weights to the data To evaluate our predictions we assumed that the speaker and hearer are characterized by Equations 2 and 3 and identiﬁed the mixture weights that best ﬁt the game show data. [sent-233, score-1.652]
</p><p>87 Assuming that the M game rounds are independent, the log likelihood for the speaker data is M  M  P (cm |wm ) =  L = log m=1  [αS log(wm → cm ) + βS log(wm ← cm )]  (6)  m=1  and a similar expression is used for the hearer data. [sent-234, score-1.095]
</p><p>88 2  −1 −2  0  S  N  H  N  S  L  H  L  S  H  N  N  S  L  10 normal lightning 5  0  S  H  H  L  Figure 3: (a) Fitted mixture weights for the speaker (S) and hearer (H) models based on bootstrapped normal (N) and lightning (L) rounds. [sent-241, score-1.508]
</p><p>89 (c) Average response times for speakers choosing clues and hearers choosing guesses in normal and lightning rounds. [sent-244, score-0.769]
</p><p>90 We ran separate analyses for normal and lightning rounds, and ran similar analyses for the hearer data. [sent-246, score-0.955]
</p><p>91 1000 estimates of each mixture weight were computed by bootstrapping game show rounds while keeping tallies of normal and lightning rounds constant. [sent-247, score-0.624]
</p><p>92 Both speaker and hearer appear to weight forward associates slightly more heavily than backward associates, but 0. [sent-250, score-1.503]
</p><p>93 The lightning rounds produce a different pattern of results and suggest that the speaker now relies much more heavily on forward than backward associates. [sent-252, score-1.172]
</p><p>94 Further conﬁdence tests show that the percentage of bootstrapped ratios exceeding 0 is 100% for the speaker in the lightning rounds, but 85% or lower in the three remaining cases. [sent-256, score-0.549]
</p><p>95 Consistent with our previous analyses, this result suggests that coordinating with the hearer requires some effort on the part of the speaker, and that this coordination is likely to break down under time pressure. [sent-257, score-0.638]
</p><p>96 The ﬁtted mixture weights, however, do not conﬁrm the prediction that time pressure makes it difﬁcult for the hearer to consider backward associations. [sent-258, score-0.946]
</p><p>97 Figure 3c helps to explain why mixture weights for the speaker but not the hearer may differ across normal and lightning rounds. [sent-259, score-1.222]
</p><p>98 The difference in response times between normal and lightning rounds is substantially greater for the speaker than the hearer, suggesting that any differences between normal and lightning rounds are more likely to emerge for the speaker than the hearer. [sent-260, score-1.392]
</p><p>99 For example, one possibility is that speakers sample a small set of words with high forward strengths, then choose the word in this sample with greatest backward strength. [sent-267, score-0.822]
</p><p>100 Different processing models might be considered, but we believe that any successful model of speaker or hearer will need to include some role for inferences about the other person. [sent-268, score-0.916]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hearer', 0.582), ('backward', 0.315), ('speaker', 0.307), ('clue', 0.216), ('lightning', 0.215), ('forward', 0.204), ('password', 0.197), ('hearers', 0.188), ('speakers', 0.181), ('sf', 0.134), ('strengths', 0.126), ('rounds', 0.11), ('game', 0.096), ('guess', 0.095), ('associates', 0.095), ('word', 0.085), ('communication', 0.084), ('sb', 0.081), ('hf', 0.081), ('bb', 0.08), ('bf', 0.079), ('associations', 0.071), ('clues', 0.071), ('ph', 0.066), ('considerate', 0.063), ('parties', 0.058), ('analyses', 0.057), ('ranks', 0.055), ('guesses', 0.054), ('hb', 0.054), ('hn', 0.053), ('mixture', 0.049), ('cb', 0.047), ('cf', 0.045), ('calibrated', 0.045), ('shovel', 0.045), ('sn', 0.044), ('normal', 0.044), ('dimension', 0.041), ('snow', 0.039), ('words', 0.037), ('calibration', 0.037), ('eb', 0.036), ('hl', 0.036), ('ebwb', 0.036), ('along', 0.035), ('ps', 0.034), ('wi', 0.032), ('wj', 0.032), ('responses', 0.031), ('wf', 0.031), ('lexicon', 0.031), ('strategy', 0.031), ('sl', 0.031), ('strength', 0.031), ('rank', 0.029), ('bootstrapped', 0.027), ('contestants', 0.027), ('mend', 0.027), ('pets', 0.027), ('pwd', 0.027), ('sew', 0.027), ('stitch', 0.027), ('television', 0.027), ('usf', 0.027), ('inferences', 0.027), ('bar', 0.025), ('collaborative', 0.025), ('weights', 0.025), ('suggesting', 0.024), ('joan', 0.024), ('strategies', 0.023), ('burden', 0.023), ('explore', 0.023), ('matched', 0.022), ('human', 0.022), ('tend', 0.022), ('matches', 0.022), ('association', 0.022), ('dimensions', 0.021), ('suggest', 0.021), ('evidence', 0.02), ('break', 0.02), ('person', 0.02), ('ranked', 0.02), ('florida', 0.019), ('placed', 0.019), ('effort', 0.018), ('passwords', 0.018), ('pragmatics', 0.018), ('counts', 0.018), ('suggests', 0.018), ('rely', 0.017), ('goes', 0.017), ('oxford', 0.017), ('wm', 0.016), ('participants', 0.016), ('response', 0.016), ('communicative', 0.016), ('dogs', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="125-tfidf-1" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>2 0.1259269 <a title="125-tfidf-2" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>Author: Javier R. Movellan, Paul L. Ruvolo</p><p>Abstract: Determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction. One popular approach to this problem is audio-visual synchrony detection [10, 21, 12]. A candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal. Here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all. Further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models. The voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g. in the case of a mobile robot). Moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection. The work here provides dramatic evidence about the efﬁcacy of two very different approaches to multimodal speech detection on a challenging database. 1</p><p>3 0.069095522 <a title="125-tfidf-3" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>4 0.051693618 <a title="125-tfidf-4" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>5 0.048431139 <a title="125-tfidf-5" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>6 0.04280062 <a title="125-tfidf-6" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>7 0.041292705 <a title="125-tfidf-7" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>8 0.040942971 <a title="125-tfidf-8" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>9 0.039026406 <a title="125-tfidf-9" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>10 0.038860604 <a title="125-tfidf-10" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>11 0.038074318 <a title="125-tfidf-11" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>12 0.036386296 <a title="125-tfidf-12" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>13 0.036339127 <a title="125-tfidf-13" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>14 0.035485703 <a title="125-tfidf-14" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>15 0.035169311 <a title="125-tfidf-15" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>16 0.033016644 <a title="125-tfidf-16" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>17 0.031416193 <a title="125-tfidf-17" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>18 0.02993774 <a title="125-tfidf-18" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>19 0.028997121 <a title="125-tfidf-19" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>20 0.028669663 <a title="125-tfidf-20" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.077), (1, 0.01), (2, -0.012), (3, 0.012), (4, -0.036), (5, 0.029), (6, -0.006), (7, 0.01), (8, -0.022), (9, 0.023), (10, 0.015), (11, 0.006), (12, -0.012), (13, -0.024), (14, 0.025), (15, 0.036), (16, 0.043), (17, 0.033), (18, -0.052), (19, -0.029), (20, -0.037), (21, 0.046), (22, 0.076), (23, -0.016), (24, 0.093), (25, -0.057), (26, 0.057), (27, -0.032), (28, -0.006), (29, 0.051), (30, 0.058), (31, 0.022), (32, 0.039), (33, -0.049), (34, 0.009), (35, -0.08), (36, 0.07), (37, -0.009), (38, -0.006), (39, -0.044), (40, -0.002), (41, -0.095), (42, -0.009), (43, -0.018), (44, 0.017), (45, -0.064), (46, -0.002), (47, 0.016), (48, 0.054), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94609636 <a title="125-lsi-1" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>2 0.60575211 <a title="125-lsi-2" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>3 0.57572013 <a title="125-lsi-3" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>4 0.5678727 <a title="125-lsi-4" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>Author: Javier R. Movellan, Paul L. Ruvolo</p><p>Abstract: Determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction. One popular approach to this problem is audio-visual synchrony detection [10, 21, 12]. A candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal. Here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all. Further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models. The voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g. in the case of a mobile robot). Moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection. The work here provides dramatic evidence about the efﬁcacy of two very different approaches to multimodal speech detection on a challenging database. 1</p><p>5 0.55420941 <a title="125-lsi-5" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>Author: Yariv Maron, Elie Bienenstock, Michael James</p><p>Abstract: Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a hig hdimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks. 1 In trod u cti on The embedding of objects in a low-dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10, 13]. Most methods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3], embed objects of heterogeneous types, based on their co-occurrence statistics. In this paper we demonstrate that the latter can be successfully applied to unsupervised part-of-speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1, 4, 5, 6, 7]. The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features). The induction task is fully unsupervised, i.e., it uses no annotations. This task has been addressed in the past using a variety of methods. Some approaches, such as [1], combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [7, 2, 5]. Using a latent-descriptor clustering approach, [15] obtain the best results to date for distributional-only unsupervised POS tagging of the widely-used WSJ corpus. Using a heterogeneous-data embedding approach for this task, we define separate embedding functions for the objects</p><p>6 0.5449543 <a title="125-lsi-6" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>7 0.48638499 <a title="125-lsi-7" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>8 0.48368052 <a title="125-lsi-8" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>9 0.46188542 <a title="125-lsi-9" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>10 0.35567632 <a title="125-lsi-10" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>11 0.35102201 <a title="125-lsi-11" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>12 0.34089503 <a title="125-lsi-12" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>13 0.33969399 <a title="125-lsi-13" href="./nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">216 nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>14 0.33208647 <a title="125-lsi-14" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>15 0.31965062 <a title="125-lsi-15" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>16 0.31518179 <a title="125-lsi-16" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>17 0.31486604 <a title="125-lsi-17" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>18 0.31315857 <a title="125-lsi-18" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>19 0.30664495 <a title="125-lsi-19" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>20 0.30364418 <a title="125-lsi-20" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.404), (13, 0.034), (17, 0.011), (25, 0.017), (27, 0.082), (30, 0.045), (35, 0.011), (45, 0.115), (50, 0.044), (52, 0.015), (60, 0.015), (77, 0.066), (78, 0.015), (90, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76955533 <a title="125-lda-1" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>2 0.51634848 <a title="125-lda-2" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>3 0.37857634 <a title="125-lda-3" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>4 0.378218 <a title="125-lda-4" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>5 0.37730968 <a title="125-lda-5" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>6 0.37724119 <a title="125-lda-6" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>7 0.3731344 <a title="125-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.37253752 <a title="125-lda-8" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>9 0.37146708 <a title="125-lda-9" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>10 0.37038857 <a title="125-lda-10" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>11 0.36990288 <a title="125-lda-11" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>12 0.36902481 <a title="125-lda-12" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>13 0.36759895 <a title="125-lda-13" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>14 0.36741921 <a title="125-lda-14" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>15 0.36676922 <a title="125-lda-15" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>16 0.36458218 <a title="125-lda-16" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>17 0.36430928 <a title="125-lda-17" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>18 0.36430246 <a title="125-lda-18" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>19 0.36393428 <a title="125-lda-19" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>20 0.36334297 <a title="125-lda-20" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
