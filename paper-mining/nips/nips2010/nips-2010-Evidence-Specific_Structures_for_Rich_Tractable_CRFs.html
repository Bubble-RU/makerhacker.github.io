<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-83" href="#">nips2010-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</h1>
<br/><p>Source: <a title="nips-2010-83-pdf" href="http://papers.nips.cc/paper/4002-evidence-specific-structures-for-rich-tractable-crfs.pdf">pdf</a></p><p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>Reference: <a title="nips-2010-83-reference" href="../nips2010_reference/nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. [sent-5, score-0.302]
</p><p>2 Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. [sent-6, score-0.238]
</p><p>3 At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. [sent-7, score-0.18]
</p><p>4 On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. [sent-8, score-0.436]
</p><p>5 A key advantage of CRFs over other probabilistic graphical models (PGMs, [3]) stems from the observation that in almost all applications, some variables are unknown at test time (we will denote such variables X ), but others, called the evidence E, are known at test time. [sent-10, score-0.151]
</p><p>6 The discriminative approach adopted by CRFs allows for better approximation quality of the learned conditional distribution P (X | E), because the representational power of the model is not “wasted” on modeling P (E). [sent-12, score-0.218]
</p><p>7 To overcome the extra computational challenges posed by the conditional random ﬁelds, practitioners usually resort to several of the following approximations throughout the process: • • • •  CRF structure is speciﬁed by hand, leading to suboptimal structures. [sent-17, score-0.234]
</p><p>8 Approximate inference at test time results in suboptimal results [5]. [sent-19, score-0.15]
</p><p>9 Replacing the CRF conditional likelihood objective with a more tractable one (e. [sent-20, score-0.205]
</p><p>10 [6]) results in suboptimal models (both in terms of learned structure and parameters). [sent-22, score-0.166]
</p><p>11 For 1  such models, parameter learning and inference can be done exactly1 ; only structure learning involves approximations. [sent-25, score-0.166]
</p><p>12 Generalize the Chow-Liu algorithm [7] to learn evidence-speciﬁc structures for tree CRFs. [sent-31, score-0.165]
</p><p>13 Generalize tree CRFs with evidence-speciﬁc structure (ESS-CRFs) to the relational setting. [sent-32, score-0.494]
</p><p>14 Demonstrate empirically the superior performance of ESS-CRFs over densely connected models in terms of both accuracy and runtime on real-life relational models. [sent-33, score-0.406]
</p><p>15 Given the features f and training data D that consists of fully observed assignments to X and E, the optimal feature weights w∗ maximize the conditional log-likelihood (CLLH) of the data:   w∗= arg max  logP (X | E,w) = arg max  (X,E)∈D     wijk fijk (Xi ,Xj ,E) − logZ(E,w)) . [sent-38, score-0.696]
</p><p>16 Moreover, ∂ log P (X | E, w) = fijk (Xi ,Xj ,E) − EP (Xi ,Xj |E,w) [fijk (Xi ,Xj ,E)] , ∂wijk where EP denotes expectation with respect to a distribution P. [sent-40, score-0.252]
</p><p>17 However, the gradient (3) contains the conditional distribution over Xi Xj , so computing (3) requires inference in the model for every datapoint. [sent-42, score-0.204]
</p><p>18 Time complexity of the exact inference is exponential in the treewidth of the graph deﬁned by edges T [5]. [sent-43, score-0.312]
</p><p>19 Therefore, exact evaluation of the CLLH objective (2)and gradient (3) and exact inference at test time are all only feasible for models with low-treewidth T. [sent-44, score-0.272]
</p><p>20 Unfortunately, restricting the space of models to only those with low treewidth severely decreases the expressive power of CRFs. [sent-45, score-0.292]
</p><p>21 Complex dependencies of real-life distributions usually cannot be adequately captured by a single tree-like structure, so most of the models used in practice have high treewidth, making exact inference infeasible. [sent-46, score-0.199]
</p><p>22 Approximate inference is NP-hard [5], so approximate inference algorithms have very few result quality guarantees. [sent-52, score-0.223]
</p><p>23 Greater expressive power of the models is thus obtained at the expense of worse quality of estimated parameters and inference. [sent-53, score-0.185]
</p><p>24 Here, we show an alternative way to increase expressive power of tree-like structured CRFs without sacriﬁcing optimal weights learning and exact inference at test time. [sent-54, score-0.341]
</p><p>25 In practice, our approach is much better suited for relational than for propositional settings, because of much higher parameters dimensionality in the propositional case. [sent-55, score-0.738]
</p><p>26 However, we ﬁrst present in detail the propositional case theory to better convey the key high-level ideas. [sent-56, score-0.203]
</p><p>27 3  Evidence-speciﬁc structure for CRFs  Observe that, given a particular evidence value E, the set of edges T in the CRF formulation (1) actually can be viewed as a supergraph of the conditional model over X . [sent-57, score-0.279]
</p><p>28 If T (E) has low treewidth for all values of E, inference and parameter learning using the effective structure are tractable, even if a priori structure T has high treewidth. [sent-62, score-0.44]
</p><p>29 Unfortunately, in practice the treewidth of T (E) is usually not much smaller than the treewidth of T. [sent-63, score-0.298]
</p><p>30 Low-treewidth effective structures are rarely used, because treewidth is a global property of the graph (even computing treewidth is NP-complete [13]), while feature design is a local process. [sent-64, score-0.458]
</p><p>31 Achieving low treewidth for the effective structures requires elaborate feature design, making model construction very difﬁcult. [sent-66, score-0.332]
</p><p>32 Instead, in this work, we separate construction of low-treewidth effective structures from feature design and weight learning, to combine the advantages of exact inference and discriminative weights learning, high expressive power of high-treewidth models, and local feature design. [sent-67, score-0.56]
</p><p>33 Observe that the CRF deﬁnition (1) can be written equivalently as P (X | E, w) = Z −1 (E, w) exp wijk × (I((i, j) ∈ T ) · fijk (Xi , Xj , E)) . [sent-68, score-0.386]
</p><p>34 In addition to the feature values f, the effective structure of the model is now controlled by the indicator functions I(·). [sent-70, score-0.157]
</p><p>35 These indicator functions provide us with a way to control the treewidth of the effective structures independently of the features. [sent-71, score-0.277]
</p><p>36 The resulting model, which we call a CRF with evidence-speciﬁc structure (ESS-CRF), deﬁnes a conditional distribution P (X | E, w, u) as follows P (X | E,w,u) = Z −1 (E,w,u) exp  ij  k  wijk (I((i, j) ∈ T (E, u)) · fijk (Xi , Xj , E)) . [sent-75, score-0.617]
</p><p>37 ESS-CRFs have an important advantage over the traditional parametrization: in (5) the parameters u that determine the model structure are decoupled from the feature weights w. [sent-78, score-0.22]
</p><p>38 , optimizing u) can be decoupled from feature selection (choosing f ) and feature weights learning (optimizing w). [sent-81, score-0.164]
</p><p>39 Such a decoupling makes it much easier to guarantee that the effective structure of the model has low treewidth by relegating all the necessary global computation to the structure construction algorithm T = T (E, u). [sent-82, score-0.385]
</p><p>40 2 Optimize weights w to maximize conditional LLH (2) of the training data. [sent-84, score-0.202]
</p><p>41 3 foreach E in test data do 4 Use conditional model (1) to deﬁne the conditional distribution P (X | E, w). [sent-86, score-0.298]
</p><p>42 Use approximate inference to compute the marginals or the most likely assignment to X . [sent-87, score-0.18]
</p><p>43 Algorithm 2: CRF with evidence-speciﬁc structures approach 1 Deﬁne features fijk (Xi , Xj , E). [sent-88, score-0.375]
</p><p>44 2 Optimize weights w to maximize conditional LLH log P (X | E, u, w) of the training data. [sent-92, score-0.202]
</p><p>45 Use exact inference to compute CLLH objective (2) and gradient (3). [sent-93, score-0.153]
</p><p>46 3 foreach E in test data do 4 Use conditional model (5) to deﬁne the conditional distribution P (X | E, w, u). [sent-94, score-0.298]
</p><p>47 Use exact inference to compute the marginals or the most likely assignment to X . [sent-95, score-0.208]
</p><p>48 Also, ∂ logP(X | E,w,u) = I((i, j) ∈ T (E, u)) fijk (Xi ,Xj ,E)−EP (Xi ,Xj |E,w,u) [fijk (Xi ,Xj ,E)] . [sent-97, score-0.252]
</p><p>49 4  Conditional Chow-Liu algorithm for tractable evidence-speciﬁc structures  Learning the most likely PGM structure from data is in most cases intractable. [sent-104, score-0.274]
</p><p>50 Unlike tree CRFs, however, likelihood of tree MRF structures decomposes into contributions of individual edges: LLH(T ) =  (i,j)∈T  I(Xi , Xj ) −  Xi ∈X  H(Xi ),  (7)  where I(·, ·) is the mutual information and H(·) is entropy. [sent-111, score-0.267]
</p><p>51 Therefore, as shown in [7], the most likely structure can be obtained by taking the maximum spanning tree of a fully connected graph, where the weight of an edge ij is I(Xi , Xj ). [sent-112, score-0.254]
</p><p>52 Given the concrete value E of evidence E, one can write down the conditional version of the tree structure likelihood (7) for that particular value of evidence: LLH(T | E) =  (i,j)∈T  IP (·|E) (Xi , Xj ) −  Xi ∈X  HP (·|E) (Xi ). [sent-114, score-0.349]
</p><p>53 (8)  If exact conditional distributions P (Xi , Xj | E) were available, then the same Chow-Liu algorithm would ﬁnd the optimal conditional structure. [sent-115, score-0.261]
</p><p>54 However, we can still plug in approximate conditionals P (· | E) learned from 4  Algorithm 3: Conditional Chow-Liu algorithm for learning evidence-speciﬁc tree structures // Parameter learning stage. [sent-117, score-0.19]
</p><p>55 with L-BFGS using the gradient (12) data using any standard density estimation technique3 In particular, with the same features fijk that are used in the CRF model, one can train a logistic regression model for P (· | E) : −1 P (Xi , Xj | E, uij ) = Zij (E, uij ) exp  k  uijk fijk (Xi , Xj , E) . [sent-123, score-0.66]
</p><p>56 [8, 15] for higher treewidth junction trees, can be used as components in the same way as Chow-Liu algorithm is used in Alg. [sent-134, score-0.192]
</p><p>57 5  Relational CRFs with evidence-speciﬁc sructure  Traditional (also called propositional) PGMs are not well suited for dealing with relational data, where every variable is an entity of some type, and entities are related to each other via different types of links. [sent-136, score-0.407]
</p><p>58 data assumption of traditional PGMs, and huge dimensionalities of relational datasets preclude learning meaningful propositional models. [sent-142, score-0.535]
</p><p>59 Instead, several formulations of relational PGMs have been proposed [16] to work with relational data, including relational CRFs. [sent-143, score-1.019]
</p><p>60 More concretely, in relational CRFs every variable Xi is assigned a type mi out of the set M of possible types. [sent-145, score-0.332]
</p><p>61 A binary relation R ∈ R, corresponding to a speciﬁc type of link between two R variables, speciﬁes the types of its input arguments, and a set of features fk (·, ·, E) and feature R weights wk . [sent-146, score-0.249]
</p><p>62 By accounting for parameter sharing, it is straightforward to adapt our ESS-CRF formulation to the relational setting. [sent-149, score-0.332]
</p><p>63 We deﬁne the relational ESS-CRF conditional distribution as P(X | E,R,w,u) ∝ exp 3  R∈R  I((i, j) ∈ T (E,u))  Xi ,Xj ∈inst(R,X )  R R wk fk (Xi , Xj , E) k  (11)  Notice that the approximation error from P (·) is the only source of approximations in all our approach. [sent-150, score-0.515]
</p><p>64 Given the structure learning algorithm T (·, ·) that is guaranteed to return low-treewidth structures, one can learn optimal feature weights w∗ and perform inference at test time exactly: Observation 4 Relational ESS-CRF log-likelihood is concave with respect to w. [sent-160, score-0.328]
</p><p>65 3) can be also extended to the relational setting by using templated logistic regression weights for estimating edge conditionals. [sent-163, score-0.436]
</p><p>66 In the relational setting, one only needs to learn O(|R|) parameters, regardless of the dataset size, for both structure selection and feature weights, as opposed to O(|X |2 ) parameters for the propositional case. [sent-169, score-0.655]
</p><p>67 Thus, relational ESS-CRFs are typically much less prone to overﬁtting than propositional ones. [sent-170, score-0.535]
</p><p>68 6  Experiments  We have tested the ESS-CRF approach on both propositional and relational data. [sent-171, score-0.535]
</p><p>69 With the large number of parameters needed for the propositional case (O(|X |2 )), our approach is only practical for cases of abundant data. [sent-172, score-0.203]
</p><p>70 So our experiments with propositional data serve only to prove the concept, verifying that ESS-CRF can successfully learn a model better than a single tree baseline. [sent-173, score-0.277]
</p><p>71 In contrast to the propositional settings, in the relational cases the relatively low parameter space dimensionality (O(|R|2 )) almost eliminates the overﬁtting problem. [sent-174, score-0.535]
</p><p>72 As a result, on relational datasets ESS-CRF is a very attractive approach in practice. [sent-175, score-0.332]
</p><p>73 Our experiments show ESS-CRFs comfortably outperforming state of the art high-treewidth discriminative models on several real-life relational datasets. [sent-176, score-0.429]
</p><p>74 1  Propositional models  We compare ESS-CRFs with ﬁxed tree CRFs, where the tree structure learned by the Chow-Liu algorithm using P (X ). [sent-178, score-0.272]
</p><p>75 The ﬁrst model, called FACES, aims to improve face recognition in collections of related images using information about similarity between different faces in addition to the standard single-face features. [sent-192, score-0.188]
</p><p>76 Pairwise features f (Xi , Xj , E), based on blob color similarity, indicate how close two faces are in appearance. [sent-195, score-0.169]
</p><p>77 The model is used in a semi-supervised way: at test time, a PGM is instantiated jointly over the train and test entities, values of the train entities are ﬁxed to the ground truth, and inference ﬁnds the (approximately) most likely labels for the test entities. [sent-197, score-0.303]
</p><p>78 We compare ESS-CRFs with a dense relational PGM encoded by a Markov logic network (MLN, [20]) using the same features. [sent-221, score-0.394]
</p><p>79 For the MLN, we had to threshold the pairwise features indicating the likelihood of label agreement and set those under the threshold to 0 to prevent (a) oversmoothing and (b) very long inference times. [sent-223, score-0.221]
</p><p>80 Also, to prevent oversmoothing by the MLN, we have found it useful to scale down the pairwise feature weights learned during training, thus weakening the smoothing effect of any single edge in the model4 . [sent-224, score-0.219]
</p><p>81 We compare ESS-CRFs to high-treewidth relational Markov networks (RMNs, [23]), max-margin Markov networks (M3Ns, [24]) and a standalone SVM classiﬁer. [sent-235, score-0.366]
</p><p>82 All the relational PGMs use the same single-variable features encoding the webpage text, and pairwise features encoding the link structure. [sent-236, score-0.478]
</p><p>83 RMNs and ESS-CRFs are trained to maximize the conditional likelihood of the labels, while M3Ns maximize the margin in likelihood between the correct assignment and all of the incorrect ones, explicitly targeting the classiﬁcation. [sent-238, score-0.235]
</p><p>84 Observe that ESS-CRF matches the accuracy of high-treewidth RMNs, again showing that the smaller expressive power of tree models can be fully compensated by exact parameter learning and inference. [sent-241, score-0.308]
</p><p>85 Still, the RMN results indicate that it may be possible to match the M3N accuracy with much faster tractable ESS models by replacing the CRF conditional likelihood objective with the max-margin objective, which is an important direction of future work. [sent-251, score-0.279]
</p><p>86 Two cornerstones of our ESS-CRF approach, namely using models that become more sparse when evidence is instantiated, and using multiple tractable models to avoid restrictions on the expressive power inherent to low-treewidth models, have been discussed in the existing literature. [sent-254, score-0.307]
</p><p>87 However, so far CSI has been treated as a local property of the model, which made reasoning about the resulting treewidth of evidencespeciﬁc models impossible. [sent-256, score-0.185]
</p><p>88 Thus, the full potential of exact inference for models with CSI remained unused. [sent-257, score-0.167]
</p><p>89 Unlike the mixture models, our approach of selecting a single structure for any given evidence value has the advantage of allowing for efﬁcient exact decoding of the most probable assignment to the unknowns X using the Viterbi algorithm [29]. [sent-262, score-0.248]
</p><p>90 Both for the mixture models and our approach, joint optimization of the structure and weights (u and w in our notation) is infeasible due to many local optima of the objective. [sent-263, score-0.197]
</p><p>91 Learning the CRF structure in general is NP-hard, which follows from the hardness results for the generative models (c. [sent-266, score-0.179]
</p><p>92 Moreover, CRF structure learning is further complicated by the fact the CRF structure likelihood does not decompose into scores of local graph components, as do scores for some generative models [3]. [sent-269, score-0.333]
</p><p>93 In practice, the hardness of CRF structure learning leads to high popularity of heuristics: chain and skip-chain [32] structures are often used, as well as grid-like structures. [sent-271, score-0.207]
</p><p>94 Finally, one can try to approximate the CRF structure score as a combination of local scores [15, 4] and use an algorithm for learning generative structures (where the score actually decomposes). [sent-277, score-0.264]
</p><p>95 Learning the weights is straightforward for tractable CRFs, because the log-likelihood is concave [1] and the gradient (3) can be used with mature convex optimization techniques. [sent-280, score-0.195]
</p><p>96 For dense structures, computing the gradient (3) exactly is intractable as even approximate inference in general models is NP-hard [5]. [sent-282, score-0.2]
</p><p>97 As a result, approximate inference techniques, such as belief propagation [10, 11] or Gibbs sampling [12] are employed, without guarantees on the quality of the result. [sent-283, score-0.168]
</p><p>98 Our experiments showed that exact weight learning for tractable models gives an advantage in approximation quality and efﬁciency over dense structures. [sent-287, score-0.243]
</p><p>99 2), has just one source of approximation, namely conditional structure scores. [sent-292, score-0.192]
</p><p>100 We have demonstrated on real-life relational datasets that our approach matches or exceeds the accuracy of state of the art dense discriminative models, and at the same time provide more than a factor of magnitude speedup. [sent-293, score-0.47]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crf', 0.465), ('mln', 0.392), ('relational', 0.332), ('crfs', 0.265), ('fijk', 0.252), ('propositional', 0.203), ('treewidth', 0.149), ('ess', 0.138), ('faces', 0.137), ('wijk', 0.134), ('llh', 0.117), ('xj', 0.109), ('conditional', 0.104), ('cllh', 0.101), ('structures', 0.091), ('structure', 0.088), ('pgm', 0.084), ('pgms', 0.084), ('xi', 0.082), ('inference', 0.078), ('tree', 0.074), ('tractable', 0.073), ('weights', 0.073), ('expressive', 0.069), ('foreach', 0.06), ('rmn', 0.059), ('inst', 0.059), ('logp', 0.059), ('evidence', 0.055), ('entities', 0.053), ('exact', 0.053), ('csi', 0.05), ('webkb', 0.05), ('pairwise', 0.049), ('fk', 0.046), ('chechetka', 0.044), ('junction', 0.043), ('quality', 0.042), ('suboptimal', 0.042), ('rmns', 0.041), ('dense', 0.039), ('ij', 0.039), ('chow', 0.038), ('accuracy', 0.038), ('power', 0.038), ('guestrin', 0.038), ('effective', 0.037), ('models', 0.036), ('uij', 0.036), ('webpages', 0.036), ('elds', 0.036), ('seconds', 0.035), ('discriminative', 0.034), ('alchemy', 0.034), ('oversmoothing', 0.034), ('standalone', 0.034), ('scores', 0.033), ('wk', 0.033), ('link', 0.033), ('dependencies', 0.032), ('edges', 0.032), ('ep', 0.032), ('features', 0.032), ('people', 0.032), ('feature', 0.032), ('edge', 0.031), ('marginals', 0.03), ('train', 0.03), ('test', 0.03), ('trees', 0.03), ('face', 0.029), ('hardness', 0.028), ('likelihood', 0.028), ('mrfs', 0.028), ('art', 0.027), ('dash', 0.027), ('unknowns', 0.027), ('alg', 0.027), ('decoupled', 0.027), ('traffic', 0.027), ('concave', 0.027), ('generative', 0.027), ('abnormality', 0.025), ('mixtures', 0.025), ('approximate', 0.025), ('maximize', 0.025), ('assignment', 0.025), ('uai', 0.024), ('markov', 0.023), ('logic', 0.023), ('belief', 0.023), ('construction', 0.023), ('formulations', 0.023), ('images', 0.022), ('likely', 0.022), ('max', 0.022), ('entity', 0.022), ('xr', 0.022), ('gradient', 0.022), ('richardson', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="83-tfidf-1" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>2 0.19921711 <a title="83-tfidf-2" href="./nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</a></p>
<p>Author: Ni Lao, Jun Zhu, Liu Xinwang, Yandong Liu, William W. Cohen</p><p>Abstract: Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this ﬂexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efﬁcient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efﬁciency; and on the other hand, the CVI algorithm efﬁciently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efﬁcient yet does not sacriﬁce its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is signiﬁcantly more efﬁcient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.</p><p>3 0.18124224 <a title="83-tfidf-3" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>Author: Tamir Hazan, Raquel Urtasun</p><p>Abstract: In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efﬁciently. We ﬁrst relate CRFs and structured SVMs and show that in CRFs a variant of the log-partition function, known as the soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efﬁcient messagepassing algorithm that is guaranteed to converge. Unlike existing approaches, this allows us to learn efﬁciently graphical models with cycles and very large number of parameters. 1</p><p>4 0.15877354 <a title="83-tfidf-4" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>Author: Abhay Jha, Vibhav Gogate, Alexandra Meliou, Dan Suciu</p><p>Abstract: Lifted Inference algorithms for representations that combine ﬁrst-order logic and graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efﬁciency by exploiting repeated structure in the ﬁrst-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we deﬁne a set of rules that look only at the logical representation to identify models for which exact efﬁcient inference is possible. Our rules yield new tractable classes that could not be solved efﬁciently by any of the existing techniques. 1</p><p>5 0.1548252 <a title="83-tfidf-5" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>Author: Nadia Payet, Sinisa Todorovic</p><p>Abstract: We combine random forest (RF) and conditional random ﬁeld (CRF) into a new computational framework, called random forest random ﬁeld (RF)2 . Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by MetropolisHastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a nonparametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2 . (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random ﬁeld of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</p><p>6 0.13967353 <a title="83-tfidf-6" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>7 0.13012308 <a title="83-tfidf-7" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>8 0.10538703 <a title="83-tfidf-8" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>9 0.089752994 <a title="83-tfidf-9" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>10 0.087230988 <a title="83-tfidf-10" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>11 0.086833484 <a title="83-tfidf-11" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>12 0.071984664 <a title="83-tfidf-12" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>13 0.071829945 <a title="83-tfidf-13" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>14 0.070637897 <a title="83-tfidf-14" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>15 0.064904436 <a title="83-tfidf-15" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>16 0.063138053 <a title="83-tfidf-16" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>17 0.063008651 <a title="83-tfidf-17" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>18 0.060173698 <a title="83-tfidf-18" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>19 0.055066474 <a title="83-tfidf-19" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>20 0.054637033 <a title="83-tfidf-20" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.059), (2, -0.014), (3, -0.021), (4, -0.109), (5, -0.045), (6, -0.022), (7, -0.005), (8, 0.106), (9, -0.008), (10, -0.198), (11, -0.018), (12, 0.075), (13, 0.019), (14, -0.009), (15, -0.086), (16, -0.052), (17, -0.065), (18, -0.049), (19, -0.085), (20, -0.099), (21, 0.019), (22, 0.171), (23, -0.112), (24, -0.108), (25, -0.01), (26, -0.048), (27, -0.045), (28, -0.06), (29, 0.053), (30, -0.171), (31, -0.239), (32, -0.208), (33, -0.128), (34, -0.119), (35, 0.118), (36, -0.033), (37, 0.09), (38, -0.056), (39, -0.043), (40, 0.039), (41, -0.029), (42, 0.018), (43, 0.001), (44, -0.092), (45, 0.029), (46, 0.127), (47, 0.13), (48, -0.081), (49, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91604644 <a title="83-lsi-1" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>2 0.85459733 <a title="83-lsi-2" href="./nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</a></p>
<p>Author: Ni Lao, Jun Zhu, Liu Xinwang, Yandong Liu, William W. Cohen</p><p>Abstract: Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this ﬂexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efﬁcient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efﬁciency; and on the other hand, the CVI algorithm efﬁciently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efﬁcient yet does not sacriﬁce its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is signiﬁcantly more efﬁcient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.</p><p>3 0.7241568 <a title="83-lsi-3" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>Author: Abhay Jha, Vibhav Gogate, Alexandra Meliou, Dan Suciu</p><p>Abstract: Lifted Inference algorithms for representations that combine ﬁrst-order logic and graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efﬁciency by exploiting repeated structure in the ﬁrst-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we deﬁne a set of rules that look only at the logical representation to identify models for which exact efﬁcient inference is possible. Our rules yield new tractable classes that could not be solved efﬁciently by any of the existing techniques. 1</p><p>4 0.60711688 <a title="83-lsi-4" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>5 0.55807799 <a title="83-lsi-5" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>Author: Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua B. Tenenbaum</p><p>Abstract: We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We extend the inﬁnite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p><p>6 0.55623901 <a title="83-lsi-6" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>7 0.50879419 <a title="83-lsi-7" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>8 0.50174737 <a title="83-lsi-8" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>9 0.45737201 <a title="83-lsi-9" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>10 0.43489251 <a title="83-lsi-10" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>11 0.41752163 <a title="83-lsi-11" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>12 0.41710648 <a title="83-lsi-12" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>13 0.38340148 <a title="83-lsi-13" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>14 0.33908632 <a title="83-lsi-14" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>15 0.33482152 <a title="83-lsi-15" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>16 0.32993925 <a title="83-lsi-16" href="./nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">188 nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>17 0.31809622 <a title="83-lsi-17" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>18 0.31480941 <a title="83-lsi-18" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>19 0.30945331 <a title="83-lsi-19" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>20 0.30695635 <a title="83-lsi-20" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.026), (17, 0.018), (27, 0.046), (30, 0.051), (35, 0.016), (41, 0.018), (45, 0.258), (50, 0.066), (52, 0.031), (59, 0.292), (60, 0.033), (77, 0.033), (90, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86224061 <a title="83-lda-1" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>Author: Alan Fern, Prasad Tadepalli</p><p>Abstract: We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We ﬁrst introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in ﬁnite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant’s action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution. 1</p><p>2 0.83660328 <a title="83-lda-2" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>3 0.80107892 <a title="83-lda-3" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>same-paper 4 0.78773773 <a title="83-lda-4" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>5 0.7490021 <a title="83-lda-5" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>6 0.71305227 <a title="83-lda-6" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>7 0.6901266 <a title="83-lda-7" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>8 0.68918973 <a title="83-lda-8" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>9 0.68916029 <a title="83-lda-9" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>10 0.68812174 <a title="83-lda-10" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>11 0.6880433 <a title="83-lda-11" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>12 0.68766665 <a title="83-lda-12" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>13 0.68729895 <a title="83-lda-13" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>14 0.68695402 <a title="83-lda-14" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>15 0.68689674 <a title="83-lda-15" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>16 0.68683964 <a title="83-lda-16" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>17 0.68607581 <a title="83-lda-17" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>18 0.68600988 <a title="83-lda-18" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>19 0.68595821 <a title="83-lda-19" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>20 0.68584025 <a title="83-lda-20" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
