<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-185" href="#">nips2010-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</h1>
<br/><p>Source: <a title="nips-2010-185-pdf" href="http://papers.nips.cc/paper/4098-nonparametric-density-estimation-for-stochastic-optimization-with-an-observable-state-variable.pdf">pdf</a></p><p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>Reference: <a title="nips-2010-185-reference" href="../nips2010_reference/nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. [sent-8, score-0.402]
</p><p>2 There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. [sent-9, score-0.414]
</p><p>3 We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. [sent-11, score-0.515]
</p><p>4 We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. [sent-13, score-0.281]
</p><p>5 The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. [sent-14, score-0.919]
</p><p>6 Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. [sent-15, score-0.413]
</p><p>7 1  Introduction  In stochastic optimization, a decision maker makes a decision and faces a random cost based on that decision. [sent-16, score-0.375]
</p><p>8 Stochastic optimization problems with continuous decision spaces have many viable solution methods, including function averaging and stochastic gradient descent [20]. [sent-18, score-0.314]
</p><p>9 However, in many situations conditions for the previous observations may not be the same as the current conditions; the conditions can be viewed as state variables. [sent-19, score-0.244]
</p><p>10 There are currently no general purpose solution methods for stochastic optimization problems with state variables, although they would be useful for ﬁnance, energy, dynamic pricing, inventory control and reinforcement learning applications. [sent-20, score-0.433]
</p><p>11 We consider the newsvendor problem, a classic inventory management problem, to illustrate existing solution methods for stochastic optimization problems with state variables and their limitations. [sent-21, score-0.682]
</p><p>12 Here, newspapers can be bought in advance for cost c, and up to D of them can be sold for price p, where D is a random demand; the goal is to determine how many papers should be ordered so as to maximize the expected proﬁt. [sent-22, score-0.201]
</p><p>13 A state variable that contains information about the random demand may also be included. [sent-23, score-0.281]
</p><p>14 For example, a rainy forecast may correlate to a lower demand while a sunny forecast may correlate to a higher. [sent-24, score-0.28]
</p><p>15 A natural solution method would be to partition the previous observations into “rainy” and “sunny” bins, and then solve the problem for each partition. [sent-25, score-0.177]
</p><p>16 Second, previous observations are sparse over these states; a vast number of observations must be gathered before there are enough to make a reasonable decision for a given state. [sent-31, score-0.327]
</p><p>17 We propose using nonparametric density estimation for the joint state and outcome distribution to group observations from “similar” states with weights. [sent-36, score-0.402]
</p><p>18 These can be efﬁciently solved by a number of commercial solvers, even with very large decision spaces (10 to 1,000+ variables and constraints). [sent-39, score-0.182]
</p><p>19 For example, if the demand is known in the newsvendor problem, then the value of all decisions is also known. [sent-43, score-0.453]
</p><p>20 When this happens, we propose constructing a separable, piecewise linear approximate objective function. [sent-47, score-0.187]
</p><p>21 A piecewise linear, convex function is created in each decision dimension by generating a slope function from a weighted, order-restricted regression of the gradients, and then integrating that function. [sent-48, score-0.232]
</p><p>22 Both methods depend heavily on weights to capture dependence between the state and the outcome. [sent-50, score-0.253]
</p><p>23 We propose two weighting schemes: kernels weights and Dirichlet process mixture model weights. [sent-51, score-0.317]
</p><p>24 Kernels are simple to implement, but Dirichlet process mixture models have certain appealing properties. [sent-52, score-0.153]
</p><p>25 First, they act as a local bandwidth selector across the state space; second, the weights are generated by partitions rather than products of uni-dimensional weights, so the results scale better to higher-dimensional settings. [sent-53, score-0.314]
</p><p>26 We contribute novel algorithms for stochastic optimization problems with a state variable that work with large, continuous decision spaces and propose a new use of Dirichlet process mixture models. [sent-54, score-0.644]
</p><p>27 In Section 2, we review traditional function-based and gradientbased optimization methods and in each case present novel algorithms to accommodate an observable state variable. [sent-57, score-0.37]
</p><p>28 We present an empirical analysis of our methods for synthetic newsvendor data and the hour ahead wind commitment problem in Section 4 and a discussion in Section 5. [sent-58, score-0.806]
</p><p>29 2  Stochastic optimization for problems with an observable state variable  Traditional stochastic optimization problems have the form min E [F (x, Z)] ,  x∈X  (1)  where x ∈ Rd is the decision, Z : Ω → Ψ is a random outcome, X is a decision set and F (x, Z(ω)) is a random objective function [20]. [sent-59, score-0.713]
</p><p>30 In the newsvendor problem, which we will use as a running example, x is the stocking level and Z is the random demand. [sent-60, score-0.299]
</p><p>31 When a state variable is inlcuded, we ﬁrst observe a random state S ∈ S that may inﬂuence F and the distribution of Z, then we make a decision x, and ﬁnally we observe the random variable Z. [sent-62, score-0.473]
</p><p>32 (2) x∈X  2  Traditional stochastic optimization techniques require us to sample from the conditional distribution of p(Z|S = s), treating each state observation independently [20]. [sent-65, score-0.335]
</p><p>33 We now describe new methods for function-based and gradientbased optimization for problems with an observable state variable. [sent-67, score-0.37]
</p><p>34 1  Function-based optimization with an observable state variable  Function-based optimization is used when a single outcome ω can tell us the value of all decisions given that outcome [19]. [sent-69, score-0.603]
</p><p>35 For example, in the newsvendor problem, if the demand is known then the value of all inventory levels is known. [sent-70, score-0.451]
</p><p>36 When a state variable is introduced, we wish to solve Eq. [sent-79, score-0.177]
</p><p>37 (3), we weight the observations based on the distance between the query state n−1 s and each observation Si with weight wn (s, Si ). [sent-87, score-0.487]
</p><p>38 The weights must sum to 1, i=0 wn (s, Si ) = 1, and the weights may change with the number of observations, n. [sent-88, score-0.359]
</p><p>39 (5)  i=0  The optimization problem becomes x∈X  ¯ Note that because F (x, Si , Z(ωi+1 )) is convex in x for every Si and ωi+1 , Fn (x|s) is convex and Eq. [sent-91, score-0.15]
</p><p>40 2  Gradient-based optimization with an observable state variable  In gradient-based optimization, we no longer observe an entire function F (x, S, Z(ω)), but only a derivative taken at x, ˆ β(xi , s, ωi+1 ) = x F (xi , s, Z(ωi+1 )). [sent-95, score-0.357]
</p><p>41 (6) Stochastic approximation is the most popular way to solve stochastic optimization problems using a gradient; it modiﬁes gradient search algorithms to account for random gradients [17, 9]. [sent-96, score-0.241]
</p><p>42 The general idea is to optimize x by iterating, xn+1 = ΓX (xn − an  x  F (xn , Z(ωn+1 )) ,  (7)  where ΓX is a projection back into the constraint set X , x F (xn , Z(ωn+1 )) is a stochastic gradient at xn and an is a stepsize. [sent-97, score-0.228]
</p><p>43 Other approaches to gradient-based optimization have included construction of piecewise linear, convex functions to approximate F (x) in the region where x is near the optimal decision, x∗ [15]. [sent-98, score-0.21]
</p><p>44 Including a state variable into gradient-based optimization is less straightforward than it is for function-based optimization. [sent-99, score-0.235]
</p><p>45 When we include state Sn , the decision xn is based on the state Sn . [sent-101, score-0.49]
</p><p>46 Moreover, constructing the approximate function Fn (x|s) is not trivial because the stochastic gradients depend on both xn and Sn . [sent-104, score-0.352]
</p><p>47 (Bottom right) Integrate isotonic regression to form fn (xn |Sn ). [sent-176, score-0.24]
</p><p>48 We approximate E[F (x, s, Z)] by a series of separable functions, d  ¯ Fn (x|s) =  k fn (xk |s), k=1  k where xk is the k th component of x. [sent-179, score-0.311]
</p><p>49 We enforce convexity restrictions on fn (x|s) for every s ∈ S. [sent-180, score-0.18]
</p><p>50 Given Sn , we choose xn as follows, d k fn (xk |Sn ). [sent-182, score-0.271]
</p><p>51 , d}; we want to construct a piecewise linear fn (x|s) date F d k k by constructing an increasing slope function, vn (x|Sn ) = dx fn (x|Sn ) based on the stochastic graˆ dient observations, β1:n . [sent-188, score-0.685]
</p><p>52 We use weights to group the gradients from states “similar” to Sn and a k weighted isotonic (order restricted) regression to construct vn (x|Sn ). [sent-189, score-0.301]
</p><p>53 , x[n−1] , and then solve to ﬁnd slopes for the decision-ordered space, n−1 k vn (x0:n−1 |Sn ) = arg min v  wn Sn , S[i]  ˆ β(xk , S[i] , ω[i+1] ) − v[i] [i]  2  ,  (8)  i=0  subject to : v[i−1] ≤ v[i] , i = 1, . [sent-193, score-0.269]
</p><p>54 (8) across the k th dimenk sion of the decision space, and then f k (x|Sn ) is created by integrating vn (x|Sn ). [sent-198, score-0.201]
</p><p>55 The monotonicity k k of vn (x|Sn ) ensures the convexity of fn (x|Sn ). [sent-199, score-0.262]
</p><p>56 Observe Sn and constructing weights ((wn (Sn , Si ))n−1 , i=0 2. [sent-202, score-0.152]
</p><p>57 Use the weights wn (Sn , Si )n−1 , previous decisions x0:n−1 and gradients to construct i=0 k slopes v1:K (Sn ) with Eq. [sent-203, score-0.446]
</p><p>58 3  Weight functions  Like the choice of step size in stochastic approximation, the choice of weight functions in Eqs. [sent-209, score-0.173]
</p><p>59 Weighting functions rely on density estimation procedures to approximate the conditional density f (z|s), where s is the state and z is the response. [sent-211, score-0.267]
</p><p>60 Conditional density estimation weights observations from a joint distribution to create a conditional distribution. [sent-212, score-0.261]
</p><p>61 We use this to obtain weights from two nonparametric density estimators, kernels and Dirichlet process mixture models. [sent-213, score-0.38]
</p><p>62 1  Kernel weights  Kernel weights rely on kernel functions, K(s), to be evaluated at each observation to approximate the conditional density. [sent-215, score-0.331]
</p><p>63 If K(s) is the kernel and hn is the bandwidth after n observations, deﬁne n−1  wn (s, Si ) = K ((s − Si )/hn ) /  K ((s − Sj )/hn ) . [sent-219, score-0.26]
</p><p>64 2  Dirichlet process weights  One of the curses of dimensionality is sparseness of data: as the number of dimensions grows, the distance between observations grows exponentially. [sent-222, score-0.312]
</p><p>65 In kernel regression, this means that only a handful of observations have weights that are effectively non-zero, producing non-stable estimates. [sent-223, score-0.283]
</p><p>66 We propose modeling the distribution of the state variable with a Dirichlet process mixture model, which is then decomposed into weights. [sent-225, score-0.33]
</p><p>67 We can use a Dirichlet process (DP) with base measure G0 and concentration parameter α to place a distribution over the joint distribution of (pi , θi ), the mixture proportion and location of component i [6, 1]. [sent-229, score-0.153]
</p><p>68 (9) The distribution P drawn from a Dirichlet process is an almost surely discrete measure over parameters, with the mixture proportion associated with θ as the atomic weight. [sent-234, score-0.153]
</p><p>69 A Dirichlet process mixture model can be used to model an unknown density, but it can simultaneously be used to produce a distribution of the partition structure of observed data [13, 8]. [sent-247, score-0.226]
</p><p>70 The partition structure induces weights on the observations, proportional to 1 if they are in the same cluster, 0 if not. [sent-251, score-0.186]
</p><p>71 , Cn(p) } be the partition of the observations {1, . [sent-255, score-0.177]
</p><p>72 Given p, we include the query state s into cluster Ci with probability  ps (Ci |p) = P(s ∈ Ci | p, S1:n ) ∝ |Ci |  g(s | θ∗ )dHCi (θ∗ ),  where |Ci | is the number of elements in Ci , and HCi (θ∗ ) is the posterior distribution of θ∗ conditioned on G0 and the set of observations {Sj : Sj ∈ Ci }. [sent-264, score-0.385]
</p><p>73 Given p, the weighting function is the probability that the hidden parameter for s would be θi , the hidden parameter for Si , n(p)  wn (s, Si ) | p = j=1  ps (Cj | p) 1{Si ∈Cj } . [sent-265, score-0.239]
</p><p>74 (11) is conditioned on a partition structure, but the Dirichlet process produces a distribution over partition structures. [sent-267, score-0.197]
</p><p>75 Integrating of the partition posterior, we obtain unconditional weights, (m)  M n(p X ps (Cj | p) 1 X X 1{Si ∈Cj } ≈ π(p|S1:n ) |Cj | M m=1 j=1 j=1 n(p)  wn (s, Si ) =  X p  )  ps (Cj | p(m) ) 1{Si ∈Cj } . [sent-269, score-0.316]
</p><p>76 1  Empirical analysis Multi-product constrained newsvendor problem  A multi-product newsvendor problem is a classic operations research inventory management problem. [sent-275, score-0.596]
</p><p>77 In the two product problem, a newsvendor is selling products A and B. [sent-276, score-0.249]
</p><p>78 Let (xA , xB ) be the stocking decisions for A and B respectively; it is subject to a budget constraint, bA xA + bB xB ≤ b, and a storage constraint, rA xA +rB xB ≤ r. [sent-280, score-0.15]
</p><p>79 An observable state S = (S1 , S2 ) contains information about DA and DB . [sent-281, score-0.262]
</p><p>80 The kernel and Dirichlet process weights performed approximately equally for each method, but the function-based methods converged more quickly than the gradient-based methods. [sent-300, score-0.23]
</p><p>81 2  Hour ahead wind commitment  In the hour ahead wind commitment problem, a wind farm manager must decide how much energy to promise a utility an hour in advance, incorporating knowledge about the current state of the world. [sent-302, score-1.639]
</p><p>82 The decision is the amount of wind energy pledged, a scalar variable. [sent-303, score-0.464]
</p><p>83 If more energy is pledged than is generated, the difference must be bought on the spot market, which is expensive with a price that is unknown when the decision is made; otherwise, the excess is lost. [sent-304, score-0.471]
</p><p>84 S The revenue that the wind farm receives, Yi+1 (x), depends on the variables Pi+1 and Wi+1 , which are not known until the next hour. [sent-307, score-0.341]
</p><p>85 We used wind speed data from the North American Land Data Assimilation System with hourly observations from 2002–2005 in the following locations: Amarillo, TX. [sent-308, score-0.405]
</p><p>86 Clean spot and contract price data for the time period were unavailable, so contract prices were generated by Gaussian random variables with a mean of 1 and variance of 0. [sent-324, score-0.389]
</p><p>87 Spot prices were generated by a mean-reverting (Ornstein-Uhlenbeck) process with a mean function that varies by time of day and time of year [18]. [sent-326, score-0.29]
</p><p>88 The wind is known, allowing maximum possible commitment, xi = Wi+1 (ωi+1 ). [sent-329, score-0.301]
</p><p>89 7%)  Table 1: Mean values of decisions by method, year and data set. [sent-387, score-0.204]
</p><p>90 Function-based optimization where the weights are generated by a Gaussian kernel. [sent-390, score-0.171]
</p><p>91 Both forms of function-based optimization outperformed the algorithm in which the state variable was ignored by a large margin (≥45% of the best possible value). [sent-405, score-0.235]
</p><p>92 Dirichlet process weights outperformed kernel weights by a smaller but still signiﬁcant margin (5. [sent-406, score-0.343]
</p><p>93 5  Discussion  We presented two new methods to solve stochastic optimization problems with an observable state variable, including state variables that are too large for partitioning. [sent-409, score-0.597]
</p><p>94 Our methods can accommodate much larger state and decision spaces than MDPs and other table lookup methods, particularly when combined with Dirichlet process mixture model weights. [sent-412, score-0.412]
</p><p>95 Unlike existing objective function approximation methods, such as basis functions, our methods provide convex objective function approximations that can be used with a variety of commercial solvers. [sent-413, score-0.193]
</p><p>96 [2000], ‘SHAPE-A stochastic hybrid approximation procedure for two-stage stochastic programs’, Operations Research 48(1), 73–79. [sent-433, score-0.274]
</p><p>97 [2000], ‘Markov chain sampling methods for Dirichlet process mixture models’, Journal of Computational and Graphical Statistics 9(2), 249–265. [sent-456, score-0.153]
</p><p>98 [2004], ‘Learning algorithms for separan ble approximations of discrete stochastic optimization problems’, Mathematics of Operations Research 29(4), 814–836. [sent-472, score-0.195]
</p><p>99 [1994], Markov decision processes: Discrete stochastic dynamic programming, John Wiley & Sons, Inc. [sent-475, score-0.256]
</p><p>100 [2002], ‘Conditioning of convex piecewise linear stochastic programs’, Mathematical Programming 94(1), 1–19. [sent-486, score-0.25]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sn', 0.334), ('wind', 0.301), ('newsvendor', 0.249), ('dirichlet', 0.191), ('fn', 0.18), ('si', 0.176), ('state', 0.14), ('stochastic', 0.137), ('wn', 0.133), ('pic', 0.125), ('tid', 0.125), ('tiy', 0.125), ('observable', 0.122), ('decision', 0.119), ('xb', 0.118), ('weights', 0.113), ('cj', 0.108), ('xa', 0.107), ('observations', 0.104), ('dp', 0.104), ('demand', 0.104), ('year', 0.104), ('mixture', 0.102), ('commitment', 0.101), ('decisions', 0.1), ('pis', 0.1), ('unction', 0.1), ('inventory', 0.098), ('powell', 0.094), ('hour', 0.092), ('xn', 0.091), ('spot', 0.089), ('price', 0.084), ('wi', 0.082), ('vn', 0.082), ('urn', 0.08), ('prices', 0.08), ('pledged', 0.075), ('partition', 0.073), ('ci', 0.071), ('nonparametric', 0.07), ('contract', 0.068), ('piecewise', 0.067), ('kernel', 0.066), ('polya', 0.066), ('ahead', 0.063), ('commercial', 0.063), ('bandwidth', 0.061), ('mises', 0.06), ('isotonic', 0.06), ('bought', 0.06), ('pricing', 0.06), ('optimization', 0.058), ('sold', 0.057), ('day', 0.055), ('ps', 0.055), ('separable', 0.054), ('slopes', 0.054), ('weighting', 0.051), ('process', 0.051), ('ernel', 0.05), ('gnore', 0.05), ('gradientbased', 0.05), ('nown', 0.05), ('rainy', 0.05), ('stocking', 0.05), ('sunny', 0.05), ('tate', 0.05), ('posterior', 0.048), ('princeton', 0.047), ('gradients', 0.046), ('convex', 0.046), ('da', 0.046), ('db', 0.044), ('density', 0.044), ('energy', 0.044), ('outcome', 0.044), ('seasonal', 0.044), ('curses', 0.044), ('ferguson', 0.044), ('pi', 0.044), ('objective', 0.042), ('farm', 0.04), ('interquartile', 0.04), ('approximate', 0.039), ('blei', 0.039), ('constructing', 0.039), ('query', 0.038), ('np', 0.038), ('xk', 0.038), ('ind', 0.038), ('forecast', 0.038), ('cb', 0.038), ('latitude', 0.038), ('longitude', 0.038), ('package', 0.037), ('sj', 0.037), ('variable', 0.037), ('weight', 0.036), ('thumb', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="185-tfidf-1" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>2 0.22685438 <a title="185-tfidf-2" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>Author: Chris Barber, Joseph Bockhorst, Paul Roebber</p><p>Abstract: Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. 1</p><p>3 0.14947087 <a title="185-tfidf-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.13788775 <a title="185-tfidf-4" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>5 0.12908834 <a title="185-tfidf-5" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>6 0.12825567 <a title="185-tfidf-6" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>7 0.11680734 <a title="185-tfidf-7" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>8 0.10128045 <a title="185-tfidf-8" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>9 0.094517961 <a title="185-tfidf-9" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>10 0.094158776 <a title="185-tfidf-10" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>11 0.087864466 <a title="185-tfidf-11" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>12 0.083874241 <a title="185-tfidf-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.08147829 <a title="185-tfidf-13" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>14 0.080453157 <a title="185-tfidf-14" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>15 0.073658846 <a title="185-tfidf-15" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>16 0.072665706 <a title="185-tfidf-16" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>17 0.07223843 <a title="185-tfidf-17" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>18 0.070800021 <a title="185-tfidf-18" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>19 0.070145264 <a title="185-tfidf-19" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>20 0.06957458 <a title="185-tfidf-20" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, -0.007), (2, 0.044), (3, 0.06), (4, -0.07), (5, 0.103), (6, 0.127), (7, 0.041), (8, 0.048), (9, 0.021), (10, 0.003), (11, -0.061), (12, -0.077), (13, -0.072), (14, -0.033), (15, -0.001), (16, 0.035), (17, 0.131), (18, 0.061), (19, 0.066), (20, -0.094), (21, 0.076), (22, 0.077), (23, -0.025), (24, 0.049), (25, 0.033), (26, -0.112), (27, 0.02), (28, -0.115), (29, 0.004), (30, -0.209), (31, 0.165), (32, -0.011), (33, 0.115), (34, -0.116), (35, 0.091), (36, -0.019), (37, -0.167), (38, 0.045), (39, -0.059), (40, -0.13), (41, -0.214), (42, -0.042), (43, -0.066), (44, 0.039), (45, -0.106), (46, -0.044), (47, 0.058), (48, 0.022), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94240874 <a title="185-lsi-1" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>2 0.69740301 <a title="185-lsi-2" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>Author: Chris Barber, Joseph Bockhorst, Paul Roebber</p><p>Abstract: Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. 1</p><p>3 0.68537265 <a title="185-lsi-3" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>Author: Bela Frigyik, Maya Gupta, Yihua Chen</p><p>Abstract: Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data. 1 Modeling Probabilities for Machine Learning Modeling probability mass functions (pmfs) as random is useful in solving many real-world problems. A common random model for pmfs is the Dirichlet distribution [1]. The Dirichlet is conjugate to the multinomial and hence mathematically convenient for Bayesian inference, and the number of parameters is conveniently linear in the size of the sample space. However, the Dirichlet is a distribution over the entire probability simplex, and for many problems this is simply the wrong domain if there is application-speciﬁc prior knowledge that the pmfs come from a restricted subset of the simplex. For example, in natural language modeling, it is common to regularize a pmf over n-grams by some generic language model distribution q0 , that is, the pmf to be modeled is assumed to have the form θ = λq + (1 − λ)q0 for some q in the simplex, λ ∈ (0, 1) and a ﬁxed generic model q0 [2]. But once q0 and λ are ﬁxed, the pmf θ can only come from a subset of the simplex. Another natural language processing example is modeling the probability of keywords in a dictionary where some words are related, such as espresso and latte, and evidence for the one is to some extent evidence for the other. This relationship can be captured with a bounded variation model that would constrain the modeled probability of espresso to be within some of the modeled probability of latte. We show that such bounds on the variation between pmf components also restrict the domain of the pmf to a subset of the simplex. As a third example of restricting the domain, the similarity discriminant analysis classiﬁer estimates class-conditional pmfs that are constrained to be monotonically increasing over an ordered sample space of discrete similarity values [3]. In this paper we propose a simple variant of the Dirichlet whose support is a subset of the simplex, explore its properties, and show how to learn the model from data. We ﬁrst discuss the alternative solution of renormalizing the Dirichlet over the desired subset of the simplex, and other related work. Then we propose the shadow Dirichlet distribution; explain how to construct a shadow Dirichlet for three types of restricted domains: the regularized pmf case, bounded variation between pmf components, and monotonic pmfs; and discuss the most general case. We show how to use the expectation-maximization (EM) algorithm to estimate the shadow Dirichlet parameter α, and present simulation results for the estimation. 1 Dirichlet Shadow Dirichlet Renormalized Dirichlet Figure 1: Dirichlet, shadow Dirichlet, and renormalized Dirichlet for α = [3.94 2.25 2.81]. 2 Related Work One solution to modeling pmfs on only a subset of the simplex is to simply restrict the support of ˜ ˜ the Dirichlet to the desired support S, and renormalize the Dirichlet over S (see Fig. 1 for an example). This renormalized Dirichlet has the advantage that it is still a conjugate distribution for the multinomial. Nallapati et al.considered the renormalized Dirichlet for language modeling, but found it difﬁcult to use because the density requires numerical integration to compute the normalizer [4] . In addition, there is no closed form solution for the mean, covariance, or peak of the renormalized Dirichlet, making it difﬁcult to work with. Table 1 summarizes these properties. Additionally, generating samples from the renormalized Dirichlet is inefﬁcient: one draws samples from the stan˜ dard Dirichlet, then rejects realizations that are outside S. For high-dimensional sample spaces, this could greatly increase the time to generate samples. Although the Dirichlet is a classic and popular distribution on the simplex, Aitchison warns it “is totally inadequate for the description of the variability of compositional data,” because of its “implied independence structure and so the Dirichlet class is unlikely to be of any great use for describing compositions whose components have even weak forms of dependence” [5]. Aitchison instead championed a logistic normal distribution with more parameters to control covariance between components. A number of variants of the Dirichlet that can capture more dependence have been proposed and analyzed. For example, the scaled Dirichlet enables a more ﬂexible shape for the distribution [5], but does not change the support. The original Dirichlet(α1 , α2 , . . . αd ) can be derived as Yj / j Yj where Yj ∼ Γ(αj , β), whereas the scaled Dirichlet is derived from Yj ∼ Γ(αj , βj ), resulting in α density p(θ) = γ j ( α −1 βj j θ j j βi θi )α1 +···+αd i , where β, α ∈ Rd are parameters, and γ is the normalizer. + Another variant is the generalized Dirichlet [6] which also has parameters β, α ∈ Rd , and allows + greater control of the covariance structure, again without changing the support. As perhaps ﬁrst noted by Karl Pearson [7] and expounded upon by Aitchison [5], correlations of proportional data can be very misleading. Many Dirichlet variants have been generalizations of the Connor-Mossiman variant, Dirichlet process variants, other compound Dirichlet models, and hierarchical Dirichlet models. Ongaro et al. [8] propose the ﬂexible Dirichlet distribution by forming a re-parameterized mixture of Dirichlet distributions. Rayens and Srinivasan [9] considered the dependence structure for the general Dirichlet family called the generalized Liouville distributions. In contrast to prior efforts, the shadow Dirichlet manipulates the support to achieve various kinds of dependence that arise frequently in machine learning problems. 3 Shadow Dirichlet Distribution We introduce a new distribution that we call the shadow Dirichlet distribution. Let S be the prob˜ ability (d − 1)-simplex, and let Θ ∈ S be a random pmf drawn from a Dirichlet distribution with density pD and unnormalized parameter α ∈ Rd . Then we say the random pmf Θ ∈ S is distributed + ˜ according to a shadow Dirichlet distribution if Θ = M Θ for some ﬁxed d × d left-stochastic (that ˜ is, each column of M sums to 1) full-rank (and hence invertible) matrix M , and we call Θ the gen2 erating Dirichlet of Θ, or Θ’s Dirichlet shadow. Because M is a left-stochastic linear map between ﬁnite-dimensional spaces, it is a continuous map from the convex and compact S to a convex and compact subset of S that we denote SM . The shadow Dirichlet has two parameters: the generating Dirichlet’s parameter α ∈ Rd , and the + d × d matrix M . Both α and M can be estimated from data. However, as we show in the following subsections, the matrix M can be proﬁtably used as a design parameter that is chosen based on application-speciﬁc knowledge or side-information to specify the restricted domain SM , and in that way impose dependency between the components of the random pmfs. The shadow Dirichlet density p(θ) is the normalized pushforward of the Dirichlet density, that is, it is the composition of the Dirichlet density and M −1 with the Jacobian: 1 α −1 p(θ) = (M −1 θ)j j , (1) B(α) |det(M )| j Γ(αj ) d j is the standard Dirichlet normalizer, and α0 = j=1 αj is the standard where B(α) Γ(α0 ) Dirichlet precision factor. Table 1 summarizes the basic properties of the shadow Dirichlet. Fig. 1 shows an example shadow Dirichlet distribution. Generating samples from the shadow Dirichlet is trivial: generate samples from its generating Dirichlet (for example, using stick-breaking or urn-drawing) and multiply each sample by M to create the corresponding shadow Dirichlet sample. Table 1: Table compares and summarizes the Dirichlet, renormalized Dirichlet, and shadow Dirichlet distributions. Dirichlet(α) Density p(θ) Mean 1 B(α) d j=1 α −1 θj j Shadow Dirichlet (α, M ) 1 B(α)|det(M )| α α0 Renormalized ˜ Dirichlet (α, S) d −1 αj −1 θ)j j=1 (M 1 ˜ S M d j=1 α α0 αj −1 qj dq d j=1 α −1 θj j ˜ θp(θ)dθ S ¯ ¯ − θ)(θ − θ)T p(θ)dθ Cov(Θ) M Cov(Θ)M T αj −1 α0 −d j M α0 −d max p(θ) stick-breaking, urn-drawing draw from Dirichlet(α), multiply by M draw from Dirichlet(α), ˜ reject if not in S ML Estimate iterative (simple functions) iterative (simple functions) unknown complexity ML Compound Estimate iterative (simple functions) iterative (numerical integration) unknown complexity Covariance Mode (if α > 1) How to Sample 3.1 α −1 ˜ (θ S ˜ θ∈S Example: Regularized Pmfs The shadow Dirichlet can be designed to specify a distribution over a set of regularized pmfs SM = ˜ ˘ ˜ ˘ ˘ {θ θ = λθ + (1 − λ)θ, θ ∈ S}, for speciﬁc values of λ and θ. In general, for a given λ and θ ∈ S, the following d × d matrix M will change the support to the desired subset SM by mapping the extreme points of S to the extreme points of SM : ˘ M = (1 − λ)θ1T + λI, (2) where I is the d × d identity matrix. In Section 4 we show that the M given in (2) is optimal in a maximum entropy sense. 3 3.2 Example: Bounded Variation Pmfs We describe how to use the shadow Dirichlet to model a random pmf that has bounded variation such that |θk − θl | ≤ k,l for any k, ∈ {1, 2, . . . , d} and k,l > 0. To construct speciﬁed bounds on the variation, we ﬁrst analyze the variation for a given M . For any d × d left stochastic matrix T d d ˜ ˜ ˜ M, θ = Mθ = M1j θj . . . Mdj θj , so the difference between any two entries is j=1 j=1 ˜ |Mkj − Mlj | θj . ˜ (Mkj − Mlj )θj ≤ |θk − θl | = (3) j j Thus, to obtain a distribution over pmfs with bounded |θk − θ | ≤ k,l for any k, components, it is sufﬁcient to choose components of the matrix M such that |Mkj − Mlj | ≤ k,l for all j = 1, . . . , d ˜ because θ in (3) sums to 1. One way to create such an M is using the regularization strategy described in Section 3.1. For this ˜ ˜ ˘ case, the jth component of θ is θj = M θ = λθj + (1 − λ)θj , and thus the variation between the j ith and jth component of any pmf in SM is: ˜ ˘ ˜ ˘ ˜ ˜ ˘ ˘ |θi − θj | = λθi + (1 − λ)θi − λθj − (1 − λ)θj ≤ λ θi − θj + (1 − λ) θi − θj ˘ ˘ ≤ λ + (1 − λ) max θi − θj . (4) i,j ˘ Thus by choosing an appropriate λ and regularizing pmf θ, one can impose the bounded variation ˘ to be the uniform pmf, and choose any λ ∈ (0, 1), then the matrix given by (4). For example, set θ M given by (2) will guarantee that the difference between any two entries of any pmf drawn from the shadow Dirichlet (M, α) will be less than or equal to λ. 3.3 Example: Monotonic Pmfs For pmfs over ordered components, it may be desirable to restrict the support of the random pmf distribution to only monotonically increasing pmfs (or to only monotonically decreasing pmfs). A d × d left-stochastic matrix M that will result in a shadow Dirichlet that generates only monotonically increasing d × 1 pmfs has kth column [0 . . . 0 1/(d − k + 1) . . . 1/(d − k + 1)]T , we call this the monotonic M . It is easy to see that with this M only monotonic θ’s can be produced, 1˜ 1˜ 1 ˜ because θ1 = d θ1 which is less than or equal to θ2 = d θ1 + d−1 θ2 and so on. In Section 4 we show that the monotonic M is optimal in a maximum entropy sense. Note that to provide support over both monotonically increasing and decreasing pmfs with one distribution is not achievable with a shadow Dirichlet, but could be achieved by a mixture of two shadow Dirichlets. 3.4 What Restricted Subsets are Possible? Above we have described solutions to construct M for three kinds of dependence that arise in machine learning applications. Here we consider the more general question: What subsets of the simplex can be the support of the shadow Dirichlet, and how to design a shadow Dirichlet for a particular support? For any matrix M , by the Krein-Milman theorem [10], SM = M S is the convex hull of its extreme points. If M is injective, the extreme points of SM are easy to specify, as a d × d matrix M will have d extreme points that occur for the d choices of θ that have only one nonzero component, as the rest of the θ will create a non-trivial convex combination of the columns of M , and therefore cannot result in extreme points of SM by deﬁnition. That is, the extreme points of SM are the d columns of M , and one can design any SM with d extreme points by setting the columns of M to be those extreme pmfs. However, if one wants the new support to be a polytope in the probability (d − 1)-simplex with m > d extreme points, then one must use a fat M with d × m entries. Let S m denote the probability 4 (m − 1)-simplex, then the domain of the shadow Dirichlet will be M S m , which is the convex hull of the m columns of M and forms a convex polytope in S with at most m vertices. In this case M cannot be injective, and hence it is not bijective between S m and M S m . However, a density on M S m can be deﬁned as: p(θ) = 1 B(α) ˜ {θ ˜α −1 ˜ θj j dθ. ˜ M θ=θ} j (5) On the other hand, if one wants the support to be a low-dimensional polytope subset of a higherdimensional probability simplex, then a thin d × m matrix M , where m < d, can be used to implement this. If M is injective, then it has a left inverse M ∗ that is a matrix of dimension m × d, and the normalized pushforward of the original density can be used as a density on the image M S m : p(θ) = 1 α −1 1/2 B(α) |det(M T M )| (M ∗ θ)j j , j If M is not injective then one way to determine a density is to use (5). 4 Information-theoretic Properties In this section we note two information-theoretic properties of the shadow Dirichlet. Let Θ be drawn ˜ from shadow Dirichlet density pM , and let its generating Dirichlet Θ be drawn from pD . Then the differential entropy of the shadow Dirichlet is h(pM ) = log |det(M )| + h(pD ), where h(pD ) is the differential entropy of its generating Dirichlet. In fact, the shadow Dirichlet always has less entropy than its Dirichlet shadow because log |det(M )| ≤ 0, which can be shown as a corollary to the following lemma (proof not included due to lack of space): Lemma 4.1. Let {x1 , . . . , xn } and {y1 , . . . , yn } be column vectors in Rn . If each yj is a convex n n combination of the xi ’s, i.e. yj = i=1 γji xi , i=1 γji = 1, γjk ≥ 0, ∀j, k ∈ {1, . . . , n} then |det[y1 , . . . , yn ]| ≤ |det[x1 , . . . , xn ]|. It follows from Lemma 4.1 that the constructive solutions for M given in (2) and the monotonic M are optimal in the sense of maximizing entropy: Corollary 4.1. Let Mreg be the set of left-stochastic matrices M that parameterize shadow Dirichlet ˜ ˘ ˜ ˘ distributions with support in {θ θ = λθ + (1 − λ)θ, θ ∈ S}, for a speciﬁc choice of λ and θ. Then the M given in (2) results in the shadow Dirichlet with maximum entropy, that is, (2) solves arg maxM ∈Mreg h(pM ). Corollary 4.2. Let Mmono be the set of left-stochastic matrices M that parameterize shadow Dirichlet distributions that generate only monotonic pmfs. Then the monotonic M given in Section 3.3 results in the shadow Dirichlet with maximum entropy, that is, the monotonic M solves arg maxM ∈Mmono h(pM ). 5 Estimating the Distribution from Data In this section, we discuss the estimation of α for the shadow Dirichlet and compound shadow Dirichlet, and the estimation of M . 5.1 Estimating α for the Shadow Dirichlet Let matrix M be speciﬁed (for example, as described in the subsections of Section 3), and let q be a d × N matrix where the ith column qi is the ith sample pmf for i = 1 . . . N , and let (qi )j be the jth component of the ith sample pmf for j = 1, . . . , d. Then ﬁnding the maximum likelihood estimate 5 of α for the shadow Dirichlet is straightforward:  N 1 arg max log + log  p(qi |α) ≡ arg max log B(α) |det(M )| α∈Rk α∈Rk + + i=1   1 αj −1  (˜i )j q , ≡ arg max log  B(α)N i j α∈Rk +  N α −1 (M −1 qi )j j  i j (6) where q = M −1 q. Note (6) is the maximum likelihood estimation problem for the Dirichlet dis˜ tribution given the matrix q , and can be solved using the standard methods for that problem (see ˜ e.g. [11, 12]). 5.2 Estimating α for the Compound Shadow Dirichlet For many machine learning applications the given data are modeled as samples from realizations of a random pmf, and given these samples one must estimate the random pmf model’s parameters. We refer to this case as the compound shadow Dirichlet, analogous to the compound Dirichlet (also called the multivariate P´ lya distribution). Assuming one has already speciﬁed M , we ﬁrst discuss o method of moments estimation, and then describe an expectation-maximization (EM) method for computing the maximum likelihood estimate α. ˘ One can form an estimate of α by the method of moments. For the standard compound Dirichlet, one treats the samples of the realizations as normalized empirical histograms, sets the normalized α parameter equal to the empirical mean of the normalized histograms, and uses the empirical variances to determine the precision α0 . By deﬁnition, this estimate will be less likely than the maximum likelihood estimate, but may be a practical short-cut in some cases. For the compound shadow Dirichlet, we believe the method of moments estimator will be a poorer estimate in general. The problem is that if one draws samples from a pmf θ from a restricted subset SM of the simplex, ˘ then the normalized empirical histogram θ of those samples may not be in SM . For example given a monotonic pmf, the histogram of ﬁve samples drawn from it may not be monotonic. Then the empirical mean of such normalized empirical histograms may not be in SM , and so setting the shadow Dirichlet mean M α equal to the empirical mean may lead to an infeasible estimate (one that is outside SM ). A heuristic solution is to project the empirical mean into SM ﬁrst, for example, by ﬁnding the nearest pmf in SM in squared error or relative entropy. As with the compound Dirichlet, this may still be a useful approach in practice for some problems. Next we state an EM method to ﬁnd the maximum likelihood estimate α. Let s be a d × N matrix ˘ of sample histograms from different experiments, such that the ith column si is the ith histogram for i = 1, . . . , N , and (si )j is the number of times we have observed the jth event from the ith pmf vi . Then the maximum log-likelihood estimate of α solves arg max log p(s|α) for α ∈ Rk . + If the random pmfs are drawn from a Dirichlet distribution, then ﬁnding this maximum likelihood estimate requires an iterative procedure, and can be done in several ways including a gradient descent (ascent) approach. However, if the random pmfs are drawn from a shadow Dirichlet distribution, then a direct gradient descent approach is highly inconvenient as it requires taking derivatives of numerical integrals. However, it is practical to apply the expectation-maximization (EM) algorithm [13][14], as we describe in the rest of this section. Code to perform the EM estimation of α can be downloaded from idl.ee.washington.edu/publications.php. We assume that the experiments are independent and therefore p(s|α) = p({si }|α) = and hence arg maxα∈Rk log p(s|α) = arg maxα∈Rk i log p(si |α). + + i p(si |α) To apply the EM method, we consider the complete data to be the sample histograms s and the pmfs that generated them (s, v1 , v2 , . . . , vN ), whose expected log-likelihood will be maximized. Speciﬁcally, because of the assumed independence of the {vi }, the EM method requires one to repeatedly maximize the Q-function such that the estimate of α at the (m + 1)th iteration is: N α(m+1) = arg max α∈Rk + Evi |si ,α(m) [log p(vi |α)] . i=1 6 (7) Like the compound Dirichlet likelihood, the compound shadow Dirichlet likelihood is not necessarily concave. However, note that the Q-function given in (7) is concave, because log p(vi |α) = − log |det(M )| + log pD,α M −1 vi , where pD,α is the Dirichlet distribution with parameter α, and by a theorem of Ronning [11], log pD,α is a concave function, and adding a constant does not change the concavity. The Q-function is a ﬁnite integration of such concave functions and hence also concave [15]. We simplify (7) without destroying the concavity to yield the equivalent problem α(m+1) = d d arg max g(α) for α ∈ Rk , where g(α) = log Γ(α0 ) − j=1 log Γ(αj ) + j=1 βj αj , and + βj = N tij i=1 zi , 1 N where tij and zi are integrals we compute with Monte Carlo integration: d (s ) log(M −1 vi )j γi tij = SM (vi )k i k pM (vi |α(m) )dvi k=1 d zi = (vi )j k(si )k pM (vi |α(m) )dvi , γi SM k=1 where γi is the normalization constant for the multinomial with histogram si . We apply the Newton method [16] to maximize g(α), where the gradient g(α) has kth component ψ0 (α0 ) − ψ0 (α1 ) + β1 , where ψ0 denotes the digamma function. Let ψ1 denote the trigamma function, then the Hessian matrix of g(α) is: H = ψ1 (α0 )11T − diag (ψ1 (α1 ), . . . , ψ1 (αd )) . Note that because H has a very simple structure, the inversion of H required by the Newton step is greatly simpliﬁed by using the Woodbury identity [17]: H −1 = − diag(ξ1 , . . . , ξd ) − 1 1 1 [ξi ξj ]d×d , where ξ0 = ψ1 (α0 ) and ξj = ψ1 (αj ) , j = 1, . . . , d. ξ − d ξ 0 5.3 j=1 j Estimating M for the Shadow Dirichlet Thus far we have discussed how to construct M to achieve certain desired properties and how to interpret a given M ’s effect on the support. In some cases it may be useful to estimate M directly from data, for example, ﬁnding the maximum likelihood M . In general, this is a non-convex problem because the set of rank d − 1 matrices is not convex. However, we offer two approximations. First, note that as in estimating the support of a uniform distribution, the maximum likelihood M will correspond to a support that is no larger than needed to contain the convex hull of sample pmfs. Second, the mean of the empirical pmfs will be in the support, and thus a heuristic is to set the kth column of M (which corresponds to the kth vertex of the support) to be a convex combination of the kth vertex of the standard probability simplex and the empirical mean pmf. We provide code that ﬁnds the d optimal such convex combinations such that a speciﬁced percentage of the sample pmfs are within the support, which reduces the non-convex problem of ﬁnding the maximum likelihood d × d matrix M to a d-dimensional convex relaxation. 6 Demonstrations It is reasonable to believe that if the shadow Dirichlet better matches the problem’s statistics, it will perform better in practice, but an open question is how much better? To motivate the reader to investigate this question further in applications, we provide two small demonstrations. 6.1 Verifying the EM Estimation We used a broad suite of simulations to test and verify the EM estimation. Here we include a simple visual conﬁrmation that the EM estimation works: we drew 100 i.i.d. pmfs from a shadow Dirichlet with monotonic M for d = 3 and α = [3.94 2.25 2.81] (used in [18]). From each of the 100 pmfs, we drew 100 i.i.d. samples. Then we applied the EM algorithm to ﬁnd the α for both the standard compound Dirichlet, and the compound shadow Dirichlet with the correct M . Fig. 2 shows the true distribution and the two estimated distributions. 7 True Distribution (Shadow Dirichlet) Estimated Shadow Dirichlet Estimated Dirichlet Figure 2: Samples were drawn from the true distribution and the given EM method was applied to form the estimated distributions. 6.2 Estimating Proportions from Sales Manufacturers often have constrained manufacturing resources, such as equipment, inventory of raw materials, and employee time, with which to produce multiple products. The manufacturer must decide how to proportionally allocate such constrained resources across their product line based on their estimate of proportional sales. Manufacturer Artifact Puzzles gave us their past retail sales data for the 20 puzzles they sold during July 2009 through Dec 2009, which we used to predict the proportion of sales expected for each puzzle. These estimates were then tested on the next ﬁve months of sales data, for January 2010 through April 2010. The company also provided a similarity between puzzles S, where S(A, B) is the proportion of times an order during the six training months included both puzzle A and B if it included puzzle A. We compared treating each of the six training months of sales data as a sample from a compound Dirichlet versus or a compound shadow Dirichlet. For the shadow Dirichlet, we normalized each column of the similarity matrix S to sum to one so that it was left-stochastic, and used that as the M matrix; this forces puzzles that are often bought together to have closer estimated proportions. We estimated each α parameter by EM to maximize the likelihood of the past sales data, and then estimated the future sales proportions to be the mean of the estimated Dirichlet or shadow Dirichlet distribution. We also compared with treating all six months of sales data as coming from one multinomial which we estimated as the maximum likelihood multinomial, and to taking the mean of the six empirical pmfs. Table 2: Squared errors between estimates and actual proportional sales. Jan. Feb. Mar. Apr. 7 Multinomial .0129 .0185 .0231 .0240 Mean Pmf .0106 .0206 .0222 .0260 Dirichlet .0109 .0172 .0227 .0235 Shadow Dirichlet .0093 .0164 .0197 .0222 Summary In this paper we have proposed a variant of the Dirichlet distribution that naturally captures some of the dependent structure that arises often in machine learning applications. We have discussed some of its theoretical properties, and shown how to specify the distribution for regularized pmfs, bounded variation pmfs, monotonic pmfs, and for any desired convex polytopal domain. We have derived the EM method and made available code to estimate both the shadow Dirichlet and compound shadow Dirichlet from data. Experimental results demonstrate that the EM method can estimate the shadow Dirichlet effectively, and that the shadow Dirichlet may provide worthwhile advantages in practice. 8 References [1] B. Frigyik, A. Kapila, and M. R. Gupta, “Introduction to the Dirichlet distribution and related processes,” Tech. Rep., University of Washington, 2010. [2] C. Zhai and J. Lafferty, “A study of smoothing methods for language models applied to information retrieval,” ACM Trans. on Information Systems, vol. 22, no. 2, pp. 179–214, 2004. [3] Y. Chen, E. K. Garcia, M. R. Gupta, A. Rahimi, and L. Cazzanti, “Similarity-based classiﬁcation: Concepts and algorithms,” Journal of Machine Learning Research, vol. 10, pp. 747–776, March 2009. [4] R. Nallapati, T. Minka, and S. Robertson, “The smoothed-Dirichlet distribution: a building block for generative topic models,” Tech. Rep., Microsoft Research, Cambridge, 2007. [5] Aitchison, Statistical Analysis of Compositional Data, Chapman Hall, New York, 1986. [6] R. J. Connor and J. E. Mosiman, “Concepts of independence for proportions with a generalization of the Dirichlet distibution,” Journal of the American Statistical Association, vol. 64, pp. 194–206, 1969. [7] K. Pearson, “Mathematical contributions to the theory of evolution–on a form of spurious correlation which may arise when indices are used in the measurement of organs,” Proc. Royal Society of London, vol. 60, pp. 489–498, 1897. [8] A. Ongaro, S. Migliorati, and G. S. Monti, “A new distribution on the simplex containing the Dirichlet family,” Proc. 3rd Compositional Data Analysis Workshop, 2008. [9] W. S. Rayens and C. Srinivasan, “Dependence properties of generalized Liouville distributions on the simplex,” Journal of the American Statistical Association, vol. 89, no. 428, pp. 1465– 1470, 1994. [10] Walter Rudin, Functional Analysis, McGraw-Hill, New York, 1991. [11] G. Ronning, “Maximum likelihood estimation of Dirichlet distributions,” Journal of Statistical Computation and Simulation, vol. 34, no. 4, pp. 215221, 1989. [12] T. Minka, “Estimating a Dirichlet distribution,” Tech. Rep., Microsoft Research, Cambridge, 2009. [13] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from incomplete data via the EM algorithm,” Journal of the Royal Statistical Society: Series B (Methodological), vol. 39, no. 1, pp. 1–38, 1977. [14] M. R. Gupta and Y. Chen, Theory and Use of the EM Method, Foundations and Trends in Signal Processing, Hanover, MA, 2010. [15] R. T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ, 1970. [16] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Cambridge, 2004. [17] K. B. Petersen and M. S. Pedersen, Matrix Cookbook, 2009, Available at matrixcookbook.com. [18] R. E. Madsen, D. Kauchak, and C. Elkan, “Modeling word burstiness using the Dirichlet distribution,” in Proc. Intl. Conf. Machine Learning, 2005. 9</p><p>4 0.62872851 <a title="185-lsi-4" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>5 0.5676288 <a title="185-lsi-5" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>Author: Nicholas Bartlett, Frank Wood, David Tax</p><p>Abstract: We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic ﬁnite automata (PDFA). We deﬁne and develop a sampler for a PDFA with an inﬁnite number of states which we call the probabilistic deterministic inﬁnite automata (PDIA). Posterior predictive inference in this model, given a ﬁnite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models. 1</p><p>6 0.43315801 <a title="185-lsi-6" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>7 0.42241094 <a title="185-lsi-7" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>8 0.41853893 <a title="185-lsi-8" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>9 0.41125304 <a title="185-lsi-9" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>10 0.40257433 <a title="185-lsi-10" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>11 0.40169054 <a title="185-lsi-11" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>12 0.39646891 <a title="185-lsi-12" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>13 0.38573739 <a title="185-lsi-13" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>14 0.38031137 <a title="185-lsi-14" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>15 0.3786048 <a title="185-lsi-15" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>16 0.37581074 <a title="185-lsi-16" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>17 0.36480269 <a title="185-lsi-17" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>18 0.35978615 <a title="185-lsi-18" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>19 0.35297963 <a title="185-lsi-19" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>20 0.34673408 <a title="185-lsi-20" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.271), (13, 0.034), (17, 0.011), (27, 0.072), (30, 0.071), (35, 0.01), (45, 0.205), (50, 0.072), (52, 0.045), (60, 0.033), (77, 0.057), (90, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76698995 <a title="185-lda-1" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>2 0.72955215 <a title="185-lda-2" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>Author: Yu Zhang, Dit-Yan Yeung</p><p>Abstract: Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we ďŹ rst analyze the scatter measures used in the conventional linear discriminant analysis (LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis (WLDA) by deďŹ ning new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classiďŹ cation. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by ďŹ nding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods. 1</p><p>3 0.68047142 <a title="185-lda-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.67989212 <a title="185-lda-4" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>5 0.67410761 <a title="185-lda-5" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>6 0.67137265 <a title="185-lda-6" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>7 0.67038006 <a title="185-lda-7" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>8 0.66997826 <a title="185-lda-8" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>9 0.66831118 <a title="185-lda-9" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>10 0.66803563 <a title="185-lda-10" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>11 0.66748095 <a title="185-lda-11" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>12 0.66702813 <a title="185-lda-12" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>13 0.66688675 <a title="185-lda-13" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>14 0.6662935 <a title="185-lda-14" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>15 0.6658994 <a title="185-lda-15" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>16 0.66400772 <a title="185-lda-16" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>17 0.66380531 <a title="185-lda-17" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>18 0.66336399 <a title="185-lda-18" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>19 0.66255504 <a title="185-lda-19" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>20 0.66250283 <a title="185-lda-20" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
