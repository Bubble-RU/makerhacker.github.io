<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-84" href="#">nips2010-84</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</h1>
<br/><p>Source: <a title="nips-2010-84-pdf" href="http://papers.nips.cc/paper/3899-exact-inference-and-learning-for-cumulative-distribution-functions-on-loopy-graphs.pdf">pdf</a></p><p>Author: Nebojsa Jojic, Chris Meek, Jim C. Huang</p><p>Abstract: Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (nonloopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide signiďŹ cantly better ďŹ ts to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p><p>Reference: <a title="nips-2010-84-reference" href="../nips2010_reference/nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Exact inference and learning for cumulative distribution functions on loopy graphs Jim C. [sent-1, score-0.337]
</p><p>2 Huang, Nebojsa Jojic and Christopher Meek Microsoft Research One Microsoft Way, Redmond, WA 98052  Abstract Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. [sent-2, score-0.118]
</p><p>3 Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. [sent-3, score-0.139]
</p><p>4 Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). [sent-4, score-0.212]
</p><p>5 Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. [sent-7, score-0.449]
</p><p>6 We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. [sent-8, score-0.434]
</p><p>7 1 Introduction The last two decades have been marked by signiďŹ cant advances in modeling multivariate probability density functions (PDFs) on graphs. [sent-10, score-0.081]
</p><p>8 Various inference and learning algorithms have been successfully developed that take advantage of known variable dependence which can be used to simplify computations and avoid overtraining. [sent-11, score-0.092]
</p><p>9 A major source of difďŹ culty for such algorithms is the need to compute a normalization term, as graphical models generally assume a factorized form for the joint PDF. [sent-12, score-0.078]
</p><p>10 Most of these methods are based on transforming the data to make it more easily modeled by Gaussian PDF-ďŹ tting techniques, an example of which is the Gaussian copula [11] parameterized as a CDF deďŹ ned on nonlinearly transformed variables. [sent-16, score-0.058]
</p><p>11 The cumulative distribution networks (CDNs) model a multivariate CDF as a product over functions, each dependent on a small subset of variables and each having a CDF form [6, 7]. [sent-20, score-0.2]
</p><p>12 One of the key advantages of this approach is that it eliminates the need to enforce normalization constraints that complicate inference and learning in graphical models of PDFs. [sent-21, score-0.103]
</p><p>13 In a CDN, inference and learning involves computation of derivatives of the joint CDF with respect to model variables and parameters. [sent-23, score-0.16]
</p><p>14 The graphical model then allows us to efďŹ ciently perform inference and learning for non-loopy CDNs using message-passing [6, 8]. [sent-24, score-0.08]
</p><p>15 Models of this form have 1  been applied to multivariate heavy-tailed data in climatology and epidemiology where they have demonstrated improved predictive performance as compared to several graphical models for PDFs despite the restriction to tree-structured CDNs. [sent-25, score-0.198]
</p><p>16 Non-loopy CDNs may however be limited models and adding functions to the CDN may provide signiďŹ cantly more expressive models, with the caveat that the resulting CDN may become loopy and previous algorithms for inference and learning in . [sent-26, score-0.254]
</p><p>17 Our aim in this paper is to provide an effective algorithm for learning and inference in loopy CDNs, thus improving on previous approaches which were limited to CDNs with non-loopy dependencies. [sent-28, score-0.211]
</p><p>18 In principle, symbolic differentiation algorithms such as Mathematica [16] and D* [4] could be used for inference and learning for loopy CDNs. [sent-29, score-0.451]
</p><p>19 In this paper, we develop the JDiff algorithm which uses the graphical structure to simplify the computation of the derivative and enables both inference and learning for CDNs of arbitrary topology. [sent-31, score-0.151]
</p><p>20 We also provide an empirical comparison of several methods for modeling multivariate distributions as applied to rainfall data and H1N1 data. [sent-33, score-0.231]
</p><p>21 We show that loopy CDNs provide signiďŹ cantly better model ďŹ ts for multivariate heavy-tailed data than non-loopy CDNs. [sent-34, score-0.232]
</p><p>22 Furthermore, these models outperform models based on Gaussian copulas [11], as well as multivariate heavy tailed models that do not allow for structure speciďŹ cation. [sent-35, score-0.13]
</p><p>23 &lsquo; ) be the set of neighboring variable nodes for function node đ? [sent-50, score-0.174]
</p><p>24 ´ [â&lsaquo;&hellip;] as the mixed derivative operator with respect to 3 variables in set đ? [sent-54, score-0.196]
</p><p>25 We also assume in the sequel that all derivatives of a CDF with respect to any and all arguments exist and are continuous and as a result any mixed derivative of the CDF is invariant to the order of differentiation (Schwarzâ&euro;&trade; theorem). [sent-70, score-0.4]
</p><p>26 The cumulative distribution network (CDN) consists of (1) an undirected bipartite graphical model consisting of a bipartite graph đ? [sent-73, score-0.154]
</p><p>27 ¸ connecting function nodes to variable nodes and (2) a speciďŹ cation of functions đ? [sent-82, score-0.206]
</p><p>28 &lsquo;  deďŹ ned over neighboring pairs of variable nodes, such that the product of functions satisďŹ es the properties of a CDF. [sent-118, score-0.095]
</p><p>29 1 Inference and learning in CDNs as differentiation For a joint CDF, the problems of inference and likelihood evaluation, or computing conditional CDFs and marginal PDFs, both correspond to mixed differentiation of the joint CDF [6]. [sent-155, score-0.534]
</p><p>30 In order to perform maximum1 likelihood estimation, we require the gradient vector â&circ;&Dagger;đ? [sent-200, score-0.063]
</p><p>31 &oelig;˝), which requires us to compute a vector of single derivatives â&circ;&sbquo;đ? [sent-208, score-0.074]
</p><p>32 2 Message-passing algorithms for differentiation in non-loopy graphs As described above, inference and learning in a CDN corresponds to computing derivatives of the CDF with respect to subsets of variables and/or model parameters. [sent-214, score-0.37]
</p><p>33 For inference in non-loopy CDNs, computing mixed derivatives of the form â&circ;&sbquo;xđ? [sent-215, score-0.227]
</p><p>34 In analogy to the way in which marginalization in graphical models for PDFs can be decomposed into a series of local computations, the DSP algorithm decomposes the global computation of the total mixed derivative â&circ;&sbquo;x [đ? [sent-223, score-0.262]
</p><p>35 š (x)] into a series of local computations by the passing of messages that correspond to mixed derivatives of đ? [sent-225, score-0.321]
</p><p>36 To evaluate the model likelihood, messages are passed from leaf nodes to the root variable node and the product of incoming root messages is differentiated. [sent-228, score-0.459]
</p><p>37 As in the DSP inference algorithm, the computation of the gradient can also be broken down into a series of local gradient computations. [sent-244, score-0.148]
</p><p>38 3 Differentiation in loopy graphs For loopy graphs, the DSP and GDP algorithms are not guaranteed to yield the correct derivative computations. [sent-246, score-0.458]
</p><p>39 For the general case of differentiating a product of CDFs, computing the total mixed derivative requires time and space exponential in the number of variables. [sent-247, score-0.226]
</p><p>40 To see this, consider the simple example of the derivative of a product of two functions đ? [sent-248, score-0.118]
</p><p>41 The mixed derivative of the product is then given by [5] â&circ;&lsquo; â&circ;&sbquo;xđ? [sent-255, score-0.192]
</p><p>42 As computing the mixed derivative of a product of more functions will entail even greater complexity, the naÂ¨ve approach will in general be intractable. [sent-269, score-0.231]
</p><p>43 Äą However, as we show in this paper, a CDNâ&euro;&trade;s sparse graphical structure may often point to ways to computing these derivatives efďŹ ciently, with non-loopy graphs being special, previously-studied cases. [sent-270, score-0.178]
</p><p>44 &lsquo;&dagger;1,2  The last step follows from identifying all derivatives that are zero, as we note that in the above, â&circ;&sbquo;xđ? [sent-453, score-0.074]
</p><p>45 The number of individual steps neededâ&circ;Š complete the differentiation in (2) depends on the size of to the variable intersection set đ? [sent-474, score-0.189]
</p><p>46 ş2 depend on two variable 3  sets that do not intersect, then the differentiation can be simpliďŹ ed by independently computing derivatives for each factor and multiplying. [sent-484, score-0.257]
</p><p>47 This suggests that we can recursively decompose the total mixed derivative and gradient computations into a series of simpler computations so that â&circ;&sbquo;x [đ? [sent-493, score-0.303]
</p><p>48 In such a recursion, the total product of factors is always broken into parts that share as few variables as possible. [sent-496, score-0.089]
</p><p>49 Such a recursive decomposition is naturally represented using a junction tree [12] for the CDN in which we will pass messages corresponding to local derivative computations. [sent-498, score-0.372]
</p><p>50 &rsquo;&#x17E;) be a tree where â&bdquo;° is the set of undirected edges so that for any pair đ? [sent-523, score-0.111]
</p><p>51 An example of a CDN and a corresponding junction tree are shown in Figures 1(a), 1(b). [sent-556, score-0.194]
</p><p>52 (a)  (b)  (c)  (d)  Figure 1: a) An example of a CDN with 7 variable nodes (circles) and 15 function nodes (diamonds); b) A junction tree obtained from the CDN of a). [sent-557, score-0.361]
</p><p>53 Separating sets are shown for each edge connecting nodes in the junction tree, each corresponding to a connected subset of variables in the CDN; c), d) CDNs used to model the rainfall and H1N1 datasets. [sent-558, score-0.453]
</p><p>54 &rsquo;Ż is a tree, we can root the tree at some node in đ? [sent-561, score-0.156]
</p><p>55 It remains to determine how we can efďŹ ciently compute messages in the above đ? [sent-743, score-0.086]
</p><p>56 At the root node, the correct mixed derivative is then given by đ? [sent-964, score-0.208]
</p><p>57 Note that the messages can be kept in a symbolic form as functions over appropriate variables, or, as is the case in the experiments section, they can simply be evaluated for the given data x. [sent-974, score-0.201]
</p><p>58 In the latter case, each message reduces to a scalar, as we can evaluate derivatives of the functions in the model for ďŹ xed x, đ? [sent-975, score-0.126]
</p><p>59 &oelig;˝ and so we do not need to store increasingly complex symbolic terms. [sent-976, score-0.095]
</p><p>60 &oelig;˝)], we can in parallel obtain the gradient of the likelihood function. [sent-983, score-0.063]
</p><p>61 &lsquo;&Yuml; ) can be decomposed in a similar fashion to the decomposition of the mixed derivative computation. [sent-1000, score-0.165]
</p><p>62 &lsquo;&ndash; in the junction tree decomposition is updated in parallel with the likelihood messages through the use of gradient messages gđ? [sent-1006, score-0.429]
</p><p>63 The algorithm for computing both the likelihood and its gradient, which we call JDiff for junction tree differentiation, is shown in Algorithm 1. [sent-1017, score-0.248]
</p><p>64 Thus by recursively computing the messages and their gradients starting from leaf nodes of đ? [sent-1018, score-0.239]
</p><p>65 &lsquo;&Yuml;, we can obtain the exact likelihood and gradient vector for the CDF modelled by đ? [sent-1020, score-0.063]
</p><p>66 &lsquo;&tilde; which is the total number of terms in the expanded sum of products form for computing mixed derivatives â&circ;&sbquo;xđ? [sent-1064, score-0.187]
</p><p>67 &lsquo;&trade;=0  since the cost of computing derivatives for each đ? [sent-1106, score-0.093]
</p><p>68 &lsquo;&tilde;)â&circ;&circ;â&bdquo;°  5  Algorithm 1: JDiff: A junction tree algorithm for computing the likelihood â&circ;&sbquo;x [đ? [sent-1145, score-0.248]
</p><p>69 The second set of experiments uses rainfall and H1N1 epidemiology data to demonstrate the practical value of loopy CDNs, which JDiff for the ďŹ rst time makes practical to learn from data. [sent-1417, score-0.357]
</p><p>70 1 Symbolic differentiation As a ďŹ rst test, we compared the runtime of JDiff to that of commonly-used symbolic differentiation tools such as Mathematica [16] and D* [4]. [sent-1419, score-0.4]
</p><p>71 A junction tree was constructed by greedily eliminating the variables with the minimal ďŹ ll-in algorithm and then constructing elimination subsets for nodes in the junction tree [10] using the MATLAB implementation of [14]. [sent-1426, score-0.509]
</p><p>72 For the 3 Ă&mdash; 3 grid, JDiff took less than 1 second to compute the symbolic derivative, whereas Mathematica and D* took 6. [sent-1428, score-0.095]
</p><p>73 Each marginal PDF can be computed analytically by taking limits followed by differentiation; d) Graphs for the H1N1 datasets with edges weighted according to mutual information under the CDN, nonparanormal and Gaussian BDGs for log-transformed data. [sent-1450, score-0.14]
</p><p>74 of CPU time) compute derivatives for graphs as large as 9 Ă&mdash; 9. [sent-1457, score-0.119]
</p><p>75 We also compared the time to compute mixed derivatives in loops of length đ? [sent-1458, score-0.183]
</p><p>76 to compute the total mixed derivative, whereas the time required by Mathematica varied from 1. [sent-1464, score-0.094]
</p><p>77 2 Learning models for rainfall and H1N1 data The JDiff algorithm allows us to compute mixed derivatives of a joint CDF for applications in which we may need to learn multivariate heavy-tailed distributions deďŹ ned on loopy graphs. [sent-1472, score-0.608]
</p><p>78 The graphical structures in our examples are based on geographical location of variables that impose dependence constraints based on spatial proximity. [sent-1473, score-0.071]
</p><p>79 (10) Models constructed by computing products of functions of the above type have the properties of both being heavy-tailed multivariate distributions and satisfying marginal independence constraints between variables that share no function nodes [8]. [sent-1509, score-0.247]
</p><p>80 Here we examined the data studied in [8], which consisted of spatial measurements for rainfall and for H1N1 mortality. [sent-1510, score-0.169]
</p><p>81 The rainfall dataset consists of 61 daily measurements of rainfall at 22 sites in China and the H1N1 dataset consists of 29 weekly mortality rates in 11 cities in the Northeastern US during the 2008-2009 epidemic. [sent-1511, score-0.351]
</p><p>82 The loopy CDN model was compared via leave-one-out cross-validation to non-loopy CDNs of [8] and disconnected CDNs corresponding to independence models. [sent-1514, score-0.171]
</p><p>83 To compare with other multivariate approaches for modelling heavy-tailed data, we also tested the following: â&circ;&trade; Gaussian bi-directed (BDG) and Markov (MRF) models with the same topology as the loopy CDNs for log-transformed data with đ? [sent-1515, score-0.255]
</p><p>84 &rsaquo;˝ = 0 only if there is no edge connecting variable nodes đ? [sent-1528, score-0.134]
</p><p>85 &rsaquo;˝ â&circ;&trade; Structured nonparanormal distributions [11], which use a Gaussian copula model, where the structure was speciďŹ ed by the same BDG and MRF graphs and estimation of the covariance was performed using the algorithms for Gaussian MRFs and BDGs on nonlinearly transformed data. [sent-1534, score-0.189]
</p><p>86 Although the nonparanormal alË&oelig; Ë&oelig; lows for structure learning as part of model ďŹ tting, for the sake of comparison the structure of the model was set to be same as those of the BDG and MRF models. [sent-1560, score-0.07]
</p><p>87 â&circ;&trade; The multivariate logistic CDF [13] that is heavy-tailed but does not model local dependencies. [sent-1561, score-0.101]
</p><p>88 Here we designed the BDG and MRF models to have the same graphical structure as the loopy CDN model such that all three model classes represent the same set of local dependencies even though the set of global dependencies is different for a BDG, MRF and CDN of the same connectivity. [sent-1562, score-0.299]
</p><p>89 Here, capturing the additional local dependencies and heavy-tailedness using loopy CDNs leads to signiďŹ cantly better ďŹ ts (đ? [sent-1565, score-0.214]
</p><p>90 To further explore the loopy CDN model, we can visualize the set of log-bivariate densities obtained from the loopy CDN model for the rainfall data in tandem with observed data (Figure 2(c)). [sent-1568, score-0.515]
</p><p>91 The marginal bivariate density for each pair of neighboring variables is obtained by taking limits of the learned multivariate CDF and differentiating the resulting bivariate CDF. [sent-1569, score-0.237]
</p><p>92 We can also examine the resulting models by comparing the mutual information (MI) between pairs of neighboring variables in the graphical models for the H1N1 dataset. [sent-1570, score-0.146]
</p><p>93 This is shown in Figure 2(d) in the form of undirected weighted graphs where edges are weighted proportional to the MI between the two variable nodes connected by that edge. [sent-1571, score-0.188]
</p><p>94 As can be seen, the loopy CDN model differs signiďŹ cantly from the nonparanormal and Gaussian BDGs for log-transformed data in the MI between pairs of variables (Figure 2(d)). [sent-1573, score-0.272]
</p><p>95 In contrast, this edge is largely missed by the nonparanormal and log-transformed Gaussian BDGs. [sent-1575, score-0.092]
</p><p>96 Future work could include learning the structure of compact probability models in the sense of graphs with bounded treewidth, with practical applications to other problem domains (e. [sent-1577, score-0.068]
</p><p>97 Finally, given the demonstrated value of adding dependency constraints to CDNs, further development of faster approximate algorithms for loopy CDNs will also be of practical value. [sent-1583, score-0.171]
</p><p>98 (2009) Cumulative distribution networks: Inference, estimation and applications of graphical models for cumulative distribution functions. [sent-1613, score-0.124]
</p><p>99 (2010) Maximum-likelihood learning of cumulative distribution functions on graphs. [sent-1622, score-0.081]
</p><p>100 (1988) Local computations with probabilities on graphical structures and their application to expert systems. [sent-1640, score-0.073]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cdn', 0.614), ('cdns', 0.503), ('jdiff', 0.251), ('cdf', 0.205), ('loopy', 0.171), ('rainfall', 0.154), ('differentiation', 0.145), ('junction', 0.133), ('mathematica', 0.098), ('bdg', 0.098), ('symbolic', 0.095), ('mixed', 0.094), ('messages', 0.086), ('derivatives', 0.074), ('nodes', 0.074), ('pdfs', 0.074), ('derivative', 0.071), ('nonparanormal', 0.07), ('dsp', 0.07), ('cumulative', 0.061), ('multivariate', 0.061), ('tree', 0.061), ('cdfs', 0.056), ('bdgs', 0.056), ('npn', 0.056), ('mrf', 0.055), ('node', 0.052), ('foreach', 0.05), ('graphs', 0.045), ('root', 0.043), ('climatology', 0.042), ('mvlogistic', 0.042), ('graphical', 0.04), ('inference', 0.04), ('mi', 0.04), ('copula', 0.038), ('likelihood', 0.035), ('computations', 0.033), ('message', 0.032), ('epidemiology', 0.032), ('variables', 0.031), ('recursively', 0.031), ('neighboring', 0.029), ('bivariate', 0.029), ('leaf', 0.029), ('gradient', 0.028), ('bidirected', 0.028), ('cities', 0.028), ('diamonds', 0.028), ('gbdg', 0.028), ('gdp', 0.028), ('edges', 0.027), ('product', 0.027), ('marginal', 0.026), ('pdf', 0.026), ('intersection', 0.025), ('subtree', 0.025), ('disc', 0.025), ('gmrf', 0.025), ('undirected', 0.023), ('gaussian', 0.023), ('models', 0.023), ('huang', 0.023), ('figures', 0.023), ('brute', 0.023), ('edge', 0.022), ('dependencies', 0.022), ('rooted', 0.022), ('local', 0.021), ('complexity', 0.021), ('subset', 0.02), ('functions', 0.02), ('tractably', 0.02), ('combinatorics', 0.02), ('jojic', 0.02), ('nonlinearly', 0.02), ('variable', 0.019), ('logistic', 0.019), ('neighbors', 0.019), ('connecting', 0.019), ('computing', 0.019), ('densities', 0.019), ('graphics', 0.018), ('broken', 0.018), ('separator', 0.018), ('limits', 0.017), ('trees', 0.016), ('distributions', 0.016), ('sequel', 0.016), ('supplemental', 0.016), ('subsets', 0.016), ('bipartite', 0.015), ('runtime', 0.015), ('measurements', 0.015), ('cpu', 0.015), ('differentiating', 0.015), ('joint', 0.015), ('loops', 0.015), ('series', 0.013), ('factors', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="84-tfidf-1" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>Author: Nebojsa Jojic, Chris Meek, Jim C. Huang</p><p>Abstract: Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (nonloopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide signiďŹ cantly better ďŹ ts to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p><p>2 0.093376845 <a title="84-tfidf-2" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>Author: Vibhav Gogate, William Webb, Pedro Domingos</p><p>Abstract: We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-speciﬁc independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efﬁcient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners. 1</p><p>3 0.060431767 <a title="84-tfidf-3" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>Author: Meritxell Vinyals, Jes\'us Cerquides, Alessandro Farinelli, Juan A. Rodríguez-aguilar</p><p>Abstract: We study worst-case bounds on the quality of any ﬁxed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start providing a bound independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with speciﬁc structures such as bipartite graphs or grids. Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% optimal) on MRFs with large variable-disjoint cycles1 . 1</p><p>4 0.059562054 <a title="84-tfidf-4" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>Author: Samy Bengio, Jason Weston, David Grangier</p><p>Abstract: Multi-class classiﬁcation becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classiﬁers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster. 1</p><p>5 0.059208624 <a title="84-tfidf-5" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>Author: David Weiss, Benjamin Sapp, Ben Taskar</p><p>Abstract: For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but ﬁlter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the ﬁltering loss of the ensemble as a theoretical justiﬁcation of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We ﬁnd that our approach signiﬁcantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem. 1</p><p>6 0.056572296 <a title="84-tfidf-6" href="./nips-2010-Copula_Bayesian_Networks.html">53 nips-2010-Copula Bayesian Networks</a></p>
<p>7 0.055655919 <a title="84-tfidf-7" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>8 0.053726096 <a title="84-tfidf-8" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>9 0.053322103 <a title="84-tfidf-9" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>10 0.052372873 <a title="84-tfidf-10" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>11 0.051783729 <a title="84-tfidf-11" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>12 0.051584873 <a title="84-tfidf-12" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>13 0.048081439 <a title="84-tfidf-13" href="./nips-2010-Copula_Processes.html">54 nips-2010-Copula Processes</a></p>
<p>14 0.043382652 <a title="84-tfidf-14" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>15 0.042767428 <a title="84-tfidf-15" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>16 0.040393602 <a title="84-tfidf-16" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>17 0.038766004 <a title="84-tfidf-17" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>18 0.036822427 <a title="84-tfidf-18" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>19 0.036438052 <a title="84-tfidf-19" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>20 0.036320601 <a title="84-tfidf-20" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.024), (2, 0.012), (3, 0.034), (4, -0.085), (5, -0.023), (6, -0.007), (7, 0.014), (8, 0.037), (9, 0.021), (10, -0.131), (11, -0.028), (12, -0.017), (13, -0.033), (14, -0.024), (15, -0.048), (16, -0.011), (17, -0.06), (18, -0.032), (19, -0.011), (20, -0.068), (21, -0.03), (22, -0.005), (23, -0.105), (24, -0.044), (25, -0.049), (26, 0.07), (27, 0.042), (28, 0.0), (29, -0.03), (30, -0.036), (31, 0.011), (32, 0.038), (33, 0.002), (34, 0.032), (35, -0.038), (36, -0.007), (37, -0.003), (38, -0.008), (39, 0.001), (40, 0.083), (41, 0.061), (42, 0.037), (43, 0.115), (44, -0.012), (45, -0.028), (46, -0.016), (47, -0.087), (48, 0.077), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88386089 <a title="84-lsi-1" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>Author: Nebojsa Jojic, Chris Meek, Jim C. Huang</p><p>Abstract: Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (nonloopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide signiďŹ cantly better ďŹ ts to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p><p>2 0.67780602 <a title="84-lsi-2" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>Author: Daniel Lowd, Pedro Domingos</p><p>Abstract: Arithmetic circuits (ACs) exploit context-speciﬁc independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the ﬁrst ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we ﬁnd that the methods based on sampling and learning work best: one such method (AC2 -F) is faster and usually more accurate than loopy belief propagation, mean ﬁeld, and Gibbs sampling; another (AC2 -G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines. 1</p><p>3 0.5923841 <a title="84-lsi-3" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>Author: Danny Bickson, Carlos Guestrin</p><p>Abstract: Heavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy-tailed distributions. In this work, we propose a novel simple linear graphical model for independent latent random variables, called linear characteristic model (LCM), defined in the characteristic function domain. Using stable distributions, a heavy-tailed family of distributions which is a generalization of Cauchy, L´ vy and Gaussian distrie butions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model. LCMs are not limited to stable distributions, in fact LCMs are always defined for any random variables (discrete, continuous or a mixture of both). We provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction. Other potential application is iterative decoding of linear channels with non-Gaussian noise. 1</p><p>4 0.58564109 <a title="84-lsi-4" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>Author: Vibhav Gogate, William Webb, Pedro Domingos</p><p>Abstract: We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-speciﬁc independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efﬁcient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners. 1</p><p>5 0.56910688 <a title="84-lsi-5" href="./nips-2010-Copula_Bayesian_Networks.html">53 nips-2010-Copula Bayesian Networks</a></p>
<p>Author: Gal Elidan</p><p>Abstract: We present the Copula Bayesian Network model for representing multivariate continuous distributions, while taking advantage of the relative ease of estimating univariate distributions. Using a novel copula-based reparameterization of a conditional density, joined with a graph that encodes independencies, our model offers great ﬂexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals. We demonstrate the advantage of our framework for generalization over standard Bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature. 1</p><p>6 0.50275838 <a title="84-lsi-6" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>7 0.48607171 <a title="84-lsi-7" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>8 0.46083006 <a title="84-lsi-8" href="./nips-2010-Copula_Processes.html">54 nips-2010-Copula Processes</a></p>
<p>9 0.44801116 <a title="84-lsi-9" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>10 0.43818456 <a title="84-lsi-10" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>11 0.43530136 <a title="84-lsi-11" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>12 0.42284471 <a title="84-lsi-12" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>13 0.40643701 <a title="84-lsi-13" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>14 0.39030054 <a title="84-lsi-14" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>15 0.38895774 <a title="84-lsi-15" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>16 0.38756919 <a title="84-lsi-16" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>17 0.38426399 <a title="84-lsi-17" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>18 0.37491742 <a title="84-lsi-18" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>19 0.37196857 <a title="84-lsi-19" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>20 0.37133175 <a title="84-lsi-20" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.039), (17, 0.032), (27, 0.036), (30, 0.077), (44, 0.333), (45, 0.177), (50, 0.075), (52, 0.041), (60, 0.03), (72, 0.012), (77, 0.02), (90, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68690127 <a title="84-lda-1" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>Author: Nebojsa Jojic, Chris Meek, Jim C. Huang</p><p>Abstract: Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (nonloopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide signiďŹ cantly better ďŹ ts to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p><p>2 0.67409819 <a title="84-lda-2" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>3 0.66053921 <a title="84-lda-3" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>4 0.53717029 <a title="84-lda-4" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>5 0.53591025 <a title="84-lda-5" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>Author: Matthias Broecheler, Lise Getoor</p><p>Abstract: Continuous Markov random ﬁelds are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random ﬁeld. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efﬁciency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classiﬁcation, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of conﬁdence. 1</p><p>6 0.53454232 <a title="84-lda-6" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>7 0.53451979 <a title="84-lda-7" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>8 0.53380972 <a title="84-lda-8" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>9 0.53223282 <a title="84-lda-9" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>10 0.53183705 <a title="84-lda-10" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>11 0.53155392 <a title="84-lda-11" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>12 0.53136259 <a title="84-lda-12" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>13 0.53128874 <a title="84-lda-13" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>14 0.53053993 <a title="84-lda-14" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>15 0.53051877 <a title="84-lda-15" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>16 0.53027201 <a title="84-lda-16" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>17 0.52975613 <a title="84-lda-17" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>18 0.52965724 <a title="84-lda-18" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>19 0.52918071 <a title="84-lda-19" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>20 0.52912688 <a title="84-lda-20" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
