<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-199" href="#">nips2010-199</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</h1>
<br/><p>Source: <a title="nips-2010-199-pdf" href="http://papers.nips.cc/paper/4077-optimal-learning-rates-for-kernel-conjugate-gradient-regression.pdf">pdf</a></p><p>Author: Gilles Blanchard, Nicole Krämer</p><p>Abstract: We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overﬁtting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: ﬁrst, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulﬁlled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators. 1</p><p>Reference: <a title="nips-2010-199-reference" href="../nips2010_reference/nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Optimal learning rates for Kernel Conjugate Gradient regression  Nicole Kr¨ mer a Weierstrass Institute Mohrenstr. [sent-1, score-0.199]
</p><p>2 de  Abstract We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overﬁtting is obtained by early stopping. [sent-6, score-0.717]
</p><p>3 This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. [sent-7, score-0.21]
</p><p>4 The rates depend on two key quantities: ﬁrst, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. [sent-8, score-0.577]
</p><p>5 Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. [sent-9, score-0.576]
</p><p>6 If this assumption is not fulﬁlled, we obtain similar convergence rates provided additional unlabeled data are available. [sent-10, score-0.264]
</p><p>7 The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators. [sent-11, score-0.417]
</p><p>8 1  Introduction  The contribution of this paper is the learning theoretical analysis of kernel-based least squares regression in combination with conjugate gradient techniques. [sent-12, score-0.371]
</p><p>9 Following the kernelization principle, we implicitly map the data into a reproducing 1 kernel Hilbert space H with a kernel k. [sent-19, score-0.387]
</p><p>10 We denote by Kn = n (k(Xi , Xj )) ∈ Rn×n the normalized n kernel matrix and by Υ = (Y1 , . [sent-20, score-0.165]
</p><p>11 The task is to ﬁnd coefﬁcients α such that the function deﬁned by the normalized kernel expansion fα (X)  =  1 n  n  αi k(Xi , X) i=1  is an adequate estimator of the true regression function f ∗ . [sent-24, score-0.254]
</p><p>12 It is well-known that to avoid overﬁtting, some form of regularization is needed. [sent-29, score-0.11]
</p><p>13 Perhaps the most well-known one is  α = (Kn + λI)−1 Υ,  (2)  known alternatively as kernel ridge regression, Tikhonov’s regularization, least squares support vector machine, or MAP Gaussian process regression. [sent-33, score-0.357]
</p><p>14 This type of regularization, which we refer to as linear regularization methods, is directly inspired from the theory of inverse problems. [sent-37, score-0.145]
</p><p>15 Popular examples include as particular cases kernel ridge regression, principal components regression and L2 -boosting. [sent-38, score-0.249]
</p><p>16 In this paper, we study conjugate gradient (CG) techniques in combination with early stopping for the regularization of the kernel based learning problem (1). [sent-41, score-0.725]
</p><p>17 In the learning context considered here, regularization corresponds to early stopping. [sent-52, score-0.155]
</p><p>18 Algorithm 1 displays the computation of the CG kernel coefﬁcients αm deﬁned by (5). [sent-57, score-0.165]
</p><p>19 Algorithm 1 Kernel Conjugate Gradient regression Input kernel matrix Kn , response vector Υ, maximum number of iterations m Initialization: α0 = 0n ; r1 = Υ; d1 = Υ; t1 = Kn Υ for i = 1, . [sent-58, score-0.216]
</p><p>20 , m do ti = ti / ti Kn ; di = di / ti Kn (normalization of the basis, resp. [sent-61, score-0.354]
</p><p>21 of Υ on basis vector) αi = αi−1 + γi di (update) ri+1 = ri − γi ti (residuals) di+1 = ri+1 − di ti , Kn ri+1 Kn ; ti+1 = Kn di+1 (new update, resp. [sent-63, score-0.252]
</p><p>22 However, the polynomial qm is not ﬁxed but depends on Υ as well, making the CG method nonlinear in the sense that the coefﬁcients αm depend on Υ in a nonlinear fashion. [sent-65, score-0.206]
</p><p>23 2  We remark that in machine learning, conjugate gradient techniques are often used as fast solvers for operator equations, e. [sent-66, score-0.303]
</p><p>24 We stress that in this paper, we study conjugate gradients as a regularization approach for kernel based learning, where the regularity is ensured via early stopping. [sent-69, score-0.559]
</p><p>25 Moreover, a similar conjugate gradient approach for non-deﬁnite kernels has been proposed and empirically evaluated by Ong et al [17]. [sent-75, score-0.161]
</p><p>26 In particular, we establish the existence of early stopping rules that lead to optimal convergence rates. [sent-77, score-0.362]
</p><p>27 We ﬁrst assume that the kernel space H is separable and that the kernel function is measurable. [sent-80, score-0.33]
</p><p>28 ) Furthermore, for all results, we make the (relatively standard) assumption that the kernel is bounded: k(x, x) ≤ κ for all x ∈ X . [sent-82, score-0.191]
</p><p>29 In particular, the ﬁrst assumption implies that not only the noise, but also the target function f ∗ is bounded in supremum norm, while the second assumption does not put any additional restriction on the target function. [sent-87, score-0.295]
</p><p>30 The regularity of the target function f ∗ is measured in terms of a source condition as follows. [sent-88, score-0.251]
</p><p>31 The kernel integral operator is given by K : L2 (PX ) → L2 (PX ), g →  k(. [sent-89, score-0.351]
</p><p>32 The regularity of the kernel operator K with respect to the marginal distribution PX is measured in terms of the so-called effective dimensionality condition, deﬁned by the two parameters s ∈ (0, 1), D ≥ 0 and the condition ED(s, D) : tr(K(K + λI)−1 ) ≤ D2 (κ−1 λ)−s for all λ ∈ (0, 1]. [sent-94, score-0.474]
</p><p>33 It is known that the best attainable rates of convergence, as a function of the number of examples n, are determined by the parameters r and s in the above conditions: It was shown in [6] that the minimax learning rate given these two parameters is lower bounded by O(n−2r/(2r+s) ). [sent-96, score-0.229]
</p><p>34 It is not difﬁcult to prove from (4) and (5) that Υ − Kn αn Kn = 0, so that the above type of stopping rule always has m ≤ n. [sent-104, score-0.337]
</p><p>35 1  Inner case without knowledge on effective dimension  The inner case corresponds to r ≥ 1/2, i. [sent-106, score-0.092]
</p><p>36 For some constants τ > 1 and 1 > γ > 0, we consider the discrepancy stopping rule with the threshold sequence κ log(2γ −1 ) √ Λm = 4τ κ αm Kn + M log(2γ −1 ) . [sent-109, score-0.432]
</p><p>37 (7) n For technical reasons, we consider a slight variation of the rule in that we stop at step m−1 instead of m if qm (0) ≥ 4κ log(2γ −1 )/n, where qm is the iteration polynomial such that αm = qm (Kn )Υ. [sent-110, score-0.525]
</p><p>38 Suppose that Y is bounded (Bounded), and that the source condition SC(r, ρ) holds for r ≥ 1/2. [sent-115, score-0.119]
</p><p>39 With probability 1 − 2γ , the estimator fm obtained by the (modiﬁed) discrepancy e stopping rule (7) satisﬁes fm − e  2 f∗ 2  ≤ c(r, τ )(M + ρ)  2r 2r+1  log2 γ −1 n  2  . [sent-116, score-0.816]
</p><p>40 2  Optimal rates in inner case  We now introduce a stopping rule yielding order-optimal convergence rates as a function of the two parameters r and s in the “inner” case (r ≥ 1/2, which is equivalent to saying that the target function belongs to H almost surely). [sent-119, score-0.868]
</p><p>41 For some constant τ > 3/2 and 1 > γ > 0, we consider the discrepancy stopping rule with the ﬁxed threshold √  Λm ≡ Λ = τ M κ  6 4D √ log γ n  2r+1 2r+s  . [sent-120, score-0.432]
</p><p>42 Suppose that the noise fulﬁlls the Bernstein assumption (Bernstein), that the source condition SC(r, ρ) holds for r ≥ 1/2, and that ED(s, D) holds. [sent-123, score-0.098]
</p><p>43 With probability 1 − 3γ , the estimator fm obtained by the discrepancy stopping rule (8) satisﬁes b fm − b  2 f∗ 2  2  ≤ c(r, τ )(M + ρ)  16D2 6 log2 n γ  2r 2r+s  . [sent-124, score-0.816]
</p><p>44 3  Optimal rates in outer case, given additional unlabeled data  We now turn to the “outer” case. [sent-127, score-0.244]
</p><p>45 We use the same threshold (8) as in the previous section for the ˜ stopping rule, except that the factor M is replaced by max(M, ρ). [sent-146, score-0.269]
</p><p>46 Then with probability 1 − 3γ , the estimator fm obtained by the discrepancy stopping rule deﬁned b above satisﬁes fm − f ∗ b  2 2  ≤ c(r, τ )(M + ρ)2  16D2 6 log2 n γ  2r 2r+s  . [sent-151, score-0.816]
</p><p>47 f ∗ ∈ H almost surely – we provide two different consistent stopping criteria. [sent-155, score-0.318]
</p><p>48 However, an interesting feature of stopping rule (7) is that the rule itself does not depend on the a priori knowledge of the regularity parameter r, while the achieved learning rate does (and with the optimal dependence in r when s = 1). [sent-158, score-0.556]
</p><p>49 1 implies that the obtained rule is automatically adaptive with respect to the regularity of the target function. [sent-160, score-0.303]
</p><p>50 This contrasts with the results obtained in [1] for linear regularization schemes of the form (3), (also in the case s = 1) for which the choice of the regularization parameter λ leading to optimal learning rates required the knowledge or r beforehand. [sent-161, score-0.414]
</p><p>51 2 provides the order-optimal convergence rate in the inner case (up to a log factor). [sent-163, score-0.101]
</p><p>52 1 however, is that the stopping rule is no longer adaptive, that is, it depends on the a priori knowledge of parameters r and s. [sent-165, score-0.37]
</p><p>53 We observe that previously obtained results for linear regularization schemes of the form (2) in [6] and of the form (3) in [5], also rely on the a priori knowledge of r and s to determine the appropriate regularization parameter λ. [sent-166, score-0.299]
</p><p>54 The outer case – when the target function does not lie in the reproducing Kernel Hilbert space H – is more challenging and to some extent less well understood. [sent-167, score-0.192]
</p><p>55 The fact that additional assumptions are made is not a particular artefact of CG methods, but also appears in the studies of other regularization techniques. [sent-168, score-0.133]
</p><p>56 [5] (to study linear regularization of the form (3)) and assume that we have sufﬁcient additional unlabeled data in order to ensure learning rates that are optimal as a function of the number of labeled data. [sent-171, score-0.305]
</p><p>57 For regularized M-estimation schemes studied in [20], availability of unlabeled data is not p 1−p required, but a condition is imposed of the form f ∞ ≤ C f H f 2 for all f ∈ H and some p ∈ (0, 1]. [sent-173, score-0.157]
</p><p>58 In [13], assumptions on the supremum norm of the eigenfunctions of the kernel integral operator are made (see [20] for an in-depth discussion on this type of assumptions). [sent-174, score-0.43]
</p><p>59 In the context of learning, our approach is most closely linked to Partial Least Squares (PLS) [21] and its kernel extension [18]. [sent-176, score-0.165]
</p><p>60 In [8, 14], consistency properties are provided for linear PLS under the assumption that the target function f ∗ depends on a ﬁnite known number of orthogonal latent components. [sent-178, score-0.112]
</p><p>61 These ﬁndings were recently extended to the nonlinear case and without the assumption of a latent components model [3], but all results come without optimal rates of convergence. [sent-179, score-0.197]
</p><p>62 For the slightly different CG approach studied by Ong et al [17], bounds on the difference between the empirical risks of the CG approximation and of the target function are derived in [16], but no bounds on the generalization error were derived. [sent-180, score-0.136]
</p><p>63 4  Proofs  Convergence rates for regularization methods of the type (2) or (3) have been studied by casting kernel learning methods into the framework of inverse problems (see [9]). [sent-181, score-0.458]
</p><p>64 5  We ﬁrst deﬁne the empirical evaluation operator Tn as follows: g ∈ H → Tn g := (g(X1 ), . [sent-183, score-0.142]
</p><p>65 , g(Xn )) ∈ Rn  Tn :  ∗ and the empirical integral operator Tn as: ∗ ∗ Tn : u = (u1 , . [sent-186, score-0.186]
</p><p>66 i=1  ∗ Using the reproducing property of the kernel, it can be readily checked that Tn and Tn are adjoint ∗ n operators, i. [sent-190, score-0.086]
</p><p>67 Based on these facts, equation (5) can be rewritten as  fm = arg  min  ∗ f ∈Km (Tn Υ,Sn )  ∗ Tn Y − Sn f  H  ,  (9)  ∗ where Sn = Tn Tn is a self-adjoint operator of H, called empirical covariance operator. [sent-194, score-0.339]
</p><p>68 This deﬁnition corresponds to that of the “usual” conjugate gradient algorithm formally applied to the so-called normal equation (in H) ∗ Sn fα = Tn Υ , ∗ which is obtained from (1) by left multiplication by Tn . [sent-195, score-0.185]
</p><p>69 , x)g(x)dPX (x) = E [k(X, ·)g(X)] ∈ H ,  and the change-of-space operator T :  g ∈ H → g ∈ L2 (PX ) . [sent-197, score-0.142]
</p><p>70 (11) P Tn Y − SfH ≤ √ γ n ∗ ∗ We note that f ∗ = T fH implies that the target function f ∗ coincides with a function fH belonging to H (remember that T is just the change-of-space operator). [sent-206, score-0.086]
</p><p>71 1  Nemirovskii’s result on conjugate gradient regularization rates  We recall a sharp result due to Nemirovskii [15] establishing convergence rates for conjugate gradient methods in a deterministic context. [sent-209, score-0.771]
</p><p>72 Consider the linear equation Az ∗ = b , 6  where A is a bounded linear operator over a Hilbert space H . [sent-212, score-0.213]
</p><p>73 Assume that the above equation has a ¯ solution and denote z ∗ its minimal norm solution; assume further that a self-adjoint operator A, and an element ¯ ∈ H are known such that b ¯ A−A ≤ δ; b −¯ ≤ ε, b (12) ¯ (with δ and ε known positive numbers). [sent-213, score-0.198]
</p><p>74 Consider the CG algorithm based on the noisy operator A ¯ giving the output at step m and data b, ¯ zm = Arg Min Az − ¯ b  2  . [sent-214, score-0.22]
</p><p>75 (13)  ¯ b) z∈Km (A,¯  The discrepancy principle stopping rule is deﬁned as follows. [sent-215, score-0.456]
</p><p>76 Consider a ﬁxed constant τ > 1 and deﬁne ¯ b m = min m ≥ 0 : Azm − ¯ < τ (δ zm + ε) . [sent-216, score-0.078]
</p><p>77 Consider a minor variation of of this rule: ¯ m ¯ max(0, m − 1) ¯  m=  if qm (0) < ηδ −1 ¯ otherwise,  ¯b where qm is the degree m − 1 polynomial such that zm = qm (A)¯ , and η is an arbitrary positive ¯ ¯ ¯ constant such that η < 1/τ . [sent-218, score-0.51]
</p><p>78 1  1 We apply Nemirovskii’s result in our setting (assuming r ≥ 2 ): By identifying the approximate ∗ ¯ operator and data as A = Sn and ¯ = Tn Y, we see that the CG algorithm considered by Nemirovskii b (13) is exactly (9), more precisely with the identiﬁcation zm = fm . [sent-225, score-0.393]
</p><p>79 The operator norm is upper bounded by the Hilbert-Schmidt norm, so that the deviation inequality for the operators is actually stronger than what is needed. [sent-233, score-0.268]
</p><p>80 We consider the discrepancy principle stopping rule associated to these parameters, the choice η = 1 1/(2τ ), and θ = 2 , thus obtaining the result, since 1  A 2 (zm − z ∗ ) b 4. [sent-234, score-0.456]
</p><p>81 3  2  1  ∗ = S 2 (fm − fH ) b  2 H  ∗ = fm − fH b  2 2  . [sent-235, score-0.173]
</p><p>82 3  The above proof shows that an application of Nemirovskii’s fundamental result for CG regularization of inverse problems under deterministic noise (on the data and the operator) allows us to obtain our ﬁrst result. [sent-238, score-0.176]
</p><p>83 1 is too coarse to prove the optimal rates of convergence taking into account the effective dimension 7  parameter. [sent-247, score-0.225]
</p><p>84 of the form (S + λI)− 2 (Tn Y − T ∗ f ∗ ) for the data, and (S + λI)− 2 (Sn − S) HS for the operator (with an appropriate choice of λ > 0) respectively. [sent-250, score-0.142]
</p><p>85 Deviations of this form were introduced and used in [5, 6] to obtain sharp rates in the framework of Tikhonov’s regularization (2) and of the more general linear regularization schemes of the form (3). [sent-251, score-0.414]
</p><p>86 Bounds on deviations of this form can be obtained via a Bernstein-type concentration inequality for Hilbert-space valued random variables. [sent-252, score-0.075]
</p><p>87 On the one hand, the results concerning linear regularization schemes of the form (3) do not apply to the nonlinear CG regularization. [sent-253, score-0.179]
</p><p>88 5  Conclusion  In this work, we derived early stopping rules for kernel Conjugate Gradient regression that provide optimal learning rates to the true target function. [sent-262, score-0.769]
</p><p>89 Depending on the situation that we study, the rates are adaptive with respect to the regularity of the target function in some cases. [sent-263, score-0.358]
</p><p>90 We also note that theoretically well-grounded model selection rules can generally help cross-validation in practice by providing a well-calibrated parametrization of regularizer functions, or, as is the case here, of thresholds used in the stopping rule. [sent-267, score-0.274]
</p><p>91 One crucial property used in the proofs is that the proposed CG regularization schemes can be conveniently cast in the reproducing kernel Hilbert space H as displayed in e. [sent-268, score-0.406]
</p><p>92 This point is the main technical justiﬁcation on why we focus on (5) rather than kernel PLS. [sent-271, score-0.165]
</p><p>93 Obtaining optimal convergence rates also valid for Kernel PLS is an important future direction and should build on the present work. [sent-272, score-0.191]
</p><p>94 Another important direction for future efforts is the derivation of stopping rules that do not depend on the conﬁdence parameter γ. [sent-273, score-0.274]
</p><p>95 Currently, this dependence prevents us to go from convergence in high probability to convergence in expectation, which would be desirable. [sent-274, score-0.086]
</p><p>96 Perhaps more importantly, it would be of interest to ﬁnd a stopping rule that is adaptive to both parameters r (target function regularity) and s (effective dimension parameter) without their a priori knowledge. [sent-275, score-0.401]
</p><p>97 We recall that our ﬁrst stopping rule is adaptive to r but at the price of being worst-case in s. [sent-276, score-0.368]
</p><p>98 In the literature on linear regularization methods, the optimal choice of regularization parameter is also non-adaptive, be it when considering optimal rates with respect to r only [1] or to both r and s [5]. [sent-277, score-0.368]
</p><p>99 An approach to alleviate this problem is to use a hold-out sample for model selection; this was studied theoretically in [7] for linear regularization methods (see also [4] for an account of the properties of hold-out in a general setup). [sent-278, score-0.11]
</p><p>100 Learning bounds for kernel regression using effective data dimensionality. [sent-413, score-0.275]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kn', 0.522), ('cg', 0.363), ('tn', 0.269), ('stopping', 0.244), ('nemirovskii', 0.211), ('fm', 0.173), ('kernel', 0.165), ('rates', 0.148), ('operator', 0.142), ('qm', 0.136), ('squares', 0.126), ('conjugate', 0.121), ('fh', 0.111), ('px', 0.11), ('regularization', 0.11), ('discrepancy', 0.095), ('rule', 0.093), ('regularity', 0.093), ('km', 0.088), ('target', 0.086), ('pls', 0.085), ('zm', 0.078), ('bernstein', 0.076), ('blanchard', 0.068), ('sn', 0.066), ('ti', 0.066), ('sc', 0.065), ('krylov', 0.064), ('inner', 0.058), ('hilbert', 0.057), ('reproducing', 0.057), ('partial', 0.053), ('regression', 0.051), ('caponnetto', 0.051), ('outer', 0.049), ('surely', 0.049), ('operators', 0.047), ('bounded', 0.047), ('unlabeled', 0.047), ('schemes', 0.046), ('di', 0.045), ('early', 0.045), ('integral', 0.044), ('ong', 0.043), ('convergence', 0.043), ('deviations', 0.041), ('hs', 0.04), ('gradient', 0.04), ('condition', 0.04), ('estimator', 0.038), ('rosipal', 0.037), ('exy', 0.037), ('vito', 0.037), ('wold', 0.037), ('inverse', 0.035), ('potsdam', 0.034), ('az', 0.034), ('attainable', 0.034), ('effective', 0.034), ('concentration', 0.034), ('ridge', 0.033), ('priori', 0.033), ('least', 0.033), ('norm', 0.032), ('source', 0.032), ('theorem', 0.032), ('rosasco', 0.032), ('kr', 0.032), ('population', 0.032), ('proof', 0.031), ('adaptive', 0.031), ('rules', 0.03), ('ri', 0.03), ('cients', 0.029), ('adjoint', 0.029), ('proofs', 0.028), ('tikhonov', 0.028), ('warped', 0.028), ('satis', 0.028), ('noiseless', 0.027), ('assumption', 0.026), ('almost', 0.025), ('remember', 0.025), ('ful', 0.025), ('stress', 0.025), ('bounds', 0.025), ('replaced', 0.025), ('supremum', 0.024), ('principle', 0.024), ('polynomial', 0.024), ('coef', 0.024), ('regularized', 0.024), ('rn', 0.024), ('equation', 0.024), ('ideas', 0.023), ('depending', 0.023), ('nonlinear', 0.023), ('assumptions', 0.023), ('belongs', 0.023), ('subspaces', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="199-tfidf-1" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>Author: Gilles Blanchard, Nicole Krämer</p><p>Abstract: We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overﬁtting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: ﬁrst, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulﬁlled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators. 1</p><p>2 0.24321038 <a title="199-tfidf-2" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>3 0.15058097 <a title="199-tfidf-3" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>Author: Noah A. Smith, Shay B. Cohen</p><p>Abstract: Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a ﬁxed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting. 1</p><p>4 0.10735481 <a title="199-tfidf-4" href="./nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">278 nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<p>Author: Tobias Glasmachers</p><p>Abstract: Steinwart was the ﬁrst to prove universal consistency of support vector machine classiﬁcation. His proof analyzed the ‘standard’ support vector machine classiﬁer, which is restricted to binary classiﬁcation problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classiﬁcation to more than two classes are inconsistent. Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart’s techniques to the multi-class case. 1</p><p>5 0.098167263 <a title="199-tfidf-5" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>6 0.096356958 <a title="199-tfidf-6" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>7 0.088697784 <a title="199-tfidf-7" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>8 0.085635446 <a title="199-tfidf-8" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>9 0.083642751 <a title="199-tfidf-9" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>10 0.082841769 <a title="199-tfidf-10" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>11 0.081317134 <a title="199-tfidf-11" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>12 0.081245624 <a title="199-tfidf-12" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>13 0.080051899 <a title="199-tfidf-13" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>14 0.076683365 <a title="199-tfidf-14" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>15 0.075746141 <a title="199-tfidf-15" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>16 0.074390925 <a title="199-tfidf-16" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>17 0.073681213 <a title="199-tfidf-17" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>18 0.071576245 <a title="199-tfidf-18" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>19 0.06899368 <a title="199-tfidf-19" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>20 0.068513364 <a title="199-tfidf-20" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, 0.027), (2, 0.088), (3, 0.034), (4, 0.157), (5, 0.069), (6, 0.133), (7, -0.003), (8, 0.038), (9, 0.019), (10, -0.019), (11, -0.035), (12, -0.126), (13, -0.056), (14, -0.038), (15, 0.008), (16, 0.101), (17, 0.093), (18, 0.012), (19, 0.005), (20, -0.057), (21, -0.017), (22, -0.003), (23, 0.009), (24, 0.165), (25, 0.016), (26, -0.067), (27, 0.025), (28, -0.06), (29, -0.006), (30, -0.029), (31, -0.012), (32, -0.046), (33, 0.139), (34, -0.013), (35, 0.181), (36, -0.142), (37, 0.051), (38, 0.068), (39, 0.014), (40, 0.005), (41, 0.08), (42, 0.043), (43, -0.035), (44, -0.115), (45, 0.059), (46, -0.101), (47, -0.145), (48, 0.063), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93065566 <a title="199-lsi-1" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>Author: Gilles Blanchard, Nicole Krämer</p><p>Abstract: We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overﬁtting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: ﬁrst, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulﬁlled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators. 1</p><p>2 0.78656769 <a title="199-lsi-2" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>3 0.72438502 <a title="199-lsi-3" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>Author: Noah A. Smith, Shay B. Cohen</p><p>Abstract: Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a ﬁxed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting. 1</p><p>4 0.59631735 <a title="199-lsi-4" href="./nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">278 nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<p>Author: Tobias Glasmachers</p><p>Abstract: Steinwart was the ﬁrst to prove universal consistency of support vector machine classiﬁcation. His proof analyzed the ‘standard’ support vector machine classiﬁer, which is restricted to binary classiﬁcation problems. In contrast, recent analysis has resulted in the common belief that several extensions of SVM classiﬁcation to more than two classes are inconsistent. Countering this belief, we prove the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart’s techniques to the multi-class case. 1</p><p>5 0.56923002 <a title="199-lsi-5" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of Rd . Examples include SVMs for the analysis of histograms or colored images, SVMs for text classiÄ?Ĺš cation and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1</p><p>6 0.4887552 <a title="199-lsi-6" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>7 0.4621124 <a title="199-lsi-7" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>8 0.4422752 <a title="199-lsi-8" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>9 0.42171863 <a title="199-lsi-9" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>10 0.40139526 <a title="199-lsi-10" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>11 0.39631766 <a title="199-lsi-11" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>12 0.39124939 <a title="199-lsi-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.3908869 <a title="199-lsi-13" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>14 0.38980621 <a title="199-lsi-14" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>15 0.38603243 <a title="199-lsi-15" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>16 0.36913151 <a title="199-lsi-16" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>17 0.36711758 <a title="199-lsi-17" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>18 0.36208999 <a title="199-lsi-18" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>19 0.36060363 <a title="199-lsi-19" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>20 0.35784644 <a title="199-lsi-20" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.046), (27, 0.05), (30, 0.078), (35, 0.031), (38, 0.205), (45, 0.189), (50, 0.062), (52, 0.032), (60, 0.059), (77, 0.035), (78, 0.027), (90, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82365698 <a title="199-lda-1" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>Author: Gilles Blanchard, Nicole Krämer</p><p>Abstract: We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overﬁtting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: ﬁrst, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if the true regression function belongs to the reproducing kernel Hilbert space. If this assumption is not fulﬁlled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates match state-of-the-art results that were recently obtained for least squares support vector machines and for linear regularization operators. 1</p><p>2 0.77406102 <a title="199-lda-2" href="./nips-2010-Probabilistic_latent_variable_models_for_distinguishing_between_cause_and_effect.html">218 nips-2010-Probabilistic latent variable models for distinguishing between cause and effect</a></p>
<p>Author: Oliver Stegle, Dominik Janzing, Kun Zhang, Joris M. Mooij, Bernhard Schölkopf</p><p>Abstract: We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y . The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results. 1</p><p>3 0.75822306 <a title="199-lda-3" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>4 0.75707257 <a title="199-lda-4" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>5 0.7434985 <a title="199-lda-5" href="./nips-2010-Permutation_Complexity_Bound_on_Out-Sample_Error.html">205 nips-2010-Permutation Complexity Bound on Out-Sample Error</a></p>
<p>Author: Malik Magdon-Ismail</p><p>Abstract: We deﬁne a data dependent permutation complexity for a hypothesis set H, which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based (like the maximum discrepancy) on dependent sampling. We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efﬁciently estimated.</p><p>6 0.73991305 <a title="199-lda-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.73928964 <a title="199-lda-7" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>8 0.73654902 <a title="199-lda-8" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>9 0.7355839 <a title="199-lda-9" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>10 0.73460817 <a title="199-lda-10" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>11 0.73385322 <a title="199-lda-11" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>12 0.73377877 <a title="199-lda-12" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>13 0.73282504 <a title="199-lda-13" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>14 0.73259032 <a title="199-lda-14" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>15 0.73174673 <a title="199-lda-15" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>16 0.72982317 <a title="199-lda-16" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>17 0.72655874 <a title="199-lda-17" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>18 0.72651833 <a title="199-lda-18" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>19 0.72641945 <a title="199-lda-19" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>20 0.72591192 <a title="199-lda-20" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
