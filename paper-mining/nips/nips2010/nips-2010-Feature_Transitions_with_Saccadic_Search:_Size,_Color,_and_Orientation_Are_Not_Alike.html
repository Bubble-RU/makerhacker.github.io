<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-95" href="#">nips2010-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</h1>
<br/><p>Source: <a title="nips-2010-95-pdf" href="http://papers.nips.cc/paper/4112-feature-transitions-with-saccadic-search-size-color-and-orientation-are-not-alike.pdf">pdf</a></p><p>Author: Stella X. Yu</p><p>Abstract: Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation. 1</p><p>Reference: <a title="nips-2010-95-reference" href="../nips2010_reference/nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. [sent-4, score-0.482]
</p><p>2 If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. [sent-5, score-0.227]
</p><p>3 We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. [sent-6, score-0.75]
</p><p>4 1  Introduction  Size, color, and orientation have long been considered elementary features [14] that are available to guide attention and visual search [17]. [sent-8, score-0.529]
</p><p>5 While size, color, and orientation are alike at parallel local detections across space, they may not be alike at serial deployment of attention across time. [sent-12, score-0.669]
</p><p>6 We investigate this issue in a gaze-tracked visual search experiment which often requires multiple saccades for the subject to locate the target (Fig. [sent-13, score-0.347]
</p><p>7 disk1  disk2  disk1  disk1  disk2  disk1  disk2  disk1  disk2  disk2  disk2  disk1  disk1  disk1  disk2  disk2  disk2  disk2  disk2  disk2  disk1  disk1  disk1  disk1  Figure 1: Two kinds of disks are uniformly randomly distributed in a ﬁxed regular layout. [sent-15, score-0.491]
</p><p>8 Only one disk changes its kind during a repeated ﬂickering presentation. [sent-16, score-0.407]
</p><p>9 For the same size of change, does it matter to visual search whether the two kinds of disks are rendered in size, color, or orientation? [sent-17, score-0.861]
</p><p>10 1  localization mouse click detection mouse click  ﬂicker stimulus 120ms per frame ﬁxation 1000ms Figure 2: Each trial goes through ﬁxation, stimulus, detection, and localization stages. [sent-18, score-0.633]
</p><p>11 A ﬁxation dot is displayed for 1 second before the onset of the ﬂicker stimulus, with disk image 1, blank, disk image 2, blank repeatedly presented for 120ms each. [sent-19, score-0.992]
</p><p>12 The subject issues a mouse click as soon as he detects the change, and the the last seen disk image remains on till he clicks the disk of change. [sent-20, score-1.028]
</p><p>13 We present two kinds of disks in a ﬁxed regular layout in a ﬂicker paradigm, and the subject’s task is to locate the only disk that changes its kind (Fig. [sent-22, score-1.073]
</p><p>14 If the magnitude of change is comparable across feature dimensions, does it matter whether the disks are rendered in size, color, or orientation? [sent-26, score-0.749]
</p><p>15 That is, does visual search vary according to whether the same array of disks are: 1) small and large, 2) black and white, or 3) horizontal and vertical disks? [sent-27, score-0.743]
</p><p>16 If size, color, and orientation are processed in the same fashion with dedicated local detectors operating in parallel across space, then the detector responses are identical spatially at any time instance in the 3 scenarios. [sent-28, score-0.351]
</p><p>17 deciding what disks to look at next and how to look, depend on which ﬁlters produce these responses. [sent-31, score-0.463]
</p><p>18 Note that our stimuli decouple the target of feature search from visual saliency in the space. [sent-32, score-0.306]
</p><p>19 Our target is deﬁned not by one of the attributes as done in static search displays [14, 17], but by the temporal change of the attribute. [sent-33, score-0.258]
</p><p>20 The effect of the attribute itself on attention can thus be studied without the confounding factor of saliency. [sent-35, score-0.256]
</p><p>21 Our model reveals that feature transition is attractive for size, repulsive for color, and largely reversible for orientation, suggesting that size, color, and orientation are not alike in dynamic attribute processing over time. [sent-38, score-0.68]
</p><p>22 2  Gaze-Tracked Change Blindness Experiment  We investigate whether visual search for attribute change differs when the stimulus is rendered in size, color, or orientation with the same layout. [sent-39, score-0.906]
</p><p>23 We establish in a separate experiment that the change is equivalent among dimensions: Detection is equally fast and accurate for a change between two attributes across dimensions and for a no-change within each dimension across two attributes. [sent-40, score-0.329]
</p><p>24 The 1st image contains 12 attribute-1 disks and 12 attribute-2 disks in a uniformly random spatial distribution. [sent-42, score-1.049]
</p><p>25 The 2nd image is identical to the 1st image except that 1 disk changes its attribute. [sent-43, score-0.523]
</p><p>26 The disk of change here is circled in both layout matrices. [sent-45, score-0.618]
</p><p>27 Orientation has 2 angles, 0◦ for horizontal and 90◦ for vertical, with disk radii 0. [sent-54, score-0.415]
</p><p>28 Both size and orientation stimuli are of black value 0. [sent-57, score-0.419]
</p><p>29 Here we restrict color to luminance only, as color hue processing is uniquely foveal, which would greatly confound explanations for search behaviours. [sent-63, score-0.557]
</p><p>30 The ﬂicker stimuli for the 3 dimensions are rendered in an identical spatial layout. [sent-64, score-0.228]
</p><p>31 These 24 disks are located centrally on a regular 4 × 6 grid, with an inter-disk distance of 5. [sent-66, score-0.463]
</p><p>32 4◦ , which is 4 times the maximal radius a disk could assume. [sent-67, score-0.38]
</p><p>33 The 1st image of the stimulus consists of uniformly randomly distributed 12 attribute-1 disks and 12 attribute-2 disks. [sent-68, score-0.606]
</p><p>34 The ﬂicker stimulus, in the sequence of disk image 1, blank, disk image 2, and blank, is then repeatedly presented for 120 ms each. [sent-84, score-0.876]
</p><p>35 Once the subject issues a mouse click to indicate his detection of the change, the last disk image remains on till the location of change is clicked (Fig. [sent-85, score-0.771]
</p><p>36 The subject is told that two images differing in only one disk are presented repeatedly. [sent-90, score-0.41]
</p><p>37 The ﬂickering then stops at the last seen disk image, and he should click the disk of change. [sent-93, score-0.846]
</p><p>38 3  3  Performance Analysis  We evaluate the task performance on both the accuracy measured by the percentage of correct change localizations and the reaction time measured from the ﬂicker stimulus onset to the subject’s ﬁrst mouse click for indicating a detection. [sent-97, score-0.399]
</p><p>39 99  Figure 4: Change localization given an identical layout is best (fastest and most accurate) in size, worse in orientation, and worst in color. [sent-101, score-0.324]
</p><p>40 size  98  accuracy (%)  97 orientation 96  95 color 94  93  2. [sent-109, score-0.529]
</p><p>41 5  The human visual system must accomplish change localization by examining more than one disk per ﬂicker cycle, since the mean reaction time is only about 5, 6, and 7 cycles (0. [sent-116, score-0.779]
</p><p>42 48 seconds per cycle) for size, orientation, and color respectively. [sent-118, score-0.222]
</p><p>43 If only one item is looked at and ruled out per cycle, on average it would require ﬁxating 50% of 24 disks till hitting the target disk, i. [sent-119, score-0.498]
</p><p>44 Our average of 6 cycles suggests that about 2 disks are examined per cycle. [sent-122, score-0.463]
</p><p>45 When a disk is being ﬁxated, all its 8 neighbouring disks are mostly out of fovea, since they are either 5. [sent-123, score-0.871]
</p><p>46 Some coarse information about neighbouring disks must be utilized in each ﬁxation. [sent-126, score-0.491]
</p><p>47 The neighbourhood effect on change localization is studied in Fig. [sent-127, score-0.43]
</p><p>48 3 s)  Figure 5: The common spatial layout that yields the best (a,b,c) or the worst (d,e,f) change localization performance in all 3 feature dimensions. [sent-138, score-0.511]
</p><p>49 Shown here is the average image of a ﬂicker stimulus, with the disk of change taking two attributes, except in the case of color: Since the average has the same intensity as background, the change is outlined in white instead. [sent-140, score-0.688]
</p><p>50 The commonly best layout has the change among uniform attributes, whereas the commonly worst layout has a mixture of attributes. [sent-141, score-0.488]
</p><p>51 4  size  dimension  color  orientation  ( 100%, 3. [sent-142, score-0.529]
</p><p>52 35 s )  dimensionspeciﬁc best spatial layout  a:  ( 100%, 2. [sent-154, score-0.213]
</p><p>53 47 s )  dimensionspeciﬁc worst spatial layout ( 90%, 2. [sent-157, score-0.26]
</p><p>54 59 s )  Figure 6: The dimension-speciﬁc spatial layout that yields the best or worst change localization performance in one dimension only, with the largest performance gap over the other 2 dimensions. [sent-160, score-0.479]
</p><p>55 The 3 rows of numbers below each image indicate the mean accuracy and reaction time for a stimulus rendered in the same layout but in size, color, and orientation respectively. [sent-163, score-0.725]
</p><p>56 The localization of a ﬂickering change is easier in a primarily large neighbourhood for size, in any homogeneous neighbourhood for color, and in a collinear neighbourhood for orientation. [sent-164, score-0.928]
</p><p>57 5 shows that a uniform neighbourhood tends to facilitate change localization, whereas a mixed neighbourhood tends to hinder change localization, no matter which dimension the disks are rendered in. [sent-166, score-1.318]
</p><p>58 For size, change localization is easier in a neighbourhood populated with large disks. [sent-169, score-0.465]
</p><p>59 That is, unlike color or orientation, the attributes of size are asymmetrical: small produces a smaller response than large, with size 0 for a response of 0 in the limiting case. [sent-173, score-0.423]
</p><p>60 For color, change localization is easier if one color, either black or white, dominates the neighbourhood. [sent-176, score-0.307]
</p><p>61 For orientation, it is easier only if the oriented disk is part of collinear layout. [sent-177, score-0.456]
</p><p>62 4  Feature Analysis with Eye Movements  Having seen that neighbourhood uniformity has an impact on the change localization performance, we investigate how it inﬂuences the decision on which item to look at in the next ﬁxation. [sent-178, score-0.43]
</p><p>63 We ﬁrst associate a ﬁxation with a set of f -numbers at that location, each measuring the overall attribute density in a neighbourhood deﬁned by a Gaussian spatial weighting function. [sent-179, score-0.472]
</p><p>64 We have: 0, no disk at loc(i) −1, disk type 1 at loc(i) f0 (i) = (1) 1, disk type 2 at loc(i) j f0 (j)G(dist(i, j); σ) fσ (i) = (2) j G(dist(i, j); σ) 5  f0  f1  f2  f4  Figure 7: The f -number images of a ﬂicker stimulus. [sent-181, score-1.14]
</p><p>65 A negative f number (in blue shades) indicates the dominance of attribute 1, whereas a positive f number (in red shades) indicates the dominance of attribute 2. [sent-182, score-0.491]
</p><p>66 fσ measures the average attribute in a Gaussian neighbourhood with standard deviation σ. [sent-184, score-0.407]
</p><p>67 While σ = 1 covers only one disk in isolation, σ = 2 also covers 8 adjacent disks, and σ = 4 covers 16 adjacent disks. [sent-186, score-0.458]
</p><p>68 An f value of −1, 1, 0 indicates the dominance of attribute 1, 2 or neither. [sent-187, score-0.231]
</p><p>69 At σ = 1, the neighbourhood could only contain one disk, thus f1 (i) = f0 (i) for most location i’s. [sent-196, score-0.244]
</p><p>70 At σ = 4, the neighbourhood is about the half size of the display, with f4 (i) = 0 for i bordering two large different uniform neighbourhoods. [sent-198, score-0.293]
</p><p>71 The two peaks of f1 in all the 3 feature dimensions demonstrate that visual search tends to ﬁxate disks rather than empty spaces between disks. [sent-201, score-0.773]
</p><p>72 There is also an attribute bias in each dimension, and the bias is weakest in orientation and strongest in size. [sent-202, score-0.525]
</p><p>73 This bias is not diminished in f2 , demonstrating that visual search tends to navigate in groups of large disks. [sent-203, score-0.251]
</p><p>74 The single peak of f4 at value 0 not only conﬁrms the uniform randomness of our stimuli, but also reveals that the empty spaces being ﬁxated tend to be those borders between different attribute neighbourhoods at a coarser scale (Fig. [sent-204, score-0.288]
</p><p>75 The single peak of f4 at 0 reveals most ﬁxations occurring near those disks separating large groups of uniform attributes. [sent-219, score-0.489]
</p><p>76 For a saccade from pixel i to pixel j, it contributes one count of transition from a to b in the f -space: saccade  Pσ (a, b|d) = Prob(fσ (i) = a, fσ (j) = b, loc(i) −→ loc(j)| dist(i, j) = d)  (3)  Fig. [sent-232, score-0.277]
</p><p>77 These transitions are thus between the same disks or between the same inter-disk empty spaces, by e. [sent-237, score-0.583]
</p><p>78 The bias towards a particular attribute is also clear in each dimension: There are more transitions between larges than between smalls, more between blacks than between whites, about the same between horizontals and between verticals. [sent-240, score-0.289]
</p><p>79 As the saccade distance increases, disks of various attributes become viable candidates to saccade to. [sent-241, score-0.8]
</p><p>80 It becomes more likely to saccade to another disk of the same or different attribute than to saccade to an empty space (i. [sent-242, score-0.89]
</p><p>81 2) For color, it is more likely to visit black from either black or white, but not from an empty space, i. [sent-255, score-0.226]
</p><p>82 , if no disk is in ﬁxation, it is more likely to visit white in the next ﬁxation. [sent-257, score-0.504]
</p><p>83 7  size π(a) P (b|a)  color π(a) P (b|a)  orientation π(a) P (b|a)  . [sent-264, score-0.529]
</p><p>84 f1 is quantized into −1, 0, 1, corresponding to attribute 1, empty space, and attribute 2 respectively, based on threshold θ = 0. [sent-301, score-0.458]
</p><p>85 For example, for size, π shows that 43% of all the ﬁxations look at small, 4% at empty, and 53% at large, whereas the 3rd row of P shows that upon ﬁxating at large, there is 51% chance of saccading to another large, 37% chance to a small disk and 12% chance to an empty space. [sent-311, score-0.613]
</p><p>86 Search in size tends to be attracted to large, search in color tends to be repelled by white, whereas search in orientation is largely reversible between horizontal and vertical. [sent-313, score-0.843]
</p><p>87 While critical spacing is always roughly half the viewing eccentricity and independent of stimulus size, crowding magnitude differs across features: Size crowding is almost as strong as orientation crowding, whereas the effect is much weaker for color [15]. [sent-315, score-0.891]
</p><p>88 Therefore, feature crowding cannot explain the different natures of feature transitions for size, color, and orientation, or why such biases persist over larger saccades. [sent-316, score-0.283]
</p><p>89 5  Summary  Size, color, and orientation are considered elementary features extracted with separate sets of detectors responding in parallel across space. [sent-317, score-0.389]
</p><p>90 We conducted a gaze-tracked change blindness experiment, where the subject needs to locate a ﬂickering change among items rendered identically in space and separately in size, color, and orientation. [sent-319, score-0.426]
</p><p>91 If the deployment of attention during search depends only on the master spatial map of responses [14, 13, 3, 17, 12], regardless of which type of ﬁlters produces them, we should observe little differences in the search performance and behaviours among the 3 dimensions. [sent-320, score-0.388]
</p><p>92 Our search performance analysis shows that change localization is fastest and most accurate in size, less in orientation, worst in color. [sent-321, score-0.376]
</p><p>93 Change in a uniform neighbourhood is easier to localize, but only if the attribute is large for size, or if the items form collinear extension for orientation. [sent-322, score-0.543]
</p><p>94 Our feature analysis with eye movements shows that search in each dimension has an attribute bias: large for size, black for color, and vertical for orientation, and a common spatial bias on border items separating large uniform groups. [sent-323, score-0.632]
</p><p>95 However, feature transitions with saccades have a strong attractor bias for large, and a repeller bias for white, and a very little bias for orientation. [sent-324, score-0.404]
</p><p>96 These biases create an interesting dynamics in serial processing over time which could explain why localization is most effective in size and worst in color. [sent-325, score-0.26]
</p><p>97 Our results and analysis methods on these elementary features thus provide new insights into the computation of visual saliency and task-speciﬁc visual features across dimensions and over time. [sent-327, score-0.305]
</p><p>98 On the generality of crowding: visual crowding in size, saturation, and hue compared to orientation. [sent-426, score-0.272]
</p><p>99 What attributes guide the deployment of visual attention and how do they do it? [sent-438, score-0.325]
</p><p>100 The effects of transient attention on spatial resolution and the size of the attentional cue. [sent-451, score-0.226]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disks', 0.463), ('disk', 0.38), ('orientation', 0.251), ('color', 0.222), ('neighbourhood', 0.211), ('icker', 0.206), ('attribute', 0.196), ('layout', 0.148), ('xation', 0.138), ('crowding', 0.137), ('localization', 0.129), ('saccade', 0.124), ('blank', 0.116), ('saccades', 0.11), ('xations', 0.105), ('rendered', 0.104), ('visual', 0.101), ('loc', 0.09), ('change', 0.09), ('attributes', 0.089), ('click', 0.086), ('stimulus', 0.085), ('alike', 0.083), ('reaction', 0.079), ('search', 0.079), ('deployment', 0.075), ('ickering', 0.075), ('white', 0.07), ('empty', 0.066), ('spatial', 0.065), ('eye', 0.061), ('attention', 0.06), ('stimuli', 0.059), ('mouse', 0.059), ('image', 0.058), ('size', 0.056), ('saccadic', 0.055), ('visit', 0.054), ('transitions', 0.054), ('black', 0.053), ('dist', 0.052), ('blindness', 0.051), ('repeller', 0.051), ('saccading', 0.051), ('vertical', 0.047), ('worst', 0.047), ('tracker', 0.045), ('shades', 0.045), ('wolfe', 0.045), ('attentional', 0.045), ('collinear', 0.041), ('detectors', 0.041), ('subjects', 0.04), ('attractor', 0.04), ('bias', 0.039), ('elementary', 0.038), ('reversible', 0.035), ('radii', 0.035), ('till', 0.035), ('dominance', 0.035), ('saliency', 0.035), ('easier', 0.035), ('items', 0.034), ('dimensionspeci', 0.034), ('hue', 0.034), ('initiating', 0.034), ('landing', 0.034), ('rensink', 0.034), ('samplings', 0.034), ('location', 0.033), ('feature', 0.032), ('tends', 0.032), ('fastest', 0.031), ('eyes', 0.031), ('across', 0.03), ('preference', 0.03), ('masking', 0.03), ('behaviours', 0.03), ('itti', 0.03), ('xated', 0.03), ('xating', 0.03), ('subject', 0.03), ('matter', 0.03), ('transition', 0.029), ('parallel', 0.029), ('chance', 0.029), ('whereas', 0.029), ('perception', 0.028), ('largely', 0.028), ('kinds', 0.028), ('cycle', 0.028), ('serial', 0.028), ('persist', 0.028), ('neighbouring', 0.028), ('laid', 0.028), ('locate', 0.027), ('changes', 0.027), ('uniform', 0.026), ('repulsive', 0.026), ('covers', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="95-tfidf-1" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>Author: Stella X. Yu</p><p>Abstract: Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation. 1</p><p>2 0.14165263 <a title="95-tfidf-2" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>3 0.13574882 <a title="95-tfidf-3" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>Author: Hugo Larochelle, Geoffrey E. Hinton</p><p>Abstract: We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several ﬁxations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of ﬁxations and it must combine the “glimpse” at each ﬁxation with the location of the ﬁxation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classiﬁcation datasets, showing that it can perform at least as well as a model trained on whole images. 1</p><p>4 0.11886198 <a title="95-tfidf-4" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>5 0.099059373 <a title="95-tfidf-5" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>6 0.074470542 <a title="95-tfidf-6" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>7 0.07419686 <a title="95-tfidf-7" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>8 0.064485192 <a title="95-tfidf-8" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>9 0.06192141 <a title="95-tfidf-9" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>10 0.061536592 <a title="95-tfidf-10" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>11 0.056454021 <a title="95-tfidf-11" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>12 0.053054228 <a title="95-tfidf-12" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>13 0.052764539 <a title="95-tfidf-13" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>14 0.051138781 <a title="95-tfidf-14" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>15 0.049025755 <a title="95-tfidf-15" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>16 0.048850734 <a title="95-tfidf-16" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>17 0.048016258 <a title="95-tfidf-17" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>18 0.046839017 <a title="95-tfidf-18" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>19 0.044396274 <a title="95-tfidf-19" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>20 0.042593036 <a title="95-tfidf-20" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.115), (1, 0.049), (2, -0.16), (3, -0.042), (4, 0.022), (5, -0.015), (6, -0.019), (7, -0.004), (8, 0.017), (9, 0.041), (10, 0.03), (11, -0.042), (12, -0.062), (13, -0.024), (14, 0.008), (15, 0.031), (16, 0.003), (17, -0.054), (18, 0.051), (19, 0.102), (20, -0.016), (21, -0.011), (22, 0.118), (23, 0.04), (24, -0.006), (25, 0.048), (26, -0.0), (27, 0.055), (28, 0.001), (29, -0.002), (30, -0.049), (31, -0.098), (32, 0.056), (33, -0.011), (34, 0.052), (35, 0.001), (36, 0.004), (37, -0.037), (38, 0.088), (39, 0.005), (40, 0.115), (41, 0.064), (42, -0.012), (43, 0.113), (44, 0.097), (45, 0.062), (46, 0.091), (47, -0.142), (48, 0.124), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94896042 <a title="95-lsi-1" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>Author: Stella X. Yu</p><p>Abstract: Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation. 1</p><p>2 0.76524734 <a title="95-lsi-2" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>3 0.62240362 <a title="95-lsi-3" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>Author: Hugo Larochelle, Geoffrey E. Hinton</p><p>Abstract: We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several ﬁxations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of ﬁxations and it must combine the “glimpse” at each ﬁxation with the location of the ﬁxation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classiﬁcation datasets, showing that it can perform at least as well as a model trained on whole images. 1</p><p>4 0.49440968 <a title="95-lsi-4" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>Author: Vicky Froyen, Jacob Feldman, Manish Singh</p><p>Abstract: Figure/ground assignment, in which the visual image is divided into nearer (ﬁgural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model ﬁgure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a ﬁgure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure “draws” border ownership so that borders are owned by the skeletal hypothesis that best explains them. We also brieﬂy present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours. Figure/ground assignment (further referred to as f/g), in which the visual image is divided into nearer (ﬁgural) and farther (ground) surfaces, is an essential step in visual processing. A number of factors are known to affect f/g assignment, including region size [9], convexity [7, 16], and symmetry [1, 7, 11]. Figural assignment (often referred to as border ownership, under the assumption that the ﬁgural side “owns” the border) is usually studied globally, meaning that entire surfaces and their enclosing boundaries are assumed to receive a globally consistent ﬁgural status. But recent psychophysical ﬁndings [8] have suggested that border ownership can vary locally along a boundary, even leading to a globally inconsistent ﬁgure/ground assignment—broadly consistent with electrophysiological evidence showing local coding for border ownership in area V2 as early as 68 msec after image onset [20]. This suggests a spatially distributed and potentially competitive process of ﬁgural assignment [15], in which adjacent surfaces compete to own their common boundary, with ﬁgural status propagating across the image as this competition proceeds. But both the principles and computational mechanisms underlying this process are poorly understood. ∗ V.F. was supported by a Fullbright Honorary fellowship and by the Rutgers NSF IGERT program in Perceptual Science, NSF DGE 0549115, J.F. by NIH R01 EY15888, and M.S. by NSF CCF-0541185 1 In this paper we consider how border ownership might propagate over both space and time—that is, across the image as well as over the progression of computation. Following Weiss et al. [18] we adopt a Bayesian belief network architecture, with nodes along boundaries representing estimated border ownership, and connections arranged so that both neighboring nodes and nonlocal integrating nodes combine to inﬂuence local estimates of border ownership. Our model is novel in two particular respects: (a) we combine both local and global inﬂuences on border ownership in an integrated and principled way; and (b) we include as a nonlocal factor skeletal (medial axis) inﬂuences on f/g assignment. Skeletal structure has not been previously considered as a factor on border ownership, but its relevance follows from a model [4] in which shapes are conceived of as generated by or “grown” from an internal skeleton, with the consequence that their boundaries are perceptually “owned” by the skeletal side. We also briey present a psychophysical experiment in which we measured local border ownership along a contour, at several distances from a strong local f/g inducing cue, and at several time delays after the onset of the cue. The results show measurable spatial differences in judged border ownership, with judgments varying with distance from the inducer; but no temporal effect, with essentially asymptotic judgments even after very brief exposures. Both results are consistent with the behavior of the network, which converges quickly to an asymptotic but spatially nonuniform f/g assignment. 1 The Model The Network. For simplicity, we take an edge map as input for the model, assuming that edges and T-junctions have already been detected. From this edge map we then create a Bayesian belief network consisting of four hierarchical levels. At the input level the model receives evidence E from the image, consisting of local contour curvature and T-junctions. The nodes for this level are placed at equidistant locations along the contour. At the ﬁrst level the model estimates local border ownership. The border ownership, or B-nodes at this level are at the same locations as the E-nodes, but are connected to their nearest neighbors, and are the parent of the E-node at their location. (As a simplifying assumption, such connections are broken at T-junctions in such a way that the occluded contour is disconnected from the occluder.) The highest level has skeletal nodes, S, whose positions are deﬁned by the circumcenters of the Delaunay triangulation on all the E-nodes, creating a coarse medial axis skeleton [13]. Because of the structure of the Delaunay, each S-node is connected to exactly three E-nodes from which they receive information about the position and the local tangent of the contour. In the current state of the model the S-nodes are “passive”, meaning their posteriors are computed before the model is initiated. Between the S nodes and the B nodes are the grouping nodes G. They have the same positions as the S-nodes and the same Delaunay connections, but to B-nodes that have the same image positions as the E-nodes. They will integrate information from distant B-nodes, applying an interiority cue that is inﬂuenced by the local strength of skeletal axes as computed by the S-nodes (Fig. 1). Although this is a multiply connected network, we have found that given reasonable parameters the model converges to intuitive posteriors for a variety of shapes (see below). Updating. Our goal is to compute the posterior p(Bi |I), where I is the whole image. Bi is a binary variable coding for the local direction of border ownership, that is, the side that owns the border. In order for border ownership estimates to be inﬂuenced by image structure elsewhere in the image, information has to propagate throughout the network. To achieve this propagation, we use standard equations for node updating [14, 12]. However while to all other connections being directed, connections at the B-node level are undirected, causing each node to be child and parent node at the same time. Considering only the B-node level, a node Bi is only separated from the rest of the network by its two neighbors. Hence the Markovian property applies, in that Bi only needs to get iterative information from its neighbors to eventually compute p(Bi |I). So considering the whole network, at each iteration t, Bi receives information from both its child, Ei and from its parents—that is neigbouring nodes (Bi+1 and Bi−1 )—as well as all grouping nodes connected to it (Gj , ..., Gm ). The latter encode for interiority versus exteriority, interiority meaning that the B-node’s estimated gural direction points towards the G-node in question, exteriority meaning that it points away. Integrating all this information creates a multidimensional likelihood function: p(Bi |Bi−1 , Bi+1 , Gj , ..., Gm ). Because of its complexity we choose to approximate it (assuming all nodes are marginally independent of each other when conditioned on Bi ) by 2 Figure 1: Basic network structure of the model. Both skeletal (S-nodes) and border-ownerhsip nodes (B-nodes) get evidence from E-nodes, though different types. S-nodes receive mere positional information, while B-nodes receive information about local curvature and the presence of T-junctions. Because of the structure of the Delaunay triangulation S-nodes and G-nodes (grouping nodes) always get input from exactly three nodes, respectively E and B-nodes. The gray color depicts the fact that this part of the network is computed before the model is initiated and does not thereafter interact with the dynamics of the model. m p(Bi |Pj , ..., Pm ) ∝ p(Bi |Pj ) (1) j where the Pj ’s are the parents of Bi . Given this, at each iteration, each node Bi performs the following computation: Bel(Bi ) ← cλ(Bi )π(Bi )α(Bi )β(Bi ) (2) where conceptually λ stands for bottom-up information, π for top down information and α and β for information received from within the same level. More formally, λ(Bi ) ← p(E|Bi ) (3) m π(Bi ) ← p(Bi |Gj )πGj (Bi ) j (4) Gj and analogously to equation 4 for α(Bi ) and β(Bi ), which compute information coming from Bi−1 and Bi+1 respectively. For these πBi−1 (Bi ), πBi+1 (Bi ), and πGj (Bi ): πGj (Bi ) ← c π(G) λBk (Gj ) (5) k=i πBi−1 (Bi ) ← c β(Bi−1 )λ(Bi−1 )π(Bi−1 ) 3 (6) and πBi+1 (Bi ) is analogous to πBi−1 (Bi ), with c and c being normalization constants. Finally for the G-nodes: Bel(Gi ) ← cλ(Gi )π(Gi ) λ(Gi ) ← (7) λBj (Gi ) (8) j m λBj (Gi ) ← λ(Bj )p(Bi |Gj )[α(Bj )β(Bj ) Bj p(Bi |Gk )πGk (Bi )] (9) k=i Gk The posteriors of the S-nodes are used to compute the π(Gi ). This posterior computes how well the S-node at each position explains the contour—that is, how well it accounts for the cues ﬂowing from the E-nodes it is connected to. Each Delaunay connection between S- and E-nodes can be seen as a rib that sprouts from the skeleton. More speciﬁcally each rib sprouts in a direction that is normal (perpendicular) to the tangent of the contour at the E-node plus a random error φi chosen independently for each rib from a von Mises distribution centered on zero, i.e. φi ∼ V (0, κS ) with spread parameter κS [4]. The rib lengths are drawn from an exponential decreasing density function p(ρi ) ∝ e−λS ρi [4]. We can now express how well this node “explains” the three E-nodes it is connected to via the probability that this S-node deserves to be a skeletal node or not, p(S = true|E1 , E2 , E3 ) ∝ p(ρi )p(φi ) (10) i with S = true depicting that this S-node deserves to be a skeletal node. From this we then compute the prior π(Gi ) in such a way that good (high posterior) skeletal nodes induce a high interiority bias, hence a stronger tendency to induce ﬁgural status. Conversely, bad (low posterior) skeletal nodes create a prior close to indifferent (uniform) and thus have less (or no) inﬂuence on ﬁgural status. Likelihood functions Finally we need to express the likelihood function necessary for the updating rules described above. The ﬁrst two likelihood functions are part of p(Ei |Bi ), one for each of the local cues. The ﬁrst one, reﬂecting local curvature, gives the probability of the orientations of the two vectors inherent to Ei (α1 and α2 ) given both direction of ﬁgure (θ) encoded in Bi as a von Mises density centered on θ, i.e. αi ∼ V (θ, κEB ). The second likelihood function, reﬂecting the presence of a T-junction, simply assumes a ﬁxed likelihood when a T-junction is present—that is p(T-junction = true|Bi ) = θT , where Bi places the direction of ﬁgure in the direction of the occluder. This likelihood function is only in effect when a T-junction is present, replacing the curvature cue at that node. The third likelihood function serves to keep consistency between nodes of the ﬁrst level. This function p(Bi |Bi−1 ) or p(Bi |Bi+1 ) is used to compute α(B) and β(B) and is deﬁned 2x2 conditional probability matrix with a single free parameter, θBB (the probability that ﬁgural direction at both B-nodes are the same). A fourth and ﬁnal likelihood function p(Bi |Gj ) serves to propagate information between level one and two. This likelihood function is 2x2 conditional probability matrix matrix with one free parameter, θBG . In this case θBG encodes the probability that the ﬁgural direction of the B-node is in the direction of the exterior or interior preference of the G-node. In total this brings us to six free parameters in the model: κS , λS , κEB , θT , θBB , and θBG . 2 Basic Simulations To evaluate the performance of the model, we ﬁrst tested it on several basic stimulus conﬁgurations in which the desired outcome is intuitively clear: a convex shape, a concave shape, a pair of overlapping shapes, and a pair of non-overlapping shapes (Fig. 2,3). The convex shape is the simplest in that curvature never changes sign. The concave shape includes a region with oppositely signed curvature. (The shape is naturally described as predominantly positively curved with a region of negative curvature, i.e. a concavity. But note that it can also be interpreted as predominantly negatively curved “window” with a region of positive curvature, although this is not the intuitive interpretation.) 4 The overlapping pair of shapes consists of two convex shapes with one partly occluding the other, creating a competition between the two shapes for the ownership of the common borderline. Finally the non-overlapping shapes comprise two simple convex shapes that do not touch—again setting up a competition for ownership of the two inner boundaries (i.e. between each shape and the ground space between them). Fig. 2 shows the network structures for each of these four cases. Figure 2: Network structure for the four shape categories (left to right: convex, concave, overlapping, non-overlapping shapes). Blue depict the locations of the B-nodes (and also the E-nodes), the red connections are the connections between B-nodes, the green connections are connections between B-nodes and G-nodes, and the G-nodes (and also the S-nodes) go from orange to dark red. This colour code depicts low (orange) to high (dark red) probability that this is a skeletal node, and hence the strength of the interiority cue. Running our model with hand-estimated parameter values yields highly intuitive posteriors (Fig. 3), an essential “sanity check” to ensure that the network approximates human judgments in simple cases. For the convex shape the model assigns ﬁgure to the interior just as one would expect even based solely on local curvature (Fig. 3A). In the concave ﬁgure (Fig. 3B), estimated border ownership begins to reverse inside the deep concavity. This may seem surprising, but actually closely matches empirical results obtained when local border ownership is probed psychophysically inside a similarly deep concavity, i.e. a “negative part” in which f/g seems to partly reverse [8]. For the overlapping shapes posteriors were also intuitive, with the occluding shape interpreted as in front and owning the common border (Fig. 3C). Finally, for the two non-overlapping shapes the model computed border-ownership just as one would expect if each shape were run separately, with each shape treated as ﬁgural along its entire boundary (Fig. 3D). That is, even though there is skeletal structure in the ground-region between the two shapes (see Fig. 2D), its posterior is weak compared to the skeletal structure inside the shapes, which thus loses the competition to own the boundary between them. For all these conﬁgurations, the model not only converged to intuitive estimates but did so rapidly (Fig. 4), always in fewer cycles than would be expected by pure lateral propagation, niterations < Nnodes [18] (with these parameters, typically about ﬁve times faster). Figure 3: Posteriors after convergence for the four shape categories (left to right: convex, concave, overlapping, non-overlapping). Arrows indicate estimated border ownership, with direction pointing to the perceived ﬁgural side, and length proportional to the magnitude of the posterior. All four simulations used the same parameters. 5 Figure 4: Convergence of the model for the basic shape categories. The vertical lines represent the point of convergence for each of the three shape categories. The posterior change is calculated as |p(Bi = 1|I)t − p(Bi = 1|I)t−1 | at each iteration. 3 Comparison to human data Beyond the simple cases reviewed above, we wished to submit our network to a more ﬁne-grained comparison with human data. To this end we compared its performance to that of human subjects in an experiment we conducted (to be presented in more detail in a future paper). Brieﬂy, our experiment involved ﬁnding evidence for propagation of f/g signals across the image. Subjects were ﬁrst shown a stimulus in which the f/g conﬁguration was globally and locally unambiguous and consistent: a smaller rectangle partly occluding a larger one (Fig. 5A), meaning that the smaller (front) one owns the common border. Then this conﬁguration was perturbed by adding two bars, of which one induced a local f/g reversal—making it now appear locally that the larger rectangle owned the border (Fig. 5B). (The other bar in the display does not alter f/g interpretation, but was included to control for the attentional affects of introducing a bar in the image.) The inducing bar creates T-junctions that serve as strong local f/g cues, in this case tending to reverse the prior global interpretation of the ﬁgure. We then measured subjective border ownership along the central contour at various distances from the inducing bar, and at different times after the onset of the bar (25ms, 100ms and 250ms). We measured border ownership locally using a method introduced in [8] in which a local motion probe is introduced at a point on the boundary between two color regions of different colors, and the subject is asked which color appeared to move. Because the ﬁgural side “owns” the border, the response reﬂects perceived ﬁgural status. The goal of the experiment was to actually measure the progression of the inﬂuence of the inducing T-junction as it (hypothetically) propagated along the boundary. Brieﬂy, we found no evidence of temporal differences, meaning that f/g judgments were essentially constant over time, suggesting rapid convergence of local f/g assignment. (This is consistent with the very rapid convergence of our network, which would suggest a lack of measurable temporal differences except at much shorter time scales than we measured.) But we did ﬁnd a progressive reduction of f/g reversal with increasing distance from the inducer—that is, the inﬂuence of the T-junction decayed with distance. Mean responses aggregated over subjects (shortest delay only) are shown in Fig. 6. In order to run our model on this stimulus (which has a much more complex structure than the simple ﬁgures tested above) we had to make some adjustments. We removed the bars from the edge map, leaving only the T-junctions as underlying cues. This was a necessary ﬁrst step because our model is not yet able to cope with skeletons that are split up by occluders. (The larger rectangle’s skeleton has been split up by the lower bar.) In this way all contours except those created by the bars were used to create the network (Fig. 7). Given this network we ran the model using hand-picked parameters that 6 Figure 5: Stimuli used in the experiment. A. Initial stimulus with locally and globally consistent and unambiguous f/g. B. Subsequently bars were added of which one (the top bar in this case) created a local reversal of f/g. C. Positions at which local f/g judgments of subjects were probed. Figure 6: Results from our experiment aggregated for all 7 subjects (shortest delay only) are shown in red. The x-axis shows distance from the inducing bar at which f/g judgment was probed. The y-axis shows the proportion of trials on which subjects judged the smaller rectangle to own the boundary. As can be seen, the further from the T-junction, the lower the f/g reversal. The ﬁtted model (green curve) shows very similar pattern. Horizontal black line indicates chance performance (ambiguous f/g). gave us the best possible qualitative similarity to the human data. The parameters used never entailed total elimination of the inﬂuence of any likelihood function (κS = 16, λS = .025, κEB = .5, θT = .9, θBB = .9, and θBG = .6). As can be seen in Fig. 6 the border-ownership estimates at the locations where we had data show compelling similarities to human judgments. Furthermore along the entire contour the model converged to intuitive border-ownership estimates (Fig. 7) very rapidly (within 36 iterations). The fact that our model yielded intuitive estimates for the current network in which not all contours were completed shows another strength of our model. Because our model included grouping nodes, it did not require contours to be amodally completed [6] in order for information to propagate. 4 Conclusion In this paper we proposed a model rooted in Bayesian belief networks to compute ﬁgure/ground. The model uses both local and global cues, combined in a principled way, to achieve a stable and apparently psychologically reasonable estimate of border ownership. Local cues included local curvature and T-junctions, both well-established cues to f/g. Global cues included skeletal structure, 7 Figure 7: (left) Node structure for the experimental stimulus. (right) The model’s local borderownership estimates after convergence. a novel cue motivated by the idea that strongly axial shapes tend to be ﬁgural and thus own their boundaries. We successfully tested this model on both simple displays, in which it gave intuitive results, and on a more complex experimental stimulus, in which it gave a close match to the pattern of f/g propagation found in our subjects. Speciﬁcally, the model, like the human subjects rapidly converged to a stable local f/g interpretation. Our model’s structure shows several interesting parallels to properties of neural coding of border ownership in visual cortex. Some cortical cells (end-stopped cells) appear to code for local curvature [3] and T-junctions [5]. The B-nodes in our model could be seen as corresponding to cells that code for border ownership [20]. Furthermore, some authors [2] have suggested that recurrent feedback loops between border ownership cells in V2 and cells in V4 (corresponding to G-nodes in our model) play a role in the rapid computation of border ownership. The very rapid convergence we observed in our model likewise appears to be due to the connections between B-nodes and G-nodes. Finally scale-invariant shape representations (such as, speculatively, those based on skeletons) are thought to be present in higher cortical regions such as IT [17], which project down to earlier areas in ways that are not yet understood. A number of parallels to past models of f/g should be mentioned. Weiss [18] pioneered the application of belief networks to the f/g problem, though their network only considered a more restricted set of local cues and no global ones, such that information only propagated along the contour. Furthermore it has not been systematically compared to human judgments. Kogo et al. [10] proposed an exponential decay of f/g signals as they spread throughout the image. Our model has a similar decay for information going through the G-nodes, though it is also inﬂuenced by an angular factor deﬁned by the position of the skeletal node. Like the model by Li Zhaoping [19], our model includes horizontal propagation between B-nodes, analogous to border-ownership cells in her model. A neurophysiological model by Craft et al. [2] deﬁnes grouping cells coding for an interiority preference that decays with the size of the receptive ﬁelds of these grouping cells. Our model takes this a step further by including shape (skeletal) structure as a factor in interiority estimates, rather than simply size of receptive ﬁelds (which is similar to the rib lengths in our model). Currently, our use of skeletons as shape representations is still limited to medial axis skeletons and surfaces that are not split up by occluders. Our future goals including integrating skeletons in a more robust way following the probabilistic account suggested by Feldman and Singh [4]. Eventually, we hope to fully integrate skeleton computation with f/g computation so that the more general problem of shape and surface estimation can be approached in a coherent and uniﬁed fashion. 8 References [1] P. Bahnsen. Eine untersuchung uber symmetrie und assymmetrie bei visuellen wahrnehmungen. Zeitschrift fur psychology, 108:129–154, 1928. [2] E. Craft, H. Sch¨ tze, E. Niebur, and R. von der Heydt. A neural model of ﬁgure-ground u organization. Journal of Neurophysiology, 97:4310–4326, 2007. [3] A. Dobbins, S. W. Zucker, and M. S. Cyander. Endstopping and curvature. Vision Research, 29:1371–1387, 1989. [4] J. Feldman and M. Singh. Bayesian estimation of the shape skeleton. Proceedings of the National Academy of Sciences, 103:18014–18019, 2006. [5] B. Heider, V. Meskenaite, and E. Peterhans. Anatomy and physiology of a neural mechanism deﬁning depth order and contrast polarity at illusory contours. European Journal of Neuroscience, 12:4117–4130, 2000. [6] G. Kanizsa. Organization inVision. New York: Praeger, 1979. [7] G. Kanizsa and W. Gerbino. Vision and Artifact, chapter Convexity and symmetry in ﬁgureground organisation, pages 25–32. New York: Springer, 1976. [8] S. Kim and J. Feldman. Globally inconsistent ﬁgure/ground relations induced by a negative part. Journal of Vision, 9:1534–7362, 2009. [9] K. Koffka. Principles of Gestalt Psychology. Lund Humphries, London, 1935. [10] N. Kogo, C. Strecha, L. Van Gool, and J. Wagemans. Surface construction by a 2-d differentiation-integration process: a neurocomputational model for perceived border ownership, depth, and lightness in kanizsa ﬁgures. Psychological Review, 117:406–439, 2010. [11] B. Machielsen, M. Pauwels, and J. Wagemans. The role of vertical mirror-symmetry in visual shape detection. Journal of Vision, 9:1–11, 2009. [12] K. Murphy, Y. Weiss, and M.I. Jordan. Loopy belief propagation for approximate inference: an empirical study. Proceedings of Uncertainty in AI, pages 467–475, 1999. [13] R. L. Ogniewicz and O. K¨ bler. Hierarchic Voronoi skeletons. Pattern Recognition, 28:343– u 359, 1995. [14] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 1988. [15] M. A. Peterson and E. Skow. Inhibitory competition between shape properties in ﬁgureground perception. Journal of Experimental Psychology: Human Perception and Performance, 34:251–267, 2008. [16] K. A. Stevens and A. Brookes. The concave cusp as a determiner of ﬁgure-ground. Perception, 17:35–42, 1988. [17] K. Tanaka, H. Saito, Y. Fukada, and M. Moriya. Coding visual images of object in the inferotemporal cortex of the macaque monkey. Journal of Neurophysiology, 66:170–189, 1991. [18] Y. Weiss. Interpreting images by propagating Bayesian beliefs. Adv. in Neural Information Processing Systems, 9:908915, 1997. [19] L. Zhaoping. Border ownership from intracortical interactions in visual area V2. Neuron, 47(1):143–153, Jul 2005. [20] H. Zhou, H. S. Friedman, and R. von der Heydt. Coding of border ownerschip in monkey visual cortex. The Journal of Neuroscience, 20:6594–6611, 2000. 9</p><p>5 0.48317397 <a title="95-lsi-5" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>6 0.47841099 <a title="95-lsi-6" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>7 0.44533476 <a title="95-lsi-7" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>8 0.43809766 <a title="95-lsi-8" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>9 0.43668666 <a title="95-lsi-9" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>10 0.43549332 <a title="95-lsi-10" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>11 0.43090862 <a title="95-lsi-11" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>12 0.42691717 <a title="95-lsi-12" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>13 0.42282701 <a title="95-lsi-13" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>14 0.38950816 <a title="95-lsi-14" href="./nips-2010-Decoding_Ipsilateral_Finger_Movements_from_ECoG_Signals_in_Humans.html">57 nips-2010-Decoding Ipsilateral Finger Movements from ECoG Signals in Humans</a></p>
<p>15 0.38362104 <a title="95-lsi-15" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>16 0.38296509 <a title="95-lsi-16" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>17 0.3827267 <a title="95-lsi-17" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>18 0.3824608 <a title="95-lsi-18" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>19 0.38158944 <a title="95-lsi-19" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>20 0.3710362 <a title="95-lsi-20" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.352), (13, 0.023), (17, 0.017), (27, 0.104), (30, 0.036), (35, 0.012), (45, 0.217), (50, 0.036), (52, 0.018), (60, 0.027), (77, 0.035), (90, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78475857 <a title="95-lda-1" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>Author: Stella X. Yu</p><p>Abstract: Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation. 1</p><p>2 0.65398091 <a title="95-lda-2" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>Author: Alex Kulesza, Ben Taskar</p><p>Abstract: We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random ﬁelds and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems. 1</p><p>3 0.58436328 <a title="95-lda-3" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>4 0.57929802 <a title="95-lda-4" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>Author: Hongjing Lu, Tungyou Lin, Alan Lee, Luminita Vese, Alan L. Yuille</p><p>Abstract: It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More speciﬁcally, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, ﬁrst-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our ﬁrst experimental session we estimate the weights of the three terms for each functional form to maximize the ﬁt to human performance. We then measured human performance for motion tasks and found that we obtained better ﬁt for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better ﬁt to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best ﬁt models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction – ranging from 3% to 8% for ﬁve human subjects over ten experimental conditions – and give further support that the human visual system uses an L1-norm (Laplace) prior.</p><p>5 0.57750618 <a title="95-lda-5" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>Author: Gael Varoquaux, Alexandre Gramfort, Jean-baptiste Poline, Bertrand Thirion</p><p>Abstract: Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reﬂects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data. Learning such models entails two main challenges: i) modeling full brain connectivity is a difﬁcult estimation problem that faces the curse of dimensionality and ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging. We describe subject-level brain functional connectivity structure as a multivariate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the ﬁrst report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the ﬁrst time that known cognitive networks appear as the integrated communities of functional connectivity graph. 1</p><p>6 0.5771364 <a title="95-lda-6" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>7 0.57591736 <a title="95-lda-7" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>8 0.57378066 <a title="95-lda-8" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>9 0.57267791 <a title="95-lda-9" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>10 0.57085288 <a title="95-lda-10" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>11 0.57084215 <a title="95-lda-11" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>12 0.56957161 <a title="95-lda-12" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>13 0.56873053 <a title="95-lda-13" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>14 0.56745791 <a title="95-lda-14" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>15 0.5673852 <a title="95-lda-15" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>16 0.56668824 <a title="95-lda-16" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>17 0.56650335 <a title="95-lda-17" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>18 0.56645006 <a title="95-lda-18" href="./nips-2010-Probabilistic_latent_variable_models_for_distinguishing_between_cause_and_effect.html">218 nips-2010-Probabilistic latent variable models for distinguishing between cause and effect</a></p>
<p>19 0.56611592 <a title="95-lda-19" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>20 0.56576759 <a title="95-lda-20" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
