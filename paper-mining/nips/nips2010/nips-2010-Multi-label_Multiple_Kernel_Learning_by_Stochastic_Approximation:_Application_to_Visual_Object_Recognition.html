<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-174" href="#">nips2010-174</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</h1>
<br/><p>Source: <a title="nips-2010-174-pdf" href="http://papers.nips.cc/paper/4177-multi-label-multiple-kernel-learning-by-stochastic-approximation-application-to-visual-object-recognition.pdf">pdf</a></p><p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>Reference: <a title="nips-2010-174-reference" href="../nips2010_reference/nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 † Korea University, Anam-dong, Seoul, 136-713, Korea  Abstract Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. [sent-18, score-1.019]
</p><p>2 In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). [sent-19, score-0.437]
</p><p>3 We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. [sent-20, score-0.974]
</p><p>4 Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. [sent-21, score-0.416]
</p><p>5 We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. [sent-22, score-0.084]
</p><p>6 Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. [sent-24, score-0.257]
</p><p>7 1  Introduction  Recent studies have shown promising performance of kernel methods for object classiﬁcation, recognition and localization [1]. [sent-25, score-0.598]
</p><p>8 Since the choice of kernel functions can signiﬁcantly affect the performance of kernel methods, kernel learning, or more speciﬁcally Multiple Kernel Learning (MKL) [2, 3, 4, 5, 6, 7], has attracted considerable amount of interest in computer vision community. [sent-26, score-1.164]
</p><p>9 In this work, we focuss on kernel learning for object recognition because the visual content of an image can be represented in many ways, depending on the methods used for keypoint detection, descriptor/feature extraction, and keypoint quantization. [sent-27, score-0.817]
</p><p>10 , kernel function), the related fusion problem can be cast into a MKL problem. [sent-30, score-0.396]
</p><p>11 Besides L1-norm [15] and L2-norm [16], Lp-norm [17] has also been proposed to regularize the weights for kernel combination. [sent-36, score-0.421]
</p><p>12 Other then the framework based on maximum margin classiﬁcation, MKL can also be formulated by using kernel alignment [18] and Fisher discriminative analysis frameworks [19]. [sent-37, score-0.368]
</p><p>13 Most of these studies assume that either the same or similar kernel functions are used by different but related classiﬁcation tasks. [sent-39, score-0.409]
</p><p>14 Since most object recognition problems involve many object classes, whose number might go up to hundreds or sometimes even to thousands, it is important to develop an efﬁcient learning algorithm for multi-class and multilabel MKL that is sublinear in the number of classes. [sent-41, score-0.378]
</p><p>15 We note that although this assumption signiﬁcantly constrains the choice of kernel functions for different classes, our empirical studies with object recognition show that it does not affect the classiﬁcation performance. [sent-43, score-0.598]
</p><p>16 A naive implementation of ML-MKL with shared kernel combination will lead to a computational cost linear in the number of classes. [sent-45, score-0.442]
</p><p>17 We alleviate this computational challenge by exploring the idea of combining worst case analysis with stochastic approximation. [sent-46, score-0.121]
</p><p>18 Our √ analysis reveals that the convergence rate of the proposed algorithm is O(m1/3 ln m), which is signiﬁcantly better than a linear dependence on m, where m is the number of classes. [sent-47, score-0.091]
</p><p>19 Our empirical studies show that the proposed MKL algorithm yields similar performance as the state-of-the-art algorithms for ML-MKL, but with a signiﬁcantly shorter running time, making it suitable for multi-label learning with a large number of classes. [sent-48, score-0.147]
</p><p>20 Section 3 summarizes the experimental results for object recognition. [sent-51, score-0.113]
</p><p>21 , s} the collection of s kernel matrices for the data a points in D, i. [sent-68, score-0.368]
</p><p>22 We denote by K(p) = a=1 pa Ka the combined kernel matrices. [sent-75, score-0.459]
</p><p>23 Our goal is to learn from the training examples the optimal kernel combination p for all the m classes. [sent-79, score-0.442]
</p><p>24 It is straightforward to verify the following dual problem of (1): m  min max  p∈P α∈Q1  L(p, α) =  1 [αk ] 1 − (αk ◦ yk ) K(p)(αk ◦ yk ) 2  k=1  ,  (2)  where Q1 = α = (α1 , . [sent-82, score-0.463]
</p><p>25 We then follow the subgradient descent approach in [10] and compute the gradient of A(p) as ∂pi A(p) = −  1 2  m  k=1  (αk (p) ◦ yk ) Ki (αk (p) ◦ yk ), 2  where αk (p) = arg maxα∈[0,C]n [αk ] 1 − (αk ◦ yk ) K(p)(αk ◦ yk ). [sent-93, score-0.948]
</p><p>26 The main computational problem with ML-MKL-Sum is that by treating every class equally, in each iteration of subgradient descent, it requires solving m kernel SVMs, making it unscalable to a very large number of classes. [sent-96, score-0.502]
</p><p>27 1  A Minimax Framework for Multi-label MKL  In order to alleviate the computational difﬁculty arising from a large number of classes, we search for the combined kernel matrix K(p) that minimizes the worst classiﬁcation error among m classes, i. [sent-99, score-0.405]
</p><p>28 In this way, we are able to achieve a running time that is sublinear in the number of classes. [sent-105, score-0.119]
</p><p>29 +  B=  2  1 2  1 [β k ] 1 − (β k ◦ yk ) K(p)(β k ◦ yk ) 2  . [sent-119, score-0.43]
</p><p>30 In order to effectively explore the power of off-the-shelf SVM solvers, we rewrite (3) as follows m  L(p, γ) = max  min max  α∈Q1  p∈P γ∈Γ  k=1  1 γ k αk 1 − (αk ◦ yk ) K(p)(αk ◦ yk ) 2  ,  (5)  where Γ = {(γ 1 , . [sent-128, score-0.496]
</p><p>31 (5) is that we can resort to a SVM solver to efﬁciently ﬁnd αk for a given combination of kernels K(p). [sent-135, score-0.142]
</p><p>32 (5), we develop a subgradient descent approach for solving the optimization problem. [sent-137, score-0.088]
</p><p>33 Unfortunately, the algorithm described above shares the same shortcoming as the other approaches for multiple label multiple kernel learning, i. [sent-143, score-0.506]
</p><p>34 Let jt be the index of the sampled classiﬁcation task. [sent-151, score-0.223]
</p><p>35 Using the sampled task jt , we estimate the γ p gradient of L(p, γ) with respect to pa and γ k , denoted by ga (pt , γt ) and gk (pt , γt ), as follows 1 p (8) ga (pt , γt ) = − (αjt ◦ yjt ) Ka (αjt ◦ yjt ), 2 0 k = jt γ (9) gk (pt , γt ) = 1 1 αk 1 − 2 (αk ◦ yk ) K(p)(αk ◦ yk ) k = jt . [sent-152, score-1.536]
</p><p>36 γk  γ p The computation of ga (pt , γt ) and gi (pt , γt ) only requires αjt and therefore only needs to solve one SVM problem, instead of m SVMs. [sent-153, score-0.14]
</p><p>37 We have γ p Et [ga (pt , γt )] = pa L(pt , γt ), Et [gi (pt , γt )] = γi L(pt , γt ), where Et [·] stands for the expectation over the randomly sampled task jt . [sent-158, score-0.343]
</p><p>38 Since gi (pt , γt ) is γ proportional to 1/γt , to ensure the norm of gi (pt , γt ) to be bounded, we need to smooth γt+1 . [sent-161, score-0.104]
</p><p>39 , ym : the assignments of m different classes to n training instances • T : number of iterations • δ: smoothing parameter 2: Initialization • γ1 = 1/m and p1 = 1/s 3: for t = 1, . [sent-192, score-0.195]
</p><p>40 , T do m 1 4: Sample a classiﬁcation task jt according to the distribution M ulti(γt , . [sent-195, score-0.223]
</p><p>41 jt jt jt 5: Compute α = arg maxα∈[0,C]n α 1 − (α ◦ y ) K(p)(α ◦ y )/2 using an off shelf SVM solver. [sent-199, score-0.669]
</p><p>42 γ p 6: Compute the estimated gradients ga (pt , γt ) and gi (pt , γt ) using Eq. [sent-200, score-0.176]
</p><p>43 7: Update pt+1 , γt+1 and γt+1 as follows pa t+1  =  k  =  [γt+1 ]  pa t p p exp(−ηγ ga (pt , γt )), a = 1, . [sent-202, score-0.27]
</p><p>44 , m; γt+1 = (1 − δ)γt+1 + Zt m  8: end for 9: Compute the ﬁnal solution p and α as  γ=  2  1 T  T  γt ,  p=  t=1  1 T  T  pt . [sent-210, score-0.245]
</p><p>45 Since we only need to solve one kernel SVM at each iteration, we have the computational complexity for the proposed algorithm on the order of O(m1/3 (ln m)/T ), sublinear in the number of classes m. [sent-213, score-0.561]
</p><p>46 3  Experiments  In this section, we empirically evaluate the proposed multiple kernel learning algorithm2 by demonstrating its efﬁciency and effectiveness on the visual object recognition task. [sent-214, score-0.758]
</p><p>47 1  Data sets  We use three benchmark data sets for visual object recognition: Caltech-101, Pascal VOC 2006 and Pascal VOC 2007. [sent-216, score-0.189]
</p><p>48 Caltech-101 contains 101 different object classes in addition to a “background” class. [sent-217, score-0.203]
</p><p>49 Unlike Caltech-101 data set, where each image is assigned to one class, images in VOC data sets can be assigned to multiple classes simultaneously, making it more suitable for multi-label learning. [sent-222, score-0.226]
</p><p>50 rar  5  Table 1: Classiﬁcation accuracy (AUC) and running times (second) of all ML-MKL algorithms on three data sets. [sent-227, score-0.089]
</p><p>51 Abbreviations SA, GMKL, Sum, Simple, VSKL, AVG stand for ML-MKL-SA, Generalized MKL, ML-MKL-Sum, SimpleMKL, variable sparsity kernel learning and average kernel, respectively dataset CALTECH-101 VOC2006 VOC2007  SA 0. [sent-228, score-0.368]
</p><p>52 2 0  0  200  400  600  800  time(sec)  ML-MKL-SA  kernel coefficients  0. [sent-260, score-0.449]
</p><p>53 5  2 4  x 10  VSKL  Figure 1: The evolution of kernel weights over time for CALTECH-101 data set. [sent-281, score-0.394]
</p><p>54 For GMKL and VSKL, the curves display the kernel weights that are averaged over all the classes since a different kernel combination is learnt for each class. [sent-282, score-0.926]
</p><p>55 (ii) PHOW gray/color: keypoints based on dense sampling; SIFT descriptors are quantized to 300 words and spatial histograms with 2x2 and 4x4 subdivisions are built to generate chi-squared kernels [30]. [sent-286, score-0.173]
</p><p>56 For VOC data sets, a different procedure, based on the reports of VOC challenges [1], is used to construct multiple visual dictionaries, and each dictionary results in a different kernel. [sent-288, score-0.145]
</p><p>57 To obtain multiple visual dictionaries, we deploy (i) three keypoint detectors, i. [sent-289, score-0.237]
</p><p>58 , dense sampling, HARHES [32] and HESLAP [33], (ii) two keypoint descriptors, i. [sent-291, score-0.092]
</p><p>59 , 500 and 1, 000 visual words, (iv) two different kernel functions, i. [sent-295, score-0.444]
</p><p>60 Using the above variants in visual dictionary construction, we constructed 22 kernels for both VOC2007 and VOC2006 data sets. [sent-299, score-0.144]
</p><p>61 We also compare ML-MKL-SA to ML-MKL-Sum, which learns a kernel combination shared by all classes as described in Section 2 using the optimization method in [21]. [sent-303, score-0.532]
</p><p>62 In all implementations of ML multiple kernel learning algorithms,we use LIBSVM implementation of one-versus-all SVM where needed. [sent-304, score-0.437]
</p><p>63 4  Experimental Results  To evaluate the effectiveness of different algorithms for multi-label multiple kernel learning, we ﬁrst compute the area under precision-recall curve (AUC) for each class, and report the value of AUC averaged over all the classes. [sent-306, score-0.466]
</p><p>64 76  400  200  400  600  800  1000  1200  number of iterations  Figure 3: Classiﬁcation accuracy (AUC) of the proposed algorithm Ml-MKL-SA on CALTECH-101 using different values of δ (for ηp = ηγ = 0. [sent-341, score-0.109]
</p><p>65 For the proposed method, itarations stop when pt −pt−1 is smaller than 0. [sent-347, score-0.272]
</p><p>66 Unless stated, the smoothing parameter pt δ is set to be 0. [sent-349, score-0.282]
</p><p>67 , ML-MKL-SA, yields the best performance among the methods in comparison, which justiﬁes the assumption of using the same kernel combination for all the classes. [sent-358, score-0.442]
</p><p>68 Note that a simple approach that uses the average of all kernels yields reasonable performance, although its classiﬁcation accuracy is signiﬁcantly worse than the proposed approach ML-MKL-SA. [sent-359, score-0.141]
</p><p>69 Second, we observe that except for the average kernel method that does not require learning the kernel combination weights, MLMKL-SA and ML-MKL-Sum are signiﬁcantly more efﬁcient than the other baseline approaches. [sent-360, score-0.81]
</p><p>70 This is not surprising as ML-MKL-SA and ML-MKL-Sum compute a single kernel combination for all classes. [sent-361, score-0.442]
</p><p>71 This is because the number of classes in CALTECH-101 is signiﬁcantly larger than that of the two VOC challenge data sets. [sent-363, score-0.145]
</p><p>72 1 shows the change in the kernel weights over time for the proposed method and the three baseline methods (i. [sent-366, score-0.421]
</p><p>73 We observe that, overall, ML-MKL-SA shares a similar pattern as GMKL and VSKL in the evolution curves of kernel weights, but is ten times faster than the two baseline methods. [sent-369, score-0.394]
</p><p>74 Although ML-MKL-Sum is signiﬁcantly more efﬁcient than GMKL and VSKL, the kernel weights learned by ML-MKL-Sum vary signiﬁcantly, particularly at the beginning of the learning process, making it a less stable algorithm than the proposed algorithm ML-MKL-SA. [sent-370, score-0.457]
</p><p>75 3 shows how the classiﬁcation accuracy (AUC) of the proposed algorithm changes over iterations on CALTECH-101 using four different values of δ. [sent-376, score-0.109]
</p><p>76 4  Conclusion and Future Work  In this paper, we present an efﬁcient optimization framework for multi-label multiple kernel learning that combines worst-case analysis with stochastic approximation. [sent-388, score-0.466]
</p><p>77 Compared to the other algorithms for ML-MKL, the key advantage of the proposed algorithm is that its computational cost is sublinear in the number of classes, making it suitable for handling a large number of classes. [sent-389, score-0.139]
</p><p>78 We verify the effectiveness of the proposed algorithm by experiments in object recognition on several benchmark data sets. [sent-390, score-0.245]
</p><p>79 Ye, “Multi-label multiple kernel learning,” in Proceedings of Neural Information Processings Systems, 2008. [sent-421, score-0.437]
</p><p>80 Jordan, “Learning the kernel matrix with semideﬁnite programming,” Journal of Machine Learning Research, vol. [sent-427, score-0.368]
</p><p>81 Rakotomamonjy, “Second order optimization of kernel parameters,” in NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels, 2008. [sent-432, score-0.368]
</p><p>82 Nowozin, “On feature combination for multiclass object classiﬁcation,” in Proceedings of the IEEE International Conference on Computer Vision, 2009. [sent-435, score-0.187]
</p><p>83 Nowozin, “Let the kernel ﬁgure it out: Principled learning of pre-processing for kernel classiﬁers,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009. [sent-438, score-0.736]
</p><p>84 Jordan, “Multiple kernel learning, conic duality, and the smo algorithm,” in Proceedings of the 21st International Conference on Machine Learning, 2004. [sent-442, score-0.395]
</p><p>85 Schafer, “A general and efﬁcient multiple kernel learning algorithm,” in Proceedings of Neural Information Processings Systems, pp. [sent-446, score-0.437]
</p><p>86 Lyu, “An extended level method for efﬁcient multiple kernel learning,” in Proceedings of Neural Information Processings Systems, pp. [sent-460, score-0.437]
</p><p>87 Lyu, “Simple and efﬁcient multiple kernel learning by group lasso,” in Proceedings of the 27th International Conference on Machine Learning, 2010. [sent-468, score-0.437]
</p><p>88 Bach, “Consistency of the group lasso and multiple kernel learning,” Journal of Machine Learning Research, vol. [sent-470, score-0.437]
</p><p>89 King, “Smooth optimization for effective multiple kernel learning,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2010. [sent-479, score-0.437]
</p><p>90 Grandvalet, “More efﬁciency in multiple kernel learning,” in Proceedings of the 24th International Conference on Machine Learning, 2007. [sent-484, score-0.437]
</p><p>91 Zien, “Comparing sparse and non-sparse multiple kernel learning,” in NIPS Workshop on Understanding Multiple Kernel Learning Methods, 2009. [sent-489, score-0.437]
</p><p>92 Zien, “Efﬁcient and accurate lp-norm multiple kernel learning,” in Proceedings of Neural Information Processings Systems, 2009. [sent-497, score-0.437]
</p><p>93 Chang, “Learning the uniﬁed kernel machines for classiﬁcation,” in Proceedings of the Conference on Knowledge Discovery and Data Mining, p. [sent-501, score-0.368]
</p><p>94 , “Discriminant kernel and regularization parameter learning via semideﬁnite programming,” in Proceedings of the International Conference on Machine Learning, p. [sent-507, score-0.368]
</p><p>95 Cheng, “Multiclass multiple kernel learning,” in Proceedings of the 24th International Conference on Machine Learning, 2007. [sent-511, score-0.437]
</p><p>96 Ye, “On multiple kernel learning with multiple labels,” in Proceedings of the 21st International Jont Conference on Artiﬁcal Intelligence, 2009. [sent-515, score-0.506]
</p><p>97 Gao, “Group-sensitive multiple kernel learning for object categorization,” in Proceedings of the IEEE International Conference on Computer Vision, 2009. [sent-521, score-0.55]
</p><p>98 Caputo, “Online-batch strongly convex multi kernel learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010. [sent-525, score-0.368]
</p><p>99 Malik, “Shape matching and object recognition using low distortion correspondences,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2005. [sent-565, score-0.189]
</p><p>100 Ramakrishan, “On the algorithmics and applications of a mixed-norm based kernel learning formulation,” in Proceedings of Neural Information Processings Systems, 2009. [sent-600, score-0.368]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mkl', 0.453), ('kernel', 0.368), ('pt', 0.245), ('jt', 0.223), ('yk', 0.215), ('vskl', 0.208), ('voc', 0.192), ('auc', 0.184), ('gmkl', 0.182), ('processings', 0.115), ('object', 0.113), ('sec', 0.094), ('keypoint', 0.092), ('pa', 0.091), ('ml', 0.091), ('classes', 0.09), ('ga', 0.088), ('ka', 0.083), ('coefficients', 0.081), ('jin', 0.079), ('visual', 0.076), ('pascal', 0.076), ('recognition', 0.076), ('zt', 0.076), ('sublinear', 0.076), ('hk', 0.075), ('sa', 0.074), ('combination', 0.074), ('keypoints', 0.07), ('multiple', 0.069), ('kernels', 0.068), ('lyu', 0.066), ('classi', 0.065), ('ln', 0.064), ('svm', 0.064), ('korea', 0.061), ('simplemkl', 0.061), ('vision', 0.06), ('challenge', 0.055), ('subgradient', 0.052), ('sonnenburg', 0.052), ('gi', 0.052), ('fk', 0.051), ('rakotomamonjy', 0.05), ('ye', 0.047), ('avg', 0.047), ('proceedings', 0.046), ('anil', 0.046), ('nowozin', 0.046), ('unscalable', 0.046), ('yjt', 0.046), ('accuracy', 0.046), ('zien', 0.045), ('king', 0.044), ('running', 0.043), ('everingham', 0.042), ('studies', 0.041), ('gool', 0.041), ('ciency', 0.041), ('ulti', 0.04), ('schmid', 0.04), ('zisserman', 0.039), ('gk', 0.039), ('lanckriet', 0.038), ('smoothing', 0.037), ('kloft', 0.037), ('alleviate', 0.037), ('descent', 0.036), ('making', 0.036), ('iterations', 0.036), ('gradients', 0.036), ('cantly', 0.036), ('descriptors', 0.035), ('jain', 0.035), ('brefeld', 0.035), ('bach', 0.033), ('lowe', 0.033), ('gehler', 0.033), ('max', 0.033), ('instances', 0.032), ('cation', 0.032), ('images', 0.031), ('aro', 0.03), ('canu', 0.03), ('grandvalet', 0.029), ('stands', 0.029), ('xu', 0.029), ('effectiveness', 0.029), ('stochastic', 0.029), ('lazebnik', 0.028), ('fusion', 0.028), ('ef', 0.028), ('conference', 0.028), ('smo', 0.027), ('proposed', 0.027), ('winn', 0.027), ('cristianini', 0.027), ('pattern', 0.026), ('weights', 0.026), ('berg', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="174-tfidf-1" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>2 0.40142527 <a title="174-tfidf-2" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>Author: Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan</p><p>Abstract: Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efﬁciently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on ﬁfty thousand points in less than half an hour on a single core. 1</p><p>3 0.37125409 <a title="174-tfidf-3" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>Author: Kun Gai, Guangyun Chen, Chang-shui Zhang</p><p>Abstract: In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of norm constraints on combination coefﬁcients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method signiﬁcantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches. 1</p><p>4 0.23726337 <a title="174-tfidf-4" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>5 0.20134841 <a title="174-tfidf-5" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>6 0.17766505 <a title="174-tfidf-6" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>7 0.12640774 <a title="174-tfidf-7" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>8 0.12510744 <a title="174-tfidf-8" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>9 0.11516701 <a title="174-tfidf-9" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>10 0.11216748 <a title="174-tfidf-10" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>11 0.10931122 <a title="174-tfidf-11" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>12 0.10059545 <a title="174-tfidf-12" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>13 0.09103664 <a title="174-tfidf-13" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>14 0.090209231 <a title="174-tfidf-14" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>15 0.089878254 <a title="174-tfidf-15" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>16 0.083601885 <a title="174-tfidf-16" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>17 0.081317134 <a title="174-tfidf-17" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>18 0.076026104 <a title="174-tfidf-18" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>19 0.070783734 <a title="174-tfidf-19" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>20 0.069708236 <a title="174-tfidf-20" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.235), (1, 0.099), (2, 0.01), (3, -0.174), (4, 0.295), (5, 0.075), (6, 0.361), (7, -0.025), (8, 0.155), (9, 0.075), (10, -0.107), (11, 0.081), (12, -0.058), (13, 0.107), (14, 0.008), (15, 0.051), (16, -0.179), (17, 0.052), (18, -0.072), (19, 0.077), (20, 0.033), (21, 0.127), (22, 0.078), (23, 0.06), (24, -0.081), (25, -0.021), (26, -0.013), (27, 0.029), (28, 0.116), (29, 0.054), (30, 0.011), (31, 0.006), (32, 0.037), (33, 0.041), (34, -0.001), (35, -0.046), (36, 0.123), (37, 0.013), (38, -0.071), (39, 0.033), (40, -0.011), (41, -0.063), (42, 0.031), (43, 0.002), (44, -0.023), (45, -0.035), (46, -0.039), (47, 0.05), (48, 0.072), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94065869 <a title="174-lsi-1" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>2 0.93652731 <a title="174-lsi-2" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>Author: Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan</p><p>Abstract: Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efﬁciently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on ﬁfty thousand points in less than half an hour on a single core. 1</p><p>3 0.87246317 <a title="174-lsi-3" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>Author: Kun Gai, Guangyun Chen, Chang-shui Zhang</p><p>Abstract: In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of norm constraints on combination coefﬁcients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method signiﬁcantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches. 1</p><p>4 0.78693724 <a title="174-lsi-4" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>5 0.74554408 <a title="174-lsi-5" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>6 0.66843957 <a title="174-lsi-6" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>7 0.53399616 <a title="174-lsi-7" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>8 0.52875149 <a title="174-lsi-8" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>9 0.5083673 <a title="174-lsi-9" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>10 0.43994433 <a title="174-lsi-10" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>11 0.38193291 <a title="174-lsi-11" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>12 0.35840157 <a title="174-lsi-12" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>13 0.34336874 <a title="174-lsi-13" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>14 0.33184725 <a title="174-lsi-14" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>15 0.31470746 <a title="174-lsi-15" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>16 0.31435418 <a title="174-lsi-16" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>17 0.31191465 <a title="174-lsi-17" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>18 0.30488276 <a title="174-lsi-18" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>19 0.297685 <a title="174-lsi-19" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>20 0.29311785 <a title="174-lsi-20" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.041), (17, 0.043), (27, 0.048), (30, 0.047), (35, 0.05), (45, 0.216), (50, 0.061), (52, 0.043), (60, 0.045), (65, 0.196), (77, 0.039), (78, 0.052), (80, 0.01), (90, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88687056 <a title="174-lda-1" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>Author: Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua B. Tenenbaum</p><p>Abstract: We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We extend the inﬁnite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p><p>same-paper 2 0.83022499 <a title="174-lda-2" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>3 0.77781612 <a title="174-lda-3" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>4 0.77322364 <a title="174-lda-4" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>5 0.76735348 <a title="174-lda-5" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>6 0.76725328 <a title="174-lda-6" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>7 0.76637489 <a title="174-lda-7" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>8 0.76609695 <a title="174-lda-8" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>9 0.76574558 <a title="174-lda-9" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>10 0.76573437 <a title="174-lda-10" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>11 0.76475602 <a title="174-lda-11" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>12 0.76410955 <a title="174-lda-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.76367813 <a title="174-lda-13" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>14 0.76294267 <a title="174-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.76267183 <a title="174-lda-15" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>16 0.76213419 <a title="174-lda-16" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>17 0.76209289 <a title="174-lda-17" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>18 0.76194644 <a title="174-lda-18" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>19 0.76178122 <a title="174-lda-19" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>20 0.7603718 <a title="174-lda-20" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
