<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-130" href="#">nips2010-130</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</h1>
<br/><p>Source: <a title="nips-2010-130-pdf" href="http://papers.nips.cc/paper/4090-interval-estimation-for-reinforcement-learning-algorithms-in-continuous-state-domains.pdf">pdf</a></p><p>Author: Martha White, Adam White</p><p>Abstract: The reinforcement learning community has explored many approaches to obtaining value estimates and models to guide decision making; these approaches, however, do not usually provide a measure of conﬁdence in the estimate. Accurate estimates of an agent’s conﬁdence are useful for many applications, such as biasing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing conﬁdence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agentenvironment interaction rarely satisﬁes traditional assumptions. Samples of valueestimates are dependent, likely non-normally distributed and often limited, particularly in early learning when conﬁdence estimates are pivotal. In this work, we investigate how to compute robust conﬁdences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute conﬁdence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applicability of our conﬁdence estimation algorithms with experiments on exploration, parameter estimation and tracking. 1</p><p>Reference: <a title="nips-2010-130-reference" href="../nips2010_reference/nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bootstrap', 0.548), ('dent', 0.465), ('interv', 0.25), ('reward', 0.227), ('reinforc', 0.226), ('con', 0.171), ('policy', 0.171), ('ag', 0.147), ('st', 0.144), ('expl', 0.124), ('block', 0.119), ('cartpol', 0.106), ('ucb', 0.106), ('qn', 0.084), ('nonst', 0.083), ('sars', 0.081), ('edgewor', 0.081), ('behavio', 0.074), ('navig', 0.07), ('episod', 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="130-tfidf-1" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>2 0.26448226 <a title="130-tfidf-2" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>3 0.23179106 <a title="130-tfidf-3" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>4 0.2297814 <a title="130-tfidf-4" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>5 0.22699709 <a title="130-tfidf-5" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>6 0.21922563 <a title="130-tfidf-6" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>7 0.21326728 <a title="130-tfidf-7" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>8 0.1928549 <a title="130-tfidf-8" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>9 0.17966028 <a title="130-tfidf-9" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>10 0.16663885 <a title="130-tfidf-10" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>11 0.1657588 <a title="130-tfidf-11" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>12 0.14393103 <a title="130-tfidf-12" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>13 0.14388926 <a title="130-tfidf-13" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>14 0.1400774 <a title="130-tfidf-14" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>15 0.13970527 <a title="130-tfidf-15" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>16 0.13203038 <a title="130-tfidf-16" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>17 0.12570354 <a title="130-tfidf-17" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>18 0.12517507 <a title="130-tfidf-18" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>19 0.1175077 <a title="130-tfidf-19" href="./nips-2010-Permutation_Complexity_Bound_on_Out-Sample_Error.html">205 nips-2010-Permutation Complexity Bound on Out-Sample Error</a></p>
<p>20 0.11730196 <a title="130-tfidf-20" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.232), (1, 0.306), (2, -0.027), (3, 0.01), (4, 0.022), (5, -0.025), (6, 0.015), (7, -0.033), (8, 0.023), (9, 0.118), (10, -0.004), (11, 0.022), (12, 0.047), (13, 0.027), (14, 0.129), (15, -0.066), (16, -0.075), (17, 0.04), (18, 0.087), (19, -0.013), (20, -0.06), (21, 0.073), (22, -0.058), (23, 0.088), (24, 0.024), (25, -0.025), (26, -0.02), (27, -0.054), (28, 0.043), (29, 0.087), (30, 0.02), (31, 0.038), (32, -0.052), (33, 0.037), (34, 0.009), (35, -0.089), (36, 0.071), (37, -0.023), (38, -0.076), (39, -0.07), (40, 0.069), (41, 0.04), (42, -0.001), (43, -0.078), (44, -0.084), (45, 0.001), (46, 0.056), (47, -0.022), (48, -0.085), (49, -0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93478811 <a title="130-lsi-1" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>2 0.79268628 <a title="130-lsi-2" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>3 0.79080182 <a title="130-lsi-3" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>4 0.71530265 <a title="130-lsi-4" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>5 0.69114512 <a title="130-lsi-5" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>6 0.65252775 <a title="130-lsi-6" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>7 0.65018302 <a title="130-lsi-7" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>8 0.64065754 <a title="130-lsi-8" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>9 0.63521701 <a title="130-lsi-9" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>10 0.63169539 <a title="130-lsi-10" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>11 0.6147446 <a title="130-lsi-11" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>12 0.59690893 <a title="130-lsi-12" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>13 0.57804358 <a title="130-lsi-13" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>14 0.57021701 <a title="130-lsi-14" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>15 0.55956012 <a title="130-lsi-15" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>16 0.5540852 <a title="130-lsi-16" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>17 0.54595983 <a title="130-lsi-17" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>18 0.51133037 <a title="130-lsi-18" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>19 0.44660923 <a title="130-lsi-19" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>20 0.44058329 <a title="130-lsi-20" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.099), (30, 0.038), (31, 0.016), (32, 0.457), (34, 0.075), (45, 0.065), (66, 0.011), (68, 0.115), (92, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99768704 <a title="130-lda-1" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>2 0.99700719 <a title="130-lda-2" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>3 0.99697733 <a title="130-lda-3" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>4 0.99375135 <a title="130-lda-4" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>5 0.98817456 <a title="130-lda-5" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>6 0.98476732 <a title="130-lda-6" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>7 0.98182333 <a title="130-lda-7" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>same-paper 8 0.97906828 <a title="130-lda-8" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>9 0.96705145 <a title="130-lda-9" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>10 0.96103144 <a title="130-lda-10" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>11 0.96047896 <a title="130-lda-11" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>12 0.95302081 <a title="130-lda-12" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>13 0.94452399 <a title="130-lda-13" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>14 0.94296676 <a title="130-lda-14" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>15 0.94119126 <a title="130-lda-15" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>16 0.94030178 <a title="130-lda-16" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>17 0.93939221 <a title="130-lda-17" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>18 0.93844736 <a title="130-lda-18" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>19 0.93727976 <a title="130-lda-19" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>20 0.93684268 <a title="130-lda-20" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
