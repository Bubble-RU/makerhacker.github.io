<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-154" href="#">nips2010-154</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</h1>
<br/><p>Source: <a title="nips-2010-154-pdf" href="http://papers.nips.cc/paper/4174-learning-sparse-dynamic-linear-systems-using-stable-spline-kernels-and-exponential-hyperpriors.pdf">pdf</a></p><p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>Reference: <a title="nips-2010-154-reference" href="../nips2010_reference/nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors  Alessandro Chiuso Department of Management and Engineering University of Padova Vicenza, Italy alessandro. [sent-1, score-0.671]
</p><p>2 it  Abstract We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. [sent-5, score-0.102]
</p><p>3 The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. [sent-6, score-0.635]
</p><p>4 Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. [sent-7, score-0.079]
</p><p>5 Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. [sent-8, score-0.108]
</p><p>6 able to extract from the large number of subsystems entering the system description just that subset which inﬂuences signiﬁcantly the system output. [sent-13, score-0.154]
</p><p>7 Such sparsity principle permeates many well known techniques in machine learning and signal processing such as feature selection, selective shrinkage and compressed sensing [3, 4]. [sent-14, score-0.048]
</p><p>8 Some novel estimation techniques inducing sparse models have been recently proposed. [sent-25, score-0.076]
</p><p>9 it favors solutions with many zero entries at the expense of few large components. [sent-31, score-0.051]
</p><p>10 Extensions of this procedure for group selection include Group Lasso and Group LAR (GLAR) [11] where the sum of the Euclidean norms of each group (in place of the absolute value of the single components) is used. [sent-35, score-0.08]
</p><p>11 Theoretical analyses of these approaches and connections with the multiple kernel learning problem can be found in [12, 13]. [sent-36, score-0.077]
</p><p>12 However, most of the work has been done in the “static” scenario while very little, with some exception [14, 15], can be found regarding the identiﬁcation of dynamic systems. [sent-37, score-0.06]
</p><p>13 In this paper we adopt a Bayesian point of view to prediction and identiﬁcation of sparse linear systems. [sent-38, score-0.073]
</p><p>14 Our starting point is the new identiﬁcation paradigm developed in [16] that relies on nonparametric estimation of impulse responses (see also [17] for extensions to predictor estimation). [sent-39, score-0.716]
</p><p>15 Rather than postulating ﬁnite-dimensional structures for the system transfer function, e. [sent-40, score-0.077]
</p><p>16 ARX, ARMAX or Laguerre [1], the system impulse response is searched for within an inﬁnite-dimensional space. [sent-42, score-0.502]
</p><p>17 In particular, working under the framework of Gaussian regression [18], in [16] the system impulse response is modeled as a Gaussian process whose autocovariance is the so called stable spline kernel that includes the BIBO stability constraint. [sent-44, score-1.331]
</p><p>18 In this paper, we extend this nonparametric paradigm to the design of optimal linear predictors for sparse systems. [sent-45, score-0.105]
</p><p>19 Without loss of generality, analysis is restricted to MISO systems so that we interpret the predictor as a system with m + 1 inputs (given by past outputs and inputs) and one output (output predictions). [sent-46, score-0.236]
</p><p>20 Thus, predictor design amounts to estimating m + 1 impulse responses modeled as realizations of Gaussian processes. [sent-47, score-0.758]
</p><p>21 We set their autocovariances to stable spline kernels with different (and unknown) scale factors which are assigned exponential hyperpriors having a common hypervariance. [sent-48, score-0.671]
</p><p>22 In this way, while GLAR uses the sum of the 1 norms of the single impulse responses, our approach favors sparsity through an 1 penalty on kernel hyperparameters. [sent-49, score-0.506]
</p><p>23 Inducing sparsity by hyperpriors is an important feature of our approach. [sent-50, score-0.103]
</p><p>24 In fact, this permits to obtain the marginal posterior of the hyperparameters in closed form and hence also their estimates in a robust way. [sent-51, score-0.066]
</p><p>25 Once the kernels are selected, the impulse responses are obtained by a convex Tikhonov-type variational problem. [sent-52, score-0.54]
</p><p>26 In Section 2, the nonparametric approach to system identiﬁcation introduced in [16] is brieﬂy reviewed. [sent-55, score-0.12]
</p><p>27 Section 3 reports the statement of the predictor estimation problem while Section 4 describes the new Bayesian model for system identiﬁcation of sparse linear systems. [sent-56, score-0.266]
</p><p>28 In Section 5, a numerical algorithm which returns the unknown components of the prior and the estimates of predictor and system impulse responses is derived. [sent-57, score-0.754]
</p><p>29 Under the stated assumptions and according to the representer theorem [21], the minimizer of (1) is the sum of N basis functions deﬁned by the kernel ﬁltered by the operators {Γt }, with coefﬁcients obtainable solving a linear system of equations. [sent-63, score-0.179]
</p><p>30 It corresponds to the minimum variance estimate of f when f is a zero-mean Gaussian process with autocovariance K and {yt − Γt [f ]} is white Gaussian noise independent of f [22]. [sent-65, score-0.275]
</p><p>31 (u)+ =  u if u ≥ 0 0 if u < 0  (2)  This is the autocovariance associated with the Bayesian interpretation of p-th order smoothing splines [23]. [sent-72, score-0.216]
</p><p>32 In particular, when p = 2, one obtains the cubic spline kernel. [sent-73, score-0.38]
</p><p>33 2  Kernels for system identiﬁcation  In the system identiﬁcation scenario, the main drawback of the kernel (2) is that it does not account for impulse response stability. [sent-75, score-0.676]
</p><p>34 1 (left) which displays 100 realizations drawn from a zero-mean Gaussian process with autocovariance proportional to W2 . [sent-78, score-0.384]
</p><p>35 One of the key contributions of [16] is the deﬁnition of a kernel speciﬁcally suited to linear system identiﬁcation leading to an estimator with favorable bias and variance properties. [sent-79, score-0.186]
</p><p>36 In particular, it is easy to see that if the autocovariance of f is proportional to Wp , the variance of f (t) is zero at t = 0 and tends to ∞ as t increases. [sent-80, score-0.314]
</p><p>37 However, if f represents a stable impulse response, we would rather let it have a ﬁnite variance at t = 0 which goes exponentially to zero as t tends to ∞. [sent-81, score-0.574]
</p><p>38 This property can be ensured by considering autocovariances proportional to the class of kernels given by Kp (s, t) = Wp (e−βs , e−βt ),  s, t ∈ R+  (3)  where β is a positive scalar governing the decay rate of the variance [16]. [sent-82, score-0.198]
</p><p>39 In practice, β will be unknown so that it is convenient to treat it as a hyperparameter to be estimated from data. [sent-83, score-0.056]
</p><p>40 In view of (3), if p = 2 the autocovariance becomes the Stable Spline kernel introduced in [16]: K2 (t, τ ) =  e−3β max(t,τ ) e−β(t+τ ) e−β max(t,τ ) − 2 6  (4)  Proposition 1 [16] Let f be zero-mean Gaussian with autocovariance K2 . [sent-84, score-0.509]
</p><p>41 Then, with probability one, the realizations of f are continuous impulse responses of BIBO stable dynamic systems. [sent-85, score-0.745]
</p><p>42 1 (middle) which displays 100 realizations drawn from a zero-mean Gaussian process with autocovariance proportional to K2 with β = 0. [sent-87, score-0.384]
</p><p>43 3  Statement of the system identiﬁcation problem  In what follows, vectors are column vectors, unless other is speciﬁed. [sent-89, score-0.077]
</p><p>44 We denote with {yt }t∈Z , yt ∈ R and {ut }t∈Z , ut ∈ Rm a pair of jointly stationary stochastic processes which represent, 3  Figure 2: Bayesian network describing the new nonparametric model for identiﬁcation of sparse linear systems where y l := [yl−1 , yl−2 , . [sent-90, score-0.382]
</p><p>45 respectively, the output and input of an unknown time-invariant dynamical system. [sent-97, score-0.058]
</p><p>46 With some abuse of notation, yt will both denote a random variable (from the random process {yt }t∈Z ) and its sample value. [sent-98, score-0.178]
</p><p>47 Our aim is to identify a linear dynamical system of the form ∞  yt =  ∞  fi ut−i + i=1  gi et−i  (5)  i=0  from {ut , yt }t=1,. [sent-100, score-0.462]
</p><p>48 In (5), fi ∈ R1×m and gi ∈ R are matrix and scalar coefﬁcients of the unknown system impulse responses while et is the Gaussian innovation sequence. [sent-103, score-0.614]
</p><p>49 Following the Prediction Error Minimization framework, identiﬁcation of the dynamical system (5) is converted in estimation of the associated one-step-ahead predictor. [sent-104, score-0.126]
</p><p>50 Letting hk := {hk }t∈N denote t the predictor impulse response associated with the k-th input {uk }t∈Z , one has t yt =  m k=1  ∞ i=1  hk uk + i t−i  ∞ i=1  hm+1 yt−i + et i  (6)  where hm+1 := {hm+1 }t∈N is the impulse response modeling the autoregressive component of the t predictor. [sent-105, score-1.793]
</p><p>51 As is well known, if the joint spectrum of {yt } and {ut } is bounded away from zero, each hk is (BIBO) stable. [sent-106, score-0.272]
</p><p>52 Under such assumption, our aim is to estimate the predictor impulse responses, in a scenario where the number of measurements N is not large, as compared with m, and many measured inputs could be irrelevant for the prediction of yt . [sent-107, score-0.796]
</p><p>53 1  A Bayesian model for identiﬁcation of sparse linear systems Prior for predictor impulse responses  We model {hk } as independent Gaussian processes whose kernels share the same hyperparameters apart from the scale factors. [sent-110, score-0.778]
</p><p>54 In particular, each hk is proportional to the convolution of a zeromean Gaussian process, with autocovariance given by the sampled version of K2 , with a parametric impulse response r, used to capture dynamics hardly represented by a smooth process, e. [sent-111, score-0.971]
</p><p>55 1 (right panel) shows some realizations (with samples linearly interpolated) drawn from a discrete-time zero-mean normal process with autocovariance given by K2 enriched by θ = [1 0. [sent-116, score-0.353]
</p><p>56 Notice that, in this way, an oscillatory behavior is introduced in the realizations 4  √ by enriching the Stable Spline kernel with the poles −0. [sent-118, score-0.229]
</p><p>57 The kernel of hk deﬁned by K2 and (7) is denoted by K : N × N → R and depends on β, θ. [sent-121, score-0.349]
</p><p>58 Thus, letting E[·] denote the expectation operator, the prior model on the impulse responses is given by E[hk hk ] = λ2 K(j, i; θ, β), j i k 4. [sent-122, score-0.76]
</p><p>59 , m + 1,  i, j ∈ N  Hyperprior for the hyperparameters  The noise variance σ 2 will always be estimated via a preliminary step using a low-bias ARX model, as described in [24]. [sent-126, score-0.068]
</p><p>60 The hyperparameters β, θ and {λk } are instead modeled as mutually independent random vectors. [sent-128, score-0.065]
</p><p>61 , N and i ∈ N, we have: [Ak ]ji = uk j−i  for  k = 1, . [sent-147, score-0.048]
</p><p>62 , m,  [Am+1 ]ji  = yj−i  (8)  In view of (6), using notation of ordinary algebra to handle inﬁnite-dimensional objects with each hk interpreted as an inﬁnite-dimensional column vector, it holds that m  y+  Ak (uk )hk + Am+1 (y + , y - )hm+1 + e  =  (9)  k=1  where y + = [y1 , y2 , . [sent-150, score-0.272]
</p><p>63 the past y - is assumed not to carry information on the predictor impulse responses and the hyperparameters. [sent-165, score-0.624]
</p><p>64 The dependence on y - is hereafter omitted as well as dependence of the {Ak } on y + or uk . [sent-168, score-0.048]
</p><p>65 Then, if {yt } and {ut } are zero mean, ﬁnite variance stationary stochastic processes, each operator {Ak } is almost surely (a. [sent-171, score-0.129]
</p><p>66 1  Estimation of the hyper-parameters and the predictor impulse responses Estimation of the hyper-parameters  We estimate the hyperparameter vector ζ by optimizing its marginal posterior, i. [sent-175, score-0.651]
</p><p>67 Proposition 2 Let {yt } and {ut } be zero mean, ﬁnite variance stationary stochastic processes. [sent-182, score-0.084]
</p><p>68 , m + 1) (12) k  ζ  5  where J is almost surely well deﬁned pointwise and given by J(y + ; ζ) =  1 1 log det[2πV [y + ]] + (y + )T (V [y + ])−1 y + + γ 2 2  with V [y + ] = σ 2 IN +  m+1 k=1  m+1  λk − log(γ)  (13)  k=1  λk Ak KAT . [sent-188, score-0.045]
</p><p>69 k  The objective (13), including the 1 penalty on {λk }, is a Bayesian modiﬁed version of that connected with multiple kernel learning, see Section 3 in [25]. [sent-189, score-0.077]
</p><p>70 Below, we describe a scheme that achieves a suboptimal solution just solving an optimization problem in R4 related to the reduced Bayesian model of Fig. [sent-192, score-0.05]
</p><p>71 2  Estimation of the predictor impulse responses for known ζ  ˆ Let HK be the RKHS associated with K, with norm · HK . [sent-208, score-0.624]
</p><p>72 Proposition 3 Under the same assumptions of Proposition 2, almost surely we have m+1  ˆ {hk }m+1 = arg k=1  y+ −  min  {f k ∈HK }m+1 k=1  m+1  Ak f k k=1  2  + σ2 k=1  fk 2 K H λ2 k  where · is the Euclidean norm. [sent-211, score-0.045]
</p><p>73 Moreover, almost surely we also have for k = 1, . [sent-212, score-0.045]
</p><p>74 , m + 1 −1  m+1  ˆ hk = λ2 KAT c, k k  c=  2  λk Ak KAT k  σ IN +  y+  (14)  k=1  After obtaining the estimates of the {hk }, simple formulas can then be used to derive the system impulse responses f and g in (5) and hence also the k-step ahead predictors, see [1] for details. [sent-215, score-0.874]
</p><p>75 6  Numerical experiments  We consider two Monte Carlo studies of 200 runs where at any run an ARMAX linear system with 15 inputs is generated as follows • the number of hk different from zero is randomly drawn from the set {0, 1, 2, . [sent-216, score-0.403]
</p><p>76 The system and the predictor poles are restricted to have modulus less than 0. [sent-221, score-0.26]
</p><p>77 In the ﬁrst Monte Carlo experiment, at any run an identiﬁcation data set of size 500 and a test set of size 1000 is generated using independent realizations of white noise as input. [sent-223, score-0.154]
</p><p>78 m using, respectively, independent realizations of a random Gaussian signal with band [0, 0. [sent-226, score-0.105]
</p><p>79 4%  Table 1: Percentage of the hk equal to zero correctly set to zero by the employed estimator. [sent-244, score-0.334]
</p><p>80 the number of null hk ) is determined using the ﬁrst 2/3 of the 500 available data as training set and the remaining part as validation data (the use of Cp statistics does not provide better results in this case). [sent-248, score-0.272]
</p><p>81 m function of the MATLAB System Identiﬁcation Toolbox [26], equipped with an oracle that, at every run, knows which predictor impulse response are zero and, having access to the test set, selects those model orders that provide the best prediction performance. [sent-251, score-0.75]
</p><p>82 For computational reasons, the number of estimated predictor coefﬁcients is 40. [sent-259, score-0.136]
</p><p>83 Percentage of the impulse responses equal to zero correctly set to zero by the estimator. [sent-265, score-0.55]
</p><p>84 k-step-ahead Coefﬁcient of Determination, denoted by CODk , quantifying how much of the test set variance is explained by the forecast. [sent-267, score-0.079]
</p><p>85 It is computed at each run as CODk := 1−  2 RM Sk 1 1000  1000 test i=1 (yt  − yt )2 ¯test  ,  RM Sk :=  1 1000  1000 test (yt − yt|t−k )2 ˆtest t=1  (15) 7  Average COD  1 Stable Spline Suboptimal Stable Spline PEM + Oracle GLAR  0. [sent-268, score-0.222]
</p><p>86 average coefﬁcient of determination relative to k-step ahead prediction, obtained during the Monte Carlo study #1 (top) and #2 (bottom) using PEM+Oracle (•), GLAR (∗) Stable Spline based on the full (◦) and the reduced (+) Bayesian model of Fig. [sent-276, score-0.058]
</p><p>87 test ˆtest where y test is the sample mean of the test set data {yt }1000 and yt|t−k is the k-step ¯ t=1 ahead prediction computed using the estimated model. [sent-278, score-0.143]
</p><p>88 3 showing the boxplots of the 200 values of COD1 obtained by 4 of the employed estimators during the two Monte Carlo studies. [sent-285, score-0.055]
</p><p>89 Table 1 reports the percentage of the predictor impulse responses equal to zero correctly estimated as zero by the estimators. [sent-288, score-0.717]
</p><p>90 Remarkably, in all the cases the Stable Spline estimators not only outperform GLAR but the achieved percentage is close to 99%. [sent-289, score-0.054]
</p><p>91 4 displays CODk as a function of the prediction horizon obtained during the Monte Carlo study #1 (top) and #2 (bottom). [sent-292, score-0.068]
</p><p>92 7  Conclusions  We have shown how identiﬁcation of large sparse dynamic systems can beneﬁt from the ﬂexibility of kernel methods. [sent-295, score-0.136]
</p><p>93 To this aim, we have extended a recently proposed nonparametric paradigm to identify sparse models via prediction error minimization. [sent-296, score-0.145]
</p><p>94 Predictor impulse responses are modeled as zero-mean Gaussian processes using stable spline kernels encoding the BIBO-stability constraint and sparsity is induced by exponential hyperpriors on their scale factors. [sent-297, score-1.186]
</p><p>95 The method compares much favorably with GLAR, with its performance close to that achievable combining PEM with an oracle which exploits the test set in order to select the best model order. [sent-298, score-0.088]
</p><p>96 Model selection and estimation in regression with grouped variables. [sent-353, score-0.05]
</p><p>97 Consistency of the group lasso and multiple kernel learning. [sent-358, score-0.129]
</p><p>98 Regression coefﬁcient and autoregressive order shrinkage and selection via the lasso. [sent-375, score-0.091]
</p><p>99 Support vector machines, reproducing kernel Hilbert spaces and randomized GACV. [sent-405, score-0.103]
</p><p>100 Kernel machines with two layers and multiple kernel learning. [sent-435, score-0.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pem', 0.431), ('impulse', 0.385), ('spline', 0.355), ('glar', 0.274), ('hk', 0.272), ('autocovariance', 0.216), ('yt', 0.178), ('armax', 0.137), ('predictor', 0.136), ('bic', 0.134), ('stable', 0.126), ('identi', 0.121), ('codk', 0.118), ('realizations', 0.105), ('responses', 0.103), ('ak', 0.08), ('hyperpriors', 0.079), ('bibo', 0.078), ('padova', 0.078), ('system', 0.077), ('kernel', 0.077), ('ut', 0.074), ('oracle', 0.066), ('wp', 0.059), ('arx', 0.059), ('autocovariances', 0.059), ('kat', 0.059), ('pillonetto', 0.059), ('bayesian', 0.055), ('cod', 0.052), ('lar', 0.052), ('kernels', 0.052), ('hm', 0.051), ('uk', 0.048), ('poles', 0.047), ('surely', 0.045), ('carlo', 0.043), ('monte', 0.043), ('nonparametric', 0.043), ('aic', 0.042), ('response', 0.04), ('prediction', 0.04), ('chiuso', 0.039), ('ahead', 0.037), ('autoregressive', 0.037), ('hyperparameters', 0.036), ('proportional', 0.035), ('automatica', 0.034), ('gaussian', 0.034), ('scenario', 0.034), ('processes', 0.033), ('sparse', 0.033), ('variance', 0.032), ('enriched', 0.032), ('boxplots', 0.032), ('percentage', 0.031), ('zero', 0.031), ('cation', 0.031), ('permits', 0.03), ('gp', 0.03), ('selection', 0.03), ('equipped', 0.03), ('modeled', 0.029), ('suboptimal', 0.029), ('paradigm', 0.029), ('dynamical', 0.029), ('proposition', 0.029), ('unknown', 0.029), ('displays', 0.028), ('yl', 0.028), ('lasso', 0.027), ('coef', 0.027), ('hyperparameter', 0.027), ('white', 0.027), ('stability', 0.026), ('reproducing', 0.026), ('dynamic', 0.026), ('roots', 0.026), ('cubic', 0.025), ('representer', 0.025), ('quantifying', 0.025), ('group', 0.025), ('shrinkage', 0.024), ('numerical', 0.024), ('sparsity', 0.024), ('parametric', 0.023), ('inputs', 0.023), ('italy', 0.023), ('estimators', 0.023), ('inducing', 0.023), ('test', 0.022), ('toolbox', 0.022), ('rkhs', 0.021), ('reduced', 0.021), ('stationary', 0.021), ('drawback', 0.02), ('estimation', 0.02), ('scalar', 0.02), ('favors', 0.02), ('matlab', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="154-tfidf-1" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>2 0.090129688 <a title="154-tfidf-2" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>3 0.086850308 <a title="154-tfidf-3" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>Author: Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan</p><p>Abstract: Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efﬁciently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on ﬁfty thousand points in less than half an hour on a single core. 1</p><p>4 0.076107353 <a title="154-tfidf-4" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>5 0.068826377 <a title="154-tfidf-5" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>6 0.06253992 <a title="154-tfidf-6" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>7 0.06200562 <a title="154-tfidf-7" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>8 0.061983421 <a title="154-tfidf-8" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>9 0.061317567 <a title="154-tfidf-9" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>10 0.05897684 <a title="154-tfidf-10" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>11 0.058873579 <a title="154-tfidf-11" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>12 0.058700655 <a title="154-tfidf-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.058645137 <a title="154-tfidf-13" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>14 0.05541515 <a title="154-tfidf-14" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>15 0.052903511 <a title="154-tfidf-15" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>16 0.050519064 <a title="154-tfidf-16" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>17 0.048608765 <a title="154-tfidf-17" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>18 0.047978245 <a title="154-tfidf-18" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>19 0.047849953 <a title="154-tfidf-19" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>20 0.047562182 <a title="154-tfidf-20" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.035), (2, 0.024), (3, 0.046), (4, 0.07), (5, -0.005), (6, 0.086), (7, 0.047), (8, -0.043), (9, 0.01), (10, -0.065), (11, 0.039), (12, -0.041), (13, 0.001), (14, -0.017), (15, 0.02), (16, -0.024), (17, 0.03), (18, -0.014), (19, 0.051), (20, -0.058), (21, 0.115), (22, 0.008), (23, 0.039), (24, -0.034), (25, 0.023), (26, 0.032), (27, 0.039), (28, 0.089), (29, 0.002), (30, 0.072), (31, -0.036), (32, -0.009), (33, 0.028), (34, 0.032), (35, -0.012), (36, 0.027), (37, -0.032), (38, -0.0), (39, 0.009), (40, 0.052), (41, 0.024), (42, -0.005), (43, -0.071), (44, 0.03), (45, -0.004), (46, 0.011), (47, 0.012), (48, 0.017), (49, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9239974 <a title="154-lsi-1" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>2 0.59253395 <a title="154-lsi-2" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>Author: Fabian L. Wauthier, Michael I. Jordan</p><p>Abstract: Heavy-tailed distributions are often used to enhance the robustness of regression and classiﬁcation methods to outliers in output space. Often, however, we are confronted with “outliers” in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed stochastic processes (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classiﬁcation estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs when the marginals of the heavy-tailed process have sufﬁciently heavy tails. The analysis is complemented by experiments on biological data which indicate signiﬁcant improvements of estimates in sparse regions while producing competitive results in dense regions. 1</p><p>3 0.59015006 <a title="154-lsi-3" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>Author: Zhaonan Sun, Nawanol Ampornpunt, Manik Varma, S.v.n. Vishwanathan</p><p>Abstract: Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efﬁciently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efﬁciency and is signiﬁcantly faster than state-of-the-art specialised p-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on ﬁfty thousand points in less than half an hour on a single core. 1</p><p>4 0.58021253 <a title="154-lsi-4" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>5 0.56676626 <a title="154-lsi-5" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>6 0.56479204 <a title="154-lsi-6" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>7 0.55517423 <a title="154-lsi-7" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>8 0.53885657 <a title="154-lsi-8" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>9 0.53364074 <a title="154-lsi-9" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>10 0.52200454 <a title="154-lsi-10" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>11 0.51989758 <a title="154-lsi-11" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>12 0.51721793 <a title="154-lsi-12" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>13 0.50427061 <a title="154-lsi-13" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>14 0.49649534 <a title="154-lsi-14" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>15 0.4909344 <a title="154-lsi-15" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>16 0.49007985 <a title="154-lsi-16" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>17 0.47699592 <a title="154-lsi-17" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>18 0.47581679 <a title="154-lsi-18" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>19 0.47275022 <a title="154-lsi-19" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>20 0.47124439 <a title="154-lsi-20" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.043), (17, 0.013), (27, 0.064), (30, 0.045), (35, 0.029), (45, 0.145), (50, 0.07), (52, 0.045), (60, 0.014), (77, 0.03), (78, 0.372), (90, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78657985 <a title="154-lda-1" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>Author: Thomas Peel, Sandrine Anthoine, Liva Ralaivola</p><p>Abstract: We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernsteintype inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efﬁcient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions. 1</p><p>2 0.72974867 <a title="154-lda-2" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>Author: Jie Luo, Francesco Orabona</p><p>Abstract: In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classiﬁers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difﬁcult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efﬁcient algorithm to solve it. Experiments conducted on artiﬁcial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.</p><p>same-paper 3 0.72897613 <a title="154-lda-3" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>4 0.66916782 <a title="154-lda-4" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>Author: Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashingbased solutions. Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our ﬁrst method’s preprocessing stage is more efﬁcient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. 1</p><p>5 0.62632042 <a title="154-lda-5" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>6 0.59410071 <a title="154-lda-6" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>7 0.55318141 <a title="154-lda-7" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>8 0.54908925 <a title="154-lda-8" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>9 0.53588754 <a title="154-lda-9" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>10 0.5345965 <a title="154-lda-10" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>11 0.52788645 <a title="154-lda-11" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>12 0.5211575 <a title="154-lda-12" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>13 0.50545335 <a title="154-lda-13" href="./nips-2010-Co-regularization_Based_Semi-supervised_Domain_Adaptation.html">47 nips-2010-Co-regularization Based Semi-supervised Domain Adaptation</a></p>
<p>14 0.50089747 <a title="154-lda-14" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>15 0.49782684 <a title="154-lda-15" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>16 0.49234015 <a title="154-lda-16" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>17 0.49152187 <a title="154-lda-17" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>18 0.49057671 <a title="154-lda-18" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>19 0.48926049 <a title="154-lda-19" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>20 0.48894739 <a title="154-lda-20" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
