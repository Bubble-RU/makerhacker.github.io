<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-12" href="#">nips2010-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</h1>
<br/><p>Source: <a title="nips-2010-12-pdf" href="http://papers.nips.cc/paper/3974-a-primal-dual-algorithm-for-group-sparse-regularization-with-overlapping-groups.pdf">pdf</a></p><p>Author: Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco</p><p>Abstract: We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. 1</p><p>Reference: <a title="nips-2010-12-reference" href="../nips2010_reference/nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A primal-dual algorithm for group sparse regularization with overlapping groups  Silvia Villa DISI- Universit` di Genova a villa@dima. [sent-1, score-0.693]
</p><p>2 it  Abstract We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. [sent-8, score-0.467]
</p><p>3 In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. [sent-9, score-0.864]
</p><p>4 This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. [sent-11, score-0.073]
</p><p>5 The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. [sent-12, score-0.19]
</p><p>6 1  Introduction  Sparsity has become a popular way to deal with small samples of high dimensional data and, in a broad sense, refers to the possibility of writing the solution in terms of a few building blocks. [sent-13, score-0.109]
</p><p>7 In particular, regularization based on 1 type penalties is a powerful approach for dealing with the problem of variable selection, since it provides sparse solutions by minimizing a convex functional. [sent-15, score-0.249]
</p><p>8 The success of 1 regularization motivated exploring different kinds of sparsity properties for (generalized) linear models, exploiting available a priori information, which restricts the admissible sparsity patterns of the solution. [sent-16, score-0.169]
</p><p>9 An example of a sparsity pattern is when the input variables are partitioned into groups (known a priori), and the goal is to estimate a sparse model where variables belonging to the same group are either jointly selected or discarded. [sent-17, score-0.692]
</p><p>10 This problem can be solved by regularizing with the group- 1 penalty, also known as group lasso penalty, which is the sum, over the groups, of the euclidean norms of the coefﬁcients restricted to each group. [sent-18, score-0.391]
</p><p>11 A possible generalization of group lasso is to consider groups of variables which can be potentially overlapping, and the goal is to estimate a model which support is the union of groups. [sent-19, score-0.689]
</p><p>12 This is a common situation in bioinformatics (especially in the context of high-throughput data such as gene expression and mass spectrometry data), where problems are characterized by a very low number of samples with several thousands of variables. [sent-20, score-0.078]
</p><p>13 Largely motivated by applications in bioinformatics, a new type of penalty is proposed in [12], which is shown to give better 1  performances than simple 1 regularization. [sent-22, score-0.106]
</p><p>14 As already mentioned in [12], though very simple, such an implementation does not scale to large datasets, when the groups have signiﬁcant overlap, and a more scalable algorithm with no data duplication is needed. [sent-24, score-0.352]
</p><p>15 For this reason we propose an alternative optimization approach to solve the group lasso problem with overlap. [sent-25, score-0.419]
</p><p>16 Our method does not require explicit replication of the features and is thus more appropriate to deal with high dimensional problems with large groups overlap. [sent-26, score-0.366]
</p><p>17 The dual problem can then be solved via Bertsekas’ projected Newton method [7]. [sent-28, score-0.115]
</p><p>18 We recall that a particular overlapping structure is the hierarchical structure, where the overlap between groups is limited to inclusion of a descendant in its ancestors. [sent-29, score-0.551]
</p><p>19 In this case the CAP penalty [24] can be used for model selection, as it has been done in [2, 13], but ancestors are forced to be selected when any of their descendant are selected. [sent-30, score-0.197]
</p><p>20 Thanks to the nested structure, the proximity operator of the penalty term can be computed exactly in a ﬁnite number of steps [14]. [sent-31, score-0.327]
</p><p>21 Finally it is worth noting that the penalty analyzed here can be applied also to hierarchical group lasso. [sent-33, score-0.337]
</p><p>22 Differently from [2, 13] selection of ancestors is no longer enforced. [sent-34, score-0.075]
</p><p>23 In Section 2 we recall the group lasso functional for overlapping groups and set some notations. [sent-36, score-0.73]
</p><p>24 Finally in Section 4 we present some numerical experiments comparing running time of our algorithm with state-of-the-art techniques. [sent-38, score-0.076]
</p><p>25 Then, for any differentiable function f : RB → R, we denote by ∂r f its partial derivative with respect to variables r, and by f = (∂r f )B its gradient. [sent-45, score-0.102]
</p><p>26 r=1 We are now ready to cast group 1 regularization with overlapping groups as the following variational problem. [sent-46, score-0.621]
</p><p>27 Given a training set {(xi , yi )n } ∈ (X × Y )n , a dictionary (ψj )d , and B subsets i=1 j=1 of variables G = {Gr }B with Gr ⊂ {1, . [sent-47, score-0.11]
</p><p>28 The penalty term ΩGoverlap : Rd → R+ is lower semicontinuous, convex, and one-homogeneous, (ΩGoverlap(λβ) = λΩGoverlap(β), ∀β ∈ Rd and λ ∈ R+ ), and is deﬁned as B  ΩG overlap (β) =  inf  P (v1 ,. [sent-52, score-0.237]
</p><p>29 r=1  The functional ΩGoverlap was introduced in [12] as a generalization of the group lasso penalty to allow overlapping groups, while maintaining the group lasso property of enforcing sparse solutions which support is a union of groups. [sent-56, score-1.077]
</p><p>30 When groups do not overlap, ΩGoverlap reduces to the group lasso 1  Note our analysis would immediately apply to other loss functions, e. [sent-57, score-0.626]
</p><p>31 Note that, as pointed out in [12], using r=1 β Gr as generalization of the group lasso penalty leads to a solution which support is the complement of the union of groups. [sent-61, score-0.533]
</p><p>32 3  The GLO-pridu Algorithm  If one needs to solve problem (1) for high dimensional data, the use of standard second-order methods such as interior-point methods is precluded (see for instance [6]), since they need to solve large systems of linear equations to compute the Newton steps. [sent-63, score-0.153]
</p><p>33 On the other hand, ﬁrst order methods inspired to Nesterov’s seminal paper [19] (see also [18]) and based on proximal methods already proved to be a computationally efﬁcient alternative in many machine learning applications [9, 21]. [sent-64, score-0.16]
</p><p>34 Due to one-homogeneity of ΩGoverlap, the proximity operator associated τ τ to σ ΩGoverlap reduces to the identity minus the projection onto the subdifferential of σ ΩGoverlap at the origin, which is a closed and convex set. [sent-67, score-0.408]
</p><p>35 We will denote such a projection as πτ /σK , where K = ∂ΩGoverlap(0). [sent-68, score-0.095]
</p><p>36 (3) p As it happens for other accelerations of the basic forward-backward splitting algorithm such as [19, 6, 4], convergence of the sequence β p is no longer guaranteed unless strong convexity is assumed. [sent-70, score-0.089]
</p><p>37 2  The projection  Note that the proximity operator of the penalty ΩGoverlap does not admit a closed form and must be computed approximatively. [sent-74, score-0.422]
</p><p>38 In fact the projection on the convex set K = ∂ΩGoverlap(0) = {v ∈ Rd , v Gr ≤ 1 for r = 1, . [sent-75, score-0.145]
</p><p>39 cannot be decomposed group-wise, as in standard group 1 regularization, which proximity operator resolves to a group-wise soft-thresholding operator (see Eq. [sent-79, score-0.504]
</p><p>40 Nonetheless, the following lemma shows that, when evaluating the projection, πK , we can restrict ourselves to a subset of ˆ ˆ ˆ B = |G| ≤ B active groups. [sent-81, score-0.115]
</p><p>41 This equivalence is crucial for speeding up the algorithm, in fact B is the number of selected groups which is small if one is interested in sparse solutions. [sent-82, score-0.276]
</p><p>42 , d}, and τ > 0, the projection onto the r=1 convex set τ K with K = {v ∈ Rd , v Gr ≤ 1 for r = 1, . [sent-86, score-0.187]
</p><p>43 (4)  The proof (given in the supplementary material) is based on the fact that the convex set τ K is the ˆ intersection of cylinders that are all centered on a coordinate subspace. [sent-91, score-0.097]
</p><p>44 Since B is typically much smaller than d, it is convenient to solve the dual problem associated to (4). [sent-92, score-0.091]
</p><p>45 , d}, and τ > 0, the projection onto the r=1 convex set τ K with K = {v ∈ Rd , v Gr ≤ τ for r = 1, . [sent-96, score-0.187]
</p><p>46 , GB }, and 1r,j is 1 if j belongs to group Gr and 0 otherwise. [sent-105, score-0.197]
</p><p>47 Equation (6) is the dual problem associated to (4), and, since strong duality holds, the minimum of (4) is equal to the maximum of the dual problem, which can be efﬁciently solved via Bertsekas’ projected Newton method described in [7], and here reported as Algorithm 1. [sent-106, score-0.178]
</p><p>48 3  Computing the regularization path  In Algorithm 2 we report the complete Group Lasso with Overlap primal-dual (GLO-pridu) scheme for computing the regularization path, i. [sent-113, score-0.288]
</p><p>49 the set of solutions corresponding to different values of the regularization parameter τ1 > . [sent-115, score-0.129]
</p><p>50 A similar warm starting is applied to the inner iteration, where at the p-th step λinit is determined by the solution of the (p−1)-th projection. [sent-120, score-0.078]
</p><p>51 The main ˜ advantage of the above formulation relies on the possibility of using any state-of-the-art optimization procedure for group lasso. [sent-154, score-0.197]
</p><p>52 In terms of proximal methods, a possible solution is given by Algorithm 3, where Sτ /σ is the proximity operator of the new penalty, and can be computed exactly as τ ˜ ˜ ˜ ˜ ˜ Sτ /σ (β) = ||β||Gr − βj , for j ∈ Gr , for r = 1, . [sent-155, score-0.417]
</p><p>53 In practice this does not yield any advantage, since the identiﬁcation of the active groups has the same computational cost of the thresholding itself. [sent-160, score-0.322]
</p><p>54 5  Computational issues  For both GL-prox and GLO-pridu, the complexity of one iteration is the sum of the complexity of computing the gradient of the data term and the complexity of computing the proximity operator ˜ of the penalty term. [sent-162, score-0.531]
</p><p>55 One should then add at each iteration, the cost of performing the projection onto K. [sent-164, score-0.137]
</p><p>56 On the other hand, ˆ the time complexity of one iteration for Algorithm 1 is driven by the number of active groups B. [sent-166, score-0.414]
</p><p>57 The complexity is thus given ˆ ˆ ˆ by the sum of the complexity of evaluating the inverse of the B × B matrix H, O(B 3 ), and the ˆ complexity of performing the product H −1 g(λ), O(B 2 ). [sent-168, score-0.196]
</p><p>58 In fact, Equation (7) tells us that the part of matrix H corresponding to the active set I+ is ˆ ˆ ˆ ˆ diagonal. [sent-171, score-0.087]
</p><p>59 As a consequence, if B = B− + B+ , where B− is the number of non active constraints, ˆ+ is the number of active constraints, then the complexity of inverting matrix H is at most and B ˆ ˆ3 ˆ ˆ O(B+ ) + O(B− ). [sent-172, score-0.307]
</p><p>60 Furthermore the B− × B− non diagonal part of matrix H is highly sparse, since ˆ ˜ ˆ3 Hr,s = 0 if Gr ∩ Gs = ∅ and the complexity of inverting it is in practice much lower than O(B− ). [sent-173, score-0.133]
</p><p>61 ˆ+ ) + O(q · B− ), ˆ3 The worst case complexity for computing the projection onto K is thus O(q · B where q is the number of iterations necessary to reach convergence. [sent-174, score-0.233]
</p><p>62 Note that even if, in order to guarantee convergence, the tolerance for evaluating convergence of the inner iteration must decrease with the number of external iterations, in practice, thanks to warm starting, we observed that q is rarely greater than 10 in the experiments presented here. [sent-175, score-0.248]
</p><p>63 Concerning the number of iterations required to reach convergence for GL-prox in the replicates formulation, we empirically observed that it requires a much higher number of iterations than GLOpridu (see Table 3). [sent-176, score-0.202]
</p><p>64 In fact, since Eτ is convex but not necessarily strictly convex – as when n < d –, uniqueness and convergence is not always guaranteed unless some further assumption is imposed. [sent-179, score-0.203]
</p><p>65 Most convergence results relative to 1 regularization link uniqueness of the solution as well as the rate of convergence of the Soft Thresholding Iteration to some measure of local conditioning of the Hessian of the differentiable part of Eτ (see for instance Proposition 4. [sent-180, score-0.314]
</p><p>66 1 in [11], where the Hessian restricted to the set of relevant variables is required to ˜ ˜ ˜ be full rank). [sent-181, score-0.09]
</p><p>67 In our case the Hessian for GL-prox is simply H = 1/nΨT Ψ, so that, if the relevant ˜ restricted to the set of relevant variables is by no means groups have non null intersection, then H full rank. [sent-182, score-0.42]
</p><p>68 4  Numerical Experiments  In this section we present numerical experiments aimed at comparing the running time performance of GLO-pridu with state-of-the-art algorithms. [sent-185, score-0.076]
</p><p>69 To ensure a fair comparison, we ﬁrst run some preliminary experiments to identify the fastest codes for group 1 regularization with no overlap. [sent-186, score-0.334]
</p><p>70 1  Comparison of different implementations for standard group lasso  We considered three algorithms which are representative of the optimization techniques used to solve group lasso: interior-point methods, (group) coordinate descent and its variations, and proximal methods. [sent-190, score-0.856]
</p><p>71 For coordinate descent methods, we employed the R-package grlplasso, which implements block coordinate gradient descent minimization for a set of possible loss functions. [sent-196, score-0.16]
</p><p>72 Finally we use our Matlab implementation of Algorithm GL-prox as an instance of proximal methods. [sent-198, score-0.16]
</p><p>73 We ﬁrst observe that the solutions of the three algorithms coincide up to an error which depends on each algorithm tolerance. [sent-199, score-0.075]
</p><p>74 We thus need to tune each tolerance in order to guarantee that all iterative algorithms are stopped when the level of approximation to the true solution is the same. [sent-200, score-0.183]
</p><p>75 6  Table 1: Running time (mean and standard deviation) in seconds for computing the entire regularization path of GL-IP, GL-BCGD, and GL-prox for different values of B, and n. [sent-201, score-0.158]
</p><p>76 Note also that with these tolerances the three solutions coincide also in terms of selection, i. [sent-240, score-0.075]
</p><p>77 tol = 10−9 for GL-IP, tol = 10−12 for GL-BCGD, and ν = 10−6 for GL-prox. [sent-244, score-0.102]
</p><p>78 In this case the dictionary coincides with the variables, Ψj (x) = xj for j = 1, . [sent-256, score-0.076]
</p><p>79 We then evaluate the entire regularization path for the three algorithms with B sequential groups of 10 variables, (G1 =[1, . [sent-260, score-0.393]
</p><p>80 Finally we build the geometric series of 50 values between τmin and τmax , and use it to evaluate the regularization path on the three algorithms. [sent-269, score-0.158]
</p><p>81 In order to obtain robust estimates of the running times, we repeat 20 times for each pair n, B. [sent-270, score-0.087]
</p><p>82 In Table 1 we report the computational times required to evaluate the entire regularization path for the three algorithms. [sent-271, score-0.197]
</p><p>83 However it is well known that standard second-order methods are typically precluded on large data sets, since they need to solve large systems of linear equations to compute the Newton steps. [sent-274, score-0.086]
</p><p>84 GL-BCGD is the fastest for B = 1000, whereas GL-prox is the fastest for B = 10, 100. [sent-275, score-0.104]
</p><p>85 Nevertheless we observed that, when the input data matrix contains a signiﬁcant fraction of replicated columns, this algorithm does not provide sparse solutions. [sent-277, score-0.101]
</p><p>86 1  Projection vs duplication  The data generation protocol is equal to the one described in the previous experiments, but β depends on the ﬁrst 12/5b variables (which correspond to the ﬁrst three groups) β = ( c, . [sent-281, score-0.211]
</p><p>87 b·12/5 times d−b·12/5 times  7  ˜ We then deﬁne B groups of size b, so that d = B · b > d. [sent-288, score-0.313]
</p><p>88 The ﬁrst three groups correspond to the subset of relevant variables, and are deﬁned as G1 = [1, . [sent-289, score-0.262]
</p><p>89 The remaining B − 3 groups are built by randomly drawing sets of b indexes from [1, d]. [sent-302, score-0.292]
</p><p>90 We also vary the number of groups B, so that the dimension of the expanded space is α times the ˜ input dimension, d = αd, with α = 1. [sent-306, score-0.349]
</p><p>91 The parameter α can be thought of as the average number of groups a single variable belongs to. [sent-309, score-0.235]
</p><p>92 When the degree of overlap α is low the computational times of GL-prox and GLO-pridu are comparable. [sent-357, score-0.17]
</p><p>93 5  Discussion  We have presented an efﬁcient optimization procedure for computing the solution of group lasso with overlapping groups of variables, which allows dealing with high dimensional problems with large groups overlap. [sent-360, score-1.04]
</p><p>94 We have empirically shown that our procedure has a great computational advantage with respect to state-of-the-art algorithms for group lasso applied on the expanded space built by replicating variables belonging to more than one group. [sent-361, score-0.681]
</p><p>95 We also mention that computational performance may improve if our scheme is used as core for the optimization step of active set methods, such as [23]. [sent-362, score-0.132]
</p><p>96 Consistency of the group lasso and multiple kernel learning. [sent-366, score-0.391]
</p><p>97 Projected newton methods for optimization problems with simple constraints. [sent-399, score-0.1]
</p><p>98 A method for unconstrained convex minimization problem with the rate of convergence o(1/k 2 ). [sent-470, score-0.101]
</p><p>99 The gradient projection method for nonlinear programming, part i: linear constraints. [sent-498, score-0.095]
</p><p>100 The group-lasso for generalized linear models: uniqueness of solutions and efﬁcient algorithms. [sent-508, score-0.124]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gr', 0.412), ('goverlap', 0.351), ('tp', 0.296), ('groups', 0.235), ('group', 0.197), ('lasso', 0.194), ('hp', 0.173), ('proximal', 0.16), ('proximity', 0.135), ('overlap', 0.131), ('rd', 0.12), ('duplication', 0.117), ('mosci', 0.117), ('penalty', 0.106), ('overlapping', 0.104), ('verri', 0.103), ('newton', 0.1), ('cp', 0.098), ('projection', 0.095), ('villa', 0.088), ('active', 0.087), ('operator', 0.086), ('regularization', 0.085), ('genova', 0.077), ('expanded', 0.075), ('path', 0.073), ('bertsekas', 0.072), ('replicates', 0.071), ('init', 0.066), ('vr', 0.066), ('dual', 0.063), ('variables', 0.063), ('replicated', 0.06), ('tolerance', 0.06), ('precluded', 0.058), ('replication', 0.058), ('built', 0.057), ('complexity', 0.056), ('iterative', 0.056), ('rb', 0.056), ('projected', 0.052), ('fastest', 0.052), ('uniqueness', 0.052), ('tol', 0.051), ('convergence', 0.051), ('belonging', 0.051), ('convex', 0.05), ('universit', 0.048), ('running', 0.048), ('dictionary', 0.047), ('coordinate', 0.047), ('descendant', 0.047), ('concerning', 0.047), ('gb', 0.047), ('scheme', 0.045), ('ancestors', 0.044), ('replicating', 0.044), ('rosasco', 0.044), ('solutions', 0.044), ('onto', 0.042), ('sparsity', 0.042), ('warm', 0.042), ('bioinformatics', 0.041), ('sparse', 0.041), ('iterations', 0.04), ('non', 0.04), ('dimensional', 0.039), ('times', 0.039), ('differentiable', 0.039), ('splitting', 0.038), ('argmin', 0.037), ('hessian', 0.037), ('inverting', 0.037), ('gene', 0.037), ('jenatton', 0.036), ('solution', 0.036), ('iteration', 0.036), ('continuation', 0.035), ('deal', 0.034), ('hierarchical', 0.034), ('descent', 0.033), ('inria', 0.032), ('initialize', 0.031), ('di', 0.031), ('protocol', 0.031), ('coincide', 0.031), ('selection', 0.031), ('guarantee', 0.031), ('beck', 0.03), ('penalties', 0.029), ('dn', 0.029), ('coincides', 0.029), ('databases', 0.029), ('proceeding', 0.029), ('numerical', 0.028), ('solve', 0.028), ('null', 0.028), ('evaluating', 0.028), ('generalized', 0.028), ('relevant', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="12-tfidf-1" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>Author: Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco</p><p>Abstract: We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. 1</p><p>2 0.19907019 <a title="12-tfidf-2" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>3 0.17365783 <a title="12-tfidf-3" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>4 0.16810389 <a title="12-tfidf-4" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>Author: Hongbo Zhou, Qiang Cheng</p><p>Abstract: Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact deﬁnition for a novel representation called sparse grouping representation (SGR), and prove a set of sufﬁcient conditions for generating such group level sparsity. Under these sufﬁcient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classiﬁcation setting. 1</p><p>5 0.16363697 <a title="12-tfidf-5" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>Author: Jean Morales, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefﬁcients. This family subsumes the ℓ1 norm and is ﬂexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the beneﬁt of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.</p><p>6 0.15249744 <a title="12-tfidf-6" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>7 0.12488718 <a title="12-tfidf-7" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>8 0.11084658 <a title="12-tfidf-8" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>9 0.10847566 <a title="12-tfidf-9" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>10 0.1013666 <a title="12-tfidf-10" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>11 0.096703149 <a title="12-tfidf-11" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>12 0.095527939 <a title="12-tfidf-12" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>13 0.09308406 <a title="12-tfidf-13" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>14 0.092692681 <a title="12-tfidf-14" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>15 0.089096658 <a title="12-tfidf-15" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>16 0.087517545 <a title="12-tfidf-16" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>17 0.085297003 <a title="12-tfidf-17" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>18 0.085114986 <a title="12-tfidf-18" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>19 0.077825457 <a title="12-tfidf-19" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>20 0.077534653 <a title="12-tfidf-20" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, 0.059), (2, 0.111), (3, 0.097), (4, 0.075), (5, -0.169), (6, -0.006), (7, 0.093), (8, -0.038), (9, -0.026), (10, 0.046), (11, 0.135), (12, -0.123), (13, 0.051), (14, -0.01), (15, -0.108), (16, -0.054), (17, 0.061), (18, -0.023), (19, -0.136), (20, 0.018), (21, -0.008), (22, -0.04), (23, -0.002), (24, -0.082), (25, -0.11), (26, -0.045), (27, -0.066), (28, -0.024), (29, -0.075), (30, -0.065), (31, -0.105), (32, 0.152), (33, 0.02), (34, -0.078), (35, 0.008), (36, -0.023), (37, 0.019), (38, 0.008), (39, 0.081), (40, 0.039), (41, -0.056), (42, -0.061), (43, 0.01), (44, -0.033), (45, 0.104), (46, 0.007), (47, -0.081), (48, 0.027), (49, -0.134)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94907558 <a title="12-lsi-1" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>Author: Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco</p><p>Abstract: We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. 1</p><p>2 0.74369663 <a title="12-lsi-2" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>Author: Jean Morales, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefﬁcients. This family subsumes the ℓ1 norm and is ﬂexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the beneﬁt of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.</p><p>3 0.73226708 <a title="12-lsi-3" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>Author: Hongbo Zhou, Qiang Cheng</p><p>Abstract: Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact deﬁnition for a novel representation called sparse grouping representation (SGR), and prove a set of sufﬁcient conditions for generating such group level sparsity. Under these sufﬁcient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classiﬁcation setting. 1</p><p>4 0.72417355 <a title="12-lsi-4" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>5 0.68573719 <a title="12-lsi-5" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>6 0.65911937 <a title="12-lsi-6" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>7 0.5987854 <a title="12-lsi-7" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>8 0.57888138 <a title="12-lsi-8" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>9 0.5269841 <a title="12-lsi-9" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>10 0.50722867 <a title="12-lsi-10" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>11 0.49562865 <a title="12-lsi-11" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>12 0.49126291 <a title="12-lsi-12" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>13 0.48589984 <a title="12-lsi-13" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>14 0.46859118 <a title="12-lsi-14" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>15 0.46096793 <a title="12-lsi-15" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>16 0.45708954 <a title="12-lsi-16" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>17 0.43634436 <a title="12-lsi-17" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>18 0.4132081 <a title="12-lsi-18" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>19 0.41140291 <a title="12-lsi-19" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>20 0.40274197 <a title="12-lsi-20" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.075), (17, 0.025), (27, 0.062), (30, 0.061), (35, 0.068), (45, 0.209), (50, 0.056), (52, 0.032), (57, 0.189), (60, 0.051), (77, 0.046), (78, 0.019), (90, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88244873 <a title="12-lda-1" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>Author: Vicky Froyen, Jacob Feldman, Manish Singh</p><p>Abstract: Figure/ground assignment, in which the visual image is divided into nearer (ﬁgural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model ﬁgure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a ﬁgure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure “draws” border ownership so that borders are owned by the skeletal hypothesis that best explains them. We also brieﬂy present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours. Figure/ground assignment (further referred to as f/g), in which the visual image is divided into nearer (ﬁgural) and farther (ground) surfaces, is an essential step in visual processing. A number of factors are known to affect f/g assignment, including region size [9], convexity [7, 16], and symmetry [1, 7, 11]. Figural assignment (often referred to as border ownership, under the assumption that the ﬁgural side “owns” the border) is usually studied globally, meaning that entire surfaces and their enclosing boundaries are assumed to receive a globally consistent ﬁgural status. But recent psychophysical ﬁndings [8] have suggested that border ownership can vary locally along a boundary, even leading to a globally inconsistent ﬁgure/ground assignment—broadly consistent with electrophysiological evidence showing local coding for border ownership in area V2 as early as 68 msec after image onset [20]. This suggests a spatially distributed and potentially competitive process of ﬁgural assignment [15], in which adjacent surfaces compete to own their common boundary, with ﬁgural status propagating across the image as this competition proceeds. But both the principles and computational mechanisms underlying this process are poorly understood. ∗ V.F. was supported by a Fullbright Honorary fellowship and by the Rutgers NSF IGERT program in Perceptual Science, NSF DGE 0549115, J.F. by NIH R01 EY15888, and M.S. by NSF CCF-0541185 1 In this paper we consider how border ownership might propagate over both space and time—that is, across the image as well as over the progression of computation. Following Weiss et al. [18] we adopt a Bayesian belief network architecture, with nodes along boundaries representing estimated border ownership, and connections arranged so that both neighboring nodes and nonlocal integrating nodes combine to inﬂuence local estimates of border ownership. Our model is novel in two particular respects: (a) we combine both local and global inﬂuences on border ownership in an integrated and principled way; and (b) we include as a nonlocal factor skeletal (medial axis) inﬂuences on f/g assignment. Skeletal structure has not been previously considered as a factor on border ownership, but its relevance follows from a model [4] in which shapes are conceived of as generated by or “grown” from an internal skeleton, with the consequence that their boundaries are perceptually “owned” by the skeletal side. We also briey present a psychophysical experiment in which we measured local border ownership along a contour, at several distances from a strong local f/g inducing cue, and at several time delays after the onset of the cue. The results show measurable spatial differences in judged border ownership, with judgments varying with distance from the inducer; but no temporal effect, with essentially asymptotic judgments even after very brief exposures. Both results are consistent with the behavior of the network, which converges quickly to an asymptotic but spatially nonuniform f/g assignment. 1 The Model The Network. For simplicity, we take an edge map as input for the model, assuming that edges and T-junctions have already been detected. From this edge map we then create a Bayesian belief network consisting of four hierarchical levels. At the input level the model receives evidence E from the image, consisting of local contour curvature and T-junctions. The nodes for this level are placed at equidistant locations along the contour. At the ﬁrst level the model estimates local border ownership. The border ownership, or B-nodes at this level are at the same locations as the E-nodes, but are connected to their nearest neighbors, and are the parent of the E-node at their location. (As a simplifying assumption, such connections are broken at T-junctions in such a way that the occluded contour is disconnected from the occluder.) The highest level has skeletal nodes, S, whose positions are deﬁned by the circumcenters of the Delaunay triangulation on all the E-nodes, creating a coarse medial axis skeleton [13]. Because of the structure of the Delaunay, each S-node is connected to exactly three E-nodes from which they receive information about the position and the local tangent of the contour. In the current state of the model the S-nodes are “passive”, meaning their posteriors are computed before the model is initiated. Between the S nodes and the B nodes are the grouping nodes G. They have the same positions as the S-nodes and the same Delaunay connections, but to B-nodes that have the same image positions as the E-nodes. They will integrate information from distant B-nodes, applying an interiority cue that is inﬂuenced by the local strength of skeletal axes as computed by the S-nodes (Fig. 1). Although this is a multiply connected network, we have found that given reasonable parameters the model converges to intuitive posteriors for a variety of shapes (see below). Updating. Our goal is to compute the posterior p(Bi |I), where I is the whole image. Bi is a binary variable coding for the local direction of border ownership, that is, the side that owns the border. In order for border ownership estimates to be inﬂuenced by image structure elsewhere in the image, information has to propagate throughout the network. To achieve this propagation, we use standard equations for node updating [14, 12]. However while to all other connections being directed, connections at the B-node level are undirected, causing each node to be child and parent node at the same time. Considering only the B-node level, a node Bi is only separated from the rest of the network by its two neighbors. Hence the Markovian property applies, in that Bi only needs to get iterative information from its neighbors to eventually compute p(Bi |I). So considering the whole network, at each iteration t, Bi receives information from both its child, Ei and from its parents—that is neigbouring nodes (Bi+1 and Bi−1 )—as well as all grouping nodes connected to it (Gj , ..., Gm ). The latter encode for interiority versus exteriority, interiority meaning that the B-node’s estimated gural direction points towards the G-node in question, exteriority meaning that it points away. Integrating all this information creates a multidimensional likelihood function: p(Bi |Bi−1 , Bi+1 , Gj , ..., Gm ). Because of its complexity we choose to approximate it (assuming all nodes are marginally independent of each other when conditioned on Bi ) by 2 Figure 1: Basic network structure of the model. Both skeletal (S-nodes) and border-ownerhsip nodes (B-nodes) get evidence from E-nodes, though different types. S-nodes receive mere positional information, while B-nodes receive information about local curvature and the presence of T-junctions. Because of the structure of the Delaunay triangulation S-nodes and G-nodes (grouping nodes) always get input from exactly three nodes, respectively E and B-nodes. The gray color depicts the fact that this part of the network is computed before the model is initiated and does not thereafter interact with the dynamics of the model. m p(Bi |Pj , ..., Pm ) ∝ p(Bi |Pj ) (1) j where the Pj ’s are the parents of Bi . Given this, at each iteration, each node Bi performs the following computation: Bel(Bi ) ← cλ(Bi )π(Bi )α(Bi )β(Bi ) (2) where conceptually λ stands for bottom-up information, π for top down information and α and β for information received from within the same level. More formally, λ(Bi ) ← p(E|Bi ) (3) m π(Bi ) ← p(Bi |Gj )πGj (Bi ) j (4) Gj and analogously to equation 4 for α(Bi ) and β(Bi ), which compute information coming from Bi−1 and Bi+1 respectively. For these πBi−1 (Bi ), πBi+1 (Bi ), and πGj (Bi ): πGj (Bi ) ← c π(G) λBk (Gj ) (5) k=i πBi−1 (Bi ) ← c β(Bi−1 )λ(Bi−1 )π(Bi−1 ) 3 (6) and πBi+1 (Bi ) is analogous to πBi−1 (Bi ), with c and c being normalization constants. Finally for the G-nodes: Bel(Gi ) ← cλ(Gi )π(Gi ) λ(Gi ) ← (7) λBj (Gi ) (8) j m λBj (Gi ) ← λ(Bj )p(Bi |Gj )[α(Bj )β(Bj ) Bj p(Bi |Gk )πGk (Bi )] (9) k=i Gk The posteriors of the S-nodes are used to compute the π(Gi ). This posterior computes how well the S-node at each position explains the contour—that is, how well it accounts for the cues ﬂowing from the E-nodes it is connected to. Each Delaunay connection between S- and E-nodes can be seen as a rib that sprouts from the skeleton. More speciﬁcally each rib sprouts in a direction that is normal (perpendicular) to the tangent of the contour at the E-node plus a random error φi chosen independently for each rib from a von Mises distribution centered on zero, i.e. φi ∼ V (0, κS ) with spread parameter κS [4]. The rib lengths are drawn from an exponential decreasing density function p(ρi ) ∝ e−λS ρi [4]. We can now express how well this node “explains” the three E-nodes it is connected to via the probability that this S-node deserves to be a skeletal node or not, p(S = true|E1 , E2 , E3 ) ∝ p(ρi )p(φi ) (10) i with S = true depicting that this S-node deserves to be a skeletal node. From this we then compute the prior π(Gi ) in such a way that good (high posterior) skeletal nodes induce a high interiority bias, hence a stronger tendency to induce ﬁgural status. Conversely, bad (low posterior) skeletal nodes create a prior close to indifferent (uniform) and thus have less (or no) inﬂuence on ﬁgural status. Likelihood functions Finally we need to express the likelihood function necessary for the updating rules described above. The ﬁrst two likelihood functions are part of p(Ei |Bi ), one for each of the local cues. The ﬁrst one, reﬂecting local curvature, gives the probability of the orientations of the two vectors inherent to Ei (α1 and α2 ) given both direction of ﬁgure (θ) encoded in Bi as a von Mises density centered on θ, i.e. αi ∼ V (θ, κEB ). The second likelihood function, reﬂecting the presence of a T-junction, simply assumes a ﬁxed likelihood when a T-junction is present—that is p(T-junction = true|Bi ) = θT , where Bi places the direction of ﬁgure in the direction of the occluder. This likelihood function is only in effect when a T-junction is present, replacing the curvature cue at that node. The third likelihood function serves to keep consistency between nodes of the ﬁrst level. This function p(Bi |Bi−1 ) or p(Bi |Bi+1 ) is used to compute α(B) and β(B) and is deﬁned 2x2 conditional probability matrix with a single free parameter, θBB (the probability that ﬁgural direction at both B-nodes are the same). A fourth and ﬁnal likelihood function p(Bi |Gj ) serves to propagate information between level one and two. This likelihood function is 2x2 conditional probability matrix matrix with one free parameter, θBG . In this case θBG encodes the probability that the ﬁgural direction of the B-node is in the direction of the exterior or interior preference of the G-node. In total this brings us to six free parameters in the model: κS , λS , κEB , θT , θBB , and θBG . 2 Basic Simulations To evaluate the performance of the model, we ﬁrst tested it on several basic stimulus conﬁgurations in which the desired outcome is intuitively clear: a convex shape, a concave shape, a pair of overlapping shapes, and a pair of non-overlapping shapes (Fig. 2,3). The convex shape is the simplest in that curvature never changes sign. The concave shape includes a region with oppositely signed curvature. (The shape is naturally described as predominantly positively curved with a region of negative curvature, i.e. a concavity. But note that it can also be interpreted as predominantly negatively curved “window” with a region of positive curvature, although this is not the intuitive interpretation.) 4 The overlapping pair of shapes consists of two convex shapes with one partly occluding the other, creating a competition between the two shapes for the ownership of the common borderline. Finally the non-overlapping shapes comprise two simple convex shapes that do not touch—again setting up a competition for ownership of the two inner boundaries (i.e. between each shape and the ground space between them). Fig. 2 shows the network structures for each of these four cases. Figure 2: Network structure for the four shape categories (left to right: convex, concave, overlapping, non-overlapping shapes). Blue depict the locations of the B-nodes (and also the E-nodes), the red connections are the connections between B-nodes, the green connections are connections between B-nodes and G-nodes, and the G-nodes (and also the S-nodes) go from orange to dark red. This colour code depicts low (orange) to high (dark red) probability that this is a skeletal node, and hence the strength of the interiority cue. Running our model with hand-estimated parameter values yields highly intuitive posteriors (Fig. 3), an essential “sanity check” to ensure that the network approximates human judgments in simple cases. For the convex shape the model assigns ﬁgure to the interior just as one would expect even based solely on local curvature (Fig. 3A). In the concave ﬁgure (Fig. 3B), estimated border ownership begins to reverse inside the deep concavity. This may seem surprising, but actually closely matches empirical results obtained when local border ownership is probed psychophysically inside a similarly deep concavity, i.e. a “negative part” in which f/g seems to partly reverse [8]. For the overlapping shapes posteriors were also intuitive, with the occluding shape interpreted as in front and owning the common border (Fig. 3C). Finally, for the two non-overlapping shapes the model computed border-ownership just as one would expect if each shape were run separately, with each shape treated as ﬁgural along its entire boundary (Fig. 3D). That is, even though there is skeletal structure in the ground-region between the two shapes (see Fig. 2D), its posterior is weak compared to the skeletal structure inside the shapes, which thus loses the competition to own the boundary between them. For all these conﬁgurations, the model not only converged to intuitive estimates but did so rapidly (Fig. 4), always in fewer cycles than would be expected by pure lateral propagation, niterations < Nnodes [18] (with these parameters, typically about ﬁve times faster). Figure 3: Posteriors after convergence for the four shape categories (left to right: convex, concave, overlapping, non-overlapping). Arrows indicate estimated border ownership, with direction pointing to the perceived ﬁgural side, and length proportional to the magnitude of the posterior. All four simulations used the same parameters. 5 Figure 4: Convergence of the model for the basic shape categories. The vertical lines represent the point of convergence for each of the three shape categories. The posterior change is calculated as |p(Bi = 1|I)t − p(Bi = 1|I)t−1 | at each iteration. 3 Comparison to human data Beyond the simple cases reviewed above, we wished to submit our network to a more ﬁne-grained comparison with human data. To this end we compared its performance to that of human subjects in an experiment we conducted (to be presented in more detail in a future paper). Brieﬂy, our experiment involved ﬁnding evidence for propagation of f/g signals across the image. Subjects were ﬁrst shown a stimulus in which the f/g conﬁguration was globally and locally unambiguous and consistent: a smaller rectangle partly occluding a larger one (Fig. 5A), meaning that the smaller (front) one owns the common border. Then this conﬁguration was perturbed by adding two bars, of which one induced a local f/g reversal—making it now appear locally that the larger rectangle owned the border (Fig. 5B). (The other bar in the display does not alter f/g interpretation, but was included to control for the attentional affects of introducing a bar in the image.) The inducing bar creates T-junctions that serve as strong local f/g cues, in this case tending to reverse the prior global interpretation of the ﬁgure. We then measured subjective border ownership along the central contour at various distances from the inducing bar, and at different times after the onset of the bar (25ms, 100ms and 250ms). We measured border ownership locally using a method introduced in [8] in which a local motion probe is introduced at a point on the boundary between two color regions of different colors, and the subject is asked which color appeared to move. Because the ﬁgural side “owns” the border, the response reﬂects perceived ﬁgural status. The goal of the experiment was to actually measure the progression of the inﬂuence of the inducing T-junction as it (hypothetically) propagated along the boundary. Brieﬂy, we found no evidence of temporal differences, meaning that f/g judgments were essentially constant over time, suggesting rapid convergence of local f/g assignment. (This is consistent with the very rapid convergence of our network, which would suggest a lack of measurable temporal differences except at much shorter time scales than we measured.) But we did ﬁnd a progressive reduction of f/g reversal with increasing distance from the inducer—that is, the inﬂuence of the T-junction decayed with distance. Mean responses aggregated over subjects (shortest delay only) are shown in Fig. 6. In order to run our model on this stimulus (which has a much more complex structure than the simple ﬁgures tested above) we had to make some adjustments. We removed the bars from the edge map, leaving only the T-junctions as underlying cues. This was a necessary ﬁrst step because our model is not yet able to cope with skeletons that are split up by occluders. (The larger rectangle’s skeleton has been split up by the lower bar.) In this way all contours except those created by the bars were used to create the network (Fig. 7). Given this network we ran the model using hand-picked parameters that 6 Figure 5: Stimuli used in the experiment. A. Initial stimulus with locally and globally consistent and unambiguous f/g. B. Subsequently bars were added of which one (the top bar in this case) created a local reversal of f/g. C. Positions at which local f/g judgments of subjects were probed. Figure 6: Results from our experiment aggregated for all 7 subjects (shortest delay only) are shown in red. The x-axis shows distance from the inducing bar at which f/g judgment was probed. The y-axis shows the proportion of trials on which subjects judged the smaller rectangle to own the boundary. As can be seen, the further from the T-junction, the lower the f/g reversal. The ﬁtted model (green curve) shows very similar pattern. Horizontal black line indicates chance performance (ambiguous f/g). gave us the best possible qualitative similarity to the human data. The parameters used never entailed total elimination of the inﬂuence of any likelihood function (κS = 16, λS = .025, κEB = .5, θT = .9, θBB = .9, and θBG = .6). As can be seen in Fig. 6 the border-ownership estimates at the locations where we had data show compelling similarities to human judgments. Furthermore along the entire contour the model converged to intuitive border-ownership estimates (Fig. 7) very rapidly (within 36 iterations). The fact that our model yielded intuitive estimates for the current network in which not all contours were completed shows another strength of our model. Because our model included grouping nodes, it did not require contours to be amodally completed [6] in order for information to propagate. 4 Conclusion In this paper we proposed a model rooted in Bayesian belief networks to compute ﬁgure/ground. The model uses both local and global cues, combined in a principled way, to achieve a stable and apparently psychologically reasonable estimate of border ownership. Local cues included local curvature and T-junctions, both well-established cues to f/g. Global cues included skeletal structure, 7 Figure 7: (left) Node structure for the experimental stimulus. (right) The model’s local borderownership estimates after convergence. a novel cue motivated by the idea that strongly axial shapes tend to be ﬁgural and thus own their boundaries. We successfully tested this model on both simple displays, in which it gave intuitive results, and on a more complex experimental stimulus, in which it gave a close match to the pattern of f/g propagation found in our subjects. Speciﬁcally, the model, like the human subjects rapidly converged to a stable local f/g interpretation. Our model’s structure shows several interesting parallels to properties of neural coding of border ownership in visual cortex. Some cortical cells (end-stopped cells) appear to code for local curvature [3] and T-junctions [5]. The B-nodes in our model could be seen as corresponding to cells that code for border ownership [20]. Furthermore, some authors [2] have suggested that recurrent feedback loops between border ownership cells in V2 and cells in V4 (corresponding to G-nodes in our model) play a role in the rapid computation of border ownership. The very rapid convergence we observed in our model likewise appears to be due to the connections between B-nodes and G-nodes. Finally scale-invariant shape representations (such as, speculatively, those based on skeletons) are thought to be present in higher cortical regions such as IT [17], which project down to earlier areas in ways that are not yet understood. A number of parallels to past models of f/g should be mentioned. Weiss [18] pioneered the application of belief networks to the f/g problem, though their network only considered a more restricted set of local cues and no global ones, such that information only propagated along the contour. Furthermore it has not been systematically compared to human judgments. Kogo et al. [10] proposed an exponential decay of f/g signals as they spread throughout the image. Our model has a similar decay for information going through the G-nodes, though it is also inﬂuenced by an angular factor deﬁned by the position of the skeletal node. Like the model by Li Zhaoping [19], our model includes horizontal propagation between B-nodes, analogous to border-ownership cells in her model. A neurophysiological model by Craft et al. [2] deﬁnes grouping cells coding for an interiority preference that decays with the size of the receptive ﬁelds of these grouping cells. Our model takes this a step further by including shape (skeletal) structure as a factor in interiority estimates, rather than simply size of receptive ﬁelds (which is similar to the rib lengths in our model). Currently, our use of skeletons as shape representations is still limited to medial axis skeletons and surfaces that are not split up by occluders. Our future goals including integrating skeletons in a more robust way following the probabilistic account suggested by Feldman and Singh [4]. Eventually, we hope to fully integrate skeleton computation with f/g computation so that the more general problem of shape and surface estimation can be approached in a coherent and uniﬁed fashion. 8 References [1] P. Bahnsen. Eine untersuchung uber symmetrie und assymmetrie bei visuellen wahrnehmungen. Zeitschrift fur psychology, 108:129–154, 1928. [2] E. Craft, H. Sch¨ tze, E. Niebur, and R. von der Heydt. A neural model of ﬁgure-ground u organization. Journal of Neurophysiology, 97:4310–4326, 2007. [3] A. Dobbins, S. W. Zucker, and M. S. Cyander. Endstopping and curvature. Vision Research, 29:1371–1387, 1989. [4] J. Feldman and M. Singh. Bayesian estimation of the shape skeleton. Proceedings of the National Academy of Sciences, 103:18014–18019, 2006. [5] B. Heider, V. Meskenaite, and E. Peterhans. Anatomy and physiology of a neural mechanism deﬁning depth order and contrast polarity at illusory contours. European Journal of Neuroscience, 12:4117–4130, 2000. [6] G. Kanizsa. Organization inVision. New York: Praeger, 1979. [7] G. Kanizsa and W. Gerbino. Vision and Artifact, chapter Convexity and symmetry in ﬁgureground organisation, pages 25–32. New York: Springer, 1976. [8] S. Kim and J. Feldman. Globally inconsistent ﬁgure/ground relations induced by a negative part. Journal of Vision, 9:1534–7362, 2009. [9] K. Koffka. Principles of Gestalt Psychology. Lund Humphries, London, 1935. [10] N. Kogo, C. Strecha, L. Van Gool, and J. Wagemans. Surface construction by a 2-d differentiation-integration process: a neurocomputational model for perceived border ownership, depth, and lightness in kanizsa ﬁgures. Psychological Review, 117:406–439, 2010. [11] B. Machielsen, M. Pauwels, and J. Wagemans. The role of vertical mirror-symmetry in visual shape detection. Journal of Vision, 9:1–11, 2009. [12] K. Murphy, Y. Weiss, and M.I. Jordan. Loopy belief propagation for approximate inference: an empirical study. Proceedings of Uncertainty in AI, pages 467–475, 1999. [13] R. L. Ogniewicz and O. K¨ bler. Hierarchic Voronoi skeletons. Pattern Recognition, 28:343– u 359, 1995. [14] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 1988. [15] M. A. Peterson and E. Skow. Inhibitory competition between shape properties in ﬁgureground perception. Journal of Experimental Psychology: Human Perception and Performance, 34:251–267, 2008. [16] K. A. Stevens and A. Brookes. The concave cusp as a determiner of ﬁgure-ground. Perception, 17:35–42, 1988. [17] K. Tanaka, H. Saito, Y. Fukada, and M. Moriya. Coding visual images of object in the inferotemporal cortex of the macaque monkey. Journal of Neurophysiology, 66:170–189, 1991. [18] Y. Weiss. Interpreting images by propagating Bayesian beliefs. Adv. in Neural Information Processing Systems, 9:908915, 1997. [19] L. Zhaoping. Border ownership from intracortical interactions in visual area V2. Neuron, 47(1):143–153, Jul 2005. [20] H. Zhou, H. S. Friedman, and R. von der Heydt. Coding of border ownerschip in monkey visual cortex. The Journal of Neuroscience, 20:6594–6611, 2000. 9</p><p>2 0.87068653 <a title="12-lda-2" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>same-paper 3 0.84042805 <a title="12-lda-3" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>Author: Sofia Mosci, Silvia Villa, Alessandro Verri, Lorenzo Rosasco</p><p>Abstract: We deal with the problem of variable selection when variables must be selected group-wise, with possibly overlapping groups deﬁned a priori. In particular we propose a new optimization procedure for solving the regularized algorithm presented in [12], where the group lasso penalty is generalized to overlapping groups of variables. While in [12] the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and projected Newton method in a reduced dual space, corresponding to the active groups. This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing for dimensionality reduction. The computational advantages of our scheme with respect to state-of-the-art algorithms using data duplication are shown empirically with numerical simulations. 1</p><p>4 0.79204327 <a title="12-lda-4" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>5 0.79156023 <a title="12-lda-5" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>Author: Hongbo Zhou, Qiang Cheng</p><p>Abstract: Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact deﬁnition for a novel representation called sparse grouping representation (SGR), and prove a set of sufﬁcient conditions for generating such group level sparsity. Under these sufﬁcient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classiﬁcation setting. 1</p><p>6 0.79104215 <a title="12-lda-6" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>7 0.78878558 <a title="12-lda-7" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>8 0.78718287 <a title="12-lda-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.78119487 <a title="12-lda-9" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>10 0.77988964 <a title="12-lda-10" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>11 0.77963746 <a title="12-lda-11" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>12 0.77958727 <a title="12-lda-12" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>13 0.77902704 <a title="12-lda-13" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>14 0.77761871 <a title="12-lda-14" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>15 0.77702177 <a title="12-lda-15" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>16 0.77638245 <a title="12-lda-16" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>17 0.77631903 <a title="12-lda-17" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>18 0.77616924 <a title="12-lda-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.77593946 <a title="12-lda-19" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>20 0.775913 <a title="12-lda-20" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
