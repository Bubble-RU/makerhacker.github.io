<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 nips-2010-Avoiding False Positive in Multi-Instance Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-36" href="#">nips2010-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 nips-2010-Avoiding False Positive in Multi-Instance Learning</h1>
<br/><p>Source: <a title="nips-2010-36-pdf" href="http://papers.nips.cc/paper/3941-avoiding-false-positive-in-multi-instance-learning.pdf">pdf</a></p><p>Author: Yanjun Han, Qing Tao, Jue Wang</p><p>Abstract: In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we deﬁne a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.</p><p>Reference: <a title="nips-2010-36-reference" href="../nips2010_reference/nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. [sent-10, score-0.748]
</p><p>2 Based on kernel principal component analysis, we deﬁne a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. [sent-11, score-1.864]
</p><p>3 We apply the Constrained Concave-Convex Procedure to solve the resulted problem. [sent-12, score-0.058]
</p><p>4 in [1] to predict the binding ability of a drug from its biochemical structure. [sent-16, score-0.139]
</p><p>5 A certain drug molecule corresponds to a set of conformations which cannot be differentiated via chemical experiments. [sent-17, score-0.185]
</p><p>6 A drug is labeled positive if any of its constituent conformations has the binding ability greater than the threshold, otherwise negative. [sent-18, score-0.415]
</p><p>7 Therefore, each sample (a drug) is a bag of instances (its constituent conformations). [sent-19, score-0.589]
</p><p>8 In multi-instance learning the label information for positive samples is incomplete in that the instances in a certain positive bag are all labeled positive. [sent-20, score-0.798]
</p><p>9 Generally, methods for multi-instance learning are modiﬁed versions of approaches for supervised learning by shifting the focus from discrimination on instances to discrimination on bags. [sent-21, score-0.269]
</p><p>10 Examples include image classiﬁcation [11], document categorization [12], computer aided diagnosis [13], etc. [sent-26, score-0.107]
</p><p>11 As far as positive bags are concerned, current research usually treat them as labyrinths in which witnesses (responsible positive instances) are encaged, and consider nonwitnesses (other instances) therein to be useless or even distractive. [sent-27, score-1.058]
</p><p>12 The information carried by nonwitnesses is not well utilized. [sent-28, score-0.469]
</p><p>13 Factually, nonwitnesses are indispensable for characterizing the overall instance distribution, and thus help to improve the learner. [sent-29, score-0.513]
</p><p>14 Several researchers realized the importance of nonwitnesses and attempted to utilize them. [sent-30, score-0.498]
</p><p>15 In MI-kernels [5] and reg-SVM [6], nonwitnesses together with witnesses are squeezed into the kernel matrix. [sent-31, score-0.546]
</p><p>16 In mi-SVM [4], the labels of all nonwitnesses are treated as unknown integer variables to be optimized. [sent-32, score-0.469]
</p><p>17 mi-SVM tends to misclassify negative instances in positive bags since the resulted margin will be larger. [sent-33, score-0.92]
</p><p>18 In MissSVM [7] and stMIL [8], multi-instance learning is addressed from the view of semisupervised learning, and nonwitnesses are treated as unlabeled data, whose labels should be assigned to maximize the margin. [sent-36, score-0.469]
</p><p>19 sbMIL [8] attempt to estimate the ratio of positive instances inside positive bags and utilize this information in the subsequent classiﬁcation. [sent-37, score-0.881]
</p><p>20 1  Figure 1: Illustration of the False Positive Phenomenon: The top image is a positive training sample, and the bottom image is a negative testing sample. [sent-39, score-0.313]
</p><p>21 The symbol ⊕ and ⊖ respectively denote positive and negative instances. [sent-40, score-0.227]
</p><p>22 The Point not enveloped is a negative bag of just one instance. [sent-42, score-0.418]
</p><p>23 The learners f and g are obtained with and without the projection constraint, respectively. [sent-44, score-0.107]
</p><p>24 The neglect of nonwitnesses in positive bags may lead to false positive and cause a model to misclassify unseen negative samples. [sent-47, score-1.323]
</p><p>25 For example, in natural scene classiﬁcation, each image is segmented to a bag of instances beforehand, and each instance is a patch (ROI, Regions Of Interest) characterized by one feature vector describing its color. [sent-48, score-0.595]
</p><p>26 The task is to predict whether an image contains a waterfall or not (Figure 1). [sent-49, score-0.128]
</p><p>27 A positive image contains some positive instances corresponding to waterfall and some negative instances from other categories such as sky, stone, grass, etc. [sent-50, score-1.026]
</p><p>28 , while a negative bag exclusively contains negative instances from other categories. [sent-51, score-0.696]
</p><p>29 Naturally, some negative instances (patches) only exist in positive bags. [sent-52, score-0.496]
</p><p>30 For instance, the end of a waterfall is often surrounded by mist. [sent-53, score-0.085]
</p><p>31 The aforementioned approaches tend to misclassify negative instances in positive bags. [sent-54, score-0.571]
</p><p>32 Given an unseen image with cirrus cloud and without waterfall, the obtained learner will misclassify this image as positive because cirrus cloud and mist are similar to each other. [sent-56, score-0.564]
</p><p>33 To avoid both false negative and false positive, we attempt to classify instances inside positive bags far from the separating hyperplane and place positive and negative instances at opposite sides. [sent-57, score-1.672]
</p><p>34 We achieve this by introducing projection constraints based on kernel principal component analysis into MI-SVM [4]. [sent-58, score-0.188]
</p><p>35 Each constraint is deﬁned on a positive bag to encourage large variance of its constituent instances along the normal direction of the separating hyperplane. [sent-59, score-0.862]
</p><p>36 In Section 3 we bring out the projection constraint and the corresponding formulation for multi-instance learning. [sent-62, score-0.163]
</p><p>37 Let X ⊆ Rp be the space containing instances and D = {(Bi , yi )}m be the training data, where Bi is the ith bag of i=1 instances {xi1 , · · · , xini } and yi ∈ Y is the label for Bi . [sent-67, score-0.777]
</p><p>38 In addition, denote the index set for instances xij of Bi by Ii . [sent-69, score-0.51]
</p><p>39 The task is to train 2  a learner to predict the label of an unseen bag. [sent-70, score-0.085]
</p><p>40 Denote the index sets for positive and negative bags by I+ and I− respectively. [sent-72, score-0.518]
</p><p>41 Without loss of generality, assume that the instances are ordered in the sequence {x11 , · · · , x1n1 , · · · , xm1 , · · · , xmnm }. [sent-73, score-0.269]
</p><p>42 We index instances by a function i−1 i−1 i−1 ∑ ∑ ∑ I(xij ) = nk + j. [sent-74, score-0.296]
</p><p>43 And I(Bi ) returns a vector ( nk + 1, · · · , nk + ni ). [sent-75, score-0.2]
</p><p>44 However, if both objective function and constraints take the form of a difference between two convex functions, then a non-convex problem can be solved efﬁciently by the constrained concave-convex procedure [14]. [sent-78, score-0.094]
</p><p>45 fi (x) − gi (x) ≤ ci ,  i = 1, · · · , m  (1)  where fi , gi (i = 0, · · · , m) are real-valued, convex and differentiable functions on Rn . [sent-82, score-0.218]
</p><p>46 At the t + 1th iteration, the non-convex parts in the objective and constraints are substituted by their ﬁrst-order Taylor expansions, and the resulted optimization problem is as follows: [ ] min f0 (x) − g0 (x(t) ) + ∇g0 (x(t) )T (x − x(t) ) (2) x [ ] s. [sent-84, score-0.094]
</p><p>47 fi (x) − gi (x(t) ) + ∇gi (x(t) )T (x − x(t) ) ≤ ci where x(t) is the optimal solution to (2) at the tth iteration. [sent-86, score-0.124]
</p><p>48 max(w xij + b) ≥ 1 − ξi , ξi ≥ 0, i ∈ I+ T  j∈Ii  − wT xij − b ≥ 1 − ξij , ξij ≥ 0, j ∈ Ii , i ∈ I− Compared with the conventional SVM, in MI-SVM the notion of slack variables for positive samples is extended from individual instances to bags while that for negative samples remains unchanged. [sent-92, score-1.303]
</p><p>49 As shown by the ﬁrst set of max constraints, only the “most positive” instance in a positive bag, or the witness, could affect the margin. [sent-93, score-0.209]
</p><p>50 And other instances, or nonwitnesses, become irrelevant for determining the position of the separating plane once the witness is speciﬁed. [sent-94, score-0.165]
</p><p>51 The max constraint at ﬁrst sight seems to well embody the characteristic of multi-instance learning. [sent-95, score-0.088]
</p><p>52 Indeed, it helps to avoid the false negative, i,e. [sent-96, score-0.097]
</p><p>53 However, it may incur false positive due to the following two reasons. [sent-98, score-0.23]
</p><p>54 Firstly, the max constraint aims at discovering the witness, and tends to skip nonwitnesses. [sent-99, score-0.088]
</p><p>55 Thus each positive bag is approximately oversimpliﬁed to a single pattern, i. [sent-100, score-0.372]
</p><p>56 Secondly, due to the characteristic of the max function and the greediness of optimization methods, the predictions of nonwitnesses are often adjusted above zero in the learning process. [sent-104, score-0.501]
</p><p>57 Nevertheless, many nonwitnesses in positive bags are factually negative instances. [sent-106, score-1.03]
</p><p>58 For example, in natural scene classiﬁcation, 3  many image patches in a positive bag are from the irrelevant background; in document categorization, many posts in a positive bag are not from the target category. [sent-107, score-0.787]
</p><p>59 Hence, many nonwitnesses are mislabeled as positive, and we obtain a falsely large margin. [sent-108, score-0.469]
</p><p>60 As shown in Figure 1, MI-SVM classiﬁes half instances in the training sample as positive, and some negative instances are mislabeled. [sent-109, score-0.632]
</p><p>61 Any approach without properly utilizing nonwitnesses has the same problem. [sent-113, score-0.492]
</p><p>62 Firstly, we treat the labels of all nonwitnesses as unknown integer variables to be optimized. [sent-115, score-0.469]
</p><p>63 yij (wT xij + b) ≥ 1 − ξij , ξij ≥ 0, j ∈ Ii , i ∈ I+ ∑ yij + 1 ≥ 1, i ∈ I+ 2 j∈Ii  − wT xij − b ≥ 1 − ξij , ξij ≥ 0, j ∈ Ii , i ∈ I− It seems that assigning labels over all nonwitnesses should lead to a reasonable model. [sent-118, score-1.029]
</p><p>64 Nevertheless, nonwitnesses are usually labeled positive since the consequent margin will be larger. [sent-119, score-0.626]
</p><p>65 For every instance in positive bags, two slack variables are introduced, measuring the distances from the instance to the positive boundary f (x) = +1 and the negative boundary f (x) = −1 respectively, and the label of the instance depends on the smaller slack variable. [sent-123, score-0.56]
</p><p>66 Still, there is no mechanism in sbMIL to avoid false positive. [sent-128, score-0.097]
</p><p>67 Although misclassiﬁcation of nonwitnesses is alleviated since at least the “most negative” nonwitness is classiﬁed correctly, the information carried by most nonwitnesses are not fully utilized. [sent-130, score-0.938]
</p><p>68 Besides, this solution is not appropriate for applications which involve positive bags only with positive instances. [sent-132, score-0.557]
</p><p>69 The third solution is the projection constraint proposed in this paper. [sent-133, score-0.163]
</p><p>70 In a maximum margin framework we want to classify instances in a positive bag far away from the separating hyperplane while place positive instances and negative instances at opposite sides. [sent-134, score-1.575]
</p><p>71 From another point of view, in the feature (kernel) space, we want to maximize the variance of instances in a positive bag along w, the normal vector of the separating hyperplane. [sent-135, score-0.725]
</p><p>72 Points not enveloped are negative bags of just one instance. [sent-146, score-0.47]
</p><p>73 The learner f and g are obtained with and without the projection constraint, respectively. [sent-148, score-0.161]
</p><p>74 ⊕ and ⊖ denote positive instances and negative instance respectively. [sent-150, score-0.54]
</p><p>75 Given a positive bag Bi , denote its instances by {xij }ni , and denote the normal vector of the separating plane in the j=1 RKHS by f . [sent-155, score-0.76]
</p><p>76 |cj | is the distance from ϕ(mi ) to the j=1  projection point of ϕ(xij ). [sent-157, score-0.107]
</p><p>77 Instead, we average (10) by the bag size ni , and use a common threshold λ to bound the averaged projection distance for different bags from above. [sent-161, score-0.783]
</p><p>78 We name the obtained inequality “the projection constraint”, as follows: αT L2 α ) 1( oi − T i ≤λ ni α Kα  (12)  This is equivalent to bounding variance of instances in positive bags along f from below [15]. [sent-162, score-1.027]
</p><p>79 Substituting (7) into (6), and adding the projection constraint (12) for each positive bag to the resulted problem, we arrive at the following optimization problem: [∑ ] ∑ 1 min αT Kα + C ξi + ξij (13) α,b,ξ 2 i∈I+  s. [sent-163, score-0.593]
</p><p>80 1 − ξi −  j∈Ii ,i∈I−  max(kT ij ) α I(x j∈Ii  + b) ≤ 0, ξi ≥ 0, i ∈ I+  kT ij ) α + b ≤ −1 + ξij , ξij ≥ 0, j ∈ Ii , i ∈ I− I(x αT (oi · K − L2 )α − λni · αT Kα ≤ 0, i ∈ I+ i 3. [sent-165, score-0.506]
</p><p>81 The ﬁrst set of constraints are all in the form of difference of two convex functions since the max function is convex. [sent-167, score-0.092]
</p><p>82 Thus for any i ∈ I+ , oi · K − L2 is semi-deﬁnite positive. [sent-169, score-0.081]
</p><p>83 Consequently, the third set of constraints i are all in the form of difference of two convex functions. [sent-170, score-0.06]
</p><p>84 Since the function max in the ﬁrst set of constraints is nonsmooth, we have to change gradients to subgradients to use the CCCP. [sent-173, score-0.068]
</p><p>85 1 − ξi − ( βij kI(xij ) α + b) ≤ 0, ξi ≥ 0, i ∈ I+  (19)  j∈Ii  kT ij ) α I(x  + b ≤ −1 + ξij , ξij ≥ 0, j ∈ Ii , i ∈ I− T  αT Si α − 2λni · α(t) K(α − α(t) ) ≤ 0, i ∈ I+ where Si = oi · K − L2 . [sent-178, score-0.334]
</p><p>86 The problem (19) is a quadratically constrained quadratic program (QCQP) i with a convex objective function and convex constraints, and thus can be readily solved via interior point methods [18]. [sent-179, score-0.082]
</p><p>87 Each drug molecule is a bag of potential conformations (instances). [sent-185, score-0.424]
</p><p>88 The Musk 1 data set consists of 47 positive bags, 45 negative bags, and totally 476 instances. [sent-186, score-0.25]
</p><p>89 The Musk 2 data set consists of 39 positive bags, 63 negative bags, and totally 6598 instances. [sent-187, score-0.25]
</p><p>90 Elephant, tiger and fox are three data sets from image categorization. [sent-189, score-0.123]
</p><p>91 A bag here is a group of ROIs (Region Of Interests) drawn from a certain image. [sent-191, score-0.239]
</p><p>92 Each data set contains 100 positive bags and 100 negative bags, and each ROI as an instance is a 230 dimensional vector. [sent-192, score-0.562]
</p><p>93 : γ in RBF kernel), and the bound parameter λ in the projection constraint. [sent-259, score-0.107]
</p><p>94 Recall that the difference between our approach and MI-SVM is just the projection constraint. [sent-277, score-0.107]
</p><p>95 2, the results in Table 1 demonstrates that the strength of nonwitnesses is well utilized via the projection constraint. [sent-279, score-0.576]
</p><p>96 Each image is regarded as a bag, and the nine dimensional ROIs (Region Of Interests) in it are regarded as its constituent instances. [sent-322, score-0.184]
</p><p>97 The results again suggest that fully utilizing the nonwitnesses is important for multi-instance classiﬁcation. [sent-330, score-0.492]
</p><p>98 5  Conclusion  We design a projection constraint to fully exploit nonwitnesses to avoid false positive. [sent-331, score-0.729]
</p><p>99 Since our approach is basically MI-SVM with projection constraints, the improved results on real world data sets validate the strength of nonwitnesses. [sent-332, score-0.107]
</p><p>100 We will introduce the universal projection constraint into other existing approaches for multi-instance learning, and related learning tasks, such as multiinstance regression, multi-label multi-instance learning, generalized multi-instance learning, etc. [sent-333, score-0.163]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nonwitnesses', 0.469), ('bags', 0.291), ('instances', 0.269), ('ij', 0.253), ('xij', 0.241), ('bag', 0.239), ('kt', 0.197), ('migraph', 0.15), ('sbmil', 0.149), ('stmil', 0.149), ('ni', 0.146), ('ii', 0.136), ('positive', 0.133), ('musk', 0.128), ('projection', 0.107), ('misssvm', 0.107), ('false', 0.097), ('negative', 0.094), ('bi', 0.092), ('enveloped', 0.085), ('waterfall', 0.085), ('separating', 0.084), ('oi', 0.081), ('constituent', 0.081), ('drug', 0.076), ('conformations', 0.075), ('misclassify', 0.075), ('cccp', 0.067), ('corel', 0.064), ('resulted', 0.058), ('gi', 0.057), ('constraint', 0.056), ('citeseer', 0.056), ('learner', 0.054), ('misclassi', 0.049), ('lt', 0.047), ('rkhs', 0.047), ('witness', 0.046), ('kernel', 0.045), ('ki', 0.045), ('dd', 0.044), ('instance', 0.044), ('image', 0.043), ('cirrus', 0.043), ('factually', 0.043), ('mist', 0.043), ('tiger', 0.042), ('fi', 0.04), ('china', 0.039), ('yij', 0.039), ('fox', 0.038), ('biochemical', 0.037), ('rois', 0.037), ('please', 0.037), ('diverse', 0.036), ('constraints', 0.036), ('plane', 0.035), ('mi', 0.035), ('aided', 0.034), ('centralized', 0.034), ('molecule', 0.034), ('qcqp', 0.034), ('slack', 0.034), ('constrained', 0.034), ('li', 0.032), ('witnesses', 0.032), ('max', 0.032), ('hyperplane', 0.032), ('unseen', 0.031), ('wt', 0.031), ('cj', 0.031), ('elephant', 0.031), ('categorization', 0.03), ('regarded', 0.03), ('taylor', 0.03), ('rows', 0.03), ('benchmark', 0.029), ('roi', 0.029), ('utilize', 0.029), ('classi', 0.029), ('page', 0.028), ('cloud', 0.028), ('nk', 0.027), ('tao', 0.027), ('tth', 0.027), ('ji', 0.027), ('concerned', 0.027), ('opposite', 0.027), ('interests', 0.026), ('binding', 0.026), ('aw', 0.026), ('expansions', 0.026), ('classify', 0.026), ('inside', 0.026), ('dietterich', 0.025), ('labeled', 0.024), ('convex', 0.024), ('totally', 0.023), ('utilizing', 0.023), ('firstly', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="36-tfidf-1" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>Author: Yanjun Han, Qing Tao, Jue Wang</p><p>Abstract: In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we deﬁne a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.</p><p>2 0.32254636 <a title="36-tfidf-2" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>Author: Fuxin Li, Cristian Sminchisescu</p><p>Abstract: We propose an approach to multiple-instance learning that reformulates the problem as a convex optimization on the likelihood ratio between the positive and the negative class for each training instance. This is casted as joint estimation of both a likelihood ratio predictor and the target (likelihood ratio variable) for instances. Theoretically, we prove a quantitative relationship between the risk estimated under the 0-1 classiﬁcation loss, and under a loss function for likelihood ratio. It is shown that likelihood ratio estimation is generally a good surrogate for the 0-1 loss, and separates positive and negative instances well. The likelihood ratio estimates provide a ranking of instances within a bag and are used as input features to learn a linear classiﬁer on bags of instances. Instance-level classiﬁcation is achieved from the bag-level predictions and the individual likelihood ratios. Experiments on synthetic and real datasets demonstrate the competitiveness of the approach.</p><p>3 0.21720135 <a title="36-tfidf-3" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>Author: Jie Luo, Francesco Orabona</p><p>Abstract: In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classiﬁers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difﬁcult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efﬁcient algorithm to solve it. Experiments conducted on artiﬁcial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.</p><p>4 0.11770437 <a title="36-tfidf-4" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classiﬁcation models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods. 1</p><p>5 0.10433047 <a title="36-tfidf-5" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>Author: Sheng-jun Huang, Rong Jin, Zhi-hua Zhou</p><p>Abstract: Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in ﬁnding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed Q UIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed Q UIRE approach outperforms several state-of -the-art active learning approaches. 1</p><p>6 0.087466531 <a title="36-tfidf-6" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>7 0.085272573 <a title="36-tfidf-7" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>8 0.078227103 <a title="36-tfidf-8" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>9 0.074344195 <a title="36-tfidf-9" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>10 0.072497927 <a title="36-tfidf-10" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>11 0.068520382 <a title="36-tfidf-11" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>12 0.060689308 <a title="36-tfidf-12" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>13 0.060200889 <a title="36-tfidf-13" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>14 0.058719732 <a title="36-tfidf-14" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>15 0.056974288 <a title="36-tfidf-15" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>16 0.055908654 <a title="36-tfidf-16" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>17 0.055718143 <a title="36-tfidf-17" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>18 0.055522636 <a title="36-tfidf-18" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>19 0.054801766 <a title="36-tfidf-19" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>20 0.054064818 <a title="36-tfidf-20" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, 0.065), (2, 0.053), (3, -0.092), (4, 0.063), (5, 0.05), (6, -0.027), (7, -0.066), (8, 0.022), (9, -0.15), (10, -0.034), (11, 0.025), (12, 0.012), (13, 0.087), (14, 0.022), (15, -0.031), (16, -0.228), (17, 0.019), (18, 0.068), (19, 0.028), (20, 0.035), (21, -0.088), (22, -0.013), (23, 0.121), (24, 0.045), (25, 0.125), (26, 0.139), (27, -0.18), (28, -0.214), (29, 0.025), (30, 0.02), (31, 0.086), (32, 0.132), (33, 0.018), (34, -0.119), (35, 0.074), (36, 0.036), (37, 0.022), (38, -0.173), (39, -0.201), (40, 0.121), (41, 0.064), (42, 0.034), (43, -0.02), (44, -0.056), (45, -0.059), (46, 0.073), (47, -0.03), (48, 0.014), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96059686 <a title="36-lsi-1" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>Author: Yanjun Han, Qing Tao, Jue Wang</p><p>Abstract: In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we deﬁne a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.</p><p>2 0.80228782 <a title="36-lsi-2" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>Author: Fuxin Li, Cristian Sminchisescu</p><p>Abstract: We propose an approach to multiple-instance learning that reformulates the problem as a convex optimization on the likelihood ratio between the positive and the negative class for each training instance. This is casted as joint estimation of both a likelihood ratio predictor and the target (likelihood ratio variable) for instances. Theoretically, we prove a quantitative relationship between the risk estimated under the 0-1 classiﬁcation loss, and under a loss function for likelihood ratio. It is shown that likelihood ratio estimation is generally a good surrogate for the 0-1 loss, and separates positive and negative instances well. The likelihood ratio estimates provide a ranking of instances within a bag and are used as input features to learn a linear classiﬁer on bags of instances. Instance-level classiﬁcation is achieved from the bag-level predictions and the individual likelihood ratios. Experiments on synthetic and real datasets demonstrate the competitiveness of the approach.</p><p>3 0.74814087 <a title="36-lsi-3" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>Author: Jie Luo, Francesco Orabona</p><p>Abstract: In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classiﬁers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difﬁcult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efﬁcient algorithm to solve it. Experiments conducted on artiﬁcial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.</p><p>4 0.44016194 <a title="36-lsi-4" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classiﬁcation models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods. 1</p><p>5 0.39164203 <a title="36-lsi-5" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>Author: Sheng-jun Huang, Rong Jin, Zhi-hua Zhou</p><p>Abstract: Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in ﬁnding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed Q UIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed Q UIRE approach outperforms several state-of -the-art active learning approaches. 1</p><p>6 0.38754171 <a title="36-lsi-6" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>7 0.34617451 <a title="36-lsi-7" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>8 0.33593774 <a title="36-lsi-8" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>9 0.33259228 <a title="36-lsi-9" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>10 0.31746534 <a title="36-lsi-10" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>11 0.3085683 <a title="36-lsi-11" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>12 0.2937372 <a title="36-lsi-12" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>13 0.29146367 <a title="36-lsi-13" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>14 0.28839663 <a title="36-lsi-14" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>15 0.28780794 <a title="36-lsi-15" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>16 0.28399557 <a title="36-lsi-16" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>17 0.27719858 <a title="36-lsi-17" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>18 0.27267602 <a title="36-lsi-18" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>19 0.25361827 <a title="36-lsi-19" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>20 0.25331205 <a title="36-lsi-20" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.055), (17, 0.025), (27, 0.036), (30, 0.039), (35, 0.033), (45, 0.188), (50, 0.042), (52, 0.037), (60, 0.052), (74, 0.251), (77, 0.041), (78, 0.066), (90, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87133574 <a title="36-lda-1" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, Ryota Tomioka</p><p>Abstract: Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only ﬁnd a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.</p><p>2 0.7948578 <a title="36-lda-2" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>Author: Abhay Jha, Vibhav Gogate, Alexandra Meliou, Dan Suciu</p><p>Abstract: Lifted Inference algorithms for representations that combine ﬁrst-order logic and graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efﬁciency by exploiting repeated structure in the ﬁrst-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we deﬁne a set of rules that look only at the logical representation to identify models for which exact efﬁcient inference is possible. Our rules yield new tractable classes that could not be solved efﬁciently by any of the existing techniques. 1</p><p>same-paper 3 0.79239511 <a title="36-lda-3" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>Author: Yanjun Han, Qing Tao, Jue Wang</p><p>Abstract: In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoiding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we deﬁne a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.</p><p>4 0.66535723 <a title="36-lda-4" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>5 0.65674078 <a title="36-lda-5" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>6 0.65639555 <a title="36-lda-6" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>7 0.65404159 <a title="36-lda-7" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>8 0.65055329 <a title="36-lda-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.6498118 <a title="36-lda-9" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>10 0.64943641 <a title="36-lda-10" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>11 0.64821124 <a title="36-lda-11" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>12 0.64820111 <a title="36-lda-12" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>13 0.64670664 <a title="36-lda-13" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>14 0.64614046 <a title="36-lda-14" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>15 0.6458323 <a title="36-lda-15" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>16 0.64579809 <a title="36-lda-16" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>17 0.64574832 <a title="36-lda-17" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>18 0.64567959 <a title="36-lda-18" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>19 0.6450457 <a title="36-lda-19" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>20 0.64486194 <a title="36-lda-20" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
