<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 nips-2010-A biologically plausible network for the computation of orientation dominance</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-17" href="#">nips2010-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 nips-2010-A biologically plausible network for the computation of orientation dominance</h1>
<br/><p>Source: <a title="nips-2010-17-pdf" href="http://papers.nips.cc/paper/3982-a-biologically-plausible-network-for-the-computation-of-orientation-dominance.pdf">pdf</a></p><p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>Reference: <a title="nips-2010-17-reference" href="../nips2010_reference/nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A biologically plausible network for the computation of orientation dominance Nuno Vasconcelos Statistical Visual Computing Laboratory University of California San Diego La Jolla, CA 92039 nuno@ece. [sent-1, score-0.695]
</p><p>2 edu  Abstract The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. [sent-4, score-0.582]
</p><p>3 This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. [sent-5, score-0.57]
</p><p>4 It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. [sent-6, score-0.225]
</p><p>5 The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. [sent-8, score-0.149]
</p><p>6 The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. [sent-9, score-0.281]
</p><p>7 This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. [sent-11, score-0.257]
</p><p>8 1  Introduction  In the past decade, computer vision research in object recognition has ﬁrmly established the efﬁcacy of representing images as collections of local descriptors of edge orientation. [sent-12, score-0.157]
</p><p>9 These descriptors are usually based on histograms of dominant orientation, for example, the edge orientation histograms of [1], the SIFT descriptor of [2], or the HOG features of [3]. [sent-13, score-0.662]
</p><p>10 The SIFT descriptor is heavily inspired by known computations of the early visual cortex [2], but has no formal detailed connection to computational neuroscience. [sent-15, score-0.326]
</p><p>11 One property that has always appeared essential to the robustness of biological vision is the ability of individual cells to adapt their dynamic range to the strength of the visual stimulus. [sent-18, score-0.242]
</p><p>12 This adaptation appears as early as in the retina [5], is prevalent throughout the visual cortex [6], and seems responsible for the remarkable ability of the visual system to adapt to lighting variations. [sent-19, score-0.287]
</p><p>13 Within the last decade, it has been explained by the implementation of gain control in individual neurons, through the divisive normalization of their responses by those of their neighbors [7, 8]. [sent-20, score-0.305]
</p><p>14 Again, hundreds of papers have been written on divisive normalization, and its consequences for visual processing. [sent-21, score-0.153]
</p><p>15 Today, there appears to be little dispute about its role as a component of the standard neurophysiological model of early vision [9]. [sent-22, score-0.186]
</p><p>16 We start by formulating the central motivating question for descriptors such as SIFT or HOG, how to represent locally dominant image orientation, as a decision-theoretic problem. [sent-25, score-0.192]
</p><p>17 An orientation θ is deﬁned as dominant, at a location l of the visual ﬁeld, if the Gabor response of orientation θ at l, xθ (l), is both large and distinct from those of other orientations. [sent-26, score-1.063]
</p><p>18 An optimal statistical test is then derived to determine if xθ (l) is distinct from the responses of remaining orientations. [sent-27, score-0.213]
</p><p>19 The core of this test is the posterior probability of orientation of the visual stimulus at l, given xθ (l). [sent-28, score-0.551]
</p><p>20 The dominance of orientation θ, within a neighborhood R, is then deﬁned as the expected strength of responses xθ (l), in R, which are distinct. [sent-29, score-0.744]
</p><p>21 This is shown to be a sum of the response amplitudes |xθ (l)| across R, with each location weighted by the posterior probability that it contains stimulus of orientation θ. [sent-30, score-0.649]
</p><p>22 The resulting representation of orientation is similar to that of SIFT, which assigns each point to a dominant orientation and integrates responses over R. [sent-31, score-1.054]
</p><p>23 The main difference is that a location could contribute to more than one orientation, since the expected strength relies on a soft assignment of locations to orientations, according to their posterior orientation probability. [sent-32, score-0.48]
</p><p>24 The proposed measure of orientation dominance can then be seen as a biologically plausible version of that used by SIFT, and is denoted by bioSIFT. [sent-34, score-0.673]
</p><p>25 BioSIFT units are shown to exhibit the trademark properties of V1 neurons: their responses are closely ﬁt by the Naka-Rushton equation [11], and they exhibit an inhibitory behavior, known as cross-orientation suppression, which is ubiquitous in V1 [12]. [sent-35, score-0.261]
</p><p>26 The main practical beneﬁt of bioSIFT is to improve the performance of biologically plausible recognition networks, whose performance it brings close to the level of the state of the art in computer vision. [sent-38, score-0.148]
</p><p>27 In the process of doing this, it points to the importance of divisive normalization in vision. [sent-39, score-0.159]
</p><p>28 It is shown that the simple replacement of Gabor ﬁlter responses with the normalized orientation descriptors of bioSIFT produces very signiﬁcant gains in recognition accuracy. [sent-42, score-0.648]
</p><p>29 This points to the alternative hypothesis that the fundamental role of contrast normalization is to determine orientation dominance. [sent-44, score-0.503]
</p><p>30 The ubiquity of orientation processing in visual cortex suggests that the estimation of local orientation is important for tasks such as object recognition. [sent-53, score-0.961]
</p><p>31 While the classical view was that the brain simply performs a linear decomposition into orientation channels, through Gabor ﬁltering, SIFT representations emphasize the estimation of dominant orientation. [sent-55, score-0.502]
</p><p>32 The latter is a very non-linear operation, involving the comparison of response strength across orientation channels, and requires inter-channel normalization. [sent-56, score-0.539]
</p><p>33 More formal estimates of dominant orientation can be obtained by formulating the problem in decisiontheoretic terms, and deriving optimal decision rules for its solution. [sent-58, score-0.525]
</p><p>34 For this, we assume that the visual system infers dominant orientation from a set of visual features x ∈ RM , which measure stimulus amplitude at each orientation. [sent-59, score-0.793]
</p><p>35 In this work, we assume these features to be the set of responses Xi = I ◦ Gi of the stimulus I, to a bank of Gabor ﬁlters Gi . [sent-60, score-0.253]
</p><p>36 In principle, determining whether there is a dominant orientation requires the joint inspection of all feature channels Xi . [sent-62, score-0.549]
</p><p>37 A more tractable question is whether the ith channel responses, Xi , are distinct from those of the other channels, Xj , j = i. [sent-64, score-0.215]
</p><p>38 PX|θ (x|i) = PXi (x), this question can be posed as a classiﬁcation problem with two hypotheses of label Y ∈ {0, 1}, where • Y = 1 if the ith channel responses are distinct, i. [sent-67, score-0.336]
</p><p>39 Given the response xi (l) of Xi at location l ∈ R, the minimum probability of error (MPE) decision rule is to declare it distinct when PXi (xi (l))Pθ (i) j PXj (xi (l))Pθ (j)  Pθ|X (i|xi (l)) =  ≥  1 . [sent-73, score-0.253]
</p><p>40 2  (2)  While this test determines if the responses of Xi are distinct from those of Xj=i , it does not determine if Xi is dominant: Xi could be distinct because it is the only feature that does not respond to the stimulus in R. [sent-74, score-0.307]
</p><p>41 The second question is to determine if the responses of Xi are both distinct and large. [sent-75, score-0.213]
</p><p>42 The expected strength of distinct responses in R is then =  |x|PY |X,θ (1|x, i)PX|θ (x|i)dx  (4)  =  EY,X|θ [S(X)|θ = i]  |x|Pθ|X (i|x)PXi (x)dx. [sent-78, score-0.245]
</p><p>43 This measure of the dominance of the ith orientation is a sum of the response amplitudes |xi (l)| across R, with each location weighted by the posterior probability that it contains stimulus of that orientation. [sent-81, score-0.859]
</p><p>44 It is similar to the measure used by SIFT, which assigns each point to a dominant orientation and integrates responses over R. [sent-82, score-0.694]
</p><p>45 The main difference is that a location could contribute to more than one orientation, since the expected strength relies on a soft assignment of locations to orientations, according to their posterior orientation probability. [sent-83, score-0.48]
</p><p>46 The response of a Gabor ﬁlter of orientation θ = 3π/4 is shown in b), and the orientation probability map Pθ|X (i|xi ) in c). [sent-85, score-0.892]
</p><p>47 Note that these probabilities are much smaller than the Gabor responses in the body of the starﬁsh, where the image is textured but there is no signiﬁcant structure of orientation θ. [sent-86, score-0.597]
</p><p>48 On the other hand, they are largest for the locations where the orientation is dominant. [sent-87, score-0.385]
</p><p>49 The combined multiplication by the Gabor responses and averaging over R magniﬁes the responses where the orientation is dominant, suppressing the details due to texture or noise. [sent-89, score-0.719]
</p><p>50 Overall, (6) is large when the ith orientation responses are 1) distinct from those of other channels and 2) large. [sent-91, score-0.67]
</p><p>51 One interesting property is that it penalizes large responses of Xi that are not informative of the presence of stimuli with orientation i. [sent-93, score-0.577]
</p><p>52 Hence, increasing the stimulus contrast does not increase S(Xi )R when responses xi (l) cannot be conﬁdently assigned to the ith orientation. [sent-94, score-0.333]
</p><p>53 This can be seen in Figure 1 f) and h), where the Gabor response and dominance measure are shown for a lowcontrast replica of the image of a). [sent-95, score-0.352]
</p><p>54 While the Gabor responses at low (f) and high (b) contrasts are substantially different, the dominance measure (d and h) stays almost constant. [sent-96, score-0.352]
</p><p>55 It is worth noting that such normalization is accomplished without modeling joint distributions of response across orientations. [sent-98, score-0.197]
</p><p>56 3  Biological plausibility  In this section we study the biological plausibility of the orientation dominance measure of (6). [sent-100, score-0.727]
</p><p>57 |βk  Ŝ(Xi)R  C1 layer  Σ S1 layer  Multi-scale image  image  Figure 2: One channel of the bioSIFT network. [sent-119, score-0.318]
</p><p>58 The simple cell computes the contribution of channel i to the expected value of the dominant response at pixel x, indicated by a ﬁlled box. [sent-121, score-0.452]
</p><p>59 Spatial pooling by the complex cell determines the channel’s contribution to the expected value of the dominant response within the pooling neighborhood. [sent-122, score-0.354]
</p><p>60 2  Biological computations  1 To derive a biologically plausible form of (6) we start by assuming that Pθ (i) = M . [sent-133, score-0.16]
</p><p>61 The computations of (11)-(13) are those performed by simple cells in the standard neurophysiological model of V1. [sent-186, score-0.191]
</p><p>62 Each response xi (l) is divisively normalized by the sum of responses in the neighborhood R, for each orientation channel k, using (13). [sent-189, score-0.926]
</p><p>63 Notice that this implies that the conditional distribution of responses of a channel is learned locally, from the sample of responses in R. [sent-190, score-0.478]
</p><p>64 Altogether, (12) implements the computations of a divisively normalized simple cell. [sent-191, score-0.147]
</p><p>65 The computation of the orientation dominance measure by (10) then corresponds to a complex cell, which pools the simple cell responses in R, modulated by the magnitude of the underlying Gabor responses. [sent-193, score-0.806]
</p><p>66 3  Naka-Rushton ﬁt  In addition to replicating the standard model of V1, the biological plausibility of the bioSIFT features can be substantiated by checking if they reproduce well-established properties of neuronal responses. [sent-197, score-0.165]
</p><p>67 One characteristic property of neural responses of monkey and cat V1 is the tightness with which they can be ﬁt by the Naka-Rushton equation [11]. [sent-198, score-0.21]
</p><p>68 The equation describes the average response to a sinusoidal grating of contrast c as R = Rmax  cq 50  cq + cq  (14)  where Rmax is the maximum mean response, c50 is the semi-saturation contrast i. [sent-199, score-0.395]
</p><p>69 the contrast at which the response is half the saturation value. [sent-201, score-0.187]
</p><p>70 The ﬁt between the contrast response of a bioSIFT unit and the Naka-Rushton function was determined, using the procedure of [11], and is shown in Figure 3 c). [sent-203, score-0.165]
</p><p>71 This suppression is observed by measuring the response of a neuron, tuned to an orientation θ, to a sinusoidal grating of orthogonal orientation (θ ± 90◦ ). [sent-210, score-1.03]
</p><p>72 When presented by itself, the grating barely evokes a response from the neuron. [sent-211, score-0.18]
</p><p>73 However, if superimposed with a grating of another orientation, it signiﬁcantly reduces the response of the neuron to the latter. [sent-212, score-0.18]
</p><p>74 These consist of measuring a simple cell response to a set of sinusoidal plaids obtained by summing 1) a test grating oriented along the cell’s preferred orientation, and 2) a mask grating of orthogonal orientation. [sent-214, score-0.365]
</p><p>75 The cell response is recorded as a function of the contrast of the gratings. [sent-216, score-0.234]
</p><p>76 The stimuli are shown on the left and the neuron’s response on the right. [sent-218, score-0.147]
</p><p>77 From a functional point of view, the great advantage of COS is the resulting increase in selectivity of the orientation channels. [sent-221, score-0.385]
</p><p>78 The ﬁgure shows the results of an experiment that measured the response of 12 Gabor ﬁlters of orientation in [0o , 180o ] to a horizontal grating. [sent-223, score-0.507]
</p><p>79 While both the ﬁrst and twelfth Gabor ﬁlters have relatively large responses to this stimulus, the twelfth channel of bioSIFT is strongly suppressed. [sent-224, score-0.311]
</p><p>80 When combined with the contrast invariance of Figure 1, this leads to a representation with strong orientation discrimination and robustness to lighting variations. [sent-225, score-0.453]
</p><p>81 An example of this is shown in Figure 3 (f) which shows the value of the dominance measure for the most dominant orientation at each image location (in “split screen” with the original image). [sent-226, score-0.767]
</p><p>82 Note how the bioSIFT features capture information about dominant orientation and object shape, suppressing uninformative or noisy pixels. [sent-227, score-0.581]
</p><p>83 A known property of the responses of bandpass features to natural images is a consistent pattern of higher order dependence, characterized by bow-tie shaped conditional distributions between feature pairs. [sent-230, score-0.229]
</p><p>84 This pattern is depicted in Figure 3 g), which shows the histogram of responses of a Gabor feature, conditioned on the response of the co-located feature of an adjacent orientation channel. [sent-231, score-0.674]
</p><p>85 Simoncelli [22] showed that divisively normalizing linear ﬁlter responses reduces these higher-order dependencies, making the features independent. [sent-232, score-0.263]
</p><p>86 As can be seen from (10), (12), and (13), the bioSIFT network divisively normalizes each Gabor response by the sum, across the spatial neighborhood R, of responses from each of the Gabor orientations (11). [sent-233, score-0.416]
</p><p>87 This implies that they are independent (knowledge of the value of one feature does not modify the distribution of responses of the other). [sent-235, score-0.167]
</p><p>88 Another important, and extensively researched, property of V1 responses is their sparseness. [sent-237, score-0.167]
</p><p>89 This is shown in Figure 3 d), which compares the sparseness of the responses of both a Gabor ﬁlter and a bioSIFT unit to a natural image. [sent-241, score-0.201]
</p><p>90 1  Biologically inspired object recognition  Biologically motivated networks for object recognition have been recently the subject of substantial research [13, 23, 14, 15]. [sent-247, score-0.246]
</p><p>91 To evaluate the impact of adding bioSIFT features to these networks, we considered the HMAX network of [13], which mimics the structure of the visual cortex as a cascade of alternating simple and complex cell layers. [sent-248, score-0.304]
</p><p>92 The ﬁrst layer encodes the input image as a set of complex cell responses, and the second layer measures the distance between these responses and a set of learned prototypes. [sent-249, score-0.365]
</p><p>93 In result, the sequence of operations is not the same, there is no guarantee that normalization provides optimal estimates of orientation dominance, or even that it corresponds to optimal statistical learning, as in (8). [sent-293, score-0.487]
</p><p>94 Overall, these results suggest that orientation dominance is an important property for visual recognition. [sent-309, score-0.635]
</p><p>95 Heeger, “Normalization of cell responses in cat striate cortex,” Visual Neuroscience, vol. [sent-346, score-0.279]
</p><p>96 Movshon, “Linearity and normalization in simple cells of macaque primary visual cortex,” Journal of Neuroscience, vol. [sent-353, score-0.205]
</p><p>97 Tolhurst, “Does a bayesian model of v1 contrast coding offer a neurophysiological account of contrast discrimination? [sent-379, score-0.18]
</p><p>98 Poggio, “Object recognition with features inspired by visual cortex,” in IEEE Conf. [sent-389, score-0.201]
</p><p>99 Hamilton, “Striate cortex of monkey and cat: contrast response function,” Journal of Neurophysiology, vol. [sent-424, score-0.225]
</p><p>100 Maffei, “Functional implications of cross orientation inhibition of cortical visual cells i. [sent-432, score-0.515]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('biosift', 0.687), ('orientation', 0.385), ('gabor', 0.192), ('hmax', 0.175), ('responses', 0.167), ('dominance', 0.16), ('channel', 0.144), ('sift', 0.122), ('response', 0.122), ('dominant', 0.117), ('neurophysiological', 0.094), ('visual', 0.09), ('normalization', 0.075), ('biologically', 0.075), ('cell', 0.069), ('px', 0.069), ('divisive', 0.063), ('cortex', 0.06), ('plausibility', 0.059), ('divisively', 0.058), ('grating', 0.058), ('computations', 0.057), ('suppression', 0.05), ('xi', 0.05), ('stimulus', 0.048), ('channels', 0.047), ('network', 0.047), ('distinct', 0.046), ('recognition', 0.045), ('image', 0.045), ('ggd', 0.044), ('pxi', 0.044), ('pxj', 0.044), ('cat', 0.043), ('contrast', 0.043), ('layer', 0.042), ('enhancements', 0.042), ('object', 0.041), ('vision', 0.041), ('cells', 0.04), ('units', 0.039), ('biological', 0.039), ('carandini', 0.038), ('features', 0.038), ('neurons', 0.037), ('yang', 0.036), ('histograms', 0.035), ('scene', 0.035), ('location', 0.035), ('sparseness', 0.034), ('cq', 0.033), ('cvpr', 0.033), ('implements', 0.032), ('lter', 0.032), ('strength', 0.032), ('amplitudes', 0.031), ('sinusoidal', 0.03), ('descriptors', 0.03), ('dispute', 0.029), ('gesture', 0.029), ('substantiated', 0.029), ('tolhurst', 0.029), ('trademark', 0.029), ('caltech', 0.029), ('cos', 0.029), ('posterior', 0.028), ('inspired', 0.028), ('mask', 0.028), ('plausible', 0.028), ('lters', 0.027), ('justi', 0.027), ('operations', 0.027), ('lazebnik', 0.027), ('py', 0.027), ('inhibitory', 0.026), ('receptive', 0.026), ('heeger', 0.026), ('networks', 0.025), ('qj', 0.025), ('stimuli', 0.025), ('ith', 0.025), ('measure', 0.025), ('lighting', 0.025), ('connection', 0.024), ('al', 0.024), ('bandpass', 0.024), ('barlow', 0.024), ('nuno', 0.024), ('pinto', 0.024), ('saturating', 0.024), ('vasconcelos', 0.024), ('pooling', 0.023), ('formal', 0.023), ('descriptor', 0.022), ('orientations', 0.022), ('early', 0.022), ('saturation', 0.022), ('substantial', 0.021), ('gains', 0.021), ('importance', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="17-tfidf-1" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>2 0.15734833 <a title="17-tfidf-2" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>3 0.14165263 <a title="17-tfidf-3" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>Author: Stella X. Yu</p><p>Abstract: Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation. 1</p><p>4 0.11678886 <a title="17-tfidf-4" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>Author: Jose Puertas, Joerg Bornschein, Joerg Luecke</p><p>Abstract: We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we ﬁnd that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reﬂects properties of simple cell receptive ﬁelds that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the ﬁrst large-scale application of such an approach. 1</p><p>5 0.10701454 <a title="17-tfidf-5" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>6 0.1026597 <a title="17-tfidf-6" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>7 0.09278217 <a title="17-tfidf-7" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>8 0.08982259 <a title="17-tfidf-8" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>9 0.083299704 <a title="17-tfidf-9" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>10 0.077897787 <a title="17-tfidf-10" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>11 0.071777202 <a title="17-tfidf-11" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>12 0.068134516 <a title="17-tfidf-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.064847834 <a title="17-tfidf-13" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>14 0.063449927 <a title="17-tfidf-14" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>15 0.063272901 <a title="17-tfidf-15" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>16 0.061423004 <a title="17-tfidf-16" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>17 0.057808928 <a title="17-tfidf-17" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>18 0.057537846 <a title="17-tfidf-18" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>19 0.056715541 <a title="17-tfidf-19" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>20 0.053560972 <a title="17-tfidf-20" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, 0.073), (2, -0.207), (3, -0.034), (4, 0.052), (5, 0.031), (6, -0.012), (7, 0.036), (8, 0.003), (9, 0.036), (10, 0.024), (11, -0.03), (12, -0.076), (13, -0.048), (14, -0.025), (15, -0.024), (16, 0.042), (17, -0.037), (18, 0.001), (19, 0.111), (20, 0.053), (21, -0.05), (22, 0.1), (23, 0.033), (24, -0.04), (25, 0.076), (26, 0.011), (27, 0.048), (28, 0.029), (29, 0.016), (30, -0.053), (31, -0.075), (32, 0.013), (33, -0.013), (34, 0.042), (35, -0.051), (36, 0.001), (37, -0.05), (38, 0.078), (39, 0.003), (40, 0.035), (41, -0.011), (42, 0.061), (43, 0.002), (44, 0.024), (45, 0.045), (46, 0.095), (47, -0.079), (48, 0.186), (49, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94586933 <a title="17-lsi-1" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>2 0.83081859 <a title="17-lsi-2" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>Author: Stella X. Yu</p><p>Abstract: Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent ﬂickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and ﬁnd out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation. 1</p><p>3 0.54705369 <a title="17-lsi-3" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>Author: Jose Puertas, Joerg Bornschein, Joerg Luecke</p><p>Abstract: We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we ﬁnd that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reﬂects properties of simple cell receptive ﬁelds that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the ﬁrst large-scale application of such an approach. 1</p><p>4 0.54649383 <a title="17-lsi-4" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>5 0.51596957 <a title="17-lsi-5" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>6 0.50302446 <a title="17-lsi-6" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>7 0.49855059 <a title="17-lsi-7" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>8 0.49183601 <a title="17-lsi-8" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>9 0.48996046 <a title="17-lsi-9" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>10 0.48752797 <a title="17-lsi-10" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>11 0.48482141 <a title="17-lsi-11" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>12 0.48327705 <a title="17-lsi-12" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>13 0.46407938 <a title="17-lsi-13" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>14 0.46317858 <a title="17-lsi-14" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>15 0.4619568 <a title="17-lsi-15" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>16 0.45817938 <a title="17-lsi-16" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>17 0.44534278 <a title="17-lsi-17" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>18 0.43451568 <a title="17-lsi-18" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>19 0.42717496 <a title="17-lsi-19" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>20 0.42266703 <a title="17-lsi-20" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.021), (17, 0.025), (27, 0.154), (30, 0.038), (35, 0.022), (45, 0.205), (50, 0.063), (52, 0.056), (60, 0.027), (77, 0.076), (79, 0.198), (90, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96219105 <a title="17-lda-1" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>same-paper 2 0.87193614 <a title="17-lda-2" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>3 0.84264845 <a title="17-lda-3" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>4 0.8135637 <a title="17-lda-4" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>5 0.80689967 <a title="17-lda-5" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>6 0.80285782 <a title="17-lda-6" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>7 0.7906847 <a title="17-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.78975046 <a title="17-lda-8" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>9 0.78363442 <a title="17-lda-9" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>10 0.78355986 <a title="17-lda-10" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>11 0.78294921 <a title="17-lda-11" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>12 0.77864933 <a title="17-lda-12" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>13 0.77821243 <a title="17-lda-13" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>14 0.77720463 <a title="17-lda-14" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>15 0.7765919 <a title="17-lda-15" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>16 0.77205878 <a title="17-lda-16" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>17 0.76844496 <a title="17-lda-17" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>18 0.76752871 <a title="17-lda-18" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>19 0.76682687 <a title="17-lda-19" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>20 0.76547825 <a title="17-lda-20" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
