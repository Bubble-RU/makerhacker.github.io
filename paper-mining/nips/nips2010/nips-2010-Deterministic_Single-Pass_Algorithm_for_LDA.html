<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 nips-2010-Deterministic Single-Pass Algorithm for LDA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-60" href="#">nips2010-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 nips-2010-Deterministic Single-Pass Algorithm for LDA</h1>
<br/><p>Source: <a title="nips-2010-60-pdf" href="http://papers.nips.cc/paper/3959-deterministic-single-pass-algorithm-for-lda.pdf">pdf</a></p><p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>Reference: <a title="nips-2010-60-reference" href="../nips2010_reference/nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. [sent-12, score-0.333]
</p><p>2 Our algorithm does not need to store old statistics for all data. [sent-13, score-0.36]
</p><p>3 The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments. [sent-14, score-0.539]
</p><p>4 1 Introduction Huge quantities of text data such as news articles and blog posts arrives in a continuous stream. [sent-15, score-0.139]
</p><p>5 Online learning has attracted a great deal of attention as a useful method for handling this growing quantity of streaming data because it processes data one at a time, whereas batch algorithms are not feasible in these settings because they need all the data at the same time. [sent-16, score-0.193]
</p><p>6 This paper focus on online learning for Latent Dirichlet allocation (LDA) (Blei et al. [sent-17, score-0.109]
</p><p>7 Existing studies were based on sampling methods such as the incremental Gibbs sampler and particle ﬁlter. [sent-23, score-0.271]
</p><p>8 Moreover, sampling algorithms often need a resampling step in which a sampling method is applied to old data. [sent-26, score-0.265]
</p><p>9 Storing old data or old samples adversely affects the good properties of online algorithms. [sent-27, score-0.519]
</p><p>10 If the number of topics is T , the vocabulary size is V and m, so the required memory size is O(m ∗ T ∗ V ). [sent-31, score-0.152]
</p><p>11 We propose two deterministic online algorithms; an incremental algorithms and a single-pass algorithm. [sent-32, score-0.303]
</p><p>12 Our incremental algorithm is an incremental variant of the reverse EM (REM) algorithm (Minka, 2001). [sent-33, score-0.572]
</p><p>13 The incremental algorithm updates parameters by replacing old sufﬁcient statistics with new one for each datum. [sent-34, score-0.648]
</p><p>14 Our single-pass algorithm is based on an incremental algorithm, but it does not need to store old statistics for all data. [sent-35, score-0.599]
</p><p>15 In our single-pass algorithm, we propose a sequential update method for the Dirichlet parameters. [sent-36, score-0.115]
</p><p>16 (2009) indicated the importance of estimating the parameters of the Dirichlet distribution, which is the distribution over the topic distributions of documents. [sent-39, score-0.132]
</p><p>17 Moreover, we can deal with the growing vocabulary size. [sent-40, score-0.099]
</p><p>18 VB-LDA is the variational inference for LDA, which is a batch inference; CVB-LDA is the collapsed variational inference for LDA (Teh et al. [sent-46, score-0.402]
</p><p>19 , 2007); iREM-LDA is our incremental algorithm; and sREM-LDA is our single-pass algorithm for LDA. [sent-47, score-0.273]
</p><p>20 2  Overview of Latent Dirichlet Allocation  This section overviews LDA where documents are represented as random mixtures over latent topics and each topic is characterized by a distribution over words. [sent-52, score-0.337]
</p><p>21 θ j denotes a T -dimensional probability vector that is the parameters of the multinomial distribution, and represents the topic distribution of document j. [sent-62, score-0.406]
</p><p>22 β t is a multinomial parameter a V -dimensional probability where βt,v speciﬁes the probability of generating word v given topic t. [sent-63, score-0.247]
</p><p>23 For each of the T topics t, draw β t ∼ Dir(β|λ) ∝ ∏ ∏ λ−1 α βt,v . [sent-66, score-0.1]
</p><p>24 For each of the M documents j, draw θ j ∼ Dir(θ|α) where Dir(θ|α) ∝ θt t −1 . [sent-67, score-0.111]
</p><p>25 v t  For each of the Nj words wj,i in document j, draw topic zj,i ∼ M ulti(z|θ j ) and draw word wj,i ∼ p(w|zj,i , β) where p(w = v|z = t, β) = βt,v . [sent-68, score-0.526]
</p><p>26 That is to say, the complete-data likelihood of a document wj is given by p(wj , z j , θ j |α, β) = p(θ j |α)  Nj ∏  p(wj,i |zj,i , β)p(z j |θ j ). [sent-69, score-0.272]
</p><p>27 q(z)  (6)  The derivation of the update equation for q(z) is slightly complicated and involves approximations to compute intractable summations. [sent-98, score-0.115]
</p><p>28 An update using only zero-order information is given by j ∑ ∑ λ + n−j,i t,wj,i −j,i ∝ ϕj,i,t , nt,v = ϕj,i,t I(wj,i = v), ∑ −j,i (αt + nj,t ), nj,t = V λ + v nt,v i=1 j,i  N  ϕj,i,t  (7)  where “-j,i” denotes subtracting ϕj,i,t . [sent-102, score-0.136]
</p><p>29 3 Deterministic Online Algorithm for LDA The purpose of this study is to process text data such as news articles and blog posts arriving in a continuous stream by using LDA. [sent-104, score-0.185]
</p><p>30 For these situations, we want to process text one at a time and then discard them. [sent-106, score-0.109]
</p><p>31 We repeat iterations only for each word within a document. [sent-107, score-0.113]
</p><p>32 That is, we update parameters from an arriving document and discard the document after doing l iterations. [sent-108, score-0.673]
</p><p>33 First, we derived an incremental algorithm for LDA, and then we extended the incremental algorithm to a single-pass algorithm. [sent-110, score-0.546]
</p><p>34 1 Incremental Learning (Neal and Hinton, 1998) provided a framework of incremental learning for the EM algorithm. [sent-112, score-0.239]
</p><p>35 In general unsupervised-learning, we estimate sufﬁcient statistics si for each data i, compute whole 3  ∑ sufﬁcient statistics σ(= i si ) from all data, and update parameters by using σ. [sent-113, score-0.251]
</p><p>36 In incremental learning, for each data i, we estimate si , compute σ (i) from si , and update parameters from σ (i) . [sent-114, score-0.402]
</p><p>37 It is easy to extend an existing batch algorithm to the incremental learning if whole sufﬁcient statistics or parameters updates are constructed by simply summarizing all data statistics. [sent-115, score-0.567]
</p><p>38 The incremental algorithm processes data i by subtracting old sold and adding new snew , i. [sent-116, score-0.615]
</p><p>39 i i i i The incremental algorithm needs to store old statistics {sold } for all data. [sent-119, score-0.599]
</p><p>40 While batch algorithms i update parameters sweeping through all data, the incremental algorithm updates parameters for each data one at a time, which results in more parameter updates than batch algorithms. [sent-120, score-0.858]
</p><p>41 Therefore, the incremental algorithm sometimes converge faster than batch algorithms. [sent-121, score-0.387]
</p><p>42 2 Incremental Learning for LDA Our motivation for devising the incremental algorithm for LDA was to compare CVB-LDA and VB-LDA. [sent-123, score-0.273]
</p><p>43 Statistics {nt,v } and {nj,t } are updated after each word is updated in CVB-LDA. [sent-124, score-0.165]
</p><p>44 This update schedule is similar to that of the incremental algorithm. [sent-125, score-0.354]
</p><p>45 This incremental property seems to be the reason CVB-LDA converges faster than VB-LDA. [sent-126, score-0.239]
</p><p>46 Below, let us consider the incremental algorithm for LDA. [sent-128, score-0.273]
</p><p>47 ϕj,i,t Γ(Nj + t αt ) t Γ(αt ) j,i,t j ˆ The maximum of F[q(z)] with respect to q(zj,i = t) = ϕj,i,t and β is given by ∑ ∑ ϕj,i,t ∝ βt,wj,i exp{Ψ(αt + ϕj,i,t )}, βtv ∝ λ + nj,t,v , i  (11)  (12)  j  The updates of α are the same as Eq. [sent-133, score-0.102]
</p><p>48 Equation (12) incrementally updates the topic distribution of a document for each word as in CVB-LDA because we do not need γj,i in Eq. [sent-137, score-0.598]
</p><p>49 That is, when we compare this algorithm with VB-LDA, it looks like a hybrid variant of a batch updates for α and β, and incremental updates for γ j , Here, we consider an incremental update for β to be analogous to CVBLDA, in which β is updated for each word. [sent-141, score-0.986]
</p><p>50 Note that in the LDA setup, each independent identically distributed data point is a document not a word. [sent-142, score-0.242]
</p><p>51 Therefore, we incrementally estimate β for each document by swapping ∑N statistics nj,t,v = i j ϕj,i,t I(wj,i = v) which is the number of word v generated from topic t in document j. [sent-143, score-0.771]
</p><p>52 This algorithm incrementally optimizes the lower bound in Eq. [sent-145, score-0.096]
</p><p>53 {λ + j Our single-pass algorithm sequentially sets a prior for each arrived document. [sent-173, score-0.126]
</p><p>54 First, we update parameters from j-th arrived document given prior parameters {λt,v } for l iterations (j)  (j)  ϕj,i,t ∝βt,wj,i exp{Ψ(αt +  ∑  (j)  (j−1)  ϕj,i,t )}, βt,v ∝ λt,v  +  i (0)  Nj ∑  ϕj,i,t I(wj,i = v),  (13)  i  (j)  where λt,v = λ and αt is explained below. [sent-175, score-0.479]
</p><p>55 Then, we set prior parameters by using statistics from the document for the next document as follows, and ﬁnally discard the document. [sent-176, score-0.599]
</p><p>56 (14)  i  Since the updates are repeated within a document, we need to store statistics {ϕj,i,t } for each word in a document, but not for all words in all documents. [sent-178, score-0.294]
</p><p>57 In the CVB and iREM algorithms, the Dirichlet parameter, α, uses batch updates, i. [sent-179, score-0.114]
</p><p>58 , α is updated by using the entire document once in one iteration. [sent-181, score-0.283]
</p><p>59 However, unlike parameter βt,v , the update of α in Eq. [sent-183, score-0.115]
</p><p>60 Therefore, we derive a single-pass update for the Dirichlet parameter α using the following interpretation. [sent-185, score-0.115]
</p><p>61 (5) to be the expectation of αt over posterior G(αt |˜t , ˜ given documents D and a b) at − 1 ˜ new prior G(αt |a0 , b0 ), i. [sent-187, score-0.142]
</p><p>62 e, αt = E[αt ]G(α|˜t ,˜ = , where a b) ˜ b at =a0 + ˜  M ∑ j  aj,t , ˜ = b0 + b  M ∑  bj ,  (15)  j  old old old old old aj,t = {Ψ(αt + nj,t ) − Ψ(αt )}αt , bj = Ψ(Nj + α0 ) − Ψ(α0 ). [sent-188, score-1.364]
</p><p>63 5  (16)  We regard aj,t and bj as statistics for each document, which indicates that the parameters that we actually update are at and ˜ in Eq. [sent-189, score-0.255]
</p><p>64 These updates are simple summarizations of aj,t and bj and ˜ b (j) prior parameters a0 and b0 . [sent-191, score-0.215]
</p><p>65 b  Analysis  This section analyze the proposed updates for parameters α and β in the previous section. [sent-195, score-0.102]
</p><p>66 We eventually update parameters α(j) and β (j) given document j as ∑j−1 a0 − 1 + d ad,t + aj,t bj (j) (j−1) α α aj,t α = αt (1 − ηj ) + ηj , ηj = αt = ∑j−1 ∑j . [sent-196, score-0.439]
</p><p>67 bj b0 + d bd + bj b0 + d bd  (19)  ∑j−1  nd,t,v + nj,t,v (Vj − Vj−1 )λ + nj,t,· (j−1) β β nj,t,v d = βt,v (1 − ηj ) + ηj , ηβ = . [sent-197, score-0.214]
</p><p>68 Our single-pass algorithm sequentially sets a prior for each arrived document, and so we can select a prior (a dimension of Dirichlet distribution) corresponding to observed vocabulary. [sent-199, score-0.157]
</p><p>69 In fact, this property is useful for our problem because the vocabulary size is growing in the text stream. [sent-200, score-0.157]
</p><p>70 These updates β α indicate that ηj and ηj interpolate the parameters estimated from old and new data. [sent-201, score-0.373]
</p><p>71 Monro, 1951; Sato and Ishii, 2000), although a stepsize algorithm interpolates sufﬁcient statistics whereas our updates interpolate parameters. [sent-204, score-0.241]
</p><p>72 In our updates, how we set the stepsize for parameter updates is equivalent to how we set the hyperparameters for priors. [sent-205, score-0.143]
</p><p>73 (j)  βt,v =  λ+  In our update of β, the appearance rate of word v in topic t in document j, nj,t,v /nj,t,· , is added (j−1) β to old parameter βt,v with weight ηj , which gradually decreases as the document is observed. [sent-207, score-1.054]
</p><p>74 Therefore, the inﬂuence of new data decreases as the number of document observations increases as shown in Theorem 1. [sent-209, score-0.242]
</p><p>75 Moreover, Theorem 1 is an important role in analyzing the convergence of parameter updates by using the super-martingale convergence theorem (Bertsekas and Tsitsiklis, 1996; Brochu et al. [sent-210, score-0.135]
</p><p>76 6  4  Experiments  We carried out experiments on document modeling in terms of perplexity. [sent-217, score-0.242]
</p><p>77 The ﬁrst was “Associated Press(AP)” where the number of documents was M = 10, 000 and the vocabulary size was V = 67, 291. [sent-219, score-0.163]
</p><p>78 The comparison metric for document modeling was the “test set perplexity”. [sent-222, score-0.242]
</p><p>79 We randomly split both data sets into a training set and a test set by assigninig 20% of the words in each document to the test set. [sent-223, score-0.265]
</p><p>80 CVB0 and CVB are collapsed variational inference for LDA using zero-order and second-order information, respectively. [sent-232, score-0.156]
</p><p>81 iREM represents the incremental reverse EM algorithm in Algorithm 3. [sent-233, score-0.299]
</p><p>82 CVB0 and CVB estimates the Dirichlet parameter α over the topic distribution for all datasets, i. [sent-234, score-0.132]
</p><p>83 L denotes the number of iterations for whole documents in Algorithms 1 and 2. [sent-238, score-0.14]
</p><p>84 l denotes the number of iterations within a document in Algorithm 4. [sent-240, score-0.272]
</p><p>85 Figure 2 demonstrates the results of experiments on the test set perplexity where lower values indicates better performance. [sent-242, score-0.234]
</p><p>86 PF and sREM calculate the test set perplexity after sweeping through all traing set. [sent-244, score-0.247]
</p><p>87 sREM does not outperform iREM in terms of perplexities, however, the performance of sREM is close to that of iREM As a results, we recommend sREM in a large number of documents or document streams. [sent-248, score-0.33]
</p><p>88 sREM does not need to store old statistics for all documents unlike other algorithms. [sent-249, score-0.414]
</p><p>89 Since we process each document individually, we can control the number of iterations corresponding to the length of each arrived document. [sent-251, score-0.333]
</p><p>90 5  Conclusions  We developed a deterministic online-learning algorithm for latent Dirichlet allocation (LDA). [sent-260, score-0.136]
</p><p>91 The proposed algorithm can be applied to excess text data in a continuous stream because it processes received documents one at a time and then discard them. [sent-261, score-0.254]
</p><p>92 The proposed algorithm was much faster than a batch algorithm and was comparable to the batch algorithm in terms of perplexity in experiments. [sent-262, score-0.539]
</p><p>93 (a) and (b) compared test set perplexity with respect to the number of topics. [sent-295, score-0.209]
</p><p>94 (c), (d), (e) and (f) compared test set perplexity with respect to the number of iterations in topic T = 100 and T = 300, respectively. [sent-296, score-0.371]
</p><p>95 (g) and (h) show the relationships between test set perplexity and the number of iterations within a document, i. [sent-297, score-0.239]
</p><p>96 On-line lda: Adaptive topic models for mining text streams with applications to topic detection and tracking. [sent-301, score-0.373]
</p><p>97 Topic models over text streams: A study of batch and online unsupervised learning. [sent-313, score-0.211]
</p><p>98 Owed to a martingale: A fast bayesian on-line em algorithm for multinomial models, 2004. [sent-332, score-0.113]
</p><p>99 A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation. [sent-390, score-0.23]
</p><p>100 Efﬁcient methods for topic model inference on streaming document collections. [sent-404, score-0.473]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('srem', 0.42), ('cvb', 0.324), ('lda', 0.296), ('irem', 0.286), ('document', 0.242), ('old', 0.24), ('incremental', 0.239), ('perplexity', 0.209), ('vb', 0.173), ('nj', 0.132), ('topic', 0.132), ('pf', 0.13), ('dirichlet', 0.124), ('testset', 0.123), ('update', 0.115), ('batch', 0.114), ('updates', 0.102), ('wsj', 0.1), ('documents', 0.088), ('word', 0.083), ('bj', 0.082), ('ap', 0.078), ('topics', 0.077), ('ons', 0.076), ('vocabulary', 0.075), ('arrived', 0.061), ('itera', 0.061), ('text', 0.058), ('sato', 0.058), ('collapsed', 0.057), ('variational', 0.055), ('streaming', 0.055), ('store', 0.053), ('url', 0.052), ('discard', 0.051), ('asuncion', 0.048), ('dir', 0.048), ('em', 0.047), ('canini', 0.046), ('inference', 0.044), ('minka', 0.043), ('sold', 0.043), ('updated', 0.041), ('stepsize', 0.041), ('teh', 0.041), ('latent', 0.04), ('vj', 0.04), ('incrementally', 0.039), ('online', 0.039), ('alsumait', 0.038), ('snew', 0.038), ('sweeping', 0.038), ('allocation', 0.037), ('algorithm', 0.034), ('ulti', 0.033), ('end', 0.033), ('et', 0.033), ('statistics', 0.033), ('multinomial', 0.032), ('particle', 0.032), ('hours', 0.032), ('posts', 0.031), ('brochu', 0.031), ('interpolate', 0.031), ('rem', 0.031), ('prior', 0.031), ('wj', 0.03), ('blei', 0.03), ('iterations', 0.03), ('blog', 0.029), ('doi', 0.029), ('mimno', 0.029), ('wallach', 0.027), ('streams', 0.026), ('yao', 0.026), ('reverse', 0.026), ('mining', 0.025), ('resampling', 0.025), ('bd', 0.025), ('japan', 0.025), ('tokyo', 0.025), ('indicates', 0.025), ('deterministic', 0.025), ('si', 0.024), ('growing', 0.024), ('arriving', 0.023), ('banerjee', 0.023), ('bertsekas', 0.023), ('neal', 0.023), ('draw', 0.023), ('words', 0.023), ('posterior', 0.023), ('optimizes', 0.023), ('summarizing', 0.023), ('stream', 0.023), ('inferences', 0.023), ('whole', 0.022), ('thomas', 0.022), ('articles', 0.021), ('subtracting', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="60-tfidf-1" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>2 0.4072302 <a title="60-tfidf-2" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>3 0.20540059 <a title="60-tfidf-3" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>4 0.18810776 <a title="60-tfidf-4" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>5 0.14774552 <a title="60-tfidf-5" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>Author: Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David B. Dunson</p><p>Abstract: We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here deﬁned by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efﬁcient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. 1</p><p>6 0.13588555 <a title="60-tfidf-6" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>7 0.11026749 <a title="60-tfidf-7" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>8 0.10994416 <a title="60-tfidf-8" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>9 0.096720472 <a title="60-tfidf-9" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>10 0.095816553 <a title="60-tfidf-10" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>11 0.084021039 <a title="60-tfidf-11" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>12 0.080884509 <a title="60-tfidf-12" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>13 0.064638264 <a title="60-tfidf-13" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>14 0.058983061 <a title="60-tfidf-14" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>15 0.056104489 <a title="60-tfidf-15" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>16 0.051813226 <a title="60-tfidf-16" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>17 0.047932703 <a title="60-tfidf-17" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>18 0.046781272 <a title="60-tfidf-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.046770643 <a title="60-tfidf-19" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>20 0.045124773 <a title="60-tfidf-20" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, 0.027), (2, 0.018), (3, -0.008), (4, -0.368), (5, 0.165), (6, 0.228), (7, 0.044), (8, -0.094), (9, 0.024), (10, 0.199), (11, 0.074), (12, 0.009), (13, 0.104), (14, -0.083), (15, 0.039), (16, -0.02), (17, 0.028), (18, -0.054), (19, 0.008), (20, 0.096), (21, -0.104), (22, -0.06), (23, 0.072), (24, -0.058), (25, -0.046), (26, 0.029), (27, -0.002), (28, 0.01), (29, -0.039), (30, 0.003), (31, -0.06), (32, 0.028), (33, -0.016), (34, -0.009), (35, 0.074), (36, -0.047), (37, 0.046), (38, 0.081), (39, -0.047), (40, 0.043), (41, -0.108), (42, 0.025), (43, 0.043), (44, -0.021), (45, -0.077), (46, 0.03), (47, -0.014), (48, -0.08), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96427739 <a title="60-lsi-1" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>2 0.91082132 <a title="60-lsi-2" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>3 0.79358554 <a title="60-lsi-3" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>4 0.62453938 <a title="60-lsi-4" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>Author: Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David B. Dunson</p><p>Abstract: We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here deﬁned by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efﬁcient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. 1</p><p>5 0.5533582 <a title="60-lsi-5" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, Ryota Tomioka</p><p>Abstract: Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only ﬁnd a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.</p><p>6 0.51010603 <a title="60-lsi-6" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>7 0.50821739 <a title="60-lsi-7" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>8 0.43458444 <a title="60-lsi-8" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>9 0.41669428 <a title="60-lsi-9" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>10 0.3365553 <a title="60-lsi-10" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>11 0.33466801 <a title="60-lsi-11" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>12 0.3312259 <a title="60-lsi-12" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>13 0.32674146 <a title="60-lsi-13" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>14 0.29238793 <a title="60-lsi-14" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>15 0.28166369 <a title="60-lsi-15" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>16 0.28025591 <a title="60-lsi-16" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>17 0.24946284 <a title="60-lsi-17" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>18 0.24452642 <a title="60-lsi-18" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>19 0.23921248 <a title="60-lsi-19" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>20 0.23661093 <a title="60-lsi-20" href="./nips-2010-Multitask_Learning_without_Label_Correspondences.html">177 nips-2010-Multitask Learning without Label Correspondences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.028), (27, 0.625), (30, 0.045), (35, 0.012), (45, 0.109), (50, 0.032), (60, 0.016), (77, 0.023), (90, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94992554 <a title="60-lda-1" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>2 0.92392212 <a title="60-lda-2" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>same-paper 3 0.90589088 <a title="60-lda-3" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>4 0.87078178 <a title="60-lda-4" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>5 0.83707821 <a title="60-lda-5" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>6 0.81355339 <a title="60-lda-6" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>7 0.73728734 <a title="60-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.69904715 <a title="60-lda-8" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>9 0.67978942 <a title="60-lda-9" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>10 0.65993083 <a title="60-lda-10" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>11 0.6351527 <a title="60-lda-11" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>12 0.63514906 <a title="60-lda-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.60915405 <a title="60-lda-13" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>14 0.60077798 <a title="60-lda-14" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>15 0.59172028 <a title="60-lda-15" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>16 0.58452696 <a title="60-lda-16" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>17 0.58067632 <a title="60-lda-17" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>18 0.57821327 <a title="60-lda-18" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>19 0.57637662 <a title="60-lda-19" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>20 0.56288803 <a title="60-lda-20" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
