<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>217 nips-2010-Probabilistic Multi-Task Feature Selection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-217" href="#">nips2010-217</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>217 nips-2010-Probabilistic Multi-Task Feature Selection</h1>
<br/><p>Source: <a title="nips-2010-217-pdf" href="http://papers.nips.cc/paper/4150-probabilistic-multi-task-feature-selection.pdf">pdf</a></p><p>Author: Yu Zhang, Dit-Yan Yeung, Qian Xu</p><p>Abstract: Recently, some variants of the đ?&lsquo;&trade;1 norm, particularly matrix norms such as the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms by considering a family of đ?&lsquo;&trade;1,đ?&lsquo;ž norms for 1 < đ?&lsquo;ž â&permil;¤ â&circ;ž and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the đ?&lsquo;&trade;1,đ?&lsquo;ž norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including đ?&lsquo;ž, automatically. Experiments have been conducted on two cancer classiďŹ cation applications using microarray gene expression data. 1</p><p>Reference: <a title="nips-2010-217-reference" href="../nips2010_reference/nips-2010-Probabilistic_Multi-Task_Feature_Selection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 &lsquo;&trade;1,â&circ;&#x17E; norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. [sent-7, score-0.428]
</p><p>2 &lsquo;&#x17E; â&permil;¤ â&circ;&#x17E; and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. [sent-13, score-0.402]
</p><p>3 Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the đ? [sent-14, score-0.861]
</p><p>4 Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. [sent-17, score-0.392]
</p><p>5 Experiments have been conducted on two cancer classiďŹ cation applications using microarray gene expression data. [sent-21, score-0.603]
</p><p>6 &lsquo;&trade;1 regularization is its ability to enforce sparsity in the solutions. [sent-25, score-0.273]
</p><p>7 &lsquo;&trade;1,â&circ;&#x17E; norms, were proposed to enforce sparsity via joint regularization [24, 17, 28, 1, 2, 15, 20, 16, 18]. [sent-29, score-0.311]
</p><p>8 Regularizers based on these two matrix norms encourage row sparsity, i. [sent-35, score-0.315]
</p><p>9 , they encourage entire rows of the matrix to have zero elements. [sent-37, score-0.143]
</p><p>10 Moreover, these norms have also been used for enforcing group sparsity among features in conventional classiďŹ cation and regression problems, e. [sent-38, score-0.453]
</p><p>11 Recently, they have been widely used in multi-task learning, compressed sensing and other related areas. [sent-41, score-0.177]
</p><p>12 In this paper, we study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection [17, 15]. [sent-43, score-0.548]
</p><p>13 &lsquo;&#x17E; â&permil;¤ â&circ;&#x17E; to ensure that all norms in this family are convex, making it easier to solve the optimization problem formulated based on it. [sent-51, score-0.274]
</p><p>14 &lsquo;&#x17E; norm, we formulate the general multi-task feature selection problem and give it a probabilistic interpretation. [sent-57, score-0.416]
</p><p>15 It is noted that the automatic relevance determination (ARD) prior [9, 3, 26] comes as a special case under this interpretation. [sent-58, score-0.195]
</p><p>16 Based on this probabilistic interpretation, we develop a probabilistic formulation using a noninformative prior called the Jeffreys prior [10]. [sent-59, score-0.568]
</p><p>17 Moreover, an underlying assumption of existing multi-task feature selection methods is that all tasks are similar to each other and they share the same features. [sent-62, score-0.482]
</p><p>18 This assumption may not be correct in practice because there may exist outlier tasks 1  or tasks with negative correlation. [sent-63, score-0.441]
</p><p>19 As another contribution of this paper, we propose to use a matrix variate generalized normal prior [13] for the model parameters to learn the relationships between tasks. [sent-64, score-0.828]
</p><p>20 The task relationships learned here can be seen as an extension of the task covariance used in [4, 32, 31]. [sent-65, score-0.235]
</p><p>21 Experiments will be reported on two cancer classiďŹ cation applications using microarray gene expression data. [sent-66, score-0.603]
</p><p>22 , document classiďŹ cation, the feature dimensionality is usually very high and it has been found that linear methods usually perform better. [sent-115, score-0.121]
</p><p>23 The objective functions of most existing multi-task feature selection methods [24, 17, 28, 1, 2, 15, 20, 16, 18] can be expressed in the following form: đ? [sent-116, score-0.267]
</p><p>24 &lsquo;&hellip;(â&lsaquo;&hellip;) is the regularization function that enforces feature sparsity under the multi-task setting, and đ? [sent-144, score-0.293]
</p><p>25 Multi-task feature selection seeks to minimize the objective function above to obtain the optimal parameters {wđ? [sent-146, score-0.267]
</p><p>26 Two regularization functions are widely used in existing multi-task feature selection methods. [sent-151, score-0.327]
</p><p>27 &lsquo;&#x17E; norm of W to deďŹ ne a more general regularization function: đ? [sent-172, score-0.163]
</p><p>28 &lsquo;&#x17E; = 1, each element of W is independent of each other and so the regularization function cannot enforce feature sparsity. [sent-184, score-0.345]
</p><p>29 &lsquo;&#x17E; â&permil;¤ â&circ;&#x17E; can enforce feature sparsity between different tasks, different values of đ? [sent-188, score-0.334]
</p><p>30 &lsquo;&#x17E; approaches 1, the cost grows almost linearly with the number of tasks that use a feature, and when đ? [sent-191, score-0.182]
</p><p>31 In the following, we ďŹ rst give a probabilistic interpretation for multi-task feature selection methods. [sent-195, score-0.499]
</p><p>32 Based on this probabilistic interpretation, we then develop a probabilistic model which, among other things, can solve the model selection problem automatically by estimating đ? [sent-196, score-0.479]
</p><p>33 3  Probabilistic Interpretation  In this section, we will show that existing multi-task feature selection methods are related to the maximum a posteriori (MAP) solution of a probabilistic model. [sent-198, score-0.416]
</p><p>34 This probabilistic interpretation sets the stage for introducing our probabilistic model in the next section. [sent-199, score-0.381]
</p><p>35 We ďŹ rst introduce the generalized normal distribution [11] which is useful for the model to be introduced. [sent-200, score-0.362]
</p><p>36 &lsquo;§ is a univariate generalized normal random variable iff its probability density function (p. [sent-202, score-0.506]
</p><p>37 &lsquo;§ is a univariate generalized normal random variable, we write đ? [sent-217, score-0.445]
</p><p>38 The (ordinary) normal distribution can be viewed as a special case of the generalized normal distribution when đ? [sent-224, score-0.619]
</p><p>39 &lsquo;&#x17E; approaches +â&circ;&#x17E;, the generalized normal distribution approaches the uniform distribution in the range [đ? [sent-228, score-0.362]
</p><p>40 The generalized normal distribution has proven useful in Bayesian analysis and robustness studies. [sent-233, score-0.362]
</p><p>41 &lsquo;&Yuml; Ă&mdash; 1 multivariate generalized normal random variable z = (đ? [sent-235, score-0.431]
</p><p>42 &lsquo;&Yuml; Ă&mdash; 1 multivariate generalized normal random variable, we write z â&circ;ź â&bdquo;łđ? [sent-248, score-0.401]
</p><p>43 &lsquo;&#x17E;  With these deďŹ nitions, we now begin to present our probabilistic interpretation for multi-task feature selection by proposing a probabilistic model. [sent-270, score-0.648]
</p><p>44 For notational simplicity, we assume that all tasks perform regression. [sent-271, score-0.182]
</p><p>45 Extension to include classiďŹ cation tasks will go through similar derivation. [sent-272, score-0.22]
</p><p>46 For a regression problem, we use the normal distribution to deďŹ ne the likelihood for xđ? [sent-273, score-0.257]
</p><p>47 &lsquo; 2 ) denotes the (univariate) normal distribution with mean đ? [sent-290, score-0.283]
</p><p>48 We impose the generalized normal prior on each element of W: đ? [sent-293, score-0.513]
</p><p>49 &lsquo;&#x17E; = 2, this becomes the ARD prior [9, 3, 26] commonly used in Bayesian methods for enforcing sparsity. [sent-321, score-0.154]
</p><p>50 From this view, the generalized normal prior can be viewed as a generalization of the ARD prior. [sent-322, score-0.481]
</p><p>51 3  So the solutions of multi-task feature selection methods can be viewed as the solution of the relaxed optimization problem above. [sent-421, score-0.298]
</p><p>52 4  A Probabilistic Framework for Multi-Task Feature Selection  In the probabilistic interpretation above, we use a type II method to estimate {đ? [sent-496, score-0.232]
</p><p>53 &lsquo;&ndash; } in the generalized normal prior which can be viewed as a generalization of the ARD prior. [sent-498, score-0.481]
</p><p>54 In the following, we will present our probabilistic framework for multi-task feature selection by imposing priors on the hyperparameters. [sent-503, score-0.416]
</p><p>55 &lsquo;&ndash; is also deďŹ ned based on the normal distribution: đ? [sent-506, score-0.226]
</p><p>56 &lsquo;&ndash; for different tasks to make our model more ďŹ&sbquo;exible. [sent-523, score-0.182]
</p><p>57 &lsquo;&ndash; as a random variable with the noninformative Jeffreys prior: â&circ;&scaron; đ? [sent-535, score-0.124]
</p><p>58 Extension to Deal with Outlier Tasks and Tasks with Negative Correlation  An underlying assumption of multi-task feature selection using the đ? [sent-965, score-0.267]
</p><p>59 &lsquo;&#x17E; norm is that all tasks are similar to each other and they share the same features. [sent-967, score-0.318]
</p><p>60 This assumption may not be correct in practice because there may exist outlier tasks (i. [sent-968, score-0.259]
</p><p>61 , tasks that are not related to all other tasks) or tasks with negative correlation (i. [sent-970, score-0.364]
</p><p>62 , tasks that are negatively correlated with some other tasks). [sent-972, score-0.182]
</p><p>63 In this section, we will discuss how to extend our probabilistic model to deal with these tasks. [sent-973, score-0.149]
</p><p>64 We ďŹ rst introduce the matrix variate generalized normal distribution [13] which is a generalization of the generalized normal distribution to random matrices. [sent-974, score-0.999]
</p><p>65 &lsquo;Ą is a matrix variate generalized normal random variable iff its p. [sent-978, score-0.698]
</p><p>66 &lsquo;&#x17E;) for a matrix variate generalized normal random variable Z. [sent-1032, score-0.667]
</p><p>67 &lsquo;&#x17E; = 2, the matrix variate generalized normal distribution becomes the (ordinary) matrix variate normal distribution [12] with row covariance matrix ÎŁÎŁđ? [sent-1034, score-1.244]
</p><p>68 From this view, ÎŁ is used to model the relationships between the rows of Z and ÎŠ is to model the relationships between the columns. [sent-1037, score-0.246]
</p><p>69 &lsquo;&#x17E;),  where 0 denotes a zero vector or matrix of proper size, Iđ? [sent-1051, score-0.127]
</p><p>70 The likelihood is still based on the normal distribution. [sent-1069, score-0.226]
</p><p>71 Since in practice the relationships between tasks are not known in advance, we also need to estimate ÎŠ from data. [sent-1070, score-0.285]
</p><p>72 For a ďŹ xed ÎŠ, the problem with respect to W is a convex problem and we use conjugate gradient to solve it with the following subgradient đ? [sent-1298, score-0.142]
</p><p>73 6  5  Related Work  Some probabilistic multi-task feature selection methods have been proposed before [28, 2]. [sent-1379, score-0.416]
</p><p>74 [30] proposed a latent variable model for multi-task learning by using the Laplace prior to enforce sparsity. [sent-1384, score-0.219]
</p><p>75 ż1,1 norm in our framework which, as discussed above, cannot enforce group sparsity among different features over all tasks. [sent-1387, score-0.346]
</p><p>76 6  Experiments  In this section, we study our methods empirically on two cancer classiďŹ cation applications using microarray gene expression data. [sent-1388, score-0.603]
</p><p>77 We compare our methods with three related methods: multi-task feature learning (MTFL) [1]1 , multi-task feature selection using đ? [sent-1389, score-0.388]
</p><p>78 &lsquo;&trade;1,2 regularization [16]2 , and multitask feature selection using đ? [sent-1390, score-0.327]
</p><p>79 1  Breast Cancer ClassiďŹ cation  We ďŹ rst conduct empirical study on a breast cancer classiďŹ cation application. [sent-1393, score-0.527]
</p><p>80 This application consists of three learning tasks with data collected under different platforms [21]. [sent-1394, score-0.268]
</p><p>81 The dataset for the ďŹ rst task, collected at the Koo Foundation Sun Yat-Sen Cancer Centre in Taipei, contains 89 samples with 8948 genes per sample. [sent-1395, score-0.123]
</p><p>82 The dataset for the third task, obtained using 22K Agilent oligonucleotide arrays, contains 114 samples with 12065 genes per sample. [sent-1398, score-0.117]
</p><p>83 Even though these three datasets were collected under different platforms, they share 6092 common genes which are used in our experiments. [sent-1399, score-0.156]
</p><p>84 This veriďŹ es the usefulness of exploiting the relationships between tasks in multi-task feature selection. [sent-1409, score-0.406]
</p><p>85 Table 1: Comparison of different methods on the breast cancer classiďŹ cation application in terms of classiďŹ cation error rate (in meanÂąstd-dev). [sent-1418, score-0.527]
</p><p>86 0263  Prostate Cancer ClassiďŹ cation  We next study a prostate cancer classiďŹ cation application consisting of two tasks. [sent-1451, score-0.576]
</p><p>87 The RMA preprocessing method was used to produce gene expression values from these images. [sent-1453, score-0.165]
</p><p>88 edu/â&circ;źaquattoni/ 2  7  hand, the Welsh dataset [25] for the second task is already in the form of gene expression values. [sent-1464, score-0.231]
</p><p>89 Table 2: Comparison of different methods on the prostate cancer classiďŹ cation application in terms of classiďŹ cation error rate (in meanÂąstd-dev). [sent-1474, score-0.576]
</p><p>90 0059  Concluding Remarks  In this paper, we have proposed a probabilistic framework for general multi-task feature selection using the đ? [sent-1496, score-0.416]
</p><p>91 Besides considering the case in which all tasks are similar, we have also considered the more general and challenging case in which there also exist outlier tasks or tasks with negative correlation. [sent-1502, score-0.623]
</p><p>92 Compressed sensing aims at recovering the sparse signal w from a measurement vector b = Aw for a given matrix A. [sent-1503, score-0.284]
</p><p>93 Compressed sensing can be extended to the multiple measurement vector (MMV) model in which the signals are represented as a set of jointly sparse vectors sharing a common set of nonzero elements [7, 6, 23]. [sent-1504, score-0.18]
</p><p>94 SpeciďŹ cally, joint compressed sensing considers the reconstruction of the signal represented by a matrix W, which is given by a dictionary (or measurement matrix) A and multiple measurement vector B such that B = AW. [sent-1505, score-0.441]
</p><p>95 Similar to multi-task feature selection, we can use â&circ;ĽWâ&circ;Ľ1,đ? [sent-1506, score-0.121]
</p><p>96 So we can use the probabilistic model presented in Section 4 to develop a probabilistic model for joint compressed sensing. [sent-1512, score-0.429]
</p><p>97 Joint covariate selection and joint subspace selection for multiple classiďŹ cation problems. [sent-1644, score-0.368]
</p><p>98 Analysis of gene expression identiďŹ es candidate markers and pharmacological targets in prostate cancer. [sent-1735, score-0.322]
</p><p>99 Model selection and estimation in regression with grouped variables. [sent-1760, score-0.177]
</p><p>100 A convex formulation for learning task relationships in multi-task learning. [sent-1772, score-0.199]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ln', 0.35), ('cancer', 0.343), ('normal', 0.226), ('ard', 0.216), ('variate', 0.205), ('th', 0.185), ('tasks', 0.182), ('norms', 0.176), ('prostate', 0.157), ('probabilistic', 0.149), ('selection', 0.146), ('generalized', 0.136), ('jeffreys', 0.126), ('det', 0.123), ('feature', 0.121), ('sparsity', 0.112), ('breast', 0.108), ('mtfl', 0.107), ('gene', 0.105), ('relationships', 0.103), ('norm', 0.103), ('enforce', 0.101), ('noninformative', 0.094), ('compressed', 0.093), ('prior', 0.088), ('genes', 0.086), ('sensing', 0.084), ('interpretation', 0.083), ('univariate', 0.083), ('outlier', 0.077), ('classi', 0.071), ('matrix', 0.07), ('enforcing', 0.066), ('task', 0.066), ('element', 0.063), ('mmv', 0.063), ('welsh', 0.063), ('em', 0.063), ('measurement', 0.061), ('regularization', 0.06), ('expression', 0.06), ('xiong', 0.058), ('wipf', 0.058), ('microarray', 0.057), ('denotes', 0.057), ('sign', 0.056), ('unify', 0.051), ('platforms', 0.049), ('reweighted', 0.045), ('aw', 0.044), ('hong', 0.043), ('mm', 0.043), ('relevance', 0.042), ('standardized', 0.041), ('diag', 0.041), ('conjugate', 0.041), ('zhang', 0.041), ('devise', 0.04), ('rows', 0.04), ('multivariate', 0.039), ('rao', 0.039), ('cation', 0.038), ('joint', 0.038), ('collected', 0.037), ('gupta', 0.037), ('gradient', 0.036), ('row', 0.036), ('sparse', 0.035), ('surrogate', 0.035), ('laplace', 0.035), ('liu', 0.035), ('solve', 0.035), ('chapman', 0.034), ('signal', 0.034), ('singh', 0.034), ('ordinary', 0.034), ('determination', 0.033), ('encourage', 0.033), ('share', 0.033), ('family', 0.032), ('automatic', 0.032), ('iff', 0.031), ('besides', 0.031), ('taipei', 0.031), ('qian', 0.031), ('febbo', 0.031), ('ladd', 0.031), ('sapinoso', 0.031), ('richie', 0.031), ('jackson', 0.031), ('laser', 0.031), ('oligonucleotide', 0.031), ('renshaw', 0.031), ('formulated', 0.031), ('viewed', 0.031), ('regression', 0.031), ('variable', 0.03), ('convex', 0.03), ('column', 0.03), ('group', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="217-tfidf-1" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>Author: Yu Zhang, Dit-Yan Yeung, Qian Xu</p><p>Abstract: Recently, some variants of the đ?&lsquo;&trade;1 norm, particularly matrix norms such as the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms by considering a family of đ?&lsquo;&trade;1,đ?&lsquo;ž norms for 1 < đ?&lsquo;ž â&permil;¤ â&circ;ž and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the đ?&lsquo;&trade;1,đ?&lsquo;ž norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including đ?&lsquo;ž, automatically. Experiments have been conducted on two cancer classiďŹ cation applications using microarray gene expression data. 1</p><p>2 0.24572971 <a title="217-tfidf-2" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><p>3 0.16409305 <a title="217-tfidf-3" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>Author: Yi Zhang, Jeff G. Schneider</p><p>Abstract: In this paper, we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overﬁtting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via ℓ1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple ﬁelds and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and ﬂexible way to model various different structures of multiple tasks.</p><p>4 0.15281466 <a title="217-tfidf-4" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>5 0.10699146 <a title="217-tfidf-5" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>6 0.10125393 <a title="217-tfidf-6" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>7 0.094783343 <a title="217-tfidf-7" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>8 0.090912342 <a title="217-tfidf-8" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>9 0.085056119 <a title="217-tfidf-9" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>10 0.083788581 <a title="217-tfidf-10" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>11 0.081768513 <a title="217-tfidf-11" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>12 0.080456495 <a title="217-tfidf-12" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>13 0.079262294 <a title="217-tfidf-13" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>14 0.078961581 <a title="217-tfidf-14" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>15 0.077456653 <a title="217-tfidf-15" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>16 0.077054769 <a title="217-tfidf-16" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>17 0.076697223 <a title="217-tfidf-17" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>18 0.073773548 <a title="217-tfidf-18" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>19 0.073704235 <a title="217-tfidf-19" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>20 0.073295131 <a title="217-tfidf-20" href="./nips-2010-Non-Stochastic_Bandit_Slate_Problems.html">183 nips-2010-Non-Stochastic Bandit Slate Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, 0.052), (2, 0.078), (3, 0.034), (4, 0.037), (5, -0.1), (6, -0.02), (7, 0.075), (8, -0.158), (9, -0.043), (10, -0.009), (11, 0.13), (12, 0.075), (13, 0.045), (14, 0.041), (15, 0.008), (16, -0.059), (17, 0.095), (18, 0.1), (19, 0.035), (20, -0.132), (21, -0.062), (22, -0.024), (23, 0.091), (24, 0.006), (25, 0.024), (26, 0.038), (27, -0.001), (28, 0.103), (29, 0.171), (30, -0.051), (31, -0.062), (32, -0.069), (33, -0.147), (34, 0.13), (35, -0.014), (36, -0.027), (37, 0.003), (38, -0.002), (39, 0.094), (40, -0.06), (41, 0.009), (42, 0.092), (43, -0.158), (44, -0.102), (45, -0.075), (46, -0.06), (47, -0.099), (48, -0.006), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95975703 <a title="217-lsi-1" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>Author: Yu Zhang, Dit-Yan Yeung, Qian Xu</p><p>Abstract: Recently, some variants of the đ?&lsquo;&trade;1 norm, particularly matrix norms such as the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms by considering a family of đ?&lsquo;&trade;1,đ?&lsquo;ž norms for 1 < đ?&lsquo;ž â&permil;¤ â&circ;ž and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the đ?&lsquo;&trade;1,đ?&lsquo;ž norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including đ?&lsquo;ž, automatically. Experiments have been conducted on two cancer classiďŹ cation applications using microarray gene expression data. 1</p><p>2 0.83724642 <a title="217-lsi-2" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><p>3 0.71184033 <a title="217-lsi-3" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>Author: Yi Zhang, Jeff G. Schneider</p><p>Abstract: In this paper, we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overﬁtting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via ℓ1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple ﬁelds and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and ﬂexible way to model various different structures of multiple tasks.</p><p>4 0.57371736 <a title="217-lsi-4" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>5 0.53498673 <a title="217-lsi-5" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>Author: Ji Liu, Peter Wonka, Jieping Ye</p><p>Abstract: We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X ∈ Rn×m (m n) and a noisy observation vector y ∈ Rn satisfying y = Xβ ∗ + where is the noise vector following a Gaussian distribution N (0, σ 2 I), how to recover the signal (or parameter vector) β ∗ when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively reﬁnes the target signal β ∗ . We show that if X obeys a certain condition, then with a large probability the difference between the solution ˆ β estimated by the proposed method and the true solution β ∗ measured in terms of the lp norm (p ≥ 1) is bounded as ˆ β − β∗ p ≤ C(s − N )1/p log m + ∆ σ, where C is a constant, s is the number of nonzero entries in β ∗ , ∆ is independent of m and is much smaller than the ﬁrst term, and N is the number of entries of √ β ∗ larger than a certain value in the order of O(σ log m). The proposed method improves the estimation bound of the standard Dantzig selector approximately √ √ from Cs1/p log mσ to C(s − N )1/p log mσ where the value N depends on the number of large entries in β ∗ . When N = s, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. 1</p><p>6 0.52571112 <a title="217-lsi-6" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>7 0.50945175 <a title="217-lsi-7" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>8 0.5045467 <a title="217-lsi-8" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>9 0.49177909 <a title="217-lsi-9" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>10 0.49138853 <a title="217-lsi-10" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>11 0.47882175 <a title="217-lsi-11" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>12 0.4695456 <a title="217-lsi-12" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>13 0.46913937 <a title="217-lsi-13" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>14 0.46807718 <a title="217-lsi-14" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>15 0.46620402 <a title="217-lsi-15" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>16 0.43817413 <a title="217-lsi-16" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>17 0.43689591 <a title="217-lsi-17" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>18 0.43616071 <a title="217-lsi-18" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>19 0.42350733 <a title="217-lsi-19" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>20 0.41221452 <a title="217-lsi-20" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.091), (17, 0.014), (27, 0.07), (30, 0.062), (35, 0.083), (45, 0.204), (50, 0.114), (52, 0.043), (60, 0.014), (71, 0.153), (77, 0.042), (90, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92411906 <a title="217-lda-1" href="./nips-2010-Sparse_Instrumental_Variables_%28SPIV%29_for_Genome-Wide_Studies.html">247 nips-2010-Sparse Instrumental Variables (SPIV) for Genome-Wide Studies</a></p>
<p>Author: Paul Mckeigue, Jon Krohn, Amos J. Storkey, Felix V. Agakov</p><p>Abstract: This paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies. The framework builds on sparse linear methods developed for regression and modiﬁed here for inferring causal structures of richer networks with latent variables. The method is motivated by the use of genotypes as “instruments” to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods. The method may be used for an effective screening of potentially interesting genotype-phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early-stage clinical trials. Where the biomarkers are gene transcripts, the method can be used for ﬁne mapping of quantitative trait loci (QTLs) detected in genetic linkage studies. The method is applied for examining effects of gene transcript levels in the liver on plasma HDL cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with ∼ 105 genetic instruments and ∼ 47 × 103 gene transcripts. 1</p><p>2 0.87040669 <a title="217-lda-2" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>Author: Sarah Filippi, Olivier Cappe, Aurélien Garivier, Csaba Szepesvári</p><p>Abstract: We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive ﬁnite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difﬁculty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to signiﬁcantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach. Keywords: multi-armed bandit, parametric bandits, generalized linear models, UCB, regret minimization. 1</p><p>same-paper 3 0.8575024 <a title="217-lda-3" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>Author: Yu Zhang, Dit-Yan Yeung, Qian Xu</p><p>Abstract: Recently, some variants of the đ?&lsquo;&trade;1 norm, particularly matrix norms such as the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the đ?&lsquo;&trade;1,2 and đ?&lsquo;&trade;1,â&circ;ž norms by considering a family of đ?&lsquo;&trade;1,đ?&lsquo;ž norms for 1 < đ?&lsquo;ž â&permil;¤ â&circ;ž and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the đ?&lsquo;&trade;1,đ?&lsquo;ž norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization (EM) algorithms to learn all model parameters, including đ?&lsquo;ž, automatically. Experiments have been conducted on two cancer classiďŹ cation applications using microarray gene expression data. 1</p><p>4 0.84002823 <a title="217-lda-4" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>5 0.8299619 <a title="217-lda-5" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>6 0.82529455 <a title="217-lda-6" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>7 0.81902111 <a title="217-lda-7" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>8 0.81823462 <a title="217-lda-8" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>9 0.81747371 <a title="217-lda-9" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>10 0.81694442 <a title="217-lda-10" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>11 0.81416196 <a title="217-lda-11" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>12 0.81360936 <a title="217-lda-12" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>13 0.81268793 <a title="217-lda-13" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>14 0.80935574 <a title="217-lda-14" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>15 0.80880213 <a title="217-lda-15" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>16 0.80729491 <a title="217-lda-16" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>17 0.80657488 <a title="217-lda-17" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>18 0.80640024 <a title="217-lda-18" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>19 0.80587143 <a title="217-lda-19" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>20 0.805206 <a title="217-lda-20" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
