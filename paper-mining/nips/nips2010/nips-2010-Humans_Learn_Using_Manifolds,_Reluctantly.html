<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 nips-2010-Humans Learn Using Manifolds, Reluctantly</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-114" href="#">nips2010-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 nips-2010-Humans Learn Using Manifolds, Reluctantly</h1>
<br/><p>Source: <a title="nips-2010-114-pdf" href="http://papers.nips.cc/paper/3905-humans-learn-using-manifolds-reluctantly.pdf">pdf</a></p><p>Author: Tim Rogers, Chuck Kalish, Joseph Harrison, Xiaojin Zhu, Bryan R. Gibson</p><p>Abstract: When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary. 1</p><p>Reference: <a title="nips-2010-114-reference" href="../nips2010_reference/nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. [sent-7, score-0.201]
</p><p>2 While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. [sent-8, score-0.388]
</p><p>3 We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. [sent-9, score-0.273]
</p><p>4 In addition, the learner is given some unlabeled items xl+1 , . [sent-20, score-0.287]
</p><p>5 The question is: given this knowledge of labeled and unlabeled data, how will the learner classify xl+1 , . [sent-28, score-0.295]
</p><p>6 Will the learner ignore the distribution information of the unlabeled data, and simply use the labeled data to form a decision boundary as in Figure 1(b)? [sent-32, score-0.334]
</p><p>7 Or will the learner propagate labels along the nonlinear manifolds as in Figure 1(c)? [sent-33, score-0.191]
</p><p>8 (a) the data  (b) supervised learning  (c) manifold learning  Figure 1: On a dataset with manifold structure, supervised learning and manifold learning make dramatically different predictions. [sent-34, score-0.736]
</p><p>9 The designer of the algorithm can choose to make the manifold assumption, also known as graph-based semi-supervised learning, which states that the labels vary slowly along the manifolds or the discrete graph formed by connecting nearby items. [sent-37, score-0.468]
</p><p>10 The mathematics of manifold learning is wellunderstood [1, 6, 9, 10]. [sent-39, score-0.236]
</p><p>11 Alternatively, the designer can choose to ignore the unlabeled data and perform supervised learning, which results in Figure 1(b). [sent-40, score-0.191]
</p><p>12 Consider that the human learner does not directly see how the items are distributed in the feature space (such as Figure 1(a)), but only a set of items (such as those in Figure 2(a)). [sent-42, score-0.315]
</p><p>13 The underlying manifold structure of the data may not be immediately obvious. [sent-43, score-0.236]
</p><p>14 Thus there are many possibilities for how the human learner will behave: 1) They may completely ignore the manifold structure and perform supervised learning; 2) They may discover the manifold under some learning conditions and not others; or 3) They may always learn using the manifold. [sent-44, score-0.662]
</p><p>15 For readers not familiar with manifold learning, the setting might seem artiﬁcial. [sent-45, score-0.236]
</p><p>16 However, if we continuously change the viewing angle, these 2D images will form a one-dimensional manifold in a very high dimensional image space. [sent-49, score-0.236]
</p><p>17 This example illustrates the importance of a manifold to facilitate learning: if we can form and maintain such a face manifold, then with a single label (e. [sent-50, score-0.284]
</p><p>18 For example, text documents in a corpus occupy a potentially nonlinear manifold in the otherwise very high dimensional space used to represent them, such as the “bag of words” representation. [sent-55, score-0.236]
</p><p>19 There exists little empirical evidence addressing the question of whether human beings can learn using manifolds when classifying objects, and the few studies we are aware of come to opposing conclusions. [sent-56, score-0.205]
</p><p>20 When participants were shown such sequences during training, their ability to match frontal and proﬁle faces during testing was impaired [8]. [sent-58, score-0.247]
</p><p>21 This might be evidence that people depend on manifold structure stemming from temporal and spatial proximity to perform face recognition. [sent-59, score-0.383]
</p><p>22 This study seeks to understand under what conditions, if any, people are capable of manifold learning in a semi-supervised setting. [sent-66, score-0.312]
</p><p>23 Second, if there are reliable methods for encouraging manifold learning in people, these methods can be employed to aid learning in other domains that are structured along manifolds. [sent-68, score-0.254]
</p><p>24 For machine learning, our study will help in the design of algorithms which can decide when to invoke the manifold learning assumption. [sent-69, score-0.236]
</p><p>25 2  Human Manifold Learning Experiments  We designed and conducted a set of experiments to study manifold learning in humans, with the following design considerations. [sent-70, score-0.236]
</p><p>26 First, the task was a “batch learning” paradigm in which participants viewed all labeled and unlabeled items at once (in contrast to “online” or sequential learning paradigm where items appear one at a time). [sent-71, score-0.65]
</p><p>27 Second, we avoided using faces or familiar 3D objects as stimuli, despite their natural manifold structures as discussed above, because we wished to avoid any bias resulting from strong prior real-world knowledge. [sent-73, score-0.236]
</p><p>28 Instead, we used unfamiliar stimuli, from which we could add or remove a manifold structure easily. [sent-74, score-0.236]
</p><p>29 Unlabeled cards were initially placed in a central white bin, with bins to 2  either side colored red and blue to indicate the two classes y ∈ {−1, 1}. [sent-79, score-0.301]
</p><p>30 Participants sorted cards by clicking and dragging with a mouse. [sent-81, score-0.305]
</p><p>31 When a card was clicked, other similar cards could be “highlighted” in gray (depending on condition). [sent-82, score-0.345]
</p><p>32 Labeled cards were pinned down in their respective red or blue bins and could not be moved, indicated by a “pin” in the corner of the card. [sent-83, score-0.301]
</p><p>33 The layout of the cards was such that all cards remained visible at all times. [sent-84, score-0.562]
</p><p>34 Unlabeled cards could be re-categorized at any time by dragging from any bin to any other bin. [sent-85, score-0.305]
</p><p>35 Upon sorting all cards, participants would click a button to indicating completion. [sent-86, score-0.266]
</p><p>36 The ﬁrst, used solely to acquaint the participants with the interface, consisted of a set of 20 cards with animal line drawings on a white background. [sent-88, score-0.505]
</p><p>37 59)  Figure 2: Experimental interface (with highlighting shown), and example crosshair stimuli. [sent-99, score-0.255]
</p><p>38 The participant was asked to sort the set of 20 animal cards into two categories, with the two ends of the continuum (a clown ﬁsh and a dachshund) labeled. [sent-103, score-0.484]
</p><p>39 Participants were told that when they clicked on a card, highlighting of similar cards might occur. [sent-104, score-0.548]
</p><p>40 In reality, highlighting was always shown for the two nearest-neighboring cards (on the deﬁned continuum) of a clicked card. [sent-105, score-0.525]
</p><p>41 Importantly, we designed the dataset so that, near the middle of the continuum, cards from opposite biological classes would be highlighted together. [sent-106, score-0.312]
</p><p>42 The intention was to indicate to the participant that highlighting is not always a clear give-away for class labels. [sent-108, score-0.342]
</p><p>43 Task 2 asked the participant to sort a set of 82 crosshair cards into two categories. [sent-112, score-0.482]
</p><p>44 The set of cards, the number of labeled cards, and the highlighting of cards depended on condition. [sent-113, score-0.565]
</p><p>45 The participant was again told that some cards might be highlighted, whether the condition actually provided for highlighting or not. [sent-114, score-0.665]
</p><p>46 The participant was also told that cards that shared highlighting may not all have the same classiﬁcation. [sent-115, score-0.646]
</p><p>47 Each of the 139 participants was randomly assigned to one of 6 conditions, shown in Figure 3, which varied according to three manipulations: The number of labeled items l can be 2 or 4 (2l vs. [sent-119, score-0.42]
</p><p>48 For conditions with two labeled items, the labeled items are always (x1 , y1 = −1), (x2 , y2 = 1); with four labeled items, they are always (x1 , y1 = −1), (x2 , y2 = 1), (x3 , y3 = 1), (x4 , y4 = −1). [sent-121, score-0.418]
</p><p>49 We chose these four labeled points by maximizing the prediction differences made by seven machine learning models, as discussed in the next section. [sent-126, score-0.216]
</p><p>50 3  Unlabeled items are distributed on a uniform grid or manifolds (gridU vs. [sent-127, score-0.225]
</p><p>51 For the moonsU conditions, the neighboring cards of any clicked card may be highlighted. [sent-136, score-0.412]
</p><p>52 8  1  4l moonsU h 23 participants  Figure 3: The six experimental conditions. [sent-190, score-0.224]
</p><p>53 3  Model Predictions  We hypothesize that human participants consider a set of models ranging from simple to sophisticated, and that they will perform model selection based on the training data given to them. [sent-193, score-0.304]
</p><p>54 In particular, we ﬁnd seven different kernels k to match GP classiﬁcation to each of the seven model predictions on all 6 conditions. [sent-213, score-0.255]
</p><p>55 That is, we extend a base RBF kernel k with a graph component: ˜ k(x, z) = k(x, z) − k⊤ (I + cLK)−1 cLkz (1) x where x, z are two arbitrary items (not necessarily on the graph), kx = (k(x, x1 ), . [sent-222, score-0.215]
</p><p>56 xl+u in the graph, K is the (l + u) × (l + u) Gram matrix with Kij = k(xi , xj ), L is the unnormalized graph Laplacian matrix derived from unweighted edges on the ǫNN graph deﬁned earlier for highlighting, and c is the parameter that we tune. [sent-228, score-0.17]
</p><p>57 In the end, our seven GPs were able to exactly match the predictions made by the seven models in Figure 4. [sent-232, score-0.25]
</p><p>58 We ﬁrst consider the aggregate behavior for all participants within each condition. [sent-288, score-0.24]
</p><p>59 One way to characterize this aggregate behavior is the “majority vote” of the participants on each item. [sent-289, score-0.24]
</p><p>60 That is, if more than half of the participants classiﬁed an item as y = 1, the majority vote classiﬁcation for that item is y = 1, and so on. [sent-290, score-0.315]
</p><p>61 We also compute how well the seven GPs predict human majority votes. [sent-293, score-0.194]
</p><p>62 (First row) the majority vote of participants within each condition. [sent-344, score-0.315]
</p><p>63 task varied widely, with some participant simply categorizing cards in the order they appeared on the screen, while others took a much longer, studied approach. [sent-400, score-0.431]
</p><p>64 Most interestingly, different participants seem to use different models, as the individual participant plots in the bottom three rows of Figure 5 suggest. [sent-401, score-0.374]
</p><p>65 In order to do this, we compute per participant accuracies of the seven models on that participant’s classiﬁcation. [sent-403, score-0.273]
</p><p>66 75, we declare that the participant is potentially using model M ; otherwise no model is deemed a good ﬁt and we say the participant is using some “other” model. [sent-406, score-0.3]
</p><p>67 We show the proportion of participants in each condition attributed to each of our seven models, plus “other”, in Table 2. [sent-407, score-0.365]
</p><p>68 67  Table 2: Percentage of participants potentially using each model Based on Figure 5, Table 1, and Table 2, we make some observations: 1. [sent-462, score-0.224]
</p><p>69 When there are only two labeled points, the unlabeled distribution does not encourage humans to perform manifold learning (comparing 2l gridU vs. [sent-463, score-0.53]
</p><p>70 With two labeled points, even if they are explicitly given the graph structure in the form of highlighting, participants still do not perform manifold learning (comparing 2l moonsU vs. [sent-468, score-0.645]
</p><p>71 When there are four labeled points but no highlighting, the distribution of unlabeled data still does not encourage people to perform manifold learning (comparing 4l gridU vs. [sent-472, score-0.546]
</p><p>72 This further suggests that people can not easily extract manifold structure from unlabeled data in order to learn, when there is no hint to do so. [sent-474, score-0.444]
</p><p>73 However, most participants have given up the simple single vertical or horizontal decision boundary, because it contradicts with the four labeled points. [sent-475, score-0.413]
</p><p>74 Finally, when we provide the graph structure, there is a marked switch to manifold learning (comparing 4l moonsU vs. [sent-477, score-0.314]
</p><p>75 This suggests that a combination of the elimination of preferred, simpler hypotheses, together with a stronger graph hint, ﬁnally gives the originally less preferred manifold learning model a chance of being used. [sent-479, score-0.314]
</p><p>76 It is under this condition that we observed human manifold learning behavior. [sent-480, score-0.305]
</p><p>77 Because our graph has disconnected components with consistently labeled seeds, this procedure will succeed. [sent-485, score-0.17]
</p><p>78 First, participants in 2l moonsU h did not learn the manifold while those in 4l moonsU h did, even though the two conditions have the same ǫNN highlighting. [sent-489, score-0.498]
</p><p>79 Second, a necessary condition for follow-the-highlighting is to always classify an unlabeled x′ according to a labeled highlighted neighbor x. [sent-490, score-0.288]
</p><p>80 Conversely, if a participant classiﬁes x′ as class y ′ , while all neighbors of x′ are either still unlabeled or have labels other than y ′ , she could not have been using follow-the-highlighting on x′ . [sent-491, score-0.307]
</p><p>81 The 4l moonsU h participants had an average of 17 leaps-of-faith among about 78 classiﬁcations3 , while strict follow-the-highlighting procedure would yield zero leaps-of-faith. [sent-493, score-0.224]
</p><p>82 Third, the basic challenge of follow-the-highlighting is that the underlying manifold structure of the stimuli may have been irrelevant. [sent-494, score-0.278]
</p><p>83 Would participants have shown the same behavior, following the highlighting, regardless of the actual stimuli? [sent-495, score-0.224]
</p><p>84 Take the 4l moonsU h graph which has 4 labeled nodes, 78 unlabeled nodes, and an adjacency matrix (i. [sent-497, score-0.296]
</p><p>85 This creates the random-looking graph in Figure 6(a) which we call 4l moonsU hR condition (the sufﬁx R stands for random), which is equivalent to the 4l moonsU h graph in structure. [sent-505, score-0.175]
</p><p>86 That is, having random highlighting is similar to having no highlighting in how it affects human categorization. [sent-514, score-0.434]
</p><p>87 6  Discussion  We have presented a set of experiments exploring human manifold learning behaviors. [sent-516, score-0.286]
</p><p>88 Our results suggest that people can perform manifold learning, but only when there is no alternative, simpler explanation of the data, and people need strong hints about the graph structure. [sent-517, score-0.472]
</p><p>89 4 In addition, if we create a GP from the Laplacian of the random highlighting graph, the GP accuracy in predicting 4l moonsU hR human majority vote is 0. [sent-523, score-0.333]
</p><p>90 46, and the percentage of participants in 4l moonsU hR who can be attributed to this model is 0. [sent-524, score-0.238]
</p><p>91 (b) The behavioral evaluation for 4l moonsU hR , where the x-axis is the shortest path length of an unlabeled point to a labeled point, and the y-axis is the fraction of participants who classiﬁed that unlabeled point consistent with the nearest labeled point. [sent-555, score-0.714]
</p><p>92 only when strong hints (highlighting) exists and agrees with the underlying unlabeled data manifold structure. [sent-558, score-0.383]
</p><p>93 Under this assumption, we can then explain the contrast between the lack of manifold learning in 2l moonsU h, and the presence of manifold learning in 4l moonsU h. [sent-559, score-0.472]
</p><p>94 On one hand, for the 2l moonsU h condition, the evidence for the seven GP models on the two labeled points are: (graph) 0. [sent-560, score-0.252]
</p><p>95 In any case, there is no reason to prefer the GP with a graph kernel, and we do not expect humans to learn on manifold in 2l moonsU h. [sent-569, score-0.413]
</p><p>96 On the other hand, for 4l moonsU h, the evidence for the seven GP models on those four labeled points are: (graph) 0. [sent-570, score-0.268]
</p><p>97 In particular, it is better than the evidence 1/16 for kernels that treat the four labeled points essentially independently. [sent-579, score-0.165]
</p><p>98 Thus, there is a reason to prefer the GP with a graph kernel, and we do expect humans to learn on manifold in 4l moonsU h. [sent-582, score-0.413]
</p><p>99 , it again suggests that people would perform manifold learning in 4l moonsU h. [sent-597, score-0.312]
</p><p>100 Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. [sent-607, score-0.218]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('moonsu', 0.745), ('cards', 0.281), ('manifold', 0.236), ('participants', 0.224), ('highlighting', 0.192), ('participant', 0.15), ('gridu', 0.134), ('unlabeled', 0.126), ('gp', 0.112), ('seven', 0.108), ('hr', 0.104), ('items', 0.104), ('manifolds', 0.102), ('labeled', 0.092), ('graph', 0.078), ('card', 0.064), ('gps', 0.064), ('humans', 0.061), ('people', 0.061), ('xl', 0.06), ('learner', 0.057), ('vote', 0.055), ('clicked', 0.052), ('human', 0.05), ('stimuli', 0.042), ('continuum', 0.039), ('evidence', 0.037), ('crosshair', 0.037), ('majority', 0.036), ('face', 0.034), ('vertical', 0.034), ('kernel', 0.033), ('blindly', 0.032), ('moons', 0.032), ('highlighted', 0.031), ('horizontal', 0.031), ('xiaojin', 0.028), ('shortest', 0.027), ('boundary', 0.027), ('psychology', 0.027), ('behavioral', 0.027), ('click', 0.026), ('interface', 0.026), ('dragging', 0.024), ('shark', 0.024), ('vandist', 0.024), ('wallis', 0.024), ('told', 0.023), ('frontal', 0.023), ('classi', 0.023), ('semisupervised', 0.022), ('prefer', 0.022), ('conditions', 0.022), ('dolphin', 0.021), ('undergraduates', 0.021), ('hint', 0.021), ('hints', 0.021), ('whale', 0.021), ('sh', 0.021), ('kernels', 0.02), ('classify', 0.02), ('designer', 0.02), ('participated', 0.02), ('bins', 0.02), ('rbf', 0.02), ('condition', 0.019), ('nn', 0.019), ('grid', 0.019), ('predictions', 0.019), ('mammal', 0.018), ('elongated', 0.018), ('along', 0.018), ('categorization', 0.017), ('neighbors', 0.017), ('vikas', 0.017), ('mikhail', 0.017), ('partha', 0.017), ('ignore', 0.016), ('learn', 0.016), ('four', 0.016), ('sorting', 0.016), ('behavior', 0.016), ('decision', 0.016), ('batch', 0.016), ('warping', 0.015), ('models', 0.015), ('perform', 0.015), ('neighboring', 0.015), ('capable', 0.015), ('niyogi', 0.015), ('comparing', 0.015), ('label', 0.014), ('attributed', 0.014), ('carl', 0.014), ('unweighted', 0.014), ('zhu', 0.014), ('pro', 0.014), ('supervised', 0.014), ('labels', 0.014), ('asked', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="114-tfidf-1" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>Author: Tim Rogers, Chuck Kalish, Joseph Harrison, Xiaojin Zhu, Bryan R. Gibson</p><p>Abstract: When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary. 1</p><p>2 0.13427962 <a title="114-tfidf-2" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>Author: Arvind Agarwal, Samuel Gerber, Hal Daume</p><p>Abstract: We present a novel method for multitask learning (MTL) based on manifold regularization: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common linear subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is ﬁxed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efﬁcient and easy to implement. We show the efﬁcacy of our method on several datasets. 1</p><p>3 0.10487509 <a title="114-tfidf-3" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>Author: Hariharan Narayanan, Sanjoy Mitter</p><p>Abstract: The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of ﬁtting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume log 1 and curvature is unavoidable. Whether the known lower bound of O( k + 2 δ ) 2 for the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of 2 log 1 log4 k log 1 O( k2 + 2 δ ) to O k min k, 2 + 2 δ . Based on these results, we 2 devise a simple algorithm for k−means and another that uses a family of convex programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional data, where the sample complexity is independent of the ambient dimension. 1</p><p>4 0.095149234 <a title="114-tfidf-4" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>5 0.07933972 <a title="114-tfidf-5" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>Author: Uri Shalit, Daphna Weinshall, Gal Chechik</p><p>Abstract: When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is hard to compute, and so is the projection operator that approximates it, we describe another second-order retraction that can be computed efﬁciently, with run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n, given rank-one gradients. We use this algorithm, LORETA, to learn a matrixform similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive- aggressive approach in a factorized model, and also improves over a full model trained over pre-selected features using the same memory requirements. LORETA also showed consistent improvement over standard methods in a large (1600 classes) multi-label image classiﬁcation task. 1</p><p>6 0.069722742 <a title="114-tfidf-6" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>7 0.058397904 <a title="114-tfidf-7" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>8 0.055729985 <a title="114-tfidf-8" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>9 0.054355241 <a title="114-tfidf-9" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>10 0.046524178 <a title="114-tfidf-10" href="./nips-2010-Gaussian_Process_Preference_Elicitation.html">100 nips-2010-Gaussian Process Preference Elicitation</a></p>
<p>11 0.045233257 <a title="114-tfidf-11" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>12 0.042433031 <a title="114-tfidf-12" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>13 0.041913744 <a title="114-tfidf-13" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>14 0.038694862 <a title="114-tfidf-14" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>15 0.038235955 <a title="114-tfidf-15" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>16 0.038003758 <a title="114-tfidf-16" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>17 0.037364747 <a title="114-tfidf-17" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>18 0.036691919 <a title="114-tfidf-18" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>19 0.036534227 <a title="114-tfidf-19" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>20 0.036516588 <a title="114-tfidf-20" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.039), (2, 0.014), (3, -0.019), (4, 0.014), (5, 0.017), (6, 0.021), (7, -0.051), (8, -0.028), (9, -0.033), (10, 0.034), (11, -0.032), (12, 0.002), (13, -0.055), (14, 0.054), (15, -0.002), (16, -0.018), (17, -0.104), (18, -0.031), (19, 0.065), (20, -0.114), (21, -0.026), (22, -0.021), (23, -0.023), (24, 0.081), (25, 0.002), (26, -0.165), (27, -0.052), (28, -0.059), (29, 0.079), (30, 0.08), (31, 0.003), (32, 0.107), (33, 0.015), (34, -0.054), (35, -0.133), (36, -0.036), (37, 0.08), (38, 0.022), (39, -0.005), (40, 0.124), (41, 0.04), (42, 0.028), (43, -0.038), (44, 0.041), (45, -0.001), (46, 0.086), (47, 0.036), (48, -0.014), (49, -0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91749811 <a title="114-lsi-1" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>Author: Tim Rogers, Chuck Kalish, Joseph Harrison, Xiaojin Zhu, Bryan R. Gibson</p><p>Abstract: When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary. 1</p><p>2 0.62196404 <a title="114-lsi-2" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>Author: Arvind Agarwal, Samuel Gerber, Hal Daume</p><p>Abstract: We present a novel method for multitask learning (MTL) based on manifold regularization: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common linear subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is ﬁxed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efﬁcient and easy to implement. We show the efﬁcacy of our method on several datasets. 1</p><p>3 0.46686554 <a title="114-lsi-3" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>Author: Hariharan Narayanan, Sanjoy Mitter</p><p>Abstract: The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of ﬁtting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume log 1 and curvature is unavoidable. Whether the known lower bound of O( k + 2 δ ) 2 for the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of 2 log 1 log4 k log 1 O( k2 + 2 δ ) to O k min k, 2 + 2 δ . Based on these results, we 2 devise a simple algorithm for k−means and another that uses a family of convex programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional data, where the sample complexity is independent of the ambient dimension. 1</p><p>4 0.46624207 <a title="114-lsi-4" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>Author: Aman Dhesi, Purushottam Kar</p><p>Abstract: The Random Projection Tree (RPT REE) structures proposed in [1] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data. We prove new results for both the RPT REE -M AX and the RPT REE -M EAN data structures. Our result for RPT REE -M AX gives a nearoptimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s ≥ 2. We also prove a packing lemma for this data structure. Our ﬁnal result shows that low-dimensional manifolds have bounded Local Covariance Dimension. As a consequence we show that RPT REE -M EAN adapts to manifold dimension as well.</p><p>5 0.41989148 <a title="114-lsi-5" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>6 0.41269735 <a title="114-lsi-6" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>7 0.35662085 <a title="114-lsi-7" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>8 0.35633132 <a title="114-lsi-8" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>9 0.35525617 <a title="114-lsi-9" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>10 0.32939103 <a title="114-lsi-10" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>11 0.3227559 <a title="114-lsi-11" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>12 0.31202507 <a title="114-lsi-12" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>13 0.30742565 <a title="114-lsi-13" href="./nips-2010-Gaussian_Process_Preference_Elicitation.html">100 nips-2010-Gaussian Process Preference Elicitation</a></p>
<p>14 0.29106194 <a title="114-lsi-14" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>15 0.29030821 <a title="114-lsi-15" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>16 0.28558877 <a title="114-lsi-16" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>17 0.27940583 <a title="114-lsi-17" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>18 0.27741992 <a title="114-lsi-18" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>19 0.27519983 <a title="114-lsi-19" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>20 0.27416378 <a title="114-lsi-20" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.188), (13, 0.052), (27, 0.066), (30, 0.054), (35, 0.03), (45, 0.197), (50, 0.042), (52, 0.03), (60, 0.044), (64, 0.011), (69, 0.026), (77, 0.045), (78, 0.017), (82, 0.018), (90, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84330118 <a title="114-lda-1" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>Author: Tim Rogers, Chuck Kalish, Joseph Harrison, Xiaojin Zhu, Bryan R. Gibson</p><p>Abstract: When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary. 1</p><p>2 0.83431357 <a title="114-lda-2" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>Author: Javad Azimi, Alan Fern, Xiaoli Z. Fern</p><p>Abstract: Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. In many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to batch Bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efﬁciently as possible. The key idea is to exploit the availability of high-quality and efﬁcient sequential policies, by using Monte-Carlo simulation to select input batches that closely match their expected behavior. Our experimental results on six benchmarks show that the proposed approach signiﬁcantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time. 1</p><p>3 0.76691395 <a title="114-lda-3" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>4 0.76683843 <a title="114-lda-4" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>5 0.76635283 <a title="114-lda-5" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>6 0.76470226 <a title="114-lda-6" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>7 0.76402247 <a title="114-lda-7" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>8 0.76270878 <a title="114-lda-8" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>9 0.7624374 <a title="114-lda-9" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>10 0.76219803 <a title="114-lda-10" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>11 0.76191866 <a title="114-lda-11" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>12 0.76112384 <a title="114-lda-12" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>13 0.76104361 <a title="114-lda-13" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>14 0.76054084 <a title="114-lda-14" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>15 0.76048595 <a title="114-lda-15" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>16 0.76000434 <a title="114-lda-16" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>17 0.75967991 <a title="114-lda-17" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>18 0.75951844 <a title="114-lda-18" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>19 0.75945008 <a title="114-lda-19" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>20 0.75929445 <a title="114-lda-20" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
