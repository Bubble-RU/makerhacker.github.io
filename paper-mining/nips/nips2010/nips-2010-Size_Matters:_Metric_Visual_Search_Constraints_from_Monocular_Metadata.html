<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-241" href="#">nips2010-241</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</h1>
<br/><p>Source: <a title="nips-2010-241-pdf" href="http://papers.nips.cc/paper/4104-size-matters-metric-visual-search-constraints-from-monocular-metadata.pdf">pdf</a></p><p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>Reference: <a title="nips-2010-241-reference" href="../nips2010_reference/nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. [sent-2, score-0.313]
</p><p>2 1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. [sent-5, score-0.58]
</p><p>3 We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. [sent-6, score-0.681]
</p><p>4 We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. [sent-7, score-0.556]
</p><p>5 Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. [sent-8, score-0.628]
</p><p>6 1  Introduction  Two themes dominate recent progress towards situated visual object recognition. [sent-9, score-0.455]
</p><p>7 Most signiﬁcantly, the availability of large scale image databases and machine learning methods has driven performance: accuracy on many category detection tasks is a function of the quantity and quality of the available training data. [sent-10, score-0.448]
</p><p>8 In this paper, we propose a method to bridge this gap and extract features from typical 2D data sources that can enhance recognition performance when 3D information is available at test time. [sent-20, score-0.314]
</p><p>9 1  Figure 1: Recovery of object size from known camera intrinsics The paradigm of recognition-by-local-features has been well established in the computer vision literature in recent years. [sent-21, score-0.654]
</p><p>10 Existing recognition schemes are designed generally to be invariant to scale and size. [sent-22, score-0.253]
</p><p>11 , 3-D shape context and SIFT [4, 3]), but we are somewhat skeptical of the ability of even the most recent 3-D sensor systems to extract the detailed local geometry required to reliably detect and describe local 3-D shapes on real world objects. [sent-27, score-0.305]
</p><p>12 1D” local feature model which augments a traditional 2D local feature (SIFT, GLOH, SURF, etc. [sent-29, score-0.336]
</p><p>13 ) with an estimate of the depth and 3-D size of an observed patch. [sent-30, score-0.359]
</p><p>14 on a full-size computer keyboard; while the keys might look locally similar, the absolute patch size would be highly distinctive. [sent-32, score-0.26]
</p><p>15 We focus on the recognition of realworld objects when additional sensors are available at test time, and show how 2. [sent-33, score-0.349]
</p><p>16 1D information can be extracted from monocular metadata already present in many online images. [sent-34, score-0.517]
</p><p>17 Our model includes both a representation of the absolute size of local features, and of the overall dimension of categories. [sent-35, score-0.229]
</p><p>18 We recover the depth and size of the local features, and thus of the bounding box of a detected object in 3-D. [sent-36, score-1.048]
</p><p>19 Efﬁcient search is an important goal, and we show a novel extension to multi-class branch-and-bound search using explicit metric 3-D constraints. [sent-37, score-0.407]
</p><p>20 1D” features  The crux of our method is the inference and exploitation of size information; we show that we can obtain such measurements from non-traditional sources that do not presume a 3-D scanner at training time, nor rely on multi-view reconstruction / structure-from-motion methods. [sent-39, score-0.287]
</p><p>21 We instead exploit cues that are readily available in many monocular camera images. [sent-40, score-0.486]
</p><p>22 1 We are not interested in reconstructing the object surface, and only estimate the absolute size of local patches, and the statistics of the bounding box of instances in the category; from these quantities we can infer the category size. [sent-41, score-0.972]
</p><p>23 We adopt a local-feature based recognition model and augment it with metric size information. [sent-42, score-0.356]
</p><p>24 While there are several possible local feature recognition schemes based on sets of such local features, we focus on the Naive Bayes nearest-neighbor model of [1] because of its simplicity and good empirical results. [sent-43, score-0.419]
</p><p>25 We assume one or more common local feature descriptors (and associated detectors or dense sampling grids): SIFT, SURF, GLOH, MSER. [sent-44, score-0.244]
</p><p>26 Our emphasis in this paper is on 1 There are a number of general paradigms by which estimates of object size can be extracted from a 2D image data source, e. [sent-45, score-0.411]
</p><p>27 , [8]), present as camera intrinsics stored as metadata in the JPEG EXIF ﬁle format. [sent-53, score-0.602]
</p><p>28 Images collected by many modern consumer-grade digital SLR cameras automatically store absolute distance-to-subject as metadata in the JPEG image. [sent-54, score-0.41]
</p><p>29 2  Figure 2: Illustration of metric object size derived from image metadata stored in EXIF ﬁelds on an image downloaded from Flickr. [sent-55, score-1.01]
</p><p>30 Absolute size is estimated by projecting bounding box of local features on object into 3-D using EXIF camera intrinsics stored in image ﬁle format. [sent-57, score-1.277]
</p><p>31 improving the accuracy of recognizing categories that are at least approximately well modeled with such-local feature schemes; size information alone cannot help recognize a category that does not repeatably and reliably produce such features. [sent-58, score-0.323]
</p><p>32 1  Metric object size from monocular metadata  Absolute pixel size can be infered using a planar object approximation and depth from focus cues. [sent-60, score-1.459]
</p><p>33 EXIF stores a wide range of intrinsic camera parameters, which often include the focus distance as an explicit parameter (in some cameras it is not provided directly, but can be estimated from other provided parameters). [sent-62, score-0.266]
</p><p>34 This gives us a workable approximation of the depth of the object, assuming it is in focus in the scene: with a pinhole camera model, we can derive the metric size of a pixel sd in the scene given these assumptions. [sent-63, score-0.869]
</p><p>35 Using simple trigonometry, the metric pixel size is ρ = f r , where s is the sensor width, d is the focus distance, f is the focal length, and r is the horizontal resolution of the sensor. [sent-64, score-0.376]
</p><p>36 As shown in Figure 2, this method provides a size estimate reference for the visual observation based on images commonly available on the internet, e. [sent-65, score-0.275]
</p><p>37 A bounding box can either be estimated from the feature locations, given an uncluttered background, or provided by manual labeling or by an object discovery technique which clusters local features to discover the segmentation of the training data. [sent-69, score-0.9]
</p><p>38 2  Naive Bayes estimation of discriminative feature weights  Our object model is based on a bag-of-words model where an object is encoded by a set of visual features xi ∈ X within the circumscribing bounding box. [sent-71, score-1.023]
</p><p>39 We denote object appearance with p(X|C); following [1], this density can be captured and modeled using Parzen window density estimates: 3  Figure 3: Metric object size for ten different categories derived from camera metadata. [sent-73, score-0.896]
</p><p>40 We compute the detection score for a given bounding box from the log-likelihood ratio computed based on the kernel density estimate from above. [sent-78, score-0.458]
</p><p>41 The core idea is to structure the search space using a search tree. [sent-84, score-0.228]
</p><p>42 Bounds can be easily computed for bag-of-words representations, which have been previously used in this context for object detection. [sent-89, score-0.237]
</p><p>43 Each feature has a learned weight wj , wherefore the score function f reads:  f (r) =  wj ,  (5)  j∈T (b)  where T (b) is the set of all features contained in the bounding box b. [sent-90, score-0.525]
</p><p>44 Our bounding box hypotheses b = (x1 , y1 , z1 , x2 , y2 , z2 ) are deﬁned explicitly in 3d and indicate the actual spatial relation of objects in the scene. [sent-93, score-0.43]
</p><p>45 We employ a constraint factor S(b) to the objective that indicates if a bounding box has a valid size given a particular class or not:  f (r) =  wj S(b)  (6)  j∈T (b)  S(b) = 1 is a basic rectangle function that takes the value 1 for valid bounding boxes and 0 otherwise. [sent-94, score-0.772]
</p><p>46 Most importantly, bounds over bounding box sets can still be efﬁciently computed. [sent-95, score-0.364]
</p><p>47 As long as the bounding box set at a given node in the search tree contains at least one bounding box of valid size, the score is unaffected. [sent-96, score-0.881]
</p><p>48 Given these measurements, we constrain the search to leverage the metric information acquired at training time. [sent-99, score-0.343]
</p><p>49 The depth for each feature in the image at test time allows us to infer their 3D location in the test scene. [sent-100, score-0.591]
</p><p>50 We can thus extend efﬁcient multi-class branchand-bound search to operate in metric 3D space under the constraints imposed by our knowledge of metric patch size and metric object size. [sent-101, score-1.075]
</p><p>51 We not only split bounding box sets along dimensions, but also split the set of object classes. [sent-103, score-0.601]
</p><p>52 proposed to up/downsample an image at multiple scales and identify the characterstic scale for each image patch [9]. [sent-108, score-0.383]
</p><p>53 A histogram of edge orientations is computed for each patch scaled to its characteristic scale in order to obtain a scale-invariant visual descriptor. [sent-109, score-0.298]
</p><p>54 With both methods, a feature in one image can be mapped to the same characteristic scale a feature in another image. [sent-112, score-0.318]
</p><p>55 Instead, it determines the metric size of any image patch and uses it to compare two features directly. [sent-115, score-0.553]
</p><p>56 There have been several works on estimating depth from single images. [sent-116, score-0.291]
</p><p>57 Some very early work estimated depth from the degree of the defocus of edges [8]. [sent-117, score-0.291]
</p><p>58 [6] describes a method to infer scene depth from structure baesd on global and local histograms of Gabor ﬁlter responses for indoor and outdoor scenes. [sent-118, score-0.589]
</p><p>59 [11] describes a supervised Markov Random Field method to predict the depth from local and global features for outdoor images. [sent-119, score-0.501]
</p><p>60 Hardware-based methods for obtaining 3D information from monocular images include modifying the structure of a conventional camera to enable it to capture 3D geometry. [sent-121, score-0.486]
</p><p>61 For example, [12] introduces the coded aperture technique by inserting a patterned occluder within the aperture of the camera lenses. [sent-122, score-0.391]
</p><p>62 Images captured by such a camera exhibit depth-dependent patterns from which a layered depth map can be extracted. [sent-123, score-0.536]
</p><p>63 Most methods based on visual feature quantization learn their codebooks using invariant features. [sent-124, score-0.245]
</p><p>64 However, the scale of each code word is lost after each image patch is normalized to its invariant region. [sent-125, score-0.35]
</p><p>65 For example, an eye of a dinosaur may be confused with an eye of a ﬁsh, because their size difference is lost once they are embedded into the visual code book. [sent-127, score-0.265]
</p><p>66 For example, [13] records the relative position of the object center in the codebook, and at test time each codebook word votes for the possible object center at multiple scales. [sent-129, score-0.612]
</p><p>67 Moreover, [14] explicitly put the orientation and scale of each feature in the codebook, so that object center location can be inferred directly. [sent-130, score-0.369]
</p><p>68 However, these works treat orientation and scale as independent of the feature descriptor and use them to post-verify whether a feature found to be consistent in terms of the appearance desciptor would also be consistent in terms of scale. [sent-131, score-0.252]
</p><p>69 A visual word would be matched only if its size is right. [sent-133, score-0.23]
</p><p>70 In other words, the visual apperance and the scale are matched simulaneously in our codebook. [sent-134, score-0.239]
</p><p>71 Depth information has been used to improve the performance of various image processing tasks, such as video retrieval, object instance detection, 3D scene recognition, and vehicle navigation. [sent-135, score-0.565]
</p><p>72 For example, [15] used depth feature for video retrieval, extracting depth from monocular video sequences by exploiting the motion parallax of the objects in the video. [sent-136, score-1.14]
</p><p>73 [16] developed an intergrated probablistic model for apperance and 3D geometry of object categories. [sent-137, score-0.297]
</p><p>74 However, their method does not expliclty assign physical size to each image patch and needs to provide scale-invariance by explictly calculating the perspective projection of objects in different 3D poposes. [sent-138, score-0.415]
</p><p>75 In contrast, our method can infer the real-world sizes of features and can establish feature correspondences at their true physical scale. [sent-139, score-0.253]
</p><p>76 [17] proposed a way to use depth estimation for real-time obstacle detection from a monocular video stream in a vehicle navigation scenario. [sent-140, score-0.824]
</p><p>77 Their method estimates scene depth from the scaling of supervised image regions and generates obstacle hypotheses from these depth estimates. [sent-141, score-0.841]
</p><p>78 5  Experiments  In the experiments we show how to improve performance of visual object classiﬁers by leveraging richer sensor modalities deployed at test time. [sent-142, score-0.546]
</p><p>79 We analyze how the different proposed means of putting visual recognition in metric context improves detection performance. [sent-143, score-0.509]
</p><p>80 1  Data  For training we explore the camera-based metadata scheme described above, where we derive the metric pixel size from EXIF data. [sent-145, score-0.653]
</p><p>81 We downloaded 38 images of 10 object categories taken with a consumer grade dSLR that stores relevant EXIF ﬁelds (e. [sent-146, score-0.346]
</p><p>82 For test data we have collected 34 scenes in our laboratory of varying complexity containing 120 object instances in ofﬁces and and a kitchen. [sent-149, score-0.326]
</p><p>83 Stereo depth observations using a calibrated camera rig are obtained with test imagery, providing an estimate of the 3-D depth of each feature point at test time. [sent-151, score-0.998]
</p><p>84 7  object bike helmet body wash juice kleenex mug pasta phone pringles toothpaste vitamins average  baseline 89. [sent-153, score-0.286]
</p><p>85 59  Table 1: Average precision for several categories for baseline 2-D branch and bound search and our 2. [sent-176, score-0.318]
</p><p>86 2  Evaluation  We start with a baseline, which uses the plain branch and bound detection scheme and 2D features. [sent-179, score-0.226]
</p><p>87 1D, adding 3D location to the interest points, as well as employing the metric size constraint. [sent-181, score-0.247]
</p><p>88 Table 1 shows the average precision for each category for baseline 2-D branch and bound search and our 2. [sent-182, score-0.355]
</p><p>89 Adding the metric object constraints (second column) improves the results signiﬁcantly. [sent-184, score-0.416]
</p><p>90 For the training data available for these categories the local evidence was apparently not strong enough to support this detection scheme, but with size constraints performance improved signiﬁcantly. [sent-188, score-0.409]
</p><p>91 6  Conclusion  Progress on large scale systems for visual categorization has been driven by the abundance of training data available from the web. [sent-189, score-0.269]
</p><p>92 3D measurements from stereo or lidar, typically found on contemporary robotic platforms, but there is rarely sufﬁcient training data to learn robust models using these sensors. [sent-192, score-0.219]
</p><p>93 In order to reconcile these two trends, we developed a method for appearance-based visual recognition in metric context, exploiting camera-based metadata to obtain size information regarding a category and local feature models that can be exploited using 3-D sensors at test time. [sent-193, score-1.205]
</p><p>94 We augmented local feature-based visual models with a “2. [sent-195, score-0.215]
</p><p>95 1D” object representation by introducing the notion of a metric patch size. [sent-196, score-0.535]
</p><p>96 We presented a fast, multi-class detection scheme based on a metric branch-and-bound formulation. [sent-198, score-0.319]
</p><p>97 While our method was demonstrated only on simple 2-D SURF features, we belive these methods will be applicable as well to multikernel schemes with additional feature modalities, as well as object level desriptors (e. [sent-199, score-0.371]
</p><p>98 Wohn, Pyramid based depth from focus, In Proceedings of Computer Vision and Pattern Recognition, 1988. [sent-241, score-0.291]
</p><p>99 Freeman, Image and depth from a convene tional camera with a coded aperture, ACM Transactions on Graphics, 2007. [sent-260, score-0.524]
</p><p>100 Freisleben, Using depth features to retrieve monocular video shots, In Proceedings of the 6th ACM international conference on image and video retrieval, 2007. [sent-265, score-0.89]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('depth', 0.291), ('metadata', 0.269), ('monocular', 0.248), ('exif', 0.239), ('object', 0.237), ('bounding', 0.209), ('camera', 0.198), ('metric', 0.179), ('box', 0.155), ('visual', 0.127), ('patch', 0.119), ('search', 0.114), ('recognition', 0.109), ('category', 0.106), ('image', 0.106), ('sensors', 0.095), ('detection', 0.094), ('scene', 0.092), ('intrinsics', 0.09), ('surf', 0.09), ('stereo', 0.089), ('local', 0.088), ('sensor', 0.088), ('branch', 0.086), ('video', 0.082), ('features', 0.081), ('feature', 0.08), ('aperture', 0.079), ('descriptors', 0.076), ('absolute', 0.073), ('icsi', 0.072), ('sensing', 0.07), ('categories', 0.069), ('cameras', 0.068), ('darrell', 0.068), ('size', 0.068), ('objects', 0.066), ('codebook', 0.064), ('sift', 0.062), ('obstacle', 0.061), ('vision', 0.061), ('apperance', 0.06), ('gloh', 0.06), ('jpeg', 0.06), ('junsong', 0.06), ('lidar', 0.06), ('rig', 0.06), ('subvolume', 0.06), ('vip', 0.06), ('zicheng', 0.06), ('nc', 0.059), ('physical', 0.056), ('modalities', 0.055), ('schemes', 0.054), ('rectangle', 0.053), ('eecs', 0.053), ('keyboard', 0.052), ('situated', 0.052), ('discriminative', 0.052), ('scale', 0.052), ('uc', 0.05), ('training', 0.05), ('scenes', 0.05), ('baseline', 0.049), ('vehicle', 0.048), ('imagery', 0.048), ('captured', 0.047), ('pami', 0.047), ('scheme', 0.046), ('sources', 0.045), ('reconcile', 0.045), ('scans', 0.045), ('stored', 0.045), ('measurements', 0.043), ('lowe', 0.043), ('nearest', 0.042), ('shape', 0.041), ('pixel', 0.041), ('occlusion', 0.041), ('outdoor', 0.041), ('indoor', 0.041), ('appearance', 0.04), ('images', 0.04), ('available', 0.04), ('test', 0.039), ('valid', 0.039), ('ying', 0.039), ('graphics', 0.039), ('mobile', 0.039), ('progress', 0.039), ('invariant', 0.038), ('wu', 0.037), ('trevor', 0.037), ('robotic', 0.037), ('retrieval', 0.036), ('infer', 0.036), ('eye', 0.035), ('matters', 0.035), ('coded', 0.035), ('word', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="241-tfidf-1" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>2 0.25362012 <a title="241-tfidf-2" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>3 0.2254048 <a title="241-tfidf-3" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>4 0.21126458 <a title="241-tfidf-4" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>5 0.17824736 <a title="241-tfidf-5" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>6 0.17504241 <a title="241-tfidf-6" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>7 0.15014175 <a title="241-tfidf-7" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>8 0.14872916 <a title="241-tfidf-8" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>9 0.14789587 <a title="241-tfidf-9" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>10 0.1439812 <a title="241-tfidf-10" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>11 0.1312404 <a title="241-tfidf-11" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>12 0.11811727 <a title="241-tfidf-12" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>13 0.11399239 <a title="241-tfidf-13" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>14 0.11155825 <a title="241-tfidf-14" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>15 0.11067326 <a title="241-tfidf-15" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>16 0.10953461 <a title="241-tfidf-16" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>17 0.10805805 <a title="241-tfidf-17" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>18 0.10055427 <a title="241-tfidf-18" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>19 0.097764678 <a title="241-tfidf-19" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>20 0.094723649 <a title="241-tfidf-20" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.244), (1, 0.122), (2, -0.207), (3, -0.297), (4, 0.018), (5, -0.044), (6, -0.045), (7, 0.038), (8, 0.017), (9, 0.077), (10, 0.065), (11, 0.042), (12, -0.153), (13, 0.003), (14, 0.065), (15, 0.012), (16, 0.093), (17, -0.13), (18, 0.081), (19, 0.062), (20, 0.007), (21, -0.038), (22, -0.022), (23, 0.018), (24, 0.005), (25, -0.054), (26, 0.037), (27, 0.049), (28, 0.005), (29, -0.054), (30, -0.076), (31, 0.012), (32, -0.029), (33, -0.075), (34, 0.05), (35, -0.029), (36, -0.013), (37, 0.012), (38, 0.019), (39, -0.012), (40, -0.009), (41, -0.009), (42, 0.054), (43, 0.013), (44, 0.04), (45, -0.06), (46, 0.035), (47, 0.042), (48, -0.011), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9658848 <a title="241-lsi-1" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>2 0.89408773 <a title="241-lsi-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.87868029 <a title="241-lsi-3" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>4 0.79603022 <a title="241-lsi-4" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>5 0.79511088 <a title="241-lsi-5" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>6 0.73564827 <a title="241-lsi-6" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>7 0.72609717 <a title="241-lsi-7" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>8 0.72051829 <a title="241-lsi-8" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>9 0.70379454 <a title="241-lsi-9" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>10 0.68432921 <a title="241-lsi-10" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>11 0.65271217 <a title="241-lsi-11" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>12 0.64982802 <a title="241-lsi-12" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>13 0.62118477 <a title="241-lsi-13" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>14 0.61382264 <a title="241-lsi-14" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>15 0.60648286 <a title="241-lsi-15" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>16 0.57063973 <a title="241-lsi-16" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>17 0.5396679 <a title="241-lsi-17" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>18 0.53200418 <a title="241-lsi-18" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>19 0.52687746 <a title="241-lsi-19" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>20 0.48525286 <a title="241-lsi-20" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(12, 0.071), (13, 0.023), (17, 0.018), (27, 0.065), (30, 0.049), (35, 0.038), (45, 0.267), (50, 0.218), (52, 0.011), (60, 0.047), (77, 0.022), (78, 0.02), (90, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96422416 <a title="241-lda-1" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>Author: Nuno Vasconcelos, Mohammad J. Saberian</p><p>Abstract: The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classiﬁcation and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classiﬁcation risk and complexity. It searches the space of cascade conﬁgurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems. 1</p><p>2 0.95448768 <a title="241-lda-2" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>Author: George Papandreou, Alan L. Yuille</p><p>Abstract: We present a technique for exact simulation of Gaussian Markov random ﬁelds (GMRFs), which can be interpreted as locally injecting noise to each Gaussian factor independently, followed by computing the mean/mode of the perturbed GMRF. Coupled with standard iterative techniques for the solution of symmetric positive deﬁnite systems, this yields a very efﬁcient sampling algorithm with essentially linear complexity in terms of speed and memory requirements, well suited to extremely large scale probabilistic models. Apart from synthesizing data under a Gaussian model, the proposed technique directly leads to an efﬁcient unbiased estimator of marginal variances. Beyond Gaussian models, the proposed algorithm is also very useful for handling highly non-Gaussian continuously-valued MRFs such as those arising in statistical image modeling or in the ﬁrst layer of deep belief networks describing real-valued data, where the non-quadratic potentials coupling different sites can be represented as ﬁnite or inﬁnite mixtures of Gaussians with the help of local or distributed latent mixture assignment variables. The Bayesian treatment of such models most naturally involves a block Gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate Gaussian continuous vector and we show that it can directly beneﬁt from the proposed methods. 1</p><p>3 0.95148259 <a title="241-lda-3" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>Author: Jan Gasthaus, Yee W. Teh</p><p>Abstract: The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efﬁcient representation, and inference algorithms operating on the new representation. Our derivations are based on precise deﬁnitions of the various processes that will also allow us to provide an elementary proof of the “mysterious” coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements. 1</p><p>4 0.94975394 <a title="241-lda-4" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>5 0.94764298 <a title="241-lda-5" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>Author: Yi Zhang, Jeff G. Schneider</p><p>Abstract: In this paper, we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overﬁtting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via ℓ1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple ﬁelds and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and ﬂexible way to model various different structures of multiple tasks.</p><p>6 0.9432677 <a title="241-lda-6" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>same-paper 7 0.92516243 <a title="241-lda-7" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>8 0.89801919 <a title="241-lda-8" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>9 0.88860297 <a title="241-lda-9" href="./nips-2010-Copula_Processes.html">54 nips-2010-Copula Processes</a></p>
<p>10 0.88654286 <a title="241-lda-10" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>11 0.88284135 <a title="241-lda-11" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>12 0.88175553 <a title="241-lda-12" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>13 0.88007241 <a title="241-lda-13" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>14 0.87858719 <a title="241-lda-14" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>15 0.87594867 <a title="241-lda-15" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>16 0.87593544 <a title="241-lda-16" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>17 0.87033355 <a title="241-lda-17" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>18 0.86738884 <a title="241-lda-18" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>19 0.86536545 <a title="241-lda-19" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>20 0.86514533 <a title="241-lda-20" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
