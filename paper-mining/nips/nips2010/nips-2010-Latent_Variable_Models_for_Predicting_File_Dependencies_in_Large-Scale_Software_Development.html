<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-139" href="#">nips2010-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</h1>
<br/><p>Source: <a title="nips-2010-139-pdf" href="http://papers.nips.cc/paper/4022-latent-variable-models-for-predicting-file-dependencies-in-large-scale-software-development.pdf">pdf</a></p><p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>Reference: <a title="nips-2010-139-reference" href="../nips2010_reference/nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. [sent-6, score-0.235]
</p><p>2 Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. [sent-7, score-0.453]
</p><p>3 We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. [sent-9, score-0.2]
</p><p>4 We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. [sent-10, score-0.362]
</p><p>5 1  Introduction  As software systems grow in size and complexity, they become more difﬁcult to develop and maintain. [sent-12, score-0.123]
</p><p>6 Nowadays, it is not uncommon for a code base to contain source ﬁles in multiple programming languages, text documents with meta information, XML documents for web interfaces, and even platform-dependent versions of the same application. [sent-13, score-0.137]
</p><p>7 This complexity creates many challenges because no single developer can be an expert in all things. [sent-14, score-0.1]
</p><p>8 One such challenge arises whenever a developer wishes to update one or more ﬁles in the code base. [sent-15, score-0.23]
</p><p>9 Often, seemingly localized changes will require many parts of the code base to be updated. [sent-16, score-0.137]
</p><p>10 Let S denote a set of starter ﬁles that the developer wishes to modify, and let R denote the set of relevant ﬁles that require updating after modifying S. [sent-18, score-0.567]
</p><p>11 In a large system, where the developer cannot possibly be familiar with the entire code base, automated tools that can recommend ﬁles in R given starter ﬁles in S are extremely useful. [sent-19, score-0.631]
</p><p>12 A number of automated tools now make recommendations of this sort by mining the development history of the code base [1, 2]. [sent-20, score-0.471]
</p><p>13 Work in this area has been facilitated by code versioning systems, such as CVS or Subversion, which record the development histories of large software projects. [sent-21, score-0.444]
</p><p>14 In these histories, transactions denote sets of ﬁles that have been jointly modiﬁed—that is, whose changes have been submitted to the code base within a short time interval. [sent-22, score-0.189]
</p><p>15 In this paper, we explore the use of latent variable models (LVMs) for modeling the development history of large code bases. [sent-24, score-0.342]
</p><p>16 We consider a number of different models, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. [sent-25, score-0.133]
</p><p>17 In these models, the problem of recommending relevant ﬁles can be viewed as a problem in binary matrix completion. [sent-26, score-0.107]
</p><p>18 We present experimental results on the development histories of three large open-source systems: Mozilla Firefox, Eclipse Subversive, and Gimp. [sent-27, score-0.239]
</p><p>19 In all of these applications, we ﬁnd that LVMs outperform the current leading method for mining development histories. [sent-28, score-0.237]
</p><p>20 1  2  Related work  Two broad classes of methods are used for identifying ﬁle dependencies in large code bases; one analyzes the semantic content of the code base while the other analyzes its development history. [sent-29, score-0.501]
</p><p>21 1  Impact analysis  The ﬁeld of impact analysis [3] draws on tools from software engineering in order to identify the consequences of code modiﬁcations. [sent-31, score-0.317]
</p><p>22 Most approaches in this tradition attempt to identify program dependencies by inspecting and/or running the program itself. [sent-32, score-0.179]
</p><p>23 These methods can identify many dependencies; however, they have trouble on certain difﬁcult cases such as cross-language dependencies (e. [sent-34, score-0.115]
</p><p>24 , between a data conﬁguration ﬁle and the code that uses it) and cross-program dependencies (e. [sent-36, score-0.167]
</p><p>25 2  Mining of development histories  Data-driven methods identify ﬁle dependencies in large software projects by analyzing their development histories. [sent-41, score-0.614]
</p><p>26 Both groups use frequent itemset mining (FIM) [9], a general heuristic for identifying frequent patterns in large databases. [sent-45, score-0.295]
</p><p>27 The patterns extracted from development histories are just those sets of ﬁles that have been jointly modiﬁed at some point in the past; the frequent patterns are the patterns that have occurred at least τ times. [sent-46, score-0.444]
</p><p>28 Given a database and a minimum support threshold, the resulting set of frequent patterns is uniquely speciﬁed. [sent-49, score-0.141]
</p><p>29 [1] uses a FIM algorithm called FP-growth, which extracts frequent patterns by using a tree-like data structure that is cleverly designed to prune the number of possible patterns to be searched. [sent-52, score-0.162]
</p><p>30 FPgrowth is used to ﬁnd all frequent patterns that contain the set of starter ﬁles; the joint sets of these frequent patterns are then returned as recommendations. [sent-53, score-0.604]
</p><p>31 [2] uses the popular Apriori algorithm [11] (which uses FIM to solve a subtask) to form association rules from the development history. [sent-56, score-0.137]
</p><p>32 ” After identifying all rules in which starter ﬁles appear on the left hand side, their tool recommends all ﬁles that appear on the right hand side. [sent-58, score-0.393]
</p><p>33 They also work with content on a ﬁner granularity, recommending not only relevant ﬁles, but also relevant code blocks within ﬁles. [sent-59, score-0.213]
</p><p>34 [2], the ground-truth recommendations are the ﬁles checked-in together at some point in the past, as revealed by the development history. [sent-66, score-0.185]
</p><p>35 Other researchers have also used the development history to detect ﬁle dependencies, but in markedly different ways. [sent-67, score-0.193]
</p><p>36 [12] formulate the problem as one of binary classiﬁcation; they label pairs of source ﬁles as relevant or non-relevant based on their joint modiﬁcation histories. [sent-69, score-0.105]
</p><p>37 Robillard [13] analyzes the topology of structural dependencies between ﬁles at the codeblock level. [sent-70, score-0.115]
</p><p>38 [15] identify clusters of dependent ﬁles by performing singular value decomposition on the development history. [sent-73, score-0.233]
</p><p>39 3  Latent variable modeling of development histories  We examine four latent variable models of ﬁle dependence in software systems. [sent-74, score-0.401]
</p><p>40 All these models represent the development history as an N × D large binary matrix, where non-zero elements in 2  the same row indicate ﬁles that were checked-in together or jointly modiﬁed at some point in time. [sent-75, score-0.222]
</p><p>41 , fD ) is an ordered collection of all ﬁles referenced in a static version of the development history. [sent-82, score-0.137]
</p><p>42 A transaction is a set of ﬁles that were modiﬁed together, according to the development history. [sent-84, score-0.338]
</p><p>43 We represent each transaction by a D-dimensional binary vector x = (x1 , . [sent-85, score-0.23]
</p><p>44 , xD ), where xi = 1 if the fi is a member of the transaction, and xi = 0 otherwise. [sent-88, score-0.083]
</p><p>45 A development history D is a set of N transaction vectors {x1 , x2 , . [sent-90, score-0.394]
</p><p>46 A starter set is a set of s starter ﬁles S = (fi1 , . [sent-96, score-0.732]
</p><p>47 , fjr ) that we label as relevant to the starter set S. [sent-104, score-0.442]
</p><p>48 1  Bernoulli mixture model  The simplest model that we explore is a Bernoulli mixture model (BMM). [sent-106, score-0.21]
</p><p>49 In training, the observed variables are the D binary elements xi ∈ {0, 1} of each transaction vector. [sent-108, score-0.259]
</p><p>50 , k} that can be viewed as assigning each transaction vector to one of k clusters. [sent-112, score-0.201]
</p><p>51 We use the EM algorithm to estimate parameters that maximize the likelihood p(D|π, µ) = n p(xn |π, µ) of the transactions in the development history. [sent-117, score-0.189]
</p><p>52 When a software developer wishes to modify a set of starter ﬁles, she can query a trained BMM to identify a set of relevant ﬁles. [sent-118, score-0.72]
</p><p>53 , xis } denote the elements of the transaction vector indicating the ﬁles in the starter set S. [sent-122, score-0.567]
</p><p>54 Let r denote the D − s remaining elements of the transaction vector indicating ﬁles that may or may not be relevant. [sent-123, score-0.201]
</p><p>55 (2)  z=1  The most likely set of relevant ﬁles, according to the model, is given by the completed transaction r∗ that maximizes the right hand side of eq. [sent-126, score-0.254]
</p><p>56 As an approximation, we sort the possibly relevant ﬁles by their individual posterior probabilities p(xi = 1|s = 1) for fi ∈ S. [sent-130, score-0.106]
</p><p>57 / Then we recommend all ﬁles whose posterior probabilities p(xi = 1|s = 1) exceed some threshold; we optimize the threshold on a held-out set of training examples. [sent-131, score-0.089]
</p><p>58 2  Bayesian Bernoulli mixture model  We also explore a Bayesian treatment of the BMM. [sent-133, score-0.119]
</p><p>59 In a Bayesian Bernoulli mixture (BBM), instead of learning point estimates of the parameters {π, µ}, we introduce a prior distribution p(π, µ) and make predictions by averaging over the posterior distribution p(π, µ|D). [sent-134, score-0.148]
</p><p>60 Figure 1: Graphical model of the Bernoulli mixture model (BMM), the Bayesian Bernoulli mixture (BBM), the restricted Boltzmann machine (RBM), and logistic PCA. [sent-140, score-0.25]
</p><p>61 In our BBMs, the mixture weight parameters are drawn from a Dirichlet prior1 : p(π|α) = Dirichlet (π |α/k, . [sent-141, score-0.091]
</p><p>62 , α/k ) ,  (3)  where k indicates (as before) the number of mixture components and α is a hyperparameter of the Dirichlet prior, the so-called concentration parameter2 . [sent-144, score-0.091]
</p><p>63 In particular, we integrate out the Bernoulli parameters µ and the cluster distribution parameters π, and we sample the cluster assignment variables z. [sent-147, score-0.108]
</p><p>64 For Gibbs sampling, we must compute the conditional probability p(zn = j|z−n , D) that the nth transaction is assigned to cluster j, given the training data D and all other cluster assignments z−n . [sent-148, score-0.309]
</p><p>65 We use T of these samples to estimate the probability that a ﬁle xi needs to be changed given ﬁles in the starter set S. [sent-151, score-0.432]
</p><p>66 The graphical model of an RBM is a fully connected bipartite 1  In preliminary experiments, we also investigated an inﬁnite mixture of Bernoulli distributions that replaces the Dirichlet prior by a Dirichlet process [16]. [sent-155, score-0.114]
</p><p>67 However, we did not ﬁnd the inﬁnite mixture model to outperform its ﬁnite counterpart, so we do not discuss it further. [sent-156, score-0.12]
</p><p>68 The product form of RBMs can model much sharper distributions over the observed variables than mixture models [17], making them an interesting alternative to consider for our application. [sent-164, score-0.091]
</p><p>69 To determine whether a ﬁle fi is relevant given starter ﬁles in S, we can either (i) clamp the observed variables representing starter ﬁles and perform Gibbs sampling on the rest, or (ii) compute the posterior over the remaining ﬁles using a fast, factorized approximation [19]. [sent-169, score-0.838]
</p><p>70 To use logistic PCA, we stack the N transaction vectors xn ∈ {0, 1}D of the development history into a N ×D binary matrix X. [sent-176, score-0.469]
</p><p>71 After obtaining a low-rank factorization of the log-odds matrix Θ = UV, we can use it to recommend relevant ﬁles from starter ﬁles S = {fi1 , fi2 , . [sent-183, score-0.48]
</p><p>72 To recommend relevant ﬁles, we compute the vector u that optimizes the regularized log-loss: s  LS (u) =  log σ(u·vij ) + j=1  3  λ u 2, 2  We use the approach in [17] known as contrastive divergence with m Gibbs sweeps (CD-m). [sent-187, score-0.178]
</p><p>73 The vector u obtained in this way is the low dimensional representation of the transaction with starter ﬁles in S. [sent-190, score-0.567]
</p><p>74 To determine whether ﬁle fi is relevant, we compute the probability p(xi = 1|u, V) = σ(u · vi ) and recommend the ﬁle if this probability exceeds some threshold. [sent-191, score-0.086]
</p><p>75 (We tune the threshold on held-out transactions from the development history). [sent-192, score-0.189]
</p><p>76 These open-source projects use software conﬁguration management (SCM) tools which provide logs that allow us to extract binary vectors indicating which ﬁles were changed during a transaction. [sent-194, score-0.233]
</p><p>77 2) by removing all ﬁles in the code base that occur very infrequently. [sent-200, score-0.137]
</p><p>78 For each transaction in the test set, we formed a “query” and “label” set by randomly picking a set of changed ﬁles as starter ﬁles. [sent-203, score-0.604]
</p><p>79 The remaining ﬁles that were changed in the transaction form the label set, which is the set of ﬁles our models must predict. [sent-204, score-0.261]
</p><p>80 The Bernoulli mixture models (BMMs) were trained by 100 or fewer iterations of the EM algorithm. [sent-208, score-0.091]
</p><p>81 The concentration parameter α was set to 50 to reﬂect our prior knowledge that ﬁle dependencies typically form a large number of small clusters. [sent-219, score-0.085]
</p><p>82 230  Table 2: Performance of FIM and LVMs on three datasets for queries with 1 or 3 starter ﬁles. [sent-463, score-0.366]
</p><p>83 Several experiments were run on different values of starter ﬁles (abbreviated “Start”) and minimum support thresholds (abbreviated “Support”). [sent-467, score-0.388]
</p><p>84 The latter measure reﬂects how well our models identify relevant ﬁles for a particular starter ﬁle, without the added complication of thresholding. [sent-469, score-0.449]
</p><p>85 The table reveals that the clusters correspond to interpretable structure in the code that span multiple data formats and languages. [sent-481, score-0.158]
</p><p>86 The ﬁrst cluster deals with the JIT compiler for JavaScript, while the second and third deal with the CSS style sheet manager and web browser properties. [sent-482, score-0.083]
</p><p>87 The dependencies in the last two clusters would have been missed by conventional impact analysis. [sent-483, score-0.157]
</p><p>88 Within each cluster, the 10 ﬁles with highest membership probabilities are shown; note how these ﬁles span multiple data formats and program languages, revealing dependencies that would escape the notice of traditional methods. [sent-517, score-0.15]
</p><p>89 Given a set S of starter ﬁles, FIM simply looks at co-occurrence data; it recommends a set of ﬁles R for which the number of transactions that contain both R and S is frequent. [sent-519, score-0.445]
</p><p>90 In the majority of our experiments, mixture models (with many mixture components) appear to outperform RBMs and logistic PCA. [sent-525, score-0.257]
</p><p>91 We tried to resolve this problem by using a sparsity prior on the states of the hidden units y to make the RBMs behave more like a mixture model [23], but in preliminary experiments, we did not ﬁnd this to improve the performance. [sent-528, score-0.114]
</p><p>92 Another interesting observation is that the Bayesian treatment of the Bernoulli mixture model generally leads to better predictions than a maximum likelihood approach, as it is less susceptible to overﬁtting. [sent-529, score-0.12]
</p><p>93 This advantage is particularly useful in ﬁle dependency prediction which requires models with a large number of mixture components to appropriately model data that consists of many small, distinct clusters while having few training instances (i. [sent-530, score-0.134]
</p><p>94 6  Conclusion  In this paper, we have described a new application of binary matrix completion for predicting ﬁle dependencies in software projects. [sent-533, score-0.237]
</p><p>95 Such information includes the identity of users who committed transactions to the code base, as well as the text of actual changes to the source code. [sent-539, score-0.163]
</p><p>96 It remains a grand challenge to incorporate all the available information from development histories into a probabilistic model for predicting which ﬁles need to be modiﬁed. [sent-540, score-0.239]
</p><p>97 In future work, we aim to explore discriminative methods for parameter estimation, as well as online algorithms for tracking non-stationary trends in the code base. [sent-541, score-0.11]
</p><p>98 Mining the maintenance history of a legacy software system. [sent-617, score-0.208]
</p><p>99 Empirical software change impact analysis using singular value decomposition. [sent-637, score-0.152]
</p><p>100 Markov chain sampling methods for Dirichlet process mixture models. [sent-642, score-0.091]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('les', 0.608), ('starter', 0.366), ('lvms', 0.266), ('transaction', 0.201), ('fim', 0.161), ('development', 0.137), ('bmm', 0.133), ('firefox', 0.133), ('software', 0.123), ('eclipse', 0.116), ('le', 0.116), ('bernoulli', 0.107), ('histories', 0.102), ('developer', 0.1), ('subversive', 0.1), ('mixture', 0.091), ('dependencies', 0.085), ('zimmerman', 0.083), ('code', 0.082), ('rbms', 0.08), ('frequent', 0.076), ('mozilla', 0.073), ('gibbs', 0.072), ('mining', 0.071), ('pca', 0.07), ('bbm', 0.067), ('bbms', 0.067), ('ying', 0.066), ('recommend', 0.061), ('rbm', 0.058), ('history', 0.056), ('boltzmann', 0.055), ('base', 0.055), ('cluster', 0.054), ('relevant', 0.053), ('transactions', 0.052), ('slicing', 0.05), ('files', 0.05), ('recommendations', 0.048), ('wishes', 0.048), ('logistic', 0.046), ('languages', 0.046), ('dirichlet', 0.045), ('patterns', 0.043), ('clusters', 0.043), ('iz', 0.04), ('nij', 0.04), ('latent', 0.039), ('nj', 0.038), ('sweeps', 0.038), ('changed', 0.037), ('bmms', 0.033), ('cpr', 0.033), ('fis', 0.033), ('formats', 0.033), ('gimp', 0.033), ('kagdi', 0.033), ('scm', 0.033), ('sherriff', 0.033), ('shirabad', 0.033), ('xni', 0.033), ('recommended', 0.033), ('program', 0.032), ('engineering', 0.031), ('identify', 0.03), ('analyzes', 0.03), ('predictions', 0.029), ('modi', 0.029), ('committed', 0.029), ('maintenance', 0.029), ('compiler', 0.029), ('itemset', 0.029), ('xnd', 0.029), ('xi', 0.029), ('impact', 0.029), ('binary', 0.029), ('outperform', 0.029), ('posterior', 0.028), ('explore', 0.028), ('admittedly', 0.027), ('recommends', 0.027), ('beta', 0.027), ('contrastive', 0.026), ('fi', 0.025), ('recommending', 0.025), ('zn', 0.025), ('et', 0.025), ('bayesian', 0.024), ('preliminary', 0.023), ('abbreviated', 0.023), ('label', 0.023), ('dependent', 0.023), ('tools', 0.022), ('support', 0.022), ('machines', 0.022), ('restricted', 0.022), ('nov', 0.022), ('logs', 0.022), ('exponential', 0.02), ('start', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="139-tfidf-1" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>2 0.29512212 <a title="139-tfidf-2" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>3 0.056362741 <a title="139-tfidf-3" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>4 0.055608362 <a title="139-tfidf-4" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>Author: Ling Huang, Jinzhu Jia, Bin Yu, Byung-gon Chun, Petros Maniatis, Mayur Naik</p><p>Abstract: Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program’s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.</p><p>5 0.054708946 <a title="139-tfidf-5" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>6 0.054438345 <a title="139-tfidf-6" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>7 0.052941944 <a title="139-tfidf-7" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>8 0.052099679 <a title="139-tfidf-8" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>9 0.050932597 <a title="139-tfidf-9" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>10 0.050899852 <a title="139-tfidf-10" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>11 0.050849151 <a title="139-tfidf-11" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>12 0.050819971 <a title="139-tfidf-12" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>13 0.048938062 <a title="139-tfidf-13" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>14 0.048474807 <a title="139-tfidf-14" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>15 0.047241956 <a title="139-tfidf-15" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>16 0.047114901 <a title="139-tfidf-16" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>17 0.046943881 <a title="139-tfidf-17" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>18 0.045782294 <a title="139-tfidf-18" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>19 0.044288985 <a title="139-tfidf-19" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>20 0.043550193 <a title="139-tfidf-20" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.034), (2, -0.008), (3, 0.021), (4, -0.09), (5, -0.001), (6, 0.019), (7, 0.036), (8, -0.041), (9, -0.002), (10, -0.004), (11, 0.007), (12, 0.06), (13, -0.042), (14, 0.065), (15, -0.013), (16, 0.017), (17, 0.117), (18, 0.052), (19, -0.05), (20, -0.015), (21, 0.139), (22, 0.077), (23, 0.036), (24, 0.077), (25, -0.006), (26, 0.018), (27, -0.016), (28, -0.105), (29, 0.071), (30, -0.084), (31, 0.006), (32, -0.006), (33, -0.085), (34, 0.042), (35, -0.226), (36, -0.017), (37, -0.007), (38, -0.102), (39, 0.206), (40, 0.03), (41, 0.042), (42, 0.046), (43, 0.158), (44, -0.02), (45, 0.074), (46, -0.124), (47, -0.094), (48, -0.23), (49, 0.176)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94202918 <a title="139-lsi-1" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>2 0.83339459 <a title="139-lsi-2" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>3 0.36408103 <a title="139-lsi-3" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>4 0.35996783 <a title="139-lsi-4" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>Author: Jan Gasthaus, Yee W. Teh</p><p>Abstract: The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efﬁcient representation, and inference algorithms operating on the new representation. Our derivations are based on precise deﬁnitions of the various processes that will also allow us to provide an elementary proof of the “mysterious” coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements. 1</p><p>5 0.35807192 <a title="139-lsi-5" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>Author: Zeeshan Syed, John V. Guttag</p><p>Abstract: Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have beneﬁted from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratiﬁcation. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identiﬁed using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We ﬁrst describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classiﬁer, and hierarchical clustering to improve risk stratiﬁcation. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically signiﬁcant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically signiﬁcantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days. 1</p><p>6 0.34881276 <a title="139-lsi-6" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>7 0.34850812 <a title="139-lsi-7" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>8 0.34358877 <a title="139-lsi-8" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>9 0.33277813 <a title="139-lsi-9" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>10 0.32494697 <a title="139-lsi-10" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>11 0.31609505 <a title="139-lsi-11" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>12 0.29109156 <a title="139-lsi-12" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>13 0.29044792 <a title="139-lsi-13" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>14 0.28141317 <a title="139-lsi-14" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>15 0.27925777 <a title="139-lsi-15" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>16 0.27821124 <a title="139-lsi-16" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>17 0.27650696 <a title="139-lsi-17" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>18 0.27580428 <a title="139-lsi-18" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>19 0.27472404 <a title="139-lsi-19" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>20 0.27052045 <a title="139-lsi-20" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.043), (17, 0.014), (27, 0.084), (30, 0.049), (33, 0.316), (35, 0.013), (45, 0.2), (50, 0.045), (52, 0.046), (60, 0.027), (77, 0.036), (90, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8033517 <a title="139-lda-1" href="./nips-2010-Generalized_roof_duality_and_bisubmodular_functions.html">102 nips-2010-Generalized roof duality and bisubmodular functions</a></p>
<p>Author: Vladimir Kolmogorov</p><p>Abstract: ˆ Consider a convex relaxation f of a pseudo-boolean function f . We say that ˆ the relaxation is totally half-integral if f (x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj , xi = 1 − xj , and xi = γ where 1 γ ∈ {0, 1, 2 } is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f . We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization ˆ of totally half-integral relaxations f by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. 1</p><p>same-paper 2 0.7428121 <a title="139-lda-2" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>3 0.70427001 <a title="139-lda-3" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>4 0.64572489 <a title="139-lda-4" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>5 0.59713012 <a title="139-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.59540159 <a title="139-lda-6" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>7 0.59477377 <a title="139-lda-7" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>8 0.59351504 <a title="139-lda-8" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>9 0.59350163 <a title="139-lda-9" href="./nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">69 nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>10 0.59328932 <a title="139-lda-10" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>11 0.5930301 <a title="139-lda-11" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>12 0.59253919 <a title="139-lda-12" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>13 0.59251511 <a title="139-lda-13" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>14 0.59235889 <a title="139-lda-14" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>15 0.59043479 <a title="139-lda-15" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>16 0.59032899 <a title="139-lda-16" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>17 0.58999681 <a title="139-lda-17" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>18 0.58957374 <a title="139-lda-18" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>19 0.58925748 <a title="139-lda-19" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>20 0.58925563 <a title="139-lda-20" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
