<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-139" href="#">nips2010-139</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</h1>
<br/><p>Source: <a title="nips-2010-139-pdf" href="http://papers.nips.cc/paper/4022-latent-variable-models-for-predicting-file-dependencies-in-large-scale-software-development.pdf">pdf</a></p><p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>Reference: <a title="nips-2010-139-reference" href="../nips2010_reference/nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('les', 0.652), ('lvms', 0.328), ('fim', 0.198), ('recommend', 0.18), ('bmm', 0.164), ('firefox', 0.164), ('softw', 0.151), ('eclips', 0.143), ('subvert', 0.143), ('le', 0.143), ('bernoull', 0.131), ('hist', 0.124), ('transact', 0.114), ('mixt', 0.111), ('zimmerm', 0.102), ('rbms', 0.098), ('mozill', 0.09), ('gib', 0.088), ('pca', 0.086), ('bbm', 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="139-tfidf-1" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>2 0.31849703 <a title="139-tfidf-2" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>3 0.078613095 <a title="139-tfidf-3" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>4 0.0736752 <a title="139-tfidf-4" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>5 0.067839399 <a title="139-tfidf-5" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>Author: Armand Joulin, Jean Ponce, Francis R. Bach</p><p>Abstract: Dimensionality reduction is commonly used in the setting of multi-label supervised classiﬁcation to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added beneﬁts in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efﬁcient algorithm based on convex relaxations and lowrank representations of the data, capable of handling large-scale problems. Experiments on text document classiﬁcation show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods. 1</p><p>6 0.066524893 <a title="139-tfidf-6" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>7 0.066494547 <a title="139-tfidf-7" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>8 0.063701309 <a title="139-tfidf-8" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>9 0.063450642 <a title="139-tfidf-9" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>10 0.06311132 <a title="139-tfidf-10" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>11 0.059235055 <a title="139-tfidf-11" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>12 0.058206864 <a title="139-tfidf-12" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>13 0.056538321 <a title="139-tfidf-13" href="./nips-2010-Optimal_Bayesian_Recommendation_Sets_and_Myopically_Optimal_Choice_Query_Sets.html">197 nips-2010-Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets</a></p>
<p>14 0.056397151 <a title="139-tfidf-14" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>15 0.055843774 <a title="139-tfidf-15" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>16 0.055803593 <a title="139-tfidf-16" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>17 0.055163838 <a title="139-tfidf-17" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>18 0.05507509 <a title="139-tfidf-18" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>19 0.054145418 <a title="139-tfidf-19" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>20 0.053575389 <a title="139-tfidf-20" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, -0.04), (2, 0.013), (3, -0.011), (4, 0.089), (5, 0.004), (6, -0.092), (7, -0.015), (8, -0.023), (9, 0.033), (10, 0.073), (11, -0.022), (12, -0.028), (13, -0.02), (14, 0.01), (15, -0.105), (16, -0.013), (17, 0.044), (18, -0.03), (19, 0.054), (20, 0.154), (21, 0.037), (22, 0.04), (23, 0.026), (24, 0.008), (25, 0.056), (26, 0.06), (27, 0.024), (28, 0.014), (29, -0.061), (30, -0.045), (31, 0.046), (32, -0.056), (33, 0.005), (34, -0.038), (35, 0.004), (36, -0.233), (37, 0.043), (38, 0.111), (39, 0.068), (40, 0.107), (41, -0.099), (42, 0.033), (43, -0.227), (44, 0.07), (45, -0.048), (46, 0.146), (47, 0.025), (48, 0.121), (49, -0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8971318 <a title="139-lsi-1" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>2 0.78574526 <a title="139-lsi-2" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>3 0.4726862 <a title="139-lsi-3" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>Author: Nimar Arora, Stuart Russell, Paul Kidwell, Erik B. Sudderth</p><p>Abstract: The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. 1</p><p>4 0.46293375 <a title="139-lsi-4" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>Author: Ken Takiyama, Masato Okada</p><p>Abstract: 019 020 We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary ﬁring rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis. 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053</p><p>5 0.38985711 <a title="139-lsi-5" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>6 0.38697147 <a title="139-lsi-6" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>7 0.35918114 <a title="139-lsi-7" href="./nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</a></p>
<p>8 0.35351694 <a title="139-lsi-8" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>9 0.35182494 <a title="139-lsi-9" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>10 0.33881864 <a title="139-lsi-10" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>11 0.33836162 <a title="139-lsi-11" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>12 0.33750543 <a title="139-lsi-12" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>13 0.33742145 <a title="139-lsi-13" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>14 0.33317366 <a title="139-lsi-14" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>15 0.32976943 <a title="139-lsi-15" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>16 0.32842007 <a title="139-lsi-16" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>17 0.32783788 <a title="139-lsi-17" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>18 0.31646127 <a title="139-lsi-18" href="./nips-2010-Gaussian_Process_Preference_Elicitation.html">100 nips-2010-Gaussian Process Preference Elicitation</a></p>
<p>19 0.3095237 <a title="139-lsi-19" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>20 0.30510932 <a title="139-lsi-20" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.12), (30, 0.047), (32, 0.144), (34, 0.068), (36, 0.01), (39, 0.291), (45, 0.078), (68, 0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89082825 <a title="139-lda-1" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>2 0.83091038 <a title="139-lda-2" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>same-paper 3 0.76386732 <a title="139-lda-3" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>4 0.65289581 <a title="139-lda-4" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>5 0.65112889 <a title="139-lda-5" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>6 0.65071851 <a title="139-lda-6" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>7 0.64926904 <a title="139-lda-7" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>8 0.64856589 <a title="139-lda-8" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>9 0.64817983 <a title="139-lda-9" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>10 0.64740086 <a title="139-lda-10" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>11 0.64658707 <a title="139-lda-11" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>12 0.64650333 <a title="139-lda-12" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>13 0.64633763 <a title="139-lda-13" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>14 0.64629507 <a title="139-lda-14" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>15 0.64628959 <a title="139-lda-15" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>16 0.64622903 <a title="139-lda-16" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>17 0.64615673 <a title="139-lda-17" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>18 0.64612842 <a title="139-lda-18" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>19 0.64582914 <a title="139-lda-19" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>20 0.64580989 <a title="139-lda-20" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
