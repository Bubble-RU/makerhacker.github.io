<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-129" href="#">nips2010-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</h1>
<br/><p>Source: <a title="nips-2010-129-pdf" href="http://papers.nips.cc/paper/3944-inter-time-segment-information-sharing-for-non-homogeneous-dynamic-bayesian-networks.pdf">pdf</a></p><p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>Reference: <a title="nips-2010-129-reference" href="../nips2010_reference/nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks  Dirk Husmeier & Frank Dondelinger Biomathematics & Statistics Scotland (BioSS) JCMB, The King’s Buildings, Edinburgh EH93JZ, United Kingdom dirk@bioss. [sent-1, score-0.199]
</p><p>2 Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. [sent-9, score-0.138]
</p><p>3 In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. [sent-11, score-0.275]
</p><p>4 We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. [sent-12, score-0.285]
</p><p>5 We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. [sent-13, score-0.373]
</p><p>6 The method proposed in [2] assumes a ﬁxed network structure and only allows the interaction parameters to vary with time. [sent-26, score-0.138]
</p><p>7 This assumption is too rigid when looking at processes where changes in the overall regulatory network structure are expected, e. [sent-27, score-0.301]
</p><p>8 These limitations are addressed in [6, 7], where the authors propose a method for continuous data that allows network structures associated with different nodes to change with time in different ways. [sent-31, score-0.181]
</p><p>9 Unlike [2], our model allows the network structure to change among segments, leading to greater model ﬂexibility. [sent-36, score-0.138]
</p><p>10 As an improvement on [6, 7], our model introduces information sharing among time series segments, which provides an essential regularization effect. [sent-37, score-0.174]
</p><p>11 In what follows, we will refer to nodes as genes and to the network as a gene regulatory network. [sent-39, score-0.502]
</p><p>12 the network deﬁned by a set of directed edges among the p genes. [sent-45, score-0.139]
</p><p>13 Mi is the subnetwork associated with target gene i, determined by the set of its parents (nodes with a directed edge feeding into gene i). [sent-46, score-0.415]
</p><p>14 The regulatory relationships among the genes, deﬁned by M, may vary across time, which we model with a multiple changepoint process. [sent-47, score-0.232]
</p><p>15 For each target gene i, an unknown number ki of changepoints h−1 deﬁne ki + 1 non-overlapping segments. [sent-48, score-0.763]
</p><p>16 , ki + 1 starts at changepoint ξi and ki +1 h−1 h−1 h h 0 0 h stops before ξi , where ξi = (ξi , . [sent-51, score-0.483]
</p><p>17 The set of changepoints is denoted by ξ = h {ξi }1≤i≤p . [sent-60, score-0.218]
</p><p>18 This changepoint process induces a partition of the time series, yi = (yi (t))ξh−1≤t<ξh , with i i different structures Mh associated with the different segments h ∈ {1, . [sent-61, score-0.301]
</p><p>19 Identiﬁability is i satisﬁed by ordering the changepoints based on their position in the time series. [sent-65, score-0.218]
</p><p>20 Regression model: For all genes i, the random variable Yi (t) refers to the expression of gene i at time t. [sent-66, score-0.285]
</p><p>21 Within any segment h, the expression of gene i depends on the p gene expression values measured at the previous time point through a regression model deﬁned by (a) a set of sh parents i h denoted by Mh = {j1 , . [sent-67, score-0.679]
</p><p>22 , p}, |Mh | = sh , and (b) a set of parameters ((ah )j∈0. [sent-73, score-0.128]
</p><p>23 2 Prior The ki + 1 segments are delimited by ki changepoints, where ki is distributed a priori as a truncated k Poisson random variable with mean λ and maximum k = N −2: P (ki |λ) ∝ λ ii 1{ki ≤k} . [sent-82, score-0.707]
</p><p>24 k 0 1 on ki changepoints, the changepoint positions vector ξi = (ξi , ξi , . [sent-84, score-0.293]
</p><p>25 There are “ −”2) possible posi(N tions for the ki changepoints, thus vector ξi has prior density P (ξi |ki ) = 1/ N −2 . [sent-88, score-0.241]
</p><p>26 For all genes i ki and all segments h, the number sh of parents for node i follows a truncated Poisson distribution2 with i h  Λsi sh ! [sent-89, score-0.706]
</p><p>27 Conditional on sh , the prior for the parent set l i i ˛ h Mi is a uniform distribution over all parent sets with cardinality sh : P (Mh ˛|Mh | = sh ) = 1/( ph ). [sent-91, score-0.435]
</p><p>28 The terms λ and Λ can be interpreted as the expected number of changepoints and parents, respectively, and δ 2 is the expected signal-to-noise ratio. [sent-99, score-0.218]
</p><p>29 These hyperparameters are drawn from vague conjugate hyperpriors, which are in the (inverse) gamma distribution family: P (Λ) = P (λ) = Ga(0. [sent-100, score-0.118]
</p><p>30 The number of changepoints and their location, k, ξ , the network structure M and the hyperparameters λ, Λ, δ 2 can be sampled from the posterior P (k, ξ, M, λ, Λ, δ 2 |y) with RJMCMC [11]. [sent-106, score-0.486]
</p><p>31 3  Model improvement: information coupling between segments  Allowing the network structure to change between segments leads to a highly ﬂexible model. [sent-108, score-0.522]
</p><p>32 If subsequent changepoints are close together, network structures have to be inferred from short time series segments. [sent-111, score-0.44]
</p><p>33 The conceptual problem is the underlying assumption that structures associated with different segments are a priori independent. [sent-113, score-0.176]
</p><p>34 For instance, for the evolution of a gene regulatory network during embryogenesis, we would assume that the network evolves gradually and that networks associated with adjacent time intervals are a priori similar. [sent-115, score-0.546]
</p><p>35 To address these problems, we propose three methods of information sharing among time series segments, as illustrated in Figure 1. [sent-116, score-0.174]
</p><p>36 The ﬁrst method is based on hard information coupling between the nodes, using the exponential distribution proposed in [13]. [sent-117, score-0.195]
</p><p>37 The second scheme is also based on hard information coupling, but uses a binomial distribution with conjugate Beta prior. [sent-118, score-0.186]
</p><p>38 The third scheme is based on the same distributional assumptions as the second scheme, but replaces the hard by a soft information coupling scheme. [sent-119, score-0.272]
</p><p>39 1(a): Hard coupling between nodes with common hyperparameter Θ regulating the strength of the coupling between structures associated with adjacent segments, Mh and Mh+1 . [sent-121, score-0.417]
</p><p>40 1(b): Soft coupling between nodes, with node-speciﬁc hyperparameters Θi coupled via level2-hyperparameters Ψ. [sent-125, score-0.23]
</p><p>41 1 Hard information coupling based on an exponential prior Denote by Ki := ki + 1 the total number of partitions in the time series associated with node i, h and recall that each time series segment yi is associated with a separate subnetwork Mh , 1 ≤ h ≤ i h−1 Ki . [sent-129, score-0.707]
</p><p>42 , MKi , β) = i i  Ki Y  h h−1 P (yi |Mh )P (Mh |Mi , β)P (β) i i  (7)  h=1  Similar to [13] we deﬁne h−1 P (Mh |Mi , β) = i  h−1 exp(−β|Mh − Mi |) i h−1 Zi (β, Mi )  (8)  for h ≥ 2, where β is a hyperparameter that deﬁnes the strength of the coupling between Mh i h−1 and Mi , and |. [sent-136, score-0.192]
</p><p>43 When proposing a new network structure Mh → Mh for segment h, the prior i i probability ratio has to be replaced by:  ˜ ˜ P (Mh+1 |Mh ,β)P (Mh |Mh−1 ,β) i i i i  P (Mh+1 |Mh ,β)P (Mh |Mh−1 ,β) i i i i  . [sent-143, score-0.281]
</p><p>44 An additional MCMC step is  introduced for sampling the hyperparameter β from the posterior distribution. [sent-144, score-0.092]
</p><p>45 When proposing a new network structure ˜ ˜ for node i and segment h, Mh → Mh , the structures Mh and Mh enter the prior probability i i i i ratio via the expression P ({Mh }|α, α, γ, γ), as i  K p ˜ P ({M1 ,. [sent-153, score-0.41]
</p><p>46 a consequence of integrating out the hyperparameters, all network structures become interdepen1 0 1 0 dent, and information about the structures is contained in the sufﬁcient statistics N1 , N1 , N0 , N0 . [sent-166, score-0.192]
</p><p>47 A new proposal move for the level-2 hyperparameters is added to the existing RJMCMC scheme of Section 2. [sent-167, score-0.136]
</p><p>48 4 so that when proposing a new ˜ network structure, Mh → Mh , the acceptance probability has to be updated with the prior ratio: i i ˜ P (M1 ,. [sent-184, score-0.227]
</p><p>49 Our program sets up an RJMCMC simulation to sample the network structure, the changepoints and the hyperparameters from the posterior distribution. [sent-206, score-0.462]
</p><p>50 The boxplots show the distributions of the scores for 10 datasets with 4 network segments each, where the horizontal bar shows the median, the box margins show the 25th and 75th percentiles, the whiskers indicate data within 2 times the interquartile range, and circles are outliers. [sent-223, score-0.227]
</p><p>51 “Same Segs” means that all segments in a dataset have the same structure, while “Different Segs” indicates that structure changes are applied to the segments sequentially. [sent-224, score-0.284]
</p><p>52 05 was reached, and then sampled 1000 network and changepoint conﬁgurations in intervals of 200 RJMCMC steps. [sent-226, score-0.217]
</p><p>53 1  Comparative evaluation on simulated data  We randomly generated 10 networks with 10 nodes each, with the number of parents per node drawn from a Poisson distribution with mean λ = 3. [sent-230, score-0.109]
</p><p>54 To simulate changes in the network structure, we created 4 different network segments by drawing the number of changes from a Poisson distribution and applying the changes uniformly at random to edges and non-edges in the previous segment. [sent-231, score-0.468]
</p><p>55 We compared the network reconstruction accuracy of the non-homogeneous DBN without information sharing proposed in [6, 7] (HetDBN-0) with the three information sharing approaches, based on the exponential prior from Section 3. [sent-234, score-0.499]
</p><p>56 1 (HetDBN-Exp), the binomial prior with hard node coupling from Section 3. [sent-235, score-0.375]
</p><p>57 2 (HetDBN-Bino1), and the binomial prior with soft node coupling from Section 3. [sent-236, score-0.382]
</p><p>58 Figures 2(a) and 2(b) shows the network reconstruction performance of the different information sharing methods in terms of AUROC and AUPRC scores. [sent-238, score-0.317]
</p><p>59 All information sharing methods show a clear improvement in network reconstruction over HetDBN-0, as conﬁrmed by paired t-tests (p < 0. [sent-239, score-0.317]
</p><p>60 We investigated two different situations, the case where all segment structures are the same (although edge weights are allowed to vary) and the case where changes are applied sequentially to the segments3 . [sent-241, score-0.141]
</p><p>61 Information sharing is most beneﬁcial for the ﬁrst case, but even when we introduce changes we still see an increase in the network reconstruction scores compared to HetDBN-0. [sent-242, score-0.351]
</p><p>62 When all segments are the same, HetDBN-Bino1 and HetDBNBino2 outperform HetDBN-Exp (p < 0. [sent-243, score-0.113]
</p><p>63 05), but there is no signiﬁcant difference between the two binomial methods. [sent-244, score-0.093]
</p><p>64 When the segments are different, all information sharing methods outperform HetDBN-0 (p < 0. [sent-246, score-0.244]
</p><p>65 05), but the difference between the information sharing methods is not signiﬁcant. [sent-247, score-0.131]
</p><p>66 2  Morphogenesis in Drosophila melanogaster  We applied our methods to a gene expression time series for eleven genes involved in the muscle development of Drosophila melanogaster [16]. [sent-249, score-0.434]
</p><p>67 The microarray data measured gene expression levels during all four major stages of morphogenesis: embryo, larva, pupa and adult. [sent-250, score-0.31]
</p><p>68 We investigated whether our methods were able to infer the correct changepoints corresponding to the known transitions between stages. [sent-251, score-0.265]
</p><p>69 Figure 3(a) shows the posterior probabilities of inferred changepoints for any gene using HetDBN-0, while Figure 3(c) shows the posterior probabilities for the information shar3  We chose to draw the number of changes from a Poisson with mean 1 for each node. [sent-252, score-0.559]
</p><p>70 All ﬁgures using HetDBN plot the posterior probability of a changepoint occurring for any node at a given time plotted against time. [sent-272, score-0.197]
</p><p>71 3(a): HetDBN-0 changepoints for Drosophila (no information sharing) 3(b): TESLA, L1norm of the difference of the regression parameter vectors associated with two adjacent time points plotted against time. [sent-273, score-0.218]
</p><p>72 3(c): HetDBN changepoints for Drosophila with information sharing; the method is indicated by the legend. [sent-274, score-0.218]
</p><p>73 3(d) HetDBN changepoints for the synthetic gene regulatory network in yeast. [sent-275, score-0.667]
</p><p>74 Our non-homogeneous DBN methods are generally more successful than TESLA, in that they recover changepoints for all three transitions (embryo → larva, larva → pupa, and pupa → adult). [sent-280, score-0.432]
</p><p>75 Both our method as well as TESLA detect additional transitions during the embryo stage, which are missing in [1]. [sent-282, score-0.114]
</p><p>76 We would argue that a complex gene regulatory network is unlikely to transition into a new morphogenic phase all at once, and some pathways might have to undergo activational changes earlier in preparation for the morphogenic transition. [sent-283, score-0.594]
</p><p>77 As such, it is not implausible that additional transitions at the gene regulatory network level occur. [sent-284, score-0.455]
</p><p>78 However, a failure to detect known morphogenic transitions can clearly be seen as a shortcoming of a method, and on these grounds our model appears to outperform the two alternative ones. [sent-285, score-0.123]
</p><p>79 We note that the main effect of information sharing is to reduce the size of the smaller peaks, while keeping the three most salient peaks (corresponding to larva → pupa, and pupa → adult, and an extra transition in the embryo phase). [sent-286, score-0.401]
</p><p>80 This reﬂects the fact that these changepoints are associated with signiﬁcant changes in network structure, and adds to the interpretability of the results. [sent-287, score-0.366]
</p><p>81 3  Reconstruction of a synthetic gene regulatory network in Saccharomyces cerevisiae  The highly topical ﬁeld of synthetic biology enables biologists to design known gene regulatory networks in living cells. [sent-290, score-0.921]
</p><p>82 The authors tried to reconstruct the known gold-standard network from these time series with two established state-of-the-art methods from computational systems biology, one based on ordinary differential equations (ODEs), called 7  Precision−Recall for Switch Off 1. [sent-292, score-0.157]
</p><p>83 0  Recall  Recall  Figure 4: Reconstruction of a known gene regulatory network from synthetic biology in yeast. [sent-316, score-0.525]
</p><p>84 The network was reconstructed from two gene expression time series obtained with RT-PCR in two experimental conditions, reﬂecting the switch in the carbon source from galactose (“switch on”) to glucose (“switch off”). [sent-317, score-0.599]
</p><p>85 The reconstruction accuracy of the methods proposed in Section 3, where the legend is explained, is shown in terms of precision (vertical axis) - recall (horizontal axis) curves. [sent-318, score-0.137]
</p><p>86 The most signiﬁcant changepoint is at the boundary between “switch on” and “switch off” data, conﬁrming that the known true changepoint is consistently identiﬁed. [sent-326, score-0.206]
</p><p>87 Interestingly, the application of the proposed information-coupling schemes reduces the height of these peaks, with the binomial models having a stronger effect than the exponential one. [sent-328, score-0.126]
</p><p>88 As we pursue a Bayesian inference scheme, we also obtain a ranking of the potential gene interactions in terms of their marginal posterior probabilities. [sent-329, score-0.223]
</p><p>89 Our non-homogeneous DBNs with information sharing outperform Banjo and TSNI both in the “switch on” and the “switch off” phase. [sent-331, score-0.131]
</p><p>90 Note that the reconstruction accuracy on the “switch off” data is generally poorer than on the “switch on” data [17]. [sent-333, score-0.096]
</p><p>91 Our results are thus plausible, suggesting that information sharing boosts the reconstruction accuracy on the poorer time series segment at the cost of a degraded performance on the stronger one. [sent-334, score-0.338]
</p><p>92 This effect is more pronounced for the exponential prior than for the binomial one, indicating a tighter coupling. [sent-335, score-0.144]
</p><p>93 Hence, the overall effect of information sharing is a performance improvement. [sent-342, score-0.131]
</p><p>94 An evaluation on simulated data has demonstrated an improved performance over [6, 7] when information sharing is introduced. [sent-344, score-0.131]
</p><p>95 The application of our method to gene expression time series taken during the life cycle of Drosophila melanogaster has revealed better agreement with known morphogenic transitions than the methods of [1] and [3]. [sent-345, score-0.486]
</p><p>96 We have carried out a comparative evaluation of different information coupling schemes: a binomial versus an exponential prior, and hard versus soft coupling. [sent-346, score-0.332]
</p><p>97 In an application to data from a topical study in synthetic biology, our methods have outperformed two established network reconstruction methods from computational systems biology. [sent-347, score-0.251]
</p><p>98 Gene regulatory network reconstruction by Bayesian integration of prior knowledge and/or different experimental conditions. [sent-451, score-0.366]
</p><p>99 Gene expression during the life cycle of Drosophila melanogaster. [sent-485, score-0.102]
</p><p>100 A yeast synthetic network for in vivo assessment of reverse-engineering and modeling approaches. [sent-498, score-0.209]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mh', 0.598), ('hetdbn', 0.288), ('changepoints', 0.218), ('ki', 0.19), ('mi', 0.166), ('gene', 0.165), ('coupling', 0.158), ('switch', 0.139), ('drosophila', 0.134), ('sharing', 0.131), ('regulatory', 0.129), ('sh', 0.128), ('network', 0.114), ('segments', 0.113), ('rjmcmc', 0.106), ('ah', 0.104), ('changepoint', 0.103), ('binomial', 0.093), ('dbns', 0.092), ('pupa', 0.091), ('segs', 0.091), ('dbn', 0.08), ('biology', 0.076), ('auprc', 0.076), ('banjo', 0.076), ('dmh', 0.076), ('larva', 0.076), ('morphogenic', 0.076), ('tsni', 0.076), ('tesla', 0.073), ('reconstruction', 0.072), ('hyperparameters', 0.072), ('segment', 0.068), ('embryo', 0.067), ('auroc', 0.067), ('genes', 0.066), ('amh', 0.061), ('timepoints', 0.061), ('posterior', 0.058), ('expression', 0.054), ('melanogaster', 0.053), ('prior', 0.051), ('bayesian', 0.05), ('cps', 0.049), ('morphogenesis', 0.049), ('transitions', 0.047), ('yi', 0.046), ('psrf', 0.045), ('bi', 0.045), ('parents', 0.045), ('soft', 0.044), ('ai', 0.044), ('series', 0.043), ('poisson', 0.042), ('synthetic', 0.041), ('ated', 0.04), ('subnetwork', 0.04), ('structures', 0.039), ('acceptance', 0.038), ('hard', 0.037), ('cerevisiae', 0.037), ('saccharomyces', 0.037), ('peaks', 0.036), ('node', 0.036), ('hyperparameter', 0.034), ('hyperprior', 0.034), ('changes', 0.034), ('scheme', 0.033), ('schemes', 0.033), ('precision', 0.033), ('grey', 0.033), ('recall', 0.032), ('reversible', 0.031), ('yeast', 0.031), ('proposal', 0.031), ('bre', 0.03), ('carbon', 0.03), ('nodes', 0.028), ('adult', 0.027), ('dirk', 0.027), ('bernardo', 0.027), ('galactose', 0.027), ('glucose', 0.027), ('inferred', 0.026), ('nk', 0.026), ('edges', 0.025), ('jump', 0.025), ('cycle', 0.025), ('poorer', 0.024), ('robinson', 0.024), ('topical', 0.024), ('structure', 0.024), ('proposing', 0.024), ('opposed', 0.024), ('priori', 0.024), ('conjugate', 0.023), ('vivo', 0.023), ('vague', 0.023), ('ej', 0.023), ('life', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="129-tfidf-1" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>2 0.10493518 <a title="129-tfidf-2" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>3 0.10465077 <a title="129-tfidf-3" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>4 0.10133035 <a title="129-tfidf-4" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>5 0.091262273 <a title="129-tfidf-5" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>Author: Nadia Payet, Sinisa Todorovic</p><p>Abstract: We combine random forest (RF) and conditional random ﬁeld (CRF) into a new computational framework, called random forest random ﬁeld (RF)2 . Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by MetropolisHastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a nonparametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2 . (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random ﬁeld of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</p><p>6 0.078545868 <a title="129-tfidf-6" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>7 0.076590285 <a title="129-tfidf-7" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>8 0.07183329 <a title="129-tfidf-8" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>9 0.069411971 <a title="129-tfidf-9" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>10 0.069362126 <a title="129-tfidf-10" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>11 0.059371207 <a title="129-tfidf-11" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>12 0.056654286 <a title="129-tfidf-12" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>13 0.056371033 <a title="129-tfidf-13" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>14 0.056314755 <a title="129-tfidf-14" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>15 0.052245017 <a title="129-tfidf-15" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>16 0.050836831 <a title="129-tfidf-16" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>17 0.050809678 <a title="129-tfidf-17" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>18 0.050587732 <a title="129-tfidf-18" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>19 0.048966054 <a title="129-tfidf-19" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>20 0.048778571 <a title="129-tfidf-20" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, 0.038), (2, -0.017), (3, 0.045), (4, -0.09), (5, -0.005), (6, 0.017), (7, 0.018), (8, -0.025), (9, -0.005), (10, -0.093), (11, -0.004), (12, 0.018), (13, 0.038), (14, 0.001), (15, -0.031), (16, -0.007), (17, 0.017), (18, 0.005), (19, 0.086), (20, -0.013), (21, 0.053), (22, 0.005), (23, 0.042), (24, 0.035), (25, 0.152), (26, 0.0), (27, -0.035), (28, -0.001), (29, 0.075), (30, -0.023), (31, 0.069), (32, -0.038), (33, -0.165), (34, 0.026), (35, -0.033), (36, 0.028), (37, 0.066), (38, 0.054), (39, 0.026), (40, -0.076), (41, 0.12), (42, 0.044), (43, 0.01), (44, -0.068), (45, -0.015), (46, 0.117), (47, 0.022), (48, 0.018), (49, -0.261)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93696117 <a title="129-lsi-1" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>2 0.53935575 <a title="129-lsi-2" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>3 0.51454353 <a title="129-lsi-3" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>4 0.51136452 <a title="129-lsi-4" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>Author: Stephen Bach, Mark Maloof</p><p>Abstract: To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods. 1</p><p>5 0.47250399 <a title="129-lsi-5" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>6 0.46656284 <a title="129-lsi-6" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>7 0.4436571 <a title="129-lsi-7" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>8 0.40860957 <a title="129-lsi-8" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>9 0.40130734 <a title="129-lsi-9" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>10 0.39542511 <a title="129-lsi-10" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>11 0.39204374 <a title="129-lsi-11" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>12 0.38608405 <a title="129-lsi-12" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>13 0.371728 <a title="129-lsi-13" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>14 0.36865261 <a title="129-lsi-14" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>15 0.36629677 <a title="129-lsi-15" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>16 0.36275473 <a title="129-lsi-16" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>17 0.35993585 <a title="129-lsi-17" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>18 0.35583904 <a title="129-lsi-18" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>19 0.34911832 <a title="129-lsi-19" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>20 0.34578383 <a title="129-lsi-20" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.045), (17, 0.028), (27, 0.069), (30, 0.048), (35, 0.045), (45, 0.152), (50, 0.069), (52, 0.044), (59, 0.011), (60, 0.017), (72, 0.322), (77, 0.043), (78, 0.011), (90, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74317861 <a title="129-lda-1" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>2 0.68298048 <a title="129-lda-2" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>Author: Meritxell Vinyals, Jes\'us Cerquides, Alessandro Farinelli, Juan A. Rodríguez-aguilar</p><p>Abstract: We study worst-case bounds on the quality of any ﬁxed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start providing a bound independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with speciﬁc structures such as bipartite graphs or grids. Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% optimal) on MRFs with large variable-disjoint cycles1 . 1</p><p>3 0.66881943 <a title="129-lda-3" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>Author: Ning Chen, Jun Zhu, Eric P. Xing</p><p>Abstract: Learning from multi-view data is important in many applications, such as image classiﬁcation and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulﬁlls a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efﬁcient inference and parameter estimation methods for the latent subspace model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classiﬁcation, annotation and retrieval.</p><p>4 0.54919559 <a title="129-lda-4" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>Author: Sivan Sabato, Nathan Srebro, Naftali Tishby</p><p>Abstract: We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin classiﬁcation with L2 regularization: We introduce the γ-adapted-dimension, which is a simple function of the spectrum of a distribution’s covariance matrix, and show distribution-speciﬁc upper and lower bounds on the sample complexity, both governed by the γ-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classiﬁcation. The bounds hold for a rich family of sub-Gaussian distributions. 1</p><p>5 0.53086025 <a title="129-lda-5" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>6 0.52210093 <a title="129-lda-6" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>7 0.51993442 <a title="129-lda-7" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>8 0.51726949 <a title="129-lda-8" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>9 0.51546079 <a title="129-lda-9" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>10 0.514018 <a title="129-lda-10" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>11 0.51306313 <a title="129-lda-11" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>12 0.51283872 <a title="129-lda-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.51145148 <a title="129-lda-13" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>14 0.51059419 <a title="129-lda-14" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>15 0.50901204 <a title="129-lda-15" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>16 0.50873828 <a title="129-lda-16" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>17 0.50786054 <a title="129-lda-17" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>18 0.50667202 <a title="129-lda-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.50644243 <a title="129-lda-19" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>20 0.50635141 <a title="129-lda-20" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
