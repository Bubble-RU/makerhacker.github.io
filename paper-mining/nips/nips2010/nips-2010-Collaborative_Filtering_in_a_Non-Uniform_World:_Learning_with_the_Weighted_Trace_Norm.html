<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-48" href="#">nips2010-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</h1>
<br/><p>Source: <a title="nips-2010-48-pdf" href="http://papers.nips.cc/paper/4102-collaborative-filtering-in-a-non-uniform-world-learning-with-the-weighted-trace-norm.pdf">pdf</a></p><p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>Reference: <a title="nips-2010-48-reference" href="../nips2010_reference/nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. [sent-3, score-0.545]
</p><p>2 We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset. [sent-4, score-0.194]
</p><p>3 1 Introduction Trace-norm regularization is a popular approach for matrix completion and collaborative ﬁltering, motivated both as a convex surrogate to the rank [7, 6] and in terms of a regularized inﬁnite factor model with connections to large-margin norm-regularized learning [17, 1, 15]. [sent-5, score-0.54]
</p><p>4 Current theoretical guarantees on using the trace-norm for matrix completion assume a uniform sampling distribution over entries of the matrix [18, 6, 5, 13]. [sent-6, score-0.48]
</p><p>5 In a collaborative ﬁltering setting, where rows of the matrix represent e. [sent-7, score-0.229]
</p><p>6 movies, this corresponds to assuming all users are equally likely to rate movies and all movies are equally likely to be rated. [sent-11, score-0.382]
</p><p>7 This of course cannot be further from the truth, as invariably some users are more active than others and some movies are rated by many people while others are rarely rated. [sent-12, score-0.282]
</p><p>8 Indeed, a non-uniform sampling distribution can lead to a signiﬁcant deterioration in prediction quality and an increase in the sample complexity. [sent-14, score-0.202]
</p><p>9 This is in sharp contrast to the uniform ˜ sampling case, in which O(n) samples are enough. [sent-18, score-0.177]
</p><p>10 It is important to note that if the rank could ˜ be minimized directly, which is in general not computationally tractable, O(n) samples would be enough to learn a low-rank model even under an arbitrary non-uniform distribution. [sent-19, score-0.322]
</p><p>11 Our analysis further suggests a weighted correction to the trace-norm regularizer, that takes into account the sampling distribution. [sent-20, score-0.238]
</p><p>12 Although appearing at ﬁrst as counter-intuitive, and indeed being the opposite of a previously suggested weighting [21], this weighting is well-motivated by our analytic analysis and we discuss how it corrects the problems that the unweighted trace-norm has with non-uniform sampling. [sent-21, score-0.313]
</p><p>13 The only other work we are aware of that studies matrix completion under non-uniform sampling is work on exact completion (i. [sent-23, score-0.328]
</p><p>14 when the matrix is assumed to be exactly low rank) under powerlaw sampling [12]. [sent-25, score-0.16]
</p><p>15 Other then being limited to one speciﬁc distribution, the requirement of the matrix being exactly low rank is central to this work, and the results cannot be directly applied in the presence of even small noise. [sent-26, score-0.263]
</p><p>16 1  2 Complexity Control in terms of Matrix Factorizations Consider the problem of predicting the entries of some unknown target matrix Y ∈ Rn×m based on a random subset S of observed entries YS . [sent-28, score-0.234]
</p><p>17 For example, n and m may represent the number of users and the number of movies, and Y may represent a matrix of partially observed rating values. [sent-29, score-0.204]
</p><p>18 Predicting elements of Y can be done by ﬁnding a matrix X minimizing the training error, here measured as a squared error, and some measure c(X) of complexity. [sent-30, score-0.202]
</p><p>19 A basic measure of complexity is the rank of X, corresponding to the minimal dimensionality k such that X = U ⊤ V for some U ∈ Rk×n and V ∈ Rk×m . [sent-34, score-0.224]
</p><p>20 Directly constraining the rank of X forms one of the most popular approaches to collaborative ﬁltering. [sent-35, score-0.334]
</p><p>21 However, the rank is non-convex and hard to minimize. [sent-36, score-0.183]
</p><p>22 X  tr  = min ′  X=U V  Scaling The rank, as a measure of complexity, does not scale with the size of the matrix. [sent-41, score-0.2]
</p><p>23 Viewing the rank as a complexity measure corresponding to the number of underlying factors, if data is explained by e. [sent-43, score-0.224]
</p><p>24 two factors, then no matter how many rows (“users”) and columns (“movies”) we consider, the data will still have rank two. [sent-45, score-0.265]
</p><p>25 To see this, note that the trace-norm is the ℓ1 norm of the spectrum, while the Frobenius norm is the ℓ2 norm of the spectrum, yielding: √ X F ≤ X tr ≤ X F rank(X) ≤ n X F . [sent-47, score-0.35]
</p><p>26 More speciﬁcally, we study the trace-norm through the complexity measure: 2  X tr , (5) nm which puts the trace-norm on a comparable scale to the rank. [sent-52, score-0.241]
</p><p>27 tc(X) =  The relationship between tc(X) and the rank is tight for “orthogonal” low-rank matrices, i. [sent-56, score-0.183]
</p><p>28 lowrank matrices X = U ⊤ V where the rows of U and also the rows of V are orthogonal and of equal 2 magnitudes. [sent-58, score-0.182]
</p><p>29 Y F = nm, we have that rows 2  √ √ in U have norm n/ k and rows in V have norm m/ k, yielding precisely tc(X) = rank(X). [sent-61, score-0.212]
</p><p>30 Such an orthogonal low-rank matrix can be obtained, e. [sent-62, score-0.15]
</p><p>31 If there is a low-rank matrix X ∗ achieving low average error relative to Y (e. [sent-69, score-0.163]
</p><p>32 if Y = X ∗ + noise), then by minimizing the training error subject to a rank constraint (a computationally intractable ˜ task), |S| = O(rank(X ∗ )(n + m)) samples are enough in order to guarantee learning a matrix X whose overall average error is close to that of X ∗ [16]. [sent-71, score-0.582]
</p><p>33 From (4) we therefore have: YS tr ≤ Cn · n = Cn and so tc(YS ) ≤ C. [sent-79, score-0.2]
</p><p>34 That is, we can “shatter” any sample of size |S| ≤ Cn with tc(X) = C: no matter what the underlying matrix Y is, we can always perfectly ﬁt the training data with a low trace-norm matrix X s. [sent-80, score-0.262]
</p><p>35 That is, when there is some, known or unknown, non-uniform distribution D over entries of the matrix Y (i. [sent-88, score-0.157]
</p><p>36 That is, we measure generalization performance in terms of the weighted sum-squared-error: X −Y  2 D  =  E(i,j)∼D (Xij − Yij )2 =  ij  D(i, j)(Xij − Yij )2 . [sent-95, score-0.166]
</p><p>37 (6)  We ﬁrst point out that when using the rank for complexity control, i. [sent-96, score-0.224]
</p><p>38 In particular, if there is some low-rank X ∗ such that X ∗ − Y D is small, then ˜ O(rank(X ∗ )(n + m)) samples are enough in order to learn (by minimizing training error subject to 2 2 a rank constraint) a matrix X with X − Y D almost as small as X ∗ − Y D [16]. [sent-100, score-0.535]
</p><p>39 To see this, consider an orthogonal rank-k square n×n matrix, and a sampling distribution which is uniform over an nA ×nA sub-matrix A, with nA = na . [sent-102, score-0.389]
</p><p>40 For any sample S, we have: 2 YS 2 YS F rank(YS ) |S|na |S| tr tc(YS ) = ≤ ≤ = 2−a , (7) 2 2 n n n2 n 3  where we again take the entries in Y to be of unit magnitude. [sent-111, score-0.339]
</p><p>41 In the second inequality above we use the fact that YS is zero outside of A, and so we can bound the rank of YS by the dimensionality nA = na of A. [sent-112, score-0.403]
</p><p>42 Setting a < 1, we see that we can shatter any sample of size1 kn2−a = ω(n) with a matrix X for ˜ ˜ which tc(X) < a < 1, with O(n) observations, restricting to even tc(X) < 1, we can neither learn Y , since we can shatter YS , nor memorize it. [sent-113, score-0.465]
</p><p>43 This is a factor of n1/3 greater than the sample size needed to learn a matrix with constant tc(X) in the uniform case. [sent-115, score-0.198]
</p><p>44 But does this mean that we cannot learn a ˜ rank-k matrix by minimizing the trace-norm using O(kn) samples when the sampling distribution is concentrated on a small submatrix? [sent-117, score-0.29]
</p><p>45 Since the samples are uniform on a small submatrix, we can just think of the submatrix A as our entire space. [sent-119, score-0.169]
</p><p>46 The target matrix still has low rank, even when restricted to A, and we are back in the uniform sampling scenario. [sent-120, score-0.206]
</p><p>47 X tr ≤ n k, is the right constraint in the uniform observation scenario. [sent-123, score-0.281]
</p><p>48 When samples are concentrated in nA , we actually need to restrict to a much √ ˜ smaller trace norm, X tr ≤ na k, which will allow learning with O(kna ) samples. [sent-124, score-0.481]
</p><p>49 We can, however, modify the example and construct a sampling distribution under which Ω(n4/3 ) samples are required in order to learn even an “orthogonal” low-rank matrix, no matter what con˜ straint is placed on the trace-norm. [sent-125, score-0.195]
</p><p>50 This is a signiﬁcantly large sample complexity than O(kn), which is what we would expect, and what is required for learning by constraining the rank directly. [sent-126, score-0.316]
</p><p>51 In order to ﬁt on B, we need to allow a trace√ ∗ norm of at least XB tr = n k, i. [sent-130, score-0.25]
</p><p>52 But as discussed above, 2 with such a generous constraint on the trace-norm, we will be able to shatter S ⊂ A whenever |S ∩ A| = |S|/2 ≤ kn2−a /4. [sent-133, score-0.158]
</p><p>53 1 measure the excess (test) error X − X ∗ D = X − Y D − Y − X ∗ D of the learned model, as well as the error contribution from A and from B, as a function of the constraint on tc(X), for the sampling distribution discussed above and a speciﬁc sample size. [sent-141, score-0.312]
</p><p>54 ˜ 2 The algorithm saw all (or most) entries of the matrix and does not need to predict any unobserved entries. [sent-146, score-0.184]
</p><p>55 2 0 10  15  20  tc (X)  tc(X)  B  1  B  Mean Squared Error  B  1  Mean Squared Error  Mean Squared Error  1. [sent-157, score-0.547]
</p><p>56 2  shift A 25  30  0 −2 10  pq  −1  0  10  1  10  10  Regularization parameter λ  Figure 1: Left: Mean squared error (MSE) of the learned model as a function of the constraint on tc(X) (left), tcpq (X) (middle). [sent-164, score-0.264]
</p><p>57 Right: The solid curves show the optimum of the mean squared error objective (9) (unweighted trace-norm), as a function of the regularization parameter λ. [sent-165, score-0.258]
</p><p>58 learning by minimizing min YS − XS X  2 F  +λ X  tr  . [sent-173, score-0.241]
</p><p>59 (8)  First observe that the characterization (3) allows us to decompose X tr = XA tr + XB tr , where w. [sent-174, score-0.6]
</p><p>60 Each one of these corresponds to a trace-norm regularized learning problem, under a uniform sampling distribution (in the corresponding submatrix) of a noisy low-rank ˜ ˜ “orthogonal” matrix, and can therefor be learned with O(knA ) and O(knB ) samples respectively. [sent-181, score-0.209]
</p><p>61 When the objective is expressed in terms of tc(·), as in (9), the regularization tradeoff is scaled differently in each part ˜ of the training objective. [sent-184, score-0.199]
</p><p>62 1, right panel, show the excess test error for the minimizer of the training objective (9), as a function of the regularization tradeoff parameter λ. [sent-189, score-0.312]
</p><p>63 Forcing the same λ on both parts of the training objective (9) yields a deterioration in the generalization performance. [sent-193, score-0.204]
</p><p>64 4 Weighted Trace Norm The decomposition (9) and the discussion in the previous section suggests weighting the trace-norm by the frequency of rows and columns. [sent-194, score-0.204]
</p><p>65 We propose using the following weighted version of the trace-norm as a regularizer: √ √ 1 2 2 X tr(p,q) = diag( p)Xdiag( q) tr = min q(j), Vj (10) p(i) Ui + X=U ′ V 2 j i 5  √ √ where diag( p) is a diagonal matrix with p(i) on its diagonal (similarly diag( q)). [sent-198, score-0.406]
</p><p>66 Furthermore, it is easy to verify that for an “orthogonal” rank-k matrix X we have tcpq (X) = k for any sampling distribution. [sent-201, score-0.26]
</p><p>67 Equipped with the weighted trace-norm as a regularizer, let us revisit the problematic sampling distribution studied in the previous Section. [sent-202, score-0.206]
</p><p>68 In order √ ﬁt the “orthogonal” rank-k X ∗ , we need a to weighted trace-norm of X ∗ tr(p,q) = tcpq (X) = k. [sent-203, score-0.226]
</p><p>69 How large a sample S ∩ A can we now √ shatter using such a weighted trace-norm? [sent-204, score-0.283]
</p><p>70 We can calculate: YS∩A  tr(p,q)  = YS∩A  tr  /(2nA ) ≤  |S ∩ A|nA /(2nA ) =  |S|/(8nA ). [sent-206, score-0.2]
</p><p>71 (11)  That is, we can shatter a sample of size up to |S| = 8knA < 8kn. [sent-207, score-0.157]
</p><p>72 It seems that now, with a ﬁxed constraint on the weighted trace-norm, we have enough capacity to ˜ both ﬁt X ∗ , and with O(kn) samples, avoid overﬁtting on A. [sent-209, score-0.211]
</p><p>73 1 (right panel), and comparing (9) with (12), we see that introducing the weighting corresponds to a relative change of nA /nB in the correspondence of nA the regularization tradeoff parameters used for A and for B. [sent-212, score-0.239]
</p><p>74 The solid blue (top) curve and the dashed red (bottom) curve thus represent the excess error on B and on A when the weighted trace norm is used, i. [sent-215, score-0.473]
</p><p>75 The dashed black (middle) curve is the overall excess error when using this training objective. [sent-218, score-0.226]
</p><p>76 As can be seen, the weighting aligns the excess errors on A and on B much better, and yields a lower overall error. [sent-219, score-0.179]
</p><p>77 It is also interesting to observe that the weighted trace-norm outperforms its unweighted counterpart for a wide range of regularization parameters λ ∈ [0. [sent-226, score-0.275]
</p><p>78 This may also suggest that in practice, particularly when working with large and imbalanced datasets, it may be easier to search for regularization parameters using weighted trace-norm. [sent-229, score-0.232]
</p><p>79 We can consider a smaller shift by using the partially-weighted trace-norm: X  tr(p,q,α)  = diag(pα/2 )Xdiag(q α/2 )  tr  =  min  X=U ⊤ V  1 ( 2  p(i)α Ui i  and the corresponding normalized complexity measure tcα (X) = X  2  q(j)α Vj  +  2  ). [sent-232, score-0.281]
</p><p>80 Other Weightings and Bayesian Perspective The weighted trace-norm motivated by the analysis here (with α = 1) implies that the frequent users (equivalently movies) get regularized much stronger than the rare users (equivalently movies). [sent-234, score-0.433]
</p><p>81 [21] speculated that with a uniform weighting (α = 0) frequent users are regularized too heavily compared to infrequent users, and so suggested regularizing frequent users (and movies) with a lower weight, corresponding to α = −1. [sent-237, score-0.529]
</p><p>82 6  The weighted regularization motivated here (with α = 1) is also quite unusual from Bayesian perspective. [sent-243, score-0.194]
</p><p>83 Let ni = j Sij and mj = Sij denote the number of observed ratings for user i and movie j respectively. [sent-252, score-0.195]
</p><p>84 Strangely, and likely originating as a “bug” in calculating the stochastic gradients by one of the participants, these steps match the stochastic training used by many practitioners on the Netﬂix dataset, without explicitly considering the weighted trace-norm [8, 19, 15]. [sent-264, score-0.168]
</p><p>85 6 Experimental results We tested the weighted trace-norm on the Netﬂix dataset, which is the largest publicly available collaborative ﬁltering dataset. [sent-265, score-0.219]
</p><p>86 The training set contains 100,480,507 ratings from 480,189 anonymous users on 17,770 movie titles. [sent-266, score-0.335]
</p><p>87 Due to the special selection scheme, ratings from users with few ratings are overrepresented in the qualiﬁcation set, relative to the training set. [sent-269, score-0.44]
</p><p>88 To be able to report results where the train and test sampling distributions are the same, we also created a “test set” by randomly selecting and removing 100,000 ratings from the training set. [sent-270, score-0.259]
</p><p>89 The dataset is very imbalanced: it includes users with over 10,000 ratings as well as users who rated fewer than 5 movies. [sent-273, score-0.414]
</p><p>90 For each value of α and k we selected the regularization tradeoff λ by minimizing the error on the 100,000 qualiﬁcation set examples set aside for validation. [sent-275, score-0.214]
</p><p>91 Recall that the sampling distribution of the “test set” matches that of the training data, while the qualiﬁcation set is sampled differently, explaining the large difference in generalization between the two. [sent-277, score-0.162]
</p><p>92 9092  For both k = 30 and k = 100, the weighted trace-norm (α = 1) signiﬁcantly outperformed the unweighted trace-norm (α = 0). [sent-336, score-0.207]
</p><p>93 For both k = 30 and k = 100, we also observed that for the weighted trace-norm (α = 1) good generalization is possible with a wide range of λ settings, while for the unweighted trace-norm (α = 0), the results were much more sensitive to the setting of λ. [sent-341, score-0.247]
</p><p>94 This conﬁrms our previous results on the synthetic experiment and strongly suggests that it may be far easier to search for regularization parameters using the weighted trace-norm. [sent-342, score-0.226]
</p><p>95 (14) i j X=U ′ V 2 Similarly to the rank, but unlike the trace-norm, generalization and learning guarantees based on the max-norm hold also under an arbitrary, non-uniform, sampling distribution. [sent-345, score-0.153]
</p><p>96 The differences between the max-norm and the weighted tracenorm are small, but it seems that using the weighted trace-norm is slightly but consistently better. [sent-352, score-0.287]
</p><p>97 7 Summary In this paper we showed both analytically and empirically that under non-uniform sampling, tracenorm regularization can lead to signiﬁcant performance deterioration and an increase in sample complexity. [sent-353, score-0.225]
</p><p>98 Our results on both synthetic and on the highly imbalanced Netﬂix datasets further demonstrate that the weighted trace-norm yields signiﬁcant improvements in prediction quality. [sent-355, score-0.164]
</p><p>99 A rank minimization heuristic with application to minimum order system approximation. [sent-410, score-0.183]
</p><p>100 Fixed point and Bregman iterative methods for matrix rank minimization. [sent-424, score-0.263]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tc', 0.547), ('ys', 0.331), ('tr', 0.2), ('na', 0.193), ('rank', 0.183), ('ix', 0.155), ('xb', 0.155), ('quali', 0.146), ('ratings', 0.137), ('movies', 0.129), ('weighted', 0.126), ('users', 0.124), ('shatter', 0.123), ('weighting', 0.116), ('net', 0.115), ('xa', 0.107), ('nb', 0.101), ('tcpq', 0.1), ('collaborative', 0.093), ('deterioration', 0.088), ('weimer', 0.088), ('completion', 0.084), ('unweighted', 0.081), ('sampling', 0.08), ('matrix', 0.08), ('entries', 0.077), ('xs', 0.073), ('ya', 0.072), ('submatrix', 0.072), ('orthogonal', 0.07), ('kn', 0.068), ('regularization', 0.068), ('yb', 0.065), ('excess', 0.063), ('tca', 0.06), ('tcb', 0.06), ('vj', 0.059), ('ui', 0.058), ('constraining', 0.058), ('rows', 0.056), ('tradeoff', 0.055), ('factorization', 0.054), ('cn', 0.054), ('samples', 0.051), ('mc', 0.051), ('error', 0.05), ('norm', 0.05), ('enough', 0.05), ('yij', 0.049), ('mse', 0.048), ('uniform', 0.046), ('srebro', 0.045), ('ltering', 0.043), ('curve', 0.043), ('training', 0.042), ('complexity', 0.041), ('returning', 0.041), ('minimizing', 0.041), ('kna', 0.04), ('memorize', 0.04), ('xdiag', 0.04), ('generalization', 0.04), ('shift', 0.04), ('squared', 0.039), ('imbalanced', 0.038), ('learn', 0.038), ('trace', 0.037), ('candes', 0.037), ('constraint', 0.035), ('qual', 0.035), ('tracenorm', 0.035), ('sample', 0.034), ('objective', 0.034), ('curves', 0.034), ('achieving', 0.033), ('regularizing', 0.033), ('solid', 0.033), ('guarantees', 0.033), ('regularized', 0.032), ('movie', 0.032), ('suggests', 0.032), ('diag', 0.031), ('roughly', 0.031), ('regularizer', 0.03), ('rk', 0.03), ('magnitude', 0.029), ('rated', 0.029), ('rennie', 0.029), ('dashed', 0.028), ('unit', 0.028), ('ruslan', 0.027), ('saw', 0.027), ('frobenius', 0.027), ('restricting', 0.027), ('frequent', 0.027), ('tting', 0.027), ('outside', 0.027), ('mj', 0.026), ('sij', 0.026), ('matter', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="48-tfidf-1" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>2 0.17361596 <a title="48-tfidf-2" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>3 0.12525171 <a title="48-tfidf-3" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>4 0.10697507 <a title="48-tfidf-4" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>5 0.10397886 <a title="48-tfidf-5" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>Author: Uri Shalit, Daphna Weinshall, Gal Chechik</p><p>Abstract: When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is hard to compute, and so is the projection operator that approximates it, we describe another second-order retraction that can be computed efﬁciently, with run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n, given rank-one gradients. We use this algorithm, LORETA, to learn a matrixform similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive- aggressive approach in a factorized model, and also improves over a full model trained over pre-selected features using the same memory requirements. LORETA also showed consistent improvement over standard methods in a large (1600 classes) multi-label image classiﬁcation task. 1</p><p>6 0.10212248 <a title="48-tfidf-6" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>7 0.090386979 <a title="48-tfidf-7" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>8 0.085589655 <a title="48-tfidf-8" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>9 0.081860892 <a title="48-tfidf-9" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>10 0.075716011 <a title="48-tfidf-10" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>11 0.07558687 <a title="48-tfidf-11" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>12 0.072439648 <a title="48-tfidf-12" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>13 0.071454518 <a title="48-tfidf-13" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>14 0.069226429 <a title="48-tfidf-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.06595359 <a title="48-tfidf-15" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>16 0.065135412 <a title="48-tfidf-16" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>17 0.064976566 <a title="48-tfidf-17" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>18 0.063970722 <a title="48-tfidf-18" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>19 0.063157514 <a title="48-tfidf-19" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>20 0.062882133 <a title="48-tfidf-20" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, 0.039), (2, 0.082), (3, 0.049), (4, 0.048), (5, -0.029), (6, 0.022), (7, -0.012), (8, -0.061), (9, -0.059), (10, 0.054), (11, -0.002), (12, 0.11), (13, 0.016), (14, 0.04), (15, 0.045), (16, 0.006), (17, -0.05), (18, 0.042), (19, 0.151), (20, 0.022), (21, 0.085), (22, -0.049), (23, -0.196), (24, 0.125), (25, -0.049), (26, -0.06), (27, -0.041), (28, -0.015), (29, -0.042), (30, -0.158), (31, -0.01), (32, 0.06), (33, 0.024), (34, -0.019), (35, 0.045), (36, 0.036), (37, -0.075), (38, 0.006), (39, -0.046), (40, 0.048), (41, -0.08), (42, 0.022), (43, 0.006), (44, -0.052), (45, 0.182), (46, -0.012), (47, -0.015), (48, -0.068), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94594383 <a title="48-lsi-1" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>2 0.78351706 <a title="48-lsi-2" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>Author: Prateek Jain, Raghu Meka, Inderjit S. Dhillon</p><p>Abstract: Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under afﬁne constraints (ARMP) and show that SVP recovers the minimum rank solution for afﬁne constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of lowrank matrix completion, for which the deﬁning afﬁne constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is signiﬁcantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem. 1</p><p>3 0.73169708 <a title="48-lsi-3" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>Author: Kaushik Mitra, Sameer Sheorey, Rama Chellappa</p><p>Abstract: Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semideﬁnite program (LRSDP) with the advantage that: 1) an efﬁcient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and 2) additional constraints such as orthonormality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm ﬁnds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the afﬁne SfM problem, non-rigid SfM and photometric stereo problems.</p><p>4 0.66259867 <a title="48-lsi-4" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>Author: Uri Shalit, Daphna Weinshall, Gal Chechik</p><p>Abstract: When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is hard to compute, and so is the projection operator that approximates it, we describe another second-order retraction that can be computed efﬁciently, with run time and memory complexity of O ((n + m)k) for a rank-k matrix of dimension m × n, given rank-one gradients. We use this algorithm, LORETA, to learn a matrixform similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive- aggressive approach in a factorized model, and also improves over a full model trained over pre-selected features using the same memory requirements. LORETA also showed consistent improvement over standard methods in a large (1600 classes) multi-label image classiﬁcation task. 1</p><p>5 0.65736467 <a title="48-lsi-5" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>6 0.59394002 <a title="48-lsi-6" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>7 0.55963802 <a title="48-lsi-7" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>8 0.51447898 <a title="48-lsi-8" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>9 0.48394045 <a title="48-lsi-9" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>10 0.47925735 <a title="48-lsi-10" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>11 0.44666809 <a title="48-lsi-11" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>12 0.42707977 <a title="48-lsi-12" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>13 0.42497924 <a title="48-lsi-13" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>14 0.42036361 <a title="48-lsi-14" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>15 0.38729486 <a title="48-lsi-15" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>16 0.38657358 <a title="48-lsi-16" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>17 0.38117135 <a title="48-lsi-17" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>18 0.38010913 <a title="48-lsi-18" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>19 0.37898198 <a title="48-lsi-19" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>20 0.37578672 <a title="48-lsi-20" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.06), (17, 0.013), (26, 0.022), (27, 0.065), (30, 0.049), (35, 0.028), (44, 0.252), (45, 0.195), (50, 0.064), (51, 0.027), (52, 0.039), (60, 0.02), (77, 0.056), (90, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78762323 <a title="48-lda-1" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>same-paper 2 0.78333366 <a title="48-lda-2" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>3 0.7816509 <a title="48-lda-3" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>Author: Nebojsa Jojic, Chris Meek, Jim C. Huang</p><p>Abstract: Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (nonloopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide signiďŹ cantly better ďŹ ts to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p><p>4 0.67986757 <a title="48-lda-4" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>5 0.67653972 <a title="48-lda-5" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>6 0.67464185 <a title="48-lda-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.67249954 <a title="48-lda-7" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>8 0.67200369 <a title="48-lda-8" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>9 0.67047125 <a title="48-lda-9" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>10 0.6696772 <a title="48-lda-10" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>11 0.66908509 <a title="48-lda-11" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>12 0.66907912 <a title="48-lda-12" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>13 0.66893691 <a title="48-lda-13" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>14 0.66823953 <a title="48-lda-14" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>15 0.66738605 <a title="48-lda-15" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>16 0.66726863 <a title="48-lda-16" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>17 0.66650087 <a title="48-lda-17" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>18 0.66557688 <a title="48-lda-18" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>19 0.66551524 <a title="48-lda-19" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>20 0.66546094 <a title="48-lda-20" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
