<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2010-Generalized roof duality and bisubmodular functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-102" href="#">nips2010-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2010-Generalized roof duality and bisubmodular functions</h1>
<br/><p>Source: <a title="nips-2010-102-pdf" href="http://papers.nips.cc/paper/4021-generalized-roof-duality-and-bisubmodular-functions.pdf">pdf</a></p><p>Author: Vladimir Kolmogorov</p><p>Abstract: ˆ Consider a convex relaxation f of a pseudo-boolean function f . We say that ˆ the relaxation is totally half-integral if f (x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj , xi = 1 − xj , and xi = γ where 1 γ ∈ {0, 1, 2 } is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f . We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization ˆ of totally half-integral relaxations f by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. 1</p><p>Reference: <a title="nips-2010-102-reference" href="../nips2010_reference/nips-2010-Generalized_roof_duality_and_bisubmodular_functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Generalized roof duality and bisubmodular functions  Vladimir Kolmogorov Department of Computer Science University College London, UK v. [sent-1, score-1.218]
</p><p>2 uk  Abstract ˆ Consider a convex relaxation f of a pseudo-boolean function f . [sent-5, score-0.167]
</p><p>3 We say that ˆ the relaxation is totally half-integral if f (x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj , xi = 1 − xj , and xi = γ where 1 γ ∈ {0, 1, 2 } is a constant. [sent-6, score-0.722]
</p><p>4 A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f . [sent-7, score-0.741]
</p><p>5 We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. [sent-8, score-0.552]
</p><p>6 First, we provide a complete characterization ˆ of totally half-integral relaxations f by establishing a one-to-one correspondence with bisubmodular functions. [sent-10, score-1.099]
</p><p>7 Second, we give a new characterization of bisubmodular functions. [sent-11, score-0.726]
</p><p>8 Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. [sent-12, score-0.913]
</p><p>9 In this paper we consider convex relaxations ˆ f : K → R of f which we call totally half-integral: ˆ Deﬁnition 1. [sent-14, score-0.392]
</p><p>10 (b) Function f : K → R is called totally half-integral if ˆ the form (x, f ˆ restrictions f : P → R are half-integral for all subsets P ⊆ K obtained from K by adding an arbitrary combination of constraints of the form xi = xj , xi = xj , and xi = γ for points x ∈ K. [sent-16, score-0.539]
</p><p>11 2 A well-known example of a totally half-integral relaxation is the roof duality relaxation for quadratic pseudo-boolean functions f (x) = i ci x i + (i,j) cij xi xj studied by Hammer, Hansen and Simeone [13]. [sent-18, score-1.197]
</p><p>12 It is known to possess the persistency property: for any half-integral minimizer ˆx x ∈ arg min f (ˆ) there exists minimizer x ∈ arg min f (x) such that xi = xi for all nodes i with ˆ ˆ integral component xi . [sent-19, score-0.535]
</p><p>13 The goal of this paper is to generalize the roof duality approach to arbitrary pseudo-boolean functions. [sent-22, score-0.514]
</p><p>14 1  We provide a complete characterization of totally half-integral relaxations. [sent-25, score-0.228]
</p><p>15 Namely, we prove in secˆ tion 2 that if f : K → R is totally half-integral then its restriction to K1/2 is a bisubmodular function, and conversely any bisubmodular function can be extended to a totally half-integral relaxation. [sent-26, score-1.767]
</p><p>16 Using this characterization, we then prove several results showing links with the roof duality relaxation (section 4). [sent-29, score-0.684]
</p><p>17 For some vision applications the roof duality approach [13] has shown a good performance [30, 32, 23, 24, 33, 1, 16, 17]. [sent-38, score-0.536]
</p><p>18 Therefore, studying generalizations of roof duality to arbitrary pseudo-boolean functions is an important task. [sent-40, score-0.582]
</p><p>19 Indeed, ˆ ˆ in practice, the relaxation f is obtained as the sum of relaxations fC constructed for each term independently. [sent-42, score-0.345]
</p><p>20 If c is sufﬁciently large, then applying the roof duality relaxation to these terms would yield constraints xi = xj and 1 x = xj present in the deﬁnition of total half-integrality. [sent-44, score-0.845]
</p><p>21 Constraints xi = γ ∈ {0, 1, 2 } can also be simulated via the roof duality, e. [sent-45, score-0.424]
</p><p>22 xi = xj , xi = xj for the same pair of nodes i, j implies xi = xj = 1 . [sent-47, score-0.44]
</p><p>23 2  Related work  Half-integrality There is a vast literature on using half-integral relaxations for various combinatorial optimization problems. [sent-49, score-0.217]
</p><p>24 In many cases these relaxations lead to 2-approximation algorithms. [sent-50, score-0.197]
</p><p>25 Hammer, Hansen and Simeone [13] established that these properties hold for the roof duality relaxation for quadratic pseudo-boolean functions. [sent-53, score-0.711]
</p><p>26 (The relaxation in [25] relied on converting function f to a multinomial representation; see section 4 for more details. [sent-55, score-0.148]
</p><p>27 Very recently, Iwata and Nagano [18] formulated a half-integral relaxation for the problem of minimizing submodular function f (x) under constraints of the form xi + xj ≥ 1. [sent-57, score-0.602]
</p><p>28 2  In computer vision, several researchers considered the following scheme: given a function f (x) = fC (x), convert terms fC (x) to quadratic pseudo-boolean functions by introducing auxiliary binary variables, and then apply the roof duality relaxation to the latter. [sent-62, score-0.784]
</p><p>29 To the best of our knowledge, all examples of totally half-integral relaxations proposed so far belong to the class of submodular relaxations, which is deﬁned in section 4. [sent-66, score-0.677]
</p><p>30 They form a subclass of more general bisubmodular relaxations. [sent-67, score-0.674]
</p><p>31 The notion of the Lov´ sz extension of a bisubmodular function introduced by Qi [29] a will be of particular importance for our work (see next section). [sent-71, score-0.757]
</p><p>32 It has been shown that some submodular minimization algorithms can be generalized to bisubmodular functions. [sent-72, score-0.997]
</p><p>33 A weakly polynomial combinatorial algorithm for minimizing bisubmodular functions was given by Fujishige and Iwata [12], and a strongly polynomial version was given by McCormick and Fujishige [26]. [sent-74, score-0.742]
</p><p>34 Recently, we introduced strongly and weakly tree-submodular functions [22] that generalize bisubmodular functions. [sent-75, score-0.704]
</p><p>35 If f : K → R is a totally half-integral relaxation then its restriction to K1/2 is bisubmodular. [sent-78, score-0.346]
</p><p>36 Conversely, if function f : K1/2 → R is bisubmodular then it has a unique totally halfˆ integral extension f : K → R. [sent-79, score-0.926]
</p><p>37 Under this change totally half-integral relaxations are transformed to totally integral relaxations: ˆ ˆ Deﬁnition 4. [sent-83, score-0.605]
</p><p>38 (a) h is called integral if it is a convex ˆ polyhedral function such that all extreme points of the epigraph {(x, z) | x ∈ L, z ≥ h(x)} have the 1/2 ˆ ˆ form (x, h(x)) where x ∈ L . [sent-85, score-0.175]
</p><p>39 (b) h is called totally integral if it is integral and for an arbitrary ordering of nodes the following functions of n − 1 variables (if n > 1) are totally integral: ˆ h (x1 , . [sent-86, score-0.617]
</p><p>40 , xn−1 , γ)  for any constant γ ∈ {−1, 0, 1}  The deﬁnition of a bisubmodular function is adapted as follows: function h : L1/2 → R is bisubmodular if inequality (1) holds for all x, y ∈ L1/2 where operations , are deﬁned by tables (2) 1 after replacements 0 → −1, 2 → 0, 1 → 1. [sent-104, score-1.37]
</p><p>41 To prove theorem 3, it sufﬁces to establish a link ˆ between totally integral relaxations h : L → R and bisubmodular functions h : L1/2 → R. [sent-105, score-1.155]
</p><p>42 To each signed ordering ω we associate labelings x0 , x1 , . [sent-112, score-0.221]
</p><p>43 Given a vector x ∈ R , select a signed ordering ω = (π, σ) as follows: (i) choose π so that values |xi |, i ∈ V are non-increasing, and rename nodes accordingly so that |x1 | ≥ . [sent-130, score-0.179]
</p><p>44 It is not difﬁcult to check that n  λi xi  x=  (4a)  i=1  where labelings xi are deﬁned in (3) (with respect to the selected signed ordering) and λi = |xi | − |xi+1 | for i = 1, . [sent-134, score-0.376]
</p><p>45 Function h is bisubmodular if and only if its Lov´ sz extension h is convex on a L. [sent-139, score-0.776]
</p><p>46 , n − 1 there holds Li = {x ∈ Lω | xi = σi−1 σi xi−1 }, and for i = n we have Ln = {x ∈ Lω | xn = 0}. [sent-180, score-0.159]
</p><p>47 Suppose function h : L → R with h(0) = 0 is totally integral. [sent-182, score-0.176]
</p><p>48 If h : L1/2 → R with h(0) = 0 is bisubmodular then its Lov´ sz extension h : L → R is a totally integral. [sent-187, score-0.933]
</p><p>49 It remains to show that functions h considered in deﬁnition 4 are V \{n} totally integral. [sent-199, score-0.206]
</p><p>50 , xn−1 , γ) , γ ∈ {−1, 0, 1}  It can be checked that these functions are bisubmodular, and their Lov´ sz extensions coincide with a ˆ respective functions h used in deﬁnition 4. [sent-218, score-0.163]
</p><p>51 3  A new characterization of bisubmodularity  In this section we give an alternative deﬁnition of bisubmodularity; it will be helpful later for describing a relationship to the roof duality. [sent-220, score-0.454]
</p><p>52 The node i for i ∈ V is called the “mate” of i; intuitively, variable ui corresponds to the complement of ui . [sent-222, score-0.392]
</p><p>53 Function f : X − → R is called bisubmodular if f (u  v) + f (u  v) ≤ f (u) + f (v)  ∀ u, v ∈ X −  (6)  where u v = u ∧ v, u v = REDUCE(u ∨ v) and REDUCE(w) is the labeling obtained from w by changing labels (wi , wi ) from (1, 1) to (0, 0) for all i ∈ V . [sent-233, score-0.73]
</p><p>54 For a labeling u ∈ X , deﬁne labeling u by (u )i = ui . [sent-236, score-0.263]
</p><p>55 Labels (ui , ui ) are transformed according to the rules (0, 1) → (0, 1)  (1, 0) → (1, 0)  (0, 0) → (1, 1)  (1, 1) → (0, 0)  (7)  Equivalently, this mapping can be written as (x, y) = (y, x). [sent-237, score-0.187]
</p><p>56 Next, we deﬁne sets X − = {u ∈ X | u ≤ u } = {u ∈ X | (ui , ui ) = (1, 1) ∀i ∈ V } X + = {u ∈ X | u ≥ u } = {u ∈ X | (ui , ui ) = (0, 0) ∀i ∈ V } X◦ X  = {u ∈ X | u = u } = {u ∈ X | (ui , ui ) ∈ {(0, 1), (1, 0)} = X− ∪ X+  ∀i ∈ V } = X − ∩ X +  Clearly, u ∈ X − if and only if u ∈ X + . [sent-239, score-0.561]
</p><p>57 4  Submodular relaxations and roof duality  Consider a submodular function g : X → R satisfying the following “symmetry” condition: ∀u ∈ X  g(u ) = g(u)  (10)  We call such function g a submodular relaxation of function f (x) = g(x, x). [sent-257, score-1.467]
</p><p>58 Clearly, it satisﬁes conditions of proposition 10, so g is also a bisubmodular relaxation of f . [sent-258, score-0.822]
</p><p>59 Review of roof duality Consider a quadratic pseudo-boolean function f : B → R: f (x) =  fi (xi ) + i∈V  fij (xi , xj )  (11)  (i,j)∈E  where (V, E) is an undirected graph and xi ∈ {0, 1} for i ∈ V are binary variables. [sent-261, score-0.873]
</p><p>60 Hammer, Hansen and Simeone [13] formulated several linear programming relaxations of this function and 3  Denote u =  1 0 1 0 0 0 0 0  and v =  0 1 0 0 0 0 1 0  where the top and bottom rows correspond to the labelings  of V and V \V respectively, with |V | = 4. [sent-262, score-0.332]
</p><p>61 One of these formulations was called a roof dual. [sent-266, score-0.361]
</p><p>62 An efﬁcient maxﬂowbased method for solving the roof duality relaxation was given by Hammer, Boros and Sun [5, 4]. [sent-267, score-0.662]
</p><p>63 We will rely on this algorithmic description of the roof duality approach [4]. [sent-268, score-0.514]
</p><p>64 Each variable xi is replaced with two binary variables ui and ui corresponding to xi and 1 − xi respectively. [sent-270, score-0.658]
</p><p>65 If u ∈ X is a minimizer of g then the roof duality relaxation has a minimizer x with ˆ xi = 1 (ui + ui ) [4]. [sent-273, score-1.018]
</p><p>66 ˆ 2 It is easy to check that g(u) = g(u ) for all u ∈ X , therefore g is a submodular relaxation. [sent-274, score-0.34]
</p><p>67 Also, f and g are equivalent when ui = ui for all i ∈ V , i. [sent-275, score-0.374]
</p><p>68 ∀x ∈ B  g(x, x) = f (x)  (13)  Invariance to variable ﬂipping Suppose that g is a (bi-)submodular relaxation of function f : B → R. [sent-277, score-0.148]
</p><p>69 Let i be a ﬁxed node in V , and consider function f (x) obtained from f (x) by a change of coordinates xi → xi and function g (u) obtained from g(u) by swapping variables ui and ui . [sent-278, score-0.554]
</p><p>70 It is easy to check that g is a (bi-)submodular relaxation of f . [sent-279, score-0.184]
</p><p>71 Furthermore, if f is a quadratic pseudoboolean function and g is its submodular relaxation constructed by the roof duality approach, then applying the roof duality approach to f yields function g . [sent-280, score-1.568]
</p><p>72 Conversion to roof duality Let us now consider a non-quadratic pseudo-boolean function f : B → R. [sent-282, score-0.514]
</p><p>73 Several papers [33, 1, 16] proposed the following scheme: (1) Convert f to a quadratic pseudo˜ ˜ boolean function f by introducing k auxiliary binary variables so that f (x) = minα∈{0,1}k f (x, α) ˜ by applying the roof for all labelings x ∈ B. [sent-283, score-0.568]
</p><p>74 (2) Construct submodular relaxation g (x, α, y, β) of f ˜ ˜ duality relaxation to f ; then ˜ g (x, α, y, β) = g (y, β, x, α) , g (x, α, x, α) = f (x, α) ˜ ˜ ˜ (3) Obtain function g by minα,β ∈{0,1}k g (x, α, y, β). [sent-284, score-0.771]
</p><p>75 ˜  minimizing  out  auxiliary  ∀x, y ∈ B, α, β ∈ {0, 1}k variables:  g(x, y)  =  One can check that g(x, y) = g(y, x), so g is a submodular relaxation4 . [sent-285, score-0.378]
</p><p>76 Existence of submodular relaxations It is easy to check that if f : B → R is submodular 1 then function g(x, y) = 2 [f (x) + f (y)] is a submodular relaxation of f . [sent-289, score-1.293]
</p><p>77 5 Thus, monomials of the form cΠi∈A xi where c ≤ 0 and A ⊆ V have submodular relaxations. [sent-290, score-0.444]
</p><p>78 Using the “ﬂipping” operation xi → xi , we conclude that submodular relaxations also exist for monomials of the form 4  It is well-known that minimizing variables out preserves submodularity. [sent-291, score-0.758]
</p><p>79 Indeed, suppose that h(x) = ˜ ˜ minα h(x, α) where h is a submodular function. [sent-292, score-0.324]
</p><p>80 Then h is also submodular since ˜ ˜ ˜ ˜ h(x) + h(y) = h(x, α) + h(y, β) ≥ h(x ∧ y, α ∧ β) + h(x ∨ y, α ∨ β) ≥ h(x ∧ y) + h(x ∨ y) 5 In fact, it dominates all other bisubmodular relaxations g : X − → R of f . [sent-293, score-1.195]
</p><p>81 ¯ g ¯  7  cΠi∈A xi Πi∈B xi where c ≤ 0 and A, B are disjoint subsets of U . [sent-296, score-0.162]
</p><p>82 This implies that any pseudo-boolean function f has a submodular relaxation. [sent-300, score-0.304]
</p><p>83 Note that this argument is due to Lu and Williams [25] who converted function f to a sum of monomials of the form cΠi∈A xi and cxk Πi∈A xi , c ≤ 0, k ∈ A. [sent-301, score-0.221]
</p><p>84 It is possible to show that the / relaxation proposed in [25] is equivalent to the submodular relaxation constructed by the scheme above (we omit the derivation). [sent-302, score-0.6]
</p><p>85 bisubmodular relaxations An important question is whether bisubmodular relaxations are more “powerful” compared to submodular ones. [sent-304, score-2.046]
</p><p>86 Let g be the submodular relaxation of a quadratic pseudo-boolean function f deﬁned by (12), and assume that the set E does not have parallel edges. [sent-307, score-0.501]
</p><p>87 Then g dominates any other bisubmodular relaxation g of f , i. [sent-308, score-0.842]
</p><p>88 we give an example of a function f of n = 4 variables which has a tight bisubmodular relaxation g (i. [sent-313, score-0.84]
</p><p>89 g has a minimizer in X ◦ ), but all submodular relaxations are not tight. [sent-315, score-0.545]
</p><p>90 Persistency Finally, we show that bisubmodular functions possess the autarky property, which implies persistency. [sent-316, score-0.775]
</p><p>91 Let f : K1/2 → R be a bisubmodular function and x ∈ K1/2 be its minimizer. [sent-318, score-0.674]
</p><p>92 Then z ∈ B and  [Persistency] Function f : B → R has a minimizer x∗ ∈ B such that x∗ = xi for nodes i ∈ V i with integral xi . [sent-322, score-0.306]
</p><p>93 It can be checked that zi = yi if xi = 2 and zi = xi if xi ∈ {0, 1}. [sent-324, score-0.283]
</p><p>94 5  Conclusions and future work  We showed that bisubmodular functions can be viewed as a natural generalization of the roof duality approach to higher-order cliques. [sent-329, score-1.218]
</p><p>95 An important ˆ open question is how to construct bisubmodular relaxations fC for individual terms. [sent-331, score-0.871]
</p><p>96 However, in our case we need to minimize a bisubmodular function which has a special structure: it is represented as a sum of low-order bisubmodular terms. [sent-337, score-1.348]
</p><p>97 We recently showed [21] that a sum of low-order submodular terms can be optimized more efﬁciently using maxﬂow-like techniques. [sent-338, score-0.304]
</p><p>98 We conjecture that similar techniques can be developed for bisubmodular functions as well. [sent-339, score-0.704]
</p><p>99 Submodularity on a tree: Unifying L -convex and bisubmodular functions. [sent-452, score-0.674]
</p><p>100 Strongly polynomial and fully combinatorial algorithms for bisubmodular function minimization. [sent-471, score-0.694]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bisubmodular', 0.674), ('roof', 0.343), ('submodular', 0.304), ('relaxations', 0.197), ('ui', 0.187), ('totally', 0.176), ('duality', 0.171), ('relaxation', 0.148), ('fij', 0.116), ('labelings', 0.115), ('hammer', 0.083), ('xi', 0.081), ('xn', 0.078), ('persistency', 0.077), ('fc', 0.07), ('signed', 0.063), ('sz', 0.063), ('lov', 0.061), ('rv', 0.06), ('fujishige', 0.059), ('satoru', 0.059), ('bisubmodularity', 0.059), ('monomials', 0.059), ('integral', 0.056), ('characterization', 0.052), ('xj', 0.051), ('quadratic', 0.049), ('boros', 0.047), ('autarky', 0.044), ('bouchet', 0.044), ('simeone', 0.044), ('nodes', 0.044), ('minimizer', 0.044), ('ordering', 0.043), ('checked', 0.04), ('hansen', 0.04), ('pseudoboolean', 0.039), ('generalizations', 0.038), ('labeling', 0.038), ('check', 0.036), ('uj', 0.035), ('inequalities', 0.033), ('iwata', 0.033), ('lempitsky', 0.031), ('functions', 0.03), ('epigraph', 0.029), ('kabadi', 0.029), ('mccormick', 0.029), ('rename', 0.029), ('rutcor', 0.029), ('woodford', 0.029), ('rother', 0.029), ('ipping', 0.028), ('qi', 0.027), ('ek', 0.027), ('polyhedral', 0.027), ('possess', 0.027), ('extreme', 0.026), ('induction', 0.026), ('rrr', 0.026), ('lu', 0.025), ('chandrasekaran', 0.024), ('carsten', 0.024), ('characterizations', 0.024), ('integrality', 0.024), ('santosh', 0.024), ('binary', 0.023), ('conversely', 0.023), ('inequality', 0.022), ('prove', 0.022), ('fi', 0.022), ('victor', 0.022), ('nemhauser', 0.022), ('ej', 0.022), ('vision', 0.022), ('restriction', 0.022), ('facets', 0.021), ('ando', 0.021), ('extension', 0.02), ('dominates', 0.02), ('combinatorial', 0.02), ('auxiliary', 0.02), ('programming', 0.02), ('suppose', 0.02), ('integer', 0.019), ('stefan', 0.019), ('submodularity', 0.019), ('cvpr', 0.019), ('convex', 0.019), ('kolmogorov', 0.019), ('minimization', 0.019), ('called', 0.018), ('fusion', 0.018), ('november', 0.018), ('claim', 0.018), ('variables', 0.018), ('discrete', 0.018), ('minimizing', 0.018), ('graph', 0.017), ('ali', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="102-tfidf-1" href="./nips-2010-Generalized_roof_duality_and_bisubmodular_functions.html">102 nips-2010-Generalized roof duality and bisubmodular functions</a></p>
<p>Author: Vladimir Kolmogorov</p><p>Abstract: ˆ Consider a convex relaxation f of a pseudo-boolean function f . We say that ˆ the relaxation is totally half-integral if f (x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj , xi = 1 − xj , and xi = γ where 1 γ ∈ {0, 1, 2 } is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f . We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization ˆ of totally half-integral relaxations f by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. 1</p><p>2 0.24130778 <a title="102-tfidf-2" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>3 0.22797899 <a title="102-tfidf-3" href="./nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">69 nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>Author: Peter Stobbe, Andreas Krause</p><p>Abstract: Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efÄ?Ĺš ciently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classiÄ?Ĺš cation-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude. 1</p><p>4 0.17492117 <a title="102-tfidf-4" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>Author: Kiyohito Nagano, Yoshinobu Kawahara, Satoru Iwata</p><p>Abstract: A number of objective functions in clustering problems can be described with submodular functions. In this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally, we evaluate the performance of the proposed algorithm through computational experiments. 1</p><p>5 0.076855741 <a title="102-tfidf-5" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>Author: Sashank J. Reddi, Sunita Sarawagi, Sundar Vishwanathan</p><p>Abstract: We propose a new LP relaxation for obtaining the MAP assignment of a binary MRF with pairwise potentials. Our relaxation is derived from reducing the MAP assignment problem to an instance of a recently proposed Bipartite Multi-cut problem where the LP relaxation is guaranteed to provide an O(log k) approximation where k is the number of vertices adjacent to non-submodular edges in the MRF. We then propose a combinatorial algorithm to efﬁciently solve the LP and also provide a lower bound by concurrently solving its dual to within an approximation. The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art message passing algorithm of [1] that tightens the local marginal polytope with third-order marginal constraints. 1</p><p>6 0.07572262 <a title="102-tfidf-6" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>7 0.074116528 <a title="102-tfidf-7" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>8 0.066384509 <a title="102-tfidf-8" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>9 0.048757717 <a title="102-tfidf-9" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>10 0.048485719 <a title="102-tfidf-10" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>11 0.045966379 <a title="102-tfidf-11" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>12 0.045018021 <a title="102-tfidf-12" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>13 0.042051375 <a title="102-tfidf-13" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>14 0.041751109 <a title="102-tfidf-14" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>15 0.039986171 <a title="102-tfidf-15" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>16 0.039961372 <a title="102-tfidf-16" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>17 0.038854521 <a title="102-tfidf-17" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>18 0.038123935 <a title="102-tfidf-18" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>19 0.038029224 <a title="102-tfidf-19" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>20 0.03708268 <a title="102-tfidf-20" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.036), (2, 0.096), (3, 0.043), (4, -0.004), (5, -0.113), (6, -0.06), (7, 0.097), (8, 0.299), (9, 0.034), (10, 0.121), (11, 0.089), (12, 0.072), (13, -0.053), (14, -0.16), (15, 0.105), (16, -0.0), (17, -0.036), (18, 0.032), (19, -0.018), (20, -0.1), (21, -0.078), (22, 0.014), (23, -0.001), (24, 0.066), (25, 0.091), (26, 0.047), (27, -0.024), (28, -0.04), (29, 0.05), (30, -0.02), (31, 0.04), (32, 0.038), (33, 0.042), (34, 0.058), (35, -0.042), (36, 0.06), (37, 0.047), (38, 0.023), (39, 0.035), (40, -0.022), (41, -0.032), (42, -0.019), (43, 0.001), (44, -0.029), (45, 0.013), (46, -0.007), (47, -0.013), (48, -0.033), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93138492 <a title="102-lsi-1" href="./nips-2010-Generalized_roof_duality_and_bisubmodular_functions.html">102 nips-2010-Generalized roof duality and bisubmodular functions</a></p>
<p>Author: Vladimir Kolmogorov</p><p>Abstract: ˆ Consider a convex relaxation f of a pseudo-boolean function f . We say that ˆ the relaxation is totally half-integral if f (x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj , xi = 1 − xj , and xi = γ where 1 γ ∈ {0, 1, 2 } is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f . We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization ˆ of totally half-integral relaxations f by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. 1</p><p>2 0.84969097 <a title="102-lsi-2" href="./nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">69 nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>Author: Peter Stobbe, Andreas Krause</p><p>Abstract: Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efÄ?Ĺš ciently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classiÄ?Ĺš cation-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude. 1</p><p>3 0.82898366 <a title="102-lsi-3" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>4 0.72852355 <a title="102-lsi-4" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>Author: Kiyohito Nagano, Yoshinobu Kawahara, Satoru Iwata</p><p>Abstract: A number of objective functions in clustering problems can be described with submodular functions. In this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally, we evaluate the performance of the proposed algorithm through computational experiments. 1</p><p>5 0.30811116 <a title="102-lsi-5" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>Author: Sashank J. Reddi, Sunita Sarawagi, Sundar Vishwanathan</p><p>Abstract: We propose a new LP relaxation for obtaining the MAP assignment of a binary MRF with pairwise potentials. Our relaxation is derived from reducing the MAP assignment problem to an instance of a recently proposed Bipartite Multi-cut problem where the LP relaxation is guaranteed to provide an O(log k) approximation where k is the number of vertices adjacent to non-submodular edges in the MRF. We then propose a combinatorial algorithm to efﬁciently solve the LP and also provide a lower bound by concurrently solving its dual to within an approximation. The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art message passing algorithm of [1] that tightens the local marginal polytope with third-order marginal constraints. 1</p><p>6 0.30290636 <a title="102-lsi-6" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>7 0.26914018 <a title="102-lsi-7" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>8 0.25976884 <a title="102-lsi-8" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>9 0.2540563 <a title="102-lsi-9" href="./nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">278 nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<p>10 0.24702314 <a title="102-lsi-10" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>11 0.24331842 <a title="102-lsi-11" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>12 0.24055091 <a title="102-lsi-12" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>13 0.23271173 <a title="102-lsi-13" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>14 0.23181151 <a title="102-lsi-14" href="./nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">274 nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<p>15 0.2238421 <a title="102-lsi-15" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>16 0.22321846 <a title="102-lsi-16" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>17 0.22262251 <a title="102-lsi-17" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>18 0.21831281 <a title="102-lsi-18" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>19 0.21638578 <a title="102-lsi-19" href="./nips-2010-Multitask_Learning_without_Label_Correspondences.html">177 nips-2010-Multitask Learning without Label Correspondences</a></p>
<p>20 0.21384554 <a title="102-lsi-20" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.033), (21, 0.01), (27, 0.026), (30, 0.068), (33, 0.359), (35, 0.028), (45, 0.135), (50, 0.053), (52, 0.036), (60, 0.062), (77, 0.036), (78, 0.024), (90, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70341671 <a title="102-lda-1" href="./nips-2010-Generalized_roof_duality_and_bisubmodular_functions.html">102 nips-2010-Generalized roof duality and bisubmodular functions</a></p>
<p>Author: Vladimir Kolmogorov</p><p>Abstract: ˆ Consider a convex relaxation f of a pseudo-boolean function f . We say that ˆ the relaxation is totally half-integral if f (x) is a polyhedral function with halfintegral extreme points x, and this property is preserved after adding an arbitrary combination of constraints of the form xi = xj , xi = 1 − xj , and xi = γ where 1 γ ∈ {0, 1, 2 } is a constant. A well-known example is the roof duality relaxation for quadratic pseudo-boolean functions f . We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions. Our contributions are as follows. First, we provide a complete characterization ˆ of totally half-integral relaxations f by establishing a one-to-one correspondence with bisubmodular functions. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality. 1</p><p>2 0.58045208 <a title="102-lda-2" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>Author: Diane Hu, Laurens Maaten, Youngmin Cho, Sorin Lerner, Lawrence K. Saul</p><p>Abstract: When software developers modify one or more ﬁles in a large code base, they must also identify and update other related ﬁles. Many ﬁle dependencies can be detected by mining the development history of the code base: in essence, groups of related ﬁles are revealed by the logs of previous workﬂows. From data of this form, we show how to detect dependent ﬁles by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we ﬁnd that LVMs improve the performance of related ﬁle prediction over current leading methods. 1</p><p>3 0.56079954 <a title="102-lda-3" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>4 0.47431675 <a title="102-lda-4" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>5 0.4496702 <a title="102-lda-5" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>6 0.44669294 <a title="102-lda-6" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>7 0.44352362 <a title="102-lda-7" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>8 0.44146279 <a title="102-lda-8" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>9 0.44045386 <a title="102-lda-9" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>10 0.43910643 <a title="102-lda-10" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>11 0.43905205 <a title="102-lda-11" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>12 0.43780485 <a title="102-lda-12" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>13 0.43762708 <a title="102-lda-13" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>14 0.43751621 <a title="102-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.43750519 <a title="102-lda-15" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>16 0.43652511 <a title="102-lda-16" href="./nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">69 nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>17 0.43459728 <a title="102-lda-17" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>18 0.43458223 <a title="102-lda-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.43436381 <a title="102-lda-19" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>20 0.43420693 <a title="102-lda-20" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
