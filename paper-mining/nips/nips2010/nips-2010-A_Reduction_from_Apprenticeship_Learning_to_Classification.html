<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2010-A Reduction from Apprenticeship Learning to Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-14" href="#">nips2010-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2010-A Reduction from Apprenticeship Learning to Classification</h1>
<br/><p>Source: <a title="nips-2010-14-pdf" href="http://papers.nips.cc/paper/4180-a-reduction-from-apprenticeship-learning-to-classification.pdf">pdf</a></p><p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>Reference: <a title="nips-2010-14-reference" href="../nips2010_reference/nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.546), ('appr', 0.499), ('sa', 0.328), ('expert', 0.255), ('prs', 0.183), ('nonst', 0.182), ('vt', 0.163), ('rmax', 0.158), ('dt', 0.142), ('imit', 0.126), ('dp', 0.123), ('reward', 0.115), ('traject', 0.1), ('bad', 0.099), ('reinforc', 0.093), ('hv', 0.071), ('lemm', 0.064), ('pr', 0.061), ('lemma', 0.053), ('st', 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="14-tfidf-1" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>2 0.47526044 <a title="14-tfidf-2" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>3 0.4615266 <a title="14-tfidf-3" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>4 0.42717931 <a title="14-tfidf-4" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>5 0.41377497 <a title="14-tfidf-5" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>6 0.3638058 <a title="14-tfidf-6" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>7 0.3323409 <a title="14-tfidf-7" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>8 0.318793 <a title="14-tfidf-8" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>9 0.30812874 <a title="14-tfidf-9" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>10 0.30723968 <a title="14-tfidf-10" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>11 0.22921348 <a title="14-tfidf-11" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>12 0.22914562 <a title="14-tfidf-12" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>13 0.19477822 <a title="14-tfidf-13" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>14 0.19217363 <a title="14-tfidf-14" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>15 0.17966028 <a title="14-tfidf-15" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>16 0.17752182 <a title="14-tfidf-16" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>17 0.15400492 <a title="14-tfidf-17" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>18 0.15142329 <a title="14-tfidf-18" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>19 0.14261623 <a title="14-tfidf-19" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>20 0.13261969 <a title="14-tfidf-20" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.244), (1, 0.574), (2, -0.029), (3, 0.078), (4, -0.027), (5, 0.0), (6, -0.032), (7, 0.036), (8, -0.008), (9, -0.069), (10, -0.032), (11, -0.042), (12, 0.025), (13, -0.007), (14, -0.014), (15, -0.017), (16, 0.052), (17, -0.005), (18, -0.035), (19, -0.019), (20, 0.029), (21, -0.02), (22, -0.002), (23, -0.009), (24, -0.012), (25, -0.028), (26, -0.005), (27, -0.002), (28, 0.047), (29, 0.053), (30, -0.031), (31, 0.027), (32, 0.013), (33, -0.016), (34, -0.009), (35, -0.022), (36, 0.005), (37, 0.03), (38, 0.036), (39, -0.042), (40, 0.06), (41, 0.01), (42, 0.027), (43, -0.047), (44, -0.072), (45, 0.036), (46, 0.01), (47, -0.059), (48, 0.0), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96963197 <a title="14-lsi-1" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>2 0.90259522 <a title="14-lsi-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>3 0.89559639 <a title="14-lsi-3" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>4 0.88738883 <a title="14-lsi-4" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>5 0.86509722 <a title="14-lsi-5" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>6 0.84867626 <a title="14-lsi-6" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>7 0.84741056 <a title="14-lsi-7" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>8 0.8389253 <a title="14-lsi-8" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>9 0.78302932 <a title="14-lsi-9" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>10 0.64695048 <a title="14-lsi-10" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>11 0.64630574 <a title="14-lsi-11" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>12 0.64445078 <a title="14-lsi-12" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>13 0.6298359 <a title="14-lsi-13" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>14 0.60597062 <a title="14-lsi-14" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>15 0.59589219 <a title="14-lsi-15" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>16 0.59279174 <a title="14-lsi-16" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>17 0.5743916 <a title="14-lsi-17" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>18 0.55976266 <a title="14-lsi-18" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>19 0.51072139 <a title="14-lsi-19" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>20 0.47169912 <a title="14-lsi-20" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.084), (23, 0.012), (30, 0.064), (32, 0.226), (34, 0.068), (45, 0.083), (61, 0.241), (68, 0.075), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83880651 <a title="14-lda-1" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>2 0.8027662 <a title="14-lda-2" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>3 0.76970178 <a title="14-lda-3" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>4 0.74155498 <a title="14-lda-4" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>5 0.74110925 <a title="14-lda-5" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>6 0.73929435 <a title="14-lda-6" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>7 0.73921251 <a title="14-lda-7" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>8 0.73888576 <a title="14-lda-8" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>9 0.73803055 <a title="14-lda-9" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>10 0.73605329 <a title="14-lda-10" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>11 0.73564225 <a title="14-lda-11" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>12 0.73457819 <a title="14-lda-12" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>13 0.73195583 <a title="14-lda-13" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>14 0.7303924 <a title="14-lda-14" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>15 0.73021275 <a title="14-lda-15" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>16 0.72901702 <a title="14-lda-16" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>17 0.7282427 <a title="14-lda-17" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>18 0.72705698 <a title="14-lda-18" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>19 0.72648776 <a title="14-lda-19" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>20 0.7255556 <a title="14-lda-20" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
