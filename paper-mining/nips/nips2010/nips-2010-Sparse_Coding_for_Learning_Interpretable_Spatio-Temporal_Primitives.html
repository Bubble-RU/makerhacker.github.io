<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-246" href="#">nips2010-246</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</h1>
<br/><p>Source: <a title="nips-2010-246-pdf" href="http://papers.nips.cc/paper/3930-sparse-coding-for-learning-interpretable-spatio-temporal-primitives.pdf">pdf</a></p><p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>Reference: <a title="nips-2010-246-reference" href="../nips2010_reference/nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. [sent-4, score-0.299]
</p><p>2 In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. [sent-5, score-0.389]
</p><p>3 We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. [sent-6, score-0.737]
</p><p>4 We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. [sent-7, score-0.995]
</p><p>5 1  Introduction  In recent years sparse coding has become a popular paradigm to learn dictionaries of natural images [10, 1, 4]. [sent-8, score-0.389]
</p><p>6 In these approaches, sparse coding was formulated as the sum of a data ﬁtting term, typically the Frobenius norm, and a regularization term that imposes sparsity. [sent-10, score-0.297]
</p><p>7 However, the sparsity induced by these norms is local; The estimated representations are sparse in that most of the activations are zero, but the sparsity has no structure, i. [sent-12, score-0.399]
</p><p>8 [9] extend the sparse coding formulation of natural images to impose structure by ﬁrst clustering the set of image patches and then learning a dictionary where members of the same cluster are encouraged to share sparsity patterns. [sent-16, score-0.547]
</p><p>9 Learning spatiotemporal representations of motion has been addressed in the neuroscience and motor control literature, in the context of motor synergies [13, 5, 14]. [sent-19, score-0.277]
</p><p>10 [3] where the goal was to recover primitives from time series of EMG signals recorded from a set of frog muscles. [sent-22, score-0.502]
</p><p>11 Using matching pursuit [11] and an 0 -type regularization as the underlying mechanism to learn primitives, [3] performed matrix factorization of the time series. [sent-23, score-0.377]
</p><p>12 The recovered factors represent the primitive dictionary and the primitive activations. [sent-24, score-0.44]
</p><p>13 In this paper we propose to extend the sparse coding framework to learn motion dictionaries. [sent-26, score-0.409]
</p><p>14 In particular, we cast the problem of learning spatio-temporal primitives as a tensor factorization prob1  lem and introduce tensor group norms over the primitives that encourage sparsity in order to learn the number of elements in the dictionary. [sent-27, score-1.297]
</p><p>15 The introduction of additional diagonal constraints in the activations, as well as smoothness constraints that are inherent to human motion, will allow us to learn interpretable representations of human motion from motion capture data. [sent-28, score-0.644]
</p><p>16 As demonstrated in our experiments, our approach outperforms state-of-the-art matching pursuit [3], as well as recently developed sparse coding algorithms [7]. [sent-29, score-0.562]
</p><p>17 2  Sparse coding for motion dictionary learning  In this section we ﬁrst review the framework of sparse coding, and then show how to extend this framework to learn interpretable dictionaries of human motion. [sent-30, score-0.771]
</p><p>18 1  Traditional sparse coding  Let Y = [y1 , · · · , yN ] be the matrix formed by concatenating the set of training examples drawn i. [sent-32, score-0.261]
</p><p>19 Sparse coding is usually formulated as a matrix factorization problem composed of a data ﬁtting term, typically the Frobenius norm, and a regularizer that encourages sparsity of the activations min ||Y − WH||2 + λψ(H) . [sent-36, score-0.481]
</p><p>20 [9] exploit group norm sparsity priors to learn dictionaries of natural images by ﬁrst clustering the training image patches, and then learning a dictionary where members of the same cluster are encouraged to share sparsity patterns. [sent-47, score-0.475]
</p><p>21 However, the structure imposed by these group norms is not sufﬁcient for learning interpretable motion primitives. [sent-50, score-0.293]
</p><p>22 We now show how in the case of motion, we can consider the activations and the primitives as tensors and impose group norm sparsity on the tensors. [sent-51, score-0.834]
</p><p>23 Moreover, we impose additional constraints such as continuity and differentiability that are inherent of human motion data, as well as diagonal constraints that ensure interpretability. [sent-52, score-0.389]
</p><p>24 2  Motion dictionary learning  Let Y ∈ D×L be a D dimensional signal of temporal length L. [sent-54, score-0.167]
</p><p>25 For simplicity in the discussion we assume that the primitives have the same length. [sent-57, score-0.473]
</p><p>26 This restriction can be easily removed by setting Q to be the maximum length of the primitives and padding the remaining elements to zero. [sent-58, score-0.522]
</p><p>27 QP ×L  are projections of the tensors to be represented  When learning dictionaries of human motion, there is additional structure and constraints that one would like the dictionary elements to satisfy. [sent-66, score-0.369]
</p><p>28 One important property of human motion is that it is smooth. [sent-67, score-0.158]
</p><p>29 We impose continuity and differentiability constraints by adding a regularization term that P encourages smooth curvature, i. [sent-68, score-0.167]
</p><p>30 One of the main difﬁculties with learning motion dictionaries is that the dictionary words might have very different temporal lengths. [sent-71, score-0.361]
</p><p>31 Note that this problem does not arise in traditional dictionary learning of natural images, since the size of the dictionary words is manually speciﬁed [4, 1, 9]. [sent-72, score-0.27]
</p><p>32 This makes the learning problem more complex since one would like to identify not only the number of elements in the dictionary, but also the size of each dictionary word. [sent-73, score-0.147]
</p><p>33 We address this problem by adding a regularization term that prefers dictionaries with small number of primitives, as well as primitives of short length. [sent-74, score-0.592]
</p><p>34 In particular, we extend the group norms over matrices to be group norms over tensors and deﬁne  r/q 1/r  Q  P  q/p  D  p   p,q,r (W) =   |Wi,j,k |   i=1  j=1       k=1  where Wi,j,k is the k-th dimension at the j-th time frame of the i-th primitive in W. [sent-75, score-0.384]
</p><p>35 We will also like to impose additional constraints on the activations H. [sent-76, score-0.265]
</p><p>36 , H and W can be recovered up to an invertible transformation WH = (WC−1 )(CH), we impose that the elements of the activation tensor should be in the unit interval, i. [sent-80, score-0.191]
</p><p>37 As in traditional sparse coding, we encourage the activations to be sparse. [sent-83, score-0.279]
</p><p>38 Finally, to impose interpretability of the results as spatio-temporal primitives, we impose that when a spatio-temporal primitive is active, it should be active across all its time-length with constant activation strength, i. [sent-85, score-0.299]
</p><p>39 Matching pursuit is able to recover the number of primitives when using refractory period, however the activations and the primitives are not correct. [sent-129, score-1.475]
</p><p>40 When we do not use the refractory period, the recovered primitives are very noisy. [sent-130, score-0.637]
</p><p>41 Sparse coding has a low reconstruction error, but neither the number of primitives, nor the primitives and the activations are correctly recovered. [sent-131, score-1.003]
</p><p>42 3  Experimental Evaluation  We compare our algorithm to two state-of-the-art approaches in the task of discovering interpretable primitives from motion capture data, namely, the sparse coding approach of [7] and matching pursuit [3]. [sent-132, score-1.233]
</p><p>43 We then show that our approach outperforms matching pursuit and sparse coding when learning dictionaries of walking and running motions. [sent-135, score-0.768]
</p><p>44 The threshold for MP with refractory period is set to 0. [sent-142, score-0.207]
</p><p>45 For each iteration in the optimization of H, an over-complete dictionary D is created by taking the primitives in W, and generating candidates by shifting each primitive in time. [sent-147, score-0.763]
</p><p>46 Note that the cardinality of the candidate dictionary is |D| = P (L + Q − 1) if W has P primitives and the data is composed of L frames. [sent-148, score-0.631]
</p><p>47 Once the dictionary is created, a set of primitives is iteratively selected (one at a time) by choosing at each iteration the primitive with the largest scalar product with respect to the residual signal that cannot be explained with the already selected primitives. [sent-149, score-0.743]
</p><p>48 Additionally, in the step of choosing elements in the dictionary, [3] introduced the refractory period, which means that when one element in the dictionary is chosen, all overlapping elements are removed from the dictionary. [sent-152, score-0.31]
</p><p>49 This is done to avoid multiple activations of primitives. [sent-153, score-0.167]
</p><p>50 In our experiments we compare our approach to matching pursuit with and without refractory period. [sent-154, score-0.419]
</p><p>51 Following [7], we solve this optimization problem alternating between solving with respect to the primitives ¯ ¯ W and the activations H. [sent-157, score-0.64]
</p><p>52 1  Estimating the number of primitives  In the ﬁrst experiment we demonstrate the ability of our approach to infer the number of primitives as well as the length of the existing primitives. [sent-159, score-0.972]
</p><p>53 For this purpose we created a simple dataset which is composed of a single sequence of multiple walking cycles performed by the same subject from the CMU mocap dataset 1 . [sent-160, score-0.247]
</p><p>54 In this case it is easy to see that since the motion is periodic, the signal could be represented by a single 2D primitive whose length is equal to the length of the period. [sent-171, score-0.318]
</p><p>55 To perform the experiments we initialize our approach and the baselines with a sum of random smooth functions (sinusoids) whose frequencies are different from the principal frequency of the periodic training data, and set the number of primitives to P = 2. [sent-172, score-0.595]
</p><p>56 One primitive is set to have approximately the same length as a cycle of the periodic motion and the other primitive is set to be 50% larger. [sent-173, score-0.465]
</p><p>57 Note that a rough estimate of the length of the primitives could be easily obtained by analyzing the principal frequencies of the signal. [sent-174, score-0.568]
</p><p>58 The ﬁrst two columns depict the two dimensional primitives recovered (W1 and W2). [sent-177, score-0.497]
</p><p>59 Note that we expect these primitives to be similar to the original signal, i. [sent-180, score-0.473]
</p><p>60 The third column depicts the activations vec(H) ∈ (Q1 +Q2 )×L recovered. [sent-185, score-0.229]
</p><p>61 We expect the successful activations to be diagonal, and to appear only once every cycle. [sent-186, score-0.167]
</p><p>62 Note that our approach is able to recover the number of primitives as well as the primitive themselves and the correct activations. [sent-187, score-0.648]
</p><p>63 Matching pursuit without refractory period (ﬁrst row) is not able to recover the primitives, their number, or the activations. [sent-188, score-0.429]
</p><p>64 Matching pursuit with refractory period (second row) is able to recover the number of primitives, however the activations are underestimated and the primitives are not very accurate. [sent-190, score-1.069]
</p><p>65 Sparse coding has a low reconstruction error, but neither the primitives, their number, nor the activations are correctly recovered. [sent-191, score-0.53]
</p><p>66 This conﬁrms the inability of traditional sparse coding to recover interpretable primitives, and the importance of having interpretability constraints such as the refractory period of matching pursuit and our diagonal constraints. [sent-192, score-1.004]
</p><p>67 As expected one primitive is not enough for accurate reconstruction. [sent-199, score-0.146]
</p><p>68 The primitives are either initialize randomly or to a smooth set of sinusoids of random frequencies. [sent-201, score-0.536]
</p><p>69 the learned W and the estimated activations Htest 1 epca = ||Vtest − vec(W)vec(Htest )||F D as well as the error in the original 59D space which can be computed by projecting back into the original space using the singular vectors. [sent-202, score-0.236]
</p><p>70 Note that W is learned at training, and the activations Htest are estimated at inference time. [sent-203, score-0.167]
</p><p>71 The primitives are initialized to a sum of sinusoids of random frequencies. [sent-210, score-0.513]
</p><p>72 We created a walking dataset composed of motions performed by the same subject. [sent-211, score-0.239]
</p><p>73 We also performed reconstruction experiments for running motions and used motions {17, 18, 20, 21, 22, 23, 24, 25} from subject 35. [sent-213, score-0.375]
</p><p>74 3 depicts reconstruction error in PCA space and in the original space as a function of the noise variance. [sent-216, score-0.323]
</p><p>75 4 depicts reconstruction error as a function of the dimensionality of the PCA space. [sent-218, score-0.323]
</p><p>76 Our approach outperforms matching pursuit with and without refractory period in all scenarios. [sent-219, score-0.508]
</p><p>77 Note that out method outperforms sparse coding when the output is noisy. [sent-220, score-0.283]
</p><p>78 This is due to the fact that, given a big enough dictionary, sparse coding overﬁts and can perfectly ﬁt the noise. [sent-221, score-0.261]
</p><p>79 We also performed reconstruction experiments for running motions performed by different subjects. [sent-222, score-0.287]
</p><p>80 In particular we use motions {03, 04, 05, 06} of subject 9 and motions {21, 23, 24, 25} of subject 35. [sent-223, score-0.208]
</p><p>81 5 depicts reconstruction error for our approach when using different numbers of primitives. [sent-225, score-0.323]
</p><p>82 As expected one primitive is not enough for accurate reconstruction. [sent-226, score-0.146]
</p><p>83 When using two primitives our approach performs comparable to sparse coding and clearly outperforms the other baselines. [sent-227, score-0.756]
</p><p>84 55  40  50  35  45  30  230  25  error  error  40  " error with missing data  35  20  15  30  " test error  25  10  0  2  4  6  8  210  200  190  180  " training error  20  15  Reconstruction error  220  5  10  12  0  170  1  2  3  − log ! [sent-234, score-0.461]
</p><p>85 P  4  5  7  6  5  4  3  2  1  log  Figure 7: Inﬂuence of η and P on the single subject walking dataset as well as using soft constraints instead of hard constraints on the activations. [sent-235, score-0.245]
</p><p>86 As expected the reconstruction error of the training data decreases when there is less regularization. [sent-237, score-0.278]
</p><p>87 For missing data, having good primitives is important, and thus regularization is necessary. [sent-239, score-0.539]
</p><p>88 Our approach is not sensitive to the value of P ; one primitive is enough for accurate reconstruction in this dataset. [sent-242, score-0.358]
</p><p>89 This is due to the fact that sparse coding does not have structure, while the structure imposed by our equality constraints, i. [sent-249, score-0.261]
</p><p>90 Note that as our approach, sparse coding is not sensitive to initialization. [sent-255, score-0.281]
</p><p>91 Towards this end we use the single subject walking dataset, and compute reconstruction error for the training and test data with and without missing data as a function of η. [sent-258, score-0.455]
</p><p>92 As expected the reconstruction error of the training data decreases when there is less regularization. [sent-261, score-0.278]
</p><p>93 When dealing with missing data, having good primitives becomes more important. [sent-263, score-0.52]
</p><p>94 We use the single subject walking dataset and report errors averaged over 10 partitions of the data. [sent-269, score-0.147]
</p><p>95 7 (middle) our approach is very insensitive to P ; in this example a single primitive is enough for accurate reconstruction. [sent-271, score-0.169]
</p><p>96 We ﬁnally investigate the inﬂuence of replacing the hard constraints on the activations by soft constraints |Hi,j,k − Hi,j+1,k+1 | ≤ α. [sent-272, score-0.265]
</p><p>97 4  Conclusion  We have proposed a sparse coding approach to learn interpretable spatio-temporal primitives of human motion. [sent-276, score-0.9]
</p><p>98 We have formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations, as well as smoothness constraints that are inherent to human motion. [sent-277, score-0.534]
</p><p>99 Our approach has proven superior to recently developed matching pursuit and sparse coding algorithms in the task of learning interpretable spatiotemporal primitives of human motion from motion capture data. [sent-278, score-1.412]
</p><p>100 Image denoising via sparse and redundant representations over learned dictionaries. [sent-299, score-0.148]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mp', 0.476), ('primitives', 0.473), ('rp', 0.306), ('sc', 0.243), ('pursuit', 0.193), ('reconstruction', 0.192), ('coding', 0.171), ('activations', 0.167), ('primitive', 0.146), ('refractory', 0.14), ('vec', 0.125), ('dictionary', 0.124), ('motion', 0.12), ('walking', 0.106), ('interpretable', 0.1), ('dictionaries', 0.1), ('sparse', 0.09), ('wh', 0.087), ('matching', 0.086), ('tensor', 0.076), ('ep', 0.073), ('error', 0.069), ('period', 0.067), ('motions', 0.063), ('depicts', 0.062), ('htest', 0.061), ('vtest', 0.061), ('dimension', 0.057), ('walk', 0.056), ('synergies', 0.049), ('constraints', 0.049), ('impose', 0.049), ('missing', 0.047), ('mairal', 0.043), ('subject', 0.041), ('sparsity', 0.04), ('sinusoids', 0.04), ('ca', 0.04), ('group', 0.038), ('human', 0.038), ('interpretability', 0.036), ('inherent', 0.036), ('factorization', 0.035), ('norms', 0.035), ('tti', 0.035), ('tensors', 0.035), ('members', 0.034), ('composed', 0.034), ('run', 0.033), ('norm', 0.032), ('denoising', 0.031), ('motor', 0.03), ('diavella', 0.03), ('mocap', 0.03), ('taehwan', 0.03), ('principal', 0.029), ('recover', 0.029), ('learn', 0.028), ('frobenius', 0.028), ('periodic', 0.027), ('variance', 0.027), ('representations', 0.027), ('alternate', 0.027), ('differentiability', 0.027), ('length', 0.026), ('uence', 0.026), ('chicago', 0.024), ('baselines', 0.024), ('recovered', 0.024), ('pca', 0.023), ('insensitive', 0.023), ('encouraged', 0.023), ('elements', 0.023), ('smooth', 0.023), ('outperforms', 0.022), ('gregory', 0.022), ('traditional', 0.022), ('diagonal', 0.021), ('urtasun', 0.021), ('spatiotemporal', 0.021), ('leftmost', 0.021), ('rough', 0.021), ('cmu', 0.021), ('non', 0.021), ('initialization', 0.02), ('sapiro', 0.02), ('created', 0.02), ('sensitive', 0.02), ('frequencies', 0.019), ('activation', 0.019), ('regularization', 0.019), ('smoothness', 0.018), ('formulated', 0.017), ('regularizer', 0.017), ('train', 0.017), ('decreases', 0.017), ('horizontal', 0.017), ('temporal', 0.017), ('image', 0.016), ('performed', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="246-tfidf-1" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>2 0.15388536 <a title="246-tfidf-2" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>3 0.13483585 <a title="246-tfidf-3" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>4 0.10988358 <a title="246-tfidf-4" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>Author: Koray Kavukcuoglu, Pierre Sermanet, Y-lan Boureau, Karol Gregor, Michael Mathieu, Yann L. Cun</p><p>Abstract: We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting ﬁlters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efﬁciency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efﬁcient feed-forward encoder that predicts quasisparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse ﬁlters, including center-surround ﬁlters, corner detectors, cross detectors, and oriented grating detectors. We show that using these ﬁlters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks. 1</p><p>5 0.10512754 <a title="246-tfidf-5" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>Author: Ariel Kleiner, Ali Rahimi, Michael I. Jordan</p><p>Abstract: We present a novel algorithm, Random Conic Pursuit, that solves semideﬁnite programs (SDPs) via repeated optimization over randomly selected two-dimensional subcones of the PSD cone. This scheme is simple, easily implemented, applicable to very general SDPs, scalable, and theoretically interesting. Its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained. This property renders Random Conic Pursuit of particular interest for machine learning applications, in which the relevant SDPs are generally based upon random data and so exact minima are often not a priority. Indeed, we present empirical results to this effect for various SDPs encountered in machine learning; these experiments demonstrate the potential practical usefulness of Random Conic Pursuit. We also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm. 1</p><p>6 0.10488939 <a title="246-tfidf-6" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>7 0.10196188 <a title="246-tfidf-7" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>8 0.10066294 <a title="246-tfidf-8" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>9 0.10041041 <a title="246-tfidf-9" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>10 0.10023946 <a title="246-tfidf-10" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>11 0.085982233 <a title="246-tfidf-11" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>12 0.081640631 <a title="246-tfidf-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.076042011 <a title="246-tfidf-13" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>14 0.072064213 <a title="246-tfidf-14" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>15 0.067945316 <a title="246-tfidf-15" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>16 0.066308118 <a title="246-tfidf-16" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>17 0.065530419 <a title="246-tfidf-17" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>18 0.0654466 <a title="246-tfidf-18" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>19 0.060203858 <a title="246-tfidf-19" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>20 0.05880205 <a title="246-tfidf-20" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, 0.037), (2, -0.059), (3, 0.058), (4, 0.078), (5, -0.154), (6, 0.004), (7, 0.143), (8, -0.089), (9, -0.025), (10, 0.11), (11, -0.059), (12, 0.034), (13, -0.035), (14, -0.05), (15, -0.011), (16, -0.099), (17, 0.023), (18, -0.03), (19, 0.009), (20, 0.028), (21, -0.077), (22, -0.057), (23, -0.107), (24, -0.03), (25, -0.076), (26, -0.057), (27, -0.083), (28, -0.012), (29, 0.09), (30, 0.105), (31, 0.124), (32, -0.173), (33, 0.061), (34, -0.023), (35, 0.013), (36, -0.118), (37, -0.025), (38, -0.104), (39, -0.024), (40, -0.043), (41, 0.04), (42, 0.079), (43, 0.083), (44, -0.046), (45, 0.072), (46, 0.08), (47, 0.008), (48, 0.093), (49, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96798313 <a title="246-lsi-1" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>2 0.67488754 <a title="246-lsi-2" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>Author: J. Z. Kolter, Siddharth Batra, Andrew Y. Ng</p><p>Abstract: Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having devicelevel energy information can cause users to conserve signiﬁcant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms speciﬁcally to maximize disaggregation performance. We show that this signiﬁcantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage. 1</p><p>3 0.63406801 <a title="246-lsi-3" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>4 0.60945582 <a title="246-lsi-4" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>5 0.60795635 <a title="246-lsi-5" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>6 0.54711372 <a title="246-lsi-6" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>7 0.51452452 <a title="246-lsi-7" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>8 0.51019078 <a title="246-lsi-8" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>9 0.48207623 <a title="246-lsi-9" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>10 0.46357039 <a title="246-lsi-10" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>11 0.43647859 <a title="246-lsi-11" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>12 0.43522951 <a title="246-lsi-12" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>13 0.38976109 <a title="246-lsi-13" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>14 0.37091586 <a title="246-lsi-14" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>15 0.35502836 <a title="246-lsi-15" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>16 0.34996006 <a title="246-lsi-16" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>17 0.3402648 <a title="246-lsi-17" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>18 0.31609347 <a title="246-lsi-18" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>19 0.30820167 <a title="246-lsi-19" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>20 0.30671659 <a title="246-lsi-20" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.223), (13, 0.107), (17, 0.045), (27, 0.074), (30, 0.065), (35, 0.063), (45, 0.183), (50, 0.034), (52, 0.052), (60, 0.013), (77, 0.021), (78, 0.018), (90, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90244192 <a title="246-lda-1" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>same-paper 2 0.82790339 <a title="246-lda-2" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>3 0.70640343 <a title="246-lda-3" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>4 0.70362574 <a title="246-lda-4" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>Author: Jean Morales, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefﬁcients. This family subsumes the ℓ1 norm and is ﬂexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the beneﬁt of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.</p><p>5 0.70325446 <a title="246-lda-5" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>6 0.69570953 <a title="246-lda-6" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>7 0.69361746 <a title="246-lda-7" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>8 0.69358456 <a title="246-lda-8" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>9 0.69311118 <a title="246-lda-9" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>10 0.68974239 <a title="246-lda-10" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>11 0.68822134 <a title="246-lda-11" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>12 0.68733728 <a title="246-lda-12" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>13 0.68732679 <a title="246-lda-13" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>14 0.68688339 <a title="246-lda-14" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>15 0.68625325 <a title="246-lda-15" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>16 0.68439883 <a title="246-lda-16" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>17 0.68354243 <a title="246-lda-17" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>18 0.68340045 <a title="246-lda-18" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>19 0.6833452 <a title="246-lda-19" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>20 0.68240476 <a title="246-lda-20" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
