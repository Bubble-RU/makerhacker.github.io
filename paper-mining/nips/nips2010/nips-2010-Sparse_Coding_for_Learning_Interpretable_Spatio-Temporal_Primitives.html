<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-246" href="#">nips2010-246</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</h1>
<br/><p>Source: <a title="nips-2010-246-pdf" href="http://papers.nips.cc/paper/3930-sparse-coding-for-learning-interpretable-spatio-temporal-primitives.pdf">pdf</a></p><p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>Reference: <a title="nips-2010-246-reference" href="../nips2010_reference/nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('primit', 0.569), ('mp', 0.489), ('rp', 0.314), ('sc', 0.249), ('pursuit', 0.208), ('dict', 0.179), ('reconstruct', 0.174), ('refract', 0.144), ('vec', 0.123), ('walk', 0.112), ('spars', 0.108), ('cod', 0.098), ('tens', 0.098), ('wh', 0.09), ('period', 0.076), ('ep', 0.068), ('htest', 0.063), ('vtest', 0.063), ('mot', 0.053), ('depict', 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="246-tfidf-1" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>2 0.16653067 <a title="246-tfidf-2" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>3 0.1343787 <a title="246-tfidf-3" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>Author: Ariel Kleiner, Ali Rahimi, Michael I. Jordan</p><p>Abstract: We present a novel algorithm, Random Conic Pursuit, that solves semideﬁnite programs (SDPs) via repeated optimization over randomly selected two-dimensional subcones of the PSD cone. This scheme is simple, easily implemented, applicable to very general SDPs, scalable, and theoretically interesting. Its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained. This property renders Random Conic Pursuit of particular interest for machine learning applications, in which the relevant SDPs are generally based upon random data and so exact minima are often not a priority. Indeed, we present empirical results to this effect for various SDPs encountered in machine learning; these experiments demonstrate the potential practical usefulness of Random Conic Pursuit. We also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm. 1</p><p>4 0.12432711 <a title="246-tfidf-4" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>Author: Koray Kavukcuoglu, Pierre Sermanet, Y-lan Boureau, Karol Gregor, Michael Mathieu, Yann L. Cun</p><p>Abstract: We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting ﬁlters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efﬁciency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efﬁcient feed-forward encoder that predicts quasisparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse ﬁlters, including center-surround ﬁlters, corner detectors, cross detectors, and oriented grating detectors. We show that using these ﬁlters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks. 1</p><p>5 0.11375941 <a title="246-tfidf-5" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>6 0.11101945 <a title="246-tfidf-6" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>7 0.10580084 <a title="246-tfidf-7" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>8 0.097118698 <a title="246-tfidf-8" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>9 0.090350442 <a title="246-tfidf-9" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>10 0.084431484 <a title="246-tfidf-10" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>11 0.081839606 <a title="246-tfidf-11" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>12 0.072783791 <a title="246-tfidf-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.071413957 <a title="246-tfidf-13" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>14 0.071016081 <a title="246-tfidf-14" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>15 0.070652135 <a title="246-tfidf-15" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>16 0.067840077 <a title="246-tfidf-16" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>17 0.066732116 <a title="246-tfidf-17" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>18 0.060001552 <a title="246-tfidf-18" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>19 0.05453483 <a title="246-tfidf-19" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>20 0.046781264 <a title="246-tfidf-20" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, -0.033), (2, -0.01), (3, 0.006), (4, 0.049), (5, 0.144), (6, 0.094), (7, 0.045), (8, -0.057), (9, -0.04), (10, 0.113), (11, 0.015), (12, -0.038), (13, -0.072), (14, -0.017), (15, -0.035), (16, -0.067), (17, 0.1), (18, -0.003), (19, 0.065), (20, -0.072), (21, -0.1), (22, -0.035), (23, 0.052), (24, 0.027), (25, -0.017), (26, 0.059), (27, 0.078), (28, -0.053), (29, -0.009), (30, 0.014), (31, -0.12), (32, 0.06), (33, -0.137), (34, 0.022), (35, 0.008), (36, -0.006), (37, -0.201), (38, -0.034), (39, -0.147), (40, 0.058), (41, -0.038), (42, 0.071), (43, -0.027), (44, -0.049), (45, 0.0), (46, 0.057), (47, -0.005), (48, -0.025), (49, -0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92872965 <a title="246-lsi-1" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>2 0.6282351 <a title="246-lsi-2" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>3 0.6236456 <a title="246-lsi-3" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>4 0.53899676 <a title="246-lsi-4" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>Author: Jacob Bien, Ya Xu, Michael W. Mahoney</p><p>Abstract: The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.</p><p>5 0.53625715 <a title="246-lsi-5" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>6 0.52135491 <a title="246-lsi-6" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>7 0.51686597 <a title="246-lsi-7" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>8 0.51409775 <a title="246-lsi-8" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>9 0.4983058 <a title="246-lsi-9" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>10 0.49070725 <a title="246-lsi-10" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>11 0.47553176 <a title="246-lsi-11" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>12 0.45581901 <a title="246-lsi-12" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>13 0.4154852 <a title="246-lsi-13" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>14 0.4078635 <a title="246-lsi-14" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>15 0.3906762 <a title="246-lsi-15" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>16 0.39044666 <a title="246-lsi-16" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>17 0.37322238 <a title="246-lsi-17" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>18 0.36309513 <a title="246-lsi-18" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>19 0.34782371 <a title="246-lsi-19" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>20 0.34474242 <a title="246-lsi-20" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.099), (30, 0.068), (32, 0.124), (34, 0.07), (45, 0.098), (68, 0.209), (77, 0.192)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88574994 <a title="246-lda-1" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>same-paper 2 0.87753284 <a title="246-lda-2" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>3 0.83750433 <a title="246-lda-3" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>Author: Tao Qin, Xiubo Geng, Tie-yan Liu</p><p>Abstract: This paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is deﬁned with a coset-permutation distance, and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efﬁcient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efﬁciency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-ofthe-art ranking accuracy, and are much more efﬁcient than previous algorithms.</p><p>4 0.82837969 <a title="246-lda-4" href="./nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">274 nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<p>Author: Amin Sayedi, Morteza Zadimoghaddam, Avrim Blum</p><p>Abstract: We discuss an online learning framework in which the agent is allowed to say “I don’t know” as well as making incorrect predictions on given examples. We analyze the trade off between saying “I don’t know” and making mistakes. If the number of don’t-know predictions is required to be zero, the model reduces to the well-known mistake-bound model introduced by Littlestone [Lit88]. On the other hand, if no mistakes are allowed, the model reduces to KWIK framework introduced by Li et. al. [LLW08]. We propose a general, though inefﬁcient, algorithm for general ﬁnite concept classes that minimizes the number of don’t-know predictions subject to a given bound on the number of allowed mistakes. We then present speciﬁc polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators with a margin.</p><p>5 0.82543671 <a title="246-lda-5" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>Author: Georg Langs, Yanmei Tie, Laura Rigolo, Alexandra Golby, Polina Golland</p><p>Abstract: Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difﬁcult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We ﬁrst embed each brain into a functional map that reﬂects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions. 1</p><p>6 0.82189596 <a title="246-lda-6" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>7 0.81951112 <a title="246-lda-7" href="./nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">141 nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>8 0.81569719 <a title="246-lda-8" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>9 0.81346196 <a title="246-lda-9" href="./nips-2010-Occlusion_Detection_and_Motion_Estimation_with_Convex_Optimization.html">187 nips-2010-Occlusion Detection and Motion Estimation with Convex Optimization</a></p>
<p>10 0.80738723 <a title="246-lda-10" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>11 0.80368745 <a title="246-lda-11" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>12 0.80263805 <a title="246-lda-12" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>13 0.80262029 <a title="246-lda-13" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>14 0.80199718 <a title="246-lda-14" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>15 0.80045307 <a title="246-lda-15" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>16 0.79851592 <a title="246-lda-16" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>17 0.79687446 <a title="246-lda-17" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>18 0.79371464 <a title="246-lda-18" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>19 0.79318053 <a title="246-lda-19" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>20 0.7930693 <a title="246-lda-20" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
