<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-153" href="#">nips2010-153</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</h1>
<br/><p>Source: <a title="nips-2010-153-pdf" href="http://papers.nips.cc/paper/4049-learning-invariant-features-using-the-transformed-indian-buffet-process.pdf">pdf</a></p><p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>Reference: <a title="nips-2010-153-reference" href="../nips2010_reference/nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Identifying the features of objects becomes a challenge when those features can change in their appearance. [sent-6, score-0.411]
</p><p>2 We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. [sent-8, score-0.461]
</p><p>3 However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? [sent-9, score-0.773]
</p><p>4 We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. [sent-11, score-0.291]
</p><p>5 One explanation for this capability is the visual system recognizes that the features of an object can occur differently across presentations, but will be transformed in a few predictable ways. [sent-15, score-0.401]
</p><p>6 Representing objects in terms of invariant features poses a challenge for models of feature learning. [sent-16, score-0.486]
</p><p>7 Despite this, people are able to identify invariant features (e. [sent-20, score-0.322]
</p><p>8 Analogous to how the Transformed Dirichlet Process extends the Dirichlet Process [7], the tIBP associates a parameter with each instantiation of a feature that determines how the feature is transformed in the given image. [sent-24, score-0.313]
</p><p>9 This allows for unsupervised learning of features that are invariant in location, size, or orientation. [sent-25, score-0.242]
</p><p>10 After deﬁning the generative model for the tIBP and presenting a Gibbs sampling inference algorithm, we show that this model can learn visual features that are location invariant by modeling previous behavioral results (from [6]). [sent-26, score-0.32]
</p><p>11 (a) Does this object have one feature that contains two vertical bars or two features that each contain one vertical bar? [sent-29, score-0.814]
</p><p>12 One new issue that arises from inferring invariant features is that it can be ambiguous whether parts of an image are the same feature with different transformations or different features. [sent-32, score-0.735]
</p><p>13 For example, an object containing two vertical bars has (at least) two representations: a single feature containing two vertical bars a ﬁxed distance apart, or two features each of which is a vertical bar with its own translational transformation (see Figure 1 (a)). [sent-33, score-1.208]
</p><p>14 Introducing transformational invariance also raises the question of what kinds of transformations a feature can undergo. [sent-36, score-0.448]
</p><p>15 A classic demonstration of the difﬁculty of deﬁning a set of permissable transformations is the Mach square/diamond [8]. [sent-37, score-0.317]
</p><p>16 We extend the tIBP to include variables that select the transformations each feature is allowed to undergo. [sent-40, score-0.445]
</p><p>17 This raises the question of whether people can infer the permissable transformations of a feature. [sent-41, score-0.506]
</p><p>18 This provides an interesting new explanation of the Mach square/diamond: People learn the allowed transformations of features for a given shape, not what transformations of features are allowed over all shapes. [sent-43, score-0.947]
</p><p>19 The Indian Buffet Process (IBP) [4] is a stochastic process that can be used as a prior in nonparametric Bayesian models where each object is represented using an unknown but potentially inﬁnite set of latent features. [sent-47, score-0.277]
</p><p>20 If there are N objects and K features, then Z is a N × K binary matrix (where object n has feature k if znk = 1) and Y is a K × D matrix (where D is the dimensionality of the observed properties of each object, e. [sent-50, score-0.614]
</p><p>21 The vector xn representing the properties of object n is generated based on its features zn and the matrix Y. [sent-62, score-0.447]
</p><p>22 The transformations are object-speciﬁc, so in a sense, when an object takes a feature, the feature is transformed with respect to the object. [sent-68, score-0.671]
</p><p>23 The following generative process deﬁnes the tIBP: Z|α Y|β  ∼ IBP(α) ∼ g(β)  rnk |η xn |rn , zn , Y, γ  iid  ∼ Φ(η) ∼ f (xn |rn (Y), zn , γ)  In this paper, we focus on binary images where the transformations are drawn uniformly at random from a ﬁnite set (though Section 5. [sent-70, score-0.816]
</p><p>24 Assuming our data are in {0, 1}D1 ×D2 , a translation shifts the starting place of its feature in each dimension by rnk = (d1 , d2 ). [sent-74, score-0.372]
</p><p>25 The likelihood p(xnd = 1|Z, Y, R) is then identical to Equation 2, substituting the vector of transformed feature interpretations rn (yd ) for yd . [sent-83, score-0.371]
</p><p>26 3  Inference by Gibbs sampling  We sample from the posterior distribution on feature assignments Z, feature interpretations Y, and transformations R given observed properties X using Gibbs sampling [11]. [sent-85, score-0.595]
</p><p>27 For features with mk > 0 (after removal of the current value of znk ), we draw znk by marginalizing over transformations. [sent-87, score-0.523]
</p><p>28 If znk = 1, we then sample rnk from p(rnk |znk = 1, Z−(nk) , R−(nk) , Y, X)  ∝ p(xn |zn , Y, R)p(rnk )  (4)  where the relevant probabilities are also used in computing Equation 3, and can thus be cached. [sent-90, score-0.36]
</p><p>29 To compute the ﬁrst term on the right hand side, we need new to marginalize over the possible new feature images and their transformations (Y(K+1):(K+Kn ) new and Rn,(K+1):(K+Kn ) ). [sent-96, score-0.557]
</p><p>30 We assume that the ﬁrst object to take a feature takes it in its canonical form and thus it is not transformed. [sent-97, score-0.293]
</p><p>31 With no transformations, drawing the new features in the noisy-OR tIBP model is equivalent to drawing the new features in the normal noisy-OR IBP model. [sent-99, score-0.382]
</p><p>32 (6) new  1 − (1 − ǫ)(1 − λ)zn rn (yd ) (1 − pλ)Kn  (7)  where rn (yd ) is the vector of transformed feature interpretations along observed dimension d. [sent-106, score-0.363]
</p><p>33 4  Prediction  To compare the feature representations our model infers to behavioral results, we need to have judgements of the model for new test objects. [sent-110, score-0.258]
</p><p>34 This is a prediction problem: computing the probability of a new object xN +1 given the set of N observed objects X. [sent-111, score-0.382]
</p><p>35 For each sweep of Gibbs sampling, we sample a vector of features zN +1 and corresponding transformations rN +1 for a new object from their conditional distribution given the values of Z, Y, and R in that sweep, under the constraint that no new features are generated. [sent-115, score-0.81]
</p><p>36 3  Demonstration: Learning Translation Invariant Features  In many situations learners need to form a feature representation of a set of objects, and the features do not reoccur in the exact same location. [sent-117, score-0.231]
</p><p>37 The tIBP provides a way for a learner to discover that features are translation invariant, and to infer them directly from the data. [sent-121, score-0.256]
</p><p>38 Fiser and colleagues [6, 12] showed that when two parts of an image always occur together (forming a “base pair”), people expect the two parts to occur together as if they had one feature representing the pair. [sent-122, score-0.367]
</p><p>39 In Experiments 1 and 2 of [6], participants viewed 144 scenes, where each scene contained three of the six base pairs in varied spatial location. [sent-123, score-0.258]
</p><p>40 Afterwards, participants chose which of two images was more familiar: a base pair (in a never seen before location) and pair of parts that occured together at least once (but were not a base pair). [sent-125, score-0.506]
</p><p>41 To demonstrate the ability of tIBP to infer translation invariant features that are made up of complex parts, we trained the model on the scenes with the same structure as those shown to participants. [sent-127, score-0.373]
</p><p>42 Figure 2 (c) shows the features inferred 4  (a)  (b)  (c)  Figure 2: Learning translation invariant features. [sent-130, score-0.297]
</p><p>43 To compare the model people’s familiarity judgments, we calculated the model’s predictive probability for each base pair in a new location and for a part in that base pair with another part that co-occured with it at least once (but not in a base pair). [sent-142, score-0.338]
</p><p>44 4  Experiment 1: One feature or two features transformed? [sent-144, score-0.231]
</p><p>45 Is an image composed of the same feature multiple times with different instantiations or is it composed with different features that may or may not be transformed? [sent-146, score-0.321]
</p><p>46 One way to decide between two possible feature representations for the object is to pick the features that allow you to encode the object and the other objects it is associated with. [sent-147, score-0.796]
</p><p>47 For example, the object from Figure 1 (a) is the ﬁrst object (from the top left) in the two sets of objects shown in Figure 3. [sent-148, score-0.535]
</p><p>48 All of the objects in this set can be represented as translations of one feature that is two vertical bars. [sent-150, score-0.451]
</p><p>49 Although this object set can also be described in terms of two features (each of which are vertical bars that can each translate independently), it is a surprising coincidence that the two vertical bars are always the same distance apart over all of the objects in the set. [sent-151, score-1.017]
</p><p>50 Using different feature representations leads to different predictions about what other objects should be expected to be in the set. [sent-154, score-0.354]
</p><p>51 Representing the objects with a single feature containing two vertical bars predicts new objects that have vertical bars where the two bars are the same distance apart (New Unitized). [sent-155, score-1.165]
</p><p>52 These objects are also expected under the feature representation that is two features that are each vertical bars; however, any object with two vertical bars is expected (New Separate) — not just those with a particular distance apart. [sent-156, score-0.981]
</p><p>53 Thus, interpreting objects with different feature representations has consequences for how to generalize set membership. [sent-157, score-0.306]
</p><p>54 In the following experiment, we test these predictions by asking people after viewing either the unitized or separate object sets to judge how likely the New Unitized or New Separate objects are to be part of the object set they viewed. [sent-158, score-1.012]
</p><p>55 We then compare the behavioral results to the features inferred by the tIBP model and the predictive probability of each of the test objects given each of the object sets. [sent-159, score-0.55]
</p><p>56 (a) Objects made from spatial translations of the unitized feature. [sent-161, score-0.29]
</p><p>57 The number of times each vertical bar is present is the same in the two object sets. [sent-163, score-0.415]
</p><p>58 The unitized group only rated those images with two vertical bars close together highly. [sent-166, score-0.659]
</p><p>59 The separate group rate any image with two vertical bars highly. [sent-167, score-0.411]
</p><p>60 Three participants were removed for failing to complete the task leaving 19 and 18 participants in the separate and unitized conditions respectively. [sent-171, score-0.653]
</p><p>61 In the training phase, participants read this cover story (adapted from [13]): “Recently a Mars rover found a cave with a collection of different images on its walls. [sent-173, score-0.343]
</p><p>62 ” They then looked through the eight images (which were either the unitized or separate object set in a random order) and scrolled down to the next section once they were ready for the test phase. [sent-176, score-0.607]
</p><p>63 2  Results  Figure 4 (a) shows the average ratings made by participants in each group for the nine test images. [sent-181, score-0.234]
</p><p>64 001) objects higher than the unitized group, but otherwise did not rate any of the other test images signiﬁcantly different. [sent-186, score-0.496]
</p><p>65 As predicted by the above analysis, the unitized group believed the Mars rover was likely to encounter the two images it observed and the New Unit image (the unitized feature in a new horizontal position), but did not think it would encounter the other objects. [sent-187, score-0.92]
</p><p>66 The separate group rated any image with two vertical bars highly. [sent-188, score-0.448]
</p><p>67 This indicates that they represent the images using two features each containing a single vertical bar varying in horizontal position. [sent-189, score-0.456]
</p><p>68 Thus, each group of participants infer a set of features invariant over the set of observed objects (taking into account the different horizontal position of the features in each object). [sent-190, score-0.811]
</p><p>69 Figure 4 (b) shows the predictions made by the tIBP model when given each object set. [sent-191, score-0.232]
</p><p>70 A non-linear monotonic transformation of these probabilities was used for visualization, 6  (a)  (b)  (c)  Rotation set  Size set  New Rotation New Size  Figure 5: Stimuli for investigating how different types of invariances are learned for different object classes. [sent-193, score-0.265]
</p><p>71 (c) Two new objects for testing the inferred type of invariance a New Rotation and a New Size object. [sent-196, score-0.284]
</p><p>72 Unlike the participants in the separate condition, the model does not infer that each object has two features and so having only one feature is not a good object. [sent-202, score-0.72]
</p><p>73 This suggests that while learning the feature representation for a set of objects, people also learn the number of features each object typically has. [sent-203, score-0.527]
</p><p>74 Investigating how people infer expectations about the number of features objects have is an interesting phenomenon that demands further study. [sent-204, score-0.478]
</p><p>75 5  Experiment 2: Learning the type of invariance  A natural next step for improving the tIBP would be to make the set of transformations Φ larger and thus extend the number of possible invariants that can be learned. [sent-205, score-0.38]
</p><p>76 This example teaches a counterintuitive moral: The best approach is not to include as many transformations as possible into the model. [sent-209, score-0.283]
</p><p>77 Though rotations are not valid transformations for what people commonly consider to be squares, they are appropriate for many objects. [sent-210, score-0.395]
</p><p>78 This suggests that people infer the set of allowable transformations for different classes of objects. [sent-211, score-0.472]
</p><p>79 Given the three objects in Figure 5 (a) (the rotation set) it seems clear that the New Rotation object in Figure 5 (c) belongs in the set, but not the New Size object. [sent-212, score-0.516]
</p><p>80 To explore this phenomenon, we ﬁrst extend the tIBP to infer the appropriate set of transformations by introducing latent variables for each feature that indicate which transformations it is allowed to use. [sent-214, score-0.833]
</p><p>81 We demonstrate this extension to the tIBP predicts the New Rotation object when given the rotation set and predicts the New Size object when given the size set — effectively learning the appropriate type of invariance for a given object class. [sent-215, score-0.865]
</p><p>82 Finally, we conﬁrm our introspective argument that people infer the type of invariance appropriate to the observed class of objects. [sent-216, score-0.245]
</p><p>83 1  Learning invariance type using the tIBP  It is straightforward to modify the tIBP such that the type of transformations allowed on a feature is inferred as well. [sent-218, score-0.531]
</p><p>84 The experiment in this section is learning whether or not the feature deﬁning a set of objects is either rotation or size invariant. [sent-221, score-0.514]
</p><p>85 Formally, we model this using a generative process that is the same as the tIBP, but introduces the latent variable tk which determines the type of transformation allowed by feature k. [sent-222, score-0.375]
</p><p>86 If tk = 1, then rotational transformations are drawn from Φρ (which is the discrete uniform distribution distribution ranging in multiples of ﬁfteen degrees from zero to 45). [sent-223, score-0.425]
</p><p>87 If tk = 0, then size transformations are drawn from Φσ (which is the discrete uniform distribution iid over [3/8, 3/7, 3/5, 5/7, 1, 7/5, 11/7, 5/3, 11/5, 7/3, 11/3]). [sent-224, score-0.42]
</p><p>88 , rnk , p(tk |X, Y, Z, R−k , t−k )  p(xn |rnk , tk , Y, Z, R−k , t−k )p(rk |tk )p(tk ). [sent-230, score-0.315]
</p><p>89 Prediction is as above except tk gives the set of transformations each feature is allowed to take. [sent-234, score-0.554]
</p><p>90 2  Methods  A total of 40 participants were recruited online and compensated a small amount, with 20 participants in both training conditions (rotation and size). [sent-236, score-0.366]
</p><p>91 Participants observed the three objects in their training set and then generalize on a scale from 0 to 6 to ﬁve test objects: Same Both (the object that is in both training sets), Same Rot (the last object of the rotation set), Same Size (the last object of the size set), New Rot and New Size. [sent-238, score-0.912]
</p><p>92 As expected, participants in the rotation condition generalize more to the New Rot object than the size condition (unpaired t(38) = 4. [sent-241, score-0.545]
</p><p>93 This conﬁrms our hypothesis; people infer the appropriate set of transformations (a subset of all transformations) features are allowed to use for a class of objects. [sent-246, score-0.647]
</p><p>94 Importantly, the model predicts that only when given the rotation set should participants generalize to the New Rot object and only when given the size set should they generalize to the New Size object. [sent-257, score-0.577]
</p><p>95 6  Conclusions and Future Directions  In this paper, we presented a solution to how people infer feature representations that are invariant over transformations and in two behavioral experiments conﬁrmed two predictions of a new model of human unsupervised feature learning. [sent-258, score-1.026]
</p><p>96 In addition to these contributions, we proposed a ﬁrst sketch of a new computational theory of shape representation — the features representing an object are transformed relative to the object and the set of transformations a feature is allowed to undergo depends on the object’s context. [sent-259, score-1.116]
</p><p>97 In the future, we would like to pursue this theory further, expanding the account of learning the types of transformations and exploring how the transformations between features in an object interact (we should expect some interaction due to real world constraints on the transformations, e. [sent-260, score-0.872]
</p><p>98 Finally, we hope to include other facets of visual perception into our model, like a perceptually realistic prior on feature instantiations and features relations (e. [sent-263, score-0.263]
</p><p>99 , the horizontal bar is always ON TOP OF the vertical bar). [sent-265, score-0.262]
</p><p>100 Inﬁnite latent feature models and the Indian buffet process. [sent-285, score-0.261]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tibp', 0.48), ('transformations', 0.283), ('unitized', 0.257), ('rnk', 0.206), ('object', 0.184), ('participants', 0.168), ('objects', 0.167), ('rotation', 0.165), ('kn', 0.164), ('rot', 0.154), ('znk', 0.154), ('sep', 0.147), ('vertical', 0.142), ('ibp', 0.137), ('buffet', 0.124), ('features', 0.122), ('indian', 0.117), ('bars', 0.115), ('people', 0.112), ('feature', 0.109), ('tk', 0.109), ('transformed', 0.095), ('base', 0.09), ('bar', 0.089), ('invariant', 0.088), ('zn', 0.086), ('yd', 0.082), ('infer', 0.077), ('images', 0.072), ('rover', 0.069), ('znew', 0.069), ('gibbs', 0.068), ('mk', 0.061), ('xnd', 0.06), ('separate', 0.06), ('human', 0.06), ('image', 0.058), ('translation', 0.057), ('invariance', 0.056), ('xn', 0.055), ('allowed', 0.053), ('austerweil', 0.051), ('mach', 0.051), ('ykd', 0.051), ('nk', 0.051), ('transformation', 0.048), ('predictions', 0.048), ('behavioral', 0.047), ('experiment', 0.045), ('parts', 0.044), ('rn', 0.043), ('grif', 0.043), ('seen', 0.042), ('interpretations', 0.042), ('invariants', 0.041), ('fiser', 0.041), ('diamond', 0.041), ('unit', 0.041), ('infers', 0.041), ('drawing', 0.038), ('nonparametric', 0.037), ('sweep', 0.037), ('rated', 0.037), ('location', 0.037), ('group', 0.036), ('cave', 0.034), ('permissable', 0.034), ('scrolled', 0.034), ('unpaired', 0.034), ('judgments', 0.034), ('perceived', 0.034), ('dirichlet', 0.033), ('berkeley', 0.033), ('rotational', 0.033), ('translations', 0.033), ('invariances', 0.033), ('unsupervised', 0.032), ('predicts', 0.032), ('marginalizing', 0.032), ('instantiations', 0.032), ('new', 0.031), ('horizontal', 0.031), ('representations', 0.03), ('apart', 0.03), ('recruited', 0.03), ('nine', 0.03), ('bayesian', 0.03), ('inferred', 0.03), ('scenes', 0.029), ('shape', 0.029), ('process', 0.028), ('ths', 0.028), ('latent', 0.028), ('size', 0.028), ('scientists', 0.028), ('wood', 0.028), ('mars', 0.026), ('undergo', 0.026), ('sampling', 0.026), ('spearman', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="153-tfidf-1" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>2 0.14620455 <a title="153-tfidf-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.11399239 <a title="153-tfidf-3" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>4 0.10300761 <a title="153-tfidf-4" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>5 0.10016263 <a title="153-tfidf-5" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>6 0.096356958 <a title="153-tfidf-6" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>7 0.093685038 <a title="153-tfidf-7" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>8 0.087215982 <a title="153-tfidf-8" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>9 0.086294971 <a title="153-tfidf-9" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>10 0.082042456 <a title="153-tfidf-10" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>11 0.079317063 <a title="153-tfidf-11" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>12 0.075820059 <a title="153-tfidf-12" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>13 0.075206198 <a title="153-tfidf-13" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>14 0.074745625 <a title="153-tfidf-14" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>15 0.069722742 <a title="153-tfidf-15" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>16 0.066268824 <a title="153-tfidf-16" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>17 0.065832309 <a title="153-tfidf-17" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>18 0.063084573 <a title="153-tfidf-18" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>19 0.062559016 <a title="153-tfidf-19" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>20 0.062113721 <a title="153-tfidf-20" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.074), (2, -0.123), (3, -0.131), (4, -0.036), (5, -0.002), (6, 0.01), (7, 0.049), (8, -0.007), (9, 0.036), (10, 0.016), (11, 0.032), (12, -0.087), (13, -0.027), (14, 0.052), (15, -0.013), (16, 0.09), (17, 0.031), (18, 0.097), (19, 0.027), (20, -0.011), (21, 0.085), (22, 0.016), (23, -0.036), (24, 0.061), (25, 0.02), (26, -0.049), (27, 0.014), (28, 0.01), (29, 0.019), (30, -0.075), (31, 0.009), (32, 0.001), (33, 0.065), (34, 0.007), (35, 0.005), (36, -0.007), (37, 0.018), (38, -0.018), (39, 0.018), (40, -0.014), (41, 0.093), (42, 0.084), (43, -0.058), (44, -0.068), (45, 0.02), (46, 0.033), (47, -0.082), (48, -0.026), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93401414 <a title="153-lsi-1" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>2 0.70551217 <a title="153-lsi-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.65717459 <a title="153-lsi-3" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>4 0.64681208 <a title="153-lsi-4" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>5 0.61722761 <a title="153-lsi-5" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>Author: Leonid Karlinsky, Michael Dinerstein, Shimon Ullman</p><p>Abstract: This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because instances of different actions may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are anchored to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance. 1</p><p>6 0.58593559 <a title="153-lsi-6" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>7 0.56038165 <a title="153-lsi-7" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>8 0.55053043 <a title="153-lsi-8" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>9 0.54780215 <a title="153-lsi-9" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>10 0.52403831 <a title="153-lsi-10" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>11 0.50773412 <a title="153-lsi-11" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>12 0.49631459 <a title="153-lsi-12" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>13 0.48749945 <a title="153-lsi-13" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>14 0.48746207 <a title="153-lsi-14" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>15 0.47732517 <a title="153-lsi-15" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>16 0.47583622 <a title="153-lsi-16" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>17 0.47124457 <a title="153-lsi-17" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>18 0.46470585 <a title="153-lsi-18" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>19 0.45050925 <a title="153-lsi-19" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>20 0.44942105 <a title="153-lsi-20" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.033), (17, 0.013), (27, 0.101), (30, 0.06), (35, 0.027), (44, 0.301), (45, 0.188), (50, 0.051), (52, 0.044), (60, 0.03), (77, 0.034), (90, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74403137 <a title="153-lda-1" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>2 0.71957779 <a title="153-lda-2" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>Author: Nebojsa Jojic, Chris Meek, Jim C. Huang</p><p>Abstract: Many problem domains including climatology and epidemiology require models that can capture both heavy-tailed statistics and local dependencies. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Existing algorithms for inference and learning in CDNs are limited to those with tree-structured (nonloopy) graphs. In this paper, we develop inference and learning algorithms for CDNs with arbitrary topology. Our approach to inference and learning relies on recursively decomposing the computation of mixed derivatives based on a junction trees over the cumulative distribution functions. We demonstrate that our systematic approach to utilizing the sparsity represented by the junction tree yields signiďŹ cant performance improvements over the general symbolic differentiation programs Mathematica and D*. Using two real-world datasets, we demonstrate that non-tree structured (loopy) CDNs are able to provide signiďŹ cantly better ďŹ ts to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p><p>3 0.71485698 <a title="153-lda-3" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>4 0.61316967 <a title="153-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.61278898 <a title="153-lda-5" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>6 0.61169505 <a title="153-lda-6" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>7 0.60998398 <a title="153-lda-7" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>8 0.6089862 <a title="153-lda-8" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>9 0.60841507 <a title="153-lda-9" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>10 0.60799569 <a title="153-lda-10" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>11 0.60751069 <a title="153-lda-11" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>12 0.60745895 <a title="153-lda-12" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>13 0.60706007 <a title="153-lda-13" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>14 0.60569698 <a title="153-lda-14" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>15 0.60330182 <a title="153-lda-15" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>16 0.60212123 <a title="153-lda-16" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>17 0.6010282 <a title="153-lda-17" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>18 0.60066283 <a title="153-lda-18" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>19 0.6002996 <a title="153-lda-19" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>20 0.5997386 <a title="153-lda-20" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
