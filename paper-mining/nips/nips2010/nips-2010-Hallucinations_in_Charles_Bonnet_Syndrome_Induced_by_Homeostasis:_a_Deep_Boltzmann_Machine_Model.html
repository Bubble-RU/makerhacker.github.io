<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-111" href="#">nips2010-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</h1>
<br/><p>Source: <a title="nips-2010-111-pdf" href="http://papers.nips.cc/paper/4097-hallucinations-in-charles-bonnet-syndrome-induced-by-homeostasis-a-deep-boltzmann-machine-model.pdf">pdf</a></p><p>Author: Peggy Series, David P. Reichert, Amos J. Storkey</p><p>Abstract: The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative ﬁndings in CBS. We also introduce a modiﬁcation to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception. 1</p><p>Reference: <a title="nips-2010-111-reference" href="../nips2010_reference/nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. [sent-10, score-0.968]
</p><p>2 And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. [sent-12, score-1.182]
</p><p>3 1  Introduction  Complex visual hallucinations [1] can offer a fascinating insight into how the brain realizes visual perception. [sent-16, score-1.004]
</p><p>4 The content of such hallucinations can be highly elaborate, consisting of people, animals, objects and whole scenes, and the images supposedly can “exceed anything seen in real life” in detail and vividness [1]. [sent-17, score-0.859]
</p><p>5 Attempts have been made to unify complex hallucinations in various pathologies in one qualitative model, but many argue that the underlying causal mechanisms are too varied to do so [2]. [sent-18, score-0.922]
</p><p>6 Of particular interest is the Charles Bonnet Syndrome (CBS) [3, 4, 5], where patients experience complex visual hallucinations which appear not to be causally related to any other impairment to mental health and where the primary pathology is one of loss of vision due to eye diseases. [sent-19, score-1.041]
</p><p>7 The hypothesis that cortical learning is driven by prediction or reconstruction of sensory input is promising as it could explain how the brain might learn in an unsupervised fashion, evaluating its internal model of the world by matching predictions to actual input [6, 7]. [sent-25, score-0.327]
</p><p>8 Consequently, the idea that disorders including hallucinations are caused by mismatches between internally generated expectations and sensory input has recently found interest in psychology [8, 9]. [sent-26, score-0.897]
</p><p>9 If generating internal imagery is an essential aspect of normal vision, then this could explain why hallucinations occur in so many different pathologies, even sometimes when there is no direct malfunction of the visual system itself [1]. [sent-27, score-1.0]
</p><p>10 They are related to Hopﬁeld networks, which have been used in the context of models of hallucinations before [11]. [sent-30, score-0.785]
</p><p>11 However, whereas the latter model some abstract memory system, DBMs (and the related Deep Belief Nets) learn hierarchical representations of data [7], thus capturing aspects of the visual cortex [12], the locus where visual hallucinations are ultimately realized [1, 13]. [sent-31, score-1.073]
</p><p>12 We thus present a DBM model of the CBS, and propose a concrete mechanism that could lead to hallucinations being formed: homeostasis. [sent-33, score-0.804]
</p><p>13 There is strong experimental evidence that homeostatic processes serve to stabilize the activity level of neuronal populations through a variety of cellular and synaptic mechanisms [14]. [sent-34, score-0.344]
</p><p>14 Hence, in CBS a lack of visual input could lead to intrinsic excitability changes of neurons setting in to restore original activity levels. [sent-36, score-0.288]
</p><p>15 In our model, we demonstrate how these changes could cause spontaneous ‘complex’ hallucinations to be formed even when sensory input is lacking. [sent-37, score-0.908]
</p><p>16 In Section 3, we ﬁrst show that homeostasis can be beneﬁcial in a DBM, enabling the model to recover correct internal representations from degraded input. [sent-39, score-0.276]
</p><p>17 1), potentially also explaining a tendency to see hallucinated objects too small for their surroundings; and, effects of cortical lesions and cortical suppression of activity (Section 3. [sent-42, score-0.358]
</p><p>18 Moreover, hallucinations in CBS tend to occur more often in states of drowsiness, implicating a role of cholinergic and serotonergic systems [1]. [sent-44, score-0.831]
</p><p>19 Finally, we speculate on the potential of the DBM to model other pathologies and on the difference between hallucinations and mental imagery (Section 5). [sent-46, score-0.936]
</p><p>20 2  Deep Boltzmann Machines  Boltzmann machines (BMs) [17] are closely related to Hopﬁeld networks, which have been employed as models of hallucinations before (e. [sent-47, score-0.785]
</p><p>21 The second difference is the possible introduction of hidden units, separating x into visible units v and hidden units h. [sent-58, score-0.256]
</p><p>22 A Deep Boltzmann Machine (DBM) [10] is a BM with a special architecture (Figure 2a) consisting of a visible layer and several subsequent hidden layers stacked on top of each other. [sent-69, score-0.296]
</p><p>23 When trained on a data set of images, each pair of adjacent layers is trained one at a time so that each layer learns to generate the activations of the layer below, using only biologically plausible local Hebbian (and anti-Hebbian) weight changes. [sent-71, score-0.371]
</p><p>24 In detail, the model had 20x20 visible units corresponding to images with 20x20 pixels, and three hidden layers of 26x26 units each. [sent-74, score-0.324]
</p><p>25 Each unit in the ﬁrst hidden layer received inputs from a 7x7 patch of visible units, whereas the higher layers received inputs from half (13x13) and all (26x26) of the units of the respective lower layer. [sent-75, score-0.379]
</p><p>26 1  Sampling and decoding the internal state  To model perception, we clamp the visible units to an image and sample the hidden units, starting from the ﬁrst hidden layer and proceeding to the topmost, then going downwards, and repeating this cycle. [sent-78, score-0.396]
</p><p>27 We are interested in the representations formed internally, in the hidden layers of the DBM, when visual input is ﬁxed or lacking in the case of CBS. [sent-81, score-0.327]
</p><p>28 Given the states xk of the hidden layer k in question, the activations ak−1 of the layer below are computed taking only xk into account, ignoring the states xk−2 further below. [sent-83, score-0.356]
</p><p>29 In the case of hallucinations, internal representations are matched against all three template shapes, taking the one that matches best as being represented and the corresponding crosscorrelation value as measure of the quality of the hallucination (Figure 2b). [sent-89, score-0.293]
</p><p>30 With the visible units (dark) clamped to an image (bottom left), the hidden layer states assume representations of that image. [sent-104, score-0.33]
</p><p>31 (a) right-hand side: After homeostatic regulation given empty images, hallucinations form spontaneously. [sent-106, score-0.974]
</p><p>32 (b): Examples of hallucinations of different qualities (computed from cross-correlations with templates). [sent-109, score-0.817]
</p><p>33 To simulate CBS, we then blank the visual input and let the model employ homeostatic regulation to recover healthy activation levels. [sent-113, score-0.441]
</p><p>34 3  Hallucinations emerging due to homeostasis  First, we demonstrate that homeostasis can be a beneﬁcial mechanism in a DBM. [sent-120, score-0.351]
</p><p>35 As the excitability of the units was adjusted, mean activity levels for each hidden layer were gradually restored. [sent-131, score-0.407]
</p><p>36 To model the CBS which is often triggered by profound retinal damage, we then repeated the homeostasis experiment with blank images. [sent-137, score-0.289]
</p><p>37 3  hallucination quality  layer 1 layer 2 layer 3  1. [sent-151, score-0.594]
</p><p>38 Figure 3: (a): For corrupted images, homeostatic restoration of original activity levels (left ﬁgure, dashed lines) in the three hidden layers, and recovering reconstruction quality (right ﬁgure). [sent-164, score-0.412]
</p><p>39 (b): For blank images, restoration of activity (left) and qualities of emerging hallucinations (right). [sent-165, score-1.014]
</p><p>40 To analyze the internal representations of the model, we computed the qualities of individual representations of the topmost hidden layer from the projected reconstructions (Figure 3b). [sent-169, score-0.418]
</p><p>41 Over an initial period, internal representations correspond to blank images even though activity levels gradually improve. [sent-170, score-0.368]
</p><p>42 At some point however, hallucinations emerge, and relatively soon can they reach high quality levels. [sent-171, score-0.829]
</p><p>43 At this point activities start to change more rapidly, hence hallucinations themselves contribute to the restoration of activity levels. [sent-172, score-0.878]
</p><p>44 These results are consistent with CBS: If loss of vision is abrupt, hallucinations emerge after an initial latent period lasting hours to days [5], which matches well the time scale on which homeostatic mechanisms take place [14]. [sent-173, score-1.033]
</p><p>45 0, there are also numerous hallucinations of lower quality, which could be in line with CBS as there complex hallucinations are often mixed with simple, less sophisticated ones. [sent-175, score-1.622]
</p><p>46 Also, some of the lower quality hallucinations are of transitory nature (if run for 200 instead of 40 cycles, mean quality rose from 0. [sent-176, score-0.873]
</p><p>47 We also repeated the experiment with images containing random noise instead of being blank (Figure 1c) to simulate a different type of visual impairment than total blindness. [sent-180, score-0.287]
</p><p>48 We found in this case that smaller overall bias shifts were necessary to restore original activity levels (not shown) and produce hallucinations (Figure 5a). [sent-181, score-0.905]
</p><p>49 This shows that the exact nature of visual impairment could have an impact on whether and when hallucinations are formed. [sent-182, score-0.933]
</p><p>50 Indeed, many CBS patients develop hallucinations as vision degrades, but stop hallucinating when vision is ﬁnally lost completely [5]. [sent-183, score-0.804]
</p><p>51 Thus, as long as there is some input, even if it is unspeciﬁc noise, hallucinations can be formed, but losing the input completely might require too much of a bias shift. [sent-185, score-0.813]
</p><p>52 Another reason for the cessation of hallucinations over time could be input speciﬁc synaptic plasticity, i. [sent-186, score-0.85]
</p><p>53 Hence, homeostasis as a short-term stabilization mechanism could lead to hallucinations, but a long-term reorganization of the cortex to represent the novel input would cause them to cease. [sent-190, score-0.266]
</p><p>54 1  Localized hallucinations with localized lesions  Another property of the hallucinations was that the represented shapes were found to be distributed over the whole image and could be any of the three categories (Figure 4a). [sent-192, score-1.636]
</p><p>55 This is of relevance as complex hallucinations in CBS vary from episode to episode in a majority of patients [4]. [sent-193, score-0.837]
</p><p>56 Some studies [5] report that hallucinations tend to be localized to the blind regions. [sent-196, score-0.843]
</p><p>57 To test whether we could reproduce this, we repeated the homeostasis experiment with images from the training data set in which only the top half had been blanked out (Figure 1d), simulating a localized impairment of vision. [sent-197, score-0.356]
</p><p>58 To test whether hallucinations would form at any location, we then tested the model on blank images. [sent-199, score-0.89]
</p><p>59 As shown in Figure 4b, the stable hallucinations were 5  3. [sent-200, score-0.807]
</p><p>60 20  10  hallucination count (1000)  hallucination count (1000)  hallucination count (1000)  3. [sent-208, score-0.417]
</p><p>61 95) hallucinations in projected images at the end of homeostatic regulation. [sent-212, score-1.004]
</p><p>62 (a): Although local hotspots exist and squares are least likely to occur, overall hallucinations vary in type and location. [sent-214, score-0.785]
</p><p>63 (b): When only the top half of visual input was blanked during homeostatic regulation, hallucinations emerged localized to the ‘blind’ half. [sent-215, score-1.152]
</p><p>64 (c): When a region too narrow for triangles was blanked instead, hallucinations were almost always squares. [sent-216, score-0.834]
</p><p>65 If hallucinations are constrained by a blind area of restricted size, and there is a tendency to see whole objects (rather than parts), then this would mean that hallucinated objects would have to be small simply to ﬁt into the blind area, often too small to ﬁt the real surroundings (e. [sent-221, score-0.96]
</p><p>66 Indeed, we found that now, stable hallucinations are mostly squares by a large margin (Figure 4c), despite the fact that for fully blank images and for top lesioned images, squares were by far less common. [sent-226, score-0.993]
</p><p>67 5 The network thus relied on hallucinations that ﬁtted the blind region to restore its activity levels. [sent-227, score-0.91]
</p><p>68 2  Cortical damage and suppression  Damage to the visual system causing complex hallucinations can also be cortical, e. [sent-229, score-1.002]
</p><p>69 Hence, if the initial loss of vision is caused by damage to early cortical areas, complex hallucinations can form over time. [sent-237, score-0.963]
</p><p>70 If on the other hand activation in early areas is suppressed when CBS symptoms have already been developed due to for example eye disease, hallucinations cease, at least temporarily. [sent-238, score-0.844]
</p><p>71 Taking the ﬁrst hidden layer as representing an early cortical area, we repeat the homeostasis experiment with the hidden units in that layer clamped to activations as if they were receiving no input (instead of clamping the visibles to blank images), simulating a cortical lesion. [sent-240, score-0.939]
</p><p>72 Again we found stable hallucinations to emerge in the higher layers (not shown). [sent-241, score-0.907]
</p><p>73 We note that due to the hierarchical receptive ﬁeld structure in the model, the topmost hidden layer plays a special role, its units having the largest receptive ﬁelds. [sent-244, score-0.281]
</p><p>74 We ﬁnd that a DBM trained without 5 Square hallucinations were found to be least common after homeostasis over several model instances, although this bias did not exist in generations from the original models. [sent-245, score-0.951]
</p><p>75 8  mean hallucination quality  mean hallucination quality  1. [sent-250, score-0.366]
</p><p>76 Thus, the top-most layer or pair of layers is necessary for generating complex hallucinations, as is the case with the higher associative visual areas (although processing in the cortical hierarchy is of course much more complicated than in our model). [sent-290, score-0.452]
</p><p>77 They are also sufﬁcient in our model in so far as homeostasis did induce hallucinations as long as the ﬁrst hidden layer was clamped at the outset. [sent-291, score-1.146]
</p><p>78 However, as the last experiment has shown, when hallucinations emerge due to activity changes in the whole system, then interfering even with a lower layer can disrupt their formation. [sent-292, score-1.022]
</p><p>79 When hallucinations are formed they evoke corresponding representations in all hidden layers, even the lower ones that by themselves cannot support stable shape representations (Figure 2b). [sent-294, score-0.992]
</p><p>80 A role of recurrent interactions for hallucinations is also suggested in [23]. [sent-297, score-0.812]
</p><p>81 4  A novel model of acetylcholine and its role in CBS  CBS hallucinations are more likely to occur in states of drowsiness [1, 5]. [sent-298, score-0.914]
</p><p>82 This suggests a role of cholinergic and serotonergic systems, which in turn are implicated in pathologies of complex hallucinations other than CBS as well [1]. [sent-299, score-0.95]
</p><p>83 There is experimental evidence that acetylcholine (ACh) acts speciﬁcally to emphasize sensory input over internally generated one, mediating “the switching of the cortical processing mode from an intracortical to an input-processing mode” [24]. [sent-300, score-0.299]
</p><p>84 In the DBM model, each (intermediate) hidden layer receives input from a layer below, conveying sensory information, and from a layer above that has learned to generate or predict the former layer’s activity. [sent-304, score-0.548]
</p><p>85 We model the effect of drowsiness on hallucinations in CBS as follows: We assume that drowsiness is being reﬂected as a decrease in ACh, modeled as α < 0. [sent-311, score-0.895]
</p><p>86 We ﬁnd that with decreased levels of ACh, not only is a much smaller homeostatic shift of excitability necessary to elicit hallucinations, but the average hallucination quality is also superior. [sent-318, score-0.432]
</p><p>87 5 at maximal bias shift, whereas hallucinations 7  at balanced ACh levels have not even emerged yet at this point. [sent-322, score-0.812]
</p><p>88 This would thus correspond to a situation where hallucinations would only occur during drowsiness. [sent-323, score-0.785]
</p><p>89 In that case, hallucinations never emerge over the course of the homeostatic process (the end of which is determined from activities computed with α = 0. [sent-326, score-0.998]
</p><p>90 5  Discussion  We have reproduced a variety of ﬁndings related to CBS, and make two main predictions: First, interfering with cortical homeostatic mechanisms after the loss of vision should delay or prevent the development of hallucinations. [sent-329, score-0.306]
</p><p>91 Second, we suggest that acetylcholine could not only inﬂuence the balance of thalamic and intracortical inputs [24], but also the balance in between bottom-up and top-down at various stages of the cortical hierarchy. [sent-330, score-0.266]
</p><p>92 In [18] schizophrenia is modeled with an approach akin to ours, with hallucinatory memories surfacing in a Hopﬁeld net due to homeostatic mechanisms that compensate for input degradation. [sent-333, score-0.325]
</p><p>93 We emphasize that the key aspect of a model of visual hallucinations is not that it generates images, but that it spontaneously generates rich internal representations of images. [sent-339, score-0.987]
</p><p>94 As current machine learning work sees DBMs applied to more and more complex problems, more powerful demonstrations of complex hallucinations should be possible in the future. [sent-341, score-0.851]
</p><p>95 In the DBM, this could be modeled by decoupling different parts of the architecture, and incorporating other sensory hierarchies to account for the fact that visual hallucinations in schizophrenia tend to come with auditory hallucinations, suggesting system wide interactions. [sent-344, score-0.973]
</p><p>96 We suggest that for mental imagery, representations are merely realized in higher areas that code for objects more abstractly, whereas for vivid hallucinations they are realized throughout the whole system [13], and hence are richer in information content. [sent-347, score-0.983]
</p><p>97 Then, lower areas really would be needed to realize all information entailed in rich perception, thus explaining the perceptual difference in between high level mental imagery and system wide vivid visual hallucinations. [sent-350, score-0.286]
</p><p>98 (1996) Visual hallucinations in psychologically normal people: Charles Bonnet’s Syndrome. [sent-380, score-0.785]
</p><p>99 (2003) Complex visual hallucinations in the visually impaired: the Charles Bonnet Syndrome. [sent-389, score-0.877]
</p><p>100 (2000) How hallucinations may arise from brain mechanisms of learning, attention, and volition. [sent-490, score-0.875]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hallucinations', 0.785), ('cbs', 0.259), ('homeostasis', 0.166), ('homeostatic', 0.166), ('dbm', 0.148), ('ach', 0.139), ('hallucination', 0.139), ('layer', 0.137), ('blank', 0.105), ('visual', 0.092), ('cortical', 0.085), ('acetylcholine', 0.074), ('layers', 0.073), ('activity', 0.073), ('damage', 0.06), ('internal', 0.059), ('hidden', 0.058), ('mental', 0.057), ('pmid', 0.057), ('units', 0.056), ('excitability', 0.056), ('bonnet', 0.055), ('drowsiness', 0.055), ('mechanisms', 0.055), ('cortex', 0.053), ('images', 0.053), ('representations', 0.051), ('sensory', 0.051), ('pathologies', 0.049), ('imagery', 0.045), ('quality', 0.044), ('balance', 0.044), ('hallucinated', 0.041), ('bm', 0.038), ('hop', 0.037), ('impairment', 0.037), ('implicated', 0.037), ('vivid', 0.037), ('boltzmann', 0.035), ('brain', 0.035), ('internally', 0.033), ('complex', 0.033), ('qualities', 0.032), ('blind', 0.032), ('hinton', 0.032), ('deep', 0.032), ('areas', 0.032), ('suppression', 0.032), ('perception', 0.03), ('topmost', 0.03), ('syndrome', 0.03), ('cycles', 0.029), ('neuronal', 0.028), ('unspeci', 0.028), ('blanked', 0.028), ('cholinergic', 0.028), ('dbms', 0.028), ('hallucinatory', 0.028), ('lesioned', 0.028), ('mediating', 0.028), ('surroundings', 0.028), ('input', 0.028), ('visible', 0.028), ('recurrent', 0.027), ('charles', 0.027), ('activation', 0.027), ('levels', 0.027), ('half', 0.027), ('emerge', 0.027), ('schizophrenia', 0.026), ('localized', 0.026), ('formed', 0.025), ('temporary', 0.024), ('cls', 0.024), ('activations', 0.024), ('regulation', 0.023), ('perceptual', 0.023), ('memories', 0.022), ('stabilize', 0.022), ('corrupted', 0.022), ('reconstruction', 0.022), ('stable', 0.022), ('neurological', 0.021), ('lesions', 0.021), ('triangles', 0.021), ('objects', 0.021), ('biases', 0.02), ('eld', 0.02), ('restore', 0.02), ('activities', 0.02), ('toy', 0.02), ('patients', 0.019), ('could', 0.019), ('degradation', 0.019), ('emerging', 0.019), ('cessation', 0.018), ('menon', 0.018), ('pathology', 0.018), ('profound', 0.018), ('serotonergic', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="111-tfidf-1" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>Author: Peggy Series, David P. Reichert, Amos J. Storkey</p><p>Abstract: The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative ﬁndings in CBS. We also introduce a modiﬁcation to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception. 1</p><p>2 0.083404981 <a title="111-tfidf-2" href="./nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">141 nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>Author: Deqing Sun, Erik B. Sudderth, Michael J. Black</p><p>Abstract: Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical ﬂow in layers that addresses many of the shortcomings of previous approaches. In particular, we deﬁne a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical ﬂow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an imagedependent hidden ﬁeld prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.</p><p>3 0.081371255 <a title="111-tfidf-3" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>4 0.07546965 <a title="111-tfidf-4" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>5 0.066368088 <a title="111-tfidf-5" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>6 0.062612087 <a title="111-tfidf-6" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>7 0.05671921 <a title="111-tfidf-7" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>8 0.053068973 <a title="111-tfidf-8" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>9 0.051961489 <a title="111-tfidf-9" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>10 0.048558779 <a title="111-tfidf-10" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>11 0.047100939 <a title="111-tfidf-11" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>12 0.046920933 <a title="111-tfidf-12" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>13 0.046649013 <a title="111-tfidf-13" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>14 0.046166461 <a title="111-tfidf-14" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>15 0.043397598 <a title="111-tfidf-15" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>16 0.041361365 <a title="111-tfidf-16" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>17 0.041163702 <a title="111-tfidf-17" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>18 0.040313199 <a title="111-tfidf-18" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>19 0.03831007 <a title="111-tfidf-19" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>20 0.03659622 <a title="111-tfidf-20" href="./nips-2010-Decoding_Ipsilateral_Finger_Movements_from_ECoG_Signals_in_Humans.html">57 nips-2010-Decoding Ipsilateral Finger Movements from ECoG Signals in Humans</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.096), (1, 0.043), (2, -0.134), (3, 0.005), (4, 0.025), (5, -0.017), (6, -0.003), (7, 0.013), (8, -0.037), (9, 0.017), (10, 0.031), (11, -0.042), (12, 0.019), (13, -0.081), (14, -0.066), (15, -0.011), (16, -0.003), (17, -0.01), (18, -0.045), (19, -0.035), (20, 0.055), (21, 0.038), (22, 0.016), (23, 0.018), (24, 0.037), (25, 0.095), (26, -0.032), (27, 0.009), (28, 0.027), (29, -0.014), (30, -0.009), (31, -0.034), (32, 0.02), (33, 0.02), (34, 0.011), (35, 0.017), (36, -0.044), (37, -0.0), (38, -0.028), (39, -0.025), (40, -0.012), (41, 0.039), (42, -0.025), (43, -0.009), (44, -0.012), (45, -0.119), (46, 0.031), (47, 0.025), (48, -0.033), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9192543 <a title="111-lsi-1" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>Author: Peggy Series, David P. Reichert, Amos J. Storkey</p><p>Abstract: The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative ﬁndings in CBS. We also introduce a modiﬁcation to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception. 1</p><p>2 0.75073797 <a title="111-lsi-2" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>3 0.72409999 <a title="111-lsi-3" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>4 0.66176397 <a title="111-lsi-4" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>5 0.634193 <a title="111-lsi-5" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>6 0.57519102 <a title="111-lsi-6" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>7 0.57283646 <a title="111-lsi-7" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>8 0.52199984 <a title="111-lsi-8" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>9 0.50524688 <a title="111-lsi-9" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>10 0.49499634 <a title="111-lsi-10" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>11 0.48252547 <a title="111-lsi-11" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>12 0.47406009 <a title="111-lsi-12" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>13 0.47313485 <a title="111-lsi-13" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>14 0.45015883 <a title="111-lsi-14" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>15 0.44886872 <a title="111-lsi-15" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>16 0.44338861 <a title="111-lsi-16" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>17 0.43278164 <a title="111-lsi-17" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>18 0.42585605 <a title="111-lsi-18" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>19 0.41424638 <a title="111-lsi-19" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>20 0.3934139 <a title="111-lsi-20" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.029), (17, 0.013), (27, 0.116), (30, 0.038), (35, 0.032), (45, 0.135), (50, 0.04), (52, 0.062), (54, 0.324), (60, 0.029), (77, 0.046), (90, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73720777 <a title="111-lda-1" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>Author: Peggy Series, David P. Reichert, Amos J. Storkey</p><p>Abstract: The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative ﬁndings in CBS. We also introduce a modiﬁcation to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception. 1</p><p>2 0.65462828 <a title="111-lda-2" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>Author: Seunghak Lee, Jun Zhu, Eric P. Xing</p><p>Abstract: To understand the relationship between genomic variations among population and complex diseases, it is essential to detect eQTLs which are associated with phenotypic effects. However, detecting eQTLs remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples. Thus, to address the problem, it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites. In this paper, we propose a novel regularized regression approach for detecting eQTLs which takes into account related traits simultaneously while incorporating many regulatory features. We ﬁrst present a Bayesian network for a multi-task learning problem that includes priors on SNPs, making it possible to estimate the signiﬁcance of each covariate adaptively. Then we ﬁnd the maximum a posteriori (MAP) estimation of regression coefﬁcients and estimate weights of covariates jointly. This optimization procedure is efﬁcient since it can be achieved by using a projected gradient descent and a coordinate descent procedure iteratively. Experimental results on simulated and real yeast datasets conﬁrm that our model outperforms previous methods for ﬁnding eQTLs.</p><p>3 0.62501866 <a title="111-lda-3" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>Author: Ping Li, Arnd Konig, Wenhao Gui</p><p>Abstract: Computing1 two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b ≥ 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simpliﬁed estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance. 1</p><p>4 0.52210134 <a title="111-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.52109534 <a title="111-lda-5" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>6 0.51561266 <a title="111-lda-6" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>7 0.50621992 <a title="111-lda-7" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>8 0.50549906 <a title="111-lda-8" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>9 0.50543618 <a title="111-lda-9" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>10 0.50487834 <a title="111-lda-10" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>11 0.50287974 <a title="111-lda-11" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>12 0.50177902 <a title="111-lda-12" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>13 0.50139165 <a title="111-lda-13" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>14 0.49975988 <a title="111-lda-14" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>15 0.49973705 <a title="111-lda-15" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>16 0.49940479 <a title="111-lda-16" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>17 0.49920851 <a title="111-lda-17" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>18 0.49860001 <a title="111-lda-18" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>19 0.49761641 <a title="111-lda-19" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>20 0.49657276 <a title="111-lda-20" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
