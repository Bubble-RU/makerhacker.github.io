<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 nips-2010-Learning To Count Objects in Images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-149" href="#">nips2010-149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 nips-2010-Learning To Count Objects in Images</h1>
<br/><p>Source: <a title="nips-2010-149-pdf" href="http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images.pdf">pdf</a></p><p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>Reference: <a title="nips-2010-149-reference" href="../nips2010_reference/nips-2010-Learning_To_Count_Objects_in_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). [sent-2, score-0.417]
</p><p>2 Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. [sent-5, score-0.729]
</p><p>3 Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. [sent-10, score-0.248]
</p><p>4 1  Introduction  The counting problem is the estimation of the number of objects in a still image or video frame. [sent-11, score-0.745]
</p><p>5 It arises in many real-world applications including cell counting in microscopic images, monitoring crowds in surveillance systems, and performing wildlife census or counting the number of trees in an aerial image of a forest. [sent-12, score-1.182]
</p><p>6 Arguably, the bare minimum of annotation is to provide the overall count of objects in each training image. [sent-15, score-0.471]
</p><p>7 This paper focusses on the next level of annotation which is to specify the object position by putting a single dot on each object instance in each image. [sent-16, score-0.432]
</p><p>8 Figure 1 gives examples of the counting problems and the dotted annotation we consider. [sent-17, score-0.613]
</p><p>9 Dotting (pointing) is the natural way to count objects for humans, at least when the number of objects is large. [sent-18, score-0.346]
</p><p>10 It may be argued therefore that providing dotted annotations for the training images is no harder for a human than giving just the raw counts. [sent-19, score-0.38]
</p><p>11 On the other hand, a spatial arrangement of the dots provides a wealth of additional information, and this paper is, in part, about how to exploit this “free lunch” (in the context of the counting problem). [sent-20, score-0.5]
</p><p>12 This paper develops a simple and general discriminative learning-based framework for counting objects in images. [sent-23, score-0.554]
</p><p>13 The high-level idea of our approach is extremely simple: given an image I, our goal is to recover a density function F as a real function of pixels in this image. [sent-26, score-0.391]
</p><p>14 Our notion of density function loosely 1  Figure 1: Examples of counting problems. [sent-27, score-0.548]
</p><p>15 Left — counting bacterial cells in a ﬂuorescence-light microscopy image (from [29]), right — counting people in a surveillance video frame (from [10]). [sent-28, score-1.403]
</p><p>16 Our framework learns to estimate the number of objects in the previously unseen images based on a set of training images of the same kind augmented with dotted annotations. [sent-31, score-0.637]
</p><p>17 Given the estimate F of the density function and the query about the number of objects in the entire image I, the number of objects in the image is estimated by integrating F over the entire I. [sent-33, score-0.873]
</p><p>18 Furthermore, integrating the density over an image subregion S ⊂ I gives an estimate of the count of objects in that subregion. [sent-34, score-0.55]
</p><p>19 Our approach assumes that each pixel p in an image is represented by a feature vector xp and models the density function as a linear transformation of xp : F (p) = wT xp . [sent-35, score-0.745]
</p><p>20 Given a set of training images, the parameter vector w is learnt in the regularized risk framework, so that the density function estimates for the training images matches the ground truth densities inferred from the user annotations (under regularization on w). [sent-36, score-0.943]
</p><p>21 The key conceptual difﬁculty with the density function is the discrete nature of both image observations (pixel grid) and, in particular, the user training annotation (sparse set of dots). [sent-37, score-0.577]
</p><p>22 As a result, while it is easy to reason about average densities over the extended image regions (e. [sent-38, score-0.296]
</p><p>23 the whole image), the notion of density is not well-deﬁned at a pixel level. [sent-40, score-0.317]
</p><p>24 Thus, given a set of dotted annotation there is no trivial answer to the question: what should be the ground truth density for this training example. [sent-41, score-0.733]
</p><p>25 Our main contribution, addressing this conceptual difﬁculty, is a speciﬁc distance metric D between density functions used as a loss in our framework, which we call the MESA distance (where MESA stands for Maximum Excess over SubArrays, as well as for the geological term for the elevated plateau). [sent-43, score-0.417]
</p><p>26 Thus, it does not matter much how exactly we deﬁne the ground truth density locally, as long as the integrals of the ground truth density over the larger regions reﬂect the counts correctly. [sent-47, score-0.988]
</p><p>27 We can then naturally deﬁne the “ground truth” density for a dotted annotation to be a sum of normalized gaussians centered at the dots. [sent-48, score-0.383]
</p><p>28 As virtually no assumptions is made about the features xp , our framework can beneﬁt from much of the research on good features for object detection. [sent-55, score-0.242]
</p><p>29 A number of approaches tackle counting problems in an unsupervised way, performing grouping based on self-similarities [3] or motion similarities [27]. [sent-59, score-0.389]
</p><p>30 However, the counting accuracy of such fully unsupervised methods is limited, and therefore others considered approaches based on supervised learning. [sent-60, score-0.389]
</p><p>31 (MATLAB jet colormap is used) Counting by detection: This assumes the use of a visual object detector, that localizes individual object instances in the image. [sent-71, score-0.346]
</p><p>32 Given the localizations of all instances, counting becomes trivial. [sent-72, score-0.389]
</p><p>33 In particular, most current object detectors operate in two stages: ﬁrst producing a real-valued conﬁdence map; and second, given such a map, a further thresholding and non-maximum suppression steps are needed to locate peaks correspoinding to individual instances [12, 26]. [sent-74, score-0.289]
</p><p>34 More generative approaches avoid nonmaximum suppression by reasoning about relations between object parts and instances [6, 14, 20, 33, 34], but they are still geared towards a situation with a small number of objects in images and require time-consuming inference. [sent-75, score-0.505]
</p><p>35 Instead, a direct mapping from some global image characteristics (mainly histograms of various features) to the number of objects is learned. [sent-79, score-0.313]
</p><p>36 As a result, a large number of training images with the supplied counts needs to be provided during training. [sent-84, score-0.292]
</p><p>37 Finally, counting by segmentation methods [10, 28] can be regarded as hybrids of counting-by-detection and counting-by-regression approaches. [sent-85, score-0.439]
</p><p>38 They segment the objects into separate clusters and then regress from the global properties of each cluster to the overall number of objects in it. [sent-86, score-0.348]
</p><p>39 It is also assumed that each pixel p in each image Ii is associated with a real-valued feature vector xi ∈ RK . [sent-93, score-0.337]
</p><p>40 It is ﬁnally assumed that each training image Ii is annotated with a set of 2D points Pi = {P1 , . [sent-95, score-0.306]
</p><p>41 The density functions in our approaches are real-valued functions over pixel grids, whose integrals over image regions should match the object counts. [sent-99, score-0.673]
</p><p>42 For a training image Ii , we deﬁne the ground truth density function to be a kernel density estimate based on the provided points: ∀p ∈ Ii ,  Fi0 (p) =  N (p; P, σ 2 12×2 ) . [sent-100, score-0.847]
</p><p>43 With this deﬁnition, the sum of the ground truth density p∈Ii Fi0 (p) over the entire image will not match the dot count Ci exactly, as dots that lie very close to the image boundary result in their Gaussian probability mass being partly outside the image. [sent-102, score-1.066]
</p><p>44 This is a natural and desirable 3  behaviour for most applications, as in many cases an object that lies partly outside the image boundary should not be counted as a full object, but rather as a fraction of an object. [sent-103, score-0.307]
</p><p>45 After the optimal weight vector has been learned from the training data, the system can produce a density estimate for an unseen image I by a simple linear weighting of the feature vector computed in each pixel as suggested by (2). [sent-107, score-0.607]
</p><p>46 2  The MESA distance  The distance D in (3) measures the mismatch between the ground truth and the estimated densities (the loss) and has a signiﬁcant impact on the performance of the entire learning framework. [sent-110, score-0.596]
</p><p>47 support vector regression and ridge regression for L1 and L2 cases respectively), where each pixel in each training image effectively provides a sample in the 2 training set. [sent-116, score-0.658]
</p><p>48 • As the overall counts is what we ultimately care about, one may choose D to be an absolute or squared difference between the overall sums over the entire images for the two arguments, e. [sent-123, score-0.464]
</p><p>49 Once again, we get either the support vector regression (for the absolute differences) or ridge regression (for the squared differences), but now each training sample corresponds to the entire training image. [sent-128, score-0.436]
</p><p>50 Thus, although this choice of the loss matches our ultimate goal of learning to count very well, it requires many annotated images for training as spatial information in the annotation is discarded. [sent-129, score-0.528]
</p><p>51 The MESA distance (in fact, a metric) can be regarded as an L∞ distance between combinatorially-long vectors of subarray sums. [sent-132, score-0.317]
</p><p>52 4  original  noise added  σ increased  dots jittered  dots removed dots reshufﬂed  Figure 3: Comparison of distances for matching density functions. [sent-134, score-0.521]
</p><p>53 Here, the top-left image shows one of the densities, computed as the ground truth density for a set of dots. [sent-135, score-0.609]
</p><p>54 In the bottom row, we compare side-by-side the per-pixel L1 distance, the absolute difference of overall counts, and the MESA distance between the original and the perturbed densities (the distances are normalized across the 5 examples). [sent-137, score-0.348]
</p><p>55 In the middle row we give per-pixel plots of the differences between the respective densities and show the boxes on which the maxima in the deﬁnition of the MESA distance are achieved. [sent-139, score-0.272]
</p><p>56 Firstly, it is directly related to the counting objective we want to optimize. [sent-141, score-0.389]
</p><p>57 Since the set of all subarrays include the full image, DMESA (F1 , F2 ) is an upper bound on the absolute difference of the overall count estimates given by the two densities F1 and F2 . [sent-142, score-0.577]
</p><p>58 Secondly, when the two density functions differ by a zero-mean high-frequency signal or an independent zero-mean noise, the DMESA distance between them is small, because positive and negative deviations of F1 from F2 pixels tend to cancel each other over the large regions. [sent-143, score-0.294]
</p><p>59 F1 and F2 are the ground truth densities corresponding to the two point sets leaning towards two different corners of the image, then the DMESA distance between F1 and F2 is large, even if F1 and F2 sum to the same counts over the entire image. [sent-147, score-0.593]
</p><p>60 (5)  p∈B  Computing both inner maxima in (5) then constitutes a 2D maximum subarray problem, which is ﬁnding the box subarray of a given 2D array with the largest sum. [sent-151, score-0.341]
</p><p>61 ξN  ∀i, ∀B ∈ Bi :  wT w + λ  ξi ,  Fi0 (p) − wT xi , p  ξi ≥  subject to  (6)  i=1  wT xi − Fi0 (p) p  ξi ≥  (7)  p∈B  p∈B  Here, ξi are the auxiliary slack variables (one for each training image) and Bi is the set of all subarrays in image i. [sent-166, score-0.52]
</p><p>62 j ξN after the iteration j, one can ﬁnd the box subarrays corresponding to the most violated constraints among (7). [sent-174, score-0.297]
</p><p>63 To do that, for each image we ﬁnd the subarrays that maximize the right hand sides of (7), which are exactly the 2D maximum subarrays of Fi0 (·) − Fi (·|j w) and Fi (·|j w) − Fi0 (·) respectively. [sent-175, score-0.703]
</p><p>64 1 2 The boxes j Bi and j Bi corresponding to these maximum subarrays are then found for each image i. [sent-176, score-0.478]
</p><p>65 The iterations terminate when for all images the sums corresponding to maximum subarrays are within (1 + ) factor from j ξi and hence no constraints are activated. [sent-178, score-0.434]
</p><p>66 3  Experiments  Our framework and several baselines were evaluated on counting tasks for two types of imagery shown in Figure 1. [sent-181, score-0.455]
</p><p>67 For the experiments, we generated a dataset of images (available at [1]), with the overall number of cells varying between 74 and 317. [sent-187, score-0.259]
</p><p>68 Few annotated datasets with real cell microscopy images also exist. [sent-188, score-0.37]
</p><p>69 While it is tempting to use real rather than synthetic imagery, all the real image datasets to the best of our knowledge are small (only few images have annotations), and, most importantly, there always are very big discrepancies between the annotations of different human experts. [sent-189, score-0.387]
</p><p>70 The latter effectively invalidates the use of such real datasets for quantitative comparison of different counting approaches. [sent-190, score-0.389]
</p><p>71 Below we discuss the comparison of the counting accuracy achieved by our approach and baseline approaches. [sent-191, score-0.425]
</p><p>72 The features used in all approaches were based on the dense SIFT descriptor [21] computing using [32] software at each pixel of each image with the ﬁxed SIFT frame radius (about the size of the cell) and ﬁxed orientation. [sent-192, score-0.439]
</p><p>73 Each algorithm was trained on N training images, while another N images were used for the validation of metaparameters. [sent-193, score-0.279]
</p><p>74 Then each pixel is represented by a vector of length K, which is 1 at the dimension corresponding to the entry of the SIFT descriptor at that pixel and 0 for all other dimensions. [sent-197, score-0.316]
</p><p>75 6  linear ridge regression kernel ridge regression detection detection detection+correction density learning density learning  Validation counting counting counting detection counting counting MESA  N =1 67. [sent-202, score-2.771]
</p><p>76 2  Table 1: Mean absolute errors for cell counting on the test set of 100 ﬂuorescent microscopy images. [sent-284, score-0.648]
</p><p>77 The last 6 columns correspond to the numbers of images in the training and validation sets. [sent-287, score-0.279]
</p><p>78 Standard deviations in the table correspond to 5 different draws of training and validation image sets. [sent-289, score-0.324]
</p><p>79 59  Table 2: Mean absolute errors for people counting in the surveillance video [10]. [sent-311, score-0.625]
</p><p>80 Each of the training images was described by a global histogram of the entries occurrences for the same codebook as above. [sent-316, score-0.26]
</p><p>81 The slope and the intercept of the correction for each combination of τ , ρ, and regularization strength were estimated via robust regression on the union of the training and validation sets. [sent-327, score-0.247]
</p><p>82 The counting algorithm here is based on adaptive thresholding and morphological analysis. [sent-331, score-0.418]
</p><p>83 The objective minimized during the validation was counting accuracy. [sent-335, score-0.455]
</p><p>84 For counting-by-detection, we also considered optimizing detection accuracy (computed via Hungarian matching with the ground truth), and, for our approach, we also considered minimizing the MESA distance with the ground truth density on the validation set. [sent-336, score-0.809]
</p><p>85 The results for a different number N of training and validation images are given in Table 1, based on 5 random draws of training and validation sets. [sent-337, score-0.424]
</p><p>86 The authors of [10] also provided the dotted ground truth for these frames, the position of the ground plane, and the region of interest, where the counts should be performed. [sent-342, score-0.582]
</p><p>87 Thus, we ﬁrst extracted the primary features in each pixel including the absolute differences with the previous frame and the background, the image intensity, and the absolute values x- and y-derivatives. [sent-354, score-0.553]
</p><p>88 The training objective was the regression from the appearance of each pixel and its neighborhood to the ground truth density. [sent-356, score-0.555]
</p><p>89 Given the pretrained forest, each pixel p gets assigned a vector xp of dimension equal to the total number of leaves in all trees, with ones corresponding to the leaves in each of the ﬁve trees the pixel falls into and zeros otherwise. [sent-358, score-0.399]
</p><p>90 Finally, to account for the perspective distortion, we multiplied xp by the square of the depth of the ground plane at p (provided with the sequence). [sent-359, score-0.249]
</p><p>91 Within each scenario, we allocated one-ﬁfth of the training frames to pick λ and the tree depth through validation via the MESA distance. [sent-360, score-0.246]
</p><p>92 In both sets of experiments, we tried two strategies for setting σ (kernel width in the deﬁnition of the ground truth densities): setting σ = 0 (effectively, the ground truth is then a sum of delta-functions), and setting σ = 4 (roughly comparable with object half-size in both experiments). [sent-363, score-0.67]
</p><p>93 in the case of pedestrians, one can store the value wt computed during learning at each leaf t in each tree, so that counting would require simply “pushing” each pixel down the forest, and summing the resulting wt from the obtained leaves. [sent-374, score-0.689]
</p><p>94 4  Conclusion  We have presented the general framework for learning to count objects in images. [sent-376, score-0.243]
</p><p>95 While our ultimate goal is the counting accuracy over the entire image, during the learning our approach is optimizing the loss based on the MESA-distance. [sent-377, score-0.491]
</p><p>96 This loss involves counting accuracy over multiple subarrays of the entire image (and not only the entire image itself). [sent-378, score-1.126]
</p><p>97 We demonstrate that given limited amount of training data, such an approach achieves much higher accuracy than optimizing the counting accuracy over the entire image directly (counting-by-regression). [sent-379, score-0.691]
</p><p>98 On the detection of multiple object instances using Hough transforms. [sent-421, score-0.269]
</p><p>99 Computational framework for simulating ﬂuorescence microscope images with cell populations. [sent-524, score-0.288]
</p><p>100 Software for quantiﬁcation of labeled bacteria from digital microscope images by automated image analysis. [sent-598, score-0.357]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('counting', 0.389), ('mesa', 0.35), ('subarrays', 0.262), ('dmesa', 0.219), ('image', 0.179), ('density', 0.159), ('pixel', 0.158), ('subarray', 0.153), ('ground', 0.139), ('images', 0.134), ('objects', 0.134), ('truth', 0.132), ('annotation', 0.131), ('crowd', 0.131), ('object', 0.128), ('densities', 0.117), ('dots', 0.111), ('microscopy', 0.109), ('frames', 0.101), ('dotted', 0.093), ('detection', 0.092), ('surveillance', 0.088), ('xp', 0.083), ('distance', 0.082), ('cell', 0.079), ('training', 0.079), ('counts', 0.079), ('count', 0.078), ('cells', 0.076), ('annotations', 0.074), ('wt', 0.071), ('absolute', 0.071), ('crowded', 0.07), ('ridge', 0.069), ('sift', 0.069), ('validation', 0.066), ('downscale', 0.066), ('upscale', 0.066), ('forest', 0.063), ('suppression', 0.06), ('bacterial', 0.058), ('microscopic', 0.058), ('correction', 0.055), ('pixels', 0.053), ('peaks', 0.052), ('segmentation', 0.05), ('fi', 0.05), ('integrals', 0.049), ('instances', 0.049), ('overall', 0.049), ('annotated', 0.048), ('lempitsky', 0.047), ('codebook', 0.047), ('pedestrians', 0.047), ('regression', 0.047), ('dot', 0.045), ('entire', 0.044), ('dotting', 0.044), ('microscope', 0.044), ('pearls', 0.044), ('selinummi', 0.044), ('video', 0.043), ('cvpr', 0.042), ('perturbations', 0.042), ('visual', 0.041), ('delaunay', 0.038), ('uorescence', 0.038), ('sums', 0.038), ('frame', 0.038), ('boxes', 0.037), ('dense', 0.036), ('differences', 0.036), ('metric', 0.036), ('baseline', 0.036), ('splits', 0.036), ('scenarios', 0.036), ('imagery', 0.035), ('box', 0.035), ('people', 0.034), ('descriptors', 0.034), ('hybrid', 0.033), ('unseen', 0.032), ('regress', 0.031), ('framework', 0.031), ('program', 0.03), ('risk', 0.03), ('detector', 0.03), ('dence', 0.03), ('distances', 0.029), ('bi', 0.029), ('loss', 0.029), ('ultimate', 0.029), ('morphological', 0.029), ('conceptual', 0.029), ('software', 0.028), ('ii', 0.028), ('rectangles', 0.028), ('iccv', 0.027), ('plane', 0.027), ('quadratic', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="149-tfidf-1" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>2 0.19621958 <a title="149-tfidf-2" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>3 0.18395422 <a title="149-tfidf-3" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>4 0.17824736 <a title="149-tfidf-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>5 0.15937486 <a title="149-tfidf-5" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>6 0.15141192 <a title="149-tfidf-6" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>7 0.1298144 <a title="149-tfidf-7" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>8 0.11489902 <a title="149-tfidf-8" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>9 0.10986864 <a title="149-tfidf-9" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>10 0.10921305 <a title="149-tfidf-10" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>11 0.099888101 <a title="149-tfidf-11" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>12 0.093685038 <a title="149-tfidf-12" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>13 0.090093948 <a title="149-tfidf-13" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>14 0.089953288 <a title="149-tfidf-14" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>15 0.086151406 <a title="149-tfidf-15" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>16 0.085586965 <a title="149-tfidf-16" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>17 0.083577633 <a title="149-tfidf-17" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>18 0.083104432 <a title="149-tfidf-18" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>19 0.078759052 <a title="149-tfidf-19" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>20 0.077274948 <a title="149-tfidf-20" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.242), (1, 0.115), (2, -0.163), (3, -0.211), (4, 0.03), (5, -0.041), (6, -0.048), (7, 0.025), (8, 0.06), (9, 0.015), (10, 0.005), (11, -0.0), (12, -0.082), (13, -0.002), (14, 0.049), (15, -0.015), (16, 0.048), (17, -0.078), (18, 0.125), (19, 0.034), (20, 0.003), (21, 0.003), (22, 0.032), (23, 0.005), (24, -0.019), (25, -0.067), (26, 0.046), (27, -0.026), (28, -0.03), (29, 0.013), (30, 0.005), (31, 0.015), (32, 0.021), (33, -0.079), (34, -0.082), (35, -0.034), (36, 0.041), (37, -0.07), (38, 0.084), (39, -0.074), (40, -0.043), (41, -0.019), (42, -0.027), (43, 0.058), (44, -0.061), (45, -0.032), (46, -0.008), (47, -0.016), (48, 0.082), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96041244 <a title="149-lsi-1" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>2 0.82512414 <a title="149-lsi-2" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>Author: Nebojsa Jojic, Alessandro Perina, Vittorio Murino</p><p>Abstract: In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds. The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our ﬁrst goal is to create a visual summary of the subject’s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g., Photosynth) or appearance-based clustering models (e.g., the epitome), is impractical due to either the large dataset size or the dramatic variations in the lighting conditions. As a remedy to these problems, we introduce a novel image representation, the ”structural element (stel) epitome,” and an associated efﬁcient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T which, as in previous epitome models, deﬁnes a mapping between the image coordinates and the coordinates in the large ”all-I-have-seen” epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap which indicates image similarity. However, the image similarity no longer depends on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial conﬁguration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination changes, that tend to uniformly affect pixels belonging to a single scene or object part. 1</p><p>3 0.81854129 <a title="149-lsi-3" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>4 0.81025702 <a title="149-lsi-4" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>Author: Stefan Harmeling, Hirsch Michael, Bernhard Schölkopf</p><p>Abstract: Modelling camera shake as a space-invariant convolution simpliﬁes the problem of removing camera shake, but often insufﬁciently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In an effort to address these limitations, (i) we introduce a taxonomy of camera shakes, (ii) we build on a recently introduced framework for space-variant ﬁltering by Hirsch et al. and a fast algorithm for single image blind deconvolution for space-invariant ﬁlters by Cho and Lee to construct a method for blind deconvolution in the case of space-variant blur, and (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time recording the spacevariant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake, even without using additionally motion sensor information. 1</p><p>5 0.76139432 <a title="149-lsi-5" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>6 0.75863278 <a title="149-lsi-6" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>7 0.74718404 <a title="149-lsi-7" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>8 0.74069226 <a title="149-lsi-8" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>9 0.73787713 <a title="149-lsi-9" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>10 0.70023149 <a title="149-lsi-10" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>11 0.66230828 <a title="149-lsi-11" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>12 0.63555723 <a title="149-lsi-12" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>13 0.63555706 <a title="149-lsi-13" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>14 0.62750512 <a title="149-lsi-14" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>15 0.62407237 <a title="149-lsi-15" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>16 0.62157136 <a title="149-lsi-16" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>17 0.61927599 <a title="149-lsi-17" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>18 0.60254663 <a title="149-lsi-18" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>19 0.58644509 <a title="149-lsi-19" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>20 0.57436132 <a title="149-lsi-20" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.031), (17, 0.012), (27, 0.083), (30, 0.054), (35, 0.32), (45, 0.26), (50, 0.045), (52, 0.025), (60, 0.026), (77, 0.038), (78, 0.015), (90, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95777631 <a title="149-lda-1" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>2 0.91082734 <a title="149-lda-2" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>3 0.906829 <a title="149-lda-3" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>Author: Georg Langs, Yanmei Tie, Laura Rigolo, Alexandra Golby, Polina Golland</p><p>Abstract: Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difﬁcult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We ﬁrst embed each brain into a functional map that reﬂects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions. 1</p><p>4 0.9017005 <a title="149-lda-4" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><p>same-paper 5 0.84195751 <a title="149-lda-5" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>6 0.79485577 <a title="149-lda-6" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>7 0.78511906 <a title="149-lda-7" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>8 0.7693578 <a title="149-lda-8" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>9 0.76034445 <a title="149-lda-9" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>10 0.74767178 <a title="149-lda-10" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>11 0.74611497 <a title="149-lda-11" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>12 0.74469519 <a title="149-lda-12" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>13 0.74305558 <a title="149-lda-13" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>14 0.7413888 <a title="149-lda-14" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>15 0.74124789 <a title="149-lda-15" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>16 0.74048901 <a title="149-lda-16" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>17 0.73815763 <a title="149-lda-17" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>18 0.73721069 <a title="149-lda-18" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>19 0.72982109 <a title="149-lda-19" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>20 0.72812891 <a title="149-lda-20" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
