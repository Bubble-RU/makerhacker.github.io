<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-132" href="#">nips2010-132</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</h1>
<br/><p>Source: <a title="nips-2010-132-pdf" href="http://papers.nips.cc/paper/4148-joint-cascade-optimization-using-a-product-of-boosted-classifiers.pdf">pdf</a></p><p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>Reference: <a title="nips-2010-132-reference" href="../nips2010_reference/nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch  Abstract The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. [sent-5, score-0.815]
</p><p>2 The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. [sent-6, score-0.149]
</p><p>3 We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. [sent-7, score-0.942]
</p><p>4 We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. [sent-10, score-0.261]
</p><p>5 1  Introduction  Object detection remains one of the core objectives of computer vision, either as an objective per se, for instance for automatic focusing on faces in digital cameras, or as means to get high-level understanding of natural scenes for robotics and image retrieval. [sent-11, score-0.209]
</p><p>6 The computational cost of such approaches is controlled traditionally with a cascade, that is a succession of classiﬁers, each one being evaluated only if the previous ones in the sequence have not already rejected the candidate location. [sent-14, score-0.102]
</p><p>7 In its original form, this approach constructs classiﬁers one after another during training, each one from examples which have not been rejected by the previous ones. [sent-16, score-0.108]
</p><p>8 Finally the third drawback is the inability of a standard cascade to properly exploit 1  the trade-off between the different levels. [sent-21, score-0.686]
</p><p>9 A response marginally below threshold at a certain level is enough to reject a sample, even if classiﬁers at other levels have strong responses. [sent-22, score-0.128]
</p><p>10 At a more conceptual level, standard training for cascades does not allow the classiﬁers to exploit their joint modeling: Each classiﬁer is trained as if it has to do the job alone, without having the opportunity to properly balance its own modeling effort and that of the other classiﬁers. [sent-23, score-0.221]
</p><p>11 We interpret the individual responses of the classiﬁers as probabilities of responding positively, and deﬁne the overall response of the cascade as the probability of all the classiﬁers responding positively under an assumption of independence. [sent-25, score-0.863]
</p><p>12 This noisy-AND model leads to a very simple criterion for a new Boosting procedure, which improves all the classiﬁers symmetrically on the positive samples, and focuses on improving the classiﬁer with the best response on every negative sample. [sent-27, score-0.217]
</p><p>13 We demonstrate the efﬁciency of this technique for face and pedestrian detection. [sent-28, score-0.178]
</p><p>14 Experiments show that this joint cascade learning requires far less negative training examples, and achieves performance better than standard cascades without the need for intensive bootstrapping. [sent-29, score-0.922]
</p><p>15 The idea common to these approaches is to rely on a form of adaptive testing : only candidates which cannot be trivially rejected as not being the object of interest will require heavy computation. [sent-32, score-0.147]
</p><p>16 1  Reducing object detection computational cost  Heisele et al. [sent-35, score-0.168]
</p><p>17 [1] propose a hierarchy of linear Support Vector Machines, each trained on images of increasing resolution, to weed out background patches, followed by a ﬁnal computationally intensive polynomial SVM. [sent-36, score-0.137]
</p><p>18 Fleuret and Geman [5] introduce a hierarchy of classiﬁers dedicated to positive populations with geometrical poses of decreasing randomness. [sent-40, score-0.1]
</p><p>19 This approach generalizes the cascade to more complex pose spaces, but as for cascades, trains the classiﬁers separately. [sent-41, score-0.667]
</p><p>20 In [6] a branch and bound approach is utilized during scanning, while in [7] a divide and conquer approach is proposed, wherein regions in the image are either accepted or rejected as a whole or split and further processed. [sent-43, score-0.124]
</p><p>21 The most popular approach however, for both its conceptual simplicity and practical efﬁciency, is the attentional cascade proposed by Viola and Jones [10]. [sent-45, score-0.715]
</p><p>22 2  Improving attentional cascades  In recent years approaches have been proposed that address some of the issues we list in the introduction. [sent-48, score-0.121]
</p><p>23 In [14] the authors train a cascade with a global performance criteria and a single set of parameters common to all stages. [sent-49, score-0.729]
</p><p>24 In [15] the authors address the asymmetric nature of the stage goals via a biased minimax probability machine, while in [16] the authors formulate the stage goals as a constrained optimization problem. [sent-50, score-0.236]
</p><p>25 In [17] a alternate boosting method dubbed FloatBoost is proposed. [sent-51, score-0.103]
</p><p>26 During training, fk (x) stands for that response after t steps of Boosting. [sent-59, score-0.17]
</p><p>27 1 pk (x) = 1+exp(−fk (x)) probability of classiﬁer k to response positively on x. [sent-60, score-0.181]
</p><p>28 During training, t pt (x) stands for the same value after t steps of Boosting, computed from fk (x). [sent-61, score-0.23]
</p><p>29 During training, pt (x) is that value after only t steps of Boosting, computed from the pt (x). [sent-63, score-0.298]
</p><p>30 k  Sochman and Matas [18] presented a Boosting algorithm based on sequential probability ratio tests, minimizing the average evaluation time subject to upper bounds on the false negative and false positive rates. [sent-64, score-0.182]
</p><p>31 A general framework for probabilistic boosting trees (of which cascades are a degenerated case) was proposed in [19]. [sent-65, score-0.2]
</p><p>32 In all these methods however, a set of free parameters concerning detection and false alarm performances must be set during training. [sent-66, score-0.167]
</p><p>33 The authors in [20] use the output of each stage as an initial weak classiﬁer of the boosting classiﬁer in the next stage. [sent-68, score-0.229]
</p><p>34 This allows the cascade to retain information between stages. [sent-69, score-0.667]
</p><p>35 No information concerning the future performance of the cascade is available to each stage. [sent-71, score-0.716]
</p><p>36 In [21] sample traces are utilized to keep track of the performance of the cascade on the training data, and thresholds are picked after the cascade training is ﬁnished. [sent-72, score-1.498]
</p><p>37 However besides a validation set, a large number of negative examples must also be bootstrapped not only during the training phase, but also during the post-processing step of threshold and order calibration. [sent-74, score-0.176]
</p><p>38 In [22] the authors attempt to jointly optimize a cascade of SVMs. [sent-77, score-0.727]
</p><p>39 As can be seen, a cascade effectively performs an AND operation over the data, enforcing that a positive example passes all stages; and that a negative example be rejected by at least one stage. [sent-78, score-0.842]
</p><p>40 In order to simulate this behavior, the authors attempt to minimize the maximum hinge loss over the SVMs for the positive examples, and to minimize the product of the hinge losses for the negative examples. [sent-79, score-0.134]
</p><p>41 In [23] the authors present a method similar to ours, jointly optimizing a cascade using the product of the output of individual logistic regression base classiﬁers. [sent-81, score-0.727]
</p><p>42 As is the case with the work in [22], the authors consider the ordering of the stages a priori ﬁxed. [sent-83, score-0.134]
</p><p>43 3  Method  Our approach can be interpreted as a noisy-AND: The classiﬁers in the cascade produce stochastic Boolean predictions, conditionally independent given the signal to classify. [sent-84, score-0.667]
</p><p>44 We deﬁne the global response of the cascade as the probability that all these predictions are positive. [sent-85, score-0.779]
</p><p>45 This can be interpreted as if we were ﬁrst computing from the signal x, for each classiﬁer in the cascade, a probability pk (x), and deﬁning the response of the cascade as the probability that K independent Bernoulli variables of parameters p1 (x), . [sent-86, score-0.816]
</p><p>46 However, their approach aims at decomposing a complex population into a collection of homogeneous populations, while our objective is to speed up the computation for the detection of a homogeneous 3  population. [sent-92, score-0.108]
</p><p>47 1  Formalization  Let fk (x) stand for the non-thresholded response of the classiﬁer at level k of the cascade. [sent-95, score-0.17]
</p><p>48 From that, we deﬁne the ﬁnal output of the cascade as the probability that all classiﬁers make positive predictions, under the assumption that they are conditionally independent, given x K  p(x) =  pk (x). [sent-97, score-0.755]
</p><p>49 Conversely the example will be classiﬁed as negative if pk (x) = 0 for at least one k. [sent-99, score-0.125]
</p><p>50 In order to train our cascade we consider the maximization of the joint maximum log likelihood of the data: y  p(xn ) n (1 − p(xn ))1−yn . [sent-107, score-0.712]
</p><p>51 k ∂fk (xn ) 1 − pt (xn )  (6)  k,t It should be noted that in this formulation, the weight wn are signed, and these assigned to negative examples are negative. [sent-110, score-0.317]
</p><p>52 k,t In the case of a positive example xn this simpliﬁes to wn = 1 − pt (xn ) and thus this criterion k pushes every classiﬁer in the cascade to maximize the response on positive samples, irrespective of the performance of the overall cascade. [sent-111, score-1.203]
</p><p>53 t  −p k,t In the case of a negative example however, the weight update rule becomes wn = 1−pt(xn )) (1 − (xn pt (xn )), each classiﬁer in the cascade is then passed information regarding the overall performance k −pt via the term 1−pt(xn )) . [sent-112, score-0.985]
</p><p>54 If the cascade is already rejecting the negative example, then this term (xn becomes 0 and the classiﬁer ignores its performance on the speciﬁc example. [sent-113, score-0.732]
</p><p>55 On the other hand, if the cascade is performing poorly, then the term becomes increasingly large and the classiﬁers put large weights on that example. [sent-114, score-0.667]
</p><p>56 Furthermore, due to the term 1 − pt (xn ), each classiﬁer puts larger weight on negative examples k that it is already performing well on, effectively partitioning the space of negative examples. [sent-115, score-0.305]
</p><p>57 1  Implementation Details  We comparatively evaluate the proposed cascade framework on two data-sets. [sent-122, score-0.667]
</p><p>58 In [10] the authors present an initial comparison between their cascade framework and an AdaBoost classiﬁer on the CMU-MIT data-set. [sent-123, score-0.708]
</p><p>59 They train the monolithic classiﬁer for 200 rounds and compare it against a simple cascade containing ten stages, each with 20 weak learners. [sent-124, score-0.766]
</p><p>60 As cascade architecture plays an important role in the ﬁnal performance of the cascade, and in order to avoid any issues in the comparison pertaining to architectural designs, we keep this structure and evaluate both the proposed cascade and the Viola and Jones cascade, using this architecture. [sent-125, score-1.334]
</p><p>61 During the training, the thresholds for each stage in the Viola and Jones cascade are set to achieve a 99. [sent-127, score-0.755]
</p><p>62 We experimented with bootstrapping a ﬁxed number M of negative examples at ﬁxed intervals, similar to [21] and attained higher performance than the one presented here. [sent-130, score-0.143]
</p><p>63 2, JointCascade Augmented is the same, but is trained with as many negative examples as the total number used by the Viola and Jones cascade, and JointCascade Exponential uses the same number of negative samples as the basic setting, but uses the exponential version of the loss described in § 3. [sent-133, score-0.225]
</p><p>64 1  Data-Sets Pedestrians  For pedestrian detection we use the INRIA pedestrian data-set [25], which contains pedestrian images of various poses with high variance concerning background and lighting. [sent-138, score-0.564]
</p><p>65 The training set consists of 1239 images of pedestrians as positive examples, and 12180 negative examples, mined from 1218 pedestrian-free images. [sent-139, score-0.308]
</p><p>66 Of these we keep 900 images for training (together with their mirror images, for a total of 1800) and 9000 negative examples. [sent-140, score-0.209]
</p><p>67 The remaining images in the original training set are put aside to be used as a validation set by the Viola and Jones cascade. [sent-141, score-0.149]
</p><p>68 The trained classiﬁers are then tested on a test set composed of 1126 images of pedestrians and 18120 non-pedestrian images. [sent-144, score-0.158]
</p><p>69 For training we use the same data-set as that used by Viola and Jones consisting of 4916 images of faces. [sent-149, score-0.112]
</p><p>70 Of these we use 4000 (plus their mirror images) for training and set apart a further 916 (plus mirror images) for use as the validation set needed by the classical cascade approach. [sent-150, score-0.836]
</p><p>71 The negative portion of the training set is comprised of 10000 non-face images, mined randomly from non-face containing images. [sent-151, score-0.15]
</p><p>72 In order to test the trained classiﬁers, we extract the 507 faces in the data-set and scale-normalize to 24x24 images, a further 12700 non-face image patches are extracted from the background of the images in the data-set. [sent-152, score-0.191]
</p><p>73 3  Bootstrap Images  As, during training, the Viola and Jones cascade needs to bootstrap false positive examples after each stage, we randomly mine a data-set of approximately 7000 images from the web. [sent-156, score-0.844]
</p><p>74 These images have been manually inspected to ensure that they do not contain either faces or pedestrians. [sent-157, score-0.14]
</p><p>75 These images are used for bootstrapping in both sets of experiments. [sent-158, score-0.116]
</p><p>76 3  Error rate  The evaluation on the face data-set can be seen in Figure 1. [sent-160, score-0.11]
</p><p>77 The ROC curves for the pedestrian detection task can be seen in Figure 2. [sent-164, score-0.197]
</p><p>78 4  Optimization of the evaluation order  As stated, one of the main motivations for using cascades is speed. [sent-171, score-0.116]
</p><p>79 We compare the average number of stages visited per negative example for the various methods presented. [sent-172, score-0.135]
</p><p>80 Typically in cascade training, the thresholds and orders of the various stages must be determined during training, either by setting them in an ad hoc manner or by using one of the optimization schemes of the many proposed. [sent-173, score-0.786]
</p><p>81 In our case however, any decision concerning the thresholds as well as the ordering of the stages can be postponed till after training. [sent-174, score-0.191]
</p><p>82 It is easy to derive for any given detection goal, a relevant threshold θ on the overall cascade responce. [sent-175, score-0.777]
</p><p>83 Subsequently the image patch will be rejected if the product of any subset of strong classiﬁers has a value smaller than θ. [sent-177, score-0.105]
</p><p>84 Based on this we use a greedy method to evaluate, using the original training set, the optimal order of classiﬁers as follows : Originally we chose as the ﬁrst stage in our cascade, the classiﬁer whose 6  Faces 1  True-positive rate  0. [sent-178, score-0.114]
</p><p>85 9  Non-cascade AdaBoost VJ cascade JointCascade JointCascade Augmented JointCascade Exponential  0. [sent-180, score-0.667]
</p><p>86 9  Non-cascade AdaBoost VJ cascade JointCascade JointCascade Augmented JointCascade Exponential  0. [sent-195, score-0.667]
</p><p>87 false-positive rate on the pedestrian data-set for the methods proposed, AdaBoost and the Viola and Jones type cascade. [sent-203, score-0.141]
</p><p>88 The JointCascade variants require marginally more operations at a ﬁxed rate on the pedestrian population, and marginally less on the faces except at very conservative rates. [sent-207, score-0.315]
</p><p>89 We then iteratively add to the order of the cascade, that classiﬁer which leads to a response smaller than θ for the most negative examples, when multiplied with the aggregated response of the stages already ordered in the cascade. [sent-250, score-0.313]
</p><p>90 As stated this ordering of the cascade stages is computed using the training set. [sent-251, score-0.808]
</p><p>91 We then measure the speed of our ordered cascade on the same test sets as above, as shown on Table 2. [sent-252, score-0.692]
</p><p>92 As can be seen, in the case of the face dataset, in almost all cases our approach is actually faster during scanning than the classical Viola and Jones approach. [sent-253, score-0.129]
</p><p>93 The speed of our JointCascade approach on the pedestrian data-set is marginally worst than that of Viola and Jones, which is due to the lower false-positive rates. [sent-255, score-0.178]
</p><p>94 5  Conclusion  We have presented a new criterion to train a cascade of classiﬁers in a joint manner. [sent-256, score-0.747]
</p><p>95 This approach has a clear probabilistic interpretation as a noisy-AND, and leads to a global decision criterion which avoids thresholding classiﬁers individually, and can exploit independence in the classiﬁer response amplitudes. [sent-257, score-0.124]
</p><p>96 Hierarchical classiﬁcation and feature reduction for fast face detection with support vector machines. [sent-274, score-0.147]
</p><p>97 Rapid object detection using a boosted cascade of simple features. [sent-314, score-0.851]
</p><p>98 Fast human detection using a cascade of histograms of oriented gradients. [sent-321, score-0.777]
</p><p>99 On the design of cascades of boosted ensembles for face detection. [sent-330, score-0.197]
</p><p>100 MCBoost: Multiple classiﬁer boosting for perceptual co-clustering of images and visual features. [sent-374, score-0.167]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cascade', 0.667), ('jointcascade', 0.387), ('viola', 0.198), ('jones', 0.17), ('ers', 0.162), ('pt', 0.149), ('classi', 0.139), ('pedestrian', 0.114), ('er', 0.104), ('xn', 0.103), ('boosting', 0.103), ('cascades', 0.097), ('response', 0.089), ('detection', 0.083), ('adaboost', 0.082), ('rejected', 0.082), ('fk', 0.081), ('wn', 0.077), ('faces', 0.076), ('cascaded', 0.07), ('stages', 0.07), ('pedestrians', 0.066), ('vision', 0.065), ('negative', 0.065), ('object', 0.065), ('face', 0.064), ('images', 0.064), ('pk', 0.06), ('fleuret', 0.055), ('bootstrapping', 0.052), ('thresholds', 0.049), ('concerning', 0.049), ('training', 0.048), ('recognition', 0.047), ('augmented', 0.047), ('weak', 0.046), ('scanning', 0.045), ('authors', 0.041), ('exponential', 0.041), ('stage', 0.039), ('marginally', 0.039), ('validation', 0.037), ('pattern', 0.037), ('heisele', 0.037), ('martigny', 0.037), ('mined', 0.037), ('sochman', 0.037), ('boosted', 0.036), ('criterion', 0.035), ('false', 0.035), ('floatboost', 0.032), ('brubaker', 0.032), ('idiap', 0.032), ('monolithic', 0.032), ('mullin', 0.032), ('positively', 0.032), ('mirror', 0.032), ('vj', 0.031), ('cyclic', 0.03), ('graf', 0.03), ('goals', 0.028), ('positive', 0.028), ('trained', 0.028), ('overall', 0.027), ('rate', 0.027), ('computer', 0.027), ('oriented', 0.027), ('sliding', 0.026), ('subwindow', 0.026), ('examples', 0.026), ('poses', 0.026), ('speed', 0.025), ('boolean', 0.025), ('yn', 0.025), ('attentional', 0.024), ('responding', 0.024), ('bootstrap', 0.024), ('switzerland', 0.024), ('concentrates', 0.024), ('conceptual', 0.024), ('joint', 0.024), ('hierarchy', 0.024), ('predictions', 0.023), ('ordering', 0.023), ('pages', 0.023), ('image', 0.023), ('populations', 0.022), ('par', 0.022), ('intensive', 0.021), ('conference', 0.021), ('train', 0.021), ('nal', 0.02), ('cost', 0.02), ('classical', 0.02), ('conservative', 0.02), ('asymmetric', 0.02), ('evaluation', 0.019), ('drawback', 0.019), ('utilized', 0.019), ('jointly', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="132-tfidf-1" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>2 0.47915101 <a title="132-tfidf-2" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>Author: Nuno Vasconcelos, Mohammad J. Saberian</p><p>Abstract: The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classiﬁcation and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classiﬁcation risk and complexity. It searches the space of cascade conﬁgurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems. 1</p><p>3 0.29392388 <a title="132-tfidf-3" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>Author: David Weiss, Benjamin Sapp, Ben Taskar</p><p>Abstract: For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but ﬁlter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the ﬁltering loss of the ensemble as a theoretical justiﬁcation of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We ﬁnd that our approach signiﬁcantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem. 1</p><p>4 0.2146039 <a title="132-tfidf-4" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>5 0.12679236 <a title="132-tfidf-5" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>6 0.12593226 <a title="132-tfidf-6" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>7 0.10471314 <a title="132-tfidf-7" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>8 0.10027369 <a title="132-tfidf-8" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>9 0.090492226 <a title="132-tfidf-9" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>10 0.090209231 <a title="132-tfidf-10" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>11 0.081411563 <a title="132-tfidf-11" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>12 0.079685077 <a title="132-tfidf-12" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>13 0.078404136 <a title="132-tfidf-13" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>14 0.069426507 <a title="132-tfidf-14" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>15 0.068382859 <a title="132-tfidf-15" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>16 0.067646869 <a title="132-tfidf-16" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>17 0.061558966 <a title="132-tfidf-17" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>18 0.055621423 <a title="132-tfidf-18" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>19 0.054876905 <a title="132-tfidf-19" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>20 0.05429725 <a title="132-tfidf-20" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.074), (2, -0.065), (3, -0.193), (4, 0.046), (5, 0.066), (6, -0.173), (7, -0.012), (8, 0.038), (9, 0.037), (10, -0.14), (11, 0.087), (12, 0.057), (13, 0.141), (14, -0.027), (15, 0.074), (16, 0.195), (17, 0.311), (18, -0.458), (19, 0.2), (20, -0.042), (21, -0.011), (22, -0.132), (23, 0.026), (24, -0.107), (25, 0.01), (26, -0.073), (27, -0.078), (28, -0.126), (29, -0.059), (30, -0.079), (31, -0.015), (32, 0.034), (33, 0.071), (34, -0.011), (35, -0.078), (36, -0.017), (37, 0.007), (38, 0.014), (39, 0.022), (40, 0.052), (41, -0.013), (42, -0.002), (43, -0.016), (44, -0.005), (45, 0.024), (46, -0.036), (47, 0.013), (48, 0.041), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94957185 <a title="132-lsi-1" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>Author: Nuno Vasconcelos, Mohammad J. Saberian</p><p>Abstract: The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classiﬁcation and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classiﬁcation risk and complexity. It searches the space of cascade conﬁgurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems. 1</p><p>same-paper 2 0.92333984 <a title="132-lsi-2" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>3 0.62458116 <a title="132-lsi-3" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>Author: David Weiss, Benjamin Sapp, Ben Taskar</p><p>Abstract: For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but ﬁlter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the ﬁltering loss of the ensemble as a theoretical justiﬁcation of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We ﬁnd that our approach signiﬁcantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem. 1</p><p>4 0.52244341 <a title="132-lsi-4" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>5 0.47424823 <a title="132-lsi-5" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>6 0.36481723 <a title="132-lsi-6" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>7 0.34065333 <a title="132-lsi-7" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>8 0.33287105 <a title="132-lsi-8" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>9 0.32364446 <a title="132-lsi-9" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>10 0.31917381 <a title="132-lsi-10" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>11 0.31752378 <a title="132-lsi-11" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>12 0.28207192 <a title="132-lsi-12" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>13 0.26775807 <a title="132-lsi-13" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>14 0.2553024 <a title="132-lsi-14" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>15 0.24930969 <a title="132-lsi-15" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>16 0.23732646 <a title="132-lsi-16" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>17 0.22462375 <a title="132-lsi-17" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>18 0.22339834 <a title="132-lsi-18" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>19 0.21690392 <a title="132-lsi-19" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>20 0.21652183 <a title="132-lsi-20" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.036), (17, 0.021), (27, 0.053), (30, 0.049), (35, 0.014), (45, 0.189), (50, 0.133), (52, 0.044), (60, 0.039), (77, 0.044), (78, 0.218), (90, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91226882 <a title="132-lda-1" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>Author: Thomas Peel, Sandrine Anthoine, Liva Ralaivola</p><p>Abstract: We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernsteintype inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efﬁcient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions. 1</p><p>2 0.89523673 <a title="132-lda-2" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>3 0.85796726 <a title="132-lda-3" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>Author: Jie Luo, Francesco Orabona</p><p>Abstract: In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classiﬁers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difﬁcult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efﬁcient algorithm to solve it. Experiments conducted on artiﬁcial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.</p><p>4 0.82943851 <a title="132-lda-4" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>Author: Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashingbased solutions. Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our ﬁrst method’s preprocessing stage is more efﬁcient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. 1</p><p>5 0.82316232 <a title="132-lda-5" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>same-paper 6 0.82300961 <a title="132-lda-6" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>7 0.76743853 <a title="132-lda-7" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>8 0.76392007 <a title="132-lda-8" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>9 0.75957954 <a title="132-lda-9" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>10 0.75529462 <a title="132-lda-10" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>11 0.74684262 <a title="132-lda-11" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>12 0.74502951 <a title="132-lda-12" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>13 0.74001312 <a title="132-lda-13" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>14 0.73927814 <a title="132-lda-14" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>15 0.73923057 <a title="132-lda-15" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>16 0.73588341 <a title="132-lda-16" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>17 0.73540622 <a title="132-lda-17" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>18 0.73268116 <a title="132-lda-18" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>19 0.7319631 <a title="132-lda-19" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>20 0.73127538 <a title="132-lda-20" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
