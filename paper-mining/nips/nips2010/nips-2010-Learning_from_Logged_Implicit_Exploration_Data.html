<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>152 nips-2010-Learning from Logged Implicit Exploration Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-152" href="#">nips2010-152</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>152 nips-2010-Learning from Logged Implicit Exploration Data</h1>
<br/><p>Source: <a title="nips-2010-152-pdf" href="http://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data.pdf">pdf</a></p><p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>Reference: <a title="nips-2010-152-reference" href="../nips2010_reference/nips-2010-Learning_from_Logged_Implicit_Exploration_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. [sent-11, score-0.332]
</p><p>2 Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. [sent-13, score-0.323]
</p><p>3 The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. [sent-14, score-0.784]
</p><p>4 1 Introduction Consider the advertisement display problem, where a search engine company chooses an ad to display which is intended to interest the user. [sent-17, score-0.378]
</p><p>5 An instance of the contextual bandit problem is speciﬁed by a distribution D over tuples (x, r) where x ∈ X is an input and r ∈ [0, 1]k is a vector of rewards [6]. [sent-26, score-0.355]
</p><p>6 The algorithm chooses an action a ∈ A, possibly as a function of x and historical information. [sent-30, score-0.322]
</p><p>7 The world announces the reward ra of action a, but not ra′ for a′ = a. [sent-32, score-0.442]
</p><p>8 1  It is critical to understand that this is not a standard supervised-learning problem, because the reward of other actions a′ = a is not revealed. [sent-36, score-0.223]
</p><p>9 The standard goal in this setting is to maximize the sum of rewards ra over the rounds of interaction. [sent-37, score-0.198]
</p><p>10 In order to do this well, it is essential to use previously recorded events to form a good policy on the ﬁrst round of interaction. [sent-38, score-0.757]
</p><p>11 Formally, given a dataset of the form S = (x, a, ra )∗ generated by the interaction of an uncontrolled logging policy, we want to construct a policy h maximizing (either exactly or approximately) V h := E(x,r)∼D [rh(x) ]. [sent-40, score-1.07]
</p><p>12 We could learn a regressor s : X × A → [0, 1] which is trained to predict the reward, on observed events conditioned on the action a and other information x. [sent-43, score-0.393]
</p><p>13 From this regressor, a policy is derived according to h(x) = argmaxa∈A s(x, a). [sent-44, score-0.571]
</p><p>14 Suppose that there are two actions a and b with action a occurring 106 times and action b occuring 102 times. [sent-47, score-0.411]
</p><p>15 Since action b occurs only a 10−4 fraction of the time, a learning algorithm forced to trade off between predicting the expected value of ra and rb overwhelmingly prefers to estimate ra well at the expense of accurate estimation for rb . [sent-48, score-0.615]
</p><p>16 This problem is only worse when action b occurs zero times, as might commonly occur in exploration situations. [sent-50, score-0.266]
</p><p>17 Existing approaches to contextual bandits such as EXP4 [1] or Epoch Greedy [6], require either interaction to gather data or require knowledge of the probability the logging policy chose the action a. [sent-58, score-1.179]
</p><p>18 It is possible to recover exploration information from action visitation frequency when a logging policy chooses actions independent of the input x (but possibly dependent on history) [5]. [sent-62, score-1.401]
</p><p>19 This doesn’t ﬁt our setting, where the logging policy is surely dependent on the input. [sent-63, score-0.863]
</p><p>20 When conducting a survey, a question about income might be included, and then the proportion of responders at various income levels can be compared to census data to estimate a probability conditioned on income that someone chooses to partake in the survey. [sent-66, score-0.383]
</p><p>21 This approach is problematic here, because the policy making decisions when logging the data may be deterministic rather than probabilistic. [sent-68, score-0.903]
</p><p>22 In other words, accurately predicting the probability of the logging policy choosing an ad implies always predicting 0 or 1 which is not useful for our purposes. [sent-69, score-1.009]
</p><p>23 Although the straightforward use of propensity scores does not work, the approach we take can be thought of as as a more clever use of a propensity score, as discussed below. [sent-70, score-0.222]
</p><p>24 For each event (x, a, ra ), estimate the probability π (a|x) that the logging policy chooses ˆ action a using regression. [sent-74, score-1.388]
</p><p>25 For each event (x, a, ra ), create a synthetic controlled contextual bandit event according to (x, a, ra , 1/ max{ˆ (a|x), τ }) where τ > 0 is some parameter. [sent-77, score-0.752]
</p><p>26 Apply an ofﬂine contextual bandit algorithm to the set of synthetic contextual bandit events. [sent-81, score-0.6]
</p><p>27 2) a variant of the argmax regressor is used with two critical modiﬁcations: (a) We limit the scope of the argmax to those actions with positive probability; (b) We importance weight events so that the training process emphasizes good estimation for each action equally. [sent-83, score-0.685]
</p><p>28 It should be emphasized that the theoretical analysis in this paper applies to any algorithm for learning on contextual bandit events—we chose this one because it is a simple modiﬁcation on existing (but fundamentally broken) approaches. [sent-84, score-0.3]
</p><p>29 Relative to it, we use a different deﬁnition of probability which is not necessarily 0 or 1 when the logging policy is completely deterministic. [sent-86, score-0.863]
</p><p>30 What does π (a|x) mean, given that the logging policy may be deterministically choosing ˆ an action (ad) a given features x? [sent-89, score-1.042]
</p><p>31 The essential observation is that a policy which deterministically chooses action a on day 1 and then deterministically chooses action b on day 2 can be treated as randomizing between actions a and b with probability 0. [sent-90, score-1.346]
</p><p>32 5 when the number of events is the same each day, and the events are IID. [sent-91, score-0.28]
</p><p>33 Thus π (a|x) is an estimate ˆ of the expected frequency with which action a would be displayed given features x over the timespan of the logged events. [sent-92, score-0.377]
</p><p>34 While creating a bias in the estimation process, it turns out that the form of this bias is mild and relatively reasonable— actions which are displayed with low frequency conditioned on x effectively have an underestimated value. [sent-102, score-0.284]
</p><p>35 We close with a generalization from policy evaluation to policy selection with a sample complexity bound in section 3. [sent-106, score-1.209]
</p><p>36 The learning algorithm is given a dataset of T samples, each of the form (x, a, ra ) ∈ X × A× [0, 1], where (x, r) is drawn from D as described in Section 1, and the action a ∼ πt (x) is chosen according to the tth policy. [sent-112, score-0.377]
</p><p>37 We denote this random process by (x, a, ra ) ∼ (D, πt (·|x)). [sent-113, score-0.171]
</p><p>38 i=1 Ofﬂine policy estimator: Given a dataset of the form (1) S = {(xt , at , rt,at )}T , t=1 ˆ where ∀t, xt ∈ X, at ∈ A, rt,at ∈ [0, 1], we form a predictor π : X × A → [0, 1] and then use it with a threshold τ ∈ [0, 1] to form an ofﬂine estimator for the value of a policy h. [sent-116, score-1.367]
</p><p>39 Formally, given a new policy h : X → A and a dataset S, deﬁne the estimator: 1 ra I(h(x) = a) h ˆˆ Vπ (S) = , |S| max{ˆ (a|x), τ } π  (2)  (x,a,r)∈S  ˆh where I(·) denotes the indicator function. [sent-117, score-0.778]
</p><p>40 The main idea is twofold: ﬁrst, we have a policy estimation step, where we estimate the (unknown) logging policy (Subsection 3. [sent-122, score-1.48]
</p><p>41 1); second, we have a policy optimization step, where we utilize our estimated logging policy (Subsection 3. [sent-123, score-1.434]
</p><p>42 The logging policy πt may be deterministic, implying that conventional approaches relying on randomization in the logging policy are not applicable. [sent-127, score-1.782]
</p><p>43 We show next that this is ok when the world is IID and the policy varies over its actions. [sent-128, score-0.598]
</p><p>44 A basic claim is that the estimator is equivalent, in expectation, to a stochastic policy deﬁned by: π(a|x) = Et∼UNIF(1,. [sent-130, score-0.731]
</p><p>45 The stochastic policy π chooses an action uniformly at random over the T policies πt . [sent-134, score-1.016]
</p><p>46 Our ﬁrst result is that the expected value of our estimator is the same when the world chooses actions according to either π or to the sequence of policies πt . [sent-135, score-0.564]
</p><p>47 1 Policy Estimation In this section we show that for a suitable choice of τ and π our estimator is sufﬁciently accurate ˆ for evaluating new policies h. [sent-144, score-0.274]
</p><p>48 We aggressively use the simpliﬁcation of the previous section, which shows that we can think of the data as generated by a ﬁxed stochastic policy π, i. [sent-145, score-0.61]
</p><p>49 In the following theorem statement, I(·) denotes the indicator ˆh function, π(a|x) the probability that the logging policy chooses action a on input x, and Vπ our ˆ estimator as deﬁned by Equation 2 based on parameter τ . [sent-151, score-1.237]
</p><p>50 Let V h (x) = Er∼D(·|x) [rh(x) ] denote the expected value of executing policy h on input x. [sent-156, score-0.599]
</p><p>51 1 shows that the expected value of our estimate Vπ of a policy h is an approximation to a lower bound of the true value of the policy h where the approximation is due to errors in the estimate π and the lower bound is due to the threshold τ . [sent-165, score-1.312]
</p><p>52 ˆ  ˆh Thus, with a perfect predictor of π, the expected value of the estimator Vπ is a guaranteed lower ˆ bound on the true value of policy h. [sent-168, score-0.774]
</p><p>53 However, as the left-hand-side of this statement suggests, it may be a very loose bound, especially if the action chosen by h often has a small probability of being chosen by π. [sent-169, score-0.222]
</p><p>54 Consider an instance of the bandit problem with a single input x and two actions a1 , a2 . [sent-172, score-0.288]
</p><p>55 Suppose that π(a1 |x) = τ + ǫ for some positive ǫ and h(x) = a1 is the policy we are evaluating. [sent-173, score-0.571]
</p><p>56 2 Policy Optimization The previous section proves that we can effectively evaluate a policy h by observing a stochastic policy π, as long as the actions chosen by h have adequate support under π, speciﬁcally π(h(x)|x) ≥ τ for all inputs x. [sent-178, score-1.36]
</p><p>57 However, we are often interested in choosing the best policy h from a set of policies H after observing logged data. [sent-179, score-0.841]
</p><p>58 Furthermore, as described in Section 2, the logged data are generated from T ﬁxed, possibly deterministic, policies π1 , . [sent-180, score-0.307]
</p><p>59 As in Section 3 we deﬁne the stochastic policy π by Equation 3, π(a|x) = Et∼UNIF(1,. [sent-184, score-0.61]
</p><p>60 ˜ ˆ = argmax ˆ h } be the hypothesis that maximizes the empirical value estimator deﬁned Let h {Vπ h∈H ˆ in Equation 2. [sent-199, score-0.178]
</p><p>61 In other words, if H contains a very good policy that has little support under π, we will not be able to detect that by our estimator. [sent-204, score-0.571]
</p><p>62 On the other hand, our estimation is safe in the sense that we will never drastically overestimate the value of any policy in H. [sent-205, score-0.639]
</p><p>63 The ﬁrst dataset consists of uniformly random exploration data, from which an unbiased estimate of any policy can be obtained. [sent-209, score-0.808]
</p><p>64 The second dataset then demonstrates how policy optimization can be done from nonrandom ofﬂine data. [sent-211, score-0.646]
</p><p>65 1 Experiment I The ﬁrst experiment involves news article recommendation in the “Today Module”, on the Yahoo! [sent-213, score-0.188]
</p><p>66 For every user visit, this module displays a high-quality news article out of a small candidate pool, which is hand-picked by human editors. [sent-215, score-0.25]
</p><p>67 This problem is modeled as a contextual bandit problem, where the context consists of both user and article information, the arms correspond to articles, and the reward of a displayed article is 1 if there is a click and 0 otherwise. [sent-218, score-0.739]
</p><p>68 Therefore, the value of a policy is exactly its overall CTR. [sent-219, score-0.571]
</p><p>69 7M events in the form of triples (x, a, r), where the context x contains user/article features, arm a was chosen uniformly at random from a dynamic candidate pool A, and r is a binary reward signal indicating whether the user clicked on a. [sent-224, score-0.376]
</p><p>70 Furthermore, a straightforward application ˆ √ ˆh of Hoeffding’s inequality guarantees that Vπ concentrates to V h at the rate of O(1/ T ) for any ˆ policy h, which is also veriﬁed empirically [9]. [sent-228, score-0.571]
</p><p>71 To obtain non-random log data, we ran the LinUCB algorithm using the ofﬂine bandit simulation procedure, both from [8], on our random log data D0 and recorded events (x, a, r) for which LinUCB chose arm a for context x. [sent-231, score-0.349]
</p><p>72 It is known that the set of recorded events has the same distribution as if we ran LinUCB on real user visits to Yahoo! [sent-234, score-0.247]
</p><p>73 To deﬁne the policy h for evaluation, we used D0 to estimate each article’s overall CTR across all users, and then h was deﬁned as selecting the article with highest estimated CTR. [sent-237, score-0.71]
</p><p>74 Since the set A of articles changes over time (with news articles being added and old articles retiring), π(a|x) is very small due to the large number of articles over the two-week period, resulting in large variance. [sent-239, score-0.369]
</p><p>75 For extremely small values of τ , however, there appears to be a consistent trend of over-estimating the policy value. [sent-247, score-0.571]
</p><p>76 Note that the logging policy we used, π, violates one of the assumptions used to prove Lemma 3. [sent-249, score-0.863]
</p><p>77 1, namely that the exploration policy at timestep t not be dependent on an earlier event. [sent-250, score-0.694]
</p><p>78 The data are comprised of logs of events (x, a, y), where each event represents a visit by a user to a particular web page x, from a set of web pages X. [sent-256, score-0.45]
</p><p>79 From a large set of advertisements A, the commercial system chooses a single ad 3  We could do so because we know A for every event in D0 . [sent-257, score-0.311]
</p><p>80 0071]  Figure 2: Results of various algorithms on the ad display dataset. [sent-277, score-0.192]
</p><p>81 It also chooses additional ads to display, but these were ignored in our test. [sent-286, score-0.227]
</p><p>82 The output y is an indicator of whether the user clicked on the ad or not. [sent-287, score-0.255]
</p><p>83 The test data contain 19 million events occurring after the events in the training data. [sent-290, score-0.28]
</p><p>84 We trained a policy h to choose an ad, based on the current page, to maximize the probability of click. [sent-293, score-0.571]
</p><p>85 For the purposes of learning, each ad and page was represented internally as a sparse highdimensional feature vector. [sent-294, score-0.212]
</p><p>86 Each ad contains, on average, 30 ad features and each page, approximately 50 page features. [sent-296, score-0.358]
</p><p>87 The particular form of f was linear over features of its input (x, a)4 The particular policy that was optimized, had an argmax form: h(x) = argmaxa∈C(X) {f (x, a)}, with a crucial distinction from previous approaches in how f (x, a) was trained. [sent-297, score-0.628]
</p><p>88 ˆ  The training samples were of the form (x, a, y), where y = 1 if the ad a was clicked after being shown on page x or y = 0 otherwise. [sent-299, score-0.26]
</p><p>89 As shown in the analysis of Section 3, this bias typically underestimates the true value of the policy h. [sent-304, score-0.616]
</p><p>90 4  Technically the feature vector that the regressor uses is the Cartesian product of the page and ad vectors. [sent-311, score-0.296]
</p><p>91 5  7  The “Random” policy is the policy that chooses randomly from the set of feasible ads: Random(x) = a ∼ UNIF(C(X)), where UNIF(·) denotes the uniform distribution. [sent-319, score-1.252]
</p><p>92 The “Naive” policy corresponds to the theoretically ﬂawed supervised learning approach detailed in the introduction. [sent-320, score-0.571]
</p><p>93 The evaluation of this policy is quite expensive, requiring one evaluation per ad per example, so the size of the test set is reduced to 8373 examples with a click, which reduces the signiﬁcance of the results. [sent-321, score-0.801]
</p><p>94 We bias the results towards the naive policy by choosing the chronologically ﬁrst events in the test set (i. [sent-322, score-0.823]
</p><p>95 Nevertheless, the naive policy receives 0 reward, which is signiﬁcantly less than all other approaches. [sent-325, score-0.638]
</p><p>96 A possible fear with the evaluation here is that the naive policy is always ﬁnding good ads that simply weren’t explored. [sent-326, score-0.797]
</p><p>97 Indeed, the estimates for both the learned policy and the random policy improve when we decrease τ from 0. [sent-334, score-1.168]
</p><p>98 5 Conclusion We stated, justiﬁed, and evaluated theoretically and empirically the ﬁrst method for solving the warm start problem for exploration from logged data with controlled bias and estimation. [sent-342, score-0.333]
</p><p>99 For example, in reinforcement learning, the standard approach to ofﬂine policy evaluation is based on importance weighted samples [3, 11]. [sent-348, score-0.651]
</p><p>100 Unbiased ofﬂine evaluation of contextualbandit-based news article recommendation algorithms. [sent-383, score-0.23]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.571), ('logging', 0.292), ('ine', 0.197), ('ra', 0.171), ('bandit', 0.163), ('policies', 0.153), ('ad', 0.146), ('action', 0.143), ('events', 0.14), ('contextual', 0.137), ('yahoo', 0.132), ('actions', 0.125), ('exploration', 0.123), ('estimator', 0.121), ('ads', 0.117), ('logged', 0.117), ('reg', 0.111), ('ctr', 0.111), ('evaluator', 0.111), ('propensity', 0.111), ('chooses', 0.11), ('article', 0.093), ('regressor', 0.084), ('unif', 0.079), ('articles', 0.075), ('news', 0.069), ('naive', 0.067), ('income', 0.067), ('lihong', 0.067), ('linucb', 0.066), ('page', 0.066), ('click', 0.063), ('user', 0.061), ('langford', 0.059), ('argmax', 0.057), ('reward', 0.057), ('randomization', 0.056), ('event', 0.055), ('web', 0.05), ('warm', 0.048), ('clicked', 0.048), ('estimate', 0.046), ('recorded', 0.046), ('display', 0.046), ('bias', 0.045), ('announces', 0.044), ('nctr', 0.044), ('strehl', 0.044), ('pool', 0.043), ('displayed', 0.043), ('evaluation', 0.042), ('critical', 0.041), ('deterministic', 0.04), ('stochastic', 0.039), ('ex', 0.039), ('overestimate', 0.039), ('nonrandom', 0.039), ('xt', 0.039), ('importance', 0.038), ('front', 0.037), ('possibly', 0.037), ('day', 0.036), ('lemma', 0.036), ('dataset', 0.036), ('bandits', 0.036), ('lambert', 0.036), ('rh', 0.036), ('deterministically', 0.036), ('regret', 0.034), ('vh', 0.033), ('companies', 0.033), ('advertising', 0.033), ('unbiased', 0.032), ('historical', 0.032), ('argmaxa', 0.032), ('chu', 0.03), ('engine', 0.03), ('safe', 0.029), ('arms', 0.029), ('john', 0.029), ('predictor', 0.029), ('visit', 0.028), ('compete', 0.028), ('rb', 0.028), ('tuples', 0.028), ('robert', 0.028), ('expected', 0.028), ('chosen', 0.027), ('rewards', 0.027), ('world', 0.027), ('adequate', 0.027), ('module', 0.027), ('draws', 0.027), ('recommendation', 0.026), ('learned', 0.026), ('conditioned', 0.026), ('statement', 0.025), ('bound', 0.025), ('wei', 0.025), ('period', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="152-tfidf-1" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>2 0.42001817 <a title="152-tfidf-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>3 0.35453713 <a title="152-tfidf-3" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present policy gradient results within the framework of linearly-solvable MDPs. For the ﬁrst time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the ﬁrst compatible function approximators and natural policy gradients for continuous-time stochastic systems.</p><p>4 0.35265294 <a title="152-tfidf-4" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>Author: Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua B. Tenenbaum</p><p>Abstract: We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning. 1</p><p>5 0.35165253 <a title="152-tfidf-5" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>6 0.35109085 <a title="152-tfidf-6" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>7 0.31986234 <a title="152-tfidf-7" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>8 0.26481473 <a title="152-tfidf-8" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>9 0.26205698 <a title="152-tfidf-9" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>10 0.21888399 <a title="152-tfidf-10" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>11 0.21852535 <a title="152-tfidf-11" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>12 0.17520756 <a title="152-tfidf-12" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>13 0.17239675 <a title="152-tfidf-13" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>14 0.17231023 <a title="152-tfidf-14" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>15 0.16755262 <a title="152-tfidf-15" href="./nips-2010-Non-Stochastic_Bandit_Slate_Problems.html">183 nips-2010-Non-Stochastic Bandit Slate Problems</a></p>
<p>16 0.16603726 <a title="152-tfidf-16" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>17 0.16299625 <a title="152-tfidf-17" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>18 0.15927033 <a title="152-tfidf-18" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>19 0.15152903 <a title="152-tfidf-19" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>20 0.13965389 <a title="152-tfidf-20" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.268), (1, -0.556), (2, -0.069), (3, -0.074), (4, 0.006), (5, -0.029), (6, 0.026), (7, -0.039), (8, 0.004), (9, -0.114), (10, -0.001), (11, 0.012), (12, -0.025), (13, 0.021), (14, 0.0), (15, 0.009), (16, 0.067), (17, -0.037), (18, -0.026), (19, -0.022), (20, -0.002), (21, -0.027), (22, 0.021), (23, 0.051), (24, 0.001), (25, -0.03), (26, 0.065), (27, -0.017), (28, -0.019), (29, 0.087), (30, 0.059), (31, -0.031), (32, 0.019), (33, -0.039), (34, 0.041), (35, 0.002), (36, -0.023), (37, -0.063), (38, -0.048), (39, 0.06), (40, -0.009), (41, -0.032), (42, -0.077), (43, 0.086), (44, 0.031), (45, 0.028), (46, 0.033), (47, 0.054), (48, 0.042), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97361892 <a title="152-lsi-1" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>2 0.91169685 <a title="152-lsi-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>3 0.90319216 <a title="152-lsi-3" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>4 0.87794173 <a title="152-lsi-4" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>5 0.8437233 <a title="152-lsi-5" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>Author: Jeffrey Johns, Christopher Painter-wakefield, Ronald Parr</p><p>Abstract: Recent work in reinforcement learning has emphasized the power of L1 regularization to perform feature selection and prevent overﬁtting. We propose formulating the L1 regularized linear ﬁxed point problem as a linear complementarity problem (LCP). This formulation offers several advantages over the LARS-inspired formulation, LARS-TD. The LCP formulation allows the use of efﬁcient off-theshelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts). We demonstrate that warm starts, as well as the efﬁciency of LCP solvers, can speed up policy iteration. Moreover, warm starts permit a form of modiﬁed policy iteration that can be used to approximate a “greedy” homotopy path, a generalization of the LARS-TD homotopy path that combines policy evaluation and optimization. 1</p><p>6 0.83528167 <a title="152-lsi-6" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>7 0.81748331 <a title="152-lsi-7" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>8 0.7849077 <a title="152-lsi-8" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>9 0.69881409 <a title="152-lsi-9" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>10 0.6936413 <a title="152-lsi-10" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>11 0.67649353 <a title="152-lsi-11" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>12 0.64722121 <a title="152-lsi-12" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>13 0.60696971 <a title="152-lsi-13" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>14 0.6027559 <a title="152-lsi-14" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>15 0.57603705 <a title="152-lsi-15" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>16 0.56541222 <a title="152-lsi-16" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>17 0.56313044 <a title="152-lsi-17" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>18 0.53301257 <a title="152-lsi-18" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>19 0.52267635 <a title="152-lsi-19" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>20 0.51527321 <a title="152-lsi-20" href="./nips-2010-Non-Stochastic_Bandit_Slate_Problems.html">183 nips-2010-Non-Stochastic Bandit Slate Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.042), (17, 0.012), (27, 0.071), (30, 0.067), (39, 0.017), (45, 0.217), (50, 0.036), (52, 0.04), (60, 0.033), (67, 0.249), (77, 0.057), (78, 0.03), (90, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87787127 <a title="152-lda-1" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>Author: Sridhar Mahadevan, Bo Liu</p><p>Abstract: This paper explores links between basis construction methods in Markov decision processes and power series expansions of value functions. This perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases. Krylov and Bellman error bases are based on the Neumann series expansion. These bases incur very large initial Bellman errors, and can converge rather slowly as the discount factor approaches unity. The Laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efﬁcient basis representations. The ﬁrst two terms in the Laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the Drazin (or group inverse) of a singular matrix derived from the transition matrix. Experiments show that Drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor. An incremental variant of Drazin bases called Bellman average-reward bases (BARBs) is described, which provides some of the same beneﬁts at lower computational cost. 1</p><p>same-paper 2 0.81537926 <a title="152-lda-2" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>3 0.70741701 <a title="152-lda-3" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>4 0.70734596 <a title="152-lda-4" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>Author: Wei Chen, Tie-yan Liu, Zhi-ming Ma</p><p>Abstract: This paper is concerned with the generalization analysis on learning to rank for information retrieval (IR). In IR, data are hierarchically organized, i.e., consisting of queries and documents. Previous generalization analysis for ranking, however, has not fully considered this structure, and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of the learned ranking model. In this paper, we propose performing generalization analysis under the assumption of two-layer sampling, i.e., the i.i.d. sampling of queries and the conditional i.i.d sampling of documents per query. Such a sampling can better describe the generation mechanism of real data, and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms. However, it is challenging to perform such analysis, because the documents associated with different queries are not identically distributed, and the documents associated with the same query become no longer independent after represented by features extracted from query-document matching. To tackle the challenge, we decompose the expected risk according to the two layers, and make use of the new concept of two-layer Rademacher average. The generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performances of ranking algorithms. 1</p><p>5 0.70587814 <a title="152-lda-5" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>6 0.70558351 <a title="152-lda-6" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>7 0.70514309 <a title="152-lda-7" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>8 0.70305985 <a title="152-lda-8" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>9 0.70233661 <a title="152-lda-9" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>10 0.70203704 <a title="152-lda-10" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>11 0.70179588 <a title="152-lda-11" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>12 0.70170361 <a title="152-lda-12" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>13 0.70162559 <a title="152-lda-13" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>14 0.7015757 <a title="152-lda-14" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>15 0.70085883 <a title="152-lda-15" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>16 0.70079029 <a title="152-lda-16" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>17 0.70064652 <a title="152-lda-17" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>18 0.70047331 <a title="152-lda-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.7002179 <a title="152-lda-19" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>20 0.69913214 <a title="152-lda-20" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
