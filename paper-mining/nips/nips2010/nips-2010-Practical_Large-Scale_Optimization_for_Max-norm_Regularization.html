<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-210" href="#">nips2010-210</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</h1>
<br/><p>Source: <a title="nips-2010-210-pdf" href="http://papers.nips.cc/paper/4124-practical-large-scale-optimization-for-max-norm-regularization.pdf">pdf</a></p><p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>Reference: <a title="nips-2010-210-reference" href="../nips2010_reference/nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. [sent-9, score-0.282]
</p><p>2 The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. [sent-11, score-0.122]
</p><p>3 These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. [sent-12, score-0.419]
</p><p>4 In a wide variety of applications, such as collaborative ﬁltering, multi-task learning, multi-class learning and clustering of multivariate observations, matrices offer a natural way to tabulate data. [sent-15, score-0.295]
</p><p>5 For such matrix models, the matrix rank provides an intellectually appealing way to describe complexity. [sent-16, score-0.145]
</p><p>6 Unfortunately, optimization problems involving rank constraints are computationally intractable except in a few basic cases. [sent-20, score-0.124]
</p><p>7 A particular example of a low-rank regularizer that has received a huge amount of recent attention is the trace-norm, equal to the sum of the matrix’s singular values (See the comprehensive survey [3] and its bibliography). [sent-22, score-0.144]
</p><p>8 The tracenorm promotes low-rank decompositions because it minimizes the 1 norm of the vector of singular values, which encourages many zero singular values. [sent-23, score-0.207]
</p><p>9 Although the trace-norm is a very successful regularizer in many applications, it does not seem to be widely known or appreciated that there are many other interesting norms that promote low rank. [sent-24, score-0.191]
</p><p>10 The current work focuses on another rank-promoting regularizer, sometimes called the maxnorm, that has been proposed as an alternative to the rank for collaborative ﬁltering problems [1, 5]. [sent-26, score-0.226]
</p><p>11 The max-norm can be deﬁned via matrix factorizations: X where ·  max  := inf  denotes the maximum  2,∞  A  2  U  2,∞  V  2,∞  : X = UV  (1)  row norm of a matrix:  2,∞  := maxj  k  A2 jk  1/2  . [sent-27, score-0.242]
</p><p>12 When X is positive semideﬁnite, we may force U = V and then verify that X max = maxj xjj , which should explain the terminology. [sent-29, score-0.122]
</p><p>13 The fundamental result in the metric theory of tensor products, due to Grothendieck, states that the max-norm is comparable with a nuclear norm (see Chapter 10 of [6]): X  max  ≈ inf  σ  1  :X=  j  σj uj vj where uj  ∞  = 1 and vj  ∞  =1 . [sent-30, score-0.451]
</p><p>14 The trace-norm, on the other hand, is equal to X  tr  := inf  σ  1  :X=  j  σj uj vj where uj  2  = 1 and vj  2  =1 . [sent-34, score-0.278]
</p><p>15 This perspective reveals that the max-norm promotes low-rank decompositions with factors in ∞ , rather than the 2 factors produced by the trace-norm! [sent-35, score-0.079]
</p><p>16 The literature already contains theoretical and empirical evidence that the max-norm is superior to the trace-norm for certain types of problems. [sent-37, score-0.077]
</p><p>17 Indeed, the max-norm offers better generalization error bounds for collaborative ﬁltering [5], and it outperforms the trace-norm in small-scale experiments [1]. [sent-38, score-0.14]
</p><p>18 The paper [7] provides further evidence that the max-norm serves better for collaborative ﬁltering with nonuniform sampling patterns. [sent-39, score-0.227]
</p><p>19 We believe that the max-norm has not achieved the same prominence as the trace-norm because of an apprehension that it is challenging to solve optimization problems involving a max-norm regularizer. [sent-40, score-0.104]
</p><p>20 In particular, we study convex programs of the form min f (X) + µ X  (2)  max  where f is a smooth function and µ is a positive penalty parameter. [sent-43, score-0.12]
</p><p>21 We also study the bound-constrained problem min f (X) subject to  X  max  ≤ B. [sent-45, score-0.123]
</p><p>22 Section 3 provides a projected gradient method for (3), and Section 5 develops a stochastic implementation that is appropriate for decomposable loss functions. [sent-47, score-0.341]
</p><p>23 In Section 6, we apply these new algorithms to large-scale collaborative ﬁltering problems, and we demonstrate performance superior to methods based on the trace-norm. [sent-49, score-0.179]
</p><p>24 We apply the algorithms to solve enormous instances of graph cut problems, and we establish that clustering based on these cuts outperforms spectral clustering on several data sets. [sent-50, score-0.723]
</p><p>25 2  2  The SDP and Factorization Approaches  The max-norm of an m × n matrix X can be expressed as the solution to a semideﬁnite program: X  max  = min t subject to  W1 X  X W2  diag(W1 ) ≤ t,  0,  diag(W2 ) ≤ t. [sent-51, score-0.169]
</p><p>26 For large-scale problems, we use an alternative formulation suggested by (1) that explicitly works with a factorization of the decision variable X. [sent-53, score-0.088]
</p><p>27 R  Burer and Monteiro showed that as long as L and R have sufﬁciently many columns, then the global optimum of (4) is equal to that of X  max  =  min  max{ L  (L,R) : LR =X  2 2,∞  , R  2 2,∞ } . [sent-56, score-0.082]
</p><p>28 This formulation of the max-norm is nonconvex because it involves a constraint on the product LR , but Burer and Monteiro proved that each local minimum of the reformulated problem is also a global optimum [9]. [sent-58, score-0.099]
</p><p>29 On the other hand, the new formulation is nonconvex with respect to L and R so it might not be efﬁciently solvable. [sent-60, score-0.099]
</p><p>30 3  Projected Gradient Method  The constrained formulation (3) admits a simple projected gradient algorithm. [sent-62, score-0.34]
</p><p>31 We replace X with the product LR and use the factored form of the max-norm (5) to obtain minimize(L,R) f (LR )  subject to max{ L  2 2,∞  , R  2 2,∞ }  ≤ B. [sent-63, score-0.086]
</p><p>32 (6)  The projected gradient descent method ﬁxes a step size τ and computes updates with the rule L ← PB R  L − τ f (LR)R R − τ f (LR) L 2  2  where PB denotes the Euclidean projection onto the set {(L, R) : max( L 2,∞ , R 2,∞ ) ≤ B}. [sent-64, score-0.298]
</p><p>33 This projection can be computed by re-scaling the rows of the current iterate whose norms exceed √ √ √ B so their norms equal B. [sent-65, score-0.224]
</p><p>34 Rows with norms less than B are unchanged by the projection. [sent-66, score-0.091]
</p><p>35 The projected gradient algorithm is elegant and simple, and it has an online implementation, described below. [sent-67, score-0.298]
</p><p>36 We employ a classical proximal point method, proposed by Fukushima and Mine [8], which forms the algorithmic foundation of many popular ﬁrst-order methods of for 1 -norm minimization [11, 12] and trace-norm minimization [13, 14]. [sent-72, score-0.148]
</p><p>37 The new cost function can then be minimized in closed form. [sent-75, score-0.08]
</p><p>38 Before describing the proximal point algorithm in detail, we ﬁrst discuss how a simple max-norm problem (the Frobenius norm plus a max-norm penalty) admits an explicit formula for its unique optimal solution. [sent-76, score-0.188]
</p><p>39 Consider the simple regularization problem minimizeW  W −V 3  2 F  +β W  2 2,∞  (7)  Algorithm 1 Compute W = squash(V , β) Require: A d × D matrix V , a positive scalar β. [sent-77, score-0.092]
</p><p>40 F 1: for k = 1 to d set nk ← vk 2 2: sort {nk } in descending order. [sent-79, score-0.116]
</p><p>41 We call this procedure squash because the rows of V with large norm have their magnitude clipped at a critical value η = η(V , β). [sent-86, score-0.508]
</p><p>42 Note that squash can be computed in O(d max{log(d), D}) ﬂops. [sent-90, score-0.392]
</p><p>43 Computing the row norms requires O(dD) ﬂops, and then the sort requires O(d log d) ﬂops. [sent-91, score-0.137]
</p><p>44 With the squash function in hand, we can now describe our proximal-point algorithm. [sent-94, score-0.392]
</p><p>45 Using the squash algorithm, we can solve minimize  −1 ˜ f (Ak ), A + τk A − Ak  2 F  +µ A  2 2,∞  (9)  in closed form. [sent-100, score-0.498]
</p><p>46 That is, the optimal solution ˜ of (9) is squash Ak − τk f (Ak ), τk µ . [sent-103, score-0.392]
</p><p>47 The cost function f is replaced with a quadratic approximation localized at the previous iterate Ak , and the resulting approximation (9) can be solved in closed form. [sent-106, score-0.08]
</p><p>48 This algorithm is guaranteed to converge to a critical point of (8) as long as the step sizes are chosen ˜ commensurate with the norm of the Hessian [8]. [sent-109, score-0.074]
</p><p>49 In particular, Nesterov has recently shown that if f has a Lipschitz-continuous gradient with Lipschitz constant L, then the algorithm will converge at a rate of 1/k where k is the iteration counter [15]. [sent-110, score-0.151]
</p><p>50 4  Algorithm 2 A proximal-point method for max-norm regularization Require: Algorithm parameters α > 0, 1 > γ > 0, tol > 0. [sent-111, score-0.099]
</p><p>51 When dealing with very large datasets, S may consist of hundreds of millions of pairs, and there are algorithmic advantages to utilizing stochastic gradient methods that only query a very small subset of S at each iteration. [sent-120, score-0.157]
</p><p>52 We can also implement an efﬁcient algorithm for stochastic gradient descent for problem (2). [sent-126, score-0.157]
</p><p>53 If we wanted to apply the squash algorithm to such a stochastic gradient step, only the norms corresponding to Li and Rj would be modiﬁed. [sent-127, score-0.64]
</p><p>54 Hence, in Algorithm 1, if the set of row norms of L and R is sorted from the previous iteration, we can implement a balanced-tree data structure that allows us to perform individual updates in amortized logarithmic time. [sent-128, score-0.091]
</p><p>55 In the experiments, however, we demonstrate that the proximal point method is still quite efﬁcient and fast when dealing with stochastic gradient updates corresponding to medium-size batches {(i, j)} selected from S, even if a full sort is performed at each squash operation. [sent-130, score-0.743]
</p><p>56 We tested our proximal point and projected gradient methods on the Netﬂix dataset, which is the largest publicly available collaborative ﬁltering dataset. [sent-132, score-0.623]
</p><p>57 The training set contains 100,480,507 ratings from 480,189 anonymous users on 17,770 movie titles. [sent-133, score-0.122]
</p><p>58 The “qualiﬁcation set” pairs were selected by Netﬂix from the most recent ratings for a subset of the users. [sent-135, score-0.082]
</p><p>59 It includes users with over 10,000 ratings as well as users who rated fewer than 5 movies. [sent-142, score-0.162]
</p><p>60 05  Algorithm  RMSE  Training RMSE X max f (X) + + µ X max 2. [sent-146, score-0.164]
</p><p>61 75 0  5  10  15  20  25  30  35  40  Number of epochs  Figure 1: Performance of regularization methods on the Netﬂix dataset. [sent-161, score-0.092]
</p><p>62 In our experiments, all ratings were normalized to be zero-mean by subtracting 3. [sent-165, score-0.082]
</p><p>63 Both proximal-point and projected gradient methods performed 40 epochs (or passes through the training set), with parameters {L, R} updated after each minibatch. [sent-168, score-0.344]
</p><p>64 For the proximal-point method, µ was set to 5×10−4 , and for the projected gradient algorithm, B was set to 2. [sent-173, score-0.298]
</p><p>65 5 GHz Intel Xeon, our implementation of projected gradient takes 20. [sent-177, score-0.298]
</p><p>66 Figure 1 shows predictive performance of both the proximal-point and projected gradient algorithms on the training and qualiﬁcation set. [sent-180, score-0.298]
</p><p>67 Observe that the proximal-point algorithm converges considerably faster than projected gradient, but both algorithms achieve a similar RMSE of 0. [sent-181, score-0.184]
</p><p>68 Figure 1, left panel, further shows that the max-norm based regularization signiﬁcantly outperforms the corresponding trace-norm based regularization, which is widely used in many large-scale collaborative ﬁltering applications. [sent-184, score-0.186]
</p><p>69 (i,j)∈E  In our nonconvex formulation, this optimization becomes (1 − Ai Aj ) subject to A  minimize  2 2,∞  ≤ 1. [sent-188, score-0.137]
</p><p>70 2  SDPLR Time 3 4 7 29 6 15 21 15 20 33  |V | 2000 2000 2000 5000 7000 10000 10000 10000 14000 20000  |E| 19990 11778 11766 29570 17148 20000 9999 20000 28000 40000  Table 1: Performance of projected gradient on Gset graphs. [sent-235, score-0.298]
</p><p>71 1% of optimal, running time for 1% of optimal, number of iterations to reach 1% of optimal, primal objective using SDPLR, running time of SDPLR, number of vertices, and number of edges. [sent-239, score-0.127]
</p><p>72 (a) Spectral Clustering  (b) Max-cut clustering  Figure 2: Comparison of spectral clustering (left) with  MAX - CUT  clustering (right). [sent-241, score-0.557]
</p><p>73 We tested our projected gradient algorithm on graphs drawn from the Gset, a collection of graphs designed for testing the efﬁcacy of max-cut algorithms [17]. [sent-242, score-0.298]
</p><p>74 On the same modern hardware, a Matlab implementation of our projected gradient method can reach . [sent-244, score-0.298]
</p><p>75 For the 2-class clustering problem, we ﬁrst build a K-nearest neighbor graph with K = 10 and weights wij deﬁned as wij = max(si (j), sj (i)), with si (j) = ||x −x ||2  exp − i2σ2j and σi equal to the distance from xi to its Kth closest neighbor. [sent-247, score-0.339]
</p><p>76 We solve the MAX - CUT problem on the graph Q to ﬁnd our cluster assignments. [sent-250, score-0.081]
</p><p>77 For the two moons experiments, we ﬁx D = 100, n = 2000 and σ = . [sent-253, score-0.106]
</p><p>78 For the clustering experiments, we repeat the randomized rounding technique [16] for 100 trials, and we choose the rounding with highest primal objective. [sent-257, score-0.282]
</p><p>79 We compare our MAX - CUT clusterings with the spectral clustering method [20] and the Total Variation Graph Cut algorithm [19]. [sent-258, score-0.247]
</p><p>80 Figure 2 shows the clustering results for spectral clustering and maxcut clustering. [sent-259, score-0.402]
</p><p>81 In all the trials, spectral clustering incorrectly clustered the two ends of both half-circles. [sent-260, score-0.247]
</p><p>82 For the clustering problems, the two measures of performance we consider are misclassiﬁcation error rate (number of misclassiﬁed points divided by n) and cut cost. [sent-261, score-0.395]
</p><p>83 The MAX - CUT clustering obtained smaller misclassiﬁcation error in 98 of the 100 trials we performed and smaller cut cost in every trial. [sent-263, score-0.441]
</p><p>84 Error rate, cut cost, and running time comparison for total variation (TV) algorithms. [sent-282, score-0.275]
</p><p>85 The balance of the cut is computed as are averaged over 100 trials. [sent-283, score-0.24]
</p><p>86 |V1 |+|V2 |  spectral, and The two moons results  approximately 1 minute to run 1,000 iterations. [sent-286, score-0.106]
</p><p>87 Our MAX - CUT clustering algorithm again performs substantially better than the spectral method. [sent-289, score-0.247]
</p><p>88 7  Summary  In this paper we presented practical methods for solving very large scale optimization problems involving a max-norm constraint or regularizer. [sent-290, score-0.071]
</p><p>89 Using this approaches, we showed evidence that the max-norm can often be superior to established techniques such as trace-norm regularization and spectral clustering, supplementing previous evidence on small-scale problems. [sent-291, score-0.253]
</p><p>90 JAT supported by ONR award N00014-08-1-0883, DARPA award N66001-08-1-2065, and AFOSR award FA955009-1-0643. [sent-294, score-0.129]
</p><p>91 Pd  i=1  pi = β, (d) wi  2  ≤t  With our candidate W = squash(V , β), we need only ﬁnd t and p to verify the optimality conditions. [sent-297, score-0.107]
</p><p>92 Let π be as in Algorithm 1 and set t = η 2 and ( vk − 1 1 ≤ π(k) ≤ q η pk = 0 otherwise This deﬁnition of p immediately gives (a). [sent-298, score-0.07]
</p><p>93 For (b), note that by the deﬁnition of q, vk ≥ η for 1 ≤ π(k) ≤ q. [sent-299, score-0.07]
</p><p>94 Moreover, P d X 1≤π(k)≤q vk pk = −q =q+β−q =β, η k=1  yielding (c). [sent-301, score-0.07]
</p><p>95 Guaranteed minimum rank solutions of matrix equations via nuclear norm minimization. [sent-315, score-0.19]
</p><p>96 A generalized proximal point algorithm for certain non-convex minimization problems. [sent-345, score-0.148]
</p><p>97 A singular value thresholding algorithm for e matrix completion. [sent-376, score-0.09]
</p><p>98 Fixed point and Bregman iterative methods for matrix rank minimization. [sent-383, score-0.099]
</p><p>99 Improved approximation algorithms for maximum cut and satisﬁability problems using semideﬁnite programming. [sent-400, score-0.273]
</p><p>100 A total variation-based graph clustering algorithm for cheeger ratio cuts. [sent-414, score-0.203]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('squash', 0.392), ('ak', 0.315), ('cut', 0.24), ('projected', 0.184), ('sdplr', 0.181), ('lr', 0.169), ('burer', 0.159), ('clustering', 0.155), ('proximal', 0.148), ('ix', 0.143), ('collaborative', 0.14), ('fukushima', 0.121), ('gset', 0.121), ('gradient', 0.114), ('preprint', 0.114), ('ltering', 0.107), ('net', 0.106), ('moons', 0.106), ('quali', 0.097), ('monteiro', 0.097), ('spectral', 0.092), ('norms', 0.091), ('email', 0.089), ('rj', 0.087), ('ratings', 0.082), ('max', 0.082), ('pb', 0.076), ('yij', 0.074), ('uj', 0.072), ('vk', 0.07), ('rmse', 0.07), ('wij', 0.068), ('nathan', 0.068), ('semide', 0.067), ('samuel', 0.062), ('mine', 0.062), ('grothendieck', 0.06), ('maxnorm', 0.06), ('minimizew', 0.06), ('nonconvex', 0.057), ('regularizer', 0.057), ('primal', 0.057), ('wi', 0.054), ('li', 0.053), ('rank', 0.053), ('tol', 0.053), ('pi', 0.053), ('nuclear', 0.051), ('srebro', 0.051), ('vj', 0.05), ('nonuniform', 0.049), ('graph', 0.048), ('matrix', 0.046), ('factorization', 0.046), ('sort', 0.046), ('regularization', 0.046), ('cost', 0.046), ('epochs', 0.046), ('kkt', 0.046), ('factored', 0.045), ('singular', 0.044), ('mnist', 0.044), ('award', 0.043), ('promote', 0.043), ('armijo', 0.043), ('stochastic', 0.043), ('huge', 0.043), ('http', 0.042), ('formulation', 0.042), ('rows', 0.042), ('misclassi', 0.042), ('dd', 0.041), ('decompositions', 0.041), ('ruslan', 0.041), ('subject', 0.041), ('norm', 0.04), ('users', 0.04), ('maxj', 0.04), ('recht', 0.04), ('minimize', 0.039), ('superior', 0.039), ('programs', 0.038), ('promotes', 0.038), ('involving', 0.038), ('evidence', 0.038), ('counter', 0.037), ('pd', 0.037), ('available', 0.037), ('running', 0.035), ('rounding', 0.035), ('jth', 0.034), ('balancing', 0.034), ('sdp', 0.034), ('closed', 0.034), ('inf', 0.034), ('critical', 0.034), ('solve', 0.033), ('nesterov', 0.033), ('jason', 0.033), ('problems', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999881 <a title="210-tfidf-1" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>2 0.17361596 <a title="210-tfidf-2" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>3 0.15645291 <a title="210-tfidf-3" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>4 0.12999673 <a title="210-tfidf-4" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><p>5 0.12256341 <a title="210-tfidf-5" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>6 0.11872406 <a title="210-tfidf-6" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>7 0.11602692 <a title="210-tfidf-7" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>8 0.10831907 <a title="210-tfidf-8" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>9 0.10569905 <a title="210-tfidf-9" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>10 0.10446548 <a title="210-tfidf-10" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>11 0.10394668 <a title="210-tfidf-11" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>12 0.09308406 <a title="210-tfidf-12" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>13 0.085178405 <a title="210-tfidf-13" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>14 0.08215826 <a title="210-tfidf-14" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>15 0.07989309 <a title="210-tfidf-15" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>16 0.07604599 <a title="210-tfidf-16" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>17 0.074767575 <a title="210-tfidf-17" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>18 0.072378695 <a title="210-tfidf-18" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>19 0.072185963 <a title="210-tfidf-19" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>20 0.070150331 <a title="210-tfidf-20" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, 0.042), (2, 0.112), (3, 0.045), (4, 0.051), (5, -0.12), (6, 0.051), (7, 0.025), (8, 0.061), (9, -0.018), (10, 0.105), (11, -0.04), (12, 0.166), (13, 0.015), (14, 0.172), (15, -0.041), (16, 0.03), (17, 0.002), (18, 0.001), (19, 0.071), (20, 0.117), (21, 0.06), (22, -0.052), (23, -0.098), (24, 0.018), (25, -0.077), (26, 0.001), (27, -0.038), (28, 0.064), (29, -0.029), (30, -0.066), (31, -0.078), (32, 0.091), (33, -0.055), (34, -0.096), (35, 0.069), (36, 0.035), (37, -0.106), (38, 0.037), (39, -0.032), (40, 0.125), (41, -0.08), (42, 0.048), (43, -0.006), (44, 0.055), (45, 0.076), (46, -0.082), (47, -0.06), (48, -0.065), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95432955 <a title="210-lsi-1" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>2 0.70232296 <a title="210-lsi-2" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>Author: Prateek Jain, Raghu Meka, Inderjit S. Dhillon</p><p>Abstract: Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under afﬁne constraints (ARMP) and show that SVP recovers the minimum rank solution for afﬁne constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of lowrank matrix completion, for which the deﬁning afﬁne constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is signiﬁcantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem. 1</p><p>3 0.70201856 <a title="210-lsi-3" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>4 0.69506234 <a title="210-lsi-4" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>5 0.6718092 <a title="210-lsi-5" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><p>6 0.63920391 <a title="210-lsi-6" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>7 0.61666572 <a title="210-lsi-7" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>8 0.59310335 <a title="210-lsi-8" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>9 0.55604446 <a title="210-lsi-9" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>10 0.55457991 <a title="210-lsi-10" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>11 0.54238141 <a title="210-lsi-11" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>12 0.54213339 <a title="210-lsi-12" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>13 0.52634305 <a title="210-lsi-13" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>14 0.52286446 <a title="210-lsi-14" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>15 0.50546199 <a title="210-lsi-15" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>16 0.49826318 <a title="210-lsi-16" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>17 0.48782787 <a title="210-lsi-17" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>18 0.48165759 <a title="210-lsi-18" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>19 0.47986704 <a title="210-lsi-19" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>20 0.46439809 <a title="210-lsi-20" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.114), (17, 0.014), (27, 0.046), (30, 0.068), (35, 0.024), (45, 0.186), (50, 0.059), (51, 0.239), (52, 0.044), (60, 0.048), (77, 0.048), (78, 0.011), (90, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81319255 <a title="210-lda-1" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>2 0.76589394 <a title="210-lda-2" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>Author: Mohammad Ghavamzadeh, Alessandro Lazaric, Odalric Maillard, Rémi Munos</p><p>Abstract: We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a highdimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm. 1</p><p>3 0.72338408 <a title="210-lda-3" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>Author: Gael Varoquaux, Alexandre Gramfort, Jean-baptiste Poline, Bertrand Thirion</p><p>Abstract: Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reﬂects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data. Learning such models entails two main challenges: i) modeling full brain connectivity is a difﬁcult estimation problem that faces the curse of dimensionality and ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging. We describe subject-level brain functional connectivity structure as a multivariate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the ﬁrst report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the ﬁrst time that known cognitive networks appear as the integrated communities of functional connectivity graph. 1</p><p>4 0.70703983 <a title="210-lda-4" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>Author: Pranjal Awasthi, Reza B. Zadeh</p><p>Abstract: Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efﬁcient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will ﬁnd the target clustering under the strongest property.</p><p>5 0.7065931 <a title="210-lda-5" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>6 0.70309913 <a title="210-lda-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.69999456 <a title="210-lda-7" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>8 0.69757193 <a title="210-lda-8" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>9 0.69360834 <a title="210-lda-9" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>10 0.69303054 <a title="210-lda-10" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>11 0.69283521 <a title="210-lda-11" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>12 0.69256216 <a title="210-lda-12" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>13 0.69200277 <a title="210-lda-13" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>14 0.69189346 <a title="210-lda-14" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>15 0.69003999 <a title="210-lda-15" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>16 0.68978149 <a title="210-lda-16" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>17 0.68950629 <a title="210-lda-17" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>18 0.68923706 <a title="210-lda-18" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>19 0.68749744 <a title="210-lda-19" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>20 0.68695158 <a title="210-lda-20" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
