<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-18" href="#">nips2010-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</h1>
<br/><p>Source: <a title="nips-2010-18-pdf" href="http://papers.nips.cc/paper/4126-a-novel-family-of-non-parametric-cumulative-based-divergences-for-point-processes.pdf">pdf</a></p><p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>Reference: <a title="nips-2010-18-reference" href="../nips2010_reference/nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A novel family of non-parametric cumulative based divergences for point processes  Sohan Seth University of Florida  Il “Memming” Park University of Texas at Austin  Mulugeta Semework SUNY Downstate Medical Center  Austin J. [sent-1, score-0.223]
</p><p>2 Pr´ncipe e ı University of Florida  Abstract Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. [sent-4, score-0.158]
</p><p>3 Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. [sent-5, score-0.222]
</p><p>4 However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. [sent-6, score-0.106]
</p><p>5 In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. [sent-7, score-0.284]
</p><p>6 A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. [sent-8, score-0.209]
</p><p>7 We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. [sent-9, score-0.669]
</p><p>8 We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. [sent-10, score-0.127]
</p><p>9 1  Introduction  Neurons communicate mostly through noisy sequences of action potentials, also known as spike trains. [sent-11, score-0.411]
</p><p>10 A point process captures the stochastic properties of such sequences of events [1]. [sent-12, score-0.118]
</p><p>11 Many neuroscience problems such as model ﬁtting (goodness-of-ﬁt), plasticity detection, change point detection, non-stationarity detection, and neural code analysis can be formulated as statistical inference on point processes [2, 3]. [sent-13, score-0.215]
</p><p>12 To avoid the complication of dealing with spike train observations, neuroscientists often use summarizing statistics such as mean ﬁring rate to compare two point processes. [sent-14, score-0.536]
</p><p>13 However, this approach implicitly assumes a model for the underlying point process, and therefore, the choice of the summarizing statistic fundamentally restricts the validity of the inference procedure. [sent-15, score-0.172]
</p><p>14 One alternative to mean ﬁring rate is to use the distance between the inhomogeneous rate functions, i. [sent-16, score-0.125]
</p><p>15 In general the rate function does not fully specify a point process, and therefore, ambiguity occurs when two distinct point processes have the same rate function. [sent-19, score-0.242]
</p><p>16 Therefore, statistical tools that capture higher order statistics, such as divergences, can improve the state-of-the-art hypothesis testing framework for spike train observations, and may encourage new scientiﬁc discoveries. [sent-21, score-0.477]
</p><p>17 1  In this paper, we present a novel family of divergence measures between two point processes. [sent-22, score-0.275]
</p><p>18 Unlike ﬁring rate function based measures, a divergence measure is zero if and only if the two point processes are identical. [sent-23, score-0.322]
</p><p>19 Applying a divergence measure for hypothesis testing is, therefore, more appropriate in a statistical sense. [sent-24, score-0.25]
</p><p>20 We show that the proposed measures can be estimated from data without any assumption on the underlying probability structure. [sent-25, score-0.091]
</p><p>21 We show that the proposed measures can be consistently estimated in a parameter free manner, making them particularly useful in practice. [sent-29, score-0.166]
</p><p>22 One of the difﬁculties of dealing with continuous-time point process is the lack of well structured space on which the corresponding probability laws can be described. [sent-30, score-0.093]
</p><p>23 In this paper we follow a rather unconventional approach for describing the point process by a direct sum of Euclidean spaces of varying dimensionality, and show that the proposed divergence measures can be expressed in terms of cumulative distribution functions (CDFs) in these disjoint spaces. [sent-31, score-0.362]
</p><p>24 To be speciﬁc, we represent the point process by the probability of having a ﬁnite number of spikes and the probability of spike times given that number of spikes, and since these time values are reals, we can represent them in a Euclidean space using a CDF. [sent-32, score-0.531]
</p><p>25 We follow this particular approach since, ﬁrst, CDFs can be easily estimated consistently using empirical CDFs without any free parameter, and second, standard tests on CDFs such as Kolmogorov–Smirnov (K-S) test [7] and Cram´ r–von-Mises (C-M) test [8] are e well studied in the literature. [sent-33, score-0.195]
</p><p>26 Our work extends the conventional K-S test and C-M test on the real line to the space of spike trains. [sent-34, score-0.433]
</p><p>27 The rest of the paper is organized as follows; in section 2 we introduce the measure space where the point process is deﬁned as probability measures, in section 3 and section 4 we introduce the extended K-S and C-M divergences, and derive their respective estimators. [sent-35, score-0.119]
</p><p>28 In section 5, we compare various point process statistics in a hypothesis testing framework. [sent-37, score-0.197]
</p><p>29 2  Basic point process  We deﬁne a point process to be a probability measure over all possible spike trains. [sent-40, score-0.567]
</p><p>30 Let Ω be the set of all ﬁnite spike trains, that is, each ω ∈ Ω can be represented by a ﬁnite set of action potential timings ω = {t1 ≤ t2 ≤ . [sent-41, score-0.445]
</p><p>31 Let Ω0 , Ω1 , · · · denote the partitions of Ω such that Ωn contains all possible spike trains with exactly n events (spikes), ∞ hence Ωn = Rn . [sent-45, score-0.553]
</p><p>32 Note that Ω = n=0 Ωn is a disjoint union, and that Ω0 has only one element representing the empty spike train (no action potential). [sent-46, score-0.453]
</p><p>33 Note that any measurable set A ∈ F can be partitioned ∞ into {An = A ∩ Ωn }n=0 , such that each An is measurable in corresponding measurable space (Ωn , B (Ωn )). [sent-49, score-0.12]
</p><p>34 Here A denotes a collection of spike trains involving varying number of action potentials and corresponding action potential timings, whereas An denotes a subset of these spike trains involving only n action potentials each. [sent-50, score-1.292]
</p><p>35 A (ﬁnite) point process is deﬁned as a probability measure P on the measurable space (Ω, F) [1]. [sent-51, score-0.159]
</p><p>36 Let P and Q be two probability measures on (Ω, F), then we are interested in ﬁnding the divergence d(P, Q) between P and Q, where a divergence measure is characterized by d(P, Q) ≥ 0 and d(P, Q) = 0 ⇐⇒ P = Q. [sent-52, score-0.405]
</p><p>37 3  Extended K-S divergence  A Kolmogorov-Smirnov (K-S) type divergence between P and Q can be derived from the L1 distance between the probability measures, following the equivalent representation, d |P − Q| ≥ sup |P (A) − Q(A)| . [sent-53, score-0.405]
</p><p>38 d1 (P, Q) =  A∈F  Ω  2  (1)  Inhomogeneous Poisson Firing 0  2  3  4  5 6 8  time  Figure 1: (Left) Illustration of how the point process space is stratiﬁed. [sent-54, score-0.093]
</p><p>39 (Right) Example of spike trains stratiﬁed by their respective spike count. [sent-55, score-0.883]
</p><p>40 Since ∪i Fi ⊂ F, sup |P (Ωn )P (A|Ωn ) − Q(Ωn )Q(A|Ωn )| . [sent-61, score-0.117]
</p><p>41 sup |P (A) − Q(A)| =  d1 (P, Q) ≥ n∈N  A∈Fn  n∈N  A∈Fn  Since each Ωn is a Euclidean space, we can induce the traditional K-S test statistic by further reduc˜ ing the search space to Fn = {×i (−∞, ti ]|t = (t1 , . [sent-62, score-0.296]
</p><p>42 This results in the following inequality, (n)  (n)  sup |P (A) − Q(A)| ≥ sup |P (A) − Q(A)| = sup FP (t) − FQ (t) , ˜ A∈Fn  A∈Fn  (2)  t∈Rn  (n)  where FP (t) = P [T1 ≤ t1 ∧ . [sent-66, score-0.351]
</p><p>43 ∧ Tn ≤ tn ] is the cumulative distribution function (CDF) corresponding to the probability measure P in Ωn . [sent-69, score-0.101]
</p><p>44 Hence, we deﬁne the K-S divergence as (n)  dKS (P, Q) =  (n)  sup P (Ωn )FP (t) − Q(Ωn )FQ (t) . [sent-70, score-0.261]
</p><p>45 ˆ ˆ (n) ˆ ˆ (n) sup P (Ωn )FP (t) − Q(Ωn )FQ (t)  ˆ dKS (P, Q) = n∈N  t∈Rn  sup  = n∈N  t∈Xn ∪Yn  ˆ ˆ (n) ˆ ˆ (n) P (Ωn )FP (t) − Q(Ωn )FQ (t) ,  (4)  ˆ ˆ where Xn = X ∩ Ωn , and P and FP are the empirical probability and empirical CDF, respectively. [sent-72, score-0.234]
</p><p>46 Notice that we only search the supremum over the locations of the realizations Xn ∪ Yn and not ˆ ˆ (n) ˆ ˆ (n) the whole Rn , since the empirical CDF difference P (Ωn )FP (t) − Q(Ωn )FQ (t) only changes values at those locations. [sent-73, score-0.136]
</p><p>47 Given probability measures for each (Ωn , Fn ) denoted as Pn and Qn , there exist corresponding unique extended measures P and Q for (Ω, F) such that their restrictions to (Ωn , Fn ) coincide with Pn and Qn , hence P = Q. [sent-78, score-0.205]
</p><p>48 Note that | supremum norm,  sup · −  sup ·| ≤  |sup · − sup ·|. [sent-83, score-0.412]
</p><p>49 Due to the triangle inequality of the  (n) (n) ˆ ˆ (n) ˆ ˆ (n) sup P (Ωn )FP (t) − Q(Ωn )FQ (t) − sup P (Ωn )FP (t) − Q(Ωn )FQ (t)  t∈Rn  t∈Rn  ≤ sup t∈Rn  (n) P (Ωn )FP (t)  −  (n) Q(Ωn )FQ (t)  ˆ ˆ (n) ˆ ˆ (n) − P (Ωn )FP (t) − Q(Ωn )FQ (t) . [sent-84, score-0.393]
</p><p>50 Notice that the inequality in (2) can be made stricter by considering the supremum over not just the product of the segments (−∞, ti ] but over the all 2n − 1 possible products of the segments (−∞, ti ] and [ti , ∞) in n dimensions [7]. [sent-89, score-0.148]
</p><p>51 4  Extended C-M divergence  We can extend equation (3) to derive a Cram´ r–von-Mises (C-M) type divergence for point proe cesses. [sent-91, score-0.328]
</p><p>52 Analogous to the relation between K-S test and C-M test, we would like to use the integrated squared deviation statistics in place of the maximal deviation statistic. [sent-96, score-0.135]
</p><p>53 To simplify the notation, we deﬁne gn (x) = P (Ωn )FP (x) − Q(Ωn )FQ (x), and gn (x) = ˆ a. [sent-109, score-0.536]
</p><p>54 Note that gn − → g by the Glivenko-Cantelli theorem and ˆ − a. [sent-112, score-0.268]
</p><p>55 Without loss 2 of generality, we only ﬁnd the bound on gn dP |n − gn dP |n , then the rest is bounded similarly ˆ2 ˆ i  i  for Q. [sent-116, score-0.536]
</p><p>56 2 gn dP |n −  g n d P |n = ˆ2 ˆ ≤  2 gn dP |n −  gn dP |n + ˆ2  2 gn − gn dP |n − ˆ2  gn dP |n − ˆ2  g n d P |n ˆ2 ˆ  ˆ g n d P |n − P |n ˆ2  Applying Glivenko-Cantelli theorem and strong law of large numbers, these two terms converges since gn is bounded. [sent-117, score-1.915]
</p><p>57 All tests are quantiﬁed by the power of the test given a signiﬁcance threshold (type-I error) at 0. [sent-124, score-0.129]
</p><p>58 1  Stationary renewal processes  Renewal process is a widely used point process model that compensates the deviation from Poisson process [10]. [sent-128, score-0.371]
</p><p>59 We consider two stationary renewal processes with gamma interval distributions. [sent-129, score-0.136]
</p><p>60 Since the mean rate of the two processes are the same, the rate function statistic and Wilcoxon test does 5  H  0  20 15  1  10  0. [sent-130, score-0.308]
</p><p>61 8  1  L2 L2  10  14 18  25 33 45 61 Number of samples  82  111 150  Figure 2: Gamma distributed renewal process with shape parameter θ = 3 (H0 ) and θ = 0. [sent-147, score-0.127]
</p><p>62 (Left) Spike trains from the null and alternate hypothesis. [sent-150, score-0.203]
</p><p>63 not yield consistent result, while the proposed measures obtain high power with a small number of samples. [sent-153, score-0.139]
</p><p>64 2  Precisely timed spike trains  When the same stimulation is presented to a neuronal system, the observed spike trains sometimes show a highly repeatable spatio-temporal pattern at the millisecond time scale. [sent-156, score-1.235]
</p><p>65 Recently these precisely timed spike trains (PTST) are abundantly reported both in vivo and in vitro preparations [11, 12, 13]. [sent-157, score-0.642]
</p><p>66 A precisely timed spike train in an interval is modeled by L number of probability density and probability pairs {(fi (t), pi )}L . [sent-160, score-0.541]
</p><p>67 The equi-intensity Poisson process has the rate function λ(t) = i pi fi (t). [sent-163, score-0.181]
</p><p>68 We test if the methods can differentiate between the PTST (H0 ) and equi-intensity Poisson process (H1 ) for L = 1, 2, 3, 4 (see Figure 3 for the L = 4 case). [sent-164, score-0.092]
</p><p>69 Since the rate function proﬁle is identical for both models, the rate function statistic λL2 fails to differentiate. [sent-169, score-0.207]
</p><p>70 In contrast to the previous example, the K-S test is consistently better than the C-M statistic in this problem. [sent-171, score-0.183]
</p><p>71 6  Optimal stimulation parameter selection  Given a set of point processes, we can ﬁnd the one which is closest to a target point process in terms of the proposed divergence. [sent-172, score-0.227]
</p><p>72 1  0 0  19  37 71 136 Number of samples  261  500  19  37 71 136 number of samples  261  500  Figure 3: [Top] Precisely timed spike train model (H0 ) versus equi-intensity Poisson process (H1 ). [sent-200, score-0.535]
</p><p>73 Spike trains from the null and alternate hypothesis for L = 4. [sent-201, score-0.252]
</p><p>74 [Bottom] Comparison of the power of each method for L = 1, 2, 3, 4 on precisely timed spike train model (H0 ) versus equi-intensity Poisson process (H1 ). [sent-202, score-0.612]
</p><p>75 The rate statistic λL2 are not labeled, since they are not able to detect the difference. [sent-204, score-0.157]
</p><p>76 (Right) Wilcoxon test on the number of action potentials. [sent-205, score-0.095]
</p><p>77 optimal electrical stimulation settings to produce cortical spiking patterns similar to those observed with tactile stimuli. [sent-207, score-0.323]
</p><p>78 The target process has 240 realizations elicited by tactile stimulation of the ventral side of the ﬁrst digit with a mechanical tactor. [sent-208, score-0.402]
</p><p>79 We seek the closest out of 19 processes elicited by electrical stimulation in the thalamus. [sent-209, score-0.222]
</p><p>80 Each process has 140 realizations that correspond to a particular setting of electrical stimulation. [sent-210, score-0.162]
</p><p>81 The channel of interest and the stimulating channels were chosen to have signiﬁcant response to tactile stimulation. [sent-212, score-0.198]
</p><p>82 The results from applying the C-M, K-S, and λL2 measures between the tactile responses and the sets from each electrical stimulation setting are shown Figure 4. [sent-213, score-0.367]
</p><p>83 The overall trend among the measures is consistent, but the location of the minima does not coincide for λL2 . [sent-214, score-0.114]
</p><p>84 7  Conclusion  In this paper, we have proposed two novel measures of divergence between point processes. [sent-215, score-0.275]
</p><p>85 The proposed measures have been derived from the basic probability law of a point process and we have shown that these measures can be efﬁciently estimated consistently from data. [sent-216, score-0.351]
</p><p>86 Using divergences for statistical inference transcends ﬁrst and second order statistics, and enables distribution-free spike train analysis. [sent-217, score-0.484]
</p><p>87 2 2 The time complexity of both methods is O where n n NP (n)NQ (n) + NP (n) + NQ (n) NP (n) is the number of spike trains from P that has n spikes. [sent-218, score-0.528]
</p><p>88 2  0  #15 (100uA,125µs)  #17 (100uA,175µs)  Trials sorted by count then 1st spike  Tactile  0. [sent-223, score-0.41]
</p><p>89 04  Figure 4: (Left) Dissimilarity/divergences from tactile response across parameter sets. [sent-233, score-0.175]
</p><p>90 (Right) Responses from the tactile response (left), stimulation settings selected by λL2 (center), and the realizations selected by K-S and C-M (right). [sent-237, score-0.344]
</p><p>91 Top row shows the spike trains stratiﬁed into number of spikes and then sorted by spike times. [sent-238, score-0.998]
</p><p>92 the binned rate function estimation which has time complexity O(BN ) where B is the number of bins and N = n n(NP (n) + NQ (n)) is the total number of spikes in all the samples. [sent-241, score-0.188]
</p><p>93 Although, we have observed that the statistic based on the L2 distance between the rate functions often outperforms the proposed method, this approach involves the search for the smoothing kernel size and bin size which can make the process slow and prohibitive. [sent-242, score-0.289]
</p><p>94 Therefore, other methods should be investigated that allow two spike trains to interact irrespective of their spike counts. [sent-247, score-0.883]
</p><p>95 Other possible approaches include the kernel-based divergence measures as proposed in [17], since the measures can be applied to any abstract space. [sent-248, score-0.326]
</p><p>96 However, it requires desinging an appropriate strictly positive deﬁnite kernel on the space of spike trains. [sent-249, score-0.385]
</p><p>97 In this literature, we have presented the divergences in the context of spike trains generated by neurons. [sent-250, score-0.615]
</p><p>98 Although we have proved consistency of the proposed measures, further statistical analysis such as small sample power analysis, rate of convergence, and asymptotic properties would be interesting to address. [sent-252, score-0.121]
</p><p>99 First spikes in ensembles of human tactile afferents code complex spatial ﬁngertip events. [sent-343, score-0.253]
</p><p>100 Quantiﬁcation of inter-trial non-stationarity in spike trains from ı periodically stimulated neural cultures. [sent-364, score-0.528]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fq', 0.431), ('fp', 0.388), ('spike', 0.355), ('gn', 0.268), ('dks', 0.253), ('dcm', 0.204), ('trains', 0.173), ('tactile', 0.148), ('divergence', 0.144), ('sup', 0.117), ('statistic', 0.107), ('strati', 0.106), ('cdfs', 0.102), ('dp', 0.098), ('stimulation', 0.094), ('measures', 0.091), ('divergences', 0.087), ('dq', 0.085), ('timed', 0.085), ('spikes', 0.083), ('realizations', 0.075), ('cram', 0.074), ('renewal', 0.074), ('fn', 0.071), ('wilcoxon', 0.068), ('ptst', 0.063), ('smirnov', 0.063), ('ms', 0.062), ('processes', 0.062), ('supremum', 0.061), ('poisson', 0.059), ('action', 0.056), ('np', 0.054), ('process', 0.053), ('rn', 0.053), ('cdf', 0.052), ('ring', 0.05), ('rate', 0.05), ('hypothesis', 0.049), ('neuroscience', 0.048), ('fi', 0.048), ('power', 0.048), ('florida', 0.045), ('train', 0.042), ('tests', 0.042), ('aertsen', 0.042), ('downstate', 0.042), ('suny', 0.042), ('tn', 0.041), ('variability', 0.04), ('point', 0.04), ('kolmogorov', 0.04), ('measurable', 0.04), ('law', 0.039), ('test', 0.039), ('nq', 0.039), ('free', 0.038), ('consistently', 0.037), ('deviation', 0.036), ('potentials', 0.034), ('timings', 0.034), ('electrical', 0.034), ('cumulative', 0.034), ('ti', 0.033), ('estimator', 0.032), ('integrable', 0.032), ('binned', 0.032), ('elicited', 0.032), ('sorted', 0.032), ('testing', 0.031), ('euclidean', 0.031), ('null', 0.03), ('kernel', 0.03), ('pi', 0.03), ('precisely', 0.029), ('response', 0.027), ('measure', 0.026), ('smoothing', 0.026), ('cortical', 0.025), ('inhomogeneous', 0.025), ('plasticity', 0.025), ('summarizing', 0.025), ('events', 0.025), ('statistics', 0.024), ('pn', 0.024), ('austin', 0.023), ('consistency', 0.023), ('count', 0.023), ('channels', 0.023), ('bin', 0.023), ('coincide', 0.023), ('bins', 0.023), ('detection', 0.022), ('quanti', 0.022), ('spiking', 0.022), ('qn', 0.022), ('ensembles', 0.022), ('inequality', 0.021), ('triangle', 0.021), ('park', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="18-tfidf-1" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>2 0.26328206 <a title="18-tfidf-2" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>3 0.25291041 <a title="18-tfidf-3" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>4 0.13868167 <a title="18-tfidf-4" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>Author: Abhay Jha, Vibhav Gogate, Alexandra Meliou, Dan Suciu</p><p>Abstract: Lifted Inference algorithms for representations that combine ﬁrst-order logic and graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efﬁciency by exploiting repeated structure in the ﬁrst-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we deﬁne a set of rules that look only at the logical representation to identify models for which exact efﬁcient inference is possible. Our rules yield new tractable classes that could not be solved efﬁciently by any of the existing techniques. 1</p><p>5 0.13240702 <a title="18-tfidf-5" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>6 0.11574437 <a title="18-tfidf-6" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>7 0.095714383 <a title="18-tfidf-7" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>8 0.085589945 <a title="18-tfidf-8" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>9 0.084667698 <a title="18-tfidf-9" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>10 0.080228448 <a title="18-tfidf-10" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>11 0.07867761 <a title="18-tfidf-11" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>12 0.070642814 <a title="18-tfidf-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.070558392 <a title="18-tfidf-13" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>14 0.065525658 <a title="18-tfidf-14" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>15 0.063673995 <a title="18-tfidf-15" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>16 0.061438512 <a title="18-tfidf-16" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>17 0.060620122 <a title="18-tfidf-17" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>18 0.057739966 <a title="18-tfidf-18" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>19 0.056285392 <a title="18-tfidf-19" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>20 0.054032315 <a title="18-tfidf-20" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.023), (2, -0.081), (3, 0.178), (4, 0.061), (5, 0.246), (6, -0.017), (7, 0.061), (8, 0.138), (9, -0.064), (10, -0.039), (11, 0.043), (12, -0.031), (13, 0.117), (14, 0.071), (15, 0.013), (16, 0.103), (17, -0.032), (18, 0.102), (19, -0.154), (20, -0.087), (21, 0.098), (22, -0.177), (23, -0.003), (24, 0.0), (25, -0.022), (26, -0.024), (27, -0.052), (28, 0.027), (29, 0.127), (30, -0.052), (31, -0.011), (32, -0.034), (33, 0.099), (34, 0.016), (35, 0.031), (36, -0.118), (37, -0.078), (38, 0.035), (39, -0.071), (40, -0.006), (41, 0.007), (42, -0.005), (43, 0.122), (44, 0.031), (45, -0.102), (46, 0.084), (47, -0.02), (48, -0.054), (49, -0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95629132 <a title="18-lsi-1" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>2 0.85775143 <a title="18-lsi-2" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>3 0.80899459 <a title="18-lsi-3" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>4 0.70165104 <a title="18-lsi-4" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>5 0.50644147 <a title="18-lsi-5" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>6 0.48111668 <a title="18-lsi-6" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>7 0.44759858 <a title="18-lsi-7" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>8 0.44430569 <a title="18-lsi-8" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>9 0.42899841 <a title="18-lsi-9" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>10 0.41758242 <a title="18-lsi-10" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>11 0.39722022 <a title="18-lsi-11" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>12 0.35940623 <a title="18-lsi-12" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>13 0.32556507 <a title="18-lsi-13" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>14 0.3227669 <a title="18-lsi-14" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>15 0.31926456 <a title="18-lsi-15" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>16 0.31127775 <a title="18-lsi-16" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>17 0.29919356 <a title="18-lsi-17" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>18 0.29752642 <a title="18-lsi-18" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>19 0.2856093 <a title="18-lsi-19" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>20 0.26629952 <a title="18-lsi-20" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.035), (17, 0.019), (27, 0.071), (30, 0.054), (35, 0.018), (45, 0.146), (50, 0.057), (52, 0.115), (60, 0.031), (76, 0.269), (77, 0.059), (90, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76725143 <a title="18-lda-1" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>2 0.70957744 <a title="18-lda-2" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>3 0.61954623 <a title="18-lda-3" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: Divisive normalization (DN) has been advocated as an effective nonlinear efﬁcient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justiﬁes DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm DN as an effective efﬁcient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small. 1</p><p>4 0.61183709 <a title="18-lda-4" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>5 0.61159027 <a title="18-lda-5" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>6 0.6110692 <a title="18-lda-6" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>7 0.60214132 <a title="18-lda-7" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>8 0.59602052 <a title="18-lda-8" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>9 0.58851671 <a title="18-lda-9" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>10 0.58548647 <a title="18-lda-10" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>11 0.5848369 <a title="18-lda-11" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>12 0.58215487 <a title="18-lda-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.58208025 <a title="18-lda-13" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>14 0.57866061 <a title="18-lda-14" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>15 0.57618833 <a title="18-lda-15" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>16 0.57178855 <a title="18-lda-16" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>17 0.57088017 <a title="18-lda-17" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>18 0.57079637 <a title="18-lda-18" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>19 0.57036388 <a title="18-lda-19" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>20 0.56981558 <a title="18-lda-20" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
