<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-56" href="#">nips2010-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</h1>
<br/><p>Source: <a title="nips-2010-56-pdf" href="http://papers.nips.cc/paper/4093-deciphering-subsampled-data-adaptive-compressive-sampling-as-a-principle-of-brain-communication.pdf">pdf</a></p><p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>Reference: <a title="nips-2010-56-reference" href="../nips2010_reference/nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication  Guy Isely Redwood Center for Theoretical Neuroscience University of California, Berkeley guyi@berkeley. [sent-1, score-0.285]
</p><p>2 edu  Abstract A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. [sent-6, score-0.566]
</p><p>3 We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. [sent-7, score-0.184]
</p><p>4 The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. [sent-9, score-0.178]
</p><p>5 Most localized structures, such as sensory organs, subcortical nuclei and cortical regions, are functionally specialized and need to communicate through ﬁber projections to produce coherent brain function [14]. [sent-11, score-0.188]
</p><p>6 Rather, we study the following fundamental communication problem: How can a localized neural population interpret a signal sent to its synaptic inputs without knowledge of how the signal was sampled or what it represents? [sent-14, score-0.191]
</p><p>7 neurons of a sensory organ or a peripheral sensory area) and then communicated to the target region through an axonal ﬁber projection. [sent-17, score-0.21]
</p><p>8 Fiber projections constitute wiring bottlenecks: The number of axons connecting a pair of regions is often signiﬁcantly smaller than the number of neurons encoding the representation within each region [10]. [sent-20, score-0.255]
</p><p>9 Our results can explain experiments in which retinal projections were redirected neonatally to the auditory thalamus and the rerouting produced visually responsive cells in auditory thalamus and cortex, with properties that are typical of cells in visual cortex [12]. [sent-30, score-0.214]
</p><p>10 Speciﬁcally, we predict that neuronal ﬁring is sparser in locally projecting neurons (upper cortical layers) and less sparse in neurons with nonlocal axonal ﬁber projections. [sent-32, score-0.26]
</p><p>11 2  Background  Sparse signals: It has been shown that many natural signals falling onto sensor organs have a higherorder structure that can be well-captured by sparse representations in an adequate basis; see [9, 6] for visual input and [1, 11] for auditory. [sent-34, score-0.411]
</p><p>12 Deﬁnition 1: An ensemble of signals X within Rn has sparse underlying structure if there is a dictionary Ω ∈ Rn×p so that any point x ∈ Rn drawn from X can be expressed as x = Ωv for a sparse vector v ∈ Rp . [sent-36, score-0.569]
</p><p>13 Deﬁnition 2: An ensemble of sparse vectors V within Rp is a sparse representation of a signal ensemble X in Rn if there exists a dictionary Ω ∈ Rn×p such that the random variable X satisﬁes X = ΩV . [sent-37, score-0.538]
</p><p>14 In one formulation [15], a signal x ∈ Rn is assumed to be k-sparse in an n × p dictionary matrix Ψ; that is, x = Ψa for some vector a ∈ Rp with at most k nonzero entries. [sent-48, score-0.226]
</p><p>15 In particular, compression ratios on the order (k log p)/p are achievable for k-sparse signals using a random Φ chosen this way. [sent-57, score-0.233]
</p><p>16 Dictionary learning by sparse coding: For some natural signals there are well-known bases (e. [sent-58, score-0.318]
</p><p>17 Gabor wavelets, the DCT) in which those signals are sparse or nearly sparse. [sent-60, score-0.252]
</p><p>18 However, an arbitrary class of signals can be sparse in unknown bases, some of which give better encodings than others. [sent-61, score-0.332]
</p><p>19 It is compelling to learn a sparse dictionary for a class of signals instead of specifying one in advance. [sent-62, score-0.406]
</p><p>20 Sparse coding methods [6] learn dictionaries by minimizing the empirical mean of an energy function that combines the 2 reconstruction error with a sparseness penalty on the encoding: E(x, a, Ψ) = ||x − Ψa||2 + λS(a). [sent-63, score-0.191]
</p><p>21 1 For a ﬁxed set of signals x and encodings a, minimizing the mean value of (3) with respect to Ψ and renormalizing columns produces an improved sparse dictionary. [sent-66, score-0.332]
</p><p>22 Alternating optimization steps of this form, one can learn a dictionary that is tuned to the statistics of the class of signals studied. [sent-67, score-0.289]
</p><p>23 Sparse coding on natural stimuli has been shown to learn basis vectors that resemble the receptive ﬁelds of neurons in early sensory areas [6, 7, 8]. [sent-68, score-0.357]
</p><p>24 Notice that once an (incoherent) sparsity-inducing dictionary Ψ is learned, inferring sparse vectors a(x) from signals x is an instance of the Lasso convex optimization problem. [sent-69, score-0.427]
</p><p>25 Blind Compressed Sensing: With access to an uncompressed class of sparse signals, dictionary learning can ﬁnd a sparsity-inducing basis which can then be used for compressive sampling. [sent-70, score-0.6]
</p><p>26 Blind compressed sensing (BCS): Given a measurement matrix Φ and measurements {y1 , . [sent-73, score-0.239]
</p><p>27 , xN } drawn from an ensemble X, ﬁnd a dictionary Ψ and k-sparse vectors {b1 , . [sent-79, score-0.221]
</p><p>28 The difﬁculty is that though it is possible to learn a sparsity-inducing dictionary Θ for the measurements Y , there are many decompositions of this dictionary into Φ and a matrix Ψ since Φ has a nullspace. [sent-87, score-0.38]
</p><p>29 Thus, without additional assumptions, one cannot uniquely recover a dictionary Ψ that can reconstruct x as Ψb. [sent-88, score-0.179]
</p><p>30 3  Adaptive Compressive Sampling  It is tantalizing to hypothesize that a neural population in the brain could combine the principles of compressive sampling and dictionary learning to form sparse representations of inputs arriving through long-range ﬁber projections. [sent-89, score-0.538]
</p><p>31 Note that information processing in the brain should rely on faithful representations of the original signals but does not require a solution of the ill-posed BCS problem which involves the full reconstruction of the original signals. [sent-90, score-0.495]
</p><p>32 Adaptive compressive sampling (ACS): Given measurements Y = ΦX generated from an unknown Φ and unknown signal ensemble X with sparse underlying structure, ﬁnd signals B(Y ) which are sparse representations of X. [sent-92, score-0.661]
</p><p>33 First, the ACS problem asks only for sparse representations b of the data, not full reconstruction. [sent-94, score-0.222]
</p><p>34 Since it is unrealistic to assume that a brain region could have knowledge of how an efferent ﬁber bundle subsamples the brain region it originates from, the second difference is also crucial. [sent-96, score-0.218]
</p><p>35 We propose a relatively simple algorithm for potentially solving the ACS problem: use sparse coding for dictionary learning in the 1 As a convention in this paper, a vs. [sent-97, score-0.36]
</p><p>36 A signal x with sparse structure in dictionary Ψ is sampled by a compressing measurement matrix Φ, constituting a transmission bottleneck. [sent-101, score-0.374]
</p><p>37 The ACS coding circuit learns a dictionary Θ for y in the compressed space, but can be seen to form sparse representations b of the original data x as witnessed by the matrix RM in (6). [sent-102, score-0.728]
</p><p>38 (4) 2 Iterated minimization of the empirical mean of this function ﬁrst with respect to b and then with respect to Θ will produce a sparsity dictionary Θ for the compressed space and sparse representations b(y) of the y. [sent-105, score-0.559]
</p><p>39 Our results verify theoretically and experimentally that once the dictionary matrix Θ has converged, the objective (4) can be used to infer sparse representations of the original signals x from the compressed data y. [sent-106, score-0.721]
</p><p>40 As has been shown in the BCS work, one cannot uniquely determine Ψ with access only to the compressed signals y. [sent-107, score-0.291]
</p><p>41 In fact, given a separate set of uncompressed signals x , we calculate a reconstruction matrix RM demonstrating that the b are indeed sparse representations of the original x. [sent-109, score-0.647]
</p><p>42 Thus, instead of minimizing the prediction error, we ask for the reconstruction matrix RM that minimizes the empirical mean of the reconstruction error: E(RM ) = x − RM b 2 . [sent-118, score-0.239]
</p><p>43 As we show below, calculating (6) from a set of uncompressed signals x yields an RM that reconstructs the original signal x from b as x = RM b. [sent-120, score-0.344]
</p><p>44 Thus, we can conclude that encodings b computed by ACS are sparse representations of the original signals. [sent-121, score-0.324]
</p><p>45 1 Suppose that an ensemble of signals is compressed with a random projection Φ. [sent-124, score-0.317]
</p><p>46 2 Suppose that an ensemble of signals has a sparse representation with dictionary Ψ. [sent-127, score-0.452]
</p><p>47 If ACS converges on a sparsity-inducing dictionary, then the outputs of ACS are a sparse representation for the original signals in the dictionary of the reconstruction matrix RM given by (6). [sent-128, score-0.584]
</p><p>48 4  (a)  (b)  (c)  Figure 2: Subsets of the reconstruction matrices RM for the ACS networks trained on synthetic sparse data generated using bases (a) standard 2D, (b) 2D DCT, (c) learned by sparse coding on natural images. [sent-130, score-0.533]
</p><p>49 We use 16 × 16 image patches which are compressed by an i. [sent-133, score-0.214]
</p><p>50 Unless otherwise stated we use a compression factor of 2; that is, the 256 dimensional patches were captured by 128 measurements sent to the ACS circuit (current experiments are successful with a compression factor of 10). [sent-137, score-0.353]
</p><p>51 Learning is performed until the ACS circuit converges on a sparsity basis for the compressed space. [sent-141, score-0.364]
</p><p>52 To assess whether the sparse representations formed by the ACS circuit are representations of the original data, we estimate a reconstruction matrix RM as in (6) by correlating a set of 10,000 uncompressed image patches with their encodings b in the ACS circuit. [sent-142, score-0.826]
</p><p>53 Using RM and the ACS circuit, we reconstruct original data from compressed data. [sent-143, score-0.202]
</p><p>54 Reconstruction performance is evaluated on a test set of 1000 image patches by computing the signal-to-noise ratio of the reconstructed signals x: SN R = 10 log10  ||x||2 X 2 ||x−x||2 X 2  . [sent-144, score-0.245]
</p><p>55 For comparison, we also performed CS using the  feature sign algorithm to solve (1) using a ﬁxed sparsity basis Ψ and reconstruction given by x = Ψb. [sent-145, score-0.26]
</p><p>56 Synthetic Data: To assess ACS performance on data of known sparsity we ﬁrst generate synthetic image patches with sparse underlying structure in known bases. [sent-146, score-0.286]
</p><p>57 single pixel images), the 2D DCT basis, and a Gabor-like basis learned by sparse coding on natural images. [sent-149, score-0.343]
</p><p>58 We generate random sparse binary vectors with k = 8, multiply these vectors by the chosen basis to get images, and then compress these images to half their original lengths to get training data. [sent-150, score-0.322]
</p><p>59 4% of ACS codes have the same eight active basis vectors as were used to generate the original image patch. [sent-162, score-0.182]
</p><p>60 To explore how ACS performs in cases where the signals cannot be modeled exactly with sparse representations, we generate sparse synthetic data (k = 8) with the 2D DCT basis and add gaussian noise. [sent-164, score-0.504]
</p><p>61 (b) and (c) compare the performances of ACS, CS with a basis learned by sparse coding on natural images and CS with the DCT basis. [sent-168, score-0.394]
</p><p>62 (a)  (b)  Figure 4: (a) RM for an ACS network trained on natural images with compression factor of 2, (b) ACS reconstruction of a 128 × 128 image using increasing compression factors. [sent-172, score-0.384]
</p><p>63 For very high levels of noise CS and ACS performances begin to diverge as the advantage of knowing the true sparsity basis becomes apparent again. [sent-178, score-0.179]
</p><p>64 Natural Images: Natural image patches have sparse underlying structure in the sense that they can be well approximated by sparse linear combinations of ﬁxed bases, but they cannot be exactly reconstructed at a level of sparsity required by the theorems of CS and ACS. [sent-179, score-0.41]
</p><p>65 To explore the performance of ACS on natural images we train ACS models on compressed image patches from whitened natural images. [sent-181, score-0.318]
</p><p>66 Since there is no true sparsity basis for natural images, we perform CS either with a dictionary learned from uncompressed natural images using sparse coding or with the 2D DCT. [sent-186, score-0.752]
</p><p>67 Both the ACS sparsity basis and sparse coding basis used with CS are learned with λ ﬁxed at . [sent-187, score-0.473]
</p><p>68 More sparse encodings have a better chance of being accurately recovered from the measurements because they obey conditions of the CS theorems better. [sent-194, score-0.251]
</p><p>69 At the same time, these are less likely to be accurate encodings of the original signal since they are limited to fewer of the basis vectors for their reconstructions. [sent-195, score-0.271]
</p><p>70 As a result, reconstruction ﬁdelity as a function of λ has a maximum at the sweet spot of sparsity for CS (decreasing the value of λ leads to sparser representations). [sent-196, score-0.213]
</p><p>71 Values of λ below this point produce representations that are not sparse enough to be accurately recovered from the compressed measurements, while values of λ above it produce representations that are too sparse to accurately model the original signal even if they could be accurately recovered. [sent-197, score-0.671]
</p><p>72 Figure 3(c) compares ACS, CS with a sparse coding basis, and CS with the 2D DCT basis. [sent-199, score-0.225]
</p><p>73 1 for both ACS and the sparse coding basis used with CS) may be suboptimal. [sent-202, score-0.298]
</p><p>74 However, one reason ACS might perform better is that learning a sparsity basis in compressed space tunes the sparsity basis with respect to the measurement matrix whereas performing dictionary learning for CS estimates the sparsity basis independently of the measurement matrix. [sent-206, score-0.861]
</p><p>75 Additionally, having its sparsity basis in the compressed space means that ACS is more efﬁcient in terms of runtime than dictionary learning for CS because the lengths of basis vectors are reduced by the compression factor. [sent-207, score-0.659]
</p><p>76 ACS in brain communication: When considering ACS as a model of communication in the brain, one important question is whether it works when the representational dimensions vary from region to region. [sent-208, score-0.181]
</p><p>77 As shown in ﬁgure 3(d), the reconstruction ﬁdelity decreases in the undercomplete case because representations in that space either have fewer total active coding vectors or are signiﬁcantly less sparse. [sent-211, score-0.355]
</p><p>78 7  Another issue to consider for ACS as a model of communication in the brain is whether signal ﬁdelity is preserved through repeated communications. [sent-215, score-0.18]
</p><p>79 In our model the input of compressed natural image patches is encoded as a sparse representation in the ﬁrst region, transmitted as a compressed signal to a second region where it is encoded sparsely, and compressively transmitted once again to a third region that performs the ﬁnal encoding. [sent-217, score-0.67]
</p><p>80 A meaningful model of cortical processing would involve additional local computations on the sparse representations before retransmission. [sent-219, score-0.244]
</p><p>81 6  Discussion  In this paper, we propose ACS, a new algorithm for learning meaningful sparse representations of compressively sampled signals without access to the full signals. [sent-224, score-0.423]
</p><p>82 First, the ACS coding circuit is formed by unsupervised learning on subsampled signals and does not require knowledge of the sparsity basis of the signals nor of the measurement matrix used for subsampling. [sent-226, score-0.727]
</p><p>83 Second, the information in the fully trained ACS coding circuit is insufﬁcient to reconstruct the original signals. [sent-227, score-0.225]
</p><p>84 To assess the usefulness of the representations formed by ACS, we developed a second estimation procedure that probes the trained ACS coding circuit with the full signals and correlates signal with encoding. [sent-228, score-0.436]
</p><p>85 Similarly to the electrophysiological approach of computing receptive ﬁelds, we computed a reconstruction matrix RM . [sent-229, score-0.189]
</p><p>86 2 proves that after convergence, ACS produces representations of the full data and that the estimation procedure ﬁnds a reconstruction matrix which can reproduce the full data. [sent-231, score-0.261]
</p><p>87 In addition, the combination of ACS circuit and RM matrix can be used in practice for data compression and be directly compared with traditional CS. [sent-233, score-0.203]
</p><p>88 The recent work on BCS [4] addressed a similar problem where the sparsity basis of compressed samples is unknown. [sent-236, score-0.294]
</p><p>89 A main difference between BCS and ACS is that BCS aims for full reconstruction of the original signals from compressed signals whereas ACS does not. [sent-237, score-0.568]
</p><p>90 We have argued that full data reconstruction is not a prerequisite for communication between brain regions. [sent-239, score-0.264]
</p><p>91 However, note that ACS can be made a full reconstruction algorithm if there is limited access to uncompressed signal. [sent-240, score-0.272]
</p><p>92 In particular, our compression results with overcomplete ACS indicate that the reconstruction quality was signiﬁcantly higher than with standard CS. [sent-244, score-0.25]
</p><p>93 Additionally, the unsupervised learning with ACS may have advantages in situations where access to uncompressed signals is limited or very expensive to acquire. [sent-245, score-0.304]
</p><p>94 With ACS it is possible to do the heavy work of learning a good sparsity basis entirely in the compressed space and only a small number of samples from the uncompressed space are required to reconstruct with RM . [sent-246, score-0.45]
</p><p>95 Our results clearly demonstrate that meaningful sparse representations can be learned on the far end of wiring bottlenecks, fully unsupervised, and without any knowledge of the subsampling scheme. [sent-248, score-0.289]
</p><p>96 In addition, ACS with overcomplete or undercomplete codes suggests how sparse representations can be communicated between neural populations of different sizes. [sent-249, score-0.352]
</p><p>97 From our study, we predict that ﬁring patterns of neurons sending long-range axons might be less sparse than those involved in local connectivity, a hypothesis that could be experimentally veriﬁed. [sent-250, score-0.226]
</p><p>98 It is intriguing to think that the elegance and simplicity of compressive sampling and sparse coding could be exploited by the brain. [sent-251, score-0.309]
</p><p>99 Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. [sent-286, score-0.197]
</p><p>100 Estimating spatio-temporal receptive ﬁelds of auditory and visual neurons from their responses to natural stimuli. [sent-345, score-0.186]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('acs', 0.777), ('cs', 0.243), ('bcs', 0.157), ('dictionary', 0.154), ('compressed', 0.136), ('signals', 0.135), ('uncompressed', 0.131), ('sparse', 0.117), ('rm', 0.116), ('dct', 0.103), ('ber', 0.103), ('reconstruction', 0.102), ('compression', 0.098), ('basis', 0.092), ('coding', 0.089), ('representations', 0.086), ('compressive', 0.086), ('encodings', 0.08), ('communication', 0.072), ('brain', 0.071), ('circuit', 0.07), ('sparsity', 0.066), ('delity', 0.063), ('undercomplete', 0.057), ('subsampled', 0.056), ('receptive', 0.052), ('overcomplete', 0.05), ('projections', 0.05), ('patches', 0.05), ('wiring', 0.049), ('rf', 0.048), ('neurons', 0.047), ('snr', 0.047), ('ensemble', 0.046), ('csr', 0.046), ('reconstructions', 0.045), ('original', 0.041), ('crr', 0.039), ('region', 0.038), ('bases', 0.038), ('measurements', 0.037), ('signal', 0.037), ('auditory', 0.037), ('matrix', 0.035), ('thalamus', 0.034), ('reconstructed', 0.032), ('measurement', 0.031), ('db', 0.031), ('blind', 0.03), ('images', 0.03), ('natural', 0.028), ('image', 0.028), ('bottlenecks', 0.028), ('axonal', 0.028), ('sensory', 0.028), ('axons', 0.026), ('compressively', 0.026), ('electrophysiologists', 0.026), ('harbor', 0.026), ('sweet', 0.026), ('reconstruct', 0.025), ('synthetic', 0.025), ('population', 0.024), ('encoding', 0.023), ('populations', 0.023), ('bers', 0.023), ('inaccuracies', 0.023), ('organs', 0.023), ('regions', 0.022), ('target', 0.022), ('visual', 0.022), ('synaptic', 0.021), ('cold', 0.021), ('css', 0.021), ('genetically', 0.021), ('vectors', 0.021), ('cortical', 0.021), ('elds', 0.021), ('performances', 0.021), ('access', 0.02), ('meaningful', 0.02), ('autocorrelation', 0.02), ('compares', 0.019), ('full', 0.019), ('sending', 0.019), ('communicated', 0.019), ('spot', 0.019), ('spring', 0.019), ('explore', 0.018), ('rn', 0.018), ('transmitted', 0.018), ('functionally', 0.018), ('incoherent', 0.018), ('unsupervised', 0.018), ('learned', 0.017), ('citeseer', 0.017), ('intriguing', 0.017), ('experimentally', 0.017), ('accurately', 0.017), ('incoherence', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="56-tfidf-1" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>2 0.19339865 <a title="56-tfidf-2" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>3 0.12204384 <a title="56-tfidf-3" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>Author: Daniel Lowd, Pedro Domingos</p><p>Abstract: Arithmetic circuits (ACs) exploit context-speciﬁc independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the ﬁrst ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we ﬁnd that the methods based on sampling and learning work best: one such method (AC2 -F) is faster and usually more accurate than loopy belief propagation, mean ﬁeld, and Gibbs sampling; another (AC2 -G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines. 1</p><p>4 0.11388585 <a title="56-tfidf-4" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>5 0.10571076 <a title="56-tfidf-5" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>6 0.10400174 <a title="56-tfidf-6" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>7 0.093643144 <a title="56-tfidf-7" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>8 0.085982233 <a title="56-tfidf-8" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>9 0.080492489 <a title="56-tfidf-9" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>10 0.080468446 <a title="56-tfidf-10" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>11 0.079280317 <a title="56-tfidf-11" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>12 0.076932766 <a title="56-tfidf-12" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>13 0.07342349 <a title="56-tfidf-13" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>14 0.072452195 <a title="56-tfidf-14" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>15 0.066684052 <a title="56-tfidf-15" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>16 0.059168998 <a title="56-tfidf-16" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>17 0.054830216 <a title="56-tfidf-17" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>18 0.054808006 <a title="56-tfidf-18" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>19 0.053835623 <a title="56-tfidf-19" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>20 0.052362476 <a title="56-tfidf-20" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.05), (2, -0.129), (3, 0.085), (4, 0.063), (5, -0.048), (6, -0.012), (7, 0.083), (8, -0.093), (9, -0.024), (10, 0.04), (11, 0.039), (12, 0.002), (13, -0.067), (14, -0.12), (15, -0.1), (16, -0.015), (17, 0.026), (18, -0.058), (19, 0.055), (20, 0.097), (21, -0.013), (22, 0.032), (23, -0.062), (24, -0.055), (25, -0.063), (26, 0.022), (27, -0.062), (28, 0.045), (29, 0.017), (30, 0.106), (31, 0.202), (32, -0.166), (33, -0.015), (34, -0.016), (35, 0.037), (36, -0.051), (37, 0.062), (38, -0.086), (39, 0.01), (40, 0.069), (41, 0.034), (42, -0.068), (43, 0.054), (44, -0.0), (45, -0.082), (46, -0.157), (47, 0.092), (48, -0.125), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95221436 <a title="56-lsi-1" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>2 0.72694033 <a title="56-lsi-2" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>3 0.55268848 <a title="56-lsi-3" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>4 0.54974347 <a title="56-lsi-4" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>5 0.5473007 <a title="56-lsi-5" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>6 0.54544824 <a title="56-lsi-6" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>7 0.54524547 <a title="56-lsi-7" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>8 0.50970954 <a title="56-lsi-8" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>9 0.46756804 <a title="56-lsi-9" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>10 0.40967 <a title="56-lsi-10" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>11 0.39407966 <a title="56-lsi-11" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>12 0.39154002 <a title="56-lsi-12" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>13 0.38985118 <a title="56-lsi-13" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>14 0.35833782 <a title="56-lsi-14" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>15 0.34124306 <a title="56-lsi-15" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>16 0.33680239 <a title="56-lsi-16" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>17 0.33642703 <a title="56-lsi-17" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>18 0.31346804 <a title="56-lsi-18" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>19 0.31180069 <a title="56-lsi-19" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>20 0.3067672 <a title="56-lsi-20" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.053), (17, 0.046), (27, 0.13), (30, 0.05), (35, 0.032), (40, 0.245), (45, 0.142), (50, 0.051), (52, 0.051), (60, 0.013), (77, 0.04), (90, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8006928 <a title="56-lda-1" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>2 0.7002424 <a title="56-lda-2" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>Author: Eiji Mizutani, Stuart Dreyfus</p><p>Abstract: In the neural-network parameter space, an attractive ﬁeld is likely to be induced by singularities. In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efﬁcient methods have been previously developed that avoided plateaus. 1</p><p>3 0.65218323 <a title="56-lda-3" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>4 0.64658988 <a title="56-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.64281142 <a title="56-lda-5" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>6 0.63110745 <a title="56-lda-6" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>7 0.63104606 <a title="56-lda-7" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>8 0.62897933 <a title="56-lda-8" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>9 0.62831008 <a title="56-lda-9" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>10 0.62830812 <a title="56-lda-10" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>11 0.62777758 <a title="56-lda-11" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>12 0.62606984 <a title="56-lda-12" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>13 0.62381333 <a title="56-lda-13" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>14 0.62321925 <a title="56-lda-14" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>15 0.62258828 <a title="56-lda-15" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>16 0.62212175 <a title="56-lda-16" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>17 0.62187201 <a title="56-lda-17" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>18 0.61932087 <a title="56-lda-18" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>19 0.61864924 <a title="56-lda-19" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>20 0.61609417 <a title="56-lda-20" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
