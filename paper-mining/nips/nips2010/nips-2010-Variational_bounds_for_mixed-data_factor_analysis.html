<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 nips-2010-Variational bounds for mixed-data factor analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-284" href="#">nips2010-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 nips-2010-Variational bounds for mixed-data factor analysis</h1>
<br/><p>Source: <a title="nips-2010-284-pdf" href="http://papers.nips.cc/paper/3947-variational-bounds-for-mixed-data-factor-analysis.pdf">pdf</a></p><p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>Reference: <a title="nips-2010-284-reference" href="../nips2010_reference/nips-2010-Variational_bounds_for_mixed-data_factor_analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. [sent-11, score-0.487]
</p><p>2 The algorithm is based on a simple quadratic bound to the log-sum-exp function. [sent-12, score-0.159]
</p><p>3 In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. [sent-13, score-0.274]
</p><p>4 We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. [sent-14, score-0.378]
</p><p>5 A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. [sent-15, score-0.296]
</p><p>6 1  Introduction  Continuous latent factor models, such as factor analysis (FA) and probabilistic principal components analysis (PPCA), are very commonly used density models for continuous-valued data. [sent-17, score-0.402]
</p><p>7 They have many applications including latent factor discovery, dimensionality reduction, and missing data imputation. [sent-18, score-0.405]
</p><p>8 The factor analysis model asserts that a low-dimensional continuous latent factor zn ∈ RL underlies each high-dimensional observed data vector yn ∈ RD . [sent-19, score-0.813]
</p><p>9 Standard factor analysis models assume the prior on the latent factor has the form p(zn ) = N (zn |0, I), while the likelihood has the form p(yn |zn ) = N (yn |Wzn + µ, Σ). [sent-20, score-0.426]
</p><p>10 W is the D × L factor loading matrix, µ is an offset term, and Σ is a D × D diagonal matrix specifying the marginal noise variances. [sent-21, score-0.283]
</p><p>11 A problem arises because the Gaussian prior on p(zn ) is not conjugate to the likelihood except when yn also has a Gaussian distribution (the standard FA model). [sent-27, score-0.166]
</p><p>12 The simplest is to approximate the posterior p(zn |yn ) using a point estimate, which is equivalent to viewing the latent variables as parameters and estimating them by maximum likelihood. [sent-29, score-0.26]
</p><p>13 We refer to it as the “MM” approach to ﬁtting the general FA model since we maximize over zn in the E-step, as well as W in the M-step. [sent-32, score-0.252]
</p><p>14 The main drawback of the MM approach is that it ignores posterior uncertainty in zn , which can result in over-ﬁtting unless the model is carefully regularized [WCS08]. [sent-33, score-0.38]
</p><p>15 This is a particular concern when we have missing data. [sent-34, score-0.154]
</p><p>16 The opposite end of the model estimation spectrum is to integrate out both zn and W using Markov chain Monte Carlo methods. [sent-35, score-0.252]
</p><p>17 We will refer to this as the “SS” approach to indicate that we are integrating out both zn and W by sampling. [sent-37, score-0.252]
</p><p>18 The SS approach preserves posterior uncertainty about zn (unlike the MM approach) and is robust to missing data, but can have a signiﬁcantly higher computational cost. [sent-38, score-0.534]
</p><p>19 In this work, we study a variational EM model ﬁtting approach that preserves posterior uncertainty about zn , is robust to missing data, and is more computationally efﬁcient than SS. [sent-39, score-0.652]
</p><p>20 We refer to this as the “VM” approach to indicate that we integrate over zn in the E-step after applying a variational bound, and maximize over W in the M-step. [sent-40, score-0.37]
</p><p>21 We focus on the case of continuous (Gaussian) and categorical data. [sent-41, score-0.177]
</p><p>22 Our main contribution is the development of variational EM algorithms for factor analysis and mixtures of factor analyzers based on a simple quadratic lower bound to the multinomial likelihood (which subsumes the Bernoulli case) [Boh92]. [sent-42, score-0.825]
</p><p>23 This bound results in an EM iteration that is computationally more efﬁcient than the bound previously proposed by Jaakkola for binary PCA when the training data is fully observed [JJ96], but is less tight. [sent-43, score-0.297]
</p><p>24 2  The Generalized Mixture of Factor Analyzers Model  In this section, we describe a model for mixed continuous and discrete data that we call the generalized mixture of factor analyzers model. [sent-45, score-0.667]
</p><p>25 This model has two important special cases: mixture models and factor analysis, both for mixed continuous and discrete data. [sent-46, score-0.486]
</p><p>26 In this work, we focus on Gaussian distributed continuous data and multinomially distributed discrete data. [sent-48, score-0.217]
</p><p>27 Superscripts C and D indicate variables associated with C continuous and discrete data respectively. [sent-61, score-0.209]
</p><p>28 We let yn ∈ RDc denote the continuous data vector and 2  D ynd ∈ {1 . [sent-62, score-0.326]
</p><p>29 1 We use a 1-of-(M + 1) encoding for the D D discrete variables where a variable ynd = m is represented by a (M + 1)-dimensional vector ynd in which m’th element is set to 1, and all remaining elements equal 0. [sent-66, score-0.402]
</p><p>30 We denote the complete data C D D vector by yn = yn , yn1 , . [sent-67, score-0.222]
</p><p>31 The generative process begins by sampling a state of the mixture indicator variable qn for each data case n from a K-state multinomial distribution with parameters π. [sent-71, score-0.273]
</p><p>32 Simultaneously, a length L latent factor vector zn ∈ RL is sampled from a zero-mean Gaussian distribution with precision parameter λz . [sent-72, score-0.503]
</p><p>33 The natural parameters of the distribution over the data variables is obtained by passing the latent factor vector zn through a linear function deﬁned by a factor loading matrix and an offset term, both of which depend on the setting of the mixture indicator variable qn . [sent-74, score-1.024]
</p><p>34 C We note that the factor loading matrices for the k th mixture component are Wk ∈ RDc ×L and M +1 Dc D M +1×L C D . [sent-82, score-0.355]
</p><p>35 We deﬁne the enand µdk ∈ R , while the offsets are µk ∈ R Wdk ∈ R D D D C semble of factor loading matrices and offsets to be Wk = [Wk , W1k , W2k , . [sent-83, score-0.297]
</p><p>36 For each row of each factor loading matrix Wk , we use a Gaussian prior of the form N (0, λ−1 I). [sent-92, score-0.265]
</p><p>37 w  As mentioned at the start of this section, this general model has two important special cases: generalized factor analysis and mixture models for mixed continuous and discrete data. [sent-94, score-0.512]
</p><p>38 The factor analysis model is obtained by using one mixture component and at least one latent factor (K = 1, L > 1). [sent-95, score-0.487]
</p><p>39 The mixture model is obtained by using no latent factors and at least one mixture component (K > 1, L = 0). [sent-96, score-0.399]
</p><p>40 In the mixture model case where L = 0, the distribution is modeled through the offset parameters µk only. [sent-97, score-0.16]
</p><p>41 Before concluding this section, we point out one key difference between the current model and other latent factor models for discrete data like multinomial PCA [BJ04] and latent Dirichlet allocation (LDA) [BNJ03]. [sent-99, score-0.547]
</p><p>42 Introduction of discrete observations, however, makes it intractable to compute the posterior as the likelihood for these observations is not conjugate to the Gaussian prior on the latent factors. [sent-110, score-0.383]
</p><p>43 To overcome these problems, we propose to use a quadratic bound on the LSE function. [sent-111, score-0.159]
</p><p>44 For simplicity, we describe the bound only for one discrete measurement with K = 1 and µk = 0 in order to suppress the n, k and d subscripts. [sent-115, score-0.216]
</p><p>45 By substituting this bound in to the log-likelihood, completing the square and exponentiating, we obtain the Gaussian lower bound described below. [sent-122, score-0.234]
</p><p>46 p(yD |z, W) ≥ h(ψ)N (˜ ψ |Wz, A−1 ) y ˜ yψ  = A  −1  (10)  D  (bψ + y )  (11) 1 T ˜ h(ψ) = |2πA | exp yψ A˜ ψ − cψ y (12) 2 We use this result to obtain a lower bound for each mixed data vector yn . [sent-124, score-0.3]
</p><p>47 , A−1 ) 1 Dd  Given this pseudo observation, the computation of the posterior means mn and covariances Vn is similar to the Gaussian FA model as seen below. [sent-136, score-0.153]
</p><p>48 This result can be generalized to the mixture case in a straightforward way. [sent-137, score-0.142]
</p><p>49 The M-step is the same as in mixtures of Gaussian factor analyzers [GH96]. [sent-138, score-0.333]
</p><p>50 −1  ˜ ˜ Vn = (WT Σ  ˜ W + λz IL )−1 ,  −1  ˜ ˜ mn = V n W T Σ  ˜ yn  (13)  The only question remaining is how to obtain the value of ψ. [sent-139, score-0.142]
</p><p>51 If there is missing data, Vn will change across data cases, so the total cost will be O(N I(L3 + L2 D)). [sent-151, score-0.154]
</p><p>52 1  Comparison with Other Bounding Methods  In the binary case, the Bohning bound reduces to the following: log(1 + eη ) ≤ 1 Aη 2 − bψ η + cψ , 2 where A = 1/4, bψ = Aψ − (1 + e−ψ )−1 , and cψ = 1 Aψ 2 − (1 + e−ψ )−1 ψ + log(1 + eψ ). [sent-153, score-0.156]
</p><p>53 It is 2 interesting to compare this bound to Jaakkola’s bound [JJ96] used in [Tip98, YT04]. [sent-154, score-0.234]
</p><p>54 This bound can ˜ ˜ also be written in the quadratic form: log(1 + eη ) ≤ 1 Aξ η 2 − ˜ξ η + cξ , where Aξ = 2λξ , ˜ξ = − 1 , b ˜ b 2 2 1 1 1 1 2 ξ cξ = −λξ ξ − 2 ξ + log(1 + e ), λξ = 2ξ ( 1+e−ξ − 2 ). [sent-155, score-0.159]
</p><p>55 Consequently, the cost of an E-step is O(N I(L3 + L2 D)), even if there is no missing data (note the L3 term inside the N I loop). [sent-158, score-0.154]
</p><p>56 To explore the speed vs accuracy trade-off, we use the synthetic binary data described in [MHG08] with N = 600, D = 16, and 10% missing data. [sent-159, score-0.252]
</p><p>57 We learn on the observed entries in the data matrix and compute the mean squared error (MSE) on the held out missing entries as in [MHG08]. [sent-161, score-0.234]
</p><p>58 We see in Figure 2 (top left) that the Jaakkola bound gives a lower MSE than Bohning’s bound in less time on this data. [sent-163, score-0.234]
</p><p>59 Figure 2 (bottom left) shows that the Bohning bound exhibits much better scalability per iteration than the Jaakkola bound in this regime. [sent-169, score-0.258]
</p><p>60 The speed issue becomes more serious when combining binary variables with categorical variables. [sent-170, score-0.195]
</p><p>61 Firstly, there is no direct extension of the Jaakkola bound to the general categorical case. [sent-171, score-0.215]
</p><p>62 Hence, to combine categorical variables with binary variables, we can use the Jaakkola bound for binary and the Bohning for the rest. [sent-172, score-0.324]
</p><p>63 For computational simplicity, we use Bohning’s bound for both binary and categorical data. [sent-174, score-0.254]
</p><p>64 Various other bounds and approximations to the multinomial likelihood also exist; however, they are all more computationally intensive, and do not give an efﬁcient variational algorithm. [sent-175, score-0.213]
</p><p>65 An extension of the Jaakkola bound to the multinomial case was given in [Bou07]. [sent-177, score-0.183]
</p><p>66 This bound does not give closed form updates for the E and M steps so a numerical optimizer needs to be used (see [BL06] for details). [sent-180, score-0.142]
</p><p>67 15 MSE  1  10  10 −2 0 2 10 10 10 Prior Strength (λ ) W  −1  10  −2  10  −3  10 −2 0 2 10 10 10 Prior Strength (λ ) W  Figure 2: Top left: accuracy vs speed of variational EM with the Bohning bound (FA-VM), Jaakkola bound (FA-VJM) and HMC (FA-SS) on synthetic binary data. [sent-195, score-0.45]
</p><p>68 Bottom left: Time per iteration of EM with Bohning bound and Jaakkola bound as we vary D. [sent-196, score-0.258]
</p><p>69 We show results on the test and training sets, for 10% and 50% missing data. [sent-198, score-0.154]
</p><p>70 1  Maximize-Maximize (MM) Method  The simplest approach to ﬁt the FA model is to maximize log p(Y, Z, W|λw , λz ) with respect to Z and W, the matrix of latent factor values and the factor loading matrix. [sent-202, score-0.49]
</p><p>71 To handle missing data, we simply evaluate the gradients by only summing over the observed entries of Y. [sent-206, score-0.194]
</p><p>72 At test time, consider a data vector consisting of missing and observed components, y∗ = [y∗m , y∗o ]. [sent-207, score-0.154]
</p><p>73 To ﬁll in the missing entries, we compute ˆ ˆ ˆ z∗ = arg max p(z∗ , y∗o |W) and use it with θ to predict y∗m . [sent-208, score-0.154]
</p><p>74 We consider the case of 10% and 50% missing data. [sent-215, score-0.154]
</p><p>75 We evaluate the sensitivity of the methods to the setting of the posterior precision parameter λw by varying it over the range 10−2 to 102 . [sent-216, score-0.163]
</p><p>76 We train on the observed entries in the training set, and then compute MSE on the missing entries in the training and test sets. [sent-219, score-0.234]
</p><p>77 We can see that this sensitivity increases as a function of the missing data rate. [sent-222, score-0.219]
</p><p>78 By contrast, the VM method takes the posterior uncertainty about Z into account, resulting in almost no sensitivity to λw over this range. [sent-227, score-0.193]
</p><p>79 To handle missing data, we can simply evaluate the gradients by only summing over the observed entries of Y. [sent-234, score-0.194]
</p><p>80 We do not need to impute the missing entries on the training set. [sent-235, score-0.223]
</p><p>81 In Figure 2 (right), we see that SS is insensitive to λw , just like VM, since it also models posterior uncertainty in Z (note that the absolute MSE values are higher for SS than VM since for continuous data, VM corresponds to EM with an exact posterior). [sent-238, score-0.207]
</p><p>82 5  Experiments on Real Data  In this section, we evaluate the performance of our model on real data with mixed continuous and discrete variables. [sent-241, score-0.25]
</p><p>83 We consider the following three cases of our model: (1) a model with latent factors but no mixtures (FA) (2) a model with mixtures but no latent factors (Mix) and (3) the general mixture of factor analyzers model (MixFA). [sent-242, score-0.841]
</p><p>84 We use the validation set to determine the number of latent factors and the number of mixtures (ranges shown in the table) with imputation error (described below) as our performance objective. [sent-252, score-0.284]
</p><p>85 One way to assess the performance of a generative model is to see how well it can impute missing data. [sent-258, score-0.183]
</p><p>86 We do this by randomly introducing missing values in the test data with a missing data rate of 0. [sent-259, score-0.308]
</p><p>87 For continuous variables, we compute the imputation MSE averaged over all the missing values (these variables are standardized beforehand). [sent-261, score-0.323]
</p><p>88 For discrete variables, we report the crossˆ entropy (averaged over missing values) deﬁned as yT log p, where pm is the estimated probability ˆ that y = m and y uses the one-of-(M + 1) encoding. [sent-262, score-0.253]
</p><p>89 L and K are the ranges of number of latent factors and mixture components used for cross validation. [sent-286, score-0.319]
</p><p>90 Right: the ﬁgure shows the imputation error for each dataset for continuous and discrete variables. [sent-288, score-0.237]
</p><p>91 6  Discussion and Future Work  In this work we have proposed a new variational EM algorithm for ﬁtting factor analysis models with mixed data. [sent-290, score-0.31]
</p><p>92 The algorithm is based on the Bohning bound, a simple quadratic bound to the log-sum-exp function. [sent-291, score-0.159]
</p><p>93 In the special case of fully observed binary data, the Bohning bound iteration is theoretically faster than Jaakkola’s bound iteration and we have demonstrated this advantage empirically. [sent-292, score-0.321]
</p><p>94 More importantly, the Bohning bound also easily extends to the categorical case. [sent-293, score-0.215]
</p><p>95 This enables, for the ﬁrst time, an efﬁcient variational method for ﬁtting a factor analysis model to mixed continuous, binary, and categorical observations. [sent-294, score-0.408]
</p><p>96 In comparison to the maximize-maximize (MM) method, which forms the basis of ePCA and other matrix factorization methods, our variational EM method accounts for posterior uncertainty in the latent factors, leading to reduced sensitivity to hyper parameters. [sent-295, score-0.442]
</p><p>97 We note that the quadratic bound that we study can be used in a variety of other models, such as linear-Gaussian state-space models with categorical observations [SH03]. [sent-301, score-0.257]
</p><p>98 The bound might also be useful in the context of the correlated topic model [BL06, AX07], where similar variational EM methods have been applied. [sent-303, score-0.235]
</p><p>99 In the Bayesian statistics literature, it is common to use latent factor models combined with a probit observation model; this allows one to perform inference for the latent states using efﬁcient auxiliary-variable MCMC techniques (see e. [sent-304, score-0.411]
</p><p>100 Factor analysis with (mixed) observed and latent variables in the exponential family. [sent-446, score-0.193]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fa', 0.417), ('bohning', 0.33), ('vm', 0.255), ('zn', 0.252), ('mm', 0.196), ('lse', 0.187), ('analyzers', 0.155), ('mixfa', 0.155), ('missing', 0.154), ('mse', 0.15), ('ynd', 0.136), ('latent', 0.131), ('jaakkola', 0.126), ('em', 0.122), ('factor', 0.12), ('ss', 0.119), ('loading', 0.119), ('variational', 0.118), ('bound', 0.117), ('mixture', 0.116), ('yn', 0.111), ('discrete', 0.099), ('mix', 0.099), ('posterior', 0.098), ('categorical', 0.098), ('ases', 0.097), ('wdk', 0.097), ('qn', 0.091), ('dd', 0.08), ('continuous', 0.079), ('wk', 0.076), ('mixed', 0.072), ('multinomial', 0.066), ('sensitivity', 0.065), ('vn', 0.064), ('pca', 0.059), ('imputation', 0.059), ('hamiltonian', 0.059), ('auto', 0.058), ('ndk', 0.058), ('wzn', 0.058), ('mixtures', 0.058), ('tting', 0.057), ('hmc', 0.053), ('md', 0.047), ('bc', 0.047), ('british', 0.046), ('dk', 0.045), ('offset', 0.044), ('sm', 0.044), ('softmax', 0.042), ('quadratic', 0.042), ('entries', 0.04), ('binary', 0.039), ('bddf', 0.039), ('emtiyaz', 0.039), ('multinomially', 0.039), ('ppca', 0.039), ('rdc', 0.039), ('vjm', 0.039), ('wdd', 0.039), ('xerox', 0.039), ('dc', 0.038), ('vancouver', 0.037), ('factors', 0.036), ('cross', 0.036), ('monte', 0.035), ('adult', 0.035), ('epca', 0.034), ('columbia', 0.034), ('vs', 0.032), ('europe', 0.031), ('ld', 0.031), ('principal', 0.031), ('exponential', 0.031), ('mn', 0.031), ('variables', 0.031), ('blei', 0.03), ('gaussian', 0.03), ('diag', 0.03), ('uncertainty', 0.03), ('covariance', 0.03), ('offsets', 0.029), ('impute', 0.029), ('probit', 0.029), ('likelihood', 0.029), ('carlo', 0.029), ('dth', 0.028), ('speed', 0.027), ('yd', 0.027), ('prior', 0.026), ('family', 0.026), ('generalized', 0.026), ('sub', 0.025), ('im', 0.025), ('optimizer', 0.025), ('canada', 0.024), ('strength', 0.024), ('pseudo', 0.024), ('iteration', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="284-tfidf-1" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>2 0.17007771 <a title="284-tfidf-2" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>Author: Armand Joulin, Jean Ponce, Francis R. Bach</p><p>Abstract: Dimensionality reduction is commonly used in the setting of multi-label supervised classiﬁcation to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added beneﬁts in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efﬁcient algorithm based on convex relaxations and lowrank representations of the data, capable of handling large-scale problems. Experiments on text document classiﬁcation show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods. 1</p><p>3 0.14884514 <a title="284-tfidf-3" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>Author: Kentaro Katahira, Kazuo Okanoya, Masato Okada</p><p>Abstract: When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p><p>4 0.13333285 <a title="284-tfidf-4" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>5 0.11218139 <a title="284-tfidf-5" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>6 0.10242649 <a title="284-tfidf-6" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>7 0.098201528 <a title="284-tfidf-7" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>8 0.087674826 <a title="284-tfidf-8" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>9 0.085443966 <a title="284-tfidf-9" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>10 0.085142501 <a title="284-tfidf-10" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>11 0.083774671 <a title="284-tfidf-11" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>12 0.08283101 <a title="284-tfidf-12" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>13 0.079128832 <a title="284-tfidf-13" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>14 0.077398449 <a title="284-tfidf-14" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>15 0.07527376 <a title="284-tfidf-15" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>16 0.071040004 <a title="284-tfidf-16" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>17 0.065987736 <a title="284-tfidf-17" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>18 0.063602075 <a title="284-tfidf-18" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>19 0.062406193 <a title="284-tfidf-19" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>20 0.062175095 <a title="284-tfidf-20" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.183), (1, 0.041), (2, 0.025), (3, 0.016), (4, -0.107), (5, 0.058), (6, 0.036), (7, 0.057), (8, -0.082), (9, 0.012), (10, -0.046), (11, -0.007), (12, 0.17), (13, -0.036), (14, -0.045), (15, 0.001), (16, 0.023), (17, 0.106), (18, 0.186), (19, 0.06), (20, -0.008), (21, 0.027), (22, -0.082), (23, -0.065), (24, 0.068), (25, 0.052), (26, -0.054), (27, 0.194), (28, -0.099), (29, -0.097), (30, 0.051), (31, 0.025), (32, -0.001), (33, 0.039), (34, -0.019), (35, -0.053), (36, 0.042), (37, -0.058), (38, -0.075), (39, 0.134), (40, -0.062), (41, 0.062), (42, 0.074), (43, 0.001), (44, 0.07), (45, 0.007), (46, -0.002), (47, 0.065), (48, 0.028), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93278629 <a title="284-lsi-1" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>2 0.64398193 <a title="284-lsi-2" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>Author: Bo Thiesson, Chong Wang</p><p>Abstract: Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-speciﬁc data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is signiﬁcantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data. 1</p><p>3 0.60049647 <a title="284-lsi-3" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>Author: Armand Joulin, Jean Ponce, Francis R. Bach</p><p>Abstract: Dimensionality reduction is commonly used in the setting of multi-label supervised classiﬁcation to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added beneﬁts in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efﬁcient algorithm based on convex relaxations and lowrank representations of the data, capable of handling large-scale problems. Experiments on text document classiﬁcation show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods. 1</p><p>4 0.5715121 <a title="284-lsi-4" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>Author: Ken Takiyama, Masato Okada</p><p>Abstract: 019 020 We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary ﬁring rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis. 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053</p><p>5 0.55527639 <a title="284-lsi-5" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>6 0.55148125 <a title="284-lsi-6" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>7 0.53177029 <a title="284-lsi-7" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>8 0.53056061 <a title="284-lsi-8" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>9 0.51315755 <a title="284-lsi-9" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>10 0.48543286 <a title="284-lsi-10" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>11 0.45769295 <a title="284-lsi-11" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>12 0.4553543 <a title="284-lsi-12" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>13 0.44982281 <a title="284-lsi-13" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>14 0.44422957 <a title="284-lsi-14" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>15 0.44091865 <a title="284-lsi-15" href="./nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">216 nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>16 0.44009459 <a title="284-lsi-16" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>17 0.43590724 <a title="284-lsi-17" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>18 0.42253801 <a title="284-lsi-18" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>19 0.42219192 <a title="284-lsi-19" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>20 0.41979814 <a title="284-lsi-20" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.498), (27, 0.05), (30, 0.064), (35, 0.011), (45, 0.161), (50, 0.047), (52, 0.02), (60, 0.021), (77, 0.016), (90, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91982955 <a title="284-lda-1" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>2 0.87193507 <a title="284-lda-2" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>Author: Andrey Bernstein, Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the online binary classiﬁcation problem, where we are given m classiﬁers. At each stage, the classiﬁers map the input to the probability that the input belongs to the positive class. An online classiﬁcation meta-algorithm is an algorithm that combines the outputs of the classiﬁers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classiﬁers. In this paper, we use sensitivity and speciﬁcity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisﬁes the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classiﬁers that satisﬁes fprate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classiﬁers, we show that this algorithm takes a very simple form. To our best knowledge, this is the ﬁrst algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting. 1</p><p>3 0.85097152 <a title="284-lda-3" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>Author: Jacob Bien, Ya Xu, Michael W. Mahoney</p><p>Abstract: The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.</p><p>4 0.82165378 <a title="284-lda-4" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>Author: Arvind Agarwal, Samuel Gerber, Hal Daume</p><p>Abstract: We present a novel method for multitask learning (MTL) based on manifold regularization: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common linear subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is ﬁxed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efﬁcient and easy to implement. We show the efﬁcacy of our method on several datasets. 1</p><p>same-paper 5 0.80987293 <a title="284-lda-5" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>6 0.80935097 <a title="284-lda-6" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>7 0.78839505 <a title="284-lda-7" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>8 0.69775993 <a title="284-lda-8" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>9 0.68748927 <a title="284-lda-9" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>10 0.62213522 <a title="284-lda-10" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>11 0.61895257 <a title="284-lda-11" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>12 0.60630471 <a title="284-lda-12" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>13 0.59269774 <a title="284-lda-13" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>14 0.58742464 <a title="284-lda-14" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>15 0.58355278 <a title="284-lda-15" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>16 0.57652998 <a title="284-lda-16" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>17 0.57082522 <a title="284-lda-17" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>18 0.5705781 <a title="284-lda-18" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>19 0.57034522 <a title="284-lda-19" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>20 0.56584889 <a title="284-lda-20" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
