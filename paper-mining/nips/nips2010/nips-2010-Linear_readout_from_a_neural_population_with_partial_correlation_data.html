<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 nips-2010-Linear readout from a neural population with partial correlation data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-161" href="#">nips2010-161</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 nips-2010-Linear readout from a neural population with partial correlation data</h1>
<br/><p>Source: <a title="nips-2010-161-pdf" href="http://papers.nips.cc/paper/4076-linear-readout-from-a-neural-population-with-partial-correlation-data.pdf">pdf</a></p><p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>Reference: <a title="nips-2010-161-reference" href="../nips2010_reference/nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Linear readout from a neural population with partial correlation data  Adrien Wohrer(1) , Ranulfo Romo(2) , Christian Machens(1) (1)  Group for Neural Theory Laboratoire de Neurosciences Cognitives ´ Ecole Normale Suprieure 75005 Paris, France {adrien. [sent-1, score-1.1]
</p><p>2 mx  Abstract How much information does a neural population convey about a stimulus? [sent-6, score-0.371]
</p><p>3 Answers to this question are known to strongly depend on the correlation of response variability in neural populations. [sent-7, score-0.482]
</p><p>4 These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. [sent-8, score-1.179]
</p><p>5 Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. [sent-9, score-0.614]
</p><p>6 Our basic assumption is that noise correlations arise due to common inputs between neurons. [sent-10, score-0.403]
</p><p>7 On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. [sent-11, score-0.558]
</p><p>8 We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. [sent-13, score-0.511]
</p><p>9 We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. [sent-15, score-1.122]
</p><p>10 1  Introduction  In the ﬁeld of population coding, a recurring question is the impact on coding efﬁciency of so-called noise correlations, i. [sent-16, score-0.64]
</p><p>11 Noise correlations have been proposed to be either detrimental or beneﬁcial to the quantity of information conveyed by a population [1, 2, 3]. [sent-19, score-0.53]
</p><p>12 Also, some proposed neural coding schemes, such as those based on synchronous spike waves, fundamentally rely on second- and higher- order correlations in the population spikes [4]. [sent-20, score-0.742]
</p><p>13 The problem of noise correlations is made particularly difﬁcult by its high dimensionality along two distinct physical magnitudes: time, and number of neurons. [sent-21, score-0.403]
</p><p>14 Ideally, one should describe the probabilistic structure of any set of spike trains, at any times, for any ensemble of neurons in the population; which is clearly impossible experimentally. [sent-22, score-0.35]
</p><p>15 As a result, when recording from a 1  population of neurons with a ﬁnite number of trials, one only has access to very partial correlation data. [sent-23, score-0.969]
</p><p>16 Second, the temporal correlation structure is generally simpliﬁed (e. [sent-25, score-0.432]
</p><p>17 , by assuming stationarity) or forgotten altogether (by studying only correlation in overall spike counts). [sent-27, score-0.489]
</p><p>18 Thus, when data are pooled over experiments involving different neurons, most pairwise noise correlation indices remain unknown. [sent-29, score-0.614]
</p><p>19 In consequence, there is always a strong need to “ﬁll the gaps” in the partial correlation data extracted experimentally from a population. [sent-30, score-0.47]
</p><p>20 In this paper, we propose a method to “ﬁll the gaps” in noise correlation data, based on signal correlation data. [sent-33, score-1.105]
</p><p>21 Indeed, noise correlations reveal a proximity of connection between neurons (through shared inputs and/or reciprocal connections) which, in turn, will generally result in some covariation of the neurons’ ﬁrst-order response to stimuli. [sent-35, score-0.704]
</p><p>22 If this statistical structure is well described, it can serve as basis to randomly generate noise correlation structures, compatible with the measured signal correlation. [sent-37, score-0.795]
</p><p>23 Furthermore, to assess the impact of this randomness, one can perform repeated picks of potential noise correlation structures, each time observing the resulting impact on the coding capacity in the population. [sent-38, score-0.83]
</p><p>24 Then, this method will provide reliable estimates (average + error bar) of the impact of noise correlations on population coding, given partial noise correlation data. [sent-39, score-1.41]
</p><p>25 The population’s response is summarized by a single number for each neuron (its mean ﬁring rate during the trial), so that in turn a correlation structure is simply given by a symmetric, positive, N xN matrix. [sent-42, score-0.538]
</p><p>26 In Section 3, we detail the method used to generate random noise correlation matrices compatible with the population’s signal correlation, which we believe to be novel. [sent-43, score-0.759]
</p><p>27 In Section 4, we apply this procedure to assess the amount of information about the stimulus in the somatosensory cortex of macaques responding to tactile stimulation. [sent-44, score-0.52]
</p><p>28 2  Model of the neural population  Population activity R. [sent-45, score-0.389]
</p><p>29 We consider a population of N neurons tested over a discrete set of possible stimuli f ∈ {f1 , . [sent-46, score-0.536]
</p><p>30 At each trial, information about f can be extracted from the spike trains Si (t) using several possible readout mechanisms. [sent-52, score-0.41]
</p><p>31 In this article, we limit ourselves to the simplest type of readout: The population activity is summarized by the N -dimensional vector R = {Ri }i=1. [sent-53, score-0.349]
</p><p>32 Given a particular stimulus f , we note λi (t, f ) the probability of observing a spike from neuron i at time t regardless of other neurons’ spikes (i. [sent-59, score-0.389]
</p><p>33 The trial-averaged ﬁring rates λi (t, f ) can also be used to deﬁne the signal correlation matrix σ = {σij }i,j=1. [sent-65, score-0.538]
</p><p>34 The Pearson correlation σij measures how much the ﬁrst-order responses of neurons i and j “look alike”, both in their temporal course and across stimuli. [sent-69, score-0.66]
</p><p>35 Being a correlation matrix, σ is positive deﬁnite, with 1s on its diagonal, and off-diagonal elements between −1 and 1. [sent-70, score-0.429]
</p><p>36 As opposed to most studies which deﬁne signal correlation only based on tuning curves, it is important for our purpose to also include the time course of response in the measure of signal similarity. [sent-71, score-0.615]
</p><p>37 For this reason a parametric model must be introduced, that will allow us to infer the correlation parameters that could not be measured. [sent-75, score-0.401]
</p><p>38 We introduce a simple model in which the noise correlation matrix ρ is independent of stimulus f : For a given stimulus f , the population activity R is supposed to follow the multivariate Gaussian N (µ(f ), Q(f )), with µi (f ) = λi (f ), λi (f )λj (f ). [sent-76, score-1.326]
</p><p>39 This model is the simplest possible for our purpose, as its only free parameter is the chosen noise correlation matrix ρ, and it has often been used in the literature [8]. [sent-81, score-0.661]
</p><p>40 In our case, these are neural recordings in the primary somatosensory cortex (S1) of monkeys responding to a frequency discrimination task (see Section 4). [sent-87, score-0.43]
</p><p>41 For all pairs (i, j) of simultaneously recorded neurons (total of several hundred pairs), we computed the two correlation coefﬁcients (σij , ρij ). [sent-88, score-0.759]
</p><p>42 We ﬁnd that F (x) = b + a exp α(x − 1) 3  (5)  Figure 1: Statistical link between signal and noise correlations. [sent-90, score-0.345]
</p><p>43 A: Experimental distribution of (σij ,ρij ) across simultaneously recorded neural pairs in population data from cortical area S1 (dark gray: noise correlation coefﬁcients signiﬁcantly different from 0). [sent-91, score-1.178]
</p><p>44 We also note that the value found here for a is higher than values generally reported for noise correlations in the literature [2], possibly due to experimental limitations ; however, this has no inﬂuence on the method proposed here, only on its quantitative results. [sent-104, score-0.403]
</p><p>45 Once that function F is ﬁtted on the subset of simultaneously recorded neural pairs, we can use the statistical relation (4)-(5) to randomly generate noise correlation matrices ρ for the full neural population, on the basis of its signal correlation matrix σ. [sent-105, score-1.411]
</p><p>46 However, such a random generation is not trivial, as one must insure at the same time that individual coefﬁcients ρij follow relation (4), and that ρ remains a (positive deﬁnite) correlation matrix. [sent-106, score-0.441]
</p><p>47 As a ﬁrst step towards this generation, note that the “average” noise correlation matrix predicted by the model, that is F (σ), is itself a correlation matrix. [sent-107, score-1.062]
</p><p>48 2  Generating random correlation matrices  Wishart and anti-Wishart distributions. [sent-114, score-0.456]
</p><p>49 Then, a second step consists in renormalizing Ω by its diagonal elements, to produce a correlation matrix ρ. [sent-126, score-0.473]
</p><p>50 If one takes the generating matrix Σ = F (σ) to be itself a correlation matrix, then E(ρ) F (σ) still holds approximately, albeit with a small bias, and the variance of ρ still scales with 1/k. [sent-128, score-0.448]
</p><p>51 Distribution W(F (σ), k) could be a good candidate to generate a random correlation matrix ρ that would approximately verify E(ρ) = F (σ). [sent-129, score-0.448]
</p><p>52 7) that k must be small (typically, around 20), so that noise correlation matrices ρ generated in this way necessarily have a very low rank (anti-Wishart distribution, Figure 2, blue traces). [sent-135, score-0.669]
</p><p>53 This creates an artiﬁcial feature of the noise correlation structure which is not at all desirable. [sent-136, score-0.645]
</p><p>54 We propose here an alternative method for generating random correlation matrices, based on iterative applications of the Wishart distribution. [sent-138, score-0.401]
</p><p>55 This method allows to create random correlation matrices with a higher variance than a Wishart distribution, while retaining a much wider eigenvalue spectrum than the more simple anti-Wishart distribution. [sent-139, score-0.515]
</p><p>56 We used the signal correlation data σ observed in a 100-neuron recorded sample from area S1, and the average noise correlation F (σ) given by our experimental ﬁt of F in that same area (Figure 1). [sent-154, score-1.272]
</p><p>57 For example, in 5  Figure 2: Random generation of noise correlation matrices. [sent-162, score-0.654]
</p><p>58 A: Empirical distribution of noise correlation ρij conditioned on signal correlation σij (mean ± std). [sent-164, score-1.13]
</p><p>59 4  Linear encoding of tactile frequency in somatosensory cortex  To illustrate the interest of random noise correlation matrix generation, we come back to our experimental data. [sent-169, score-0.971]
</p><p>60 They consist of neural recordings in the somatosensory cortex of macaques during a two-frequency discrimination task. [sent-170, score-0.369]
</p><p>61 Most neurons there have a positive tuning (λi (f ) grows with f ) and positive noise correlations ; however, negative tunings (resulting in the appearance of negative signal correlations) and signiﬁcant negative noise-correlations can also be found (Figure 1-A). [sent-174, score-0.814]
</p><p>62 Our goal is to estimate the amount of information about stimulus f which can be extracted from a linear readout of neural activities, depending on the number of neurons N in the population. [sent-177, score-0.735]
</p><p>63 We thus generate a random noise correlation structure ρ following the above procedure, and assume the resulting distribution for neural activity R to follow eq. [sent-179, score-0.754]
</p><p>64 1  Linear stimulus discriminability in a neural population  Linear readout from the population. [sent-183, score-1.121]
</p><p>65 ∆f measures what we call the linear discriminability of stimulus f in this neural population. [sent-198, score-0.494]
</p><p>66 It provides an estimate of the amount of information about the stimulus linearly present in the population activity R. [sent-199, score-0.491]
</p><p>67 The previous paragraphs have described a means to estimate the linear discriminability ∆f of a given neural population, with a given noise correlation structure. [sent-202, score-0.966]
</p><p>68 For each N , ∆f (N ) is computed to approximate the linear discriminability of the best N -tuple population available from our recorded sample. [sent-208, score-0.706]
</p><p>69 As it is not tractable to test all possible N -tuples, we resort to the following recursive scheme: Search for neuron i1 with best discriminability, then search for neuron i2 with the best discriminability for 2-tuple {i1 , i2 }, etc. [sent-209, score-0.524]
</p><p>70 We term the resulting curve ∆f (N ) the discriminability curve for the population. [sent-210, score-0.468]
</p><p>71 Note that this curve is not necessarily decreasing, as the last neurons to be included in the population can actually deteriorate the overall readout, by their inﬂuence on the LDA axis a. [sent-211, score-0.614]
</p><p>72 Each draw of a sample noise correlation structure gives rise to a different discriminability curve. [sent-212, score-0.957]
</p><p>73 To better assess the possible impact of noise correlations, we performed 20 random draws of possible noise correlation structures, each time computing the discriminability curve. [sent-213, score-1.233]
</p><p>74 This produces an average discrimination curve ﬂanked by a conﬁdence interval modelling our ignorance of the exact full correlation structure in the population (Figure 3, red lines). [sent-214, score-0.889]
</p><p>75 This means that, if our statistical model for the link between signal and noise correlation (4)-(5) is correct, it is possible to assess with good precision the content of information present in a neural population, even with very partial knowledge of its correlation structure. [sent-216, score-1.292]
</p><p>76 Since the resulting conﬁdence interval on ∆f (N ) is small, one could assume that the impact of noise correlations is only driven by the “statistical average” matrix F (σ). [sent-217, score-0.506]
</p><p>77 When the noise correlation matrix ρ is (deterministically) set equal to F (σ), the resulting linear discriminability is underestimated (blue curve in Figure 3). [sent-219, score-1.051]
</p><p>78 1, induce an overcorrelation of certain neural pairs, and a decorrelation of other pairs (including a signiﬁcant minority of negative correlation indices – as observed in our data, Figure 1). [sent-221, score-0.511]
</p><p>79 The net effect of the decorrelated pairs is stronger and improves the overall discriminability in the population as compared to the “statistical average”. [sent-222, score-0.719]
</p><p>80 In our particular case, the predicted discriminability curve is actually closer to what it would be in a totally decorrelated population (ρ = 0, green curve). [sent-223, score-0.759]
</p><p>81 The resulting psychometric index ∆fmonkey can then directly be compared with ∆f , to assess the behavioral relevance of the proposed linear readout (Figure 3, black dotted line). [sent-229, score-0.556]
</p><p>82 In our model, the neurometric discriminability curve crosses the monkey’s psychometric index at around N 8. [sent-230, score-0.566]
</p><p>83 Using the “statistical average” of the noise correlation structure, the monkey’s psychometric index is approached around N 20. [sent-232, score-0.738]
</p><p>84 7  Figure 3: Discriminability curves for various correlation structures. [sent-233, score-0.401]
</p><p>85 First, a known fact: the chosen noise correlation structure in a model can have a strong impact on the neural readout. [sent-240, score-0.741]
</p><p>86 Maybe not so known is the fact that considering a simpliﬁed, “statistical average” of noise correlations may lead to dramatically different results in the estimation of certain quantities such as discriminability. [sent-241, score-0.403]
</p><p>87 Thus, inferring a noise correlation structure must be done with as much care as possible in sticking to the available structure in the data. [sent-242, score-0.714]
</p><p>88 We think the method of extrapolation of noise correlation matrices proposed here offers a means to stick closer to the statistical structure (partially) observed in the data, than more simplistic methods. [sent-243, score-0.77]
</p><p>89 Second, a comment must be made on the typical number of neurons required to attain the monkey’s behavioral level of performance (N ≤ 10 using our extrapolation method for noise correlations). [sent-244, score-0.526]
</p><p>90 No matter the exact computation and sensory modality, it is a known fact that a few sensory neurons are sufﬁcient to convey as much information about the stimulus as the monkey seems to be using, when their spikes are counted over long periods of time (typically, several hundreds of ms) [13, 14]. [sent-245, score-0.669]
</p><p>91 In this optic, we have started to study an alternative type of linear readout from a neural population, based on its instantaneous spiking activity, which we term ‘online readout’ [7]. [sent-249, score-0.4]
</p><p>92 We believe that such an approach, combined with the method proposed here to account for noise correlations with more accuracy, will lead to better approximations of the number of neurons and typical integration times used by the monkey in solving this type of task. [sent-250, score-0.77]
</p><p>93 5  Conclusion  We have proposed a new method to account for the noise correlation structure in a neural population, on the basis of partial correlation data. [sent-251, score-1.118]
</p><p>94 The method is based on the statistical link between signal and noise correlation, which is a reﬂection of the underlying neural connectivity, and can be estimated through pairwise simultaneous recordings. [sent-252, score-0.42]
</p><p>95 We applied this method to estimate the linear discriminability in N -tuples of neurons from area S1 when their spikes are counted over 200 msec. [sent-254, score-0.662]
</p><p>96 We found that less than 10 neurons can account for the monkey’s behavioral accuracy, suggesting that percepts based on full neural populations are likely based on much shorter integration times. [sent-255, score-0.356]
</p><p>97 (2006) Neural correlations, population coding and computation, Nature Reviews Neuroscience 7(5): 358–366 [4] Abeles, M. [sent-270, score-0.371]
</p><p>98 (1998) Variability and correlated noise in the discharge of neurons in motor and parietal areas of the primate cortex, Journal of Neuroscience 18(3) [6] Petersen, R. [sent-277, score-0.498]
</p><p>99 (2010) Online readout of frequency information in areas SI and SII Computational and Systems Neuroscience 2010 (CoSyne) [8] Abbott, LF and Dayan, P. [sent-286, score-0.363]
</p><p>100 (1999) The effect of correlated variability on the accuracy of a population code, Neural Computation 11(1): 91–101 [9] Horn, R. [sent-287, score-0.371]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('correlation', 0.401), ('readout', 0.322), ('discriminability', 0.312), ('population', 0.305), ('neurons', 0.231), ('ij', 0.218), ('noise', 0.213), ('correlations', 0.19), ('stimulus', 0.142), ('wishart', 0.139), ('monkey', 0.136), ('somatosensory', 0.132), ('neuron', 0.106), ('ring', 0.101), ('signal', 0.09), ('recorded', 0.089), ('spike', 0.088), ('psychometric', 0.087), ('romo', 0.086), ('cortex', 0.08), ('curve', 0.078), ('discrimination', 0.074), ('coding', 0.066), ('decorrelated', 0.064), ('expectancy', 0.064), ('spectrum', 0.059), ('tactile', 0.057), ('impact', 0.056), ('matrices', 0.055), ('ri', 0.053), ('spikes', 0.053), ('neurometric', 0.052), ('kt', 0.05), ('matrix', 0.047), ('behavioral', 0.047), ('activities', 0.046), ('activity', 0.044), ('covariation', 0.043), ('ligns', 0.043), ('macaques', 0.043), ('mexico', 0.043), ('newsome', 0.043), ('salinas', 0.043), ('vibration', 0.043), ('wohrer', 0.043), ('link', 0.042), ('variability', 0.041), ('std', 0.041), ('frequency', 0.041), ('neural', 0.04), ('generation', 0.04), ('area', 0.039), ('populations', 0.038), ('instantaneous', 0.038), ('assess', 0.038), ('pairs', 0.038), ('machens', 0.038), ('sticking', 0.038), ('index', 0.037), ('gaps', 0.037), ('experimentally', 0.037), ('traces', 0.036), ('statistical', 0.035), ('neuroscience', 0.035), ('monkeys', 0.035), ('shadlen', 0.035), ('extrapolation', 0.035), ('detrimental', 0.035), ('si', 0.034), ('tuning', 0.034), ('symmetric', 0.033), ('intricate', 0.032), ('minority', 0.032), ('supposed', 0.032), ('partial', 0.032), ('ni', 0.032), ('structure', 0.031), ('cients', 0.03), ('discharge', 0.029), ('ll', 0.029), ('dence', 0.029), ('responding', 0.028), ('positive', 0.028), ('neuronal', 0.028), ('across', 0.028), ('sensory', 0.027), ('shared', 0.027), ('psychophysical', 0.027), ('counted', 0.027), ('pdf', 0.026), ('elsewhere', 0.026), ('univ', 0.026), ('yellow', 0.026), ('convey', 0.026), ('correlated', 0.025), ('distribution', 0.025), ('measured', 0.025), ('cells', 0.025), ('dotted', 0.025), ('diagonal', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999869 <a title="161-tfidf-1" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>2 0.32152444 <a title="161-tfidf-2" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>3 0.20621674 <a title="161-tfidf-3" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>4 0.1634302 <a title="161-tfidf-4" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>5 0.14596979 <a title="161-tfidf-5" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>6 0.1304635 <a title="161-tfidf-6" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>7 0.11659696 <a title="161-tfidf-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.11534947 <a title="161-tfidf-8" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>9 0.099589743 <a title="161-tfidf-9" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>10 0.097405575 <a title="161-tfidf-10" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>11 0.09623412 <a title="161-tfidf-11" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>12 0.095977694 <a title="161-tfidf-12" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>13 0.094974533 <a title="161-tfidf-13" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>14 0.094701044 <a title="161-tfidf-14" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>15 0.086963408 <a title="161-tfidf-15" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>16 0.086386971 <a title="161-tfidf-16" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>17 0.085272573 <a title="161-tfidf-17" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>18 0.084801048 <a title="161-tfidf-18" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>19 0.080228448 <a title="161-tfidf-19" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>20 0.072851278 <a title="161-tfidf-20" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, 0.058), (2, -0.232), (3, 0.273), (4, 0.093), (5, 0.164), (6, -0.058), (7, 0.044), (8, 0.004), (9, -0.077), (10, 0.043), (11, -0.001), (12, 0.02), (13, 0.057), (14, 0.04), (15, -0.021), (16, -0.042), (17, -0.023), (18, -0.022), (19, 0.145), (20, 0.079), (21, -0.154), (22, 0.132), (23, 0.049), (24, 0.001), (25, 0.013), (26, 0.005), (27, 0.062), (28, -0.069), (29, 0.043), (30, -0.03), (31, -0.086), (32, 0.022), (33, 0.044), (34, -0.042), (35, -0.03), (36, 0.049), (37, 0.03), (38, -0.047), (39, -0.008), (40, 0.033), (41, 0.002), (42, -0.111), (43, 0.012), (44, -0.042), (45, 0.041), (46, -0.051), (47, -0.004), (48, -0.059), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96711349 <a title="161-lsi-1" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>2 0.88839173 <a title="161-lsi-2" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>3 0.86013365 <a title="161-lsi-3" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>4 0.8395012 <a title="161-lsi-4" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>5 0.67189837 <a title="161-lsi-5" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>6 0.62011218 <a title="161-lsi-6" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>7 0.60233569 <a title="161-lsi-7" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>8 0.58849609 <a title="161-lsi-8" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>9 0.57663351 <a title="161-lsi-9" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>10 0.57548469 <a title="161-lsi-10" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>11 0.50416929 <a title="161-lsi-11" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>12 0.48738056 <a title="161-lsi-12" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>13 0.48299757 <a title="161-lsi-13" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>14 0.46723911 <a title="161-lsi-14" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>15 0.45167211 <a title="161-lsi-15" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>16 0.40592223 <a title="161-lsi-16" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>17 0.39471877 <a title="161-lsi-17" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>18 0.39072353 <a title="161-lsi-18" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>19 0.38466102 <a title="161-lsi-19" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>20 0.38100192 <a title="161-lsi-20" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.046), (17, 0.016), (27, 0.2), (30, 0.037), (35, 0.034), (39, 0.01), (45, 0.148), (48, 0.205), (50, 0.048), (52, 0.039), (60, 0.032), (77, 0.068), (90, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8398844 <a title="161-lda-1" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>2 0.77498311 <a title="161-lda-2" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>3 0.77430183 <a title="161-lda-3" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>4 0.77090812 <a title="161-lda-4" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>5 0.76848108 <a title="161-lda-5" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>6 0.76798272 <a title="161-lda-6" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>7 0.7626605 <a title="161-lda-7" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>8 0.7572121 <a title="161-lda-8" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>9 0.74511164 <a title="161-lda-9" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>10 0.73366743 <a title="161-lda-10" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>11 0.72324407 <a title="161-lda-11" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>12 0.72004193 <a title="161-lda-12" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>13 0.71808553 <a title="161-lda-13" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>14 0.71132094 <a title="161-lda-14" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>15 0.71110153 <a title="161-lda-15" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>16 0.71070713 <a title="161-lda-16" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>17 0.70828444 <a title="161-lda-17" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>18 0.70719087 <a title="161-lda-18" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>19 0.70134103 <a title="161-lda-19" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>20 0.69974542 <a title="161-lda-20" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
