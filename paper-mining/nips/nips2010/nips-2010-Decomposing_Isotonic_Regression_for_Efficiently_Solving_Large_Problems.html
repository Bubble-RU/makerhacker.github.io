<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-58" href="#">nips2010-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</h1>
<br/><p>Source: <a title="nips-2010-58-pdf" href="http://papers.nips.cc/paper/4079-decomposing-isotonic-regression-for-efficiently-solving-large-problems.pdf">pdf</a></p><p>Author: Ronny Luss, Saharon Rosset, Moni Shahar</p><p>Abstract: A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. These network ﬂow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints.</p><p>Reference: <a title="nips-2010-58-reference" href="../nips2010_reference/nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 il  Abstract A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. [sent-12, score-1.117]
</p><p>2 We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. [sent-13, score-0.367]
</p><p>3 Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints. [sent-15, score-1.007]
</p><p>4 Deﬁne G as the family of isotonic functions, that is, g ∈ G satisﬁes x1  x2 ⇒ g(x1 ) ≤ g(x2 ),  where the partial order here will usually be the standard Euclidean one, i. [sent-23, score-0.789]
</p><p>5 Given these deﬁnitions, isotonic regression solves ˆ f = arg min y − g(x) 2 . [sent-26, score-0.942]
</p><p>6 g∈G  x2 if x1j ≤ x2j (1)  As many authors have noted, the optimal solution to this problem comprises a partitioning of the ˆ space X into regions obeying a monotonicity property with a constant ﬁtted to f in each region. [sent-27, score-0.291]
</p><p>7 It is clear that isotonic regression is a very attractive model for situations where monotonicity is a reasonable assumption, but other common assumptions like linearity or additivity are not. [sent-28, score-0.914]
</p><p>8 Practicality of isotonic regression has already been demonstrated in various ﬁelds and in this paper we focus on algorithms for computing isotonic regressions on large problems. [sent-30, score-1.701]
</p><p>9 An equivalent formulation of L2 isotonic regression seeks an optimal isotonic ﬁt yi at every point ˆ by solving n  minimize  (ˆi − yi )2 y i=1 yi ≤ ˆ  (2)  subject to yj ˆ ∀(i, j) ∈ I where I denotes a set of isotonic constraints. [sent-31, score-2.835]
</p><p>10 Problem (2) is a quadratic program subject to 1  simple linear constraints, and, according to a literature review, appears to be largely ignored due to computational difﬁculty on large problems. [sent-35, score-0.181]
</p><p>11 The discussion of isotonic regression originally focused on the case x ∈ R, where denoted a complete order [4]. [sent-37, score-0.889]
</p><p>12 For this case, the well known pooled adjacent violators algorithm (PAVA) efﬁciently solves the isotonic regression problem. [sent-38, score-0.942]
</p><p>13 Problem (1) can also be treated as a separable quadratic program subject to simple linear equality constraints. [sent-41, score-0.181]
</p><p>14 Related algorithms in [9] to those described here were applied to problems for scheduling reorder intervals in production systems and are of complexity O(n4 ) and connections to isotonic regression can be made through [1]. [sent-44, score-1.022]
</p><p>15 An even better complexity of O(n log n) can be obtained for the optimal solution when the isotonic constraints take a special structure such as a tree, e. [sent-48, score-0.949]
</p><p>16 1 Contribution Our novel approach to isotonic regression offers an exact solution of (1) with a complexity bounded by O(n4 ), but acts on the order of O(n3 ) for practical problems. [sent-52, score-0.988]
</p><p>17 The main goal of this paper is to make isotonic regression a reasonable computational tool for large data sets, as the assumptions in this framework are very applicable in real-world applications. [sent-54, score-0.889]
</p><p>18 Our framework solves quadratic programs with 2 × 105 variables and more than 107 constraints, a problem of size not solved anywhere in previous isotonic regression literature, and with the decomposition detailed below, even larger problems can be solved. [sent-55, score-1.104]
</p><p>19 Section 2 describes a partitioning algorithm for isotonic regression and proves convergence to the globally optimal solution. [sent-57, score-1.143]
</p><p>20 Section 4 demonstrates that the partitioning algorithm is signiﬁcantly better in practice than the O(n4 ) worst-case complexity. [sent-59, score-0.236]
</p><p>21 A and B are then said to be isotonic blocks (or obey isotonicity). [sent-68, score-0.871]
</p><p>22 A group of nodes X majorizes (minorizes) another group Y if X Y (X Y ). [sent-69, score-0.308]
</p><p>23 A group X is a majorant (minorant) of X ∪ A where A = ∪k Ai if X Ai (X Ai ) ∀i = 1 . [sent-70, score-0.208]
</p><p>24 i=1  2 Partitioning Algorithm We ﬁrst describe the structure of the classic L2 isotonic regression problem and continue to detail the partitioning algorithm. [sent-74, score-1.075]
</p><p>25 The section concludes by proving convergence of the algorithm to the globally optimal isotonic regression solution. [sent-75, score-0.957]
</p><p>26 1 Structure Problem (2) is a quadratic program subject to simple linear constraints. [sent-77, score-0.181]
</p><p>27 Observations are divided into k groups where the ﬁts in each group take the group mean observation value. [sent-79, score-0.236]
</p><p>28 This can be seen through the equations given by the following Karush-Kuhn-Tucker (KKT) conditions: 1 (a) yi = yi − ( ˆ 2  λij −  j:(i,j)∈I  λji ) j:(j,i)∈I  (b) yi ≤ yj ∀(i, j) ∈ I ˆ ˆ (c) λij ≥ 0 ∀(i, j) ∈ I (d) λij (ˆi − yj ) = 0 ∀(i, j) ∈ I. [sent-80, score-0.288]
</p><p>29 Hence λij can be non-zero only within blocks in the isotonic solution which ˆ ˆ have the same ﬁtted value. [sent-82, score-0.891]
</p><p>30 Thus, we get the familiar characterization of the isotonic regression problem as one of ﬁnding a division into isotonic blocks. [sent-87, score-1.678]
</p><p>31 2 Partitioning In order to take advantage of the optimal solution’s structure, we propose solving the isotonic regression problem (2) as a sequence of subproblems that divides a group of nodes into two groups at each iteration. [sent-89, score-1.304]
</p><p>32 An important property of our partitioning approach is that nodes separated at one iteration are never rejoined into the same group in future iterations. [sent-90, score-0.423]
</p><p>33 We now describe the partitioning criterion used for each subproblem. [sent-92, score-0.186]
</p><p>34 Suppose a current block V is optimal and thus yi = y V ∀i ∈ V. [sent-93, score-0.19]
</p><p>35 From condition (a) of the KKT conditions, we deﬁne the net ˆ∗ outﬂow of a group V as i∈V (yi − yi ). [sent-94, score-0.199]
</p><p>36 Finding two groups within V such that the net outﬂow from ˆ the higher group is greater than the net outﬂow from the lower group should be infeasible, according to the KKT conditions. [sent-95, score-0.284]
</p><p>37 isotonic) cuts through the network deﬁned by nodes in V. [sent-99, score-0.212]
</p><p>38 A cut is called isotonic if the two blocks created by the cut are isotonic. [sent-100, score-1.081]
</p><p>39 The optimal cut is determined as the cut that solves the problem max (yi − y V ) − (yi − y V ) (3) c∈CV  − i∈Vc  + i∈Vc  − + Vc (Vc )  where is the group on the lower (upper) side of the edges of cut c. [sent-101, score-0.536]
</p><p>40 In terms of isotonic regression, the optimal cut is such that the difference in the sum of the normalized ﬁts (yi − y V ) at each node of a group is maximized. [sent-102, score-1.069]
</p><p>41 The optimal cut problem (3) can also be written as the binary program maximize i xi (yi − y V ) subject to xi ≤ xj ∀(i, j) ∈ I xi ∈ {−1, +1} ∀i ∈ V. [sent-104, score-0.364]
</p><p>42 This group-wise partitioning operation is the basis for our partitioning algorithm which is explicitly given in Algorithm 1. [sent-106, score-0.372]
</p><p>43 , n}), and recursively splits each group optimally by solving subproblem (5). [sent-112, score-0.207]
</p><p>44 At each 3  iteration, a list C of potential optimal cuts for each group generated thus far is maintained, and the cut among them with the highest objective value is performed. [sent-113, score-0.278]
</p><p>45 As proven next, this algorithm terminates with the optimal global (isotonic) solution to the isotonic regression problem (2). [sent-118, score-0.969]
</p><p>46 4: for all v ∈ {w− , w+ } do 5: Set zi = yi − yv ∀i ∈ v where y v is the mean of observations in v. [sent-132, score-0.303]
</p><p>47 3 Convergence Theorem 1 next states the main result that allows for a no-regret partitioning algorithm for isotonic regression. [sent-140, score-0.975]
</p><p>48 Theorem 1 Assume a group V is a union of blocks from the optimal solution to problem (2). [sent-145, score-0.26]
</p><p>49 Then a cut made by solving (5) does not cut through any block in the global optimal solution. [sent-146, score-0.381]
</p><p>50 Deﬁne M1 (MK ) to be a minorant (majorant) block in M. [sent-149, score-0.24]
</p><p>51 , n} which is a union of (all) optimal blocks, we can conclude from this theorem that partitions never cut an optimal block. [sent-159, score-0.295]
</p><p>52 3 Efﬁcient solutions of the subproblems Linear program (5) has a special structure that can be taken advantage of in order to solve larger problems faster. [sent-163, score-0.212]
</p><p>53 We ﬁrst show why these problems can be solved faster than typical linear programs, followed by a novel decomposition of the structure that allows problems of extremely large size to be solved efﬁciently. [sent-164, score-0.193]
</p><p>54 The network ﬂow n constraints are identical to those in (6) below, but the objective is 1 i=1 (s2 + t2 ), which, to the i i 4 author’s knowledge, currently still precludes this dual from being efﬁciently solved with special network algorithms. [sent-167, score-0.253]
</p><p>55 While this structure does not help solve directly the quadratic program, the network structure allows the linear program for the subproblems to be solved very efﬁciently. [sent-168, score-0.379]
</p><p>56 The dual program to (5) is (si + ti )  minimize i∈V  λij −  subject to j:(i,j)∈I  λji − si + ti = zi  ∀i ∈ V  (6)  j:(j,i)∈I  λ, s, t ≥ 0 where again zi = yi − y V . [sent-169, score-0.376]
</p><p>57 Linear program (6) is a network ﬂow problem with |V| + 2 nodes and |I| + 2|V| arcs. [sent-170, score-0.282]
</p><p>58 The network ﬂow problem here minimizes the total sum of ﬂow over links from the source and into the sink with the goal to leave zi units of ﬂow at each node i ∈ V. [sent-172, score-0.223]
</p><p>59 Note that this is very similar to the network ﬂow problem solved in [14] where zi there represents the classiﬁcation performance on node i. [sent-173, score-0.238]
</p><p>60 nodes with zi < 0 (zi ≥ 0) where where zi = yi − y V , that are bounded only from above (below). [sent-182, score-0.338]
</p><p>61 It is trivial to observe that these nodes will be be equal to −1 (+1) in the optimal solution and that eliminating them does not affect solving (5) without them. [sent-183, score-0.275]
</p><p>62 The main idea is that it can be solved through a sequence of smaller linear programs that reduce the total size of the full linear program on each iteration. [sent-187, score-0.228]
</p><p>63 Consider a minorant group of nodes J ⊆ V and the subset of arcs IJ ⊆ I connecting them. [sent-188, score-0.402]
</p><p>64 Solving problem (5) on this reduced network with the original input z divides the nodes in J into a lower and upper group, denoted JL and JU . [sent-189, score-0.209]
</p><p>65 In addition, the same problem solved on the remaining nodes in V \ JL will give the optimal solutions to these nodes. [sent-191, score-0.217]
</p><p>66 Proposition 3 Let J ⊆ V be a minorant group of nodes in V. [sent-193, score-0.377]
</p><p>67 The optimal solution for the remaining nodes (V \ J ) can be found by solving (5) over only those nodes. [sent-196, score-0.231]
</p><p>68 The same claims can be made when J ⊆ V is a majorant group of nodes in V where ∗ instead wi = +1 ⇒ x∗ = +1 ∀i ∈ J . [sent-197, score-0.322]
</p><p>69 Clearly, the solution to Problem (5) over nodes in W has the solution with all variables equal to −1. [sent-200, score-0.219]
</p><p>70 Problem (5) can be written in the following form with separable objective:  zi xi +  maximize i∈W  subject to  zi xi i∈V\W  xi ≤ xj xi ≤ xj −1 ≤ xi ≤ 1  ∀(i, j) ∈ I, i, j ∈ W ∀(i, j) ∈ I, i ∈ V, j ∈ V \ W ∀i ∈ V 5  (7)  Start with an initial solution xi = 1 ∀i ∈ V. [sent-201, score-0.382]
</p><p>71 First, an upper triangular adjacency matrix C can be constructed to represent I, where Cij = 1 if xi ≤ xj is an isotonic constraint and Cij = 0 otherwise. [sent-209, score-0.816]
</p><p>72 A minorant (majorant) subtree with k nodes is then constructed as the upper left (lower right) k × k sub-matrix of C. [sent-210, score-0.313]
</p><p>73 1: while |V| ≥ M AXSIZE do 2: ELIMINATE A MINORANT SET OF NODES: 3: Build a minorant subtree T . [sent-219, score-0.199]
</p><p>74 ˆ 5: L = L ∪ {v ∈ T : yv = −1}, V = V \ {v ∈ T : yv = −1}. [sent-221, score-0.234]
</p><p>75 ˆ 9: U = U ∪ {v ∈ T : yv = +1}, V = V \ {v ∈ T : yv = +1}. [sent-224, score-0.234]
</p><p>76 ˆ 12: L = L ∪ {v ∈ T : yv = −1}, U = U ∪ {v ∈ T : yv = +1}. [sent-226, score-0.234]
</p><p>77 ˆ ˆ The computational bottleneck of Algorithm 2 is solving linear program (5), which is done efﬁciently by solving the dual network ﬂow problem (6). [sent-227, score-0.286]
</p><p>78 This shows that, if the ﬁrst network ﬂow problem is too large to solve, it can be solved by a sequence of smaller network ﬂow problems as illustrated in Figure 1. [sent-228, score-0.228]
</p><p>79 In the worst case, many network ﬂow problems will be solved until the original full-size network ﬂow problem is solved. [sent-230, score-0.259]
</p><p>80 The result follows from repeated application of Proposition 3 over the set of nodes V that has not yet been optimally solved for. [sent-235, score-0.222]
</p><p>81 4 Complexity of the partitioning algorithm Linear program (5) can be solved in O(n3 ) using interior point methods. [sent-236, score-0.389]
</p><p>82 Consider the case of balanced partitioning at each iteration until there are n ﬁnal blocks. [sent-240, score-0.212]
</p><p>83 In this case, we can represent the partitioning path as a binary tree with log n n levels, and at each level k, LP (5) is solved 2k times on instances of size 2k which leads to a total complexity of log n  log n  2k ( k=0  n 3 ) = n3 ( 2k  k=0  1 1 − . [sent-241, score-0.329]
</p><p>84 33, and hence in this case the partitioning algorithm has complexity O(1. [sent-245, score-0.243]
</p><p>85 LP (5) solved on the entire set of nodes in the ﬁrst picture may be too large for memory. [sent-354, score-0.179]
</p><p>86 Hence subproblems are solved on the lower left (red dots) and upper right (green dots) of the networks and some nodes are ﬁxed from the solution of these subproblems. [sent-355, score-0.284]
</p><p>87 5 Numerical experiments We here demonstrate that exact isotonic regression is computationally tractable for very large problems, and compare against the time it takes to get an approximation. [sent-384, score-0.889]
</p><p>88 We ﬁrst show the computational performance of isotonic regression on simulated data sets as large as 2 × 105 training points with more than 107 constraints. [sent-385, score-0.974]
</p><p>89 We then show the favorable predictive performance of isotonic regression on large simulated data sets. [sent-386, score-0.963]
</p><p>90 1 Large-Scale Computations Figure 2 demonstrates that the partitioning algorithm with decompositions of the partitioning step can solve very large isotonic regressions. [sent-388, score-1.245]
</p><p>91 The left ﬁgure shows that the partitioning algorithm ﬁnds the globally optimal isotonic regression solution in not much more time than it takes to ﬁnd an approximation as done in [6] for very large problems. [sent-391, score-1.185]
</p><p>92 More partitions (left axis) require solving more network ﬂow problems, however, as discussed, they reduce in size very quickly over the partitioning path, resulting in the practical complexity seen in the ﬁgure on the left. [sent-395, score-0.431]
</p><p>93 2 Predictive Performance Here we show that isotonic regression is a useful tool when the data ﬁts the monotonic framework. [sent-409, score-0.919]
</p><p>94 A comparison is made between isotonic regression and linear least squares regression. [sent-414, score-0.91]
</p><p>95 5000 training points is sufﬁcient to ﬁt the model well with up to 4 dimensions, after which linear regression outperforms the isotonic regression, and 50000 training points ﬁts the model well up with up to 5 dimensions. [sent-416, score-1.008]
</p><p>96 6 Conclusion This paper demonstrates that isotonic regression can be used to solve extremely large problems. [sent-483, score-0.948]
</p><p>97 Indeed, isotonic regression as done here performs with a complexity of O(n3 ) in practice. [sent-485, score-0.946]
</p><p>98 As also shown, isotonic regression performs well at reasonable dimensions, but suffers from overﬁtting as the dimension of the data increases. [sent-486, score-0.889]
</p><p>99 Statistical complexity of the models generated by partitioning will be examined. [sent-488, score-0.243]
</p><p>100 Furthermore, similar results will be made for isotonic regression with different loss functions. [sent-489, score-0.889]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('isotonic', 0.789), ('partitioning', 0.186), ('minorant', 0.166), ('ow', 0.153), ('irp', 0.148), ('yv', 0.117), ('cut', 0.116), ('nodes', 0.114), ('majorant', 0.111), ('mk', 0.106), ('regression', 0.1), ('program', 0.097), ('group', 0.097), ('partitions', 0.08), ('yi', 0.078), ('block', 0.074), ('pava', 0.074), ('zi', 0.073), ('network', 0.071), ('mse', 0.066), ('solved', 0.065), ('subproblems', 0.063), ('blocks', 0.06), ('complexity', 0.057), ('kkt', 0.056), ('aviv', 0.055), ('isotonicity', 0.055), ('tel', 0.055), ('lp', 0.053), ('solves', 0.053), ('vc', 0.047), ('ij', 0.043), ('optimally', 0.043), ('groups', 0.042), ('solution', 0.042), ('interior', 0.041), ('optimal', 0.038), ('favorable', 0.038), ('simplex', 0.038), ('solving', 0.037), ('axsize', 0.037), ('burdakov', 0.037), ('gpav', 0.037), ('moni', 0.037), ('sysoev', 0.037), ('simulated', 0.036), ('observations', 0.035), ('jl', 0.033), ('subtree', 0.033), ('saharon', 0.032), ('reorder', 0.032), ('val', 0.032), ('subject', 0.032), ('ls', 0.032), ('tted', 0.032), ('worst', 0.031), ('quadratic', 0.031), ('solve', 0.031), ('varies', 0.03), ('monotonic', 0.03), ('globally', 0.03), ('axis', 0.03), ('subproblem', 0.03), ('node', 0.029), ('demonstrates', 0.028), ('mosek', 0.028), ('cuts', 0.027), ('xi', 0.027), ('yj', 0.027), ('points', 0.027), ('sink', 0.026), ('bold', 0.026), ('iteration', 0.026), ('monotonicity', 0.025), ('arcs', 0.025), ('decompositions', 0.025), ('net', 0.024), ('cv', 0.024), ('divides', 0.024), ('ts', 0.024), ('programs', 0.024), ('links', 0.024), ('eliminating', 0.023), ('cij', 0.023), ('constraints', 0.023), ('intervals', 0.023), ('demonstrated', 0.023), ('dual', 0.023), ('union', 0.023), ('practice', 0.022), ('obey', 0.022), ('proposition', 0.022), ('training', 0.022), ('problems', 0.021), ('linear', 0.021), ('trivial', 0.021), ('dots', 0.021), ('variables', 0.021), ('path', 0.021), ('monotone', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="58-tfidf-1" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>Author: Ronny Luss, Saharon Rosset, Moni Shahar</p><p>Abstract: A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. These network ﬂow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints.</p><p>2 0.12759221 <a title="58-tfidf-2" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>3 0.087864466 <a title="58-tfidf-3" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>4 0.078342862 <a title="58-tfidf-4" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>5 0.077359572 <a title="58-tfidf-5" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>Author: Tamir Hazan, Raquel Urtasun</p><p>Abstract: In this paper we propose an approximated structured prediction framework for large scale graphical models and derive message-passing algorithms for learning their parameters efﬁciently. We ﬁrst relate CRFs and structured SVMs and show that in CRFs a variant of the log-partition function, known as the soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an intuitive approximation for the structured prediction problem, using duality, based on a local entropy approximation and derive an efﬁcient messagepassing algorithm that is guaranteed to converge. Unlike existing approaches, this allows us to learn efﬁciently graphical models with cycles and very large number of parameters. 1</p><p>6 0.075861514 <a title="58-tfidf-6" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>7 0.073055848 <a title="58-tfidf-7" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>8 0.071374021 <a title="58-tfidf-8" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>9 0.068972446 <a title="58-tfidf-9" href="./nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">141 nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>10 0.063747764 <a title="58-tfidf-10" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>11 0.063461594 <a title="58-tfidf-11" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>12 0.063170001 <a title="58-tfidf-12" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>13 0.057889163 <a title="58-tfidf-13" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>14 0.055554364 <a title="58-tfidf-14" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>15 0.051382933 <a title="58-tfidf-15" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>16 0.050344642 <a title="58-tfidf-16" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>17 0.050101008 <a title="58-tfidf-17" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>18 0.049862783 <a title="58-tfidf-18" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>19 0.048846494 <a title="58-tfidf-19" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>20 0.048390701 <a title="58-tfidf-20" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, 0.032), (2, 0.057), (3, 0.041), (4, -0.041), (5, -0.076), (6, -0.02), (7, 0.064), (8, 0.063), (9, 0.002), (10, -0.034), (11, -0.041), (12, -0.028), (13, 0.071), (14, -0.027), (15, -0.059), (16, -0.04), (17, -0.042), (18, -0.026), (19, -0.066), (20, 0.078), (21, -0.04), (22, 0.03), (23, 0.019), (24, 0.007), (25, -0.045), (26, -0.071), (27, 0.019), (28, -0.015), (29, -0.008), (30, -0.039), (31, 0.033), (32, 0.065), (33, -0.012), (34, -0.062), (35, 0.047), (36, -0.014), (37, -0.038), (38, 0.01), (39, 0.053), (40, -0.053), (41, 0.029), (42, -0.128), (43, -0.07), (44, 0.023), (45, -0.007), (46, 0.01), (47, -0.085), (48, 0.002), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92267776 <a title="58-lsi-1" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>Author: Ronny Luss, Saharon Rosset, Moni Shahar</p><p>Abstract: A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. These network ﬂow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints.</p><p>2 0.78539175 <a title="58-lsi-2" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>3 0.64430338 <a title="58-lsi-3" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>Author: Gilbert Leung, Novi Quadrianto, Kostas Tsioutsiouliklis, Alex J. Smola</p><p>Abstract: We present a fast online solver for large scale parametric max-ﬂow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-ﬂow problems by performing stochastic gradient descent on a set of ﬂows. We apply the algorithm to optimize tier arrangement of over 84 million web pages on a layered set of caches to serve an incoming query stream optimally. 1</p><p>4 0.59579682 <a title="58-lsi-4" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>Author: Nikos Karampatziakis</p><p>Abstract: We cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence. In general, inference in segmentation models, such as semi-CRFs, can be cubic in the length of the sequence. By taking advantage of the structure of our problem, we derive a linear-time inference algorithm which makes our approach practical, given that even small programs are tens or hundreds of thousands bytes long. Furthermore, we introduce two loss functions which are appropriate for our problem and show how to use structural SVMs to optimize the learned mapping for these losses. Finally, we present experimental results that demonstrate the advantages of our method against a strong baseline. 1</p><p>5 0.58855665 <a title="58-lsi-5" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>Author: Katya Scheinberg, Shiqian Ma, Donald Goldfarb</p><p>Abstract: Gaussian graphical models are of great interest in statistical learning. Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an ℓ1 -regularization term. In this paper, we propose a ﬁrst-order method based on an alternating linearization technique that exploits the problem’s special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an ϵ-optimal solution in O(1/ϵ) iterations. Numerical experiments on both synthetic and real data from gene association networks show that a practical version of this algorithm outperforms other competitive algorithms. 1</p><p>6 0.57681417 <a title="58-lsi-6" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>7 0.56485003 <a title="58-lsi-7" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>8 0.54998642 <a title="58-lsi-8" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>9 0.53697091 <a title="58-lsi-9" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>10 0.53678393 <a title="58-lsi-10" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>11 0.52498066 <a title="58-lsi-11" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>12 0.49659094 <a title="58-lsi-12" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>13 0.47471488 <a title="58-lsi-13" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>14 0.45932066 <a title="58-lsi-14" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>15 0.4578588 <a title="58-lsi-15" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>16 0.45199338 <a title="58-lsi-16" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>17 0.43485901 <a title="58-lsi-17" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>18 0.41871589 <a title="58-lsi-18" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>19 0.41781527 <a title="58-lsi-19" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>20 0.4166964 <a title="58-lsi-20" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.049), (17, 0.012), (27, 0.044), (30, 0.436), (35, 0.012), (45, 0.181), (50, 0.051), (52, 0.026), (60, 0.035), (77, 0.049), (90, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99038583 <a title="58-lda-1" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>Author: Hariharan Narayanan, Sanjoy Mitter</p><p>Abstract: The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of ﬁtting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume log 1 and curvature is unavoidable. Whether the known lower bound of O( k + 2 δ ) 2 for the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of 2 log 1 log4 k log 1 O( k2 + 2 δ ) to O k min k, 2 + 2 δ . Based on these results, we 2 devise a simple algorithm for k−means and another that uses a family of convex programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional data, where the sample complexity is independent of the ambient dimension. 1</p><p>2 0.9774043 <a title="58-lda-2" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>3 0.95117289 <a title="58-lda-3" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>Author: Alexandre Bouchard-côté, Michael I. Jordan</p><p>Abstract: Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. 1</p><p>same-paper 4 0.89276123 <a title="58-lda-4" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>Author: Ronny Luss, Saharon Rosset, Moni Shahar</p><p>Abstract: A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. These network ﬂow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints.</p><p>5 0.85744846 <a title="58-lda-5" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>Author: Tian Lan, Yang Wang, Weilong Yang, Greg Mori</p><p>Abstract: We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predeﬁned structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can signiﬁcantly improve activity recognition performance. 1</p><p>6 0.83948249 <a title="58-lda-6" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>7 0.79697794 <a title="58-lda-7" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>8 0.7368294 <a title="58-lda-8" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>9 0.72744203 <a title="58-lda-9" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>10 0.70795137 <a title="58-lda-10" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>11 0.70579666 <a title="58-lda-11" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>12 0.69806534 <a title="58-lda-12" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>13 0.68624288 <a title="58-lda-13" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>14 0.68194044 <a title="58-lda-14" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>15 0.68127733 <a title="58-lda-15" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>16 0.67687875 <a title="58-lda-16" href="./nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">274 nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<p>17 0.6717177 <a title="58-lda-17" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>18 0.66639906 <a title="58-lda-18" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>19 0.66371179 <a title="58-lda-19" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>20 0.65840167 <a title="58-lda-20" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
