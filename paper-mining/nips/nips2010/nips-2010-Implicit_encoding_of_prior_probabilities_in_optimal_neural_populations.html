<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-119" href="#">nips2010-119</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</h1>
<br/><p>Source: <a title="nips-2010-119-pdf" href="http://papers.nips.cc/paper/4130-implicit-encoding-of-prior-probabilities-in-optimal-neural-populations.pdf">pdf</a></p><p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="nips-2010-119-reference" href="../nips2010_reference/nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here we consider the inﬂuence of a prior probability distribution over sensory variables on the optimal allocation of neurons and spikes in a population. [sent-5, score-0.469]
</p><p>2 We model the spikes of each cell as samples from an independent Poisson process with rate governed by an associated tuning curve. [sent-6, score-0.503]
</p><p>3 For this response model, we approximate the Fisher information in terms of the density and amplitude of the tuning curves, under the assumption that tuning width varies inversely with cell density. [sent-7, score-1.064]
</p><p>4 This family includes lower bounds on mutual information and perceptual discriminability as special cases. [sent-9, score-0.396]
</p><p>5 In all cases, we ﬁnd a closed form expression for the optimum, in which the density and gain of the cells in the population are power law functions of the stimulus prior. [sent-10, score-0.968]
</p><p>6 We show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus priors, physiologically measured neural response properties (cell density, tuning widths, and ﬁring rates), and psychophysically measured discrimination thresholds. [sent-12, score-0.933]
</p><p>7 1  Introduction  Many bottom up theories of neural encoding posit that sensory systems are optimized to represent sensory information, subject to limitations of noise and resources (e. [sent-13, score-0.498]
</p><p>8 A substantial literature has considered population models in which each neuron’s mean response to a scalar variable is characterized by a tuning curve [e. [sent-17, score-0.802]
</p><p>9 In these results, the distribution of sensory variables is assumed to be uniform and the populations are assumed to be homogeneous with regard to tuning curve shape, spacing, and amplitude. [sent-21, score-0.618]
</p><p>10 It would seem natural that a neural system should devote more resources to regions of sensory space that occur with higher probability, analogous to results in coding theory [11]. [sent-23, score-0.328]
</p><p>11 At the population level, non-uniform allocations of neurons with identical tuning curves have been shown to be optimal for non-uniform stimulus distributions [16, 17]. [sent-25, score-1.216]
</p><p>12 Here, we examine the inﬂuence of a sensory prior on the optimal allocation of neurons and spikes in a population, and the implications of this optimal allocation for subsequent perception. [sent-26, score-0.568]
</p><p>13 Given a prior distribution over a scalar stimulus parameter, and a resource budget of N neurons with an average of R spikes/sec for the entire population, we seek the optimal shapes, positions, and amplitudes of tuning curves. [sent-27, score-0.86]
</p><p>14 We assume a population with independent Poisson spiking, and consider a family of objective functions based on Fisher information. [sent-28, score-0.415]
</p><p>15 We then approximate the Fisher information in terms of two continuous resource variables, the density and gain of the tuning curves. [sent-29, score-0.668]
</p><p>16 For all objective functions, we ﬁnd that the optimal tuning curve properties (cell density, tuning width, and gain) are power-law functions of the stimulus prior, with exponents dependent on the speciﬁc choice of objective function. [sent-31, score-1.14]
</p><p>17 Through the Fisher information, we also derive a bound on perceptual discriminability, again in the form a power-law of the stimulus prior. [sent-32, score-0.362]
</p><p>18 Thus, our framework provides direct and experimentally testable links between sensory priors, properties of the neural representation, and perceptual discriminability. [sent-33, score-0.396]
</p><p>19 2  Encoding model  We assume a conventional model for a population of N neurons responding to a single scalar variable, s [1–6]. [sent-35, score-0.483]
</p><p>20 The number of spikes emitted (per unit time) by the nth neuron is a sample from an independent Poisson process, with mean rate determined by its tuning function, hn (s). [sent-36, score-0.573]
</p><p>21 The probability density of the population response can be written as N  p(r|s) =  hn (s)rn e−hn (s) . [sent-37, score-0.645]
</p><p>22 n=1  We also assume the total expected spike rate, R, of the population is ﬁxed, which places a constraint on the tuning curves: N  hn (s) ds = R,  p(s)  (1)  n=1  where p(s) is the probability distribution of stimuli in the environment. [sent-39, score-0.99]
</p><p>23 We refer to this as a sensory prior, in anticipation of its future use in Bayesian decoding of the population response. [sent-40, score-0.567]
</p><p>24 To formulate a family of objective functions which depend on both p(s), and the tuning curves, we ﬁrst rely on Fisher information, If (s), which can be written as a function of the tuning curves [1, 18]: If (s) = −  p(r|s)  ∂2 log p(r|s) dr ∂s2  N  =  h′2 (s) n . [sent-42, score-0.894]
</p><p>25 hn (s) n=1  The Fisher information can be used to express lower bounds on mutual information [16], the variance of an unbiased estimator [18], and perceptual discriminability [19]. [sent-43, score-0.524]
</p><p>26 The Cramer-Rao inequality allows us to express the minimum expected squared 2  stimulus discriminability achievable by any decoder1 : p(s) ds. [sent-45, score-0.43]
</p><p>27 We formulate a generalized objective function that includes the Fisher bounds on information and discriminability as special cases: N  N  arg max hn (s)  p(s) f  h′2 (s) n hn (s) n=1  ds,  s. [sent-47, score-0.496]
</p><p>28 To make the problem tractable, we ﬁrst introduce a parametrization of the population in terms of cell density and gain. [sent-62, score-0.593]
</p><p>29 The cell density controls both the spacing and width of the tuning curves, and the gain controls their maximum average ﬁring rates. [sent-63, score-0.844]
</p><p>30 Finally, re-writing the objective function and constraints in these terms allows us to obtain closed-form solutions for the optimal tuning curves. [sent-65, score-0.463]
</p><p>31 1  Density and gain for a homogeneous population  If p(s) is uniform, then by symmetry, the Fisher information for an optimal neural population should also be uniform. [sent-67, score-0.932]
</p><p>32 We assume a convolutional population of tuning curves, evenly spaced on the unit lattice, such that they approximately “tile” the space: N  h(s − n) ≈ 1. [sent-68, score-0.869]
</p><p>33 n=1  We also assume that this population has an approximately constant Fisher information: N  If (s) =  h′2 (s − n) h(s − n) n=1 N  φ(s − n) ≈ Iconv . [sent-69, score-0.397]
</p><p>34 =  (5)  n=1  That is, we assume that the Fisher information curves for the individual neurons, φ(s − n), also tile the stimulus space. [sent-70, score-0.438]
</p><p>35 The value of the constant, Iconv , is dependent on the details of the tuning curve shape, h(s), which we leave unspeciﬁed. [sent-71, score-0.408]
</p><p>36 1(a-b) shows that the Fisher information for a convolutional population of Gaussian tuning curves, with appropriate width, is approximately constant. [sent-73, score-0.836]
</p><p>37 Now we introduce two scalar values, a gain (g), and a density (d), that affect the convolutional population as follows: n (6) hn (s) = g h d(s − ) . [sent-74, score-0.808]
</p><p>38 Here, we use it to bound the squared discriminability of the estimator, as expressed in the stimulus space, which is independent of bias [19]. [sent-76, score-0.43]
</p><p>39 (a) Homogeneous population with Gaussian tuning curves on the unit lattice. [sent-80, score-0.858]
</p><p>40 55 is chosen so that the curves approximately tile the stimulus space. [sent-82, score-0.471]
</p><p>41 (b) The Fisher information of the convolutional population (green) is approximately constant. [sent-83, score-0.487]
</p><p>42 The cumulative integral of this density, D(s), alters the positions and widths of the tuning curves in the convolutional population. [sent-85, score-0.658]
</p><p>43 (d) The warped population, with tuning curve peaks (aligned with tick marks, at locations sn = D−1 (n)), is scaled by the gain function, g(s) (blue). [sent-86, score-0.641]
</p><p>44 A single tuning curve is highlighted (red) to illustrate the effect of the warping and scaling operations. [sent-87, score-0.408]
</p><p>45 (e) The Fisher information of the inhomogeneous population is approximately proportional to d2 (s)g(s). [sent-88, score-0.397]
</p><p>46 The density controls both the spacing and width of the tuning curves: as the density increases, the tuning curves become narrower, and are spaced closer together so as to maintain their tiling of stimulus space. [sent-90, score-1.49]
</p><p>47 (5), that the Fisher information of the convolutional population is approximately constant with respect to s. [sent-93, score-0.487]
</p><p>48 If the original (unit-spacing) convolutional population is supported on the interval (0, Q) of the stimulus space, then the number of neurons in the modulated population must be N (d) = Qd to cover the same interval. [sent-95, score-1.146]
</p><p>49 Under the assumption that the tuning curves tile the stimulus space, Eq. [sent-96, score-0.787]
</p><p>50 2  Density and gain for a heterogeneous population  Intuitively, if p(s) is non-uniform, the optimal Fisher information should also be non-uniform. [sent-99, score-0.557]
</p><p>51 This can be achieved through inhomogeneities in either the tuning curve density or gain. [sent-100, score-0.531]
</p><p>52 We thus generalize density and gain to be continuous functions of the stimulus, d(s) and g(s), that warp and scale the convolutional population: hn (s) = g(sn ) h(D(s) − n). [sent-101, score-0.444]
</p><p>53 Optimal heterogeneous population properties, for objective functions speciﬁed by Eq. [sent-103, score-0.475]
</p><p>54 s  Here, D(s) = −∞ d(t)dt, the cumulative integral of d(s), warps the shape of the prototype tuning curve. [sent-105, score-0.349]
</p><p>55 The value sn = D−1 (n) represents the preferred stimulus value of the (warped) nth tuning curve (Fig. [sent-106, score-0.696]
</p><p>56 Note that the warped population retains the tiling properties of the original convolutional population. [sent-108, score-0.534]
</p><p>57 As in the uniform case, the density controls both the spacing and width of the tuning curves. [sent-109, score-0.635]
</p><p>58 We can now write the Fisher information of the heterogeneous population of neurons in Eq. [sent-113, score-0.543]
</p><p>59 As earlier, the constant Iconv is determined by the precise shape of the tuning curves. [sent-118, score-0.349]
</p><p>60 To attain the proper rate, we use the fact that the warped tuning curves sum to unity (before multiplication by the gain function) and use Eq. [sent-121, score-0.648]
</p><p>61 3  Objective function and solution for a heterogeneous population  Approximating Fisher information as proportional to squared density and gain allows us to re-write the objective function and resource constraints of Eq. [sent-124, score-0.826]
</p><p>62 In all cases, the solution speciﬁes a power-law relationship between the prior, and the density and gain of the tuning curves. [sent-130, score-0.575]
</p><p>63 In general, all solutions allocate more neurons, with correspondingly narrower tuning curves, to higher-probability stimuli. [sent-131, score-0.426]
</p><p>64 The shape of the optimal gain function depends on the objective function: for α < 0, neurons with lower ﬁring rates are used to represent stimuli with higher probabilities, and for α > 0, neurons with higher ﬁring rates are used for stimuli with higher probabilities. [sent-133, score-0.506]
</p><p>65 (c) Orientation discrimination thresholds averaged across four human subjects [24]. [sent-138, score-0.369]
</p><p>66 (d & e) Infomax and discrimax predictions of orientation distribution. [sent-139, score-0.424]
</p><p>67 In addition to power-law relationships between tuning properties and sensory priors, our formulation offers a direct relationship between the sensory prior and perceptual discriminability. [sent-144, score-0.948]
</p><p>68 5  Experimental evidence  Our framework predicts a quantitative link between the sensory prior, physiological parameters (the density, tuning widths, and gain of cells), and psychophysically measured discrimination thresholds. [sent-148, score-0.905]
</p><p>69 We obtained subsets of these quantities for two visual stimulus variables, orientation and spatial frequency, both of believed to be encoded by cells in primary visual cortex (area V1). [sent-149, score-0.502]
</p><p>70 For each variable, we use the infomax and discrimax solutions to convert the physiological and perceptual measurements, using the appropriate exponents from Table 1, into predictions of the stimulus prior p(s). [sent-150, score-1.055]
</p><p>71 1  Orientation  We estimated the prior distribution of orientations in the environment by averaging orientation statistics across three natural image databases. [sent-153, score-0.316]
</p><p>72 The average distribution of orientations exhibits higher probability at the cardinal orientations (vertical and horizontal) than at the oblique orientations (Fig. [sent-156, score-0.33]
</p><p>73 Measurements of cell density for a population of 79 orientation-tuned V1 cells in Macaque [23] show more cells tuned to the cardinal orientations than the oblique orientations (Fig. [sent-158, score-1.075]
</p><p>74 Finally, perceptual discrimination thresholds, averaged across four human subjects [24] show a similar bias (Fig. [sent-160, score-0.426]
</p><p>75 If a neural population is designed to maximize information, then the cell density and inverse discrimination thresholds should match the stimulus prior, as expressed in infomax column of Table 1. [sent-163, score-1.275]
</p><p>76 (b) Cell density as a function of preferred spatial frequency for a population of 317 V1 cells [25, 28] Dark blue: average number of cells tuned to each spatial frequency. [sent-167, score-0.953]
</p><p>77 (d & e) Infomax and discrimax predictions of spatial frequency distribution. [sent-172, score-0.464]
</p><p>78 We see that the predictions arising from cell density and discrimination thresholds are consistent with one another, and both are consistent with the stimulus prior. [sent-177, score-0.771]
</p><p>79 For the discrimax objective function, the exponents in the power-law relationships (expressed in Table 1) are too small, resulting in poor qualitative agreement between the stimulus prior and predictions from the physiology and perception (Fig. [sent-179, score-0.81]
</p><p>80 For example, predicting the prior from perceptual data, under the discrimax objective function, requires exponentiating discrimination thresholds to the fourth power, resulting in an over exaggeration of the cardinal bias. [sent-181, score-0.803]
</p><p>81 2  Spatial frequency  We obtained a prior distribution over spatial frequencies averaged across two natural image databases [20, 21]. [sent-183, score-0.344]
</p><p>82 We also obtained spatial frequency tuning properties for a population of 317 V1 cells [25]. [sent-188, score-0.958]
</p><p>83 On average, we see there are more cells, with correspondingly narrower tuning widths, tuned to low spatial frequencies (Fig. [sent-189, score-0.548]
</p><p>84 These data support the model assumption that tuning width is inversely proportional to cell density. [sent-191, score-0.562]
</p><p>85 We also obtained average discrimination thresholds for sinusoidal gratings of different spatial frequencies from two studies (Fig. [sent-192, score-0.401]
</p><p>86 We again test the infomax and discrimax solutions by comparing predicted distributions obtained from the physiological and perceptual data, to the measured prior. [sent-196, score-0.701]
</p><p>87 The infomax case shows striking agreement between the measured stimulus prior, and predictions based on the physiological and perceptual measurements (Fig. [sent-198, score-0.727]
</p><p>88 However, as in the orientation case, discrimax predictions are poor (Fig. [sent-200, score-0.424]
</p><p>89 3(e)), suggesting that information maximization provides a better optimality principle for explaining the neural and perceptual encoding of spatial frequency than discrimination maximization. [sent-201, score-0.536]
</p><p>90 7  6  Discussion  We have examined the inﬂuence sensory priors on the optimal allocation of neural resources, as well as the inﬂuence of these optimized resources on subsequent perception. [sent-202, score-0.343]
</p><p>91 Fisher information is known to provide a poor bound on mutual information when there are a small number of neurons, a short decoding time, or non-smooth tuning curves [16, 29]. [sent-207, score-0.577]
</p><p>92 These assumptions allow us to approximate Fisher information in terms of cell density and gain (Fig. [sent-211, score-0.332]
</p><p>93 Our framework offers an important generalization of the population coding literature, allowing for non-uniformity of sensory priors, and corresponding heterogeneity in tuning and gain properties. [sent-213, score-1.043]
</p><p>94 Second, tuning curve encoding models only specify neural responses to single stimulus values. [sent-216, score-0.701]
</p><p>95 Previous studies assume that prior probabilities are either uniform [6], represented in the spiking activity of a separate population of neurons [5], or represented (in sample form) in the spontaneous activity [35]. [sent-224, score-0.543]
</p><p>96 Our encoding formulation provides a mechanism whereby the prior is implicitly encoded in the density and gains of tuning curves, which presumably arise from the strength of synaptic connections. [sent-225, score-0.581]
</p><p>97 Narrow versus wide tuning curves: What’s best for a population code? [sent-258, score-0.713]
</p><p>98 Optimal tuning widths in population coding of periodic variables. [sent-264, score-0.84]
</p><p>99 Maximally informative stimuli and tuning curves for sigmoidal ratecoding neurons and populations. [sent-280, score-0.655]
</p><p>100 Optimal neural population coding of an auditory spatial cue. [sent-286, score-0.54]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('population', 0.364), ('tuning', 0.349), ('fisher', 0.258), ('discrimax', 0.234), ('stimulus', 0.209), ('discriminability', 0.189), ('infomax', 0.189), ('sensory', 0.174), ('perceptual', 0.153), ('discrimination', 0.153), ('curves', 0.145), ('comput', 0.128), ('hn', 0.128), ('density', 0.123), ('neurons', 0.119), ('ds', 0.107), ('width', 0.107), ('cell', 0.106), ('orientation', 0.106), ('gain', 0.103), ('cells', 0.099), ('iconv', 0.098), ('thresholds', 0.096), ('resource', 0.093), ('convolutional', 0.09), ('spatial', 0.088), ('predictions', 0.084), ('tile', 0.084), ('orientations', 0.08), ('sn', 0.079), ('widths', 0.074), ('resources', 0.066), ('cpd', 0.063), ('prior', 0.06), ('heterogeneous', 0.06), ('curve', 0.059), ('frequency', 0.058), ('perception', 0.056), ('spacing', 0.056), ('cardinal', 0.056), ('subjects', 0.054), ('mutual', 0.054), ('coding', 0.053), ('dec', 0.051), ('nov', 0.051), ('warped', 0.051), ('objective', 0.051), ('physiological', 0.051), ('encoding', 0.049), ('neuron', 0.048), ('spikes', 0.048), ('macaque', 0.048), ('aug', 0.047), ('ring', 0.046), ('jul', 0.044), ('ja', 0.044), ('narrower', 0.044), ('stimuli', 0.042), ('neuronal', 0.042), ('exponents', 0.042), ('environment', 0.042), ('measured', 0.041), ('databases', 0.039), ('bair', 0.039), ('cavanaugh', 0.039), ('surround', 0.039), ('oct', 0.038), ('averaged', 0.038), ('allocation', 0.038), ('relationships', 0.038), ('homogeneous', 0.036), ('law', 0.036), ('poisson', 0.036), ('physiology', 0.036), ('neurosci', 0.036), ('neural', 0.035), ('psychophysically', 0.034), ('oblique', 0.034), ('testable', 0.034), ('power', 0.034), ('tuned', 0.034), ('solutions', 0.033), ('frequencies', 0.033), ('approximately', 0.033), ('evenly', 0.033), ('squared', 0.032), ('tj', 0.032), ('exponent', 0.032), ('gratings', 0.031), ('jr', 0.031), ('jp', 0.031), ('implications', 0.031), ('optimal', 0.03), ('response', 0.03), ('feb', 0.029), ('mar', 0.029), ('tiling', 0.029), ('decoding', 0.029), ('across', 0.028), ('ep', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="119-tfidf-1" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>2 0.32152444 <a title="119-tfidf-2" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>3 0.27011311 <a title="119-tfidf-3" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>4 0.22286481 <a title="119-tfidf-4" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>5 0.21901914 <a title="119-tfidf-5" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>6 0.14555848 <a title="119-tfidf-6" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>7 0.11893485 <a title="119-tfidf-7" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>8 0.10718781 <a title="119-tfidf-8" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>9 0.1026597 <a title="119-tfidf-9" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>10 0.088675387 <a title="119-tfidf-10" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>11 0.085691139 <a title="119-tfidf-11" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>12 0.077668034 <a title="119-tfidf-12" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>13 0.074470542 <a title="119-tfidf-13" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>14 0.070412725 <a title="119-tfidf-14" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>15 0.070328057 <a title="119-tfidf-15" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>16 0.069272757 <a title="119-tfidf-16" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>17 0.068883516 <a title="119-tfidf-17" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>18 0.066898763 <a title="119-tfidf-18" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>19 0.066094577 <a title="119-tfidf-19" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>20 0.063078143 <a title="119-tfidf-20" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, 0.051), (2, -0.276), (3, 0.217), (4, 0.095), (5, 0.159), (6, -0.049), (7, 0.022), (8, 0.012), (9, -0.025), (10, 0.074), (11, -0.042), (12, -0.038), (13, -0.053), (14, 0.003), (15, -0.042), (16, -0.045), (17, -0.035), (18, -0.02), (19, 0.241), (20, 0.066), (21, -0.236), (22, 0.212), (23, 0.018), (24, 0.002), (25, 0.027), (26, 0.046), (27, 0.137), (28, -0.072), (29, 0.103), (30, -0.053), (31, -0.124), (32, 0.018), (33, -0.001), (34, -0.043), (35, -0.022), (36, 0.069), (37, -0.028), (38, 0.058), (39, 0.005), (40, 0.059), (41, -0.053), (42, -0.106), (43, -0.028), (44, -0.043), (45, 0.083), (46, -0.07), (47, -0.096), (48, -0.038), (49, -0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98444879 <a title="119-lsi-1" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>2 0.90451616 <a title="119-lsi-2" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>3 0.8439008 <a title="119-lsi-3" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>4 0.74311841 <a title="119-lsi-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.68180758 <a title="119-lsi-5" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>6 0.62636817 <a title="119-lsi-6" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>7 0.528763 <a title="119-lsi-7" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>8 0.49805784 <a title="119-lsi-8" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>9 0.48006254 <a title="119-lsi-9" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>10 0.45839351 <a title="119-lsi-10" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>11 0.42893928 <a title="119-lsi-11" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>12 0.40298176 <a title="119-lsi-12" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>13 0.3995207 <a title="119-lsi-13" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>14 0.39870507 <a title="119-lsi-14" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>15 0.39737576 <a title="119-lsi-15" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>16 0.39176133 <a title="119-lsi-16" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>17 0.36858398 <a title="119-lsi-17" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>18 0.32309851 <a title="119-lsi-18" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>19 0.30332962 <a title="119-lsi-19" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>20 0.29500145 <a title="119-lsi-20" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.019), (17, 0.016), (27, 0.601), (30, 0.037), (35, 0.01), (45, 0.109), (50, 0.02), (52, 0.029), (60, 0.016), (77, 0.04), (90, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95471531 <a title="119-lda-1" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>2 0.92717946 <a title="119-lda-2" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>3 0.90552294 <a title="119-lda-3" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>4 0.87523639 <a title="119-lda-4" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>5 0.840002 <a title="119-lda-5" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>6 0.81636739 <a title="119-lda-6" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>7 0.74552161 <a title="119-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.70714164 <a title="119-lda-8" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>9 0.68337446 <a title="119-lda-9" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>10 0.67964637 <a title="119-lda-10" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>11 0.64794916 <a title="119-lda-11" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>12 0.64085478 <a title="119-lda-12" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>13 0.6159054 <a title="119-lda-13" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>14 0.60185415 <a title="119-lda-14" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>15 0.60027206 <a title="119-lda-15" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>16 0.595061 <a title="119-lda-16" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>17 0.59005803 <a title="119-lda-17" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>18 0.58883065 <a title="119-lda-18" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>19 0.58101171 <a title="119-lda-19" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>20 0.5756861 <a title="119-lda-20" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
