<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-186" href="#">nips2010-186</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</h1>
<br/><p>Source: <a title="nips-2010-186-pdf" href="http://papers.nips.cc/paper/4008-object-bank-a-high-level-image-representation-for-scene-classification-semantic-feature-sparsification.pdf">pdf</a></p><p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>Reference: <a title="nips-2010-186-reference" href="../nips2010_reference/nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. [sent-4, score-0.699]
</p><p>2 Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns. [sent-6, score-0.521]
</p><p>3 1 illustrates the gradient-based GIST features [25] and texture-based Spatial Pyramid representation [19] of two different scenes (foresty mountain vs. [sent-13, score-0.28]
</p><p>4 ) Comparison of object bank (OB) representation with two low-level feature representations, GIST and SIFT-SPM of two types of images, mountain vs. [sent-18, score-0.768]
</p><p>5 From left to right, for each input image, we show the selected ﬁlter responses in the GIST representation [25], a histogram of the SPM representation of SIFT patches [19], and a selected number of OB responses. [sent-20, score-0.266]
</p><p>6 1  While more sophisticated low-level feature engineering and recognition model design remain important sources of future developments, we argue that the use of semantically more meaningful feature space, such as one that is directly based on the content (e. [sent-22, score-0.269]
</p><p>7 In this paper, we propose “Object Bank” (OB), a new representation of natural images based on objects, or more rigorously, a collection of object sensing ﬁlters built on a generic collection of labeled objects. [sent-25, score-0.617]
</p><p>8 We show that an image representation based on objects can be very useful in high-level visual recognition tasks for scenes cluttered with objects. [sent-27, score-0.633]
</p><p>9 1, these two different scenes show very different image responses to objects such as tree, street, water, sky, etc. [sent-30, score-0.375]
</p><p>10 Given the availability of large-scale image datasets such as LabelMe [30] and ImageNet [5], it is no longer inconceivable to obtain trained object detectors for a large number of visual concepts. [sent-31, score-0.801]
</p><p>11 In fact we envision the usage of thousands if not millions of these available object detectors as the building block of such image representation in the future. [sent-32, score-0.839]
</p><p>12 , number of objects) of the object bank and the dimensionality of the response vector for each object. [sent-35, score-0.557]
</p><p>13 Typically, for a modest sized picture, even hundreds of object detectors would result into a representation of tens of thousands of dimensions. [sent-36, score-0.698]
</p><p>14 In this paper, we propose a regularized logistic regression method, akin to the group lasso approach for structured sparsity, to explore both feature sparsity and object sparsity in the Object Bank representation for learning and classifying complex scenes. [sent-38, score-0.814]
</p><p>15 2 Related Work A plethora of image descriptors have been developed for object recognition and image classiﬁcation [25, 1, 23]. [sent-40, score-0.643]
</p><p>16 We particularly draw the analogy between our object bank and the texture ﬁlter banks [26, 10]. [sent-41, score-0.531]
</p><p>17 In this work, we mainly use the current state-of-the-art object detectors of Felzenszwalb et. [sent-43, score-0.553]
</p><p>18 The idea of using object detectors as the basic representation of images is analogous [12, 33, 35]. [sent-48, score-0.777]
</p><p>19 In contrast to our work, in [12] and [33] each semantic concept is trained by using the entire images or frames of video. [sent-49, score-0.249]
</p><p>20 As there is no localization of object concepts in scenes, understanding cluttered images composed of many objects will be challenging. [sent-50, score-0.732]
</p><p>21 In [35], a small number of concepts are trained and only the most probable concept is used to form the representation for each region, whereas in our approach all the detector responses are used to encode richer semantic information. [sent-51, score-0.373]
</p><p>22 The idea of using many object detectors as the basic representation of images is analogous to approaches applying a large number of “semantic concepts” to video and image annotation and retrieval [12, 33, 35]. [sent-52, score-0.892]
</p><p>23 In contrast to our work, in [12, 33, 35] each semantic concept is trained by using entire images or frames of videos. [sent-53, score-0.249]
</p><p>24 There is no sense of localized representation of meaningful object concepts in scenes. [sent-54, score-0.565]
</p><p>25 Combinations of small set of (∼ a dozen of) off-the-shelf object detectors with global scene context have been used to improve object detection [14, 28, 29]. [sent-56, score-1.171]
</p><p>26 A large number of object detectors are ﬁrst applied to an input image at multiple scales. [sent-60, score-0.668]
</p><p>27 For each object at each scale, a three-level spatial pyramid representation of the resulting object ﬁlter map is used, resulting in No. [sent-61, score-0.96]
</p><p>28 Scales × (12 + 22 + 42 ) grids; the maximum response for each object in each grid is then computed, resulting in a No. [sent-63, score-0.396]
</p><p>29 To our knowledge, this is the ﬁrst work that use such high-level image features at different image location and scale. [sent-67, score-0.268]
</p><p>30 3  The Object Bank Representation of Images  Object Bank (OB) is an image representation constructed from the responses of many object detectors, which can be viewed as the response of a “generalized object convolution. [sent-68, score-1.027]
</p><p>31 ” We use two state-of-the-art detectors for this operation: the latent SVM object detectors [9] for most of the blobby objects such as tables, cars, humans, etc, and a texture classiﬁer by Hoiem [13] for more texture- and material-based objects such as sky, road, sand, etc. [sent-69, score-1.094]
</p><p>32 Our image representation is agnostic to any speciﬁc type of object detector; we take the “outsourcing” approach and assume the availability of these pre-trained detectors. [sent-71, score-0.627]
</p><p>33 A large number of object detectors are run across an image at different scales. [sent-74, score-0.668]
</p><p>34 For each scale and each detector, we obtain an initial response map of the image (see Appendix for more details of using the object detectors [9, 13]). [sent-75, score-0.721]
</p><p>35 In this paper, we use 200 object detectors at 12 detection scales and 3 spatial pyramid levels (L=0,1,2) [19]. [sent-76, score-0.699]
</p><p>36 We use the same set of object detectors regardless of the scenes or the testing dataset. [sent-78, score-0.608]
</p><p>37 1  Implementation Details of Object Bank  So what are the “objects” to use in the object bank? [sent-80, score-0.37]
</p><p>38 Not enough research has yet gone into building robust object detector for tens of thousands of generic objects. [sent-85, score-0.448]
</p><p>39 1 in Appendix shows, the distribution of objects follows Zipf’s Law, which implies that a small proportion of object classes account for the majority of object instances. [sent-88, score-0.919]
</p><p>40 An important practical consideration for our study is to ensure the availability of enough training images for each object detectors. [sent-90, score-0.496]
</p><p>41 We therefore focus our attention on obtaining the objects from popular image datasets such as ESP [31], LabelMe [30], ImageNet [5] and the Flickr online photo sharing community. [sent-91, score-0.356]
</p><p>42 After ranking the objects according to their frequencies in each of these datasets, we take the intersection set of the most frequent 1000 objects, resulting in 200 objects, where the identities and semantic relations of some of them are illustrated in Fig. [sent-92, score-0.296]
</p><p>43 To train each of the 200 object detectors, we use 100∼200 images and their object bounding box information from the LabelMe [30] (86 objects) and ImageNet [5] datasets (177 objects). [sent-94, score-0.882]
</p><p>44 We use a subset of LabelMe scene dataset to evaluate the object detector performance. [sent-95, score-0.676]
</p><p>45 Final object detectors are selected based on their performance on the validation set from LabelMe (see Appendix for more details). [sent-96, score-0.553]
</p><p>46 1 This criterion prevents us from using the Caltech101/256 datasets to train our object detectors [6, 11] where the objects are chosen without any particular considerations of their relevance to daily life pictures. [sent-97, score-0.77]
</p><p>47 We refer this semantic-preserving compression as content-based compression to contrast the conventional information-theoretic compression that aims at lossless reconstruction of the data. [sent-101, score-0.446]
</p><p>48 ; xT ] ∈ RN ×J , an N × J 1 2 N matrix, represent the design built on the J-dimensional object bank representation of N images; and let Y = (y1 , . [sent-108, score-0.674]
</p><p>49 We investigate content-based compression of the high-dimensional OB representation that exploits raw feature-, object-, and (feature+object)sparsity, respectively, using LR with appropriate regularization. [sent-122, score-0.271]
</p><p>50 , features grouped by an object j), and · 2 is the vector 2 -norm, we set the feature group to be corresponding to that of all features induced by the same object in the OB. [sent-133, score-0.903]
</p><p>51 Therefore, the sparsity is now imposed on object level, rather than merely on raw feature level. [sent-135, score-0.501]
</p><p>52 Such structured sparsity is often desired because it is expected to generate semantically more meaningful lossless compression, that is, out of all the objects in the OB, only a few are needed to represent any given natural image. [sent-136, score-0.457]
</p><p>53 We call such a sparsity pattern object sparsity, and denote the resultant coefﬁcient estimator by β O . [sent-137, score-0.508]
</p><p>54 In the LabelMe dataset, the “ideal” classiﬁcation accuracy is 90%, where we use the human ground-truth object identities to predict the labels of the scene classes. [sent-165, score-0.595]
</p><p>55 The blue bar in the last panel is the performance of “pseudo” object bank representation extracted from the same number of “pseudo” object detectors. [sent-166, score-1.021]
</p><p>56 Joint object/feature sparsity via 1 / 2 + 1 (sparse group) regularized LR (LRG1) The groupregularized LR does not, however, yield sparsity within a group (object) for those groups with nonzero total weights. [sent-170, score-0.246]
</p><p>57 5  Experiments and Results  Dataset We evaluate the OB representation on 4 scene datasets, ranging from generic natural scene images (15-Scene, LabelMe 9-class scene dataset3 ), to cluttered indoor images (MIT Indoor Scene), and to complex event and activity images (UIUC-Sports). [sent-178, score-1.227]
</p><p>58 We list below the experiment setting for each dataset: • 15-Scene: This is a dataset of 15 natural scene classes. [sent-180, score-0.253]
</p><p>59 50 images randomly drawn images from each scene classes are used for training and 50 for testing. [sent-183, score-0.433]
</p><p>60 • MIT Indoor: This is a dataset of 15620 images over 67 indoor scenes assembled by [27]. [sent-184, score-0.257]
</p><p>61 Experiment Setup We compare OB in scene classiﬁcation tasks with different types of conventional image features, such as SIFT-BoW [23, 3], GIST [25] and SPM [19]. [sent-188, score-0.366]
</p><p>62 The substantial performance gain on the UIUC-Sports and the MIT-Indoor scene datasets illustrates the importance of using a semantically meaningful representation for complex scenes cluttered with objects. [sent-198, score-0.614]
</p><p>63 This result underscores the effectiveness of OB, highlighting the fact that in high-level visual tasks such as complex scene recognition, a higher level image representation can be very useful. [sent-200, score-0.531]
</p><p>64 We further decompose the spatial structure and semantic meaning encoded in OB by using a “pseudo” OB without semantic meaning. [sent-201, score-0.279]
</p><p>65 The signiﬁcant improvement of OB in classiﬁcation performance over the “pseudo object bank” is largely attributed to the effectiveness of using object detectors trained from image. [sent-202, score-0.951]
</p><p>66 For each of the existing scene datasets (UIUC-Sports, 15-Scene and MIT-Indoor), we also compare the reported state of the arts performances to our OB algorithm (using a standard LR classiﬁer). [sent-203, score-0.292]
</p><p>67 We compare the object recognition performance on the Caltech 256 dataset to [33], a high Table 1: Comparison of classiﬁcation relevel image representation obtained as the output of a sults using OB with reported state-of-thelarge number of weakly trained object classiﬁers on the art algorithms. [sent-216, score-1.074]
</p><p>68 By encoding the spatial locations of the objects more complex model and supervised information, whereas our results are obtained by within an image, OB (39%) signiﬁcantly outperforms applying simple logistic regression. [sent-218, score-0.252]
</p><p>69 We focus on the practical issues directly relevant to the effectiveness of OB representation and quality of feature sparsiﬁcation, and study the following three aspects of the scene classiﬁer: 1) robustness, 2) feasibility of lossless content-based compression, 3) proﬁtability over growing OB. [sent-222, score-0.466]
</p><p>70 X-axis is the size of compressed feature dimension, represented as the ratio of the compressed feature dimension over the full OB representation dimension (44604). [sent-241, score-0.266]
</p><p>71 3 rounds of randomized sampling is performed to choose the object ﬁlters from all the object detectors. [sent-247, score-0.74]
</p><p>72 We investigate the robustness of the logistic regression classiﬁer built on 4 We also evaluate the classiﬁcation performance of using the detected object location and its detection score of each object detector as the image representation. [sent-249, score-1.008]
</p><p>73 Such compression can be attractive in reducing representation cost of image query, and improving the speed of query inference. [sent-264, score-0.36]
</p><p>74 To study the extend of information loss as a function of different number of features being retained in the classiﬁer, we re-train an LR classiﬁer using features from the top x% percentile of the rank list, where x is a compression scale ranging from 0. [sent-267, score-0.255]
</p><p>75 We observe that algorithms imposing sparsity in features (LR1, LRG, and LRG1) outperform unregularized algorithm (LR) with a larger margin when the compression ratio becomes higher. [sent-279, score-0.244]
</p><p>76 We ask, are image inference tasks such as scene classiﬁcation going to beneﬁt from this trend? [sent-285, score-0.366]
</p><p>77 As group regularized LR imposes sparsity on object level, we choose to use it to investigate how the number of objects will affect the discriminative power of OB representation. [sent-286, score-0.778]
</p><p>78 To simulate what happens when the size of OB grows, we randomly sample subsets of object detectors at 1%, 5%, 10%, 25%, 50% and 75% of total number of objects for multiple rounds. [sent-287, score-0.732]
</p><p>79 We conjecture that this is due to the accumulation of discriminative object features, and we believe that future growth of OB will lead to stronger representation power and discriminability of images models build on OB. [sent-290, score-0.632]
</p><p>80 4  Interpretability of the Compressed Representation  Intuitively, a few key objects can discriminate a scene class from another. [sent-292, score-0.404]
</p><p>81 In this experiment, we aim to discover the object sparsity and investigate its interpretability. [sent-293, score-0.477]
</p><p>82 Again, we use group regularized LR (LRG) since the sparsity is imposed on object level and hence generates a more semantically meaningful compression. [sent-294, score-0.661]
</p><p>83 The purple bounding box shows the size of the object ﬁlter at this scale, centered at the peak of the heat map. [sent-301, score-0.394]
</p><p>84 Bottom: example scene images masked by the feature weights in image space (at the highest weighted scale), highlighting the most relevant object dimension. [sent-302, score-0.864]
</p><p>85 5 the object-wise coefﬁcients of the comsailing beach pression results for 4 sample scene classes. [sent-304, score-0.269]
</p><p>86 The object weight is obtained by accumulating the coefﬁcient of β O from the feature dimensions of each object (at different scales and spatial locations) learned by LRG. [sent-305, score-0.885]
</p><p>87 5 shows that objects that are church mountain “representative” for each scene are retained by LRG. [sent-308, score-0.533]
</p><p>88 For example, “sailboat”, “boat”, and “sky” are objects with very high weight in the “sailing” scene class. [sent-309, score-0.404]
</p><p>89 This suggests that the representation compression via LRG is virtually based upon the image content and is semantically meaningful; therefore, it is nearly “semantically lossless”. [sent-310, score-0.44]
</p><p>90 8  r  he  s as gr  r ca  r ca  e  tre  y sk  g  ap cr  in ild  bu  ys sk  le  r he ot ng i ild bu  ud  n  ai  nt  op  r ca  clo  pe  k  e  c ro  ou m  tre  y sk  scene class. [sent-326, score-0.793]
</p><p>91 Selected objects correspond to  Knowing the important objects learned by the compres- non-zero β values learned by LRG. [sent-327, score-0.358]
</p><p>92 sion algorithm, we further investigate the discriminative dimensions within the object level. [sent-328, score-0.461]
</p><p>93 3, we introduce that each feature dimension in the OB representation is directly related to a speciﬁc scale, geometric location and object identity. [sent-331, score-0.563]
</p><p>94 Hence, the weights in β OF reﬂects the importance of an object at a certain scale and location. [sent-332, score-0.397]
</p><p>95 To verify the hypothesis, we examine the importance of objects across scales by summing up the weights of related spatial locations and pyramid resolutions. [sent-333, score-0.302]
</p><p>96 We show one representative object in a scene and visualize the feature patterns within the object group. [sent-334, score-1.015]
</p><p>97 6(Middle), we observe that the regions with high intensities are also the locations where the object frequently appears. [sent-339, score-0.37]
</p><p>98 For example, cloud usually appears in the upper half of a scene in the beach class. [sent-340, score-0.298]
</p><p>99 Learning to detect unseen object classes by between-class attribute transfer. [sent-449, score-0.37]
</p><p>100 Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. [sent-464, score-0.28]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ob', 0.542), ('object', 0.37), ('scene', 0.225), ('lrg', 0.22), ('lr', 0.214), ('detectors', 0.183), ('objects', 0.179), ('labelme', 0.177), ('bank', 0.161), ('compression', 0.125), ('representation', 0.12), ('semantic', 0.117), ('gist', 0.115), ('image', 0.115), ('images', 0.104), ('spm', 0.089), ('classi', 0.083), ('bow', 0.083), ('sparsity', 0.081), ('semantically', 0.08), ('lossless', 0.071), ('sparsi', 0.07), ('indoor', 0.07), ('mountain', 0.067), ('sky', 0.065), ('pseudo', 0.063), ('ild', 0.059), ('tre', 0.059), ('scenes', 0.055), ('er', 0.055), ('pyramid', 0.055), ('detector', 0.053), ('cluttered', 0.05), ('imagenet', 0.05), ('feature', 0.05), ('bu', 0.047), ('regularized', 0.047), ('cvpr', 0.047), ('meaningful', 0.046), ('spatial', 0.045), ('visual', 0.045), ('beach', 0.044), ('recognition', 0.043), ('sk', 0.042), ('hoiem', 0.042), ('sailboat', 0.039), ('classification', 0.038), ('discriminative', 0.038), ('datasets', 0.038), ('features', 0.038), ('ot', 0.037), ('group', 0.037), ('coef', 0.037), ('iccv', 0.036), ('church', 0.035), ('resultant', 0.035), ('boat', 0.033), ('magni', 0.033), ('cation', 0.033), ('percent', 0.031), ('concepts', 0.029), ('clo', 0.029), ('tability', 0.029), ('tower', 0.029), ('performances', 0.029), ('cloud', 0.029), ('logistic', 0.028), ('trained', 0.028), ('correctness', 0.028), ('dataset', 0.028), ('dimensions', 0.027), ('scale', 0.027), ('water', 0.027), ('retained', 0.027), ('regularization', 0.027), ('schemes', 0.027), ('investigate', 0.026), ('millions', 0.026), ('response', 0.026), ('responses', 0.026), ('tasks', 0.026), ('appendix', 0.025), ('thousands', 0.025), ('shrinkage', 0.024), ('purple', 0.024), ('photo', 0.024), ('cr', 0.024), ('envisage', 0.024), ('dimension', 0.023), ('detection', 0.023), ('pami', 0.023), ('built', 0.023), ('scales', 0.023), ('street', 0.022), ('availability', 0.022), ('representations', 0.022), ('svm', 0.022), ('estimator', 0.022), ('sailing', 0.022), ('ud', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="186-tfidf-1" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>2 0.27681178 <a title="186-tfidf-2" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>3 0.22373798 <a title="186-tfidf-3" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>4 0.21126458 <a title="186-tfidf-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>5 0.20659371 <a title="186-tfidf-5" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>6 0.19663212 <a title="186-tfidf-6" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>7 0.16558853 <a title="186-tfidf-7" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>8 0.15141192 <a title="186-tfidf-8" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>9 0.14620455 <a title="186-tfidf-9" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>10 0.14431195 <a title="186-tfidf-10" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>11 0.13769794 <a title="186-tfidf-11" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>12 0.12208635 <a title="186-tfidf-12" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>13 0.1079475 <a title="186-tfidf-13" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>14 0.10533413 <a title="186-tfidf-14" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>15 0.092901982 <a title="186-tfidf-15" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>16 0.09103664 <a title="186-tfidf-16" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>17 0.089849524 <a title="186-tfidf-17" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>18 0.088778101 <a title="186-tfidf-18" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>19 0.083090909 <a title="186-tfidf-19" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>20 0.079685077 <a title="186-tfidf-20" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, 0.126), (2, -0.185), (3, -0.326), (4, 0.027), (5, -0.049), (6, -0.084), (7, 0.045), (8, -0.0), (9, 0.033), (10, 0.064), (11, 0.131), (12, -0.152), (13, 0.027), (14, 0.045), (15, 0.005), (16, 0.133), (17, -0.068), (18, 0.137), (19, 0.087), (20, 0.03), (21, 0.035), (22, -0.033), (23, -0.006), (24, -0.031), (25, -0.078), (26, -0.0), (27, 0.045), (28, 0.055), (29, -0.031), (30, -0.04), (31, -0.014), (32, -0.017), (33, -0.007), (34, -0.03), (35, 0.036), (36, 0.0), (37, 0.067), (38, -0.063), (39, -0.082), (40, -0.009), (41, 0.006), (42, 0.11), (43, -0.06), (44, 0.076), (45, -0.038), (46, -0.003), (47, -0.03), (48, -0.054), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97412461 <a title="186-lsi-1" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>2 0.92634416 <a title="186-lsi-2" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>3 0.88653451 <a title="186-lsi-3" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>4 0.85393071 <a title="186-lsi-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>5 0.74771875 <a title="186-lsi-5" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>6 0.70227212 <a title="186-lsi-6" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>7 0.70084739 <a title="186-lsi-7" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>8 0.69546688 <a title="186-lsi-8" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>9 0.69217902 <a title="186-lsi-9" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>10 0.637766 <a title="186-lsi-10" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>11 0.61964953 <a title="186-lsi-11" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>12 0.55052471 <a title="186-lsi-12" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>13 0.53852922 <a title="186-lsi-13" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>14 0.53596181 <a title="186-lsi-14" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>15 0.49491057 <a title="186-lsi-15" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>16 0.47194326 <a title="186-lsi-16" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>17 0.47142065 <a title="186-lsi-17" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>18 0.43656304 <a title="186-lsi-18" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>19 0.42953855 <a title="186-lsi-19" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>20 0.41597375 <a title="186-lsi-20" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.035), (17, 0.026), (27, 0.085), (30, 0.06), (35, 0.037), (45, 0.209), (50, 0.067), (52, 0.027), (60, 0.033), (62, 0.177), (77, 0.035), (78, 0.017), (90, 0.12)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88387209 <a title="186-lda-1" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>2 0.8523041 <a title="186-lda-2" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>Author: Seth Myers, Jure Leskovec</p><p>Abstract: In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1 -like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.</p><p>3 0.8139838 <a title="186-lda-3" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>4 0.80535239 <a title="186-lda-4" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>5 0.80192065 <a title="186-lda-5" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>6 0.79338735 <a title="186-lda-6" href="./nips-2010-Permutation_Complexity_Bound_on_Out-Sample_Error.html">205 nips-2010-Permutation Complexity Bound on Out-Sample Error</a></p>
<p>7 0.79196668 <a title="186-lda-7" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>8 0.7912845 <a title="186-lda-8" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>9 0.79120278 <a title="186-lda-9" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>10 0.79113483 <a title="186-lda-10" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>11 0.79056144 <a title="186-lda-11" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>12 0.79044807 <a title="186-lda-12" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>13 0.7850464 <a title="186-lda-13" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>14 0.78396654 <a title="186-lda-14" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>15 0.78210127 <a title="186-lda-15" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>16 0.78178281 <a title="186-lda-16" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>17 0.78132629 <a title="186-lda-17" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>18 0.77799141 <a title="186-lda-18" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>19 0.77752131 <a title="186-lda-19" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>20 0.77424139 <a title="186-lda-20" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
