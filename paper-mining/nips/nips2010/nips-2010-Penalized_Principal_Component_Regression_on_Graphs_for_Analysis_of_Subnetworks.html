<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-204" href="#">nips2010-204</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</h1>
<br/><p>Source: <a title="nips-2010-204-pdf" href="http://papers.nips.cc/paper/3957-penalized-principal-component-regression-on-graphs-for-analysis-of-subnetworks.pdf">pdf</a></p><p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>Reference: <a title="nips-2010-204-reference" href="../nips2010_reference/nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Network models are widely used to capture interactions among component of complex systems, such as social and biological. [sent-3, score-0.079]
</p><p>2 Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. [sent-5, score-0.547]
</p><p>3 We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. [sent-6, score-0.104]
</p><p>4 The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. [sent-7, score-0.657]
</p><p>5 Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. [sent-8, score-0.082]
</p><p>6 The performance of the proposed methodology is illustrated using simulated and real data examples from biology. [sent-9, score-0.067]
</p><p>7 This idea has motivated the method of gene set enrichment analysis (GSEA), along with a number of related methods [1, 2]. [sent-12, score-0.163]
</p><p>8 genes), interactions among them can be preserved, and more efﬁcient inference methods can be developed. [sent-15, score-0.082]
</p><p>9 [3, 4] and references therein) has focused on directly incorporating the network information in order to achieve better efﬁciency in assessing the signiﬁcance of individual components. [sent-18, score-0.08]
</p><p>10 These ideas have been combined in [5, 6], by introducing a model for incorporating the regulatory gene network, and developing an inference framework for analysis of subnetworks deﬁned by biological pathways. [sent-19, score-0.795]
</p><p>11 In this frameworks, called NetGSA, a global model is introduced with parameters 1  for individual genes/proteins, and the parameters are then combined appropriately in order to assess the signiﬁcance of biological pathways. [sent-20, score-0.073]
</p><p>12 However, the main challenge in applying NetGSA in realworld biological applications is the extensive computational time. [sent-21, score-0.073]
</p><p>13 In this paper, we propose a dimension reduction technique for networks, based on Laplacian eigenmaps, with the goal of providing an optimal low-dimensional projection for the space of random variables in each subnetwork. [sent-23, score-0.162]
</p><p>14 We then propose a general inference framework for analysis of subnetworks by reformulating the inference problem as a penalized principal regression problem on the graph. [sent-24, score-0.739]
</p><p>15 In Section 2, we review the Laplacian eigenmaps and establish their connection to principal component analysis (PCA) for random variables on a graph. [sent-25, score-0.441]
</p><p>16 Inference for signiﬁcance of subnetworks is discussed in Section 3, where we introduce Laplacian eigenmaps with Neumann boundary conditions and present the group-penalized principal component regression framework for analysis of arbitrary subnetworks. [sent-26, score-1.064]
</p><p>17 Results of applying the new methodology to simulated and real data examples are presented in Section 4, and the results are summarized in Section 5. [sent-27, score-0.067]
</p><p>18 expression values of genes) deﬁned on nodes of an undirected (weighted) graph G = (V, E). [sent-33, score-0.196]
</p><p>19 Here V is the set of nodes of G and E ⊆ V ×V its edge set. [sent-34, score-0.07]
</p><p>20 Throughout this paper, we represent the edge set and the strength of associations among nodes through the adjacency matrix of the graph A. [sent-35, score-0.31]
</p><p>21 Finally, we denote the observed values of the random variables by the n × p data matrix X. [sent-38, score-0.103]
</p><p>22 The subnetworks of interest are deﬁned based on additional knowledge about their attributes and functions. [sent-39, score-0.523]
</p><p>23 In biological applications, these subnetworks are deﬁned by common biological function, co-regulation or chromosomal location. [sent-40, score-0.669]
</p><p>24 The objective of the current paper is to develop dimension reduction methods on networks, in order to assess the signiﬁcance of a priori deﬁned subnetworks (e. [sent-41, score-0.596]
</p><p>25 1  Graph Laplacian and Eigenmaps  Laplacian eigenmaps are deﬁned using the eigenfunctions of the graph Laplacian, which is commonly used in spectral graph theory, computer science and image processing. [sent-45, score-0.68]
</p><p>26 Applications based on Laplacian eigenmaps include image segmentation and the normalized cut algorithm of [7], spectral clustering [8, 9] and collaborative ﬁltering [10]. [sent-46, score-0.234]
</p><p>27 The Laplacian matrix and its eigenvectors have also been used in biological applications. [sent-47, score-0.161]
</p><p>28 For example, in [11], the Laplacian matrix has been used to deﬁne a network-penalty for variable selection on graphs, and the interpretation of Laplacian eigenmaps as a Fourier basis was exploited in [12] to propose supervised and unsupervised classiﬁcation methods. [sent-48, score-0.235]
</p><p>29 Different deﬁnitions and representations have been proposed for the spectrum of a graph, and the results may vary depending on the deﬁnition of the Laplacian matrix (see [13] for a review). [sent-49, score-0.071]
</p><p>30 Dii = ∑ j Ai j ≡ di , and deﬁne the Laplacian matrix of the graph by L = D−1/2 (D − A)D−1/2 , or alternatively   1 − Ajj j = i, d j = 0  dj  Ai j Li j = −√ j∼i  di d j   0 o. [sent-53, score-0.167]
</p><p>31 Its eigenfunctions are known as the spectrum of G , and optimize the Rayleigh quotient ∑i∼ j ( f (i) − f ( j))2 g, L g , = g, g ∑ j f ( j)2 d j  (1)  It can be seen from (1), that the 0-eigenvalue of L is g = D1/2 1, corresponding to the average over the graph G . [sent-59, score-0.385]
</p><p>32 In graphical models, the undirected graph G among random variables corresponds naturally to a Markov random ﬁeld [14]. [sent-63, score-0.237]
</p><p>33 The following result establishes the relationship between the Laplacian eigenmaps and the principal components of the random variables deﬁned on the nodes of the graph, in case of Gaussian observations. [sent-64, score-0.457]
</p><p>34 , X p ) be random variables deﬁned on the nodes of graph G = (V, E) and denote by L and L + the Laplacian matrix of G and its Moore-Penrose generalized inverse. [sent-69, score-0.299]
</p><p>35 , ν p−1 denote the eigenfunctions corresponding to eigenvalues of L . [sent-74, score-0.244]
</p><p>36 , ν p−1 are the principal components of X, with ν0 corresponding to the leading principal component. [sent-78, score-0.229]
</p><p>37 For Gaussian random variables, the inverse covariance (or precision) matrix has the same non-zero pattern as the adjacency matrix of the graph, i. [sent-80, score-0.156]
</p><p>38 p}\i and C = [ci j ] has the same non-zero pattern as the adjacency matrix of the graph A, and amounts to a proper probability distribution for X. [sent-89, score-0.216]
</p><p>39 Then, it is easily seen that the principal components suppose, without loss of generality, that τi ˜ ˜ of X are given by eigenfunctions of L −1 , which are in turn equal to the eigenfunctions of L with the ordering of the eigenvalues reversed. [sent-97, score-0.569]
</p><p>40 However, since eigenfunctions of L + ζ I p and L are equal, the principal components of X are obtained from eigenfunctions of L . [sent-98, score-0.519]
</p><p>41 3  X1  ρ1  X2  ρ2  X3  Figure 1: Left: A simple subnetwork of interest, marked with the dotted circle. [sent-99, score-0.23]
</p><p>42 Right: Illustration of the Neumann random walk, the dotted curve indicates the boundary of the subnetwork. [sent-100, score-0.163]
</p><p>43 An alternative justiﬁcation for the above result, for general probability distributions deﬁned on graphs, can be given by assuming that the graph represents “similarities” among random variables and using an optimal embedding of graph G in a lower dimensional Euclidean space1 . [sent-102, score-0.416]
</p><p>44 In the case of one dimensional embedding, the goal is to ﬁnd an embedding v = (v1 , . [sent-103, score-0.078]
</p><p>45 , v p )T that preserves the distances among the nodes of the graph. [sent-106, score-0.094]
</p><p>46 Lemma 1 provides an efﬁcient dimension reduction framework that summarizes the information in the entire network into few feature vectors. [sent-110, score-0.124]
</p><p>47 Although the resulting dimension reduction method can be used efﬁciently in classiﬁcation (as in [12]), the eigenfunctions of G do not provide any information about signiﬁcance of arbitrary subnetworks, and therefore cannot be used to analyze the changes in subnetworks. [sent-111, score-0.267]
</p><p>48 However, in order to achieve dimension reduction, we need a method that only incorporates local information at the level of each subnetwork, and possibly its neighbors (see the left panel of Figure 1). [sent-114, score-0.071]
</p><p>49 Using the connection of the Laplace operator in Reimannian manifolds to heat ﬂow (see e. [sent-115, score-0.124]
</p><p>50 [17]), the problem of analysis of arbitrary subnetworks can be reformulated as a heat equation with boundary conditions. [sent-117, score-0.747]
</p><p>51 It then follows that in order to assess the “effect” of each subnetwork, the appropriate boundary conditions should block the ﬂow of heat at the boundary of the set. [sent-118, score-0.364]
</p><p>52 This corresponds to insulating the boundary, also known as the Neumann boundary condition. [sent-119, score-0.138]
</p><p>53 For the general heat equation τ(v, x), this boundary condition is given by ∂ τ (x) = 0 at each boundary point x, where v is ∂v the normal direction orthogonal to the tangent hyperplane at x. [sent-120, score-0.338]
</p><p>54 The eigenvalues of subgraphs with boundary conditions are studied in [13]. [sent-121, score-0.214]
</p><p>55 In particular, let S be any (connected) subnetwork of G , and denote by δ S the boundary of S in G . [sent-122, score-0.368]
</p><p>56 The Neumann boundary condition states that for every x ∈ δ S, ∑y:{x,y}∈δ S ( f (x) − f (y)) = 0. [sent-123, score-0.138]
</p><p>57 The Neumann eigenfunctions of S are then the optimizers of the restricted Rayleigh quotient λS,i = inf sup  f g∈Ci−1  ∑{t,u}∈S∪δ S ( f (t) − f (u))2 2 ∑t∈S ( f (t) − g(t)) dt  where Ci−1 is the projection to the space of previous eigenfunctions. [sent-124, score-0.28]
</p><p>58 ˜ Let P and P denote the transition probability matrix of the reﬂected random walk, and the original random walk deﬁned on G , respectively. [sent-128, score-0.192]
</p><p>59 For the general case of weighted graphs, deﬁne the transition probability matrix of the reﬂected random walk by  j ∼ i, i, j ∈ S  Pi j Aik A ˜ Pi j + d d k j j ∼ k ∼ i, k ∈ S / Pi j = (3) i k  0 o. [sent-130, score-0.167]
</p><p>60 The connection with the Neumann random walk also sheds light into the effect of the proposed boundary condition on the joint probability distribution of the random variables on the graph. [sent-135, score-0.386]
</p><p>61 To illustrate this, consider the simple graph in the right panel of Figure 1. [sent-136, score-0.162]
</p><p>62 For the moment, suppose that the random variables X1 , X2 , X3 are Gaussian, and the edges from X1 and X2 to X3 are directed. [sent-137, score-0.086]
</p><p>63 The reﬂected random walk, for the original problem, therefore corresponds to ﬁrst setting all the inﬂuences from other nodes in the graph to nodes in the set S to zero (resulting in directed edges) and then conditioning on the boundary variables. [sent-139, score-0.459]
</p><p>64 1  Group-Penalized PCR on Graph  Using the Neumann eigenvectors of subnetworks, we now deﬁne a principal component regression on graphs, which can be used to analyze the signiﬁcance of subnetworks. [sent-142, score-0.206]
</p><p>65 Let N j denote the |S j | × m j matrix of the m j smallest Neumann eigenfunctions for subgraph S j . [sent-143, score-0.235]
</p><p>66 An m j -dimensional projection of the original data ˜ matrix X ( j) is then given by X ( j) = X ( j) N j . [sent-145, score-0.068]
</p><p>67 Different methods can be used in order to determine the number of eigenfunctions m j for each subnetwork. [sent-146, score-0.194]
</p><p>68 5  The signiﬁcance of subnetwork S j is a function of the combined effect of all the nodes, captured ˜ by the transformed data matrix X ( j) . [sent-150, score-0.319]
</p><p>69 Also, let X be the mn × Jmr design matrix corresponding to the experimental settings, where r is the number of parameters used to model experimental conditions, and β be the vector of regression coefﬁcients. [sent-153, score-0.098]
</p><p>70 To evaluate the combined effect of each subnetwork, we impose a group penalty on the coefﬁcient of the regression of y on the design matrix X . [sent-159, score-0.099]
</p><p>71 Speciﬁcally, let qi be the total number of subnetworks conγ sidered signiﬁcant based on the value of γ in the ith bootstrap sample. [sent-164, score-0.595]
</p><p>72 In other words, if Pi is the probability of selecting the coefﬁcients corresponding to subnetwork j in the ith bootstrap sample, the subnetwork j is considered ( j) signiﬁcant if maxγ Pi ≥ π. [sent-166, score-0.532]
</p><p>73 We assume the columns of design matrix X are normalized so that n−1 Xi T Xi = 1, Throughout this paper, we consider the case where the total number of nodes in the graph p, and the number of design parameters r are allowed to diverge (the p n setting). [sent-170, score-0.237]
</p><p>74 Also suppose that for j = k, the transformed random variables X ( j) and X (k) are independent. [sent-174, score-0.086]
</p><p>75 Upon establishing the fact that for the proposed tuning parameter γ ∼ log p/(nm3/2 ), it follows from the results in [22] that for each bootstrap sample, there exists ε = ε(n) > 0 such that with probability at least 1 − (rp)−ε the signiﬁcant subnetworks are correctly selected. [sent-178, score-0.622]
</p><p>76 Thus if π ≤ 1−(rp)−ε , the coefﬁcients for signiﬁcant subnetworks are included in the ﬁnal 2 The  problem in (5) can be solved using the R-package grplasso [19]. [sent-179, score-0.523]
</p><p>77 In particular, it can be shown that ζ = Φ{ B(1 − (rp)−ε − π)/2}, where B is the number of bootstrap samples and Φ is the cumulative normal distribution. [sent-182, score-0.072]
</p><p>78 Next, note that the normality assumption, and the fact that the eigenfunctions within each sub˜ ( j) network are orthogonal, imply that for each j, Xi , i = 1, . [sent-184, score-0.245]
</p><p>79 4  Experiments  We illustrate the performance of the proposed method using simulated data motivated by biological applications, as well as a real data application based on gene expression analysis. [sent-198, score-0.175]
</p><p>80 In the simulation, we generate a small network of 80 nodes (genes), with 8 subnetworks. [sent-199, score-0.121]
</p><p>81 Under the null hypothesis, µnull = 1 and the association weight ρ for all edges of the network is set to 0. [sent-201, score-0.075]
</p><p>82 Table 1 also includes the estimated powers of the tests for subnetworks based on 200 simulations with n = 50 observations. [sent-205, score-0.588]
</p><p>83 It can be seen that the proposed GPCR method offers improvements over GSEA [1], especially in case of subnetworks 3 and 6. [sent-206, score-0.562]
</p><p>84 In [5], the pathways involved in Galactose utilization in yeast were analyzed based on the data from [23], and the performances of the NetGSA and GSEA methods were compared. [sent-208, score-0.146]
</p><p>85 The interactions among genes, along with signiﬁcance of individual genes (based on single gene analysis) are given in the right panel of Figure 2, and the results of signiﬁcance analysis based on NetGSA, GSEA and the proposed GPCR are given in Table 2. [sent-209, score-0.258]
</p><p>86 As in the simulated example, the results of this analysis indicate that GPCR results in improved efﬁciency over GSEA, while failing to detect the signiﬁcance of some of the pathways detected by NetGSA. [sent-210, score-0.108]
</p><p>87 5  Conclusion  We proposed a principal component regression method for graphs, called GPCR, using Laplacian eigenmaps with Neumann boundary conditions. [sent-211, score-0.491]
</p><p>88 The proposed method offers a systematic approach  Table 1: Parameter settings under the alternative and estimated powers for the simulation study. [sent-212, score-0.156]
</p><p>89 Right: Network of yeast genes involved in Galactose utilization. [sent-254, score-0.126]
</p><p>90 for dimension reduction in networks, with a priori deﬁned subnetworks of interest. [sent-255, score-0.596]
</p><p>91 It can also incorporate both weighted and unweighted adjacency matrices and can be easily extended to analyzing complex experimental conditions through the framework of linear models. [sent-256, score-0.1]
</p><p>92 Our simulation studies, and the real data example indicate that the proposed GPCR method offers signiﬁcant improvements over the methods of gene set enrichment analysis (GSEA). [sent-258, score-0.229]
</p><p>93 In addition, NetGSA requires that r < n, whilst the dimension reduction and the penalization of the proposed GPCR removes the need for any such restriction and facilitates the analysis of complex experiments in the settings with small sample sizes. [sent-263, score-0.122]
</p><p>94 Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression proﬁles. [sent-283, score-0.066]
</p><p>95 Discovering regulatory and signalling circuits in molecular interaction networks. [sent-296, score-0.068]
</p><p>96 A markov random ﬁeld model for network-based analysis of genomic data. [sent-299, score-0.076]
</p><p>97 Analysis of gene sets based on the underlying regulatory network. [sent-304, score-0.116]
</p><p>98 The principal components analysis of a graph, and its relationships to spectral clustering. [sent-321, score-0.195]
</p><p>99 A novel way of computing dissimilarities between nodes of a graph, with application to collaborative ﬁltering and subspace projection of the graph nodes. [sent-337, score-0.223]
</p><p>100 Laplacian eigenmaps and spectral techniques for embedding and clustering. [sent-377, score-0.287]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subnetworks', 0.523), ('gpcr', 0.305), ('netgsa', 0.262), ('neumann', 0.244), ('subnetwork', 0.23), ('gsea', 0.218), ('eigenmaps', 0.194), ('eigenfunctions', 0.194), ('laplacian', 0.184), ('cance', 0.159), ('boundary', 0.138), ('graph', 0.126), ('walk', 0.101), ('principal', 0.098), ('shojaie', 0.087), ('galactose', 0.077), ('genes', 0.073), ('gene', 0.073), ('biological', 0.073), ('bootstrap', 0.072), ('nodes', 0.07), ('enrichment', 0.066), ('powers', 0.065), ('heat', 0.062), ('pathways', 0.055), ('ected', 0.055), ('embedding', 0.053), ('network', 0.051), ('graphs', 0.051), ('eigenvalues', 0.05), ('adjacency', 0.049), ('eigenvectors', 0.047), ('fwer', 0.044), ('ideker', 0.044), ('manova', 0.044), ('pcr', 0.044), ('reimannian', 0.044), ('subnet', 0.044), ('regulatory', 0.043), ('alt', 0.041), ('matrix', 0.041), ('spectral', 0.04), ('offers', 0.039), ('remark', 0.038), ('transport', 0.038), ('fouss', 0.038), ('rayleigh', 0.038), ('utilization', 0.038), ('pi', 0.038), ('methodology', 0.038), ('reduction', 0.038), ('ai', 0.038), ('variables', 0.037), ('rp', 0.037), ('connection', 0.036), ('panel', 0.036), ('signi', 0.036), ('quotient', 0.035), ('acid', 0.035), ('arbor', 0.035), ('dimension', 0.035), ('regression', 0.034), ('components', 0.033), ('pathway', 0.031), ('ann', 0.031), ('ci', 0.031), ('bioinformatics', 0.031), ('yeast', 0.03), ('functionally', 0.03), ('conditioning', 0.03), ('spectrum', 0.03), ('inference', 0.03), ('incorporating', 0.029), ('simulated', 0.029), ('michigan', 0.029), ('interactions', 0.028), ('tuning', 0.027), ('projection', 0.027), ('simulation', 0.027), ('genomic', 0.027), ('synthesis', 0.027), ('component', 0.027), ('cant', 0.026), ('manifolds', 0.026), ('stress', 0.026), ('conditions', 0.026), ('random', 0.025), ('molecular', 0.025), ('unweighted', 0.025), ('settings', 0.025), ('dimensional', 0.025), ('effect', 0.024), ('inf', 0.024), ('transformed', 0.024), ('edges', 0.024), ('ecml', 0.024), ('analysis', 0.024), ('among', 0.024), ('mn', 0.023), ('involved', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="204-tfidf-1" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>2 0.12932985 <a title="204-tfidf-2" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>3 0.085979432 <a title="204-tfidf-3" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>4 0.084033549 <a title="204-tfidf-4" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>5 0.080197856 <a title="204-tfidf-5" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>6 0.078828737 <a title="204-tfidf-6" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>7 0.076040648 <a title="204-tfidf-7" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>8 0.070308901 <a title="204-tfidf-8" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>9 0.069411971 <a title="204-tfidf-9" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>10 0.067986101 <a title="204-tfidf-10" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>11 0.067756802 <a title="204-tfidf-11" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>12 0.067423098 <a title="204-tfidf-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.064251214 <a title="204-tfidf-13" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>14 0.062794194 <a title="204-tfidf-14" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>15 0.05761243 <a title="204-tfidf-15" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>16 0.057554469 <a title="204-tfidf-16" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>17 0.055198967 <a title="204-tfidf-17" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>18 0.055146754 <a title="204-tfidf-18" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>19 0.054227095 <a title="204-tfidf-19" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>20 0.054136466 <a title="204-tfidf-20" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.155), (1, 0.042), (2, 0.032), (3, 0.1), (4, -0.012), (5, -0.094), (6, 0.016), (7, -0.005), (8, -0.021), (9, 0.007), (10, -0.048), (11, -0.002), (12, -0.022), (13, 0.001), (14, 0.048), (15, -0.081), (16, 0.043), (17, -0.085), (18, -0.038), (19, 0.072), (20, 0.011), (21, 0.007), (22, -0.107), (23, 0.023), (24, 0.057), (25, 0.075), (26, 0.051), (27, 0.046), (28, 0.001), (29, 0.08), (30, 0.076), (31, 0.047), (32, 0.02), (33, -0.089), (34, -0.038), (35, -0.123), (36, -0.019), (37, 0.005), (38, 0.035), (39, 0.055), (40, -0.072), (41, 0.027), (42, 0.078), (43, -0.079), (44, -0.009), (45, -0.01), (46, 0.146), (47, -0.015), (48, 0.002), (49, -0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91390467 <a title="204-lsi-1" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>2 0.81944138 <a title="204-lsi-2" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>3 0.77709973 <a title="204-lsi-3" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>4 0.70167649 <a title="204-lsi-4" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>5 0.68044364 <a title="204-lsi-5" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>Author: Ulrike V. Luxburg, Agnes Radl, Matthias Hein</p><p>Abstract: The commute distance between two vertices in a graph is the expected time it takes a random walk to travel from the ﬁrst to the second vertex and back. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph at all and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. As an alternative we introduce the ampliﬁed commute distance that corrects for the undesired large sample effects. 1</p><p>6 0.65237725 <a title="204-lsi-6" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>7 0.6401369 <a title="204-lsi-7" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>8 0.59893662 <a title="204-lsi-8" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>9 0.53845435 <a title="204-lsi-9" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>10 0.53757995 <a title="204-lsi-10" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>11 0.48333603 <a title="204-lsi-11" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>12 0.46268481 <a title="204-lsi-12" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>13 0.45447928 <a title="204-lsi-13" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>14 0.44318187 <a title="204-lsi-14" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>15 0.42944175 <a title="204-lsi-15" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>16 0.41615373 <a title="204-lsi-16" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>17 0.41126198 <a title="204-lsi-17" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>18 0.40864608 <a title="204-lsi-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.40742576 <a title="204-lsi-19" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>20 0.38151991 <a title="204-lsi-20" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.064), (17, 0.028), (27, 0.051), (30, 0.065), (35, 0.041), (45, 0.167), (50, 0.041), (52, 0.042), (60, 0.018), (77, 0.046), (78, 0.015), (79, 0.309), (90, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84376693 <a title="204-lda-1" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>same-paper 2 0.73008835 <a title="204-lda-2" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>3 0.66805798 <a title="204-lda-3" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>4 0.64481616 <a title="204-lda-4" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>5 0.56532079 <a title="204-lda-5" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>6 0.56097925 <a title="204-lda-6" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>7 0.55775398 <a title="204-lda-7" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>8 0.55774528 <a title="204-lda-8" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>9 0.55593485 <a title="204-lda-9" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>10 0.55463493 <a title="204-lda-10" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>11 0.55387068 <a title="204-lda-11" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>12 0.55298287 <a title="204-lda-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.55108637 <a title="204-lda-13" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>14 0.55067587 <a title="204-lda-14" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>15 0.55033374 <a title="204-lda-15" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>16 0.55027229 <a title="204-lda-16" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>17 0.54969925 <a title="204-lda-17" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>18 0.54953539 <a title="204-lda-18" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>19 0.54921305 <a title="204-lda-19" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>20 0.54773974 <a title="204-lda-20" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
