<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-109" href="#">nips2010-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</h1>
<br/><p>Source: <a title="nips-2010-109-pdf" href="http://papers.nips.cc/paper/3997-group-sparse-coding-with-a-laplacian-scale-mixture-prior.pdf">pdf</a></p><p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>Reference: <a title="nips-2010-109-reference" href="../nips2010_reference/nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. [sent-7, score-0.439]
</p><p>2 Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. [sent-8, score-0.263]
</p><p>3 We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. [sent-9, score-0.203]
</p><p>4 When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. [sent-10, score-0.372]
</p><p>5 We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. [sent-11, score-0.491]
</p><p>6 We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. [sent-12, score-0.516]
</p><p>7 1  Introduction  The concept of sparsity is widely used in the signal processing, machine learning and statistics communities for model ﬁtting and solving inverse problems. [sent-13, score-0.19]
</p><p>8 This approach has been applied to problems such as image coding, compressive sensing [4], or classiﬁcation [5]. [sent-16, score-0.286]
</p><p>9 Hence, the 2 signal model assumed by BPDN is linear, generative, and the basis function coefﬁcients are independent. [sent-20, score-0.191]
</p><p>10 It has also been observed in the context of generative image models that the inferred sparse coefﬁcients exhibit pronounced statistical dependencies [15, 16], and therefore the independence assumption is violated. [sent-22, score-0.358]
</p><p>11 It has been proposed in block- 1 methods to account for dependencies among the coefﬁcients by dividing them into subspaces such that dependencies within the subspaces are allowed, but not across the subspaces [17] . [sent-23, score-0.311]
</p><p>12 The coefﬁcient prior is thus a mixture of Laplacian distributions which we denote “Laplacian Scale Mixture” (LSM), which is an analogy to the Gaussian scale mixture (GSM) [12]. [sent-28, score-0.29]
</p><p>13 Higher-order dependencies of feedforward responses of wavelet coefﬁcients [12] or basis functions learned using independent component analysis [14] have been captured using GSMs, and we extend this approach to a generative sparse coding model using LSMs. [sent-29, score-0.622]
</p><p>14 We deﬁne the Laplacian scale mixture in Section 2, and we describe the inference algorithms in the resulting sparse coding models with an LSM prior on the coefﬁcients in Section 3. [sent-30, score-0.536]
</p><p>15 We present an example of a factorial LSM model in Section 4, and of a non-factorial LSM model in Section 5 that is particularly well suited to signals having the “group sparsity” property. [sent-31, score-0.198]
</p><p>16 We show that the nonfactorial LSM results in a divisive normalization rule for inferring the coefﬁcients. [sent-32, score-0.256]
</p><p>17 When the groups are organized topographically and the basis is trained on natural images, the resulting model resembles the neighborhood divisive normalization that has been hypothesized to occur in visual cortex. [sent-33, score-0.564]
</p><p>18 We also demonstrate that the proposed LSM inference algorithm provides superior performance in image coding and compressive sensing recovery. [sent-34, score-0.484]
</p><p>19 2  The Laplacian Scale Mixture distribution  A random variable si is a Laplacian scale mixture if it can be written si = λ−1 ui , where ui has i 1 a Laplacian distribution with scale 1, i. [sent-35, score-0.884]
</p><p>20 Conditioned on the parameter λi , the coefﬁcient si has a Laplacian distribution with inverse scale λi , i. [sent-39, score-0.412]
</p><p>21 The distribution over si is therefore a continuous mixture of Laplacian 2 distributions with different inverse scales, and it can be computed by integrating out λi ∞  ∞  p(si | λi )p(λi )dλi =  p(si ) = 0  0  λi −λi |si | e p(λi )dλi . [sent-42, score-0.421]
</p><p>22 3  Inference in a sparse coding model with LSM prior  We propose the linear generative model m  x = Φs + ν =  si ϕi + ν,  (2)  i=1  where x ∈ Rn , Φ = [ϕ1 , . [sent-46, score-0.687]
</p><p>23 , ϕm ] ∈ Rn×m is an overcomplete transform or basis set, and the columns ϕi are its basis functions. [sent-49, score-0.244]
</p><p>24 2  Given a signal x, we wish to infer its sparse representation s in the dictionary Φ. [sent-53, score-0.263]
</p><p>25 1 that it is tractable if the prior on λ is factorial and each λi has a Gamma distribution, as the Laplacian distribution and the Gamma distribution are conjugate. [sent-73, score-0.29]
</p><p>26 4  Sparse coding with a factorial LSM prior  We propose in this Section a sparse coding model where the distribution of the multipliers is factoα−1 rial, and each multiplier has a Gamma distribution, i. [sent-76, score-0.782]
</p><p>27 With this particular choice of a prior on the multiplier, we can compute the probability distribution of si analytically: αβ α . [sent-79, score-0.384]
</p><p>28 the posterior probability of λi given si is also a Gamma distribution when the prior over λi is a Gamma distribution and the conditional probability of si given λi is a Laplace distribution with inverse scale λi . [sent-85, score-0.823]
</p><p>29 Hence, the posterior of λi given si is a Gamma distribution with parameters α + 1 and β + |si |. [sent-86, score-0.301]
</p><p>30 In the factorial model we have (t) p(λ | s) = i p(λi | si ). [sent-89, score-0.427]
</p><p>31 We saw that with the Gamma prior we can compute the distribution of si analytically, and therefore we can compute the gradient of log p(s | x) with respect to s. [sent-94, score-0.426]
</p><p>32 Whereas they derive this rule from mathematical intuitions regarding the L1 ball, we show that this update rule follows from from Bayesian inference assuming a Gamma prior over λ. [sent-103, score-0.258]
</p><p>33 It was also shown that evidence maximization in a sparse coding model with an automatic relevance determination prior can also be solved via a sequence of reweighted 1 optimization problems [22]. [sent-104, score-0.399]
</p><p>34 2  Application to image coding  It has been shown that the convex relaxation consisting of replacing the 0 norm with the 1 norm is able to identify the sparsest solution under some conditions on the dictionary of basis functions [23]. [sent-106, score-0.435]
</p><p>35 For instance, it was observed in [16] that it is possible to infer sparser representations with a prior over the coefﬁcients that is a mixture of a delta function at zero and a Gaussian distribution than with the Laplacian prior. [sent-108, score-0.243]
</p><p>36 We show that our proposed inference algorithm also leads to representations that are more sparse, as the LSM prior with Gamma hyperprior has heavier tails than the Laplacian distribution. [sent-109, score-0.256]
</p><p>37 We selected 1000 16 × 16 image patches at random, and computed their sparse representations in a dictionary with 256 basis functions using both the conventional Laplacian prior and our LSM prior. [sent-110, score-0.532]
</p><p>38 The dictionary is learned from the statistics of natural images [24] using a Laplacian prior over the coefﬁcients. [sent-111, score-0.285]
</p><p>39 We can see in Figure 2 that the representations using the LSM prior are indeed more sparse by approximately a factor of 2. [sent-115, score-0.239]
</p><p>40 Note that the computational complexity to compute these sparse representations is much lower than that of [16]. [sent-116, score-0.156]
</p><p>41 Sparsity of the representation  140  λ1  λ2  λj  120  λm  s1  s2  sj  LSM prior  100  sm  80 60 40  φij  20  x1  xi  xn  00  Figure 1: Graphical model representation of our proposed generative model where the multipliers distribution is factorial. [sent-117, score-0.259]
</p><p>42 We show here that the LSM prior can be used to capture this group structure in natural images, and we propose an efﬁcient inference algorithm for this case. [sent-124, score-0.284]
</p><p>43 1  Group sparsity  We consider a dictionary Φ such that the basis functions can be divided in a set of disjoint groups or neighborhoods indexed by Nk , i. [sent-126, score-0.463]
</p><p>44 A signal having the group sparsity property is such that the sparse coefﬁcients occur in groups, i. [sent-132, score-0.376]
</p><p>45 The group sparsity structure can be captured with the LSM prior by having all the coefﬁcients in a group share the same inverse scale parameter, i. [sent-135, score-0.446]
</p><p>46 This addresses the case where dependencies are allowed within groups, but not across groups as in the block- 1 method [17]. [sent-139, score-0.18]
</p><p>47 Note that for some types of dictionaries it is more natural to consider overlapping groups to avoid blocking artifacts. [sent-140, score-0.191]
</p><p>48 λ(k)  si-2  si-1  λ(l)  si  si+1  si+2  λi-1  si+3  si-2  Figure 3: The two groups N(k) = {i − 2, i − 1, i} and N(l) = {i + 1, i + 2, i + 3} are nonoverlapping. [sent-143, score-0.338]
</p><p>49 2  λi  λi+1  λi+2  si-1  si  si+1  si+2  si+3  Figure 4: The basis function coefﬁcients in the neighborhood deﬁned by N (i) = {i−1, i, i+1} share the same multiplier λi . [sent-145, score-0.536]
</p><p>50 Using the structure of the dependencies in the probabilistic model shown in Figure 3, we have (11) λi p(λi |s(t) ) = λ(k) p(λ |s(t) ) (k)  Nk  where the index i is in the group Nk , and sNk = (sj )j∈Nk is the vector containing all the coefﬁcients in the group. [sent-149, score-0.193]
</p><p>51 Using the conjugacy of the Laplacian and Gamma distributions, the distribution of λ(k) given all the coefﬁcients in the neighborhood is a Gamma distribution with parameters α + |Nk | and β + j∈Nk |sj |, where |Nk | denotes the size of the neighborhood. [sent-150, score-0.196]
</p><p>52 (12) (t) β + j∈Nk |sj | The resulting update rule is a form of divisive normalization. [sent-152, score-0.213]
</p><p>53 We saw in Section 2 that we can write sk = λ−1 uk , where uk is a Laplacian random variable with scale 1, and thus after convergence we (k) (∞)  (∞)  (∞)  have uk = (α + |Nk |)sk /(β + j∈Nk |sj play an important role in the visual system. [sent-153, score-0.204]
</p><p>54 Let N (i) denote the indices of the neighborhood that is centered around si (see Figure 4 for an example). [sent-156, score-0.329]
</p><p>55 A coefﬁcient is indeed a member of many neighborhoods as shown in Figure 4, and the structure of the dependencies implies p(λi | s) = p(λi | sN (i) ). [sent-160, score-0.18]
</p><p>56 The authors show improved coding efﬁciency in the context of natural images. [sent-164, score-0.191]
</p><p>57 3  Compressive sensing recovery  In compressed sensing, we observe a number n of random projections of a signal s0 ∈ Rm , and it is in principle impossible to recover s0 if n < m. [sent-166, score-0.294]
</p><p>58 If the signal has structure beyond sparsity, one can in principle recover the signal with even fewer measurements using an algorithm that exploits this structure [19, 29]. [sent-171, score-0.188]
</p><p>59 (15)  i=1  We denote by RWBP the algorithm with the factorial update (9), and RW3 BP (resp. [sent-173, score-0.195]
</p><p>60 RW5 BP) the algorithm with our proposed divisive normalization update (13) with group size 3 (resp. [sent-174, score-0.299]
</p><p>61 We consider 50-dimensional signals that are sparse in the canonical basis and where the neighborhood size is 3. [sent-176, score-0.342]
</p><p>62 To sample such a signal s ∈ R50 , we draw a number d of “centroids” i, and we sample three values for si−1 , si and si+1 using a normal distribution of variance 1. [sent-177, score-0.37]
</p><p>63 A compressive sensing recovery problem is parameterized by (m, n, d). [sent-179, score-0.324]
</p><p>64 We ﬁx m = 50 and parameterize the phase plots using the indeterminacy of the system indexed by δ = n/m, and the approximate sparsity of the system 6  indexed by ρ = 3d/m. [sent-181, score-0.25]
</p><p>65 For a given value (δ, ρ) on the grid, we sample 10 sparse signals using the corresponding (m, n, d) parameters. [sent-185, score-0.165]
</p><p>66 The underlying sparse signal is recovered using the three algorithms and we average the recovery error s − s0 2 / s0 2 for each of them. [sent-186, score-0.274]
</p><p>67 0  Figure 5: Compressive sensing recovery results using synthetic data. [sent-249, score-0.19]
</p><p>68 Shown are the phase plots for a sequence of BP problems with the factorial update (RWBP), and a sequence of BP problems with the divisive normalization update with neighborhood size 3 (RW3 BP). [sent-250, score-0.524]
</p><p>69 On the x-axis is the sparsity of the system indexed by ρ = 3d/m, and on the y-axis is the indeterminacy of the system indexed by δ = n/m. [sent-251, score-0.198]
</p><p>70 However, the basis functions are learned under a probabilistic model where the probability density over the basis functions coefﬁcients is factorial, whereas the sparse coefﬁcients exhibit statistical dependencies [15, 16]. [sent-255, score-0.527]
</p><p>71 Hence, a generative model with factorial LSM is not rich enough to capture the complex statistics of natural images. [sent-256, score-0.264]
</p><p>72 We ﬁx a topography where the basis functions coefﬁcients are arranged on a 2D grid, and with overlapping neighborhoods of ﬁxed size 3 × 3. [sent-258, score-0.311]
</p><p>73 The corresponding inference algorithm uses the divisive normalization update (13). [sent-259, score-0.275]
</p><p>74 We learn the optimal dictionary of basis functions Φ using the learning rule ∆Φ = η (x − Φˆ)ˆT ss as in [24], where η is the learning rate, s are the basis functions coefﬁcients inferred under the model ˆ (13), and the average is taken over a batch of size 100. [sent-260, score-0.441]
</p><p>75 The learned basis functions are shown in Figure 6. [sent-262, score-0.17]
</p><p>76 We see here that the neighborhoods of size 3 × 3 group basis functions at a similar position, scale and orientation. [sent-263, score-0.35]
</p><p>77 The topography is similar to how neurons are arranged in the visual cortex, and is reminiscent of the results obtained in topographic ICA [13] and topographic mixture of experts models [31]. [sent-264, score-0.261]
</p><p>78 An important difference is that our model is based on a generative sparse coding model in which both inference and learning can be implemented via local network interactions [7]. [sent-265, score-0.383]
</p><p>79 Because of the topographic organization, we also obtain a neighborhoodbased divisive normalization rule. [sent-266, score-0.236]
</p><p>80 Does the proposed non-factorial model represent image structure more efﬁciently than those with factorial priors? [sent-267, score-0.225]
</p><p>81 To answer this question we measured the models’ ability to recover sparse structure in the compressed sensing setting. [sent-268, score-0.285]
</p><p>82 We note that the basis functions are learned such that they represent the sparse structure in images, as opposed to representing the images exactly (there is a noise term in the generative model (2)). [sent-269, score-0.438]
</p><p>83 Hence, we design our experiment such that we measure the recovery of this sparse structure. [sent-270, score-0.205]
</p><p>84 Using the basis functions shown in Figure 6, we ﬁrst infer the 7  sparse coefﬁcients s0 of an image patch x such that x − Φs0 2 < δ using the inference algorithm corresponding to the model. [sent-271, score-0.366]
</p><p>85 We ﬁx δ such that the SNR is 10, and thus the three sparse approximations for the three models contain the same amount of signal power. [sent-272, score-0.189]
</p><p>86 We compare the recovery performance Φˆ − Φs0 2 / Φs0 0 for 100 16 × 16 image patches selected at random, and s we use 110 random projections. [sent-276, score-0.158]
</p><p>87 We can see in Figure 7 that the model with non-factorial LSM prior outperforms the other models as it is able to capture the group sparsity structure in natural images. [sent-277, score-0.304]
</p><p>88 Figure 6: Basis functions learned in a nonfactorial LSM model with overlapping groups of size 3 × 3  6  Figure 7: Compressive sensing recovery. [sent-278, score-0.301]
</p><p>89 On the x-axis is the recovery performance for the factorial LSM model (RWBP), and on the y-axis the recovery performance for the non-factorial LSM model with 3 × 3 overlapping groups (RW3×3 BP). [sent-279, score-0.435]
</p><p>90 Conclusion  We introduced a new class of probability densities that can be used as a prior for the coefﬁcients in a generative sparse coding model of images. [sent-282, score-0.413]
</p><p>91 By exploiting the conjugacy of the Gamma and Laplacian prior, we were able to derive an efﬁcient inference algorithm that consists of solving a sequence of reweighted 1 least-square problems, thus leveraging the multitude of algorithms already developed for BPDN. [sent-283, score-0.191]
</p><p>92 Our framework also makes it possible to capture higher-order dependencies through group sparsity. [sent-284, score-0.168]
</p><p>93 When applied to natural images, the learned basis functions of the model may be topographically organized according to the speciﬁed group structure. [sent-285, score-0.329]
</p><p>94 We also showed that exploiting the group sparsity results in performance gains for compressive sensing recovery on natural images. [sent-286, score-0.52]
</p><p>95 Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems. [sent-352, score-0.308]
</p><p>96 A multi-layer sparse coding network learns contour coding from natural a images. [sent-391, score-0.456]
</p><p>97 Learning horizontal connections in a sparse coding model of natural images. [sent-398, score-0.311]
</p><p>98 Just relax: convex programming methods for identifying sparse signals in noise. [sent-447, score-0.165]
</p><p>99 Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. [sent-454, score-0.166]
</p><p>100 Natural image statistics and divisive normalization: Modeling nonlinearity and adaptation in cortical neurons. [sent-462, score-0.178]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsm', 0.549), ('si', 0.274), ('coef', 0.242), ('laplacian', 0.226), ('cients', 0.209), ('bp', 0.206), ('bpdn', 0.161), ('factorial', 0.153), ('gamma', 0.151), ('coding', 0.145), ('rwbp', 0.142), ('compressive', 0.134), ('divisive', 0.131), ('basis', 0.122), ('nk', 0.121), ('sparse', 0.12), ('sensing', 0.105), ('dependencies', 0.091), ('conjugacy', 0.087), ('recovery', 0.085), ('multiplier', 0.085), ('prior', 0.083), ('group', 0.077), ('dictionary', 0.074), ('sparsity', 0.073), ('mixture', 0.072), ('signal', 0.069), ('generative', 0.065), ('neighborhoods', 0.064), ('groups', 0.064), ('scale', 0.063), ('gsm', 0.061), ('sj', 0.06), ('images', 0.058), ('topographic', 0.056), ('neighborhood', 0.055), ('inference', 0.053), ('reweighted', 0.051), ('em', 0.05), ('normalization', 0.049), ('overlapping', 0.048), ('inverse', 0.048), ('image', 0.047), ('natural', 0.046), ('signals', 0.045), ('subspaces', 0.043), ('update', 0.042), ('indexed', 0.042), ('ui', 0.042), ('saw', 0.042), ('indeterminacy', 0.041), ('rule', 0.04), ('vancouver', 0.039), ('occur', 0.037), ('representations', 0.036), ('garrigues', 0.036), ('nonfactorial', 0.036), ('topographically', 0.036), ('cand', 0.035), ('compressed', 0.035), ('inferred', 0.035), ('reconstruction', 0.035), ('olshausen', 0.033), ('cevher', 0.033), ('duarte', 0.033), ('blocking', 0.033), ('arg', 0.031), ('wavelet', 0.031), ('hoyer', 0.031), ('topography', 0.031), ('hyperprior', 0.031), ('phase', 0.03), ('heavier', 0.028), ('distribution', 0.027), ('baraniuk', 0.027), ('hyv', 0.027), ('donoho', 0.027), ('patches', 0.026), ('canada', 0.026), ('structure', 0.025), ('uk', 0.025), ('allowed', 0.025), ('sparser', 0.025), ('tails', 0.025), ('synthesis', 0.025), ('wainwright', 0.025), ('functions', 0.024), ('multipliers', 0.024), ('february', 0.024), ('proc', 0.024), ('learned', 0.024), ('visual', 0.024), ('jensen', 0.023), ('pursuit', 0.023), ('berkeley', 0.023), ('rn', 0.023), ('solution', 0.023), ('analytically', 0.023), ('plots', 0.022), ('arranged', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="109-tfidf-1" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>2 0.16856603 <a title="109-tfidf-2" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>3 0.1162459 <a title="109-tfidf-3" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>4 0.11388585 <a title="109-tfidf-4" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>5 0.11186928 <a title="109-tfidf-5" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>Author: Jose Puertas, Joerg Bornschein, Joerg Luecke</p><p>Abstract: We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we ﬁnd that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reﬂects properties of simple cell receptive ﬁelds that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the ﬁrst large-scale application of such an approach. 1</p><p>6 0.10755602 <a title="109-tfidf-6" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>7 0.10490483 <a title="109-tfidf-7" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>8 0.099895261 <a title="109-tfidf-8" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>9 0.091490574 <a title="109-tfidf-9" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>10 0.090493582 <a title="109-tfidf-10" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>11 0.090423062 <a title="109-tfidf-11" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>12 0.083874241 <a title="109-tfidf-12" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>13 0.082391769 <a title="109-tfidf-13" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>14 0.081768513 <a title="109-tfidf-14" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>15 0.081640631 <a title="109-tfidf-15" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>16 0.080107011 <a title="109-tfidf-16" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>17 0.077534653 <a title="109-tfidf-17" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>18 0.072661459 <a title="109-tfidf-18" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>19 0.067423098 <a title="109-tfidf-19" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>20 0.066519454 <a title="109-tfidf-20" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, 0.07), (2, -0.084), (3, 0.087), (4, 0.025), (5, -0.091), (6, 0.047), (7, 0.154), (8, -0.113), (9, -0.04), (10, 0.04), (11, 0.034), (12, -0.066), (13, -0.068), (14, -0.092), (15, -0.113), (16, -0.009), (17, 0.101), (18, 0.003), (19, 0.012), (20, 0.084), (21, -0.026), (22, -0.025), (23, 0.019), (24, -0.065), (25, -0.039), (26, -0.055), (27, 0.001), (28, -0.043), (29, 0.085), (30, 0.023), (31, 0.146), (32, -0.095), (33, 0.012), (34, -0.07), (35, -0.042), (36, -0.04), (37, -0.026), (38, 0.016), (39, 0.016), (40, -0.053), (41, 0.025), (42, 0.135), (43, -0.005), (44, 0.018), (45, -0.025), (46, 0.048), (47, 0.077), (48, 0.029), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9608435 <a title="109-lsi-1" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>2 0.76355368 <a title="109-lsi-2" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>Author: J. Z. Kolter, Siddharth Batra, Andrew Y. Ng</p><p>Abstract: Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having devicelevel energy information can cause users to conserve signiﬁcant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms speciﬁcally to maximize disaggregation performance. We show that this signiﬁcantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage. 1</p><p>3 0.7341212 <a title="109-lsi-3" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>4 0.70678812 <a title="109-lsi-4" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>Author: Taehwan Kim, Gregory Shakhnarovich, Raquel Urtasun</p><p>Abstract: Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend the sparse coding framework to learn interpretable spatio-temporal primitives. We formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations that provide interpretability as well as smoothness constraints that are inherent to human motion. We demonstrate the effectiveness of our approach to learn interpretable representations of human motion from motion capture data, and show that our approach outperforms recently developed matching pursuit and sparse coding algorithms. 1</p><p>5 0.69373381 <a title="109-lsi-5" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>Author: Jose Puertas, Joerg Bornschein, Joerg Luecke</p><p>Abstract: We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we ﬁnd that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reﬂects properties of simple cell receptive ﬁelds that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the ﬁrst large-scale application of such an approach. 1</p><p>6 0.68756753 <a title="109-lsi-6" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>7 0.61696297 <a title="109-lsi-7" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>8 0.59301835 <a title="109-lsi-8" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>9 0.53175503 <a title="109-lsi-9" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>10 0.50723118 <a title="109-lsi-10" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>11 0.50710487 <a title="109-lsi-11" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>12 0.45954391 <a title="109-lsi-12" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>13 0.45807821 <a title="109-lsi-13" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>14 0.45746428 <a title="109-lsi-14" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>15 0.45697153 <a title="109-lsi-15" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>16 0.4428761 <a title="109-lsi-16" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>17 0.43354085 <a title="109-lsi-17" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>18 0.41392612 <a title="109-lsi-18" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>19 0.4095982 <a title="109-lsi-19" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>20 0.40455458 <a title="109-lsi-20" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.043), (17, 0.038), (27, 0.102), (30, 0.057), (35, 0.082), (45, 0.193), (50, 0.068), (52, 0.08), (60, 0.019), (77, 0.067), (83, 0.149), (90, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89055264 <a title="109-lda-1" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>2 0.82813525 <a title="109-lda-2" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>3 0.82730663 <a title="109-lda-3" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>Author: Georg Langs, Yanmei Tie, Laura Rigolo, Alexandra Golby, Polina Golland</p><p>Abstract: Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difﬁcult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We ﬁrst embed each brain into a functional map that reﬂects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions. 1</p><p>4 0.8253653 <a title="109-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.82037652 <a title="109-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.81990206 <a title="109-lda-6" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>7 0.81426829 <a title="109-lda-7" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>8 0.81319994 <a title="109-lda-8" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>9 0.81239736 <a title="109-lda-9" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>10 0.80732089 <a title="109-lda-10" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>11 0.80724686 <a title="109-lda-11" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>12 0.80631846 <a title="109-lda-12" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>13 0.80619794 <a title="109-lda-13" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>14 0.80443817 <a title="109-lda-14" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>15 0.80105466 <a title="109-lda-15" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>16 0.80065972 <a title="109-lda-16" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>17 0.80037093 <a title="109-lda-17" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>18 0.79913956 <a title="109-lda-18" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>19 0.79884768 <a title="109-lda-19" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>20 0.79805487 <a title="109-lda-20" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
