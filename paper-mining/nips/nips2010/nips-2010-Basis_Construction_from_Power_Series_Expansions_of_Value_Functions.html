<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-37" href="#">nips2010-37</a> knowledge-graph by maker-knowledge-mining</p><h1>37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</h1>
<br/><p>Source: <a title="nips-2010-37-pdf" href="http://papers.nips.cc/paper/3898-basis-construction-from-power-series-expansions-of-value-functions.pdf">pdf</a></p><p>Author: Sridhar Mahadevan, Bo Liu</p><p>Abstract: This paper explores links between basis construction methods in Markov decision processes and power series expansions of value functions. This perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases. Krylov and Bellman error bases are based on the Neumann series expansion. These bases incur very large initial Bellman errors, and can converge rather slowly as the discount factor approaches unity. The Laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efﬁcient basis representations. The ﬁrst two terms in the Laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the Drazin (or group inverse) of a singular matrix derived from the transition matrix. Experiments show that Drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor. An incremental variant of Drazin bases called Bellman average-reward bases (BARBs) is described, which provides some of the same beneﬁts at lower computational cost. 1</p><p>Reference: <a title="nips-2010-37-reference" href="../nips2010_reference/nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bebf', 0.598), ('drazin', 0.391), ('barb', 0.358), ('krylov', 0.345), ('pvf', 0.239), ('bellm', 0.239), ('reward', 0.209), ('mp', 0.1), ('laur', 0.093), ('discount', 0.072), ('neuman', 0.071), ('dbf', 0.065), ('expand', 0.055), ('barbk', 0.054), ('span', 0.048), ('chain', 0.048), ('er', 0.046), ('transit', 0.044), ('eigenvect', 0.044), ('pvfs', 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="37-tfidf-1" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>2 0.12725751 <a title="37-tfidf-2" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>3 0.11914077 <a title="37-tfidf-3" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>4 0.11788081 <a title="37-tfidf-4" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>5 0.10323945 <a title="37-tfidf-5" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>6 0.094231792 <a title="37-tfidf-6" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>7 0.082839541 <a title="37-tfidf-7" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>8 0.08281979 <a title="37-tfidf-8" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>9 0.080532074 <a title="37-tfidf-9" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>10 0.078099512 <a title="37-tfidf-10" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>11 0.076011173 <a title="37-tfidf-11" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>12 0.072150543 <a title="37-tfidf-12" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>13 0.069154829 <a title="37-tfidf-13" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>14 0.066865154 <a title="37-tfidf-14" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>15 0.060636245 <a title="37-tfidf-15" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>16 0.059623905 <a title="37-tfidf-16" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>17 0.05453483 <a title="37-tfidf-17" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>18 0.054218479 <a title="37-tfidf-18" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>19 0.053597622 <a title="37-tfidf-19" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>20 0.053093322 <a title="37-tfidf-20" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, 0.134), (2, -0.012), (3, -0.006), (4, 0.001), (5, 0.025), (6, 0.001), (7, -0.015), (8, 0.02), (9, 0.051), (10, 0.011), (11, 0.007), (12, -0.0), (13, 0.027), (14, 0.073), (15, -0.031), (16, -0.075), (17, 0.032), (18, 0.077), (19, 0.005), (20, -0.076), (21, 0.067), (22, -0.028), (23, 0.052), (24, 0.079), (25, 0.0), (26, -0.055), (27, -0.017), (28, 0.009), (29, -0.011), (30, 0.026), (31, 0.053), (32, -0.035), (33, -0.074), (34, 0.031), (35, -0.011), (36, 0.02), (37, -0.003), (38, 0.017), (39, -0.061), (40, -0.062), (41, 0.045), (42, 0.089), (43, -0.045), (44, 0.09), (45, -0.112), (46, -0.055), (47, 0.032), (48, -0.037), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88971388 <a title="37-lsi-1" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>2 0.73383909 <a title="37-lsi-2" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>3 0.71222043 <a title="37-lsi-3" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>4 0.66423607 <a title="37-lsi-4" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>5 0.62387615 <a title="37-lsi-5" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>6 0.58425122 <a title="37-lsi-6" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>7 0.5587064 <a title="37-lsi-7" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>8 0.55583006 <a title="37-lsi-8" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>9 0.48159078 <a title="37-lsi-9" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>10 0.48104006 <a title="37-lsi-10" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>11 0.46927404 <a title="37-lsi-11" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>12 0.44955975 <a title="37-lsi-12" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>13 0.40827253 <a title="37-lsi-13" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>14 0.34662944 <a title="37-lsi-14" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>15 0.33238891 <a title="37-lsi-15" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>16 0.33216664 <a title="37-lsi-16" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>17 0.32120281 <a title="37-lsi-17" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>18 0.31771317 <a title="37-lsi-18" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>19 0.30922139 <a title="37-lsi-19" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>20 0.30705014 <a title="37-lsi-20" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.388), (7, 0.066), (9, 0.013), (30, 0.031), (32, 0.149), (34, 0.055), (45, 0.065), (68, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.6613431 <a title="37-lda-1" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>2 0.64176089 <a title="37-lda-2" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>3 0.60007375 <a title="37-lda-3" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>4 0.52913308 <a title="37-lda-4" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>5 0.49930412 <a title="37-lda-5" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>6 0.46273935 <a title="37-lda-6" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>7 0.46217269 <a title="37-lda-7" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>8 0.46126038 <a title="37-lda-8" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>9 0.46055186 <a title="37-lda-9" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>10 0.46048486 <a title="37-lda-10" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>11 0.45991337 <a title="37-lda-11" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>12 0.45927963 <a title="37-lda-12" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>13 0.45926046 <a title="37-lda-13" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>14 0.45924875 <a title="37-lda-14" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>15 0.45909327 <a title="37-lda-15" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>16 0.45851332 <a title="37-lda-16" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>17 0.45837554 <a title="37-lda-17" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>18 0.45805073 <a title="37-lda-18" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>19 0.45771164 <a title="37-lda-19" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>20 0.45763236 <a title="37-lda-20" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<br/><br/><br/></body>
</html>
