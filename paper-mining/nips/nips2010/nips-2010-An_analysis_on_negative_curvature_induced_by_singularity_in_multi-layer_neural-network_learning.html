<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-31" href="#">nips2010-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</h1>
<br/><p>Source: <a title="nips-2010-31-pdf" href="http://papers.nips.cc/paper/4046-an-analysis-on-negative-curvature-induced-by-singularity-in-multi-layer-neural-network-learning.pdf">pdf</a></p><p>Author: Eiji Mizutani, Stuart Dreyfus</p><p>Abstract: In the neural-network parameter space, an attractive ﬁeld is likely to be induced by singularities. In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efﬁcient methods have been previously developed that avoided plateaus. 1</p><p>Reference: <a title="nips-2010-31-reference" href="../nips2010_reference/nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An analysis on negative curvature induced by singularity in multi-layer neural-network learning  Eiji Mizutani Department of Industrial Management Taiwan Univ. [sent-1, score-0.421]
</p><p>2 In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). [sent-8, score-0.178]
</p><p>3 Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. [sent-10, score-0.623]
</p><p>4 For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. [sent-11, score-0.486]
</p><p>5 1  Introduction  Consider a general two-hidden-layer multilayer perceptron (MLP) having a single (terminal) output, H nodes at the second hidden layer (next to the terminal layer), I nodes at the ﬁrst hidden layer, and J nodes at the input layer; hence, a J-I-H-1 MLP. [sent-13, score-0.592]
</p><p>6 y = f (θ; x) = φ hT p = φ (1) + + j=0 pj φ(z+vj ) j=0 pj hj = φ  Here, ﬁctitious outputs x0 = z0 = h0 = 1 are included in the output vectors with subscript “+” for thresholds p0 , v0,j , and w0,k ; pj (j = 1, . [sent-17, score-0.883]
</p><p>7 (2) For parameter optimization, one may attempt to minimize the squared error over m data m m 1 1 1 2 2 (3) E(θ) = {f (θ; xd )−td } = rd (θ ) = rT r, 2 2 2 d=1 d=1 where td is a desired output on datum d; each residual rd a smooth function from n to ; and r an m-vector of residuals. [sent-34, score-0.224]
</p><p>8 The gradient and Hessian of E can be expressed as below E(θ) =  m X  rd rd = JT r, and  2  E(θ) =  m X d=1  d=1  T rd rd +  m X d=1  rd  2  rd ≡ JT J+S,  where J ≡ r, an m×n Jacobian matrix of r, and the dth row of J is denoted by 1  T rd . [sent-36, score-0.493]
</p><p>9 Since JT J is positive (semi)deﬁnite, natural gradient learning has no chance to exploit negative curvature. [sent-42, score-0.108]
</p><p>10 Learning behaviors of layered networks may be attributable to singularities [3, 2, 4]. [sent-44, score-0.12]
</p><p>11 Singularities have been well discussed in the nonlinear least squares literature also: For instance, Jennrich & Sampson (pp. [sent-45, score-0.079]
</p><p>12 (5) If the target data follow the path of a single exponential then the two hidden parameters, v 1 and v2 , become identical (i. [sent-48, score-0.163]
</p><p>13 This is a typical over-realizable scenario, in which the true teacher lies at the singularity (see [6] for details about 1-2-1 MLP-learning). [sent-52, score-0.147]
</p><p>14 Those drawbacks of the Gauss-Newton-type methods indicate that negative curvature often arises in MLP-learning when JT J is singular (i. [sent-58, score-0.36]
</p><p>15 , in a rank-deﬁcient nonlinear least squares problem), and/or when S is more dominant than J T J. [sent-60, score-0.079]
</p><p>16 We thus verify this fact mathematically, and then discuss how exploiting negative curvature is a good way to escape from singularity plateaus, thereby enhancing the learning capacity. [sent-61, score-0.463]
</p><p>17 2  Negative curvature induced by singularity  In rank-deﬁcient nonlinear least squares problems, where J ≡ r is rank deﬁcient, negative curvature often arises. [sent-62, score-0.788]
</p><p>18 This is true with an arbitrary MLP model, but to make our analysis concrete, P we consider a single terminal linear-output two-hidden-layer MLP: f (θ; x) = H pj hj in Eq. [sent-63, score-0.441]
</p><p>19 j=0 Then, the n weights separate into linear p and non-linear v and w. [sent-65, score-0.079]
</p><p>20 1  An existence of the 4 × 4 indeﬁnite Hessian block H in  2  E  In the posed two-hidden-layer MLP-learning, as indicated after Eq. [sent-69, score-0.128]
</p><p>21 (1), the n weights are organized ˆ ˜ as θ T ≡ pT |vT |wT . [sent-70, score-0.079]
</p><p>22 Now, we pay attention to two particular hidden nodes j and k at the second hidden layer. [sent-71, score-0.301]
</p><p>23 The weights connecting to those two nodes are pj , pk , vj , and vk ; they are arranged in the following manner: ˆ ˜ θ T = p0 , p1 , . [sent-72, score-0.758]
</p><p>24 , | wT , (6) where vi,k is a weight from node i at the ﬁrst hidden layer to node k at the second hidden layer. [sent-101, score-0.448]
</p><p>25 (4), is given as below using the output vector z+ (including z0 = 1) at the ﬁrst hidden layer ˆ ˜ uT ≡ r T = . [sent-103, score-0.198]
</p><p>26 , (7) + + where only four entries are shown that are associated with four weights: pj , pk , v0,j , and v0,k . [sent-118, score-0.564]
</p><p>27 The locations of those four weights in the n-vector θ are denoted by l1 , l2 , l3 , and l4 , respectively, where l1 ≡ j +1, l2 ≡ k+1, l3 ≡ (I +1)(j −1)+1, l4 ≡ (I +1)(k−1)+1. [sent-119, score-0.121]
</p><p>28 (8) Given J, we interchange columns 1 and l1 ; then, do columns 2 and l2 ; then columns 3 and l3 ; and ﬁnally columns 4 and l4 ; this interchanging procedure moves those four columns to the ﬁrst four. [sent-120, score-0.263]
</p><p>29 We then apply the above interchanging procedure to both rows and columns of 2 E appropriately, which can be readily accomplished by PT 2 E P, where four permutation matrices Pi (i = 1, . [sent-122, score-0.111]
</p><p>30 As a result, H, the 4-by-4 Hessian block (at the upper-left corner) of the ﬁrst four leading rows and columns of PT 2 E P has the following structure: 2  6 H =6 4  4×4  (hj )2  hj hk (hk )2  Symmetric  3 2 hj φj (. [sent-126, score-0.467]
</p><p>31 )pk r  (9)  The posed Hessian block H is associated with a vector of the four weights [pj , pk , v0,j , v0,k ]T . [sent-139, score-0.447]
</p><p>32 Obviously, no matter how many data are accumu+ lated, two columns hj and hk of J in Eq. [sent-142, score-0.237]
</p><p>33 (4) are identical; therefore, J is rank deﬁcient; hence, JT J is singular. [sent-143, score-0.091]
</p><p>34 The posed singularity gives rise to negative curvature because the above 4-by-4 dense Hessian block is almost always indeﬁnite (so is 2 E of size n × n) to be proved next. [sent-144, score-0.549]
</p><p>35 2  Case 1: vj = vk ≡ v; hence, hj = hk ≡ h = φ(zT v), and pj = pk +  Given a set of m (training) data, the gradient vector E and the Hessian matrix 2 E in Eq. [sent-146, score-0.825]
</p><p>36 We then apply the aforementioned orthogonal matrix P to them as PT E and PT 2 EP, yielding the gradient vector g of length 4 and the 4-by-4 Hessian block H [see Eq. [sent-148, score-0.135]
</p><p>37 , m); hence, hd is the hidden-node output on datum d (but not the dth hidden-node output) common to both nodes j and k due to v j = vk = v. [sent-153, score-0.224]
</p><p>38 2 The eigenvalues of the 2-by-2 block at the lower-right corner are obtainable by  √  ˛ h i˛ ˛ e ˛ ˛λI − 0 τ ˛ = λ(λ − τ ) − e2 = λ2 − τ λ − e2 = 0, e  1 which yields 2 (τ ± τ 2 + 4e2 ), the “sign-different” eigenvalues as long as e = 0 holds. [sent-157, score-0.212]
</p><p>39 (13), JT J is positive semi-deﬁnite (singular of rank 2 even when m ≥ 2), and S has an indeﬁnite structure. [sent-165, score-0.091]
</p><p>40 When e = 0 (hence, E = 0), we can prove below that there always exists negative curvature (i. [sent-166, score-0.274]
</p><p>41 ” Its block of size 2 × 2 at the lower-right corner has the sign-different eigenvalues determined by λ2 −dλ−e2 = 0. [sent-173, score-0.152]
</p><p>42 2 QED 2 Now, we investigate stationary points, where the n-length gradient vector E = 0; hence, g = 0 in Eq. [sent-174, score-0.119]
</p><p>43 In Case (b), S becomes a diagonal matrix, and the above TT H T shows that H is of (at most) rank 3 (when d = 0); hence, H becomes singular. [sent-177, score-0.091]
</p><p>44 , Case (a)], then the stationary point θ ∗ is a saddle. [sent-180, score-0.088]
</p><p>45 (13) has a negative eigenvalue; hence, the entire Hessian matrix 2 E of size n × n is indeﬁnite 2 QED 2 Theorem 4 is a special case of Case (b). [sent-184, score-0.077]
</p><p>46 If d = pD > 0, then H becomes positive semi-deﬁnite; however, we could alter the eigen-spectrum of H by changing linear parameters p in conjunction with scalar ζ for pj = 2ζp and pk = 2(1−ζ)p such that pj +pk = 2p with no change in E and E = 0 held ﬁxed (to be conﬁrmed in simulation; see Fig. [sent-185, score-0.735]
</p><p>47 (11)] and v1 = v2 (≡ v) with E = 0, for which p = 0 and e = 0 (hence, S is diagonal), then choosing scalar ζ appropriately for pj = 2ζp and pk = 2(1−ζ)p can render H and thus 2 E indeﬁnite. [sent-187, score-0.483]
</p><p>48 Obviously, given p, C, and D, there exists ζ such that the quadratic function value becomes negative (see later Fig. [sent-190, score-0.077]
</p><p>49 This implies that adjusting ζ can produce a negative diagonal entry of H; hence, indeﬁnite. [sent-192, score-0.077]
</p><p>50 Then, again by Cauchy’s interlace theorem, so is 2 E . [sent-193, score-0.078]
</p><p>51 Then, adding a node at the second hidden layer can increase learning capacity in the sense that E can be further reduced. [sent-202, score-0.26]
</p><p>52 Sketch of Proof: Choose a node j among H hidden nodes, and add a hidden node (call node k) by duplicating the hidden weights by vk = vj with pk = 0; hence, totally n ≡ n+(I +2) weights. [sent-203, score-1.029]
</p><p>53 Then, by the interlace theorem, new 2 E of size n × n becomes indeﬁnite. [sent-206, score-0.078]
</p><p>54 Furthermore, even if we know in advance the minimum number of hidden nodes, Hmin , for a certain task, we may not be able to ﬁnd a local-minimum point of an MLP with one less hidden nodes, H min −1. [sent-208, score-0.252]
</p><p>55 weights), any local minimum point may not be found by optimizing a 2-1-1 MLP (ﬁve weights), since the hidden weights tend to be divergent (or weight-∞ attractors). [sent-227, score-0.205]
</p><p>56 We solved E = 0 to ﬁnd all stationary points of a two-weight 1-1-1 MLP with a logistic hidden1 node function φ(x) ≡ 1+e−x , and found p∗ ≈ 1. [sent-229, score-0.15]
</p><p>57 There was another type of attractive points, where φ is driven to saturation due to a large hidden weight v in magnitude (weight-∞ attractors). [sent-237, score-0.163]
</p><p>58 Hence, the line is a collection of degenerate stationary points. [sent-260, score-0.088]
</p><p>59 In this way, singularities may be closely related to ﬂat regions, where any updates of 5  parameters do not change the objective function value. [sent-261, score-0.12]
</p><p>60 Back to MLP-learning, Blum [10] describes a different linear manifold of stationary points (see Sec. [sent-262, score-0.088]
</p><p>61 If θ ∗ = [1, 1, 0, 0, 0]T , then E(θ ∗ ) = 0 with the indeﬁnite Hessian 2 E (hence, θ ∗ a saddle point) below, in which all diagonal entries of S are zero due to D = 0: 2  Here,  ∗  E(θ ) =  2  0 0 6 0 0 6 0 0 4 0 0 0 0  0 0 0 0  3 0 0 0 0 7 7 0 5 0 0  with  ≡  m X  xd r d . [sent-269, score-0.165]
</p><p>62 When H = O, a 4 × 4 block of zeros, 2 E would be indeﬁnite (again by the interlace theorem) as long as non-zero off-diagonal entries exist in 2 E , as in Example 3 above. [sent-272, score-0.169]
</p><p>63 Typical is an aforementioned weight-∞ case, where the sigmoid-shaped hidden-node functions are driven to saturation limits due to very large hidden weights. [sent-276, score-0.126]
</p><p>64 Then, only part of JT J associated with linear weights p appear in 2 E since S = O even if residuals are still large. [sent-277, score-0.115]
</p><p>65 It should be noted that a regularization scheme to penalize large weights is quite orthogonal to our scheme to exploit negative curvature. [sent-279, score-0.199]
</p><p>66 (6) reduces to T T θ T ≡ [pT |vT ] = [pT |v1 |v2 ] = [p0 , p1 , p2 |v0,1 , v1,1 , v2,1 |v0,2 , v1,2 , v2,2 ], where vj is a (hidden) weight vector connecting to the jth hidden node. [sent-282, score-0.276]
</p><p>67 Here, all weights are non1 linear since both hidden and ﬁnal outputs are produced by sigmoidal logistic function φ(x) ≡ 1+e−x . [sent-283, score-0.247]
</p><p>68 1  Insensitivity to the initial weights in the singular XOR problem  The world-renowned XOR problem (involving only four data of binary values: ON and off) with a standard nine-weight 2-2-1 MLP is inevitably a singular problem because the Gauss-Newton Hessian JT J in Eq. [sent-285, score-0.293]
</p><p>69 (4) is always singular (at most rank 4), whereas S tends to be of (nearly) full rank; so does 2 E (cf. [sent-286, score-0.207]
</p><p>70 This implies that singularity in terms of JT J is everywhere in the posed neuro-manifolds. [sent-288, score-0.214]
</p><p>71 , see [13]) that the origin (p = 0 and v = 0) is a singular saddle point, where E = 0 and 2 E = JT J with only one positive eigenvalue and eight zeros. [sent-291, score-0.267]
</p><p>72 An interesting observation is that there always exists a descending path to the solution from any initial point θ init as long as θ init is randomly generated in a small range; i. [sent-292, score-0.542]
</p><p>73 That is, ﬁrst go directly down towards the origin from θ init , and then move in a descent direction of negative curvature so as to escape from that singular saddle point. [sent-295, score-0.819]
</p><p>74 In this way, the 2-2-1 MLP can develop insensitivity to initial weights, always solving the posed XOR problem. [sent-296, score-0.098]
</p><p>75 6  Blum’s Claim (page 539 [10]): There are many stationary points that are not absolute minima. [sent-312, score-0.088]
</p><p>76 This is certainly a limitation of the second-order Hessian analysis, and thus more efforts using higher-order derivatives were needed to disprove Blum’s claim (see [14, 15]), and it turned out that Blum’s line is a collection of singular saddle points. [sent-322, score-0.286]
</p><p>77 The 5-by-5 Hessian matrix 2  4A 4A 2wA 4A 2wA 6 4A 6 2 2 E =6 2wA 2wA w2 A | {z } 6 4 wA wA w A 5×5 2 2 wA wA w A 2  E at a stationary point θ ∗ = [L, w, 0, 0, 0] is given by 3 wA wA 8 wA wA >A ≡ {φ (L + w)}2 7 < 2 2 7 w w 7 with A A “ ” 2 2 7 2 > w2 :S ≡ φ (L+w) φ(L+w)− off . [sent-324, score-0.088]
</p><p>78 A(w + 8) + 2k ± [A(w  We thus obtain two non-zero eigenvalues of  (18)  2  (19)  Now, the smaller eigenvalue can be rendered negative when the following condition holds: “ ” 2 4k − w 2 A < 0 ⇐⇒ A + S = {φ (L + w)} + φ (L + w) φ(L + w) − off < 0. [sent-326, score-0.183]
</p><p>79 Because 2 E is indeﬁnite, the posed stationary point is a saddle point with E = 1 (ON − off)2 (≈ 1. [sent-331, score-0.29]
</p><p>80 Both are singular problems, because rank(JT J) ≤ 5; yet, both S and 2 E tend to be of full rank; therefore, the 9×9 Hessian 2 E tends to be indeﬁnite (see Theorems 1 and 2). [sent-346, score-0.116]
</p><p>81 81 in [17], a conﬁguration of two separation lines, like two solid lines given by θ init in Fig. [sent-348, score-0.297]
</p><p>82 But its failure does not imply that there is no descending way out of the two-solid-line conﬁguration given by θ init because the convergence of the steepestdescent method to a (local) minimizer can be guaranteed by examining negative curvature (e. [sent-351, score-0.616]
</p><p>83 In the ﬁve-data case, the steepest-descent method moves θ init to a point, where the weights become relatively large; the gradient vector E ≈ 0; the Hessian 2 E is positive semi-deﬁnite; and Eq. [sent-356, score-0.36]
</p><p>84 We can ﬁnd such a point analytically by a linear-equation solving: Given θ init in Fig. [sent-358, score-0.25]
</p><p>85 5, 1, 1]T, where the norm of p∗ becomes relatively 0 1 2 large O(102 ), gives the zero gradient vector, the positive semi-deﬁnite Hessian of rank 5, and E = 1 (ON − off)2 , as mentioned above. [sent-368, score-0.122]
</p><p>86 5  Figure 3: Gori & Tesi’s two-class pattern classiﬁcation problems (left) three-data case; (right) ﬁvedata case; and (middle) a 2-2-1 MLP with initial weight values θ init ≡ [0, 1, −1; −1. [sent-385, score-0.25]
</p><p>87 Its corresponding initial conﬁguration gives two solid lines of net-inputs (to two hidden nodes) in the input space, where “◦” stands for two ON-data (1,0), (0,1), whereas “×” for one off-data (0. [sent-388, score-0.173]
</p><p>88 A solution to both problems may be given by the two dotted lines with θ sol ≡ [0, 1, −1; −0. [sent-393, score-0.097]
</p><p>89 2  E indeﬁnite of full rank (since S is dominant): rank(S) = rank(  2  E) = 9 with rank(JT J) = 4; this  suggests a descend direction (other than the steepest descent) to follow from θ init to a solution θ sol . [sent-396, score-0.47]
</p><p>90 Intriguingly enough, it is easy to conﬁrm for the three-data case that the posed “descent” direction of negative curvature ∆θ is orthogonal to − E, the steepest-descent direction. [sent-399, score-0.416]
</p><p>91 Claim: Line search from θ init to θ sol monotonically decreases the squared error E (θ init + η∆θ) as the step size η (scalar) changes from 0 to 1; hence, no plateau. [sent-400, score-0.597]
</p><p>92 ) Using target values ON=1 and off=0, let q(η) ≡ E(θ init +η∆θ)−E(θ init ). [sent-402, score-0.537]
</p><p>93 2  4  Summary  In a general setting, we have proved that negative curvature can arise in MLP-learning. [sent-431, score-0.274]
</p><p>94 To make it analytically tractable, we intentionally used noise-free small data sets but on “noisy” data, the conditions for Theorems 1 and 2 most likely hold in the vicinity of singularity regions; it then follows that the Hessian 2 E tends to be indeﬁnite (of nearly full rank). [sent-432, score-0.221]
</p><p>95 Our numerical results conﬁrm that the negative-curvature information is of immense value for escaping from singularity plateaus including some problems where no method was developed to alleviate plateaus. [sent-433, score-0.198]
</p><p>96 In simulation, we employed the second-order stagewise backpropagation [12] (that can evaluate 2 E and JT J at the essentially same cost; see proof therein) to obtain 2 E explicitly and its eigen-directions so as to exploit negative curvature. [sent-434, score-0.111]
</p><p>97 This approach is suitable for up to medium-scale problems, for which our analysis suggests using existing trust-region globalization strategies whose theory has thrived on negative curvature including indeﬁnite dogleg [19]. [sent-435, score-0.274]
</p><p>98 Second-order stagewise backpropagation for Hessian-matrix analyses and investigation of negative curvature. [sent-509, score-0.111]
</p><p>99 The error surface of the 2-2-1 XOR network: The ﬁnite stationary points. [sent-519, score-0.088]
</p><p>100 A comment on a paper of Blum: Blum’s local minima are saddle points. [sent-532, score-0.19]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hessian', 0.306), ('mlp', 0.298), ('inde', 0.274), ('jt', 0.267), ('pj', 0.252), ('init', 0.25), ('pk', 0.198), ('curvature', 0.197), ('xor', 0.154), ('singularity', 0.147), ('saddle', 0.135), ('blum', 0.131), ('hj', 0.127), ('hidden', 0.126), ('singularities', 0.12), ('gori', 0.097), ('sol', 0.097), ('tesi', 0.097), ('rank', 0.091), ('pt', 0.091), ('stationary', 0.088), ('wa', 0.088), ('singular', 0.086), ('vj', 0.082), ('weights', 0.079), ('eig', 0.078), ('interlace', 0.078), ('negative', 0.077), ('zt', 0.075), ('layer', 0.072), ('hk', 0.072), ('amari', 0.07), ('posed', 0.067), ('rd', 0.066), ('vk', 0.063), ('terminal', 0.062), ('node', 0.062), ('block', 0.061), ('eigenvalues', 0.06), ('datum', 0.059), ('multilayer', 0.059), ('boers', 0.058), ('dreyfus', 0.058), ('mizutani', 0.058), ('ozeki', 0.058), ('qed', 0.058), ('minima', 0.055), ('hd', 0.053), ('nite', 0.051), ('plateaus', 0.051), ('minimizer', 0.05), ('nodes', 0.049), ('hence', 0.048), ('solid', 0.047), ('tt', 0.047), ('eigenvalue', 0.046), ('krylov', 0.044), ('vicinity', 0.044), ('orthogonal', 0.043), ('totally', 0.043), ('nonlinear', 0.043), ('four', 0.042), ('descending', 0.042), ('sigmoidal', 0.042), ('escape', 0.042), ('cousseau', 0.039), ('jennrich', 0.039), ('leiden', 0.039), ('parlett', 0.039), ('columns', 0.038), ('attractive', 0.037), ('target', 0.037), ('squares', 0.036), ('pe', 0.036), ('residuals', 0.036), ('connecting', 0.035), ('pm', 0.035), ('dv', 0.035), ('industrial', 0.035), ('net', 0.034), ('stagewise', 0.034), ('attractors', 0.034), ('dennis', 0.034), ('eiji', 0.034), ('lanczos', 0.034), ('sampson', 0.034), ('theorem', 0.034), ('gn', 0.034), ('scalar', 0.033), ('residual', 0.033), ('jth', 0.033), ('certainly', 0.033), ('direction', 0.032), ('claim', 0.032), ('insensitivity', 0.031), ('interchanging', 0.031), ('gradient', 0.031), ('corner', 0.031), ('tends', 0.03), ('entries', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="31-tfidf-1" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>Author: Eiji Mizutani, Stuart Dreyfus</p><p>Abstract: In the neural-network parameter space, an attractive ﬁeld is likely to be induced by singularities. In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efﬁcient methods have been previously developed that avoided plateaus. 1</p><p>2 0.135299 <a title="31-tfidf-2" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>3 0.10931122 <a title="31-tfidf-3" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>Author: Serhat Bucak, Rong Jin, Anil K. Jain</p><p>Abstract: Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity of kernel learning in computer vision problems. In this work, we develop an efﬁcient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under consideration share the same combination of kernel functions, and the objective is to ﬁnd the optimal kernel combination that beneﬁts all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge frequently encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis √ shows that the complexity of our algorithm is O(m1/3 lnm), where m is the number of classes. Empirical studies with object recognition show that while achieving similar classiﬁcation accuracy, the proposed method is signiﬁcantly more efﬁcient than the state-of-the-art algorithms for ML-MKL. 1</p><p>4 0.087272897 <a title="31-tfidf-4" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>Author: Hariharan Narayanan, Sanjoy Mitter</p><p>Abstract: The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of ﬁtting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume log 1 and curvature is unavoidable. Whether the known lower bound of O( k + 2 δ ) 2 for the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of 2 log 1 log4 k log 1 O( k2 + 2 δ ) to O k min k, 2 + 2 δ . Based on these results, we 2 devise a simple algorithm for k−means and another that uses a family of convex programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional data, where the sample complexity is independent of the ambient dimension. 1</p><p>5 0.084517717 <a title="31-tfidf-5" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>Author: Sashank J. Reddi, Sunita Sarawagi, Sundar Vishwanathan</p><p>Abstract: We propose a new LP relaxation for obtaining the MAP assignment of a binary MRF with pairwise potentials. Our relaxation is derived from reducing the MAP assignment problem to an instance of a recently proposed Bipartite Multi-cut problem where the LP relaxation is guaranteed to provide an O(log k) approximation where k is the number of vertices adjacent to non-submodular edges in the MRF. We then propose a combinatorial algorithm to efﬁciently solve the LP and also provide a lower bound by concurrently solving its dual to within an approximation. The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art message passing algorithm of [1] that tightens the local marginal polytope with third-order marginal constraints. 1</p><p>6 0.071982712 <a title="31-tfidf-6" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>7 0.066035859 <a title="31-tfidf-7" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>8 0.061358299 <a title="31-tfidf-8" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>9 0.06081643 <a title="31-tfidf-9" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>10 0.058832217 <a title="31-tfidf-10" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>11 0.056987211 <a title="31-tfidf-11" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>12 0.056634456 <a title="31-tfidf-12" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>13 0.056535222 <a title="31-tfidf-13" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>14 0.055827621 <a title="31-tfidf-14" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>15 0.055544101 <a title="31-tfidf-15" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>16 0.055304639 <a title="31-tfidf-16" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>17 0.055176824 <a title="31-tfidf-17" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>18 0.054061595 <a title="31-tfidf-18" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>19 0.053648256 <a title="31-tfidf-19" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>20 0.0532385 <a title="31-tfidf-20" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.025), (2, 0.024), (3, 0.016), (4, 0.04), (5, -0.029), (6, 0.052), (7, 0.025), (8, -0.023), (9, 0.044), (10, -0.003), (11, -0.092), (12, 0.06), (13, -0.015), (14, 0.012), (15, -0.016), (16, -0.009), (17, -0.044), (18, -0.114), (19, -0.01), (20, 0.073), (21, 0.128), (22, -0.044), (23, 0.035), (24, 0.001), (25, 0.065), (26, -0.058), (27, 0.006), (28, 0.047), (29, -0.049), (30, -0.063), (31, -0.03), (32, 0.11), (33, 0.061), (34, -0.017), (35, -0.016), (36, 0.105), (37, 0.08), (38, -0.111), (39, -0.104), (40, 0.041), (41, -0.081), (42, -0.06), (43, 0.096), (44, -0.018), (45, -0.032), (46, 0.072), (47, -0.084), (48, 0.008), (49, -0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93756384 <a title="31-lsi-1" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>Author: Eiji Mizutani, Stuart Dreyfus</p><p>Abstract: In the neural-network parameter space, an attractive ﬁeld is likely to be induced by singularities. In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efﬁcient methods have been previously developed that avoided plateaus. 1</p><p>2 0.59501034 <a title="31-lsi-2" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>3 0.51467353 <a title="31-lsi-3" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>Author: Prateek Jain, Raghu Meka, Inderjit S. Dhillon</p><p>Abstract: Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under afﬁne constraints (ARMP) and show that SVP recovers the minimum rank solution for afﬁne constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of lowrank matrix completion, for which the deﬁning afﬁne constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is signiﬁcantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem. 1</p><p>4 0.49076506 <a title="31-lsi-4" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>5 0.47541106 <a title="31-lsi-5" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>6 0.42708555 <a title="31-lsi-6" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>7 0.42016011 <a title="31-lsi-7" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>8 0.40325877 <a title="31-lsi-8" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>9 0.40228415 <a title="31-lsi-9" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>10 0.39961869 <a title="31-lsi-10" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>11 0.39864725 <a title="31-lsi-11" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>12 0.39681545 <a title="31-lsi-12" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>13 0.39047843 <a title="31-lsi-13" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>14 0.38608351 <a title="31-lsi-14" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>15 0.38528603 <a title="31-lsi-15" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>16 0.37732339 <a title="31-lsi-16" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>17 0.37481964 <a title="31-lsi-17" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>18 0.37174043 <a title="31-lsi-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.35945094 <a title="31-lsi-19" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>20 0.35911828 <a title="31-lsi-20" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.058), (17, 0.013), (27, 0.038), (30, 0.072), (35, 0.023), (40, 0.25), (45, 0.168), (50, 0.039), (52, 0.08), (60, 0.063), (77, 0.057), (78, 0.018), (90, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78466326 <a title="31-lda-1" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>Author: Eiji Mizutani, Stuart Dreyfus</p><p>Abstract: In the neural-network parameter space, an attractive ﬁeld is likely to be induced by singularities. In such a singularity region, ﬁrst-order gradient learning typically causes a long plateau with very little change in the objective function value E (hence, a ﬂat region). Therefore, it may be confused with “attractive” local minima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers) that allow us to conﬁrm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau. Even for those small problems, no efﬁcient methods have been previously developed that avoided plateaus. 1</p><p>2 0.77730888 <a title="31-lda-2" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>Author: Guy Isely, Christopher Hillar, Fritz Sommer</p><p>Abstract: A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efﬁcient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through ﬁber bottlenecks are able to form coherent response properties. 1</p><p>3 0.68177998 <a title="31-lda-3" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>Author: Tamir Hazan, Joseph Keshet, David A. McAllester</p><p>Abstract: In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classiﬁcation one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptronlike learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem. 1</p><p>4 0.65106136 <a title="31-lda-4" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>5 0.64932555 <a title="31-lda-5" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>Author: Mohsen Bayati, José Pereira, Andrea Montanari</p><p>Abstract: We consider the problem of learning a coefﬁcient vector x0 ∈ RN from noisy linear observation y = Ax0 + w ∈ Rn . In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator x. In this case, a popular approach consists in solving an ℓ1 -penalized least squares problem known as the LASSO or Basis Pursuit DeNoising (BPDN). For sequences of matrices A of increasing dimensions, with independent gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the ﬁrst rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efﬁcient algorithm, that is inspired from graphical models ideas. Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.</p><p>6 0.64859194 <a title="31-lda-6" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>7 0.64385271 <a title="31-lda-7" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>8 0.64042699 <a title="31-lda-8" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>9 0.6396271 <a title="31-lda-9" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>10 0.63947678 <a title="31-lda-10" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>11 0.63660568 <a title="31-lda-11" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>12 0.63600433 <a title="31-lda-12" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>13 0.63574719 <a title="31-lda-13" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>14 0.63457906 <a title="31-lda-14" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>15 0.63445419 <a title="31-lda-15" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>16 0.63300222 <a title="31-lda-16" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>17 0.63265955 <a title="31-lda-17" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>18 0.63234693 <a title="31-lda-18" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>19 0.62985986 <a title="31-lda-19" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>20 0.62945712 <a title="31-lda-20" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
