<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2010-Link Discovery using Graph Feature Tracking</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-162" href="#">nips2010-162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2010-Link Discovery using Graph Feature Tracking</h1>
<br/><p>Source: <a title="nips-2010-162-pdf" href="http://papers.nips.cc/paper/3911-link-discovery-using-graph-feature-tracking.pdf">pdf</a></p><p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>Reference: <a title="nips-2010-162-reference" href="../nips2010_reference/nips-2010-Link_Discovery_using_Graph_Feature_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. [sent-11, score-0.609]
</p><p>2 The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. [sent-12, score-0.798]
</p><p>3 The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. [sent-13, score-0.371]
</p><p>4 Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. [sent-14, score-0.493]
</p><p>5 We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. [sent-15, score-0.926]
</p><p>6 Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. [sent-16, score-0.189]
</p><p>7 Link prediction can also be seen as a special case of matrix completion where the goal is to estimate the missing entries of the adjacency matrix of the graph where the entries can be only ”0s” and ”1s”. [sent-19, score-1.353]
</p><p>8 Matrix completion became popular after the Netﬂix Challenge and has been extensively studied on both theoretical and algorithmic aspects [15]. [sent-20, score-0.284]
</p><p>9 In this paper we consider a special case of predicting the evolution of a graph, where we only predict the new edges given a ﬁxed set of vertices of an undirected graph by using the dynamics of the graph over time. [sent-21, score-1.254]
</p><p>10 Most of the existing methods in matrix completion assume that weights over the entries (i. [sent-22, score-0.502]
</p><p>11 Consider for instance the issue of link prediction in recommender systems. [sent-28, score-0.345]
</p><p>12 In that case, we consider a bipartite graph for which the vertices represent products and users, and the edges connect users with the products they have purchased in the past. [sent-29, score-0.722]
</p><p>13 The setup we consider in the present paper corresponds to 1  the binary case where we only observe purchase data, say the presence of a link in the graph, without any score or feedback on the product for a given user. [sent-30, score-0.31]
</p><p>14 Hence, we will deal here with the situation where the components of snapshots of the adjacency matrix only consist of ”1s” and missing values. [sent-31, score-0.51]
</p><p>15 Moreover, link prediction methods typically use only one snapshot of the graph’s adjacency matrix the most recent one - to predict its missing entries [9], or rely on latent variables providing semantic information for each vertex [11]. [sent-32, score-1.052]
</p><p>16 Since these methods do not use any information over time, they can be called static methods. [sent-33, score-0.189]
</p><p>17 However, information about how the links of the graph and its topological features have been evolving over time may also be useful to predict future links. [sent-35, score-0.739]
</p><p>18 In the example of recommender systems, knowing that a particular product has been purchased by increasingly more people in a short time window provides useful information about the type of the recommendations to be made in the next period. [sent-36, score-0.207]
</p><p>19 The main idea underlying our work lies in the observation that a few graph features can capture the dynamics of the graph evolution and provide information for predicting future links. [sent-37, score-1.007]
</p><p>20 The purpose of the paper is to present a procedure which exploits the dynamics of the evolution of the graph to ﬁnd unrevealed links in the graph. [sent-38, score-0.647]
</p><p>21 The main idea is to learn over time the evolution of well-chosen local features (at the level of the vertices) of the graph and then, use the predicted value of these features on the next time period to discover the missing links. [sent-39, score-0.816]
</p><p>22 Our approach is related to two theoretical streams of research: matrix completion and diffusion models. [sent-40, score-0.458]
</p><p>23 In the latter only the dynamics over time of the degree of a particular vertex of the graph are modeled - the diffusion of the product corresponding to that vertex for example [17, 14]. [sent-41, score-0.556]
</p><p>24 Beyond the large number of static matrix completion methods, only a few methods have been developed that combine static and dynamic information mainly using parametric methods – see [4] for a survey. [sent-42, score-0.844]
</p><p>25 For example, [13] embeds graph vertices on a latent space and use either a Markov model or a Gaussian one to track the position of the vertices in this space; [10] uses a probabilistic model of the time interval between the appearance of two edges or subgraphs to predict future edges or subgraphs. [sent-43, score-0.937]
</p><p>26 The setup of dynamic feature-based matrix completion is presented in Section 2. [sent-45, score-0.466]
</p><p>27 In Section 3, we develop a fast linearized algorithm for efﬁcient link prediction. [sent-46, score-0.337]
</p><p>28 2  Dynamic feature-based matrix completion  Setup. [sent-49, score-0.423]
</p><p>29 We consider a sequence of T undirected graphs with n vertices and n × n binary adjacency matrices At , t ∈ {1, 2, . [sent-50, score-0.464]
</p><p>30 , T } where for each t the edges of the graph are also contained in the graph at time t + 1. [sent-53, score-0.711]
</p><p>31 , T } the goal is to predict the edges of the graph that are most likely to appear at time T + 1, that is, the most likely non-zero elements of the binary adjacency matrix AT +1 . [sent-57, score-0.885]
</p><p>32 To this purpose we want to learn an n × n real-valued matrix S whose elements indicate how likely it is that there is a non-zero value at the corresponding position of matrix AT +1 . [sent-58, score-0.278]
</p><p>33 The edges that we predict to be the most likely ones at time T + 1 are the ones corresponding to the largest values in S. [sent-59, score-0.242]
</p><p>34 We assume that certain features of matrices At evolve over time smoothly. [sent-60, score-0.197]
</p><p>35 Such an assumption is necessary to allow learnability of the evolution of At over time. [sent-61, score-0.183]
</p><p>36 For simplicity we consider a linear feature map f : At → Ft where Ft is an n × k matrix of the form Ft = At Φ, with Φ an n × k matrix of features. [sent-62, score-0.366]
</p><p>37 We discuss an example of such features Φ and a way to predict FT +1 given past values of the feature map F1 , F2 , . [sent-64, score-0.315]
</p><p>38 , FT in Section 4 – but other features or prediction methods can be used in combination with the main part of the proposed approach. [sent-67, score-0.145]
</p><p>39 The procedure we propose for link prediction is based on the assumption that the dynamics of graph features also drive the discovery of the location of new links. [sent-70, score-0.799]
</p><p>40 Given the last adjacency matrix AT , a set of features Φ, and an estimate F of FT +1 based on the sequence 2  of adjacency matrices At , t ∈ {1, 2, . [sent-71, score-0.75]
</p><p>41 , T }, we want to ﬁnd a matrix S which fulﬁlls the following requirements: • S has low rank - this is a standard assumption in matrix completion problems [15]. [sent-74, score-0.642]
</p><p>42 • S is close to the last adjacency matrix AT - the distance between these two matrices will provide a proxy for the training error. [sent-75, score-0.441]
</p><p>43 For any matrix M , we denote by M F = Tr(M M ) , the Frobenius norm of M , with M being the transpose of M and the trace operator Tr(N ) computes the sum of the diagonal elements of the n square matrix N . [sent-77, score-0.315]
</p><p>44 We also deﬁne M ∗ = k=1 σk (M ) , the nuclear norm of a square matrix M of size n × n, where σk (M ) denotes the k-th largest singular value of M . [sent-78, score-0.363]
</p><p>45 We recall that a singular value of matrix M corresponds to the square root of an eigenvalue of M M ordered decreasingly. [sent-79, score-0.239]
</p><p>46 The proposed optimization problem for feature-based matrix completion is then: 1 1 S − AT 2 + ν SΦ − F 2 , (1) F F 2 2 and where τ and ν are positive regularization parameters. [sent-80, score-0.423]
</p><p>47 Each term of the functional L reﬂects the aforementioned requirements for the desired matrix S. [sent-81, score-0.215]
</p><p>48 In the case where ν = 0, we do not use information about the dynamics of the graph. [sent-82, score-0.103]
</p><p>49 The minimizer of L corresponds to the singular value thresholding approach developed in [2], which is therefore a special case of (1). [sent-83, score-0.15]
</p><p>50 Note that a key difference between link prediction and matrix completion is that in (1) the training error uses all entries of the adjacency matrix while in the case of matrix completion only the known entries (in our case the ”1s”) are used. [sent-84, score-1.658]
</p><p>51 with L(S, τ, ν) = τ S  ∗  +  An algorithm for link discovery  Solving (1) is computationally slow. [sent-87, score-0.273]
</p><p>52 Here, the functional L(S, τ, ν) is continuous and convex but not differentiable with respect to S. [sent-89, score-0.115]
</p><p>53 We propose to convert the minimization of the target functional L(S, τ, ν) into a tractable problem through the following steps: 1. [sent-90, score-0.118]
</p><p>54 Smoothing the nuclear norm - We recall the variational formulation of the nuclear norm S ∗ = maxZ { S, Z : σ1 (Z) ≤ 1}. [sent-97, score-0.248]
</p><p>55 Using the technique from [12], we can use a smooth approximation of the nuclear norm and replace g in the functional by a surrogate function gη with η > 0 being a smoothing parameter: η gη (S, τ ) = τ · max S, Z − Z 2 : σ1 (Z) ≤ 1 F Z 2 3. [sent-98, score-0.238]
</p><p>56 Alternating minimization - We propose to minimize the functional which is continuous, differentiable and convex: . [sent-99, score-0.157]
</p><p>57 We denote by mG (S) the minimizer of ˜ Gη,µ (S, S) with respect to S and mH (S) the minimizer of Hµ (S, S) with respect to S. [sent-104, score-0.1]
</p><p>58 We can now formulate an algorithm for the fast minimization of the functional Lη (S, S) inspired by the algorithm FALM in [5] (see Algorithm 1). [sent-105, score-0.154]
</p><p>59 Note that, in the alternating descent for the simultaneous minimization of the two functions Gη,µ and Hµ , we use an auxiliary matrix Zk . [sent-106, score-0.26]
</p><p>60 This matrix is a linear combination of the updates for S and S. [sent-107, score-0.139]
</p><p>61 Key formulas in the link prediction algorithm are those of the minimizers mG (S) and mH (S). [sent-109, score-0.357]
</p><p>62 It turns out that in our case, these minimizers have explicit expressions which can be derived when solving the ﬁrst-order optimality condition as Proposition 1 shows. [sent-110, score-0.107]
</p><p>63 do Sk ← mG (Zk ) and Sk ← mH (Sk ) 1 Wk ← (Sk + Sk ) 2 1 2 αk+1 ← (1 + 1 + 4αk ) 2 1 αk (Sk − Wk−1 ) − (Wk − Wk−1 ) Zk+1 ← Wk + αk+1 end for  ˆ Proposition 1 Let S = S − µ h(S) and the singular value decomposition S = U Diag(σ)V . [sent-114, score-0.1]
</p><p>64 We also consider the singular value decomposition of S denoted by S = U Diag(ηλ)V . [sent-115, score-0.1]
</p><p>65 With our notations, we can easily derive here: µ = min η/τ, 1/(1 + νσ1 (Φ)) , where σ1 (Φ) is the largest singular value of Φ. [sent-128, score-0.1]
</p><p>66 4  Learning the graph features  As discussed above one can use various features Φ and methods to predict the n × k matrix FT +1 given past values of the feature map F1 , F2 , . [sent-129, score-0.815]
</p><p>67 In particular, we use as features Φ the ﬁrst k eigenvectors of the adjacency matrix AT . [sent-134, score-0.448]
</p><p>68 Note that (:,1:k) AT Φ = Ω(:,1:k) and that Ω(:,1:k) is the most informative n × k matrix for the reconstruction of AT . [sent-137, score-0.139]
</p><p>69 , T } which describes the evolution of the j-th feature over the n vertices of the graph. [sent-149, score-0.347]
</p><p>70 We now describe the procedure for learning the evolution of this j-th graph feature over time: 1. [sent-150, score-0.511]
</p><p>71 Fix an integer m < T to learn a map between m past values (At−m Φj , . [sent-151, score-0.095]
</p><p>72 , k}, we obtain the estimate F for the matrix FT +1 used in (1). [sent-167, score-0.139]
</p><p>73 Static matrix completion corresponding to ν = 0 in (1). [sent-175, score-0.423]
</p><p>74 The Katz algorithm [8] considered as one of the best static link prediction methods. [sent-177, score-0.478]
</p><p>75 The Preferential Attachment method [1] for which the score (”likelihood”) of an edge {u, v} is du dv where du and dv are the degrees of u and v. [sent-179, score-0.217]
</p><p>76 We ﬁrst generate a sequence of T matrices Q(t) of size n × r whose entries Qi,j (t) are increasing over time as a sigmoid function :    1 t − µi,j  Qi,j (t) = 1 + erf  2 2σ 2 i,j  where µi,j ∈ [0; T ], σi,j ∈ [0; T /3] are picked uniformly for each (i, j). [sent-182, score-0.235]
</p><p>77 These matrices provide a synthetic model for the evolution of the graph over time. [sent-183, score-0.537]
</p><p>78 We then add noise to the time dynamics as follows. [sent-184, score-0.141]
</p><p>79 Having constructed the matrices Q(t), we then generate matrices S(t) = Q(t)Q(t) which are of rank r. [sent-189, score-0.232]
</p><p>80 We ﬁnally generate the adjacency matrix At as A(t) = 1[θ;∞[ (S(t)) for a threshold θ. [sent-190, score-0.365]
</p><p>81 2  Real Data  Collaborative Filtering1 We can see the purchase histories of e-commerce websites as graph sequences where links are established between a user and a product when the user purchases that product. [sent-196, score-0.532]
</p><p>82 We use data from 10 months music purchase history of a major e-commerce website to evaluate our method. [sent-197, score-0.134]
</p><p>83 For our test we selected a set of 103 users and 103 products that had the highest degrees (number of sales). [sent-198, score-0.141]
</p><p>84 5 × 103 edges of the graph (corresponding to purchases) into two parts following their occurrence time. [sent-200, score-0.395]
</p><p>85 We used the data of the 8 ﬁrst months to predict the features at the end of the 10th month and use these features as well as the matrix at the end of the 8th month to discover the purchases during the 2 last months. [sent-201, score-0.695]
</p><p>86 For the simulation data we report the average AUC over 10 simulation runs. [sent-205, score-0.144]
</p><p>87 From the simulation results we observe that for low rank underlying matrices, our method outperforms the rivals. [sent-206, score-0.152]
</p><p>88 Our method (as well as the static low rank method based on the low rank hypothesis) however fails when the rank of S(t) is high. [sent-208, score-0.429]
</p><p>89 However, even in this case our method outperforms the method of static matrix completion. [sent-209, score-0.328]
</p><p>90 The results with the real data further indicate the advantage of using information about the evolution of the graph over time. [sent-210, score-0.461]
</p><p>91 Similarly to the simulation data, the proposed method outperforms the static matrix completion one. [sent-211, score-0.684]
</p><p>92 6  Conclusion  The main contribution of this work is the formulation of a learning problem that can be used to predict the evolution of the edges of a graph over time. [sent-212, score-0.665]
</p><p>93 A regularization approach to combine both static graph information as well as information about the dynamics of the evolution of the graph over time is proposed and an optimization algorithm is developed. [sent-213, score-1.069]
</p><p>94 Despite using simple graph features 1 Notice that we are looking to discover only unobserved links and not new occurences of past links. [sent-214, score-0.581]
</p><p>95 as well as estimation of the evolution of the feature values over time, experiments indicate that the proposed optimization method improves performance relative to benchmarks. [sent-281, score-0.233]
</p><p>96 Testing, or learning, other graph features as well as other ways to model their dynamics over time may further improve performance and is part of future work. [sent-282, score-0.538]
</p><p>97 A singular value thresholding algorithm for matrix e completion. [sent-334, score-0.239]
</p><p>98 The power of convex relaxation: Near-optimal matrix e completion. [sent-338, score-0.139]
</p><p>99 Fast alternating linearization methods for minimizing the sum of two convex functions. [sent-345, score-0.13]
</p><p>100 Link propagation: A fast semi-supervised learning algorithm for link prediction. [sent-351, score-0.263]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('completion', 0.284), ('graph', 0.278), ('link', 0.227), ('adjacency', 0.226), ('ft', 0.205), ('static', 0.189), ('evolution', 0.183), ('auc', 0.166), ('matrix', 0.139), ('mg', 0.136), ('mh', 0.127), ('edges', 0.117), ('cachan', 0.116), ('cmla', 0.116), ('ens', 0.116), ('katz', 0.116), ('vertices', 0.114), ('sk', 0.112), ('dynamics', 0.103), ('singular', 0.1), ('wk', 0.095), ('diag', 0.089), ('purchases', 0.088), ('predict', 0.087), ('nuclear', 0.087), ('purchase', 0.083), ('features', 0.083), ('links', 0.083), ('rank', 0.08), ('entries', 0.079), ('alternating', 0.079), ('snapshot', 0.078), ('snapshots', 0.078), ('matrices', 0.076), ('functional', 0.076), ('linearized', 0.074), ('simulation', 0.072), ('collaborative', 0.072), ('topological', 0.069), ('minimizers', 0.068), ('purchased', 0.068), ('missing', 0.067), ('evolving', 0.065), ('zk', 0.064), ('attachment', 0.063), ('prediction', 0.062), ('month', 0.059), ('preferential', 0.059), ('emmanuel', 0.059), ('past', 0.057), ('france', 0.056), ('nicolas', 0.056), ('recommender', 0.056), ('users', 0.051), ('vertex', 0.051), ('mining', 0.051), ('simulated', 0.051), ('months', 0.051), ('linearization', 0.051), ('marketing', 0.051), ('goldfarb', 0.051), ('feature', 0.05), ('social', 0.05), ('minimizer', 0.05), ('benchmarks', 0.049), ('undirected', 0.048), ('products', 0.047), ('dv', 0.046), ('proposition', 0.046), ('predicting', 0.046), ('discover', 0.046), ('discovery', 0.046), ('recommendations', 0.045), ('cand', 0.045), ('saul', 0.044), ('dynamic', 0.043), ('degrees', 0.043), ('tr', 0.043), ('minimization', 0.042), ('picked', 0.042), ('ridge', 0.041), ('du', 0.041), ('differentiable', 0.039), ('optimality', 0.039), ('map', 0.038), ('surrogate', 0.038), ('time', 0.038), ('norm', 0.037), ('lawrence', 0.037), ('latent', 0.036), ('future', 0.036), ('fast', 0.036), ('diffusion', 0.035), ('occurences', 0.034), ('penetration', 0.034), ('sarkar', 0.034), ('masashi', 0.034), ('koren', 0.034), ('newsletter', 0.034), ('schubert', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="162-tfidf-1" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>2 0.1828198 <a title="162-tfidf-2" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>3 0.12525171 <a title="162-tfidf-3" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>Author: Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We show that matrix completion with trace-norm regularization can be signiﬁcantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields signiﬁcant gains on the highly non-uniformly sampled Netﬂix dataset.</p><p>4 0.12347706 <a title="162-tfidf-4" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>Author: Francesco Orabona, Koby Crammer</p><p>Abstract: We propose a general framework to online learning for classiﬁcation problems with time-varying potential functions in the adversarial setting. This framework allows to design and prove relative mistake bounds for any generic loss function. The mistake bounds can be specialized for the hinge loss, allowing to recover and improve the bounds of known online classiﬁcation algorithms. By optimizing the general bound we derive a new online classiﬁcation algorithm, called NAROW, that hybridly uses adaptive- and ﬁxed- second order information. We analyze the properties of the algorithm and illustrate its performance using synthetic dataset. 1</p><p>5 0.12028517 <a title="162-tfidf-5" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>6 0.11702031 <a title="162-tfidf-6" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>7 0.11602692 <a title="162-tfidf-7" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>8 0.11359324 <a title="162-tfidf-8" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>9 0.113244 <a title="162-tfidf-9" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>10 0.11108918 <a title="162-tfidf-10" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>11 0.11021642 <a title="162-tfidf-11" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>12 0.10927834 <a title="162-tfidf-12" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>13 0.10693595 <a title="162-tfidf-13" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>14 0.10465077 <a title="162-tfidf-14" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>15 0.10368912 <a title="162-tfidf-15" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>16 0.098857962 <a title="162-tfidf-16" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>17 0.096343115 <a title="162-tfidf-17" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>18 0.093995653 <a title="162-tfidf-18" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>19 0.093267746 <a title="162-tfidf-19" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>20 0.088569358 <a title="162-tfidf-20" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.246), (1, 0.064), (2, 0.059), (3, 0.123), (4, -0.025), (5, -0.117), (6, 0.024), (7, -0.071), (8, -0.067), (9, 0.036), (10, -0.048), (11, 0.026), (12, 0.159), (13, 0.14), (14, 0.102), (15, -0.001), (16, 0.023), (17, -0.147), (18, -0.03), (19, 0.156), (20, 0.069), (21, 0.164), (22, -0.106), (23, -0.092), (24, 0.125), (25, 0.057), (26, 0.009), (27, 0.101), (28, 0.011), (29, 0.076), (30, -0.004), (31, -0.025), (32, -0.027), (33, 0.02), (34, -0.074), (35, 0.044), (36, 0.094), (37, 0.05), (38, 0.087), (39, 0.005), (40, -0.005), (41, 0.028), (42, 0.061), (43, 0.066), (44, 0.019), (45, 0.098), (46, 0.115), (47, 0.005), (48, -0.031), (49, -0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96063966 <a title="162-lsi-1" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>2 0.6653372 <a title="162-lsi-2" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>Author: Kaushik Mitra, Sameer Sheorey, Rama Chellappa</p><p>Abstract: Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semideﬁnite program (LRSDP) with the advantage that: 1) an efﬁcient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and 2) additional constraints such as orthonormality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm ﬁnds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the afﬁne SfM problem, non-rigid SfM and photometric stereo problems.</p><p>3 0.65817958 <a title="162-lsi-3" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>4 0.65691537 <a title="162-lsi-4" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>Author: Prateek Jain, Raghu Meka, Inderjit S. Dhillon</p><p>Abstract: Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under afﬁne constraints (ARMP) and show that SVP recovers the minimum rank solution for afﬁne constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of lowrank matrix completion, for which the deﬁning afﬁne constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is signiﬁcantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem. 1</p><p>5 0.63995302 <a title="162-lsi-5" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>6 0.6212579 <a title="162-lsi-6" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>7 0.62040764 <a title="162-lsi-7" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>8 0.60788804 <a title="162-lsi-8" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>9 0.58774906 <a title="162-lsi-9" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>10 0.57514358 <a title="162-lsi-10" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>11 0.5745331 <a title="162-lsi-11" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>12 0.53623796 <a title="162-lsi-12" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>13 0.53194857 <a title="162-lsi-13" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>14 0.50409585 <a title="162-lsi-14" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>15 0.497926 <a title="162-lsi-15" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>16 0.49640498 <a title="162-lsi-16" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>17 0.4742018 <a title="162-lsi-17" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>18 0.47032398 <a title="162-lsi-18" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>19 0.44740793 <a title="162-lsi-19" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>20 0.44567287 <a title="162-lsi-20" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.059), (17, 0.01), (27, 0.072), (30, 0.065), (35, 0.023), (45, 0.198), (50, 0.06), (52, 0.063), (60, 0.031), (77, 0.082), (78, 0.015), (79, 0.23), (90, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94974703 <a title="162-lda-1" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>2 0.85167408 <a title="162-lda-2" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>same-paper 3 0.80899304 <a title="162-lda-3" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>4 0.80001372 <a title="162-lda-4" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>Author: Kritika Muralidharan, Nuno Vasconcelos</p><p>Abstract: The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation θ, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justiﬁcation for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classiﬁcation tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems. 1</p><p>5 0.73861653 <a title="162-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.73343748 <a title="162-lda-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.72375703 <a title="162-lda-7" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>8 0.72286081 <a title="162-lda-8" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>9 0.72264868 <a title="162-lda-9" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>10 0.72261596 <a title="162-lda-10" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>11 0.72139859 <a title="162-lda-11" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>12 0.7207126 <a title="162-lda-12" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>13 0.72034413 <a title="162-lda-13" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>14 0.71955669 <a title="162-lda-14" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>15 0.71754831 <a title="162-lda-15" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>16 0.71575654 <a title="162-lda-16" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>17 0.71519732 <a title="162-lda-17" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>18 0.71500564 <a title="162-lda-18" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>19 0.71495122 <a title="162-lda-19" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>20 0.71488738 <a title="162-lda-20" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
