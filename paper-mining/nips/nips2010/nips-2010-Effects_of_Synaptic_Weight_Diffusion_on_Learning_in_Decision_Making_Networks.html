<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-68" href="#">nips2010-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</h1>
<br/><p>Source: <a title="nips-2010-68-pdf" href="http://papers.nips.cc/paper/4029-effects-of-synaptic-weight-diffusion-on-learning-in-decision-making-networks.pdf">pdf</a></p><p>Author: Kentaro Katahira, Kazuo Okanoya, Masato Okada</p><p>Abstract: When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p><p>Reference: <a title="nips-2010-68-reference" href="../nips2010_reference/nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. [sent-11, score-0.29]
</p><p>2 Choice behavior has been empirically found to follow Herrnstein’s matching law. [sent-12, score-0.361]
</p><p>3 Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. [sent-13, score-0.888]
</p><p>4 However, their proof did not take into account the change in entire synaptic distributions. [sent-14, score-0.235]
</p><p>5 In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. [sent-15, score-0.996]
</p><p>6 This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. [sent-16, score-0.437]
</p><p>7 We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior. [sent-18, score-0.573]
</p><p>8 The choice behavior of subjects in such experiments is known to obey Herrnstein’s matching law [1]. [sent-20, score-0.571]
</p><p>9 In this study, by means of a statistical mechanical approach [8, 9, 10, 11], we analyze the properties of the covariance rule in a limit where the number of plastic synapses is inﬁnite . [sent-23, score-0.305]
</p><p>10 This result is caused by the diffusion of synaptic weights. [sent-25, score-0.416]
</p><p>11 The term “diffusion” refers to a phenomenon where the distributions over the population of synaptic weights broadens. [sent-26, score-0.294]
</p><p>12 This diffusion increases the variance in the potential of output units since the broader synaptic weight distributions are, the more they amplify ﬂuctuations in individual inputs. [sent-27, score-0.582]
</p><p>13 This makes the choice 1  behavior of the network more random and moves the probabilities of choosing alternatives to equal probabilities, than that predicted by the matching law. [sent-28, score-0.533]
</p><p>14 Our results suggest that when we discuss the learning processes in a decision making network, it may be insufﬁcient to only consider a steady state for individual weight updates, and we should therefore consider the dynamics of the weight distribution and the network architecture. [sent-30, score-0.375]
</p><p>15 Here, we consider stochastic choice behavior, where at each time step, a subject chooses alternative a with probability pa . [sent-34, score-0.337]
</p><p>16 The expected return, ⟨r|a⟩, refers to the average reward per choice a, and the income, Ia , refers to the total amount of reward resulting ∑n from the choice a and Ia / ( a′a I∑) is a fractional income from choice a. [sent-37, score-0.528]
</p><p>17 The matching law states that Ia / ( a′a Ia′ ) = pa for all a with pa ̸= 0. [sent-40, score-0.813]
</p><p>18 For a large number of trials, the fraction of income from an alternative a is expressed as ∑ ⟨r|a⟩pa ′ = ⟨r|a⟩pa ′ ⟨r⟩ a′ ⟨r|a ⟩pa Then, the matching law states that this quantity equals pa for all a. [sent-41, score-0.622]
</p><p>19 Equation 1 is a condition for the matching law, and we will often use this identity. [sent-44, score-0.247]
</p><p>20 3  Model  Decision Making Network: The decision making network we study consists of sensory-input neurons and output neurons that represent the subjective value of each alternative (we call the output neurons value-encoding neurons). [sent-45, score-0.351]
</p><p>21 The choice is made in such a way that alternative a is chosen if the potential of output unit ua , which will be speciﬁed below, is higher than that of the other alternative. [sent-54, score-0.289]
</p><p>22 A A B B With the synaptic efﬁcacies (or weights) J A = (J1 , . [sent-58, score-0.235]
</p><p>23 , JN ), the net input to the output units are given by  ha =  N ∑  Jia xa , a = A, B. [sent-64, score-0.449]
</p><p>24 This means that the mean of ha is O( N ), thus diverges for large N , while the variance is kept of order unity. [sent-71, score-0.244]
</p><p>25 Using the order parameters N 1 ∑ a ¯ la = ||J a ||, Ja = √ Ji , N i=1  (3)  √ ¯ 2 we ﬁnd ha ∼ N ( N X0 Ja , la ) where N (µ, σ 2 ) denotes the gaussian distribution with mean µ √ 2 and variance σ . [sent-76, score-0.422]
</p><p>26 We assume ua obeys a gaussian distribution of mean Ca ua / N , and variance 2 2 Ca Var[ua ] + σp due to the reccurent balancing mechanism [14]. [sent-77, score-0.382]
</p><p>27 Then, ua is computed √ √ ¯ ¯ ¯ as ua = ha − ha + σp ϵ with ha = (1 − 1/ N )E[ha ] where E[ha ] = N X0 Ja and ϵ is a rec rec gaussian random variable with unit mean and unit variance. [sent-79, score-1.006]
</p><p>28 Then, ua obey the independent Gaussian 2 2 ¯ distributions whose means and variances are respectively given by Ja and la + σp . [sent-80, score-0.284]
</p><p>29 This parameter is more convenient for gaining insights into the evolution of the weight than 2 the weight norm, la . [sent-86, score-0.327]
</p><p>30 The diffusion of weight distributions is reﬂected by increases in σa , i. [sent-87, score-0.297]
</p><p>31 The expectation of · these updates is proportional to covariance between the reward, r, and a measure of neural activity (y a (xa − cx ) for RM-Hebb rule, and xa − cx for the delta rule). [sent-91, score-0.566]
</p><p>32 The delta rule has been used as an example of the covariance rule [3, 7] and has also been used for the learning rule in the model of perceptual learning [21]. [sent-93, score-0.591]
</p><p>33 Jia (t + 1) = Jia (t) +  2 From this assumption, this model can be transformed into a simple mathematical equivalent form that the  √ distribution of input xa is replaced with N (X0 / N , 1) and the potential in output is replaced with ua = i ∑N a a a a i=1 Ji xi + σp ξ , where ξ ∼ N (0, 1). [sent-98, score-0.407]
</p><p>34 To do this, we ﬁrst rewrite the learning rule in a vector form: 1 J a (t + 1) = J a (t) + Fa (xa − cx ), (7) N where for the RM-Hebb rule, Fa = η(rt − rt )y a and for the delta rule, Fa = η(rt − rt ). [sent-100, score-0.403]
</p><p>35 Taking the ¯ ¯ 2 2 2 ˜ a + 1 Fa (t)2 + square norm of each side of equation 7, we obtain la (t + 1) = la (t) + N Fa (t) h N ∑N ˜ O(1/N 2 ), where we have deﬁned ha = Jia (xa − cx ). [sent-101, score-0.516]
</p><p>36 Summing up over all components i i=1 1 ¯ ¯ x on both sides of equation 7, we obtain Ja (t + 1) = Ja (t) + N Fa (t)˜a , where we have deﬁned ∑N a ˜ xa = i=1 (xi − cx ). [sent-102, score-0.298]
</p><p>37 , α = t/N , the evolutions of the order parameters obey ordinary differential equations: 2 ¯ dla dJa 2 ˜ ˜ = 2⟨Fa ha ⟩ + ⟨Fa ⟩, = ⟨Fa xa ⟩, (8) dα dα where ⟨·⟩ denotes the ensemble average over all possible inputs and arrivals of rewards. [sent-108, score-0.48]
</p><p>38 x ˜ The conditional averages ⟨ha |a⟩ and ⟨˜ a |a⟩ in these equations are computed as x ( ( 2 2) 2 2) ¯ X 0 DJ X 0 DJ l2 Ja ¯ ¯ ˜ ¯ ⟨ha |a⟩ = Ja X0 + √ a exp − , ⟨˜ a |a⟩ = X0 + √ x exp − , 2L2 2L2 pa 2πL2 pa 2πL2 (9) √ 2 2 2 ¯ ¯ where we have deﬁned L = lA + lB + 2σp and DJ = JB − JA . [sent-110, score-0.529]
</p><p>39 Next, we consider weight normalization in which the total length of the weight vector is kept constant. [sent-112, score-0.328]
</p><p>40 This is achieved by modifying the learning rule in the following way [22]: √ 1 2(J a (t) + N Fa xa ) J a (t) + Fa xa a √ J (t + 1) = = √ , (10) 1 + F/N ||J A (t) + 1 FA xA ||2 + ||J B (t) + 1 FB xB ||2 N  N  with F ≡ FA u + FB u + + provided that ||J ||2 = 2 holds at trial t. [sent-123, score-0.555]
</p><p>41 Expanding the right-hand side to ﬁrst order in 1/N , we can obtain the differential equations similarly to Equation 8: 2 ¯ dla dJa 1 2 2 ˜ ¯ ˜ = 2⟨Fa ha ⟩ + ⟨Fa ⟩ − ⟨F⟩la , = ⟨Fa xa ⟩ − ⟨F⟩Ja . [sent-124, score-0.479]
</p><p>42 8  A  With normarization p  A  Order parameters  No normarization  1  RM-Hebb rule B  100  200  300  400  500  0   0 -0. [sent-150, score-0.275]
</p><p>43 5  Figure 1: Evolution of choice probability and order parameters for RM-Hebb rules (A, B, E, F) and delta rule (C, D, G, H), without weight normalization (A-D) and with normalization (E-H). [sent-151, score-0.697]
</p><p>44 5  Results  To demonstrate the behavior of the model, we used a time-discrete version of a variable-interval (VI) reward schedule, which is commonly used for studying the matching law. [sent-164, score-0.466]
</p><p>45 For this task setting, the choice probability that yields matching behavior (denoted as pmatch ) is pmatch = 0. [sent-170, score-1.083]
</p><p>46 Figure 1(A-D) plots the evolution of choice A A probability and order parameters in two learning rules without a weight normalization constraint. [sent-172, score-0.362]
</p><p>47 We can see that the choice probability approaches a value that yields matching behavior ¯ (pmatch ), while the order parameters Ja and σa continue to change without becoming saturated. [sent-175, score-0.447]
</p><p>48 A The weight standard deviation, σa , always increases (the synaptic weight diffusion). [sent-176, score-0.443]
</p><p>49 For the RM-Hebb rule, the choice probability saturates at a value below pmatch . [sent-179, score-0.432]
</p><p>50 For the delta rule, the choice probability ﬁrst approaches pmatch , but without A A reaching pmatch . [sent-180, score-0.834]
</p><p>51 1  Matching Behavior Is Not Necessarily Steady State of Learning  From Figure 1, the choice probability seems to asymptotically approach matching behavior for the case without wight normalization. [sent-184, score-0.447]
</p><p>52 However, matching behavior is not necessarily a steady state of learning. [sent-185, score-0.463]
</p><p>53 In Figure 2, the order parameters are initialized so that pA (0) = pmatch and then A Equations 8 and 11 are numerically solved. [sent-186, score-0.318]
</p><p>54 We see that pA does not remain at pmatch but changes A toward the uniform choice (pA = 0. [sent-187, score-0.438]
</p><p>55 Then, for the RM-Hebb rule, pA evolves toward pmatch , but not do so for the delta rule. [sent-189, score-0.464]
</p><p>56 55 0  500  1000  0  500  1000  Figure 2: Strict matching behavior is not equilibrium point. [sent-199, score-0.361]
</p><p>57 We set initial value of order parameters to derive perfect matching for (A) no normalization condition and (B) normalization condition. [sent-200, score-0.504]
</p><p>58 In both cases, choice probability that yields perfect matching is repulsive. [sent-201, score-0.326]
</p><p>59 For normalization condition, these values were rescaled so that normalization so that pA = pA condition was met. [sent-205, score-0.264]
</p><p>60 repulsive property of matching behavior, let us substitute the condition of the matching law, ⟨r|A⟩ = ˜ ⟨r|B⟩ = ⟨r⟩ into Equations 11, for the no normalization condition. [sent-206, score-0.614]
</p><p>61 We then ﬁnd that ⟨Fa ha ⟩ 2 ˜ and ⟨Fa xa ⟩ are zero but ⟨Fa ⟩ is non-zero and positive except for the non-interesting case where r always takes the same value. [sent-207, score-0.42]
</p><p>62 Therefore, when pA = pmatch , the variance in the weight increases, A 2 2 ¯2 i. [sent-208, score-0.41]
</p><p>63 This moves the choice probabilities toward unbiased choice behavior, pA = 0. [sent-211, score-0.252]
</p><p>64 This result is in A contrast with the N = 1 case [7] where the average changes stop when pA converges to pmatch . [sent-214, score-0.318]
</p><p>65 Hence, again, the choice probability at pmatch A approaches unbiased choice behavior due to the diffusion effect. [sent-217, score-0.863]
</p><p>66 Nevetheless, the choice probability of the RM-Hebb rule without weight normalization asymptotically converges to pmatch . [sent-218, score-0.759]
</p><p>67 The “diffusion term”, ⟨Fa ⟩, which moves pA away from pmatch depends on pA but not on A ¯ the magnitude of Ja ’s. [sent-222, score-0.35]
</p><p>68 Thus, within the order parameter set satisfying pA = pmatch , the larger the A ¯ ¯ magnitudes of Ja ’s are, the weaker is the repulsive effect. [sent-223, score-0.354]
</p><p>69 If |JB − JA | → ∞ while σA , σB are ﬁnite, ¯ ¯ pA stays at pmatch . [sent-224, score-0.318]
</p><p>70 Because |JB − JA | can increase faster than σA and σB in the RM-Hebb rule A without any weight constraints, the network approaches such situations. [sent-225, score-0.271]
</p><p>71 This is the reason that in Figure 2A the pA returned to pmatch after it was repulsed from pmatch . [sent-226, score-0.636]
</p><p>72 When weight normalization A A ¯ ¯ ¯ is imposed, the magnitude of Ja ’s are limited as |JB − JA | < 2. [sent-227, score-0.24]
</p><p>73 Thus, the diffusion effect prevents ¯ pA from approaching pmatch . [sent-228, score-0.524]
</p><p>74 Top rows are for non-weight normalization condition and bottom rows are for normalization condition. [sent-277, score-0.264]
</p><p>75 Columns at left are for RM-Hebb rule and those at right are for delta rule. [sent-278, score-0.259]
</p><p>76 Therefore, if η is small, the repulsive effect from matching behavior due to the diffusion effect is expected to weaken. [sent-285, score-0.628]
</p><p>77 As a whole, as η is decreased, the asymptotic value, pA , approaches matching behavior, but relaxation slows down due to the diffusion of synaptic weights. [sent-287, score-0.652]
</p><p>78 As we previously discussed, the diffusion effect is more evident for the delta rule than for the RM-Hebb rule, and for the weight-normalization condition than the non- normalization condition. [sent-288, score-0.613]
</p><p>79 For the RM-Hebb rule without normalization, networks approach matching behavior even for a very ¯ large learning rate (η = 1000). [sent-290, score-0.508]
</p><p>80 Although there is still a deviation from perfect matching (see inset of Figure 3A), the asymptotic value is almost unaffected in the RM-Hebb rule. [sent-293, score-0.261]
</p><p>81 For the delta rule without normalization, the asymptotic values gradually depend on η. [sent-294, score-0.28]
</p><p>82 With normalization constraints, the RM-Hebb rule also demonstrate graded dependence of asymptotic probability on η. [sent-295, score-0.284]
</p><p>83 3  Deviation from Matching Law  Choices by animals in many experiments deviate slightly from matching behavior toward unbiased random choice, a phenomenon called undermatching [2, 23]. [sent-298, score-0.664]
</p><p>84 For the RM-rule with weight normalization, as the learning rate η increases, the choice probabilities deviate from matching behavior towards unbiased random choice, pA = 0. [sent-302, score-0.613]
</p><p>85 We see that the larger the η is, the broader the weight distributions due the the synaptic diffusion effects (Figure 4A). [sent-306, score-0.535]
</p><p>86 This result suggests that the weight diffusion effect causes undermathing regardless of the way of weight constraint, as long as the synaptic weights are conﬁned to a ﬁnite range, as predicted by our theory. [sent-307, score-0.648]
</p><p>87 7  Figure 4: Constraints on synaptic weights leads to the undermatching behavior through synaptic diffusion effects. [sent-308, score-0.979]
</p><p>88 As the learning rate η increases, the choice probabilities deviate from matching behavior towards unbiased random choice, pA = 0. [sent-315, score-0.521]
</p><p>89 (B) The same plot with (A) for the RM-rule with the hard bound condition √ (the synaptic weights are restricted to the interval [0, Jmax / N ] where Jmax = 5. [sent-317, score-0.29]
</p><p>90 If only a single plastic synapse is taken into consideration, covariance learning rules seem to make matching behavior a steady state of learning. [sent-328, score-0.577]
</p><p>91 However, under certain situations where a large number of synapses simultaneously modify their efﬁcacy, matching behavior cannot be a steady state. [sent-329, score-0.505]
</p><p>92 Choice behavior in many experiments deviates slightly from matching behavior toward unbiased choice behavior, a phenomenon called undermatching [23, 2]. [sent-332, score-0.868]
</p><p>93 The authors interpreted that the departure from matching behavior due to limitations in the learning rule was a possible mechanism for undermatching. [sent-337, score-0.552]
</p><p>94 , undermatching can be caused by the diffusion of synaptic efﬁcacies. [sent-342, score-0.575]
</p><p>95 The diffusion effects provide a robust mechanism for undermatching: It reproduces undermatching behavior, regardless of speciﬁc task settings. [sent-343, score-0.411]
</p><p>96 To achieve random choice behavior, it is thought to require ﬁne-tuning of network parameters [16], whereas random choice behavior is often observed in behavioral experiments. [sent-344, score-0.376]
</p><p>97 Our results suggest that the broad distributions of synaptic weights observed in experiments [24] can make it easier to realize stochastic random choice behavior perhaps than previously thought. [sent-345, score-0.49]
</p><p>98 Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity. [sent-367, score-0.593]
</p><p>99 A biophysically based neural model of matching law behavior: melioration by stochastic synapses. [sent-373, score-0.311]
</p><p>100 Robustness of learning that is based on covariance-driven synaptic plasticity. [sent-387, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ja', 0.372), ('fa', 0.332), ('pmatch', 0.318), ('pa', 0.251), ('synaptic', 0.235), ('ha', 0.216), ('matching', 0.215), ('xa', 0.204), ('diffusion', 0.181), ('undermatching', 0.159), ('ua', 0.153), ('rule', 0.147), ('behavior', 0.146), ('jb', 0.132), ('jia', 0.12), ('normalization', 0.116), ('delta', 0.112), ('reward', 0.105), ('la', 0.103), ('steady', 0.102), ('law', 0.096), ('cx', 0.094), ('weight', 0.092), ('choice', 0.086), ('fb', 0.077), ('neurons', 0.068), ('erfc', 0.064), ('jmax', 0.064), ('normarization', 0.064), ('income', 0.06), ('lb', 0.056), ('sensory', 0.054), ('ia', 0.051), ('jn', 0.051), ('uctuations', 0.05), ('herrnstein', 0.048), ('loewenstein', 0.048), ('okanoya', 0.048), ('plastic', 0.048), ('sakai', 0.048), ('soltani', 0.048), ('xb', 0.047), ('unbiased', 0.046), ('mechanism', 0.044), ('schedule', 0.043), ('synapses', 0.042), ('evolution', 0.04), ('covariance', 0.038), ('repulsive', 0.036), ('ub', 0.036), ('phenomenon', 0.036), ('toward', 0.034), ('condition', 0.032), ('decision', 0.032), ('network', 0.032), ('magnitude', 0.032), ('dja', 0.032), ('dla', 0.032), ('europhysics', 0.032), ('katahira', 0.032), ('mistuning', 0.032), ('reccurent', 0.032), ('sugrue', 0.032), ('japan', 0.031), ('reinforcement', 0.031), ('alternatives', 0.031), ('mechanical', 0.03), ('output', 0.029), ('obey', 0.028), ('rules', 0.028), ('kept', 0.028), ('saturates', 0.028), ('weakens', 0.028), ('deviate', 0.028), ('effects', 0.027), ('equations', 0.027), ('competition', 0.027), ('neuron', 0.026), ('behavioral', 0.026), ('dj', 0.026), ('macroscopic', 0.026), ('rec', 0.026), ('schedules', 0.026), ('effect', 0.025), ('perfect', 0.025), ('rt', 0.025), ('making', 0.025), ('plos', 0.025), ('proportionally', 0.024), ('cb', 0.024), ('increases', 0.024), ('updates', 0.024), ('recurrent', 0.023), ('choosing', 0.023), ('weights', 0.023), ('unity', 0.023), ('scaled', 0.022), ('mechanics', 0.022), ('potential', 0.021), ('asymptotic', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="68-tfidf-1" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>Author: Kentaro Katahira, Kazuo Okanoya, Masato Okada</p><p>Abstract: When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p><p>2 0.14884514 <a title="68-tfidf-2" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>3 0.12469679 <a title="68-tfidf-3" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>4 0.09755259 <a title="68-tfidf-4" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>Author: Daniel Golovin, Andreas Krause, Debajyoti Ray</p><p>Abstract: We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise–free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near–optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2 , a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the ﬁrst competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non–uniform cost and their noise is correlated. We also propose E FF ECXTIVE , a particularly fast approximation of EC 2 , and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty. 1</p><p>5 0.091479868 <a title="68-tfidf-5" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>Author: K. Wong, He Wang, Si Wu, Chi Fung</p><p>Abstract: Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We ﬁnd that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we ﬁnd that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p><p>6 0.070436865 <a title="68-tfidf-6" href="./nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">69 nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>7 0.067615375 <a title="68-tfidf-7" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>8 0.067212112 <a title="68-tfidf-8" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>9 0.064061396 <a title="68-tfidf-9" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>10 0.063502081 <a title="68-tfidf-10" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>11 0.063078143 <a title="68-tfidf-11" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>12 0.059825361 <a title="68-tfidf-12" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>13 0.059227854 <a title="68-tfidf-13" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>14 0.055974409 <a title="68-tfidf-14" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>15 0.055177066 <a title="68-tfidf-15" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>16 0.054570869 <a title="68-tfidf-16" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>17 0.054261263 <a title="68-tfidf-17" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>18 0.051581215 <a title="68-tfidf-18" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>19 0.050941382 <a title="68-tfidf-19" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>20 0.050415173 <a title="68-tfidf-20" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, -0.027), (2, -0.076), (3, 0.105), (4, 0.02), (5, 0.076), (6, -0.043), (7, 0.032), (8, 0.04), (9, 0.055), (10, 0.012), (11, 0.06), (12, 0.05), (13, -0.042), (14, -0.03), (15, 0.033), (16, -0.037), (17, -0.017), (18, 0.032), (19, 0.064), (20, 0.01), (21, 0.003), (22, -0.005), (23, -0.059), (24, 0.062), (25, 0.018), (26, -0.006), (27, 0.056), (28, -0.033), (29, -0.124), (30, -0.053), (31, 0.067), (32, 0.01), (33, 0.035), (34, -0.112), (35, -0.028), (36, 0.004), (37, -0.024), (38, -0.08), (39, 0.028), (40, -0.157), (41, 0.037), (42, 0.062), (43, -0.007), (44, -0.013), (45, 0.012), (46, -0.034), (47, 0.118), (48, 0.074), (49, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94988561 <a title="68-lsi-1" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>Author: Kentaro Katahira, Kazuo Okanoya, Masato Okada</p><p>Abstract: When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p><p>2 0.62215954 <a title="68-lsi-2" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>3 0.6147036 <a title="68-lsi-3" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>Author: K. Wong, He Wang, Si Wu, Chi Fung</p><p>Abstract: Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We ﬁnd that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we ﬁnd that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p><p>4 0.57051641 <a title="68-lsi-4" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>5 0.46727064 <a title="68-lsi-5" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>Author: Shaul Druckmann, Dmitri B. Chklovskii</p><p>Abstract: A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive ﬁeld of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit. 1</p><p>6 0.46168634 <a title="68-lsi-6" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>7 0.45378751 <a title="68-lsi-7" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>8 0.44154534 <a title="68-lsi-8" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>9 0.4107542 <a title="68-lsi-9" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>10 0.40105361 <a title="68-lsi-10" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>11 0.39775962 <a title="68-lsi-11" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>12 0.39585012 <a title="68-lsi-12" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>13 0.38329297 <a title="68-lsi-13" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>14 0.37309691 <a title="68-lsi-14" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>15 0.36873588 <a title="68-lsi-15" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>16 0.3662205 <a title="68-lsi-16" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>17 0.35956836 <a title="68-lsi-17" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>18 0.35409603 <a title="68-lsi-18" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>19 0.35179168 <a title="68-lsi-19" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>20 0.35012478 <a title="68-lsi-20" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.029), (27, 0.095), (30, 0.038), (45, 0.124), (50, 0.047), (52, 0.05), (60, 0.024), (75, 0.323), (77, 0.137), (90, 0.02), (99, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73319536 <a title="68-lda-1" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>Author: Kentaro Katahira, Kazuo Okanoya, Masato Okada</p><p>Abstract: When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modiﬁcations in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein & Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufﬁciently strong so that the ﬂuctuations in input from individual sensory neurons inﬂuence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p><p>2 0.53333688 <a title="68-lda-2" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>Author: K. Wong, He Wang, Si Wu, Chi Fung</p><p>Abstract: Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We ﬁnd that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we ﬁnd that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p><p>3 0.53078568 <a title="68-lda-3" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>4 0.52824521 <a title="68-lda-4" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>5 0.52814692 <a title="68-lda-5" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>6 0.5193218 <a title="68-lda-6" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>7 0.51019305 <a title="68-lda-7" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>8 0.50938731 <a title="68-lda-8" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>9 0.50646019 <a title="68-lda-9" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>10 0.50500935 <a title="68-lda-10" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>11 0.5048852 <a title="68-lda-11" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>12 0.50118631 <a title="68-lda-12" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>13 0.49605393 <a title="68-lda-13" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>14 0.49207321 <a title="68-lda-14" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>15 0.49140784 <a title="68-lda-15" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>16 0.48809642 <a title="68-lda-16" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>17 0.4875401 <a title="68-lda-17" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>18 0.48678929 <a title="68-lda-18" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>19 0.48278913 <a title="68-lda-19" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>20 0.48246601 <a title="68-lda-20" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
