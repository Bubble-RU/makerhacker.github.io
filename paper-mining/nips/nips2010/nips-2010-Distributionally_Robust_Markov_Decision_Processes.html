<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2010-Distributionally Robust Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-64" href="#">nips2010-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 nips-2010-Distributionally Robust Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2010-64-pdf" href="http://papers.nips.cc/paper/3927-distributionally-robust-markov-decision-processes.pdf">pdf</a></p><p>Author: Huan Xu, Shie Mannor</p><p>Abstract: We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different conﬁdence level so that a set of admissible probability distributions of the unknown parameters is speciﬁed. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the conﬁdence region of parameters abound. We propose a decision criterion based on distributional robustness: the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible (i.e., it agrees with the a-priori information). We show that ﬁnding the optimal distributionally robust policy can be reduced to a standard robust MDP where the parameters belong to a single uncertainty set, hence it can be computed in polynomial time under mild technical conditions.</p><p>Reference: <a title="nips-2010-64-reference" href="../nips2010_reference/nips-2010-Distributionally_Robust_Markov_Decision_Processes_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ps', 0.609), ('cs', 0.363), ('robust', 0.298), ('policy', 0.279), ('ht', 0.238), ('mdp', 0.185), ('uncertain', 0.179), ('rs', 0.154), ('hr', 0.153), ('uncertainty', 0.128), ('storm', 0.094), ('horizon', 0.092), ('nomin', 0.087), ('admiss', 0.077), ('mdps', 0.076), ('sin', 0.068), ('decid', 0.065), ('exit', 0.058), ('vt', 0.056), ('visit', 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="64-tfidf-1" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>2 0.2964876 <a title="64-tfidf-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>3 0.24842197 <a title="64-tfidf-3" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>4 0.21719986 <a title="64-tfidf-4" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>5 0.2137119 <a title="64-tfidf-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>6 0.20196635 <a title="64-tfidf-6" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>7 0.19902232 <a title="64-tfidf-7" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>8 0.19477822 <a title="64-tfidf-8" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>9 0.1868342 <a title="64-tfidf-9" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>10 0.17954759 <a title="64-tfidf-10" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>11 0.17881474 <a title="64-tfidf-11" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>12 0.17851955 <a title="64-tfidf-12" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>13 0.13774382 <a title="64-tfidf-13" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>14 0.13202375 <a title="64-tfidf-14" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>15 0.12570354 <a title="64-tfidf-15" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>16 0.11934419 <a title="64-tfidf-16" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>17 0.11518253 <a title="64-tfidf-17" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>18 0.11476408 <a title="64-tfidf-18" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>19 0.11154516 <a title="64-tfidf-19" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>20 0.11046337 <a title="64-tfidf-20" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.207), (1, 0.308), (2, -0.019), (3, 0.016), (4, 0.008), (5, -0.0), (6, 0.023), (7, 0.019), (8, -0.016), (9, -0.046), (10, -0.005), (11, -0.04), (12, 0.038), (13, 0.018), (14, -0.045), (15, 0.01), (16, -0.006), (17, 0.075), (18, -0.042), (19, 0.047), (20, -0.011), (21, 0.001), (22, 0.091), (23, 0.017), (24, -0.016), (25, -0.171), (26, 0.033), (27, 0.15), (28, 0.115), (29, -0.006), (30, 0.031), (31, -0.102), (32, 0.024), (33, -0.046), (34, 0.018), (35, -0.045), (36, -0.097), (37, 0.029), (38, -0.14), (39, 0.148), (40, -0.083), (41, 0.09), (42, -0.019), (43, -0.101), (44, 0.067), (45, 0.046), (46, -0.09), (47, 0.201), (48, -0.02), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94889033 <a title="64-lsi-1" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>2 0.60163456 <a title="64-lsi-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>3 0.58638209 <a title="64-lsi-3" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>4 0.57337338 <a title="64-lsi-4" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>5 0.5689463 <a title="64-lsi-5" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>6 0.55985606 <a title="64-lsi-6" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>7 0.55879748 <a title="64-lsi-7" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>8 0.55142498 <a title="64-lsi-8" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>9 0.53928047 <a title="64-lsi-9" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>10 0.49853468 <a title="64-lsi-10" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>11 0.47620422 <a title="64-lsi-11" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>12 0.45593229 <a title="64-lsi-12" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>13 0.42935425 <a title="64-lsi-13" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>14 0.42934471 <a title="64-lsi-14" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>15 0.42268258 <a title="64-lsi-15" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>16 0.4190712 <a title="64-lsi-16" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>17 0.41242331 <a title="64-lsi-17" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>18 0.4072355 <a title="64-lsi-18" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>19 0.40636516 <a title="64-lsi-19" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>20 0.40449011 <a title="64-lsi-20" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.103), (30, 0.04), (32, 0.237), (34, 0.111), (41, 0.172), (45, 0.16), (68, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88123018 <a title="64-lda-1" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>2 0.86631703 <a title="64-lda-2" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>3 0.86396587 <a title="64-lda-3" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>4 0.85352182 <a title="64-lda-4" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>5 0.85236752 <a title="64-lda-5" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>6 0.85196429 <a title="64-lda-6" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>7 0.84969634 <a title="64-lda-7" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>8 0.84942627 <a title="64-lda-8" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>9 0.84921795 <a title="64-lda-9" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>10 0.84706342 <a title="64-lda-10" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>11 0.84543836 <a title="64-lda-11" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>12 0.84531474 <a title="64-lda-12" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>13 0.84517461 <a title="64-lda-13" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>14 0.84438694 <a title="64-lda-14" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>15 0.84366995 <a title="64-lda-15" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>16 0.8430832 <a title="64-lda-16" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>17 0.84277129 <a title="64-lda-17" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>18 0.84064627 <a title="64-lda-18" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>19 0.83694786 <a title="64-lda-19" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>20 0.83678615 <a title="64-lda-20" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
