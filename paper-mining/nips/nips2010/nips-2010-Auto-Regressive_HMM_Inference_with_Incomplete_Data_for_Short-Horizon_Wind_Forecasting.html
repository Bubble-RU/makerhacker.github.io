<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-35" href="#">nips2010-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</h1>
<br/><p>Source: <a title="nips-2010-35-pdf" href="http://papers.nips.cc/paper/3892-auto-regressive-hmm-inference-with-incomplete-data-for-short-horizon-wind-forecasting.pdf">pdf</a></p><p>Author: Chris Barber, Joseph Bockhorst, Paul Roebber</p><p>Abstract: Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. 1</p><p>Reference: <a title="nips-2010-35-reference" href="../nips2010_reference/nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. [sent-2, score-0.666]
</p><p>2 Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. [sent-3, score-0.192]
</p><p>3 Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. [sent-4, score-0.748]
</p><p>4 Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). [sent-7, score-0.375]
</p><p>5 In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. [sent-9, score-0.688]
</p><p>6 1  Introduction  Accurate wind speed and direction forecasts are essential for efﬁcient integration of wind energy into electrical transmission systems. [sent-10, score-1.378]
</p><p>7 Because information on the 30 minute to six-hour time horizon is actionable for many control decisions, and the current stateof-the-art is considered inadequate, there has been a recent surge of interest in improving forecasts in this range. [sent-13, score-0.303]
</p><p>8 The short-term wind forecasting (STWF) problem presents numerous challenges to the modeler. [sent-14, score-0.67]
</p><p>9 Numerical weather predictions (NWP) methods are the primary means for producing the large-scale weather forecasts used throughout the world, but are not competitive for STWF. [sent-16, score-0.463]
</p><p>10 In fact, NWP based wind speed predictions are less accurate than “persistence” forecasts (Giebel, 2003), a surprisingly robust method for time horizons less than a few hours. [sent-17, score-0.916]
</p><p>11 (2006) “hard code” their regimes based on wind direction, while Pinson and Madsen (2008) learns regimes for a single forecasting site with complete data. [sent-23, score-1.104]
</p><p>12 We propose a novel approach to STWF that automatically reasons about learned weather regimes across multiple sites while naturally handling missing observations. [sent-25, score-0.701]
</p><p>13 Our approach is based on switching conditional linear Gaussian (CLG) models, variously known as switching vector autoregressive models or autoregressive hidden Markov models. [sent-26, score-0.176]
</p><p>14 We introduce a novel and simple approximate inference approach that exploits the tendency of regimes to persist for several hours. [sent-30, score-0.268]
</p><p>15 Predictions by our learned models are signiﬁcantly better than baseline persistence predictions in experiments on national climatic data center (NCDC) data from two sites in the United States: one in the Paciﬁc Northwest and one in southern Wisconsin. [sent-31, score-0.604]
</p><p>16 Switching CLG models have been applied in other domains where missing observations are an issue, such as meteorology (Tang, 2004; Paroli et al. [sent-33, score-0.214]
</p><p>17 2  Methods  We consider the setting in which wind observations from a set of M stations arrive at regular intervals (hourly in our experiments). [sent-40, score-0.614]
</p><p>18 Let Ut and Vt be M -by-1 vector of random variables for the u and v components of the wind at all sites at time t. [sent-41, score-0.84]
</p><p>19 Our approach to STWF is based on auto-regressive HMMs where at each time t we have a single discrete random variable Rt that represents the active regime, and a continuous valued vector random variable Wt that represents measured wind speeds. [sent-43, score-0.564]
</p><p>20 As local probabilities are linear Gaussian (LG) we denote the model in which the regime variables Rt have cardinality C by AR-LG(C). [sent-44, score-0.23]
</p><p>21 We represent Pr(Rt+1 |Rt ) by the C-by-C transition matrix T where T (r, s) > 0 is the probability of transitioning from regime r to regime s. [sent-48, score-0.46]
</p><p>22 Since weather regimes tend to persist for multiple hours, the self-transition probabilities T (r, r) are typically the largest. [sent-49, score-0.353]
</p><p>23 2  Figure 1: Graphical structures of wind speed models. [sent-54, score-0.59]
</p><p>24 Exact inference in (b) with missing observations is NP-hard (Lerner and Parr, 2001). [sent-58, score-0.222]
</p><p>25 (c) Truncated AR-LG(C) HOMO approximation of (b) for predictions of wind speeds at target time t + h made at t with K = 2 and horizon h = 2. [sent-59, score-0.706]
</p><p>26 Our approximation assumes the regime does not change in the window t − K to t + h. [sent-60, score-0.23]
</p><p>27 (e) Detailed structure of (d) for 3 sites showing within time-slice conditional independencies and assorted missing observations. [sent-62, score-0.37]
</p><p>28 We ˙ ˙ ˙ ˙ denote the sequence of partial observations as w1:L = (w1 , w2 , · · · wL ) where the “dot” notation ˙ wt denotes a potentially incomplete vector with missing data. [sent-68, score-0.354]
</p><p>29 Our inference tasks are to calculate ˙ ˙ Pr(Wt−1 , Wt , Rt |w1:L ) and Pr(Rt−1 , Rt |w1:L ) while training to compute the expected sufﬁcient statistics needed for estimation of CLG parameters using EM (Murphy, 1998), and to compute ˙ Pr(Wt+H |w1:t ) for horizon H forecasting at time t. [sent-69, score-0.247]
</p><p>30 The training posteriors Pr(Wt−1 , Wt |w1:L ) have C dl +2+dr components where dl and dr are the number of consecutive time steps to the left of t − 1 and right of t with at least one missing observation. [sent-73, score-0.196]
</p><p>31 Because of the nature of data collection, most wind data sets with multiple sites will have a number of missing observations. [sent-74, score-0.934]
</p><p>32 Let P (V ) refer to a desired posterior distribution under the truncated model given evidence, which we assume has at least one missing observation at each time step. [sent-87, score-0.227]
</p><p>33 Each mixing proportion ωj is associated with a regime state sequence and pj (V ) is the posterior Gaussian for that sequence. [sent-89, score-0.302]
</p><p>34 This is the case for us as weather regimes tend to persist for a number of hours, and thus regime sequences with frequent regime switching are highly unlikely. [sent-93, score-0.872]
</p><p>35 2  Approach 2: HOMO  Our second method for setting Q(V ) is a simple but often effective approach that assumes no regime changes in the truncated model. [sent-106, score-0.286]
</p><p>36 This approach, which we call HOMO has n = C components, one for each homogeneous regime sequence. [sent-107, score-0.23]
</p><p>37 If the self transition probabilities T (r, r) are largest, then the most likely regime sequence a-priori is homogeneous, and thus is also chosen by PRIOR(n). [sent-108, score-0.23]
</p><p>38 The other components of PRIOR(n) may be only small variations from this homogeneous regime sequence, however, the components selected by HOMO are very different from one another. [sent-109, score-0.31]
</p><p>39 We would like to select the components for the top n regime sequences with maximum posterior likelihood, however, this too is NP-hard (Lerner and Parr, 2001). [sent-114, score-0.332]
</p><p>40 We instead use a fast approximation in which the posterior potential of set˙ ˙ tings to regime variables is set by local evidence. [sent-115, score-0.267]
</p><p>41 The “Count” row lists the number of hours in our WI data set (21 sites total) in which the number of sites with missing values was exactly equal to the value “# Sites Missing”. [sent-117, score-0.665]
</p><p>42 9%  be the potential for Rt = r, and then run BMMF on the model where Pr(rt−K , · · · , rt+K ) ∝ t+K τt−k (rt−K ) t =t−K+1 τt (rt )T (rt −1 , rt ). [sent-126, score-0.172]
</p><p>43 3  Experimental evaluation  We compare the forecasts of our models to forecasts of persistence models, which represent the current state-of-the-art for STWF. [sent-129, score-0.61]
</p><p>44 Are STWFs of our single-regime models more accurate than persistence forecasts? [sent-132, score-0.254]
</p><p>45 Are STWFs of our models that consider regimes more accurate than those of the single-regime models? [sent-134, score-0.21]
</p><p>46 The National Climatic Data Center (NCDC) maintains a publicly accessible database of hourly historical climatic surface data, from which we obtained 4 years of data from a number of sites in both regions. [sent-140, score-0.381]
</p><p>47 We have data from 21 and 24 sites in WI and PNW, respectively. [sent-142, score-0.236]
</p><p>48 We collect wind direction and wind speed at each site, as measured at 10 meters above ground level. [sent-147, score-1.181]
</p><p>49 Since our primary motivation is wind power forecasting, we prefer wind speed measurements taken at turbine height (approximately 50-100 meters above ground level). [sent-148, score-1.302]
</p><p>50 With each learned model we forecast wind speeds at all sites and all test-year hours at six horizons (1-6 hours). [sent-165, score-1.043]
</p><p>51 For the persistence model, we only make a horizon h forecast for target time t + h if the time t observation at that site is available. [sent-171, score-0.448]
</p><p>52 For point-predictions we predict the expected value of the posterior wind distribution at the prediction time. [sent-172, score-0.628]
</p><p>53 5  0 0  1  2  3  4  5  6  0 0  7  1  2  3  4  5  6  7  Horizon (hour)  Horizon (hour)  Figure 2: Mean RMSE over all sites and folds for single-regime model (AR-LG(1)) and persistence models in WI (left) and PNW (right). [sent-181, score-0.526]
</p><p>54 4  1 hour Persistence, WI AR−LG(1), WI  4  3 hour  4  3. [sent-183, score-0.21]
</p><p>55 5  RMSE (m/s)  5 hour  Site  Site  0  Site  Figure 3: Site-by-site average RMSE values for AR-LG(1) and persistence models in WI for 1, 3 and 5 hour horizons. [sent-195, score-0.464]
</p><p>56 This approximation method is simplest and suits our domain, where we expect distinct regimes to generally persist over a period of a few hours. [sent-198, score-0.23]
</p><p>57 For a given geographical region we denote the RMSE of the horizon h prediction sequence made by AR-LG(C) model for site s and year y by e(h, s, y, C). [sent-201, score-0.242]
</p><p>58 In a similar way we denote RMSE of a persistence prediction sequence by ep (h, s, y). [sent-202, score-0.273]
</p><p>59 For example, e(1, :, :, 2) is the collection of RMSE values for 1 hour predictions for the 2 regime model across all sites and years, and mean [e(1, :, :, 2)] and std [e(1, :, :, 2)] are the collection’s mean and standard deviation. [sent-204, score-0.61]
</p><p>60 We calculate LL values of the AR-LG(C) models relative to LL values of a persistent Gaussian model wt+h = wt + h . [sent-205, score-0.2]
</p><p>61 In order to make meaningful comparisons between AR-LG(C) and persistence models, we calculate performance measures for all horizon h prediction sequences from only hours for which a corresponding horizon h persistence prediction is available. [sent-207, score-0.792]
</p><p>62 3  Results  To compare our approximation methods, we evaluate the three approximate inference procedures, HOMO, PRIOR(2) and POST(2) using simulated data from a situation with ten sites arrayed linearly (eg, east-to-west) and two regimes. [sent-209, score-0.274]
</p><p>63 The parameters of regime 1 were set for a east-to-west 6  4  4  x 10  3. [sent-210, score-0.23]
</p><p>64 5 0 1  5  YR1 YR2 YR3 YR4  1  2 3 4 Number of regimes  5  Figure 4: Performance of multi- versus single-regime models at a 2 hour prediction horizon, in each of 4 folds. [sent-217, score-0.342]
</p><p>65 Performance measures of test-set relative log-likelihood across all sites for WI (left) and PNW (right), with number of regimes, C, on the x-axis. [sent-218, score-0.236]
</p><p>66 moving weather regime and the parameters for regime 2 were set for a west-to-east weather regime. [sent-220, score-0.706]
</p><p>67 Then we make 2-hr ahead forecasts at all time points using window size K from 2 to 15 hours. [sent-224, score-0.178]
</p><p>68 In all further experiments, we attempted to assess the effectiveness of the AR-LG(C) models, using the real wind data described above. [sent-230, score-0.564]
</p><p>69 To answer the ﬁrst question above, we compute mean [e(h, :, :, 1)] and mean [ep (h, :, :)] for the RMSE collections of the AR-LG(1) and persistence models for both geographical locations and all horizons h = 1, 2, . [sent-231, score-0.427]
</p><p>70 In both WI and PNW the AR-LG(1) model has signiﬁcantly lower RMSE than persistence for 1 and 2 hour time horizons. [sent-238, score-0.329]
</p><p>71 In PNW the gap between AR-LG(1) and persistence grows with h, while in in WI the AR-LG(1) performance begins to degrade relative to persistence starting with h = 3. [sent-240, score-0.448]
</p><p>72 At 3 and 4 hour horizons we see an increase in the variance of AR-LG(1), but still a lower mean RMSE than persistence. [sent-241, score-0.214]
</p><p>73 For h = 5 and h = 6 the persistence model has lower mean RMSE than the AR-LG(1). [sent-242, score-0.224]
</p><p>74 To gain insight into decreasing performance at longer horizons in WI, we plot in Figure 3 the mean and standard deviation RMSE values for the site speciﬁc collections e(h, s, :, 1) and ep (h, s, :) at all WI sites for 1,3 and 5 hour horizons. [sent-243, score-0.572]
</p><p>75 For h = 1 our AR-LG(1) model beats persistence at all sites, usually by by multiple standard deviation units. [sent-245, score-0.224]
</p><p>76 This is a signiﬁcant result because persistence forecasts have been shown to be difﬁcult to improve upon for very short horizons. [sent-246, score-0.402]
</p><p>77 Although at most sites AR-LG(1) has improved further upon persistence accuracy, two sites display high variance and one (second from left) has high variance and very high RMSE near 3 m/s. [sent-248, score-0.696]
</p><p>78 At h = 5 high variance is widespread and the RMSE of the ill behaving sites at h = 3 have grown. [sent-249, score-0.236]
</p><p>79 This suggests that large erroneous predictions at a small number of sites spread throughout the system as it evolves forward in time. [sent-250, score-0.275]
</p><p>80 Next, we consider the performance of multiple regime models. [sent-251, score-0.23]
</p><p>81 While in WI there is no obvious trend from 2 to 5 regimes, in PNW there is a clear increase in performance as the number of regimes increases. [sent-255, score-0.18]
</p><p>82 (a) Mean wind vectors (u, v) at each of 21 sites in WI, in each of 5 regimes (regime indicated by shape). [sent-267, score-0.98]
</p><p>83 (b) Mean regime posteriors with respect to test-set hour-of-day (CST), showing diurnal trends. [sent-268, score-0.293]
</p><p>84 (c) Mean regime posteriors with respect to test-set month. [sent-269, score-0.252]
</p><p>85 Increase in forecast skill and test-set log-likelihood indicate that regimes in the multi-regime models are capturing important generalizable patterns in short-term wind dynamics, whose features ought to arise from underlying meteorology. [sent-270, score-0.821]
</p><p>86 Figure 5 shows an analysis of a ﬁve regime model trained on the WI dataset. [sent-272, score-0.23]
</p><p>87 Figure 5 (a) plots learned wind vectors in the ﬁrst time-slice between the ﬁve regimes. [sent-273, score-0.592]
</p><p>88 Figures 5 (b) and (c) analyze posterior regime likelihoods with respect to diurnal (daily) and seasonal time-frames. [sent-274, score-0.335]
</p><p>89 4  Conclusion  We have described a model for short-term wind forecasting (STWF), an important task in the wind power industry. [sent-276, score-1.285]
</p><p>90 Our model is set apart from previous STWF approaches in three important ways: Firstly, forecasts are informed by off-site evidence through a representation of the dynamical evolution of winds in the region. [sent-277, score-0.178]
</p><p>91 Secondly, our models can learn and reason about meteorological regimes unique to the local climate. [sent-278, score-0.265]
</p><p>92 Finally, our model is tolerant to missing data which is present in most sources of wind data. [sent-279, score-0.698]
</p><p>93 These points are shown empirically through an improvement in forecasting error versus state-of-the-art, and observation of meteorological properties of learned regimes. [sent-280, score-0.189]
</p><p>94 We presented novel approximate inference procedures that enables AR-HMMs to be gracefully used in situations with missing data. [sent-281, score-0.172]
</p><p>95 Calibrated probabilistic forecasting at the stateline wind energy center. [sent-306, score-0.716]
</p><p>96 Short-term prediction of wind farm power: A data mining approach. [sent-317, score-0.629]
</p><p>97 20% wind energy by 2030: Increasing wind energy’s contribution to U. [sent-335, score-1.174]
</p><p>98 Probabilistic forecasting of wind power at the minute time-scale with markov-switching autoregressive models. [sent-374, score-0.782]
</p><p>99 The economic value of day-ahead wind forecasts for power grid operations. [sent-379, score-0.793]
</p><p>100 Prediction of wind farm power ramp rates: A data-mining approach. [sent-425, score-0.653]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wind', 0.564), ('sites', 0.236), ('regime', 0.23), ('persistence', 0.224), ('pnw', 0.187), ('regimes', 0.18), ('forecasts', 0.178), ('rt', 0.172), ('wt', 0.17), ('rmse', 0.162), ('homo', 0.14), ('lerner', 0.14), ('missing', 0.134), ('clg', 0.125), ('stwf', 0.125), ('weather', 0.123), ('horizons', 0.109), ('forecasting', 0.106), ('hour', 0.105), ('horizon', 0.103), ('pr', 0.103), ('wi', 0.098), ('parr', 0.092), ('site', 0.074), ('kusiak', 0.062), ('meteorologically', 0.062), ('ncdc', 0.062), ('hours', 0.059), ('truncated', 0.056), ('meteorological', 0.055), ('power', 0.051), ('hourly', 0.05), ('lg', 0.05), ('persist', 0.05), ('observations', 0.05), ('forecast', 0.047), ('climatic', 0.047), ('errorbars', 0.047), ('gneiting', 0.047), ('mbe', 0.047), ('northwest', 0.047), ('pinson', 0.047), ('stwfs', 0.047), ('turbine', 0.047), ('wl', 0.047), ('energy', 0.046), ('ll', 0.042), ('diurnal', 0.041), ('lauritzen', 0.041), ('components', 0.04), ('autoregressive', 0.039), ('predictions', 0.039), ('inference', 0.038), ('farm', 0.038), ('geographical', 0.038), ('madsen', 0.038), ('posterior', 0.037), ('folds', 0.036), ('zheng', 0.035), ('pj', 0.035), ('post', 0.035), ('switching', 0.034), ('paci', 0.032), ('ar', 0.032), ('bmmf', 0.031), ('giebel', 0.031), ('lindenberg', 0.031), ('marti', 0.031), ('mvg', 0.031), ('nwp', 0.031), ('paroli', 0.031), ('piwko', 0.031), ('saskatchewan', 0.031), ('toscani', 0.031), ('models', 0.03), ('learned', 0.028), ('meters', 0.027), ('seasonal', 0.027), ('gaussian', 0.027), ('qj', 0.027), ('discarded', 0.027), ('prediction', 0.027), ('speed', 0.026), ('collections', 0.026), ('weiss', 0.025), ('ary', 0.025), ('years', 0.025), ('sequences', 0.025), ('gaussians', 0.024), ('caruana', 0.024), ('readings', 0.024), ('yanover', 0.024), ('publicly', 0.023), ('koller', 0.023), ('murphy', 0.023), ('height', 0.023), ('posteriors', 0.022), ('united', 0.022), ('ep', 0.022), ('minute', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="35-tfidf-1" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>Author: Chris Barber, Joseph Bockhorst, Paul Roebber</p><p>Abstract: Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. 1</p><p>2 0.22685438 <a title="35-tfidf-2" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>3 0.095354363 <a title="35-tfidf-3" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>4 0.091557175 <a title="35-tfidf-4" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>Author: Gergely Neu, Andras Antos, András György, Csaba Szepesvári</p><p>Abstract: We consider online learning in ﬁnite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and, assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of the new algorithm is O T 2/3 (ln T )1/3 , giving the ﬁrst rigorously proved regret bound for the problem. 1</p><p>5 0.067147538 <a title="35-tfidf-5" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>Author: Francesco Orabona, Koby Crammer</p><p>Abstract: We propose a general framework to online learning for classiﬁcation problems with time-varying potential functions in the adversarial setting. This framework allows to design and prove relative mistake bounds for any generic loss function. The mistake bounds can be specialized for the hinge loss, allowing to recover and improve the bounds of known online classiﬁcation algorithms. By optimizing the general bound we derive a new online classiﬁcation algorithm, called NAROW, that hybridly uses adaptive- and ﬁxed- second order information. We analyze the properties of the algorithm and illustrate its performance using synthetic dataset. 1</p><p>6 0.058594331 <a title="35-tfidf-6" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>7 0.056956206 <a title="35-tfidf-7" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>8 0.055630669 <a title="35-tfidf-8" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>9 0.053654462 <a title="35-tfidf-9" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>10 0.053370234 <a title="35-tfidf-10" href="./nips-2010-Copula_Processes.html">54 nips-2010-Copula Processes</a></p>
<p>11 0.052423693 <a title="35-tfidf-11" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>12 0.049569506 <a title="35-tfidf-12" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>13 0.046509944 <a title="35-tfidf-13" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>14 0.044466376 <a title="35-tfidf-14" href="./nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">188 nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>15 0.042390969 <a title="35-tfidf-15" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>16 0.042230897 <a title="35-tfidf-16" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>17 0.041949797 <a title="35-tfidf-17" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>18 0.040328823 <a title="35-tfidf-18" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>19 0.038773175 <a title="35-tfidf-19" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>20 0.037997503 <a title="35-tfidf-20" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, -0.016), (2, 0.004), (3, 0.014), (4, -0.026), (5, 0.036), (6, 0.018), (7, -0.004), (8, -0.064), (9, 0.024), (10, -0.023), (11, -0.047), (12, 0.09), (13, -0.037), (14, -0.013), (15, 0.004), (16, -0.017), (17, -0.001), (18, 0.072), (19, 0.05), (20, -0.114), (21, 0.095), (22, 0.066), (23, -0.051), (24, -0.006), (25, -0.007), (26, -0.045), (27, -0.018), (28, -0.104), (29, -0.029), (30, -0.062), (31, 0.081), (32, -0.062), (33, 0.098), (34, -0.109), (35, 0.015), (36, 0.089), (37, -0.119), (38, 0.098), (39, -0.105), (40, -0.145), (41, -0.148), (42, -0.001), (43, -0.004), (44, 0.117), (45, -0.151), (46, -0.045), (47, 0.034), (48, 0.021), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92879152 <a title="35-lsi-1" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>Author: Chris Barber, Joseph Bockhorst, Paul Roebber</p><p>Abstract: Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. 1</p><p>2 0.67034632 <a title="35-lsi-2" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>Author: Lauren Hannah, Warren Powell, David M. Blei</p><p>Abstract: In this paper we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation to take observations from the joint state-outcome distribution and use them to infer the optimal decision for a given query state s. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We examine two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show that in some cases Dirichlet process weights offer substantial beneﬁts over kernel based weights and more generally that nonparametric estimation methods provide good solutions to otherwise intractable problems. 1</p><p>3 0.45714158 <a title="35-lsi-3" href="./nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">188 nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>Author: Andrew Gelfand, Yutian Chen, Laurens Maaten, Max Welling</p><p>Abstract: The paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms. It is shown that both algorithms can be viewed as an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that, like CRFs or discriminative RBMs, make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classiﬁers such as the voted perceptron and the discriminative RBM. 1</p><p>4 0.4405807 <a title="35-lsi-4" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>Author: Byron Boots, Geoffrey J. Gordon</p><p>Abstract: We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identiﬁcation. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identiﬁcation (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD ﬁnds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difﬁcult optimal stopping problem. 1</p><p>5 0.4313482 <a title="35-lsi-5" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>Author: Nicholas Bartlett, Frank Wood, David Tax</p><p>Abstract: We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic ﬁnite automata (PDFA). We deﬁne and develop a sampler for a PDFA with an inﬁnite number of states which we call the probabilistic deterministic inﬁnite automata (PDIA). Posterior predictive inference in this model, given a ﬁnite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models. 1</p><p>6 0.41397655 <a title="35-lsi-6" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>7 0.40338016 <a title="35-lsi-7" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>8 0.39358938 <a title="35-lsi-8" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>9 0.38871178 <a title="35-lsi-9" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>10 0.38186783 <a title="35-lsi-10" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>11 0.36553425 <a title="35-lsi-11" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>12 0.35916337 <a title="35-lsi-12" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>13 0.35896361 <a title="35-lsi-13" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>14 0.35762262 <a title="35-lsi-14" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>15 0.35330302 <a title="35-lsi-15" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>16 0.34339809 <a title="35-lsi-16" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>17 0.33001259 <a title="35-lsi-17" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>18 0.32644057 <a title="35-lsi-18" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>19 0.32566875 <a title="35-lsi-19" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>20 0.32505909 <a title="35-lsi-20" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.051), (20, 0.379), (27, 0.071), (30, 0.045), (35, 0.019), (45, 0.18), (50, 0.06), (52, 0.026), (60, 0.022), (77, 0.027), (90, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73008353 <a title="35-lda-1" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>Author: Chris Barber, Joseph Bockhorst, Paul Roebber</p><p>Abstract: Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efﬁcient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). We introduce a simple approximate inference method for AR-HMMs, which we believe has applications in other problem domains. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes signiﬁcantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes. 1</p><p>2 0.59617823 <a title="35-lda-2" href="./nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">216 nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>Author: Oliver Williams, Frank Mcsherry</p><p>Abstract: We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy deﬁnition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We ﬁnd that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured. 1</p><p>3 0.49244833 <a title="35-lda-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.4910273 <a title="35-lda-4" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>5 0.49031842 <a title="35-lda-5" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>6 0.48957944 <a title="35-lda-6" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>7 0.48831868 <a title="35-lda-7" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>8 0.48821259 <a title="35-lda-8" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>9 0.48810893 <a title="35-lda-9" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>10 0.48781398 <a title="35-lda-10" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>11 0.48685065 <a title="35-lda-11" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>12 0.48670653 <a title="35-lda-12" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>13 0.48654777 <a title="35-lda-13" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>14 0.48634592 <a title="35-lda-14" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>15 0.48616639 <a title="35-lda-15" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>16 0.4860332 <a title="35-lda-16" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>17 0.48551342 <a title="35-lda-17" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>18 0.48503634 <a title="35-lda-18" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>19 0.48476821 <a title="35-lda-19" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>20 0.48441288 <a title="35-lda-20" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
