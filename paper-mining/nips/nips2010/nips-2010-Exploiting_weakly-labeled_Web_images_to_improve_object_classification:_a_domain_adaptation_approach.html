<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-86" href="#">nips2010-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</h1>
<br/><p>Source: <a title="nips-2010-86-pdf" href="http://papers.nips.cc/paper/4064-exploiting-weakly-labeled-web-images-to-improve-object-classification-a-domain-adaptation-approach.pdf">pdf</a></p><p>Author: Alessandro Bergamo, Lorenzo Torresani</p><p>Abstract: Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classiﬁers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data. In this paper we investigate and compare methods that learn image classiﬁers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few stronglylabeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classiﬁers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces signiﬁcant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classiﬁers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.</p><p>Reference: <a title="nips-2010-86-reference" href="../nips2010_reference/nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Exploiting weakly-labeled Web images to improve object classiﬁcation: a domain adaptation approach  Alessandro Bergamo Lorenzo Torresani Computer Science Department Dartmouth College Hanover, NH 03755, U. [sent-1, score-0.703]
</p><p>2 edu  Abstract Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. [sent-6, score-0.667]
</p><p>3 In order to address this shortcoming, in recent years several authors have proposed to learn object classiﬁers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. [sent-8, score-0.763]
</p><p>4 In this paper we investigate and compare methods that learn image classiﬁers by combining very few manually annotated examples (e. [sent-10, score-0.34]
</p><p>5 , 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. [sent-12, score-0.664]
</p><p>6 We cast this as a domain adaptation problem: given a few stronglylabeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classiﬁers yielding small generalization error on the target domain. [sent-13, score-1.651]
</p><p>7 Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces signiﬁcant recognition rate improvements over the best published results (e. [sent-14, score-0.449]
</p><p>8 , 65% better when using 5 labeled training examples per class) and that our classiﬁers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets. [sent-16, score-0.346]
</p><p>9 1 Introduction The last few years have seen a proliferation of human efforts to collect labeled image data sets for the purpose of training and evaluating visual recognition systems. [sent-17, score-0.422]
</p><p>10 Label information in these collections comes in different forms, ranging from simple object category labels to detailed semantic pixel-level segmentations. [sent-18, score-0.339]
</p><p>11 In order to increase the variety and the number of labeled object classes, a few authors have designed online games and appealing software tools encouraging common users to participate in these image annotation efforts [23, 30]. [sent-20, score-0.409]
</p><p>12 Despite the tremendous research contribution brought by such attempts, even the largest labeled image collections today [6] are limited to a number of classes that is at least one order of magnitude smaller than the number of object categories that humans can recognize [3]. [sent-21, score-0.566]
</p><p>13 In order to overcome this limitation and in an attempt to build classiﬁers for arbitrary object classes, several authors have proposed systems that learn from weakly-labeled Internet photos [10, 9, 29, 20]. [sent-22, score-0.413]
</p><p>14 Most of these approaches rely on keyword-based image search engines to retrieve image examples of speciﬁed object classes. [sent-23, score-0.672]
</p><p>15 Most prior work has attempted to address this problem by means of outlier rejection mechanisms discarding irrelevant images from the retrieved results. [sent-25, score-0.437]
</p><p>16 However, despite the dynamic research activity in this area, weakly-supervised approaches today still yield signiﬁcantly lower recognition accuracy than fully supervised object classiﬁers trained on clean data (see, e. [sent-26, score-0.39]
</p><p>17 In this paper we argue that the poor performance of models learned from weakly-labeled Internet data is not only due to undetected outliers contaminating the training data, but it is also a consequence of the statistical differences often present between Web images and the test data. [sent-29, score-0.488]
</p><p>18 Figure 1 shows sample images for some of the Caltech256 object categories versus the top six images retrieved by Bing using the class names as keywords1. [sent-30, score-0.773]
</p><p>19 Note that for most of the classes in ﬁgure 1 it is not clear a priori which are the “relevant” Internet images to be used for training until we compare them to the photos in the corresponding Caltech256 categories. [sent-37, score-0.539]
</p><p>20 In this paper we show that a few strongly-labeled examples from the test domain (e. [sent-38, score-0.308]
</p><p>21 a few Caltech256 images for the class of interest) are indeed sufﬁcient to disambiguate this relevancy problem and to model the distribution differences between the weakly-labeled Internet data and the test application data, so as to signiﬁcantly improve recognition performance on the test set. [sent-40, score-0.442]
</p><p>22 These techniques exploit ample availability of training data from a source domain to learn a model that works effectively in a related target domain for which only few training examples are available. [sent-42, score-1.127]
</p><p>23 More formally, let pt (X, Y ) and ps (X, Y ) be the distributions generating the target and the source data, respectively. [sent-43, score-0.551]
</p><p>24 The domain adaptation problem arises whenever pt (X, Y ) differs from ps (X, Y ). [sent-45, score-0.47]
</p><p>25 When such differences are relatively small, however, knowledge gained by analyzing data in the source domain may still yield valuable information to perform prediction for test target data. [sent-56, score-0.667]
</p><p>26 1  Note that image search results may have changed since these examples were captured. [sent-58, score-0.311]
</p><p>27 2  Caltech256  Bing  (a)  (b)  (c)  (d)  (e)  (f)  (g) Figure 1: Images in Caltech256 for several categories and top results retrieved by Bing image search for the corresponding keywords. [sent-59, score-0.399]
</p><p>28 3  2 Relationship to other methods Most of the prior work on learning visual models from image search has focused on the task of “cleaning up” Internet photos. [sent-61, score-0.259]
</p><p>29 [10], visual ﬁlters learned from image search were used to rerank photos on the basis of visual consistency. [sent-63, score-0.564]
</p><p>30 Subsequent approaches [2, 25, 20] have employed similar outlier rejection schemes to automatically construct clean(er) data sets of images for training and testing object classiﬁers. [sent-64, score-0.547]
</p><p>31 Even techniques aimed at learning explicit object classiﬁers from image search [9, 29] have identiﬁed outlier removal as the key-ingredient to improve recognition. [sent-65, score-0.425]
</p><p>32 In our paper we focus on another fundamental, yet largely ignored, aspect of the problem: we argue that the current poor performance of classiﬁcation models learned from the Web is due to the distribution differences between Internet photos and image test examples. [sent-66, score-0.472]
</p><p>33 To the best of our knowledge we propose the ﬁrst systematic empirical analysis of domain adaptation methods to address sample distribution differences in object categorization due to the use of weakly-labeled Web images as training data. [sent-67, score-0.895]
</p><p>34 [24] have also analyzed cross-domain adaptation of object classiﬁers. [sent-69, score-0.369]
</p><p>35 In computer vision, transfer learning has been applied to a wide range of problems including object categorization (see, e. [sent-72, score-0.284]
</p><p>36 The fundamental difference is that in domain adaptation we have a single task but different domains, i. [sent-89, score-0.356]
</p><p>37 As our approach relies on a mix of labeled and weakly-labeled images, it is loosely related to semisupervised methods for object classiﬁcation [15, 19]. [sent-92, score-0.277]
</p><p>38 1 Experimental setup Our objective is to evaluate domain adaptations methods on the task of object classiﬁcation, using photos from a human-labeled data set as target domain examples and images retrieved by a keywordbased image search engine as examples of the source domain. [sent-96, score-1.901]
</p><p>39 We used Caltech256 as the data set for the target domain since it is an established benchmark for object categorization and it contains a large number of classes (256) thus allowing us to average out performance variations due to especially easy or difﬁcult categories. [sent-97, score-0.669]
</p><p>40 From each class, we randomly sampled nT images as target training examples, and other mT images as target test examples. [sent-98, score-0.872]
</p><p>41 We formed the weakly-labeled source data by collecting the top nS images retrieved by Bing image search for each of the Caltech256 category text labels. [sent-99, score-0.831]
</p><p>42 Although it may have been possible to improve the relevancy of the image results for some of the classes by manually selecting less ambiguous search keywords, we chose to issue queries on the unchanged Caltech256 text class labels to avoid subjective alteration of the results. [sent-100, score-0.449]
</p><p>43 However, in order to ensure valid testing, we removed near duplicates of Caltech256 images from the source training set by a human-supervised process. [sent-101, score-0.497]
</p><p>44 2 Feature representation and classiﬁcation model In order to study the effect of large weakly-labeled training sets on object recognition performance, we need a baseline system that achieves good performance on object categorization and that supports efﬁcient learning and test evaluation. [sent-103, score-0.592]
</p><p>45 The descriptor measures the closeness of an image to a basis set of classes and can be used as an intermediate representation to learn classiﬁers for new classes. [sent-109, score-0.293]
</p><p>46 The basis classiﬁers of the classeme descriptor are learned from weakly-labeled data collected for a large and semantically broad set of attributes (the ﬁnal descriptor contains 2659 attributes). [sent-110, score-0.27]
</p><p>47 We use a binarized version of this descriptor obtained by thresholding to 0 the output of the attribute classiﬁers: this yields for each image a 2625-dimensional binary vector describing the predicted presence/absence of visual attributes in the photo. [sent-112, score-0.294]
</p><p>48 Object class recognition is traditionally formulated as a multiclass classiﬁcation problem: given a test image x, predict the class label y ∈ {1, . [sent-115, score-0.44]
</p><p>49 The k-th binary classiﬁer (distint guishing between class k and the other classes) is trained on a target training set Dk and a collection t s Dk of weakly-labeled source training examples. [sent-120, score-0.715]
</p><p>50 Dk is formed by aggregating the Caltech256 training images of all classes, using the data from the k-th class as positive examples and the data from t t the remaining classes as negative examples, i. [sent-121, score-0.498]
</p><p>51 Dk = {(f t , yi,k )}Nt where f t = f (xt ) denotes i i i i=1 the feature vector of the i-th image, Nt = (K · nt ) is the total number of images in the stronglyt labeled data set, and yi,k ∈ {−1, 1} is 1 iff example i belongs to class k. [sent-123, score-0.486]
</p><p>52 The source training s ns s set Dk = {f i,k }i=1 is the collection of ns images retrieved by Bing using the category name of the k-th class as keyword. [sent-124, score-1.294]
</p><p>53 As discussed in the next section, different methods will make different assumptions on the labels of the source examples. [sent-125, score-0.274]
</p><p>54 This choice is primarily motivated by the availability of several simple yet effective domain adaptation variants of SVM [5, 26], in addition to the aforementioned reasons of good performance and efﬁciency. [sent-127, score-0.389]
</p><p>55 4 Methods We now present the speciﬁc domain adaptation SVM algorithms. [sent-128, score-0.356]
</p><p>56 The hyperparameters C of all classiﬁers are selected so as to minimize the multiclass cross validation error on the target training data. [sent-130, score-0.526]
</p><p>57 1 Baselines: SVMs , SVMt , SVMs∪t We include in our evaluation three algorithms not based on domain adaptation and use them as comparative baselines. [sent-133, score-0.356]
</p><p>58 We indicate with SVMt a linear SVM learned exclusively from the target examples. [sent-134, score-0.275]
</p><p>59 SVMs denotes an SVM learned from the source examples using the one-versus-the-rest scheme and assuming no outliers are present in the image search results. [sent-135, score-0.663]
</p><p>60 SVMs∪t is a linear SVM trained on the union of the target and source examples. [sent-136, score-0.483]
</p><p>61 2 Mixture of source and target hypotheses: MIXSVM One of the simplest possible strategies for domain adaptation consists of using as ﬁnal classiﬁer a convex combination of the two SVM hypotheses learned independently from the source and target data. [sent-142, score-1.297]
</p><p>62 5  Let us represent the source and target multiclass hypotheses as vector-valued functions hs (f ) → RK , ht (f ) → RK , where the k-th outputs are the respective SVM scores for class k. [sent-144, score-0.705]
</p><p>63 The parameter β ∈ [0, 1] is determined via grid search by optimizing multiclass error on the target training set. [sent-151, score-0.453]
</p><p>64 Last, we learn the ﬁnal hypothesis ht using the entire target training set. [sent-153, score-0.393]
</p><p>65 3 Domain weighting: DWSVM Another straightforward yet popular domain adaptation approach is to train a classiﬁer using both the source and the target examples by weighting differently the two domains in the learning objective [5, 12, 4]. [sent-155, score-0.937]
</p><p>66 We follow the implementation proposed in [26] and weight the loss function values differently for the source and target examples by using two distinct SVM hyperparameters, Cs and Ct , encoding the relative importance of the two domains. [sent-156, score-0.555]
</p><p>67 The values of these hyperparameters are selected by minimizing the multiclass 5-fold cross validation error on the target training set. [sent-157, score-0.526]
</p><p>68 4 Feature augmentation: AUGSVM We denote with AUGSVM the domain adaptation method described in [5]. [sent-159, score-0.356]
</p><p>69 A linear SVM is then trained on the union of the feature-augmented source and target examples (using a single hyperparameter). [sent-161, score-0.601]
</p><p>70 5 Transductive learning: TSVM The previous methods implement different strategies to adjust the relative importance of the source and the training examples in the learning process. [sent-164, score-0.443]
</p><p>71 Unfortunately, in our practical problem this assumption is violated due to outliers and irrelevant results being present in the images retrieved by keyword search. [sent-166, score-0.454]
</p><p>72 The scalar parameter ρ deﬁnes the fraction of source examples that we expect to be positive and is tuned via cross validation. [sent-171, score-0.41]
</p><p>73 Figure 3: Manual annotation saving: the plot shows for a varying number of labeled examples given to TSVM the number of additional labeled images that would be needed by SVMt to achieve the same accuracy. [sent-173, score-0.47]
</p><p>74 prediction errors on both source and target data. [sent-174, score-0.437]
</p><p>75 Figure 2 shows the accuracy achieved by the different algorithms when using ns = 300 and a varying number of training target examples (nt ). [sent-189, score-0.723]
</p><p>76 The accuracy is measured as the average of the mean recognition rate per class, using mt = 25 test examples for each class. [sent-190, score-0.259]
</p><p>77 The best accuracy is achieved by the domain adaptation methods TSVM and DWSVM, which produce signiﬁcant improvements over the SVM trained using only target examples (SVMt ), particularly for small values of nt . [sent-191, score-0.967]
</p><p>78 For nt = 5, TSVM yields a 65% improvement over the best published results on this benchmark (for the same number of examples, an accuracy of 16. [sent-192, score-0.274]
</p><p>79 It is interesting to note that while using solely source training images yields very low accuracy (14. [sent-195, score-0.548]
</p><p>80 5% for SVMs ), adding even just a single labeled target image produces a signiﬁcant improvement (TSVM achieves 18. [sent-196, score-0.406]
</p><p>81 1% with nt = 5): this indicates that the method can indeed adapt the classiﬁer to work effectively on the target domain given a small amount of strongly-labeled data. [sent-198, score-0.558]
</p><p>82 It is interesting to note that while TSVM implements a form of outlier rejection as it solves for the labels of the source s examples, DWSVM assumes that all source images in Dk are positive examples for class k. [sent-199, score-0.953]
</p><p>83 Yet, DWSVM achieves results similar to those of TSVM: this suggests that domain adaptation rather than outlier rejection is the key-factor contributing to the improvement with respect to the baselines. [sent-200, score-0.464]
</p><p>84 By analyzing the performance of the baselines in ﬁgure 2 we observe that training exclusively with Web images (SVMs ) yields much lower accuracy than using strongly-labeled data (SVMt ): this is consistent with prior work [9, 29]. [sent-201, score-0.354]
</p><p>85 Furthermore, the poor accuracy of SVMs∪t compared to SVMt suggests that na¨vely adding a large number of source examples to the target training set without ı consideration of the domain differences not only does not help but actually worsens the recognition. [sent-202, score-0.937]
</p><p>86 Figure 3 illustrates the signiﬁcant manual annotation saving produced by our approach: the x-axis is the number of target labeled images provided to TSVM while the y-axis shows the number of additional labeled examples that would be needed by SVMt to achieve the same accuracy. [sent-203, score-0.706]
</p><p>87 10  15 20 25 30 t Number of target training images (n )  35  40  Figure 5: Training time: time needed to learn a multiclass classiﬁer for Caltech256 using TSVM. [sent-205, score-0.591]
</p><p>88 We notice that the performance of the SVM trained only on source images (SVMs ) peaks at ns = 100 and decreases monotonically after this value. [sent-208, score-0.709]
</p><p>89 This result can be explained by observing that image search engines provide images sorted according to estimated relevancy with respect to the keyword. [sent-209, score-0.487]
</p><p>90 It is conceivable to assume that images far down in the ranking list will often tend to be outliers, which may lead to degradation of recognition particularly for non-robust models. [sent-210, score-0.263]
</p><p>91 Contrast these robust performances with the accuracy of SVMs∪t , which grows as we begin adding source examples but then decays rapidly after ns = 10 and approaches the poor recognition of SVMs for large values of ns . [sent-212, score-1.017]
</p><p>92 Our approach compares very favorably with competing algorithms also in terms of computational complexity: training TSVM (without cross validation) on Caltech256 with nt = 5 and ns = 300 takes 84 minutes on a AMD Opteron Processor 280 2. [sent-213, score-0.601]
</p><p>93 4GHz; training the multiclass method of [13] using 5 labeled examples per class takes about 23 hours on the same machine (for fairness of comparison, we excluded cross validation even for this method). [sent-214, score-0.531]
</p><p>94 A detailed analysis of training time as a function of the number of labeled training examples is reported in ﬁgure 5. [sent-215, score-0.376]
</p><p>95 6 Discussion and future work In this work we have investigated the application of domain adaptation methods to object categorization using Web photos as source data. [sent-218, score-1.031]
</p><p>96 Our analysis indicates that, while object classiﬁers learned exclusively from Web data are inferior to fully-supervised models, the use of domain adaptation methods to combine Web photos with small amounts of strongly labeled data leads to state-of-theart results. [sent-219, score-0.883]
</p><p>97 Future work will include application of our approach to combine data from multiple source domains (e. [sent-221, score-0.259]
</p><p>98 , images obtained from different search engines or photo sharing sites) and different media (e. [sent-223, score-0.295]
</p><p>99 Additional material including software and our source training data may be obtained from [1]. [sent-226, score-0.325]
</p><p>100 An empirical analysis of domain adaptation o a algorithms for genomic sequence analysis. [sent-415, score-0.356]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tsvm', 0.321), ('bing', 0.263), ('ns', 0.258), ('source', 0.233), ('photos', 0.207), ('target', 0.204), ('adaptation', 0.194), ('svmt', 0.192), ('nt', 0.192), ('object', 0.175), ('images', 0.172), ('domain', 0.162), ('retrieved', 0.157), ('dwsvm', 0.15), ('image', 0.128), ('svm', 0.124), ('examples', 0.118), ('internet', 0.117), ('svms', 0.106), ('web', 0.096), ('classi', 0.094), ('ers', 0.094), ('multiclass', 0.092), ('training', 0.092), ('dk', 0.089), ('outliers', 0.087), ('augsvm', 0.085), ('mixsvm', 0.085), ('category', 0.076), ('labeled', 0.074), ('classes', 0.068), ('visual', 0.066), ('descriptor', 0.066), ('ht', 0.066), ('search', 0.065), ('relevancy', 0.064), ('recognition', 0.062), ('categorization', 0.06), ('transductive', 0.06), ('cross', 0.059), ('ct', 0.059), ('ps', 0.059), ('fergus', 0.059), ('engines', 0.058), ('outlier', 0.057), ('pt', 0.055), ('torresani', 0.052), ('accuracy', 0.051), ('rejection', 0.051), ('categories', 0.049), ('transfer', 0.049), ('validation', 0.048), ('class', 0.048), ('collections', 0.047), ('trained', 0.046), ('classeme', 0.043), ('polysemy', 0.043), ('unequal', 0.043), ('keywords', 0.042), ('cvpr', 0.041), ('labels', 0.041), ('differences', 0.04), ('er', 0.04), ('exclusively', 0.039), ('keyword', 0.038), ('saenko', 0.038), ('szummer', 0.038), ('poor', 0.037), ('hypotheses', 0.035), ('manually', 0.035), ('lorenzo', 0.034), ('holub', 0.034), ('traditionally', 0.034), ('attributes', 0.034), ('availability', 0.033), ('saving', 0.032), ('annotation', 0.032), ('learned', 0.032), ('published', 0.031), ('learn', 0.031), ('hyperparameters', 0.031), ('despite', 0.031), ('sindhwani', 0.031), ('augmentation', 0.031), ('meanings', 0.031), ('eccv', 0.03), ('google', 0.03), ('semantically', 0.029), ('supervision', 0.029), ('degradation', 0.029), ('hyperparameter', 0.029), ('gure', 0.028), ('test', 0.028), ('annotated', 0.028), ('loosely', 0.028), ('hs', 0.027), ('iccv', 0.026), ('domains', 0.026), ('today', 0.025), ('shift', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="86-tfidf-1" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani</p><p>Abstract: Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classiﬁers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data. In this paper we investigate and compare methods that learn image classiﬁers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few stronglylabeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classiﬁers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces signiﬁcant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classiﬁers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.</p><p>2 0.19348519 <a title="86-tfidf-2" href="./nips-2010-Co-regularization_Based_Semi-supervised_Domain_Adaptation.html">47 nips-2010-Co-regularization Based Semi-supervised Domain Adaptation</a></p>
<p>Author: Abhishek Kumar, Avishek Saha, Hal Daume</p><p>Abstract: This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in E ASYA DAPT (EA) [1]) and harnesses unlabeled data in target domain to further assist the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in terms of Rademacher complexity) of EA and EA++ show that the hypothesis class of EA++ has lower complexity (compared to EA) and hence results in tighter generalization bounds. Experimental results on sentiment analysis tasks reinforce our theoretical ﬁndings and demonstrate the efﬁcacy of the proposed method when compared to EA as well as few other representative baseline approaches.</p><p>3 0.18871294 <a title="86-tfidf-3" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>4 0.16558853 <a title="86-tfidf-4" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>5 0.15563497 <a title="86-tfidf-5" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>6 0.15014175 <a title="86-tfidf-6" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>7 0.1298144 <a title="86-tfidf-7" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>8 0.12641785 <a title="86-tfidf-8" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>9 0.12044764 <a title="86-tfidf-9" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>10 0.11216748 <a title="86-tfidf-10" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>11 0.1082212 <a title="86-tfidf-11" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>12 0.10027369 <a title="86-tfidf-12" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>13 0.092935063 <a title="86-tfidf-13" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>14 0.092660017 <a title="86-tfidf-14" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>15 0.092310116 <a title="86-tfidf-15" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>16 0.088384204 <a title="86-tfidf-16" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>17 0.087351121 <a title="86-tfidf-17" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>18 0.085422225 <a title="86-tfidf-18" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>19 0.083467394 <a title="86-tfidf-19" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>20 0.08320266 <a title="86-tfidf-20" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.242), (1, 0.109), (2, -0.09), (3, -0.264), (4, 0.058), (5, 0.04), (6, -0.071), (7, -0.056), (8, 0.017), (9, -0.019), (10, 0.05), (11, 0.072), (12, 0.004), (13, -0.017), (14, 0.038), (15, 0.054), (16, 0.024), (17, -0.035), (18, 0.034), (19, 0.008), (20, -0.028), (21, 0.062), (22, 0.038), (23, 0.002), (24, -0.05), (25, -0.031), (26, -0.006), (27, 0.048), (28, 0.125), (29, -0.029), (30, 0.031), (31, 0.138), (32, -0.11), (33, -0.035), (34, 0.084), (35, 0.037), (36, -0.034), (37, 0.22), (38, 0.045), (39, -0.051), (40, 0.014), (41, -0.078), (42, -0.046), (43, 0.045), (44, -0.041), (45, 0.049), (46, -0.046), (47, -0.08), (48, 0.09), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95843375 <a title="86-lsi-1" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani</p><p>Abstract: Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classiﬁers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data. In this paper we investigate and compare methods that learn image classiﬁers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few stronglylabeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classiﬁers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces signiﬁcant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classiﬁers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.</p><p>2 0.74514836 <a title="86-lsi-2" href="./nips-2010-Co-regularization_Based_Semi-supervised_Domain_Adaptation.html">47 nips-2010-Co-regularization Based Semi-supervised Domain Adaptation</a></p>
<p>Author: Abhishek Kumar, Avishek Saha, Hal Daume</p><p>Abstract: This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in E ASYA DAPT (EA) [1]) and harnesses unlabeled data in target domain to further assist the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in terms of Rademacher complexity) of EA and EA++ show that the hypothesis class of EA++ has lower complexity (compared to EA) and hence results in tighter generalization bounds. Experimental results on sentiment analysis tasks reinforce our theoretical ﬁndings and demonstrate the efﬁcacy of the proposed method when compared to EA as well as few other representative baseline approaches.</p><p>3 0.66164321 <a title="86-lsi-3" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>4 0.64750248 <a title="86-lsi-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>5 0.63063139 <a title="86-lsi-5" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>6 0.60744303 <a title="86-lsi-6" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>7 0.60666299 <a title="86-lsi-7" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>8 0.57892436 <a title="86-lsi-8" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>9 0.57526612 <a title="86-lsi-9" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>10 0.54506636 <a title="86-lsi-10" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>11 0.52942008 <a title="86-lsi-11" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>12 0.52907956 <a title="86-lsi-12" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>13 0.51740128 <a title="86-lsi-13" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>14 0.51738828 <a title="86-lsi-14" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>15 0.51601553 <a title="86-lsi-15" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>16 0.5108577 <a title="86-lsi-16" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>17 0.50067878 <a title="86-lsi-17" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>18 0.49535832 <a title="86-lsi-18" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>19 0.49078155 <a title="86-lsi-19" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>20 0.48783618 <a title="86-lsi-20" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.152), (13, 0.031), (17, 0.019), (27, 0.063), (30, 0.041), (35, 0.036), (45, 0.232), (50, 0.06), (52, 0.044), (59, 0.018), (60, 0.024), (77, 0.073), (78, 0.037), (90, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95149308 <a title="86-lda-1" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>2 0.92769694 <a title="86-lda-2" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>Author: Chang Su, Sargur Srihari</p><p>Abstract: A method for computing the rarity of latent ﬁngerprints represented by minutiae is given. It allows determining the probability of ﬁnding a match for an evidence print in a database of n known prints. The probability of random correspondence between evidence and database is determined in three procedural steps. In the registration step the latent print is aligned by ﬁnding its core point; which is done using a procedure based on a machine learning approach based on Gaussian processes. In the evidence probability evaluation step a generative model based on Bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the conﬁdence of their presence in the evidence. In the speciﬁc probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a speciﬁc birthday. The generative model is validated using a goodness-of-ﬁt test evaluated with a standard database of ﬁngerprints. The probability of random correspondence for several latent ﬁngerprints are evaluated for varying numbers of minutiae. 1</p><p>same-paper 3 0.88168836 <a title="86-lda-3" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani</p><p>Abstract: Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classiﬁers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data. In this paper we investigate and compare methods that learn image classiﬁers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few stronglylabeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classiﬁers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces signiﬁcant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classiﬁers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.</p><p>4 0.85193491 <a title="86-lda-4" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>5 0.85002768 <a title="86-lda-5" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>6 0.8498612 <a title="86-lda-6" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>7 0.84515721 <a title="86-lda-7" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>8 0.8421914 <a title="86-lda-8" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>9 0.83941931 <a title="86-lda-9" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>10 0.839257 <a title="86-lda-10" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>11 0.8384738 <a title="86-lda-11" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>12 0.83734477 <a title="86-lda-12" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>13 0.8367672 <a title="86-lda-13" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>14 0.83630711 <a title="86-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.83617008 <a title="86-lda-15" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>16 0.83465302 <a title="86-lda-16" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>17 0.83445847 <a title="86-lda-17" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>18 0.83396691 <a title="86-lda-18" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>19 0.83329982 <a title="86-lda-19" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>20 0.83283812 <a title="86-lda-20" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
