<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-189" href="#">nips2010-189</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</h1>
<br/><p>Source: <a title="nips-2010-189-pdf" href="http://papers.nips.cc/paper/3922-on-a-connection-between-importance-sampling-and-the-likelihood-ratio-policy-gradient.pdf">pdf</a></p><p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>Reference: <a title="nips-2010-189-reference" href="../nips2010_reference/nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. [sent-3, score-1.001]
</p><p>2 We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. [sent-4, score-1.27]
</p><p>3 We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. [sent-6, score-1.834]
</p><p>4 Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. [sent-7, score-0.991]
</p><p>5 In this paper we describe a novel connection between likelihood ratio based policy gradient methods and importance sampling. [sent-12, score-1.219]
</p><p>6 Speciﬁcally, we show that the likelihood ratio policy gradient estimate is equivalent to the gradient of an importance sampled estimate of the expected return function estimated using only data from the current policy. [sent-13, score-1.684]
</p><p>7 This insight indicates that likelihood ratio policy gradients are quite naive in terms of data use, and suggests an opportunity for novel algorithms which use all past data more efﬁciently by working with the importance sampled expected return function directly. [sent-14, score-1.384]
</p><p>8 First, we develop algorithms for global search over the importance sampled expected return function, allowing us to make more progress for a given amount of experience. [sent-16, score-0.584]
</p><p>9 Our approach uses estimates of the importance sampling variance to constrain the search in a principled way. [sent-17, score-0.475]
</p><p>10 Second, we derive generalizations of optimal policy gradient baselines which are applicable to the importance sampled expected return function. [sent-18, score-1.438]
</p><p>11 Section 2 describes preliminaries on Markov decision processes (MDPs), policy gradient methods and importance sampling. [sent-19, score-1.022]
</p><p>12 Section 3 describes the novel connection between importance sampling and likelihood ratio policy gradients, and Section 4 examines our novel minimum variance baselines. [sent-20, score-1.258]
</p><p>13 A policy π is a mapping from states S to a probability distribution over the set of actions A. [sent-26, score-0.647]
</p><p>14 We denote the expected return of a policy πθ by H t=0  U (θ) = EP (τ ;θ)  γ t R(st , ut )|πθ =  τ  P (τ ; θ)R(τ ). [sent-28, score-0.898]
</p><p>15 1)  Here P (τ ; θ) is the probability distribution induced by the policy πθ over all possible state-action traH jectories τ = (s0 , u0 , s1 , u1 , . [sent-30, score-0.647]
</p><p>16 Likelihood ratio policy gradient methods perform a (stochastic) gradient ascent over the policy parameter space Θ to ﬁnd a local optimum of U (θ). [sent-36, score-1.728]
</p><p>17 Hence no access to a dynamics model is required θ log P (τ to compute an unbiased estimate of the policy gradient. [sent-42, score-0.86]
</p><p>18 This can be readily leveraged to estimate the expected return of a stochastic policy [10] as follows: U (θ) =  1 m  m P (τ (i) ;θ) (i) ), i=1 Q(τ (i) ) R(τ  τ (i) ∼ Q  (2. [sent-58, score-0.914]
</p><p>19 If we choose Q(τ ) = P (τ ; θ ), then we are estimating the return of a policy πθ from sample paths obtained from acting according to a policy πθ . [sent-60, score-1.488]
</p><p>20 2  3  Likelihood Ratio Policy Gradient via Importance Sampling  We now outline a novel connection between policy gradients and importance sampling. [sent-64, score-0.91]
</p><p>21 , τ (m) } sampled from policy πθ∗ induces a distribution over paths Q(τ ) = P (τ ; θ∗ ). [sent-68, score-0.72]
</p><p>22 Let U (θ∗ ) denote the importance sampled estimate of U (θ) at θ∗ . [sent-69, score-0.301]
</p><p>23 1 is the j’th entry of the likelihood ratio based estimate of the gradient of U (θ) at θ∗ . [sent-74, score-0.388]
</p><p>24 Instead of only using local information from a single policy to drive our learning, we can use global information provided by U (θ) using trials run under all past policies. [sent-77, score-0.902]
</p><p>25 Such importance sampling based methods (as have been proposed in [10]) should be able to learn from fewer trial runs than the currently widely popular likelihood ratio based methods. [sent-78, score-0.456]
</p><p>26 The observation that past rewards do not depend on future states or actions is leveraged by the G(PO)MDP [8] and the Policy Gradient Theorem [11] variations on REINFORCE to reduce the variance on their gradient estimates. [sent-80, score-0.391]
</p><p>27 1), but our generalization of baselines, and our policy search algorithm are equally applicable when using the expression for U (θ) we present in Equation (3. [sent-85, score-0.771]
</p><p>28 4  Generalized Unbiased Baselines  Previous work has shown that the REINFORCE gradient estimate beneﬁts greatly from the addition of an optimal baseline term [12, 9, 8]. [sent-87, score-0.403]
</p><p>29 In this section, we show that policy gradient baselines are special cases of a more general variance reduction technique. [sent-88, score-1.072]
</p><p>30 Setting the gradient of the ﬁrst term with respect to b equal to zero yields the minimum variance baseline b = E Pθ [  θ  log Pθ (X)  θ  log Pθ (X) ]−1 EPθ [  θ  log Pθ (X)h(X)]. [sent-96, score-0.574]
</p><p>31 1)  The baselines commonly employed with REINFORCE, GPOMDP, and other likelihood ratio policy gradient methods can be derived as special cases of this generalized baseline [12]. [sent-98, score-1.401]
</p><p>32 For each entry in h(X) we can compute a minimum variance baseline vector b using Equation (4. [sent-104, score-0.299]
</p><p>33 Indeed, in the case of REINFORCE we would obtain a baseline matrix, rather than a baseline scalar (as in the original work [7]) and rather than a vector baseline (as described in later work, such as [12]). [sent-108, score-0.658]
</p><p>34 Using standard policy gradient methods, it can be impractical to run enough trials to accurately ﬁt such baselines. [sent-110, score-0.96]
</p><p>35 By using importance sampling to reuse data we can use richer baseline terms in our estimators. [sent-111, score-0.498]
</p><p>36 It is possible to recursively insert minimum variance unbiased baseline terms into these expectations in order to reduce the variance on the baseline estimates. [sent-114, score-0.675]
</p><p>37 It uses importance sampling with optimal generalized baselines to obtain estimates U (θ) of the expected return function based on the data gathered so far. [sent-120, score-0.7]
</p><p>38 It maintains a list of candidate policy parameters from which it searches for improvements. [sent-122, score-0.67]
</p><p>39 Memory-based search allows backtracking away from unpromising parts of the search space without taking additional, costly trials on the real platform. [sent-123, score-0.367]
</p><p>40 Input: domain of policy parameters Θ, initial policy πθ0 ˆ for i = 0 to . [sent-124, score-1.318]
</p><p>41 Update policy: θi+1 = arg maxθj U (θj ) end for Figure 1: Our policy search algorithm. [sent-130, score-0.771]
</p><p>42 Estimate of Expected Returns: We use weighted importance sampling, and add a baseline to Equation (2. [sent-131, score-0.4]
</p><p>43 2) we get the following sample based estimate of the optimal baseline b for the estimate of the expected return function:2 b=  m i=1  1 m 1 m  P (τ (i) ;θ) Q(τ (i) )  m i=1  −1  2  P (τ (i) ;θ) Q(τ (i) )  θ  log Pθ (τ (i) )  2 θ  θ  log P (τ (i) ; θ)  log P (τ (i) ; θ)R(τ (i) )) . [sent-135, score-0.569]
</p><p>44 2)  ESS Search Region: As our policy search steps away from areas of Θ where we have gathered sample data, the variance of our estimator U increases and our function estimate becomes unrelim able. [sent-137, score-0.937]
</p><p>45 The effective sample size ESS = 1+Var(wi ) is commonly used to measure the quality of an importance sampled estimate [13]. [sent-138, score-0.301]
</p><p>46 Our policy search only considers parameter values θ with sufﬁciently high ESS. [sent-140, score-0.771]
</p><p>47 Step Direction: We use the ﬁnite-difference gradient of U as the step direction for the inner loop of the policy search. [sent-141, score-0.866]
</p><p>48 4 For traditional likelihood ratio policy search methods this would require additional trials. [sent-145, score-0.948]
</p><p>49 By contrast, no new trials are required when using importance sampling. [sent-146, score-0.327]
</p><p>50 In contrast to Sutton’s DYNA, our method attempts to directly optimize the expected return function by varying policy parameters rather than building a model for the environment. [sent-150, score-0.85]
</p><p>51 Cao [17] also uses importance sampling to reuse past data for estimating policy gradients, but focuses on estimating local gradient information rather than global surface information. [sent-151, score-1.301]
</p><p>52 The work of Peshkin and Shelton [10] is most similar in spirit to our policy search method. [sent-152, score-0.771]
</p><p>53 They use importance sampling to construct a “proxy” environment from sampled data which can be used to evaluate the expected return at arbitrary policies. [sent-153, score-0.509]
</p><p>54 They apply a hill-climbing policy search to this “proxy” surface. [sent-154, score-0.771]
</p><p>55 This technique does not use estimates of the importance sampling variance to restrict the search, does not use generalized minimum variance baselines, and does not use memory. [sent-155, score-0.515]
</p><p>56 Our experiments show that these improvements are necessary to outperform standard policy gradient methods across our test domains. [sent-156, score-0.814]
</p><p>57 Our general approach of estimating and optimizing the expected return function instead of the gradient of the expected return function allows for non-local policy steps. [sent-157, score-1.201]
</p><p>58 Recent EM-based policy search methods [18, 14] are able to make larger steps by optimizing a local lower bound on the expected return function. [sent-158, score-0.952]
</p><p>59 PEGASUS [19] is an efﬁcient alternative policy search method but can only be used if a simulation model is available. [sent-162, score-0.771]
</p><p>60 The natural gradient approach is a parameterization invariant second order method which ﬁnds the direction which 2 Estimating the baseline from the same data as the other terms in Equation (5. [sent-164, score-0.389]
</p><p>61 This is often done in policy gradient methods and we do so in our experiments. [sent-166, score-0.814]
</p><p>62 Other step directions or policy improvement rules may be substituted for the ﬁnite difference gradient step. [sent-169, score-0.836]
</p><p>63 For example, we could follow the natural gradient direction, or use an EM-based policy update [14]. [sent-170, score-0.814]
</p><p>64 5 We can extend standard likelihood ratio policy gradient methods to use the importance sampled expected return estimate. [sent-173, score-1.429]
</p><p>65 We have a matrix baseline (MAT), and a recursive baseline (REC). [sent-176, score-0.431]
</p><p>66 The algorithms considered are the GPOMDP likelihood ratio policy gradient method (GP), GPOMDP with importance sampling (ISGP), Peshkin and Shelton’s algorithm (PS), and our approach (OUR). [sent-179, score-1.27]
</p><p>67 Our approach exploits a similar intuition through consideration of variance through the effective sampling size (ESS)—preferring regions for which the past experience gives a good estimate. [sent-181, score-0.294]
</p><p>68 In the episodic setting, which we consider in this paper, the only difference between episodic NAC and natural gradient is in the estimate of the baseline. [sent-183, score-0.323]
</p><p>69 For each testbed we randomly generated a pool of initial policies until one is found that does not achieve the worst case return We then used our policy gradient algorithms to optimize performance. [sent-190, score-1.067]
</p><p>70 We focus on an analysis of performance when only allowed for a small number of trials: In each of the following experiments we run 50 iterations of policy search, running M trials for each policy at each iteration. [sent-192, score-1.44]
</p><p>71 8  Experimental Results  In our experimental results, we ﬁrst evaluate several generalized baselines in the context of our policy search algorithm. [sent-193, score-0.989]
</p><p>72 Our policy search outperform likelihood ratio methods on two of the testbeds and performs equally well on the two remaining ones. [sent-195, score-1.041]
</p><p>73 Generalized Baseline Experiments: There are a variety of choices in our generalized baseline technique: We can vary the dimensionality of the baseline terms to add, the depth of the recursive baseline, and what (if any) regularization to use. [sent-200, score-0.463]
</p><p>74 We implemented our policy search using three different baseline techniques. [sent-201, score-0.963]
</p><p>75 Figure 2 (a) shows the average reward received plotted against the number of trials run for the matrix (MAT) and recursive tensor (REC) baselines. [sent-203, score-0.294]
</p><p>76 The matrix baseline outperforms the other baselines and we use it going forward. [sent-205, score-0.378]
</p><p>77 6  (a)  (b)  (c)  Figure 3: This ﬁgure demonstrates the effect of (a) memory based search (b) optimal baselines, and (c) ESS search region on cartpole performance. [sent-208, score-0.401]
</p><p>78 In addition, we show the performance with memory only (PS+M), baselines only (PS+B), and ESS only (PS+E), and our approach with memory (OUR-M), baselines (OUR-B), and ESS (OUR-E) removed. [sent-210, score-0.45]
</p><p>79 Comparison With Likelihood Ratio Policy Gradients: We have compared several episodic likelihood ratio algorithms against our global policy search algorithm. [sent-218, score-1.026]
</p><p>80 For the likelihood ratio algorithms, we use the appropriate optimal baselines [12] and hand-tune the step size. [sent-220, score-0.385]
</p><p>81 As a comparison, we have also implemented policy gradient algorithms which use importance sampling to estimate the gradient of U . [sent-221, score-1.304]
</p><p>82 We plot our global search approach against GPOMDP, an importance sampled GPOMDP (IS GPOMDP), and an implementation of Peshkin and Shelton’s global search. [sent-223, score-0.425]
</p><p>83 7 Our approach is consistently able to improve its initial policy, outperforming likelihood ratio policy gradient methods on both the cartpole and LQR testbeds. [sent-224, score-1.129]
</p><p>84 In general, importance sampling based methods outperform non-importance sampling based algorithms, which work poorly when given few trials. [sent-225, score-0.35]
</p><p>85 9  Conclusion  We have shown that policy gradient methods are a special case of gradient descent over the importance sampled expected return function U . [sent-227, score-1.419]
</p><p>86 Since our approach provides a full approximation of the expected return function, we can use global information in addition to gradient information to achieve faster learning. [sent-228, score-0.37]
</p><p>87 We have also shown that optimal baselines for standard policy gradient methods can be seen as special cases of a more general variance reduction technique. [sent-229, score-1.072]
</p><p>88 Our importance sampling approach allows us to leverage more data to ﬁt generalized baseline terms in our estimators. [sent-230, score-0.503]
</p><p>89 Our experiments show our algorithm requires fewer trials than current policy gradient methods on several testbeds and no more trials on the remaining testbeds, making it appealing for robotic learning tasks for which trials are expensive. [sent-231, score-1.287]
</p><p>90 Following the formulation given in [24], our control input is drawn from the policy ˙ u ∼ N (K x, σ), with state x = [x, x, θ, θ] and policy parameters K = [K1 , K2 , K3 , K4 , σ]. [sent-250, score-1.318]
</p><p>91 ˙ ¨ ˙ ˙ 2 sin F −mp l(θ cos θ−θ 2 sin θ) ¨ g sin θ(mc +mp )−(ut +mp lθ 2 θ) cos θ . [sent-251, score-0.359]
</p><p>92 We run each episode for 200 time steps, though the episode terminates once the cartpole has failed (deﬁned as whenever |x| > 2. [sent-260, score-0.321]
</p><p>93 The task involves two states [x, x] and ˙ three policy parameters [K1 , K2 , σ]. [sent-266, score-0.647]
</p><p>94 Our parameterized policy is given by π(ut = 1|xt , xt ) = P (K1 sign(xt )x2 + K2 + t < xt ), ˙ ˙ ˙t where t ∼ N (0, σ). [sent-268, score-0.755]
</p><p>95 Our control input is drawn from the policy u ∼ N (Lx + K φ(x), σ). [sent-282, score-0.671]
</p><p>96 Each episode is run for 400 time steps, though the episode terminates once the acrobot has failed (deﬁned as whenever the height of the second link t = − cos(θ1 ) − cos(θ1 + θ3 ) < 0. [sent-293, score-0.312]
</p><p>97 8 We followed standard formulations of the control policy for LQR and cartpole. [sent-296, score-0.671]
</p><p>98 All policies are designed as functions of a linear combination of the policy parameters and hand-selected features. [sent-297, score-0.695]
</p><p>99 Pegasus: A policy search method for large mdps and pomdps. [sent-389, score-0.771]
</p><p>100 Evaluation of policy gradient methods and variants on the cart-pole benchmark. [sent-409, score-0.814]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.647), ('importance', 0.208), ('gpomdp', 0.195), ('baseline', 0.192), ('baselines', 0.186), ('gradient', 0.167), ('ess', 0.165), ('reinforce', 0.163), ('return', 0.125), ('search', 0.124), ('trials', 0.119), ('cartpole', 0.114), ('lqr', 0.111), ('peshkin', 0.111), ('cos', 0.109), ('ep', 0.107), ('acrobot', 0.105), ('ratio', 0.1), ('testbeds', 0.093), ('unbiased', 0.089), ('past', 0.087), ('reinforcement', 0.087), ('ps', 0.081), ('mp', 0.08), ('likelihood', 0.077), ('shelton', 0.075), ('variance', 0.072), ('sampling', 0.071), ('ut', 0.07), ('experience', 0.064), ('mdp', 0.064), ('episode', 0.057), ('peters', 0.057), ('episodic', 0.056), ('testbed', 0.056), ('isgp', 0.056), ('mountaincar', 0.056), ('expected', 0.056), ('xt', 0.054), ('sampled', 0.049), ('policies', 0.048), ('sin', 0.047), ('recursive', 0.047), ('mc', 0.047), ('nac', 0.045), ('dynamics', 0.044), ('estimate', 0.044), ('terminates', 0.044), ('leveraged', 0.042), ('reward', 0.041), ('armijo', 0.04), ('memory', 0.039), ('scalar', 0.038), ('horizon', 0.038), ('jan', 0.038), ('dyna', 0.037), ('pegasus', 0.037), ('log', 0.036), ('minimum', 0.035), ('gradients', 0.035), ('car', 0.034), ('generalized', 0.032), ('st', 0.032), ('plotted', 0.032), ('gj', 0.031), ('direction', 0.03), ('rec', 0.03), ('eq', 0.029), ('gp', 0.028), ('estimator', 0.028), ('po', 0.028), ('examines', 0.028), ('lx', 0.028), ('robotics', 0.028), ('tensor', 0.028), ('run', 0.027), ('reuse', 0.027), ('jie', 0.027), ('defense', 0.025), ('mat', 0.025), ('sutton', 0.025), ('estimating', 0.025), ('technique', 0.025), ('paths', 0.024), ('control', 0.024), ('mountain', 0.024), ('initial', 0.024), ('rewards', 0.023), ('searches', 0.023), ('robotic', 0.023), ('tang', 0.023), ('expectations', 0.023), ('step', 0.022), ('failed', 0.022), ('gathered', 0.022), ('global', 0.022), ('rather', 0.022), ('equation', 0.021), ('connection', 0.02), ('acting', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="189-tfidf-1" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>2 0.44196939 <a title="189-tfidf-2" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present policy gradient results within the framework of linearly-solvable MDPs. For the ﬁrst time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the ﬁrst compatible function approximators and natural policy gradients for continuous-time stochastic systems.</p><p>3 0.42001817 <a title="189-tfidf-3" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>4 0.39982334 <a title="189-tfidf-4" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>5 0.37368959 <a title="189-tfidf-5" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>6 0.35325781 <a title="189-tfidf-6" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>7 0.35056046 <a title="189-tfidf-7" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>8 0.29490611 <a title="189-tfidf-8" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>9 0.24055596 <a title="189-tfidf-9" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>10 0.23594207 <a title="189-tfidf-10" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>11 0.21919382 <a title="189-tfidf-11" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>12 0.18870954 <a title="189-tfidf-12" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>13 0.18774842 <a title="189-tfidf-13" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>14 0.18023185 <a title="189-tfidf-14" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>15 0.17132396 <a title="189-tfidf-15" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>16 0.16752172 <a title="189-tfidf-16" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>17 0.14377044 <a title="189-tfidf-17" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>18 0.13782787 <a title="189-tfidf-18" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>19 0.13542123 <a title="189-tfidf-19" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>20 0.13414165 <a title="189-tfidf-20" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.269), (1, -0.57), (2, -0.091), (3, -0.072), (4, 0.015), (5, -0.08), (6, 0.09), (7, -0.005), (8, 0.02), (9, -0.224), (10, -0.011), (11, -0.036), (12, 0.021), (13, 0.044), (14, -0.045), (15, 0.027), (16, 0.089), (17, -0.066), (18, -0.024), (19, -0.001), (20, -0.002), (21, 0.001), (22, 0.048), (23, -0.003), (24, -0.045), (25, -0.022), (26, 0.02), (27, 0.033), (28, -0.025), (29, 0.009), (30, 0.019), (31, -0.0), (32, 0.026), (33, -0.029), (34, 0.015), (35, -0.007), (36, -0.043), (37, -0.002), (38, -0.049), (39, 0.043), (40, 0.011), (41, -0.016), (42, -0.014), (43, 0.053), (44, 0.027), (45, 0.025), (46, -0.054), (47, 0.048), (48, 0.039), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98015666 <a title="189-lsi-1" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>2 0.91947186 <a title="189-lsi-2" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>3 0.91358429 <a title="189-lsi-3" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>4 0.90740389 <a title="189-lsi-4" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>5 0.90214187 <a title="189-lsi-5" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>Author: Jeffrey Johns, Christopher Painter-wakefield, Ronald Parr</p><p>Abstract: Recent work in reinforcement learning has emphasized the power of L1 regularization to perform feature selection and prevent overﬁtting. We propose formulating the L1 regularized linear ﬁxed point problem as a linear complementarity problem (LCP). This formulation offers several advantages over the LARS-inspired formulation, LARS-TD. The LCP formulation allows the use of efﬁcient off-theshelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts). We demonstrate that warm starts, as well as the efﬁciency of LCP solvers, can speed up policy iteration. Moreover, warm starts permit a form of modiﬁed policy iteration that can be used to approximate a “greedy” homotopy path, a generalization of the LARS-TD homotopy path that combines policy evaluation and optimization. 1</p><p>6 0.90209782 <a title="189-lsi-6" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>7 0.84123868 <a title="189-lsi-7" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>8 0.8348493 <a title="189-lsi-8" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>9 0.7177034 <a title="189-lsi-9" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>10 0.66603279 <a title="189-lsi-10" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>11 0.63999474 <a title="189-lsi-11" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>12 0.62802291 <a title="189-lsi-12" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>13 0.61258608 <a title="189-lsi-13" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>14 0.57885736 <a title="189-lsi-14" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>15 0.56823701 <a title="189-lsi-15" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>16 0.52999085 <a title="189-lsi-16" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>17 0.52256298 <a title="189-lsi-17" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>18 0.48284084 <a title="189-lsi-18" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>19 0.45835155 <a title="189-lsi-19" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>20 0.45759028 <a title="189-lsi-20" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.045), (27, 0.051), (30, 0.06), (35, 0.019), (45, 0.236), (50, 0.058), (52, 0.024), (60, 0.04), (77, 0.069), (78, 0.014), (90, 0.029), (99, 0.249)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82582289 <a title="189-lda-1" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>Author: Rob Fergus, George Williams, Ian Spiro, Christoph Bregler, Graham W. Taylor</p><p>Abstract: This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that realworld performance can be improved through the use of synthetic data. 1</p><p>2 0.81794792 <a title="189-lda-2" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>same-paper 3 0.81620646 <a title="189-lda-3" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>4 0.73507398 <a title="189-lda-4" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>5 0.7313143 <a title="189-lda-5" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>Author: Matthias Broecheler, Lise Getoor</p><p>Abstract: Continuous Markov random ﬁelds are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random ﬁeld. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efﬁciency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classiﬁcation, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of conﬁdence. 1</p><p>6 0.73052758 <a title="189-lda-6" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>7 0.72958422 <a title="189-lda-7" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>8 0.72957146 <a title="189-lda-8" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>9 0.72917604 <a title="189-lda-9" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>10 0.72697115 <a title="189-lda-10" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>11 0.72696894 <a title="189-lda-11" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>12 0.72655421 <a title="189-lda-12" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>13 0.72629642 <a title="189-lda-13" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>14 0.72604346 <a title="189-lda-14" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>15 0.72515619 <a title="189-lda-15" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>16 0.72515452 <a title="189-lda-16" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>17 0.72507972 <a title="189-lda-17" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>18 0.72467279 <a title="189-lda-18" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>19 0.72445804 <a title="189-lda-19" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>20 0.72431612 <a title="189-lda-20" href="./nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">29 nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
