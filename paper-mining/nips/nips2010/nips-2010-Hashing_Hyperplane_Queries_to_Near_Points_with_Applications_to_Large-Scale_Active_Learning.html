<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-112" href="#">nips2010-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</h1>
<br/><p>Source: <a title="nips-2010-112-pdf" href="http://papers.nips.cc/paper/4088-hashing-hyperplane-queries-to-near-points-with-applications-to-large-scale-active-learning.pdf">pdf</a></p><p>Author: Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashingbased solutions. Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our ﬁrst method’s preprocessing stage is more efﬁcient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. 1</p><p>Reference: <a title="nips-2010-112-reference" href="../nips2010_reference/nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. [sent-6, score-0.868]
</p><p>2 Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. [sent-8, score-0.554]
</p><p>3 Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. [sent-9, score-0.462]
</p><p>4 Both use hashing to retrieve near points in sub-linear time. [sent-10, score-0.343]
</p><p>5 We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. [sent-12, score-0.65]
</p><p>6 We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. [sent-13, score-0.394]
</p><p>7 Often the search problem is considered in the domain of point data: given a database of vectors listing some attributes of the data objects, which points are nearest to a novel query vector? [sent-15, score-0.553]
</p><p>8 Existing algorithms provide efﬁcient data structures for point-to-point retrieval tasks with various useful distance functions, producing either exact or approximate near neighbors while forgoing a brute force scan through all database items, e. [sent-16, score-0.365]
</p><p>9 In particular, little previous work addresses the hyperplane-to-point search problem: given a database of points, which are nearest to a novel hyperplane query? [sent-20, score-0.609]
</p><p>10 This problem is critical to pool-based active learning, where the goal is to request labels for those points that appear most informative. [sent-21, score-0.283]
</p><p>11 The widely used margin-based selection criterion of [8, 9, 10] seeks those points that are nearest to the current support vector machine’s hyperplane decision boundary, and can substantially reduce total human annotation effort. [sent-22, score-0.575]
</p><p>12 However, for large-scale active learning, it is impractical to exhaustively apply the classiﬁer to all unlabeled points at each round of learning; to exploit massive unlabeled pools, a fast (sub-linear time) hyperplane search method is needed. [sent-23, score-0.933]
</p><p>13 For each, we introduce randomized hash functions that offer query times sub-linear in the size of the database, and provide bounds for the approximation error of the neighbors retrieved. [sent-25, score-0.841]
</p><p>14 Our ﬁrst approach devises a two-bit hash function that is locality-sensitive for the angle between the hyperplane normal and a database point. [sent-26, score-1.135]
</p><p>15 Our second approach embeds the inputs such that the Euclidean distance reﬂects the hyperplane distance, thereby making them searchable with existing approximate nearest neighbor algorithms for vector data. [sent-27, score-0.565]
</p><p>16 We demonstrate our algorithms’ signiﬁcant practical impact for large-scale active learning with SVM classiﬁers. [sent-29, score-0.19]
</p><p>17 Our results show that our method helps scale-up active learning for realistic problems with massive unlabeled pools on the order of millions of examples. [sent-30, score-0.396]
</p><p>18 2  Related Work  We brieﬂy review related work on approximate similarity search, subspace search methods, and pool-based active learning. [sent-31, score-0.304]
</p><p>19 Locality-sensitive hashing (LSH) methods devise randomized hash functions that map similar points to the same hash buckets, so that only a subset of the database must be searched after hashing a novel query [3, 4, 5]. [sent-35, score-2.024]
</p><p>20 In [13], a Euclidean embedding is developed such that the norm in the embedding space directly reﬂects the principal angle-based distance between the original subspaces. [sent-41, score-0.238]
</p><p>21 We provide a related embedding to ﬁnd the points nearest to the hyperplane; however, in contrast to [13], we provide LSH bounds, and our embedding is more compact due to our proposed sampling strategy. [sent-45, score-0.357]
</p><p>22 Finally, a sub-linear time method to map a line query to its nearest points is derived in [15]. [sent-48, score-0.349]
</p><p>23 In contrast to all the above work, we propose specialized methods for the hyperplane search problem, and show that they handle high-dimensional data and large databases very efﬁciently. [sent-49, score-0.405]
</p><p>24 Existing active classiﬁer learning methods for pool-based selection generally scan all database instances before selecting which to have labeled next. [sent-51, score-0.443]
</p><p>25 1 One well-known and effective active selection criterion for support vector machines (SVMs) is to choose points that are nearest to the current separating hyperplane [8, 9, 10]. [sent-52, score-0.765]
</p><p>26 Unfortunately, even for inexpensive selection functions, very large unlabeled datasets make the cost of exhaustively searching the pool impractical. [sent-56, score-0.274]
</p><p>27 Researchers have previously attempted to cope with this issue by clustering or randomly downsampling the pool [19, 20, 21, 22]; however, such strategies provide no guarantees as to the potential loss in active selection quality. [sent-57, score-0.312]
</p><p>28 In contrast, when applying our approach for this task, we can consider orders of magnitude fewer points when making the next active label request, yet guarantee selections within a known error of the traditional exhaustive pool-based technique. [sent-58, score-0.481]
</p><p>29 1  We consider only a speciﬁc hyperplane criterion in this paper; see [16] for an active learning survey. [sent-61, score-0.536]
</p><p>30 , xN ] of N points in Rd , the goal is to retrieve the points from the database that are closest to a given hyperplane query whose normal is given by w ∈ Rd . [sent-66, score-0.814]
</p><p>31 We call this the nearest neighbor to a query hyperplane (NNQH) problem. [sent-67, score-0.631]
</p><p>32 Without loss of generality, we assume that the hyperplane passes through origin, and that each xi , w is unit norm. [sent-68, score-0.343]
</p><p>33 The Euclidean distance of a point x to a given hyperplane hw parameterized by normal w is: d(hw , x) = (xT w)w = |xT w|. [sent-70, score-0.406]
</p><p>34 Our ﬁrst approach maps the data to binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point, thereby permitting sub-linear time retrieval with hashing. [sent-77, score-0.636]
</p><p>35 Our second approach computes a sparse Euclidean embedding for the query hyperplane that maps the desired search task to one handled well by existing approximate nearest-point methods. [sent-78, score-0.691]
</p><p>36 In the following, we ﬁrst provide necessary background on locality-sensitive hashing (LSH). [sent-79, score-0.193]
</p><p>37 5, we explain how either method can be applied to large-scale active learning. [sent-85, score-0.19]
</p><p>38 1  Background: Locality-Sensitive Hashing (LSH)  Informally, LSH [3] requires randomized hash functions guaranteeing that the probability of collision of two vectors is inversely proportional to their “distance”, where “distance” is deﬁned according to the task at hand. [sent-87, score-0.702]
</p><p>39 ) to fall into the same hash bucket, one need only search those database items with which a novel query collides in the hash table. [sent-91, score-1.639]
</p><p>40 [3] Let hH denote a random choice of a hash function from the family H. [sent-95, score-0.653]
</p><p>41 A k-bit LSH function computes a hash “key” by concatenating the bits returned by a random sampling of H: g(p) = (2) (k) (1) hH (p), hH (p), . [sent-98, score-0.675]
</p><p>42 During a preprocessing stage, all database points are 1 2 mapped to a series of l hash tables indexed by independently constructed g1 , . [sent-103, score-0.882]
</p><p>43 Then, given a query q, an exhaustive search is carried out only on those examples in the union of the l buckets to which q hashes. [sent-107, score-0.455]
</p><p>44 For that hash function, ρ = log p2 ≤ 1+ǫ , and using l = N ρ 1  hash tables, a (1+ǫ)-approximate solution can be retrieved in time O(N (1+ǫ) ). [sent-110, score-1.265]
</p><p>45 Our contribution is to deﬁne two locality-sensitive hash functions for the NNQH problem. [sent-114, score-0.619]
</p><p>46 2  Hyperplane Hashing based on Angle Distance (H-Hash)  Recall that we want to retrieve the database vector(s) x for which |wT x| is minimized. [sent-116, score-0.201]
</p><p>47 If the vectors are unit norm, then this means that for the “good” (close) database vectors, w and x are almost perpendicular. [sent-117, score-0.194]
</p><p>48 We deﬁne our hyperplane hash (H-Hash) function family H as: hH (z) =  hu,v (z, z), hu,v (z, −z),  if z is a database point vector, if z is a query hyperplane vector. [sent-124, score-1.606]
</p><p>49 Next, we prove that this family of hash functions is locality-sensitive (Deﬁnition 3. [sent-125, score-0.653]
</p><p>50 Since the vectors u, v used by hash function hu,v are sampled independently, then for a query hyperplane vector w and a database point vector x, Pr[hH (w) = hH (x)] = Pr[hu (w) = hu (x) and hv (−w) = hv (x)], = Pr[hu (w) = hu (x)] Pr[hv (−w) = hv (x)]. [sent-131, score-1.642]
</p><p>51 That is, to hash a database point x we use hu,v (x, x), whereas to hash a query hyperplane w, we use hu,v (w, −w). [sent-137, score-1.872]
</p><p>52 The purpose of the two-bit hash is to constrain the angle with respect to both w and −w, so that we do not simply retrieve examples for which we know only that x is π/2 or less away from w. [sent-138, score-0.763]
</p><p>53 With these functions in hand, we can now form hash keys by concatenating k two-bit pairs from k hash functions from H, store the database points in the hash tables, and query with a novel hyperplane to retrieve its closest points (see Sec. [sent-139, score-2.709]
</p><p>54 3  Embedded Hyperplane Hashing based on Euclidean Distance (EH-Hash)  Our second approach for the NNQH problem relies on a Euclidean embedding for the hyperplane and points. [sent-151, score-0.408]
</p><p>55 Given this, we deﬁne our embedding-hyperplane hash (EH-Hash) function family E as: hE (z) =  hu (V (z)) , if z is a database point vector, hu (−V (z)) , if z is a query hyperplane vector,  where hu (z) = sign(uT z) is a one-bit hash function parameterized by u ∼ N (0, I). [sent-162, score-2.176]
</p><p>56 On the other hand, EH-Hash’s hash functions are signiﬁcantly more expensive to compute. [sent-174, score-0.619]
</p><p>57 To alleviate this problem, we use a form of randomized sampling when computing the hash bits for a query that reduces the time 2 to O(1/ǫ′ ), for ǫ′ > 0. [sent-176, score-0.901]
</p><p>58 The lemma implies that at query time our hash function hE (w) can be computed while incurring a small additive error in time O( ǫ12 ), by sampling ′ its embedding V (w) accordingly, and then cycling through only the non-zero indices of V (w) to compute uT (−V (w)). [sent-186, score-0.967]
</p><p>59 Note that we can substantially reduce the error in the hash function compu˜ tation by sampling O( ǫ1 ) elements of the vector w and then using vec(wwT ) as the embedding ′2 for w. [sent-187, score-0.739]
</p><p>60 The latter is problematic when ﬁelding an arbitrary number of queries over time or storing a growing database of points—both properties that are intrinsic to our target active learning application. [sent-190, score-0.387]
</p><p>61 In contrast, our sampling method is instance-dependent and incurs very little overhead for computing the hash function. [sent-191, score-0.65]
</p><p>62 In particular, they deﬁne Euclidean embeddings for afﬁne subspace queries and database points which could be used for NNQH, although they do not speciﬁcally apply it to hyperplane-to-point search in their work. [sent-195, score-0.366]
</p><p>63 4  Recap of the Hashing Approaches  To summarize, we presented two locality-sensitive hashing approaches for the NNQH problem. [sent-199, score-0.193]
</p><p>64 Our ﬁrst H-Hash approach deﬁnes locality-sensitivity in the context of NNHQ, and then provides suitable two-bit hash functions together with a bound on retrieval time. [sent-200, score-0.674]
</p><p>65 Our second EH-Hash approach consists of a d2 -dimensional Euclidean embedding for vectors of dimension d that in turn reduces NNHQ to the Euclidean space nearest neighbor problem, for which efﬁcient search structures (including LSH) are available. [sent-201, score-0.317]
</p><p>66 While EH-Hash has better bounds than H-Hash, its hash functions are more expensive. [sent-202, score-0.619]
</p><p>67 To mitigate the expense for high-dimensional data, we use a well-justiﬁed heuristic where we randomly sample the given query embedding, reducing the query time to linear in d. [sent-203, score-0.401]
</p><p>68 Note that both of our approaches attempt to minimize dθ (w, x) between the retrieved x and the hyperplane w. [sent-204, score-0.319]
</p><p>69 Since that distance is only dependent on the angle between x and w, any scaling of the vectors do not effect our methods, and we can safely treat the provided vectors to be unit norm. [sent-205, score-0.198]
</p><p>70 We are especially interested in their relevance for making active learning scalable. [sent-208, score-0.19]
</p><p>71 A practical paradox with pool-based active learning algorithms is that their intended value—to reduce learning time by choosing informative examples to label ﬁrst—conﬂicts with the real expense of applying them to very large “unprepared” unlabeled datasets. [sent-209, score-0.388]
</p><p>72 In reality, one would like to deploy an active learner on a massive truly unlabeled data pool (e. [sent-211, score-0.382]
</p><p>73 Given a hyperplane classiﬁer and an unlabeled pool of vector data U = {x1 , . [sent-217, score-0.453]
</p><p>74 Our two NNQH solutions supply exactly the hash functions needed to rapidly identify the next point to label: ﬁrst we hash the unlabeled database into tables, and then at each active learning loop, we hash the current classiﬁer w as a query. [sent-221, score-2.281]
</p><p>75 2  Distances to hyperplane 2  H −H as h  Time (secs) − log scale  AUROC Improvement (%)  0. [sent-229, score-0.319]
</p><p>76 Both of our approximate methods (H-Hash and EH-Hash) signiﬁcantly outperform the passive baseline; they are nearly as accurate as ideal exhaustive active selection, yet require 1-2 orders of magnitude less time to select an example. [sent-236, score-0.423]
</p><p>77 ) Learning curves − All 10 classes  Selection time  2  10  Distances to hyperplane 2  (a)  |w. [sent-238, score-0.346]
</p><p>78 x|  e tiv ha  us  as Ex  as H  EH  −H  −H  h  m do an  us  h  e tiv  h as  (b)  0  R  300  ha  250  Ex  100 150 200 Selection iterations  −H  50  EH  −0. [sent-239, score-0.26]
</p><p>79 Our EH-Hash provides more accurate selection than our H-Hash (see (c)), though requires noticeably more query time (see (b)). [sent-248, score-0.282]
</p><p>80 4  Results  We demonstrate our approach applied to large-scale active learning tasks. [sent-249, score-0.19]
</p><p>81 3) to two baselines: 1) passive learning, where the next label request is randomly selected, and 2) exhaustive active selection, where the margin criterion in (1) is computed over all unlabeled examples in order to ﬁnd the true minimum. [sent-254, score-0.529]
</p><p>82 The main goal is to show our algorithms can retrieve examples nearly as well as the exhaustive approach, but with substantially greater efﬁciency. [sent-255, score-0.246]
</p><p>83 For all datasets, we train a linear SVM in the one-vs-all setting using a randomly selected labeled set (5 examples per class), and then run active selection for 300 iterations. [sent-265, score-0.324]
</p><p>84 The active learners (exact and approximate) have the steepest curves, indicating that they are learning more effectively from the chosen labels compared to the random baseline. [sent-271, score-0.19]
</p><p>85 Both of our hashing methods perform similarly to the exhaustive selection, yet require scanning an order of magnitude fewer examples (b). [sent-272, score-0.406]
</p><p>86 1(c) shows the actual values of |wT x| for the selected examples over all iterations, categories, and runs; in line with our methods’ guarantees, they select points close to those found with exhaustive search. [sent-275, score-0.271]
</p><p>87 5  airplane  (a)  e tiv us ha Ex  us ha Ex  0  10  h  e tiv  h as −H EH  R  an  do  m  0  1  as  1 0. [sent-287, score-0.304]
</p><p>88 Averaged over all classes, we happen to outperform exhaustive selection (Fig. [sent-295, score-0.239]
</p><p>89 2(a)); this can happen since there is no guarantee that the best active choice will help test accuracy, and it also reﬂects the wider variation across per-class results. [sent-296, score-0.19]
</p><p>90 The boxplots in (c) more directly show the hashing methods are behaving as expected. [sent-297, score-0.193]
</p><p>91 Figure 3(a) shows example image selection results; both exhaustive search and our hashing methods manage to choose images useful for learning about airplanes/non-airplanes. [sent-299, score-0.519]
</p><p>92 ) These results best show the advantage of our approximate methods: accounting for both types of cost inherent to training the classiﬁer, they outperform both exhaustive and random selection in terms of the accuracy gains per unit time. [sent-303, score-0.288]
</p><p>93 While exhaustive active selection suffers because of its large selection time, random selection suffers because it wastes expensive labeling time on irrelevant examples. [sent-304, score-0.677]
</p><p>94 Finally, to demonstrate the practical capability of our hyperplane hashing approach, we perform active selection on the one million tiny image set. [sent-307, score-0.901]
</p><p>95 The 1M set lacks any labels, making this a “live” test of active learning (we ourselves annotated whatever the methods selected). [sent-309, score-0.19]
</p><p>96 Even on this massive collection, our method’s selections are very similar in quality to the exhaustive method (see Fig. [sent-311, score-0.266]
</p><p>97 The images (c) show the selections made from this large pool during the “live” labeling test; among all one million unlabeled examples (nearly all of which likely belong to one of the other 1000s of classes) our method retrieves seemingly relevant instances. [sent-313, score-0.326]
</p><p>98 To our knowledge, this experiment exceeds any previous active selection results in the literature in terms of the scale of the unlabeled pool. [sent-314, score-0.364]
</p><p>99 Both permit efﬁcient large-scale search for points near to a hyperplane, and experiments with three datasets clearly demonstrate the practical value for active learning with massive unlabeled pools. [sent-317, score-0.492]
</p><p>100 For future work, we plan to further explore more accurate hash-functions for our H-hash scheme and also investigate sublinear time methods for non-linear kernel based active learning. [sent-318, score-0.217]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hash', 0.619), ('hyperplane', 0.319), ('lsh', 0.218), ('hh', 0.216), ('hashing', 0.193), ('active', 0.19), ('query', 0.174), ('exhaustive', 0.158), ('database', 0.141), ('nnqh', 0.134), ('eh', 0.127), ('unlabeled', 0.093), ('hu', 0.09), ('embedding', 0.089), ('nearest', 0.088), ('pr', 0.087), ('selection', 0.081), ('secs', 0.074), ('ut', 0.071), ('newsgroups', 0.069), ('tiv', 0.067), ('ha', 0.063), ('tiny', 0.062), ('search', 0.061), ('points', 0.06), ('distance', 0.06), ('hv', 0.06), ('retrieve', 0.06), ('labeling', 0.059), ('auroc', 0.059), ('exhaustively', 0.059), ('massive', 0.058), ('angle', 0.056), ('retrieval', 0.055), ('euclidean', 0.054), ('proccedings', 0.05), ('selections', 0.05), ('neighbor', 0.05), ('cos', 0.049), ('embeddings', 0.047), ('ex', 0.044), ('airplane', 0.044), ('sign', 0.042), ('pool', 0.041), ('aat', 0.041), ('keys', 0.038), ('indyk', 0.038), ('family', 0.034), ('buckets', 0.034), ('goh', 0.034), ('intl', 0.034), ('nnhq', 0.034), ('request', 0.033), ('rd', 0.032), ('tables', 0.032), ('scan', 0.031), ('sampling', 0.031), ('near', 0.03), ('million', 0.03), ('preprocessing', 0.03), ('millions', 0.03), ('svm', 0.03), ('collision', 0.029), ('andoni', 0.029), ('automobile', 0.029), ('basri', 0.029), ('vectors', 0.029), ('stronger', 0.029), ('queries', 0.029), ('subspace', 0.028), ('ects', 0.028), ('er', 0.028), ('examples', 0.028), ('criterion', 0.027), ('hw', 0.027), ('scanning', 0.027), ('time', 0.027), ('expense', 0.026), ('image', 0.026), ('randomized', 0.025), ('pools', 0.025), ('multimedia', 0.025), ('tradeoffs', 0.025), ('retrieves', 0.025), ('bits', 0.025), ('grauman', 0.025), ('databases', 0.025), ('items', 0.025), ('approximate', 0.025), ('selected', 0.025), ('classi', 0.025), ('wt', 0.024), ('xt', 0.024), ('intended', 0.024), ('unit', 0.024), ('neighbors', 0.023), ('vec', 0.023), ('handled', 0.023), ('embeds', 0.023), ('orders', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="112-tfidf-1" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>Author: Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashingbased solutions. Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our ﬁrst method’s preprocessing stage is more efﬁcient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. 1</p><p>2 0.1436478 <a title="112-tfidf-2" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>Author: Sheng-jun Huang, Rong Jin, Zhi-hua Zhou</p><p>Abstract: Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in ﬁnding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed Q UIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed Q UIRE approach outperforms several state-of -the-art active learning approaches. 1</p><p>3 0.12359539 <a title="112-tfidf-3" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>Author: Alina Beygelzimer, John Langford, Zhang Tong, Daniel J. Hsu</p><p>Abstract: We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classiﬁcation. 1</p><p>4 0.12219014 <a title="112-tfidf-4" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classiﬁcation models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods. 1</p><p>5 0.11811676 <a title="112-tfidf-5" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>Author: Ping Li, Arnd Konig, Wenhao Gui</p><p>Abstract: Computing1 two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b ≥ 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simpliﬁed estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance. 1</p><p>6 0.091883987 <a title="112-tfidf-6" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>7 0.091430016 <a title="112-tfidf-7" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>8 0.088743486 <a title="112-tfidf-8" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>9 0.087377034 <a title="112-tfidf-9" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>10 0.084690206 <a title="112-tfidf-10" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>11 0.08423762 <a title="112-tfidf-11" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>12 0.063655168 <a title="112-tfidf-12" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>13 0.061882537 <a title="112-tfidf-13" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>14 0.061791316 <a title="112-tfidf-14" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>15 0.058304463 <a title="112-tfidf-15" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>16 0.058119785 <a title="112-tfidf-16" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>17 0.057394397 <a title="112-tfidf-17" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>18 0.056590792 <a title="112-tfidf-18" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>19 0.054586783 <a title="112-tfidf-19" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>20 0.054480728 <a title="112-tfidf-20" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, 0.035), (2, 0.059), (3, -0.075), (4, 0.008), (5, 0.058), (6, -0.043), (7, -0.1), (8, 0.013), (9, -0.149), (10, 0.098), (11, -0.0), (12, -0.066), (13, -0.085), (14, 0.033), (15, -0.048), (16, -0.148), (17, -0.046), (18, -0.093), (19, 0.045), (20, -0.057), (21, 0.077), (22, -0.104), (23, 0.005), (24, 0.019), (25, -0.044), (26, 0.044), (27, -0.024), (28, -0.024), (29, -0.096), (30, -0.055), (31, -0.044), (32, -0.089), (33, -0.057), (34, 0.025), (35, -0.06), (36, -0.005), (37, -0.069), (38, 0.101), (39, 0.045), (40, -0.053), (41, 0.01), (42, -0.075), (43, -0.022), (44, 0.023), (45, -0.079), (46, 0.008), (47, -0.038), (48, 0.045), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93558449 <a title="112-lsi-1" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>Author: Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashingbased solutions. Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our ﬁrst method’s preprocessing stage is more efﬁcient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. 1</p><p>2 0.78258073 <a title="112-lsi-2" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>Author: Sheng-jun Huang, Rong Jin, Zhi-hua Zhou</p><p>Abstract: Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in ﬁnding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed Q UIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed Q UIRE approach outperforms several state-of -the-art active learning approaches. 1</p><p>3 0.73199219 <a title="112-lsi-3" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classiﬁcation models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods. 1</p><p>4 0.62886471 <a title="112-lsi-4" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>Author: Wei Wang, Zhi-hua Zhou</p><p>Abstract: The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be O(log 1 ), contrasting to single-view setting where the polynomial improveǫ ment is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is O( 1 ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise, ǫ contrasting to previous polynomial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. 1</p><p>5 0.61140442 <a title="112-lsi-5" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>Author: Alina Beygelzimer, John Langford, Zhang Tong, Daniel J. Hsu</p><p>Abstract: We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classiﬁcation. 1</p><p>6 0.58348548 <a title="112-lsi-6" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>7 0.56742394 <a title="112-lsi-7" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>8 0.48630336 <a title="112-lsi-8" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>9 0.45992759 <a title="112-lsi-9" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>10 0.45611548 <a title="112-lsi-10" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>11 0.44746533 <a title="112-lsi-11" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>12 0.41960576 <a title="112-lsi-12" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>13 0.40544406 <a title="112-lsi-13" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>14 0.40425763 <a title="112-lsi-14" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>15 0.40221918 <a title="112-lsi-15" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>16 0.39862666 <a title="112-lsi-16" href="./nips-2010-Large_Margin_Multi-Task_Metric_Learning.html">138 nips-2010-Large Margin Multi-Task Metric Learning</a></p>
<p>17 0.39439961 <a title="112-lsi-17" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>18 0.3726044 <a title="112-lsi-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.36463815 <a title="112-lsi-19" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>20 0.33973774 <a title="112-lsi-20" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.041), (27, 0.041), (30, 0.052), (35, 0.025), (45, 0.208), (50, 0.039), (52, 0.027), (60, 0.035), (77, 0.035), (78, 0.366), (90, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85916501 <a title="112-lda-1" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>Author: Thomas Peel, Sandrine Anthoine, Liva Ralaivola</p><p>Abstract: We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernsteintype inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efﬁcient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions. 1</p><p>2 0.81732589 <a title="112-lda-2" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>Author: Jie Luo, Francesco Orabona</p><p>Abstract: In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classiﬁers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difﬁcult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efﬁcient algorithm to solve it. Experiments conducted on artiﬁcial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to an SVM trained with the ground-truth labels, and outperforms other baselines.</p><p>3 0.79234099 <a title="112-lda-3" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>Author: Alessandro Chiuso, Gianluigi Pillonetto</p><p>Abstract: We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as deﬁned by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art parametric identiﬁcation techniques based on prediction error minimization. 1</p><p>same-paper 4 0.76533973 <a title="112-lda-4" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>Author: Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We consider the problem of retrieving the database points nearest to a given hyperplane query without exhaustively scanning the database. We propose two hashingbased solutions. Our ﬁrst approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reﬂects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our ﬁrst method’s preprocessing stage is more efﬁcient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classiﬁer as a query, our algorithm identiﬁes those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods’ tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points. 1</p><p>5 0.70566314 <a title="112-lda-5" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>6 0.67678338 <a title="112-lda-6" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>7 0.66201019 <a title="112-lda-7" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>8 0.64458477 <a title="112-lda-8" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>9 0.62665355 <a title="112-lda-9" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>10 0.62576586 <a title="112-lda-10" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>11 0.62530625 <a title="112-lda-11" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>12 0.62456203 <a title="112-lda-12" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>13 0.60601383 <a title="112-lda-13" href="./nips-2010-Co-regularization_Based_Semi-supervised_Domain_Adaptation.html">47 nips-2010-Co-regularization Based Semi-supervised Domain Adaptation</a></p>
<p>14 0.59812975 <a title="112-lda-14" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>15 0.59801763 <a title="112-lda-15" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>16 0.59734422 <a title="112-lda-16" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>17 0.59628117 <a title="112-lda-17" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>18 0.59610319 <a title="112-lda-18" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>19 0.58986223 <a title="112-lda-19" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>20 0.58788878 <a title="112-lda-20" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
