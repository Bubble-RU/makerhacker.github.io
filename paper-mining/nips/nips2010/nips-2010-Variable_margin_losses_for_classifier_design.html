<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>282 nips-2010-Variable margin losses for classifier design</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-282" href="#">nips2010-282</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>282 nips-2010-Variable margin losses for classifier design</h1>
<br/><p>Source: <a title="nips-2010-282-pdf" href="http://papers.nips.cc/paper/4024-variable-margin-losses-for-classifier-design.pdf">pdf</a></p><p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>Reference: <a title="nips-2010-282-reference" href="../nips2010_reference/nips-2010-Variable_margin_losses_for_classifier_design_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Variable margin losses for classiﬁer design  Nuno Vasconcelos Statistical Visual Computing Laboratory, University of California, San Diego La Jolla, CA 92039 nuno@ucsd. [sent-1, score-0.482]
</p><p>2 A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. [sent-4, score-0.875]
</p><p>3 It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. [sent-5, score-0.749]
</p><p>4 These enable a precise characterization of the loss for a popular class of link functions. [sent-6, score-0.495]
</p><p>5 It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. [sent-7, score-1.598]
</p><p>6 Novel families of Bayes consistent loss functions, of variable margin, are derived. [sent-8, score-0.316]
</p><p>7 These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. [sent-9, score-0.498]
</p><p>8 Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. [sent-11, score-0.686]
</p><p>9 Examples of such losses include the hinge loss, used in SVM design, the exponential loss, used by boosting algorithms such as AdaBoost, or the logistic loss, used in both classical logistic regression and more recent methods, such as LogitBoost. [sent-16, score-0.84]
</p><p>10 The main idea is that there are three quantities that matter in risk minimization: the loss function φ, a corresponding optimal link ∗ function fφ , which maps posterior class probabilities to classiﬁer predictions, and a minimum risk ∗ Cφ , associated with the optimal link. [sent-22, score-1.105]
</p><p>11 While the standard approach to classiﬁer design is to deﬁne a loss φ, and then optimize it to obtain ∗ ∗ ∗ ∗ fφ and Cφ , [3] showed that there is an alternative: to specify fφ and Cφ , and analytically derive the loss φ. [sent-23, score-0.527]
</p><p>12 This 1  loss is then used to design a robust boosting algorithm, denoted SavageBoost. [sent-26, score-0.636]
</p><p>13 SavageBoost has been, more recently, shown to outperform most other boosting algorithms in computer vision problems, where outliers are prevalent [5]. [sent-27, score-0.375]
</p><p>14 It turns out that many pairs (Cφ ,fφ ) are compatible with any Bayes consistent loss φ. [sent-29, score-0.338]
</p><p>15 In practice, the design has to resort to trial and error, by 1) testing combinations of the latter and, 2) verifying whether the loss has the desired properties. [sent-32, score-0.334]
</p><p>16 In this work, we consider one such problem: how to control the size of the margin enforced by the ∗ ∗ loss. [sent-36, score-0.279]
</p><p>17 We start by showing that, while many pairs (Cφ ,fφ ) are compatible with a given φ, one of these ∗ pairs establishes a very tight connection between the optimal link and the minimum risk: that fφ is ∗ the derivative of Cφ . [sent-37, score-0.502]
</p><p>18 We refer to the risk function associated with such a pair as a canonical risk, ∗ ∗ and show that it leads to an equally tight connection between the pair (Cφ ,fφ ) and the loss φ. [sent-38, score-1.143]
</p><p>19 For a canonical risk, all three functions can be obtained from each other with one-to-one mappings of ∗ ∗ trivial analytical tractability. [sent-39, score-0.688]
</p><p>20 This implies that, for a canonical risk, the choice of a particular link in the inverse-sigmoidal family only impacts the behavior of φ around the origin, i. [sent-45, score-0.856]
</p><p>21 This is exploited to design parametric families of loss functions that allow explicit control of the classiﬁcation margin. [sent-52, score-0.445]
</p><p>22 These losses are applied to the design of novel boosting algorithms of tunable margin. [sent-53, score-0.59]
</p><p>23 Finally, it is shown that the requirements of 1) a canonical risk, and 2) an inverse-sigmoidal link are not unduly restrictive for classiﬁer design. [sent-54, score-0.851]
</p><p>24 Given a non-negative loss function L(x, y), the classiﬁer is optimal if it minimizes the risk R(f ) = EX,Y [L(h(x), y)]. [sent-64, score-0.51]
</p><p>25 This is equivalent to minimizing the conditional risk EY |X [L(h(x), y)|X = x] for all x ∈ X . [sent-65, score-0.286]
</p><p>26 The associated conditional risk is C0/1 (η, f ) = η  1 − sign(f ) 1 + sign(f ) + (1 − η) = 2 2  The risk is minimized if    f (x) > 0 f (x) = 0  f (x) < 0  if η(x) > if η(x) = if η(x) < 2  1 2 1 2 1 2  1 − η, if f ≥ 0; η, if f < 0. [sent-68, score-0.595]
</p><p>27 (2)  (3)  ∗ ∗ ∗ Table 1: Loss φ, optimal link fφ (η), optimal inverse link [fφ ]−1 (v) , and minimum conditional risk Cφ (η)  for popular learning algorithms. [sent-69, score-0.893]
</p><p>28 The associated optimal ∗ ∗ classiﬁer h = sign[f ] is the well known Bayes decision rule (BDR), and the associated minimum conditional (zero-one) risk is ∗ C0/1 (η) = η  1 1 − sign(2η − 1) + (1 − η) 2 2  1 1 + sign(2η − 1) . [sent-71, score-0.435]
</p><p>29 These include the exponential loss of boosting, the log loss of logistic regression, and the hinge loss of SVMs. [sent-74, score-0.823]
</p><p>30 The associated conditional risk Cφ (η, f ) = ηφ(f ) + (1 − η)φ(−f ). [sent-78, score-0.312]
</p><p>31 (5)  ∗ fφ (η) = arg min Cφ (η, f )  (6)  is minimized by the link f  ∗ ∗ leading to the minimum conditional risk function Cφ (η) = Cφ (η, fφ ). [sent-79, score-0.59]
</p><p>32 Table 1 lists the loss, optimal link, and minimum risk of some of the most popular classiﬁer design methods. [sent-80, score-0.423]
</p><p>33 Conditional risk minimization is closely related to classical probability elicitation in statistics [4]. [sent-81, score-0.364]
</p><p>34 Rather than specifying a loss φ and minimizing Cφ (η, f ), so as to obtain whatever ∗ ∗ ∗ ∗ optimal link fφ and minimum expected risk Cφ (η) results, it is possible to specify fφ and Cφ (η) ∗ and derive, from (14) with J(η) = −Cφ (η), the underlying loss φ. [sent-99, score-1.027]
</p><p>35 The main advantage is the ability to control directly the quantities that matter for classiﬁcation, namely the predictor and risk of the ∗ ∗ ∗ optimal classiﬁer. [sent-100, score-0.327]
</p><p>36 3  Canonical risk minimization  ∗ ∗ In general, given J(η) = −Cφ (η), there are multiple pairs (φ, fφ ) that satisfy (14). [sent-102, score-0.276]
</p><p>37 Hence, speciﬁcation of either the minimum risk or optimal link does not completely characterize the loss. [sent-103, score-0.569]
</p><p>38 Whenever this holds, the risk is said to be in canonical form, and (f ∗ , J) are denoted a canonical pair [6] . [sent-112, score-1.541]
</p><p>39 If the optimal link ∗ associated with Cφ (η) is ∗ fφ (η) = J ′ (η) (16) ∗ the risk Cφ (η, f ) is said to be in canonical form. [sent-115, score-1.178]
</p><p>40 fφ (η) is denoted a canonical link and φ(v), the loss given by (14), a canonical loss. [sent-116, score-1.689]
</p><p>41 For example, the risk of boosting is derived from the convex, differentiable, and symmetric J(η) = −2 η(1 − η). [sent-118, score-0.607]
</p><p>42 Since this has derivative J ′ (η) =  2η − 1 η(1 − η)  =  1 η ∗ log = fφ (η), 2 1−η  (17)  the risk is not in canonical form. [sent-119, score-0.939]
</p><p>43 What follows from (16) is that it is possible to derive a canonical risk for any maximal reward J(η), including that of boosting (J(η) = −2 η(1 − η)). [sent-120, score-1.242]
</p><p>44 ∗ While canonical risks can be easily designed by specifying either J(η) or fφ (η), and then using (14) and (16), it is much less clear how to directly specify a loss φ(v) for which (14) holds with a canonical pair (f ∗ , J). [sent-122, score-1.535]
</p><p>45 Let Cφ (η, f ) be the canonical risk derived from a convex and symmetric J(η). [sent-125, score-0.925]
</p><p>46 2  margin parameter v  Figure 1: Left: canonical losses compatible with an IS optimal link. [sent-139, score-1.133]
</p><p>47 Right: Average classiﬁcation rank as a function of margin parameter, on the UCI data. [sent-140, score-0.277]
</p><p>48 First, it establishes an easy-to-verify necessary 1 ∗ condition for the canonical form. [sent-142, score-0.648]
</p><p>49 For example, logistic regression has [fφ ]−1 (v) = 1+e−v and e 1 ∗ ∗ φ′ (v) = − 1+e−v = [fφ ]−1 (v) − 1, while boosting has [fφ ]−1 (v) = 1+e−2v and φ′ (v) = −e−v = ∗ −1 ∗ [fφ ] (v) − 1. [sent-143, score-0.481]
</p><p>50 This (plus the symmetry of J and fφ ) shows that the former is in canonical form but the latter is not. [sent-144, score-0.698]
</p><p>51 Second, it makes it clear that, up to additive constants, the three components ∗ ∗ (φ, Cφ , and fφ ) of a canonical risk are related by one-to-one relationships. [sent-145, score-0.874]
</p><p>52 Hence, it is possible to control the properties of the three components of the risk by manipulating a single function (which can be any of the three). [sent-146, score-0.318]
</p><p>53 Finally, it enables a very detailed characterization of the losses compatible with most optimal links of Table 1. [sent-147, score-0.415]
</p><p>54 −v  4  Inverse-sigmoidal links  Inspection of Table 1 suggests that the classiﬁers produced by boosting, logistic regression, and vari∗ ∗ ants have sigmoidal inverse links [fφ ]−1 . [sent-148, score-0.346]
</p><p>55 (24)  It follows that, as illustrated in Figure 1, the loss φ(v) is convex, monotonically decreasing, linear (with slope −1) for large negative v, constant for large positive v, and has slope −. [sent-154, score-0.349]
</p><p>56 The set of losses compatible with an IS link is, thus, strongly constrained. [sent-156, score-0.478]
</p><p>57 5  1  −20  −15  −10  v  −5  0  5  10  15  20  v  Figure 2: canonical link (left) and loss (right) for various values of a. [sent-179, score-1.051]
</p><p>58 What is interesting is that these are the degrees of freedom that control the margin characteristics of the loss φ. [sent-181, score-0.526]
</p><p>59 Hence, by controlling the behavior of the IS link around the origin, it is possible to control the margin of the optimal classiﬁer. [sent-182, score-0.528]
</p><p>60 In particular, the margin is a decreasing function of the ∗ curvature of the loss at the origin, φ(2) (0). [sent-183, score-0.479]
</p><p>61 5  Variable margin loss functions  The results above enable the derivation of families of canonical losses with controllable margin. [sent-185, score-1.395]
</p><p>62 In Section 3, we have seen that the boosting loss is not canonical, but there is a canonical loss for the minimum risk of boosting. [sent-186, score-1.702]
</p><p>63 (25)  From (16), the canonical optimal link is ∗ fφ (η; a) =  2η − 1 a η(1 − η)  (26)  and it can be shown that −1  ∗ [fφ ]  (v; a) =  1 av + 2 2 4 + (av)2  (27)  is an IS link, i. [sent-188, score-0.983]
</p><p>64 Using (18), the corresponding canonical loss is φ(v; a) =  1 ( 4 + (av)2 − av). [sent-191, score-0.836]
</p><p>65 2a  (28)  Because it shares the minimum risk of boosting, we refer to this loss as the canonical boosting loss. [sent-192, score-1.483]
</p><p>66 9  link is indeed sigmoidal, and that the margin is determined by a. [sent-209, score-0.434]
</p><p>67 It is also possible to derive variable margin extensions of existing canonical losses. [sent-211, score-0.836]
</p><p>68 For example, consider the parametric extension of the minimum risk of logistic regression J(η; a) =  1 1 η log(η) + (1 − η) log(1 − η). [sent-212, score-0.5]
</p><p>69 log [fφ ] (v; a) = a 1−η 1 + eav This is again a sigmoidal inverse link and, from (18), ∗ [fφ ](v; a) =  (30)  1 [log(1 + eav ) − av] . [sent-214, score-0.525]
</p><p>70 (31) a We denote this loss the canonical logistic loss. [sent-215, score-0.965]
</p><p>71 4 φ(v; a) =  Note that, in (28) and (31), margin control is not achieved by simply rescaling the domain of the loss function. [sent-218, score-0.474]
</p><p>72 While this type of re-scaling occurs in both families of loss functions above (which are both functions of av), it is localized around the origin, and only inﬂuences the margin properties of the loss. [sent-223, score-0.587]
</p><p>73 6  Experiments  A number of easily reproducible experiments were conducted to study the effect of variable margin losses on the accuracy of the resulting classiﬁers. [sent-227, score-0.413]
</p><p>74 The GradientBoost algorithm [8], with histogram-based weak learners, was then used to design boosted classiﬁers which minimize the canonical logistic and boosting losses, for various margin parameters. [sent-231, score-1.361]
</p><p>75 50 boosting iterations were applied to each training set, for 19 values of a ∈ {0. [sent-234, score-0.327]
</p><p>76 To explore this question we show, in Figure-1, the average rank of the classiﬁer designed with each loss and margin parameter a. [sent-246, score-0.496]
</p><p>77 For the canonical logistic loss, the best values of a is in the range 0. [sent-250, score-0.746]
</p><p>78 Note that the average rank for this range (between 5 and 6), is better than that (close to 7) achieved with the logistic loss of LogitBoost [2] (a = 1). [sent-253, score-0.406]
</p><p>79 In fact, as can be seen from Table 2, the canonical 7  Table 3: Classiﬁcation error for each loss function and UCI dataset. [sent-254, score-0.836]
</p><p>80 6  logistic loss with a = 1 did not achieve rank 1 on any dataset, whereas canonical logistic losses with 0. [sent-332, score-1.346]
</p><p>81 For the canonical boosting loss, there is also a range (0. [sent-337, score-0.944]
</p><p>82 4 produces a loss of much larger margin for canonical boosting. [sent-341, score-1.055]
</p><p>83 Furthermore, the canonical boosting loss has a heavier tail and approaches zero more slowly than the canonical logistic loss. [sent-342, score-1.909]
</p><p>84 Although certain ranges of margin parameters seem to produce best results for both canonical loss functions, the optimal parameter value is likely to be dataset dependent. [sent-343, score-1.113]
</p><p>85 Table 3 presents the 5-fold cross validation test error (# of misclassiﬁed points) obtained for each UCI dataset and canonical loss. [sent-346, score-0.661]
</p><p>86 The table also shows the results of AdaBoost, LogitBoost (canonical logistic, a = 1), and canonical boosting loss with a = 1. [sent-347, score-1.191]
</p><p>87 Cross validating the margin results in better performance for 9 out of 10 (8 out 10) datasets for the canonical logistic (boosting) loss, when compared to the ﬁxed margin (a = 1) counterparts. [sent-348, score-1.215]
</p><p>88 In this case, canonical logistic and canonical boosting outperform both LogitBoost and AdaBoost in 7 and 5 of the ten datasets, respectively. [sent-354, score-1.757]
</p><p>89 LogitBoost and AdaBoost outperforming both canonical losses only happens in 2 and 3 datasets, respectively. [sent-357, score-0.811]
</p><p>90 7  Conclusion  The probability elicitation approach to loss function design, introduced in [3], enables the derivation of new Bayes consistent loss functions. [sent-358, score-0.59]
</p><p>91 In general, it is difﬁcult to anticipate the properties, and shape, of a loss function that results from combining a certain minimal risk with a certain link function. [sent-360, score-0.691]
</p><p>92 In this work, we have addressed this problem for the class of canonical risks. [sent-361, score-0.617]
</p><p>93 We have shown that the associated canonical loss functions lend themselves to analysis, due to a simple connection between the associated minimum conditional risk and optimal link functions. [sent-362, score-1.539]
</p><p>94 This analysis was shown to enable a precise characterization of 1) the relationships between loss, optimal link, and minimum risk, and 2) the properties of the loss whenever the optimal link is in the family of inverse sigmoid functions. [sent-363, score-0.721]
</p><p>95 These properties were then exploited to design parametric families of loss functions with explicit margin control. [sent-364, score-0.653]
</p><p>96 Experiments with boosting algorithms derived from these variable margin losses have shown better performance than those of classical algorithms, such as AdaBoost or LogitBoost. [sent-365, score-0.799]
</p><p>97 Vasconcelos, “On the design of loss functions for classiﬁcation: theory, robustness to outliers, and savageboost,” in NIPS, 2008, pp. [sent-376, score-0.317]
</p><p>98 Bischof, “On robustness of on-line boosting - a competitive study,” in IEEE ICCV Workshop on On-line Computer Vision, 2009. [sent-388, score-0.327]
</p><p>99 Zhang, “Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization,” Annals of Statistics, 2004. [sent-394, score-0.309]
</p><p>100 Friedman, “Greedy function approximation: A gradient boosting machine,” The Annals of Statistics, vol. [sent-397, score-0.327]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('canonical', 0.617), ('boosting', 0.327), ('risk', 0.257), ('margin', 0.219), ('loss', 0.219), ('link', 0.215), ('losses', 0.194), ('logitboost', 0.154), ('logistic', 0.129), ('av', 0.117), ('adaboost', 0.112), ('classi', 0.106), ('bayes', 0.101), ('sigmoidal', 0.095), ('uci', 0.081), ('er', 0.073), ('elicitation', 0.071), ('sign', 0.069), ('compatible', 0.069), ('design', 0.069), ('families', 0.066), ('eav', 0.066), ('minimum', 0.063), ('rank', 0.058), ('yf', 0.058), ('boost', 0.056), ('symmetry', 0.055), ('breast', 0.05), ('lim', 0.049), ('slope', 0.047), ('inverse', 0.046), ('ers', 0.046), ('cancer', 0.045), ('bdr', 0.044), ('gradientboost', 0.044), ('savageboost', 0.044), ('ten', 0.042), ('analytical', 0.042), ('decreasing', 0.041), ('reward', 0.041), ('risks', 0.04), ('origin', 0.039), ('links', 0.038), ('log', 0.037), ('classical', 0.036), ('monotonically', 0.036), ('control', 0.036), ('nuno', 0.036), ('vasconcelos', 0.036), ('optimal', 0.034), ('ev', 0.033), ('jolla', 0.032), ('characterization', 0.031), ('establishes', 0.031), ('consistent', 0.031), ('datasets', 0.031), ('enable', 0.03), ('enables', 0.029), ('functions', 0.029), ('counterparts', 0.029), ('conditional', 0.029), ('said', 0.029), ('table', 0.028), ('convex', 0.028), ('derivative', 0.028), ('guaranteeing', 0.028), ('freedom', 0.028), ('py', 0.027), ('parametric', 0.026), ('latter', 0.026), ('margins', 0.026), ('minimized', 0.026), ('associated', 0.026), ('outperform', 0.025), ('regression', 0.025), ('properties', 0.025), ('degrees', 0.024), ('connection', 0.024), ('ranked', 0.024), ('sigmoid', 0.024), ('behavior', 0.024), ('dataset', 0.024), ('diego', 0.024), ('enforced', 0.024), ('derived', 0.023), ('outliers', 0.023), ('holds', 0.022), ('cation', 0.022), ('derivation', 0.021), ('denoted', 0.021), ('la', 0.02), ('specify', 0.02), ('cross', 0.02), ('detailed', 0.02), ('trial', 0.02), ('invertible', 0.02), ('conditions', 0.02), ('pairs', 0.019), ('unduly', 0.019), ('prognostic', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="282-tfidf-1" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>2 0.19663693 <a title="282-tfidf-2" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>3 0.1831174 <a title="282-tfidf-3" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>4 0.1707509 <a title="282-tfidf-4" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>Author: Nathan Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: √ ˜ We establish an excess risk bound of O HR2 + HL∗ Rn for ERM with an H-smooth loss n function and a hypothesis class with Rademacher complexity Rn , where L∗ is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = R/n, this translates to ˜ ˜ a learning rate of O (RH/n) in the separable (L∗ = 0) case and O RH/n + L∗ RH/n more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective. 1</p><p>5 0.1381449 <a title="282-tfidf-5" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>6 0.1121522 <a title="282-tfidf-6" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>7 0.096841201 <a title="282-tfidf-7" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>8 0.090492226 <a title="282-tfidf-8" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>9 0.085856952 <a title="282-tfidf-9" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>10 0.07896512 <a title="282-tfidf-10" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>11 0.077366717 <a title="282-tfidf-11" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>12 0.073898666 <a title="282-tfidf-12" href="./nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">191 nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>13 0.073169589 <a title="282-tfidf-13" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>14 0.070817277 <a title="282-tfidf-14" href="./nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">29 nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<p>15 0.070637703 <a title="282-tfidf-15" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>16 0.070347488 <a title="282-tfidf-16" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>17 0.067819513 <a title="282-tfidf-17" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>18 0.06541124 <a title="282-tfidf-18" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>19 0.06409429 <a title="282-tfidf-19" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>20 0.061508 <a title="282-tfidf-20" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.178), (1, 0.033), (2, 0.116), (3, -0.058), (4, 0.071), (5, 0.084), (6, -0.143), (7, -0.076), (8, -0.06), (9, -0.01), (10, -0.043), (11, -0.005), (12, 0.082), (13, 0.099), (14, -0.081), (15, 0.087), (16, 0.088), (17, 0.19), (18, -0.073), (19, -0.025), (20, 0.07), (21, -0.08), (22, -0.004), (23, -0.075), (24, 0.009), (25, 0.017), (26, 0.06), (27, 0.212), (28, 0.094), (29, 0.068), (30, -0.064), (31, -0.012), (32, 0.024), (33, -0.042), (34, 0.026), (35, -0.007), (36, 0.026), (37, -0.064), (38, 0.037), (39, -0.127), (40, 0.036), (41, 0.13), (42, 0.007), (43, -0.028), (44, -0.015), (45, 0.071), (46, 0.107), (47, 0.015), (48, -0.027), (49, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97896206 <a title="282-lsi-1" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>2 0.76163906 <a title="282-lsi-2" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>3 0.72685856 <a title="282-lsi-3" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>4 0.67731845 <a title="282-lsi-4" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>5 0.655168 <a title="282-lsi-5" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>Author: Nathan Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: √ ˜ We establish an excess risk bound of O HR2 + HL∗ Rn for ERM with an H-smooth loss n function and a hypothesis class with Rademacher complexity Rn , where L∗ is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = R/n, this translates to ˜ ˜ a learning rate of O (RH/n) in the separable (L∗ = 0) case and O RH/n + L∗ RH/n more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective. 1</p><p>6 0.63494676 <a title="282-lsi-6" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>7 0.60025114 <a title="282-lsi-7" href="./nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">191 nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>8 0.57571399 <a title="282-lsi-8" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>9 0.52706242 <a title="282-lsi-9" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>10 0.49378452 <a title="282-lsi-10" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>11 0.48646942 <a title="282-lsi-11" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>12 0.47598684 <a title="282-lsi-12" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>13 0.45644462 <a title="282-lsi-13" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>14 0.43842849 <a title="282-lsi-14" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>15 0.43706906 <a title="282-lsi-15" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>16 0.42465743 <a title="282-lsi-16" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>17 0.41297275 <a title="282-lsi-17" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>18 0.40658525 <a title="282-lsi-18" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>19 0.40261576 <a title="282-lsi-19" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>20 0.40210956 <a title="282-lsi-20" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.024), (17, 0.018), (27, 0.074), (30, 0.055), (35, 0.022), (45, 0.227), (50, 0.075), (52, 0.029), (60, 0.041), (66, 0.166), (77, 0.073), (78, 0.037), (90, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89567161 <a title="282-lda-1" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>same-paper 2 0.89237124 <a title="282-lda-2" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>3 0.89092505 <a title="282-lda-3" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>Author: Meihong Wang, Fei Sha, Michael I. Jordan</p><p>Abstract: We apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction. In this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses. We extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same. Our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data. Furthermore, when used in conjunction with supervised learners for classiﬁcation, our methods lead to lower classiﬁcation errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.</p><p>4 0.83334351 <a title="282-lda-4" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>5 0.83062762 <a title="282-lda-5" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>6 0.82991564 <a title="282-lda-6" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>7 0.82957906 <a title="282-lda-7" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>8 0.82874709 <a title="282-lda-8" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>9 0.82856637 <a title="282-lda-9" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>10 0.82835031 <a title="282-lda-10" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>11 0.82428986 <a title="282-lda-11" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>12 0.82236326 <a title="282-lda-12" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>13 0.82234991 <a title="282-lda-13" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>14 0.82204431 <a title="282-lda-14" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>15 0.82143247 <a title="282-lda-15" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>16 0.82103062 <a title="282-lda-16" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>17 0.82036978 <a title="282-lda-17" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>18 0.81999588 <a title="282-lda-18" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>19 0.81905955 <a title="282-lda-19" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>20 0.81881213 <a title="282-lda-20" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
