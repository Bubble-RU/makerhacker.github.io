<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>262 nips-2010-Switched Latent Force Models for Movement Segmentation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-262" href="#">nips2010-262</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>262 nips-2010-Switched Latent Force Models for Movement Segmentation</h1>
<br/><p>Source: <a title="nips-2010-262-pdf" href="http://papers.nips.cc/paper/4001-switched-latent-force-models-for-movement-segmentation.pdf">pdf</a></p><p>Author: Mauricio Alvarez, Jan R. Peters, Neil D. Lawrence, Bernhard Schölkopf</p><p>Abstract: Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology. 1</p><p>Reference: <a title="nips-2010-262-reference" href="../nips2010_reference/nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. [sent-4, score-0.282]
</p><p>2 In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. [sent-5, score-0.293]
</p><p>3 To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. [sent-6, score-0.688]
</p><p>4 We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. [sent-8, score-0.088]
</p><p>5 Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology. [sent-9, score-0.131]
</p><p>6 1  Introduction  Latent force models [1] are a new approach for modeling data that allows combining dimensionality reduction with systems of differential equations. [sent-10, score-0.159]
</p><p>7 The assumption is that the R forcing functions drive the D observed functions through a set of differential equation models. [sent-12, score-0.088]
</p><p>8 Each differential equation is driven by a weighted mix of latent forcing functions. [sent-13, score-0.191]
</p><p>9 Sets of coupled differential equations arise in many physics and engineering problems particularly when the temporal evolution of a system needs to be described. [sent-14, score-0.071]
</p><p>10 A latent force model differs from classical approaches as it places a probabilistic process prior over the latent functions and hence can make statements about the uncertainty in the system. [sent-18, score-0.345]
</p><p>11 A joint Gaussian process model over the latent forcing functions and the observed data functions can be recovered using a Gaussian process prior in conjunction with linear differential equations [1]. [sent-19, score-0.208]
</p><p>12 The resulting latent force modeling framework allows the combination of the knowledge of the systems dynamics with a data driven model. [sent-20, score-0.232]
</p><p>13 If a single Gaussian process prior is used to represent each latent function then the models we consider are limited to smooth driving functions. [sent-22, score-0.131]
</p><p>14 However, discontinuities and segmented latent forces are omnipresent in real-world data. [sent-23, score-0.167]
</p><p>15 For example, impact forces due to contacts in a mechanical dynamical system (when grasping an object or when the feet touch the ground) or a switch in an electrical circuit result in discontinuous latent forces. [sent-24, score-0.218]
</p><p>16 In this paper, we extract a sequence of dynamical systems motor primitives modeled by second order linear differential equations in conjunction with forcing functions (as in [1, 6]) from human movement to be used as demonstrations of elementary movements for an anthropomorphic robot. [sent-27, score-0.223]
</p><p>17 As human trajectories have a large variability: both due to planned uncertainty of the human’s movement policy, as well as due to motor execution errors [7], a probabilistic model is needed to capture the underlying motor primitives. [sent-28, score-0.107]
</p><p>18 A set of second order differential equations is employed as mechanical systems are of the same type and a temporal Gaussian process prior is used to allow probabilistic modeling [1]. [sent-29, score-0.082]
</p><p>19 To be able to obtain a sequence of dynamical systems, we augment the latent force model to include discontinuities in the latent function and change dynamics. [sent-30, score-0.405]
</p><p>20 We introduce discontinuities by switching between different Gaussian process models (superﬁcially similar to a mixture of Gaussian processes; however, the switching times are modeled as parameters so that at any instant a single Gaussian process is driving the system). [sent-31, score-0.27]
</p><p>21 Continuity of the observed functions is then ensured by constraining the relevant state variables (for example in a second order differential equation velocity and displacement) to be continuous across the switching points. [sent-32, score-0.169]
</p><p>22 2  Review of Latent force models (LFM)  Latent force models [1] are hybrid models that combine mechanistic principles and Gaussian processes as a ﬂexible way to introduce prior knowledge for data modeling. [sent-35, score-0.248]
</p><p>23 A set of D functions {yd (t)}D is modeled as the set of output functions of a series of coupled differential equations, d=1 whose common input is a linear combination of R latent functions, {ur (t)}R . [sent-36, score-0.182]
</p><p>24 We assume the output yd (t) is described by Ad  d2 yd (t) dyd (t) + Cd + κd yd (t) = dt2 dt  R r=1 Sd,r ur (t),  where, for a mass-spring-damper system, Ad would represent the mass, Cd the damper and κd , the spring constant associated to the output d. [sent-38, score-0.941]
</p><p>25 They are used to represent the relative strength that the latent force r exerts over the output d. [sent-40, score-0.259]
</p><p>26 Note that models that learn a forcing function to drive a linear system have proven to be well-suited for imitation learning for robot systems [6]. [sent-42, score-0.103]
</p><p>27 The solution of the second order ODE follows yd (t) = yd (0)cd (t) + yd (0)ed (t) + fd (t, u), ˙  (1)  where yd (0) and yd (0) are the output and the velocity at time t = 0, respectively, known as the ˙ 2 initial conditions (IC). [sent-43, score-1.549]
</p><p>28 The angular frequency is given by ωd = (4Ad κd − Cd )/(4A2 ) and the d remaining variables are given by αd e−αd t cd (t) = e−αd t cos(ωd t) + sin(ωd t) , ed (t) = sin(ωd t), ωd ωd t t Sd Sd fd (t, u) = Gd (t − τ )u(τ )dτ = e−αd (t−τ ) sin[(t − τ )ωd ]u(τ )dτ, Ad ωd 0 Ad ω d 0 with αd = Cd /(2Ad ). [sent-44, score-0.097]
</p><p>29 Note that fd (t, u) has an implicit dependence on the latent function u(t). [sent-45, score-0.144]
</p><p>30 (1) is due to the fact that the latent force u(t) and the initial conditions yd (0) and yd (0) are not known. [sent-47, score-0.817]
</p><p>31 We will assume that the latent function u(t) is sampled from a zero ˙ mean Gaussian process prior, u(t) ∼ GP(0, ku,u (t, t )), with covariance function ku,u (t, t ). [sent-48, score-0.161]
</p><p>32 So the covariance function kfd ,fd (t, t ) depends on the covariance function of the latent force u(t). [sent-56, score-0.38]
</p><p>33 If we assume the latent function has a radial basis function (RBF) covariance, ku,u (t, t ) = exp[−(t − t )2 / 2 ], then kfd ,fd (t, t ) can be computed analytically [1] (see also supplementary material). [sent-57, score-0.181]
</p><p>34 The latent force model induces a joint Gaussian process model across all the outputs. [sent-58, score-0.233]
</p><p>35 The parameters of the covariance function are given by the parameters of the differential equations and the length scale of the latent force. [sent-59, score-0.204]
</p><p>36 In this paper we look to extend the framework to the case where there can be discontinuities in the latent functions. [sent-62, score-0.127]
</p><p>37 We do this through switching between different Gaussian process models to drive the system. [sent-63, score-0.121]
</p><p>38 3  Switching dynamical latent force models (SDLFM)  We now consider switching the system between different latent forces. [sent-64, score-0.482]
</p><p>39 This allows us to change the dynamical system and the driving force for each segment. [sent-65, score-0.207]
</p><p>40 By constraining the displacement and velocity at each switching time to be the same, the output functions remain continuous. [sent-66, score-0.165]
</p><p>41 1 Deﬁnition of the model We assume that the input space is divided in a series of non-overlapping intervals [tq−1 , tq ]Q . [sent-68, score-0.897]
</p><p>42 q=1 During each interval, only one force uq−1 (t) out of Q forces is active, that is, there are {uq−1 (t)}Q q=1 forces. [sent-69, score-0.158]
</p><p>43 The force uq−1 (t) is activated after time tq−1 (switched on) and deactivated (switched off) after time tq . [sent-70, score-0.992]
</p><p>44 A particular output zd (t) at a particular time instant t, in the interval (tq−1 , tq ), is expressed as q q q zd (t) = yd (t − tq−1 ) = cq (t − tq−1 )yd (tq−1 ) + eq (t − tq−1 )yd (tq−1 ) + fd (t − tq−1 , uq−1 ). [sent-72, score-1.592]
</p><p>45 ˙q d d  This equation is assummed to be valid for describing the output only inside the interval (tq−1 , tq ). [sent-73, score-0.957]
</p><p>46 q Here we highlighted this idea by including the superscript q in yd (t − tq−1 ) to represent the interval q for which the equation holds, although later we will omit it to keep the notation uncluttered. [sent-74, score-0.328]
</p><p>47 Note that for Q = 1 and t0 = 0, we recover the original latent force model given in equation (1). [sent-75, score-0.232]
</p><p>48 0  Given the parameters θ = {{Ad , Cd , κd , Sd }D , { q−1 }Q }, the uncertainty in the outputs is q=1 d=1 q induced by the prior over the initial conditions yd (tq−1 ), yd (tq−1 ) for all values of tq−1 and the ˙q prior over latent force uq−1 (t) that is active during (tq−1 , tq ). [sent-77, score-1.725]
</p><p>49 We place independent Gaussian process priors over each of these latent forces uq−1 (t), assuming independence between them. [sent-78, score-0.155]
</p><p>50 q For initial conditions yd (tq−1 ), yd (tq−1 ), we could assume that they are either parameters to ˙q be estimated or random variables with uncertainty governed by independent Gaussian distribuq tions with covariance matrices KIC as described in the last section. [sent-79, score-0.653]
</p><p>51 However, for the class of applications we will consider: mechanical systems, the outputs should be continuous across the switching points. [sent-80, score-0.134]
</p><p>52 We therefore assume that the uncertainty about the initial conditions q for the interval q, yd (tq−1 ), yd (tq−1 ) are proscribed by the Gaussian process that describes the ˙q outputs zd (t) and velocities zd (t) in the previous interval q − 1. [sent-81, score-0.937]
</p><p>53 We also consider ˙ q−1 ˙ q−1 ˙ ˙ covariances between zd (tq−1 ) and zd (tq −1 ), this is, between positions and velocities for different ˙ values of q and d. [sent-83, score-0.243]
</p><p>54 Let us assume we have one output (D = 1) and three switching intervals (Q = 3) with switching points t0 , t1 and t2 . [sent-85, score-0.256]
</p><p>55 Figure 1 shows an example of the switching ˙ ˙ dynamical latent force model scenario. [sent-92, score-0.365]
</p><p>56 To ensure the continuity of the outputs, the initial condition is forced to be equal to the output of the last interval evaluated at the switching point. [sent-93, score-0.197]
</p><p>57 2  z(t)  The covariance function  The derivation of the covariance function for the switching model is rather y 2 (t − t1 ) involved. [sent-95, score-0.189]
</p><p>58 For continuous output signals, we must take into account cony 2 (t2 − t1 ) straints at each switching y 2 (t1 ) time. [sent-96, score-0.136]
</p><p>59 This effort is worth- ical latent force model with Q = 3. [sent-98, score-0.22]
</p><p>60 The initial conditions y q (tq−1 ) for each while though as the result- interval are matched to the value of the output in the last interval, evaluated at q q−1 ing model is very ﬂexible the switching point tq−1 , this is, y (tq−1 ) = y (tq−1 − tq−2 ). [sent-99, score-0.197]
</p><p>61 and can take advantage of the switching dynamics to represent a range of signals. [sent-100, score-0.097]
</p><p>62 As a taster, Figure 2 shows samples from a covariance function of a switching dynamical latent force model with D = 1 and Q = 3. [sent-101, score-0.411]
</p><p>63 Note that while the latent forces (a and c) are discrete, the outputs (b and d) are continuous and have matching gradients at the switching points. [sent-102, score-0.263]
</p><p>64 The switching times turn out to be parameters of the covariance function. [sent-104, score-0.143]
</p><p>65 4  In general, we need to compute the covariance kzd ,zd (t, t ) = cov[zd (t), zd (t )] for zd (t) in time interval (tq−1 , tq ) and zd (t ) in time interval (tq −1 , tq ). [sent-119, score-2.387]
</p><p>66 By deﬁnition, this covariance follows q q cov[zd (t), zd (t )] = cov yd (t − tq−1 ), yd (t − tq −1 )) . [sent-120, score-1.665]
</p><p>67 We assumme independence between the latent forces uq (t) and independence between the initial conditions yIC and the latent forces uq (t). [sent-121, score-0.431]
</p><p>68 ˙q ˙q ˙ ˙  q q q kfd ,fd (t, t ) = cov[fd (t − tq−1 )fd (t − tq−1 )]. [sent-123, score-0.068]
</p><p>69 q−1 q−1 In expression (3), kzd ,zd (tq−1 , tq−1 ) = cov[yd (tq−1 − tq−2 ), yd (tq−1 − tq−2 )] and values for kzd ,zd (tq−1 , tq−1 ), kzd ,zd (tq−1 , tq−1 ) and kzd ,zd (tq−1 , tq−1 ) can be obtained by similar ex˙ ˙ ˙ ˙ q pressions. [sent-124, score-1.104]
</p><p>70 The covariance kfd ,fd (t, t ) follows a similar expression that the one for kfd ,fd (t, t ) in equation (2), now depending on the covariance kuq−1 ,uq−1 (t, t ). [sent-125, score-0.24]
</p><p>71 We will assume that the covariances for the latent forces follow the RBF form, with length-scale q . [sent-126, score-0.156]
</p><p>72 q When q > q , we have to take into account the correlation between the initial conditions yd (tq−1 ), q yd (tq−1 ) and the latent force uq −1 (t ). [sent-127, score-0.876]
</p><p>73 This correlation appears because of the contribution of ˙ q uq −1 (t ) to the generation of the initial conditions, yd (tq−1 ), yd (tq−1 ). [sent-128, score-0.642]
</p><p>74 q q 3 We will write fd (t − tq−1 , uq−1 ) as fd (t − tq−1 ) for notational simplicity. [sent-138, score-0.084]
</p><p>75 The authors call this covariance continuous conditionally independent covariance function. [sent-149, score-0.092]
</p><p>76 In our switched latent force model, a more natural option is to use the initial conditions as the way to transit smoothly between different regimes. [sent-150, score-0.272]
</p><p>77 4, the authors propose covariances that account for a sudden change in the input scale and a sudden change in the output scale. [sent-153, score-0.091]
</p><p>78 This reference is less concerned about the particular type of change that is represented by the model: in our application scenario, the continuity of the covariance function between two regimes must be assured beforehand. [sent-157, score-0.07]
</p><p>79 The covariance functions kzd ,zd (t, t ), kzd ,zd (t, t ) and ˙ ˙ kzd ,zd (t, t ) are obtained by taking derivatives of kzd ,zd (t, t ) with respect to t and t [10]. [sent-162, score-0.866]
</p><p>80 , zd (tN )] , Kz,z is a D × D block-partitioned matrix with blocks Kzd ,zd . [sent-171, score-0.108]
</p><p>81 The entries in each of these blocks are evaluated using kzd ,zd (t, t ). [sent-172, score-0.205]
</p><p>82 Furthermore, kzd ,zd (t, t ) is computed using the expressions (3), and (4), according to the relative values of q and q . [sent-173, score-0.205]
</p><p>83 In the ﬁrst experiment, we sample from a model with D = 2, R = 1 and Q = 3, with switching points t0 = −1, t1 = 5 and t2 = 12. [sent-180, score-0.097]
</p><p>84 1 0  5  10  15  20  Figure 4: Mean and two standard deviations for the predictions over the latent force and two of the three outputs in the test set. [sent-258, score-0.244]
</p><p>85 For the second toy experiment, we assume D = 3, Q = 2 and switching points t0 = −2 and t1 = 8. [sent-269, score-0.141]
</p><p>86 In ﬁgures 4(d), 4(e) and 4(f), the inferred latent force and the predictions made for two of the three outputs. [sent-278, score-0.22]
</p><p>87 2 Segmentation of human movement data for robot imitation learning In this section, we evaluate the feasibility of the model for motion segmentation with possible applications in the analysis of human movement data and imitation learning. [sent-280, score-0.176]
</p><p>88 To do so, we had a human teacher take the robot by the hand and have him demonstrate striking movements in a cooperative game of table tennis with another human being as shown in Figure 3. [sent-281, score-0.131]
</p><p>89 4  Latent Force  Value of the log−likelihood  10 Time  (e) Latent force Try 2. [sent-293, score-0.118]
</p><p>90 5  −2  −1  5  10 Time  15  5  10 Time  15  Figure 5: Employing the switching dynamical LFM model on the human movement data collected as in Fig. [sent-296, score-0.194]
</p><p>91 The ﬁrst row corresponds to the loglikelihood, latent force and one of four outputs for trial one. [sent-298, score-0.265]
</p><p>92 angular velocities, and angular acceleration of the robot for two independent trials of the same table tennis exercise. [sent-301, score-0.076]
</p><p>93 For each trial, we selected four output positions and train several models for different values of Q, including the latent force model without switches (Q = 1). [sent-302, score-0.268]
</p><p>94 Figure 5 shows the log-likelihood, the inferred latent force and one output for trial one (ﬁrst row) and the corresponding quantities for trial two (second row). [sent-304, score-0.301]
</p><p>95 As the movement has few gaps and the data has several output dimensions, it is hard even for a human being to detect the transitions between movements (unless it is visualized as in a movie). [sent-306, score-0.11]
</p><p>96 As a result, we obtained not only a segmentation of the movement but also a generative model for table tennis striking movements. [sent-309, score-0.07]
</p><p>97 7  Conclusion  We have introduced a new probabilistic model that develops the latent force modeling framework with switched Gaussian processes. [sent-310, score-0.243]
</p><p>98 This allows for discontinuities in the latent space of forces. [sent-311, score-0.127]
</p><p>99 We have shown the application of the model in toy examples and on a real world robot problem, in which we were interested in ﬁnding and representing striking movements. [sent-312, score-0.098]
</p><p>100 Other applications of the switching latent force model that we envisage include modeling human motion capture data using the second order ODE and a ﬁrst order ODE for modeling of complex circuits in biological networks. [sent-313, score-0.337]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tq', 0.874), ('yd', 0.284), ('kzd', 0.205), ('force', 0.118), ('zd', 0.108), ('latent', 0.102), ('switching', 0.097), ('cov', 0.069), ('kfd', 0.068), ('uq', 0.059), ('cq', 0.057), ('dynamical', 0.048), ('smse', 0.048), ('covariance', 0.046), ('cd', 0.044), ('toy', 0.044), ('fd', 0.042), ('differential', 0.041), ('forces', 0.04), ('robot', 0.039), ('eq', 0.039), ('output', 0.039), ('xd', 0.033), ('interval', 0.032), ('gd', 0.029), ('movement', 0.029), ('kic', 0.027), ('msll', 0.027), ('shef', 0.027), ('ad', 0.026), ('discontinuities', 0.025), ('motor', 0.024), ('outputs', 0.024), ('forcing', 0.024), ('switched', 0.023), ('intervals', 0.023), ('movements', 0.022), ('vd', 0.022), ('neil', 0.022), ('ode', 0.022), ('sd', 0.021), ('trial', 0.021), ('gaussian', 0.021), ('kmd', 0.02), ('lfm', 0.02), ('standarized', 0.02), ('human', 0.02), ('velocity', 0.019), ('yic', 0.018), ('sin', 0.017), ('mauricio', 0.016), ('driving', 0.016), ('alvarez', 0.015), ('tennis', 0.015), ('system', 0.015), ('striking', 0.015), ('equations', 0.015), ('try', 0.015), ('initial', 0.015), ('covariances', 0.014), ('conditions', 0.014), ('imitation', 0.014), ('continuity', 0.014), ('garnett', 0.014), ('manchester', 0.014), ('sensitivities', 0.014), ('sfe', 0.014), ('mechanical', 0.013), ('velocities', 0.013), ('process', 0.013), ('driven', 0.012), ('subindex', 0.012), ('barrett', 0.012), ('haptic', 0.012), ('wam', 0.012), ('mlss', 0.012), ('mq', 0.012), ('osborne', 0.012), ('processes', 0.012), ('equation', 0.012), ('segmentation', 0.011), ('angular', 0.011), ('luengo', 0.011), ('ur', 0.011), ('michalis', 0.011), ('supplementary', 0.011), ('drive', 0.011), ('gp', 0.01), ('uncertainty', 0.01), ('transcription', 0.01), ('employing', 0.01), ('change', 0.01), ('displacement', 0.01), ('roman', 0.01), ('subsections', 0.009), ('switches', 0.009), ('hq', 0.009), ('material', 0.009), ('instant', 0.009), ('sudden', 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="262-tfidf-1" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>Author: Mauricio Alvarez, Jan R. Peters, Neil D. Lawrence, Bernhard Schölkopf</p><p>Abstract: Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology. 1</p><p>2 0.053193528 <a title="262-tfidf-2" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>3 0.052130792 <a title="262-tfidf-3" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>Author: Ning Chen, Jun Zhu, Eric P. Xing</p><p>Abstract: Learning from multi-view data is important in many applications, such as image classiﬁcation and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulﬁlls a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efﬁcient inference and parameter estimation methods for the latent subspace model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classiﬁcation, annotation and retrieval.</p><p>4 0.051046975 <a title="262-tfidf-4" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>5 0.049538396 <a title="262-tfidf-5" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>6 0.047161669 <a title="262-tfidf-6" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>7 0.046316549 <a title="262-tfidf-7" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>8 0.045841545 <a title="262-tfidf-8" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>9 0.032931749 <a title="262-tfidf-9" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>10 0.031864863 <a title="262-tfidf-10" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>11 0.031685889 <a title="262-tfidf-11" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>12 0.030853737 <a title="262-tfidf-12" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>13 0.02870244 <a title="262-tfidf-13" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>14 0.02842227 <a title="262-tfidf-14" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>15 0.025914766 <a title="262-tfidf-15" href="./nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">29 nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<p>16 0.023872172 <a title="262-tfidf-16" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>17 0.023766315 <a title="262-tfidf-17" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>18 0.022655532 <a title="262-tfidf-18" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>19 0.022579517 <a title="262-tfidf-19" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>20 0.022093609 <a title="262-tfidf-20" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.058), (1, 0.011), (2, -0.034), (3, 0.018), (4, -0.03), (5, 0.014), (6, 0.008), (7, 0.03), (8, -0.019), (9, -0.007), (10, -0.006), (11, -0.003), (12, 0.013), (13, -0.009), (14, 0.001), (15, 0.038), (16, -0.018), (17, 0.044), (18, 0.064), (19, -0.006), (20, -0.031), (21, 0.061), (22, -0.043), (23, -0.022), (24, -0.027), (25, -0.006), (26, 0.013), (27, 0.029), (28, 0.03), (29, -0.017), (30, 0.132), (31, -0.039), (32, 0.072), (33, 0.024), (34, -0.017), (35, 0.008), (36, 0.02), (37, 0.003), (38, -0.038), (39, 0.006), (40, -0.01), (41, 0.072), (42, 0.058), (43, -0.024), (44, -0.076), (45, 0.002), (46, -0.028), (47, 0.081), (48, 0.013), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93279076 <a title="262-lsi-1" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>Author: Mauricio Alvarez, Jan R. Peters, Neil D. Lawrence, Bernhard Schölkopf</p><p>Abstract: Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology. 1</p><p>2 0.56790274 <a title="262-lsi-2" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>Author: Iain Murray, Ryan P. Adams</p><p>Abstract: The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be speciﬁed using unknown hyperparameters. Integrating over these hyperparameters considers diﬀerent possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes. 1</p><p>3 0.53134853 <a title="262-lsi-3" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>4 0.52978206 <a title="262-lsi-4" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>Author: Ning Chen, Jun Zhu, Eric P. Xing</p><p>Abstract: Learning from multi-view data is important in many applications, such as image classiﬁcation and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulﬁlls a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efﬁcient inference and parameter estimation methods for the latent subspace model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classiﬁcation, annotation and retrieval.</p><p>5 0.47442192 <a title="262-lsi-5" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>6 0.46340346 <a title="262-lsi-6" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>7 0.45667943 <a title="262-lsi-7" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>8 0.42610538 <a title="262-lsi-8" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>9 0.40632284 <a title="262-lsi-9" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>10 0.38945878 <a title="262-lsi-10" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>11 0.37520352 <a title="262-lsi-11" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>12 0.37496436 <a title="262-lsi-12" href="./nips-2010-Copula_Processes.html">54 nips-2010-Copula Processes</a></p>
<p>13 0.37146747 <a title="262-lsi-13" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>14 0.33555806 <a title="262-lsi-14" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>15 0.33031175 <a title="262-lsi-15" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>16 0.32304183 <a title="262-lsi-16" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>17 0.31719291 <a title="262-lsi-17" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>18 0.31037703 <a title="262-lsi-18" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>19 0.31025529 <a title="262-lsi-19" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>20 0.29987118 <a title="262-lsi-20" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.349), (16, 0.022), (27, 0.041), (30, 0.029), (35, 0.014), (36, 0.014), (45, 0.175), (50, 0.06), (52, 0.019), (60, 0.01), (77, 0.034), (78, 0.018), (85, 0.012), (90, 0.041), (97, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96021688 <a title="262-lda-1" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>2 0.93010372 <a title="262-lda-2" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>Author: Andrey Bernstein, Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the online binary classiﬁcation problem, where we are given m classiﬁers. At each stage, the classiﬁers map the input to the probability that the input belongs to the positive class. An online classiﬁcation meta-algorithm is an algorithm that combines the outputs of the classiﬁers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classiﬁers. In this paper, we use sensitivity and speciﬁcity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisﬁes the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classiﬁers that satisﬁes fprate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classiﬁers, we show that this algorithm takes a very simple form. To our best knowledge, this is the ﬁrst algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting. 1</p><p>3 0.91335702 <a title="262-lda-3" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>Author: Jacob Bien, Ya Xu, Michael W. Mahoney</p><p>Abstract: The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.</p><p>4 0.89468902 <a title="262-lda-4" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>Author: Arvind Agarwal, Samuel Gerber, Hal Daume</p><p>Abstract: We present a novel method for multitask learning (MTL) based on manifold regularization: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common linear subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is ﬁxed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efﬁcient and easy to implement. We show the efﬁcacy of our method on several datasets. 1</p><p>5 0.88042831 <a title="262-lda-5" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>Author: Mohammad E. Khan, Guillaume Bouchard, Kevin P. Murphy, Benjamin M. Marlin</p><p>Abstract: We propose a new variational EM algorithm for ﬁtting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function. In the special case of fully observed binary data, the bound we propose is signiﬁcantly faster than previous variational methods. We show that EM is signiﬁcantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods. A further beneﬁt of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method. 1</p><p>6 0.86342323 <a title="262-lda-6" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>7 0.85866892 <a title="262-lda-7" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>same-paper 8 0.80150384 <a title="262-lda-8" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>9 0.79647779 <a title="262-lda-9" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>10 0.72633016 <a title="262-lda-10" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>11 0.72321165 <a title="262-lda-11" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>12 0.70398676 <a title="262-lda-12" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>13 0.70080745 <a title="262-lda-13" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>14 0.69735748 <a title="262-lda-14" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>15 0.6879136 <a title="262-lda-15" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>16 0.68692225 <a title="262-lda-16" href="./nips-2010-Sparse_Coding_for_Learning_Interpretable_Spatio-Temporal_Primitives.html">246 nips-2010-Sparse Coding for Learning Interpretable Spatio-Temporal Primitives</a></p>
<p>17 0.68501925 <a title="262-lda-17" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>18 0.68344551 <a title="262-lda-18" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>19 0.68184912 <a title="262-lda-19" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>20 0.68162262 <a title="262-lda-20" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
