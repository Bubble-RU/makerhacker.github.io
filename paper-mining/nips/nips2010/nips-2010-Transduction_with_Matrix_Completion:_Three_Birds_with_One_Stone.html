<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-275" href="#">nips2010-275</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</h1>
<br/><p>Source: <a title="nips-2010-275-pdf" href="http://papers.nips.cc/paper/3932-transduction-with-matrix-completion-three-birds-with-one-stone.pdf">pdf</a></p><p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>Reference: <a title="nips-2010-275-reference" href="../nips2010_reference/nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We pose transductive classiﬁcation as a matrix completion problem. [sent-6, score-0.535]
</p><p>2 We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. [sent-8, score-0.176]
</p><p>3 Our method allows for different loss functions to apply on the feature and label entries of the matrix. [sent-9, score-0.369]
</p><p>4 The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. [sent-10, score-0.196]
</p><p>5 In this work, we present two transductive learning methods under the novel assumption that the feature-by-item and label-by-item matrices are jointly low rank. [sent-12, score-0.245]
</p><p>6 This assumption effectively couples different label prediction tasks, allowing us to implicitly use observed labels in one task to recover unobserved labels in others. [sent-13, score-0.559]
</p><p>7 In fact, our methods learn in the difﬁcult regime of multi-label transductive learning with missing data that one sometimes encounters in practice. [sent-15, score-0.41]
</p><p>8 That is, each item is associated with many class labels, many of the items’ labels may be unobserved (some items may be completely unlabeled across all labels), and many features may also be unobserved. [sent-16, score-0.426]
</p><p>9 Our methods build upon recent advances in matrix completion, with efﬁcient algorithms to handle matrices with mixed real-valued features and discrete labels. [sent-17, score-0.159]
</p><p>10 xn ] be the d × n feature matrix whose columns are the items. [sent-26, score-0.181]
</p><p>11 Let ΩX be the index set of observed features in X, such that (i, j) ∈ ΩX if and only if xij is observed. [sent-35, score-0.257]
</p><p>12 Similarly, let ΩY be the index set of observed labels in Y. [sent-36, score-0.195]
</p><p>13 Our main goal is to predict the missing labels yij for (i, j) ∈ ΩY . [sent-37, score-0.543]
</p><p>14 Of course, this reduces to standard transductive / learning when t = 1, |ΩX | = nd (no missing features), and 1 < |ΩY | < n (some missing labels). [sent-38, score-0.602]
</p><p>15 In our more general setting, as a side product we are also interested in imputing the missing features, and de-noising the observed features, in X. [sent-39, score-0.306]
</p><p>16 In a nutshell, we assume that X and Y are jointly produced by an underlying low rank matrix. [sent-43, score-0.176]
</p><p>17 We then take advantage of the sparsity to ﬁll in the missing labels and features using a modiﬁed method of matrix completion. [sent-44, score-0.461]
</p><p>18 It starts from a d × n low rank “pre”-feature matrix X0 , with rank(X0 ) min(d, n). [sent-46, score-0.278]
</p><p>19 The actual feature matrix X is obtained by adding iid Gaussian noise to the entries of X0 : X = X0 + , 0 0 0 where ij ∼ N (0, σ 2 ). [sent-47, score-0.398]
</p><p>20 ytj ≡ yj ∈ Rt of item j are 0 0 produced by yj = Wxj + b, where W is a t × d weight matrix, and b ∈ Rt is a bias vector. [sent-51, score-0.245]
</p><p>21 Note the combined (t + d) × n matrix Y0 ; X0 is low rank too: rank( Y0 ; X0 ) ≤ rank(X0 ) + 1. [sent-56, score-0.278]
</p><p>22 The actual label yij ∈ {−1, 1} is generated randomly 0 0 via a sigmoid function: P (yij |yij ) = 1/ 1 + exp(−yij yij ) . [sent-57, score-0.562]
</p><p>23 Finally, two random masks ΩX , ΩY are applied to expose only some of the entries in X and Y, and we use ω to denote the percentage of observed entries. [sent-58, score-0.281]
</p><p>24 Given the partially observed features and labels as speciﬁed by X, Y, ΩX , ΩY , we would like to recover the intermediate low rank matrix Y0 ; X0 . [sent-62, score-0.546]
</p><p>25 The key assumption is that the (t + d) × n stacked matrix Y0 ; X0 is of low rank. [sent-64, score-0.182]
</p><p>26 sign(zij ) = yij , ∀(i, j) ∈ ΩY ;  z(i+t)j = xij , ∀(i, j) ∈ ΩX  Here, Z is meant to recover Y0 ; X0 by directly minimizing the rank while obeying the observed features and labels. [sent-68, score-0.63]
</p><p>27 To index the corresponding element in the larger stacked matrix Z, we need to shift the row index by t to skip the part for Y0 , and hence the constraints z(i+t)j = xij . [sent-73, score-0.369]
</p><p>28 Following recent work in matrix completion [3, 2], we relax rank() with the convex nuclear norm Z ∗ = min(t+d,n) σk (Z), where σk ’s are the singular values of Z. [sent-76, score-0.465]
</p><p>29 Instead of the equality constraints in (1), we minimize a loss function cx (z(i+t)j , xij ). [sent-79, score-0.205]
</p><p>30 The observed labels are of a different type than the observed features. [sent-82, score-0.22]
</p><p>31 We therefore introduce another loss function cy (zij , yij ) to account for the heterogeneous data. [sent-83, score-0.349]
</p><p>32 In addition to these changes, we will model the bias b either explicitly or implicitly, leading to two alternative matrix completion formulations below. [sent-85, score-0.391]
</p><p>33 Here, Z corresponds to the stacked matrix WX0 ; X0 instead of Y0 ; X0 , making it potentially lower rank. [sent-88, score-0.155]
</p><p>34 The optimization problem is argmin Z,b  µ Z  ∗  +  λ |ΩY |  cy (zij + bi , yij ) + (i,j)∈ΩY  2  1 |ΩX |  cx (z(i+t)j , xij ), (i,j)∈ΩX  (2)  where µ, λ are positive trade-off weights. [sent-89, score-0.543]
</p><p>35 Once the optimal Z, b are found, we recover the task-i label of item j by sign(zij + bi ), and feature k of item j by z(k+t)j . [sent-92, score-0.549]
</p><p>36 Similar to how bias is commonly handled in linear classiﬁers, we append an additional feature with constant value one to each item. [sent-95, score-0.153]
</p><p>37 Under the same label assumption yj = Wx0 + b, the rows of the soft label matrix j 0 0 Y are linear combinations of rows in X ; 1 , i. [sent-97, score-0.45]
</p><p>38 We then let Z correspond to the (t + d + 1) × n stacked matrix Y0 ; X0 ; 1 , by forcing its last row to be 1 (hence the name): 1 λ cy (zij , yij ) + cx (z(i+t)j , xij ) (3) argmin µ Z ∗+ |ΩY | |ΩX | Z∈R(t+d+1)×n (i,j)∈ΩY  (i,j)∈ΩX  s. [sent-100, score-0.697]
</p><p>39 Once the optimal Z is found, we recover the task-i label of item j by sign(zij ), and feature k of item j by z(k+t)j . [sent-104, score-0.509]
</p><p>40 One way is to let Z correspond to Y0 ; X0 directly, without introducing bias b or the all-1 row, and hope nuclear norm minimization will prevail. [sent-108, score-0.169]
</p><p>41 3  Input: Initial matrix Z0 , Input: Initial matrix Z0 , bias b0 , parameters µ, λ, Step sizes τZ parameters µ, λ, Step sizes τb , τZ Determine µ1 > µ2 > · · · > µL = µ > 0. [sent-124, score-0.334]
</p><p>42 In the shrinkage step, SτZ µ (·) is a matrix shrinkage operator. [sent-136, score-0.222]
</p><p>43 4  Experiments  We now empirically study the ability of matrix completion to perform multi-class transductive classiﬁcation when there is missing data. [sent-162, score-0.727]
</p><p>44 We ﬁrst present a family of 24 experiments on a synthetic task by systematically varying different aspects of the task, including the rank of the problem, noise level, number of items, and observed label and feature percentage. [sent-163, score-0.555]
</p><p>45 We then present experiments on two real-world datasets: music emotions and yeast microarray. [sent-164, score-0.251]
</p><p>46 We then run our matrix completion algorithms using 4 of the observed entries, measure its performance on the remaining 1 , and average over 5 5 the ﬁve folds. [sent-169, score-0.372]
</p><p>47 Since our main goal is to predict unobserved labels, we use label error as the CV performance criterion to select parameters. [sent-170, score-0.289]
</p><p>48 25 and, as in [10], consider µ values starting at σ1 ηµ , where σ1 is the largest singular value of the matrix of observed entries in [Y; X] (with the unobserved entries set to 0), and decrease µ until 10−5 . [sent-173, score-0.513]
</p><p>49 We initialized b0 to be all zero and Z0 to be the rank-1 approximation of the matrix of observed entries in [Y; X] (with unobserved entries set to 0) obtained by performing an SVD and reconstructing the matrix using only the largest singular value and corresponding left and right singular vectors. [sent-175, score-0.668]
</p><p>50 Baselines: We compare to the following baselines, each consisting of some missing feature imputation step on X ﬁrst, then using a standard SVM to predict the labels: [FPC+SVM] Matrix completion on X alone using FPC [10]. [sent-180, score-0.898]
</p><p>51 [EM(k)+SVM] Expectation Maximization algorithm to impute missing X entries using a mixture of k Gaussian components. [sent-181, score-0.365]
</p><p>52 As in [9], missing features, mixing component parameters, and the assignments of items to components are treated as hidden variables, which are estimated in an iterative manner to maximize the likelihood of the data. [sent-182, score-0.265]
</p><p>53 [Mean+SVM] Impute each missing feature by the mean of the observed entries for that feature. [sent-183, score-0.444]
</p><p>54 Evaluation Method: To evaluate performance, we consider two measures: transductive label error, i. [sent-191, score-0.364]
</p><p>55 , the percentage of unobserved labels predicted incorrectly; and relative feature imputation error ˆ 2 / ij ∈ΩX x2 , where x is the predicted feature value. [sent-193, score-0.847]
</p><p>56 In the tables below, ˆ ij ij ∈ΩX (xij − xij ) / / for each parameter setting, we report the mean performance (and standard deviation in parenthesis) of different algorithms over 10 random trials. [sent-194, score-0.185]
</p><p>57 We ﬁrst create a rank-r matrix X0 = LR , where L ∈ Rd×r and R ∈ Rn×r with entries drawn iid from N (0, 1). [sent-199, score-0.255]
</p><p>58 Next, we create a weight matrix W ∈ Rt×d and bias vector b ∈ Rt , with all entries drawn iid from N (0, 10). [sent-201, score-0.329]
</p><p>59 Synthetic experiment results: Table 1 shows the transductive label errors, and Table 2 shows the relative feature imputation errors, on the synthetic datasets. [sent-209, score-0.891]
</p><p>60 However, the imputations are not perfect, because in these particular parameter settings the ratio between the number of observed entries over the degrees of freedom needed to describe the feature matrix (i. [sent-212, score-0.354]
</p><p>61 , r(d + n − r)) is below the necessary condition for perfect matrix completion [2], and because there is some feature noise. [sent-214, score-0.425]
</p><p>62 Furthermore, our CV tuning procedure selects parameters µ, λ to optimize label error, which often leads to suboptimal imputation performance. [sent-215, score-0.494]
</p><p>63 html  5  Table 1: Transductive label error of six algorithms on the 24 synthetic datasets. [sent-219, score-0.261]
</p><p>64 The varying parameters are feature noise σ 2 , rank(X0 ) = r, number of items n, and observed label and feature percentage ω. [sent-220, score-0.519]
</p><p>65 Each cell shows the mean(standard deviation) of transductive label error (in percentage) over 10 random trials. [sent-222, score-0.407]
</p><p>66 0  imputation error, both MC-b and MC-1 did achieve perfect feature imputation. [sent-520, score-0.456]
</p><p>67 We believe the fact that MC-b and MC-1 can use information in Y to enhance feature imputation in X made them better than FPC+SVM. [sent-523, score-0.427]
</p><p>68 Observation 2: MC-1 is the best for multi-label transductive classiﬁcation, as suggested by Table 1. [sent-524, score-0.218]
</p><p>69 Surprisingly, the feature imputation advantage of MC-b did not translate into classiﬁcation, and FPC+SVM took second place. [sent-525, score-0.427]
</p><p>70 Observation 3: The same factors that affect standard matrix completion also affect classiﬁcation performance of MC-b and MC-1. [sent-526, score-0.317]
</p><p>71 As the tables show, everything else being equal, less feature noise (smaller σ 2 ), lower rank r, more items, or more observed features and labels, reduce label error. [sent-527, score-0.515]
</p><p>72 Table 3 reveals that both MC methods achieve statistically signiﬁcantly better label prediction and imputation performance with t = 10 than with only t = 2 (as determined by two-sample t-tests at signiﬁcance level 0. [sent-532, score-0.581]
</p><p>73 html  6  Table 2: Relative feature imputation error on the synthetic datasets. [sent-544, score-0.542]
</p><p>74 The algorithm Zero+SVM is not shown because it by deﬁnition has relative feature imputation error 1. [sent-545, score-0.498]
</p><p>75 02  Table 3: More tasks help matrix completion (ω = 10%, n = 400, r = 2, d = 20, σ 2 = 0. [sent-793, score-0.345]
</p><p>76 03) relative feature imputation error  Table 4: Performance on the music emotions data. [sent-819, score-0.666]
</p><p>77 4) transductive label error  Algorithm MC-b MC-1 FPC+SVM EM1+SVM EM4+SVM Mean+SVM Zero+SVM  ω =40% 60% 80% 0. [sent-862, score-0.407]
</p><p>78 00) relative feature imputation error  We vary the percentage of observed entries ω = 40%, 60%, 80%. [sent-904, score-0.729]
</p><p>79 Most importantly, these results show that MC-1 is useful for this realworld multi-label classiﬁcation problem, leading to the best (or statistically indistinguishable from the best) transductive error performance with 60% and 80% of the data available, and close to the best with only 40%. [sent-908, score-0.369]
</p><p>80 , no indices are missing from ΩX ) and the training labels in ΩY to a standard SVM, and let it predict the unspeciﬁed labels. [sent-912, score-0.335]
</p><p>81 On the same random trials, for observed percentage ω = 40%, 60%, 80%, the oracle baseline achieved label error rate 22. [sent-913, score-0.396]
</p><p>82 For this larger dataset, we omitted the computationally expensive EM4+SVM methods, and tuned only µ for matrix completion while ﬁxing λ = 1. [sent-926, score-0.359]
</p><p>83 Table 5 reveals that MC-b leads to statistically signiﬁcantly lower transductive label error for this biological dataset. [sent-927, score-0.494]
</p><p>84 Although not highlighted in the table, MC-1 is also statistically better than the SVM methods in label error. [sent-928, score-0.206]
</p><p>85 In terms of feature imputation performance, the MC methods are weaker than FPC+SVM. [sent-929, score-0.427]
</p><p>86 However, it seems simultaneously predicting the missing labels and features appears to provide a large advantage to the MC methods. [sent-930, score-0.359]
</p><p>87 It should be pointed out that all algorithms except Zero+SVM in fact have small but non-zero standard deviation on imputation error, despite what the ﬁxed-point formatting in the table suggests. [sent-931, score-0.391]
</p><p>88 Again, we compared these algorithms to an oracle SVM baseline with 100% observed entries in ΩX . [sent-936, score-0.267]
</p><p>89 The oracle SVM approach achieves label error of 20. [sent-937, score-0.25]
</p><p>90 We attribute this advantage to a combination of multi-label learning and transduction that is intrinsic to our matrix completion methods. [sent-946, score-0.376]
</p><p>91 4) transductive label error  5  Algorithm MC-b MC-1 FPC+SVM EM1+SVM Mean+SVM Zero+SVM  ω =40% 60% 80% 0. [sent-984, score-0.407]
</p><p>92 00) relative feature imputation error  Discussions and Future Work  We have introduced two matrix completion methods for multi-label transductive learning with missing features, which outperformed several baselines. [sent-1020, score-1.225]
</p><p>93 In terms of problem formulation, our methods differ considerably from sparse multi-task learning [11, 1, 13] in that we regularize the feature and label matrix directly, without ever learning explicit weight vectors. [sent-1021, score-0.327]
</p><p>94 Our methods also differ from multi-label prediction via reduction to binary classiﬁcation or ranking [15], and via compressed sensing [7], which assumes sparsity in that each item has a small number of positive labels, rather than the low-rank nature of feature matrices. [sent-1022, score-0.198]
</p><p>95 These methods do not naturally allow for missing features. [sent-1023, score-0.192]
</p><p>96 Learning in the presence of missing data typically involves imputation followed by learning with completed data [9]. [sent-1026, score-0.579]
</p><p>97 Our methods perform imputation plus learning in one step, similar to EM on missing labels and features [6], but the underlying model assumption is quite different. [sent-1027, score-0.707]
</p><p>98 One future extension is to explicitly map the partial feature matrix to a partially observed polynomial (or other) kernel Gram matrix, and apply our methods there. [sent-1029, score-0.236]
</p><p>99 Though such mapping proliferates the missing entries, we hope that the low-rank structure in the kernel matrix will allow us to recover labels that are nonlinear functions of the original features. [sent-1030, score-0.45]
</p><p>100 Fixed point and Bregman iterative methods for matrix rank minimization. [sent-1083, score-0.251]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fpc', 0.484), ('imputation', 0.348), ('transductive', 0.218), ('completion', 0.215), ('svm', 0.208), ('yij', 0.208), ('missing', 0.192), ('zij', 0.173), ('rank', 0.149), ('label', 0.146), ('item', 0.119), ('entries', 0.118), ('xij', 0.115), ('labels', 0.11), ('matrix', 0.102), ('continuation', 0.101), ('nuclear', 0.095), ('emotions', 0.085), ('cy', 0.085), ('yeast', 0.083), ('music', 0.083), ('svd', 0.08), ('feature', 0.079), ('bias', 0.074), ('items', 0.073), ('synthetic', 0.072), ('unobserved', 0.067), ('zk', 0.066), ('cx', 0.064), ('cv', 0.064), ('oracle', 0.061), ('statistically', 0.06), ('shrinkage', 0.06), ('fixed', 0.059), ('imputing', 0.059), ('transduction', 0.059), ('percentage', 0.058), ('features', 0.057), ('ak', 0.055), ('impute', 0.055), ('observed', 0.055), ('singular', 0.053), ('stacked', 0.053), ('masks', 0.05), ('trohidis', 0.048), ('formulation', 0.048), ('rt', 0.048), ('indistinguishable', 0.048), ('mc', 0.046), ('bk', 0.046), ('recover', 0.046), ('error', 0.043), ('table', 0.043), ('tsoumakas', 0.043), ('tuned', 0.042), ('sdp', 0.041), ('classi', 0.041), ('bi', 0.04), ('row', 0.039), ('completed', 0.039), ('unspeci', 0.037), ('emmanuel', 0.037), ('xiaojin', 0.037), ('ij', 0.035), ('iid', 0.035), ('foreach', 0.035), ('elisseeff', 0.035), ('cance', 0.033), ('baseline', 0.033), ('predict', 0.033), ('subspace', 0.033), ('donald', 0.032), ('goldfarb', 0.032), ('story', 0.032), ('step', 0.031), ('argmin', 0.031), ('sign', 0.03), ('index', 0.03), ('soft', 0.03), ('heterogeneous', 0.03), ('baselines', 0.029), ('noise', 0.029), ('perfect', 0.029), ('modi', 0.029), ('tasks', 0.028), ('sizes', 0.028), ('cand', 0.028), ('relative', 0.028), ('reveals', 0.027), ('generation', 0.027), ('low', 0.027), ('afosr', 0.027), ('zoubin', 0.027), ('yj', 0.026), ('microarray', 0.026), ('loss', 0.026), ('em', 0.025), ('systematically', 0.025), ('implicitly', 0.025), ('trials', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="275-tfidf-1" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>2 0.24102053 <a title="275-tfidf-2" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>3 0.1828198 <a title="275-tfidf-3" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>4 0.13031918 <a title="275-tfidf-4" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>Author: Umar Syed, Ben Taskar</p><p>Abstract: We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efﬁcient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines. 1</p><p>5 0.12554112 <a title="275-tfidf-5" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>6 0.11605684 <a title="275-tfidf-6" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>7 0.10831907 <a title="275-tfidf-7" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>8 0.10697507 <a title="275-tfidf-8" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>9 0.10552412 <a title="275-tfidf-9" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>10 0.10503173 <a title="275-tfidf-10" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>11 0.10448495 <a title="275-tfidf-11" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>12 0.093387671 <a title="275-tfidf-12" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>13 0.089798227 <a title="275-tfidf-13" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>14 0.085142501 <a title="275-tfidf-14" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>15 0.084425777 <a title="275-tfidf-15" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>16 0.079418622 <a title="275-tfidf-16" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>17 0.078227103 <a title="275-tfidf-17" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>18 0.075235583 <a title="275-tfidf-18" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>19 0.074965008 <a title="275-tfidf-19" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>20 0.07244315 <a title="275-tfidf-20" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.216), (1, 0.067), (2, 0.059), (3, -0.074), (4, 0.057), (5, -0.029), (6, 0.014), (7, -0.027), (8, -0.104), (9, -0.104), (10, -0.007), (11, 0.061), (12, 0.207), (13, 0.109), (14, 0.131), (15, -0.034), (16, -0.066), (17, -0.048), (18, 0.077), (19, 0.083), (20, 0.006), (21, 0.083), (22, 0.001), (23, -0.237), (24, 0.173), (25, 0.088), (26, 0.009), (27, 0.022), (28, -0.107), (29, -0.017), (30, -0.009), (31, 0.021), (32, -0.067), (33, 0.052), (34, 0.014), (35, -0.007), (36, 0.108), (37, 0.051), (38, 0.055), (39, 0.14), (40, 0.022), (41, -0.1), (42, -0.078), (43, 0.083), (44, 0.068), (45, 0.009), (46, 0.032), (47, -0.058), (48, 0.03), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9507485 <a title="275-lsi-1" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>2 0.76548368 <a title="275-lsi-2" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>Author: David Grangier, Iain Melvin</p><p>Abstract: We present a new learning strategy for classiﬁcation problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-speciﬁc subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classiﬁcation strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then nonlinearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the ﬁnal classiﬁcation objective. This simple strategy allows great ﬂexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets. 1</p><p>3 0.76459807 <a title="275-lsi-3" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>Author: Kaushik Mitra, Sameer Sheorey, Rama Chellappa</p><p>Abstract: Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semideﬁnite program (LRSDP) with the advantage that: 1) an efﬁcient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and 2) additional constraints such as orthonormality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm ﬁnds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the afﬁne SfM problem, non-rigid SfM and photometric stereo problems.</p><p>4 0.6848098 <a title="275-lsi-4" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>Author: Prateek Jain, Raghu Meka, Inderjit S. Dhillon</p><p>Abstract: Minimizing the rank of a matrix subject to afﬁne constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under afﬁne constraints (ARMP) and show that SVP recovers the minimum rank solution for afﬁne constraints that satisfy a restricted isometry property (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of lowrank matrix completion, for which the deﬁning afﬁne constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of [5, 18, 14], for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is signiﬁcantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem. 1</p><p>5 0.63846838 <a title="275-lsi-5" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>6 0.60406548 <a title="275-lsi-6" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>7 0.60223281 <a title="275-lsi-7" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>8 0.57314003 <a title="275-lsi-8" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>9 0.47789761 <a title="275-lsi-9" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>10 0.45714727 <a title="275-lsi-10" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>11 0.449839 <a title="275-lsi-11" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>12 0.44502121 <a title="275-lsi-12" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>13 0.44472697 <a title="275-lsi-13" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>14 0.44149444 <a title="275-lsi-14" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>15 0.43940857 <a title="275-lsi-15" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>16 0.42289671 <a title="275-lsi-16" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>17 0.41950741 <a title="275-lsi-17" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>18 0.40764067 <a title="275-lsi-18" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>19 0.40104631 <a title="275-lsi-19" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>20 0.3969909 <a title="275-lsi-20" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.083), (17, 0.037), (26, 0.181), (27, 0.088), (30, 0.051), (35, 0.022), (45, 0.216), (50, 0.05), (51, 0.016), (52, 0.042), (60, 0.036), (77, 0.038), (78, 0.034), (90, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88835526 <a title="275-lda-1" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>Author: Daniel Golovin, Andreas Krause, Debajyoti Ray</p><p>Abstract: We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise–free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near–optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2 , a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the ﬁrst competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non–uniform cost and their noise is correlated. We also propose E FF ECXTIVE , a particularly fast approximation of EC 2 , and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty. 1</p><p>same-paper 2 0.87414968 <a title="275-lda-2" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>Author: Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, Xiaojin Zhu</p><p>Abstract: We pose transductive classiﬁcation as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspeciﬁed, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modiﬁed ﬁxed-point continuation method that is guaranteed to ﬁnd the global optimum. 1</p><p>3 0.85353601 <a title="275-lda-3" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>Author: Armand Joulin, Jean Ponce, Francis R. Bach</p><p>Abstract: Dimensionality reduction is commonly used in the setting of multi-label supervised classiﬁcation to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added beneﬁts in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efﬁcient algorithm based on convex relaxations and lowrank representations of the data, capable of handling large-scale problems. Experiments on text document classiﬁcation show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods. 1</p><p>4 0.80741102 <a title="275-lda-4" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>5 0.8022967 <a title="275-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.80031586 <a title="275-lda-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.80028039 <a title="275-lda-7" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>8 0.79834992 <a title="275-lda-8" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>9 0.7973271 <a title="275-lda-9" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>10 0.79677719 <a title="275-lda-10" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>11 0.79564607 <a title="275-lda-11" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>12 0.79525065 <a title="275-lda-12" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<p>13 0.79335135 <a title="275-lda-13" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>14 0.79314762 <a title="275-lda-14" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>15 0.79303104 <a title="275-lda-15" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>16 0.79285568 <a title="275-lda-16" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>17 0.79276448 <a title="275-lda-17" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>18 0.79267627 <a title="275-lda-18" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>19 0.79245126 <a title="275-lda-19" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>20 0.79188555 <a title="275-lda-20" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
