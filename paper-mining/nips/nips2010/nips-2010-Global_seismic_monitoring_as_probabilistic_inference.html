<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 nips-2010-Global seismic monitoring as probabilistic inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-107" href="#">nips2010-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 nips-2010-Global seismic monitoring as probabilistic inference</h1>
<br/><p>Source: <a title="nips-2010-107-pdf" href="http://papers.nips.cc/paper/4100-global-seismic-monitoring-as-probabilistic-inference.pdf">pdf</a></p><p>Author: Nimar Arora, Stuart Russell, Paul Kidwell, Erik B. Sudderth</p><p>Abstract: The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. 1</p><p>Reference: <a title="nips-2010-107-reference" href="../nips2010_reference/nips-2010-Global_seismic_monitoring_as_probabilistic_inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Global seismic monitoring as probabilistic inference  Nimar S. [sent-1, score-0.436]
</p><p>2 edu  Abstract The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. [sent-9, score-0.581]
</p><p>3 We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. [sent-10, score-0.479]
</p><p>4 The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. [sent-11, score-0.436]
</p><p>5 VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. [sent-12, score-0.363]
</p><p>6 The IMS is the world’s primary global-scale, continuous, real-time system for seismic event monitoring. [sent-15, score-0.773]
</p><p>7 Perfect performance remains well beyond the reach of current technology: the IDC’s automated system, a highly complex and welltuned piece of software, misses nearly one third of all seismic events in the magnitude range of interest, and about half of the reported events are spurious. [sent-17, score-0.867]
</p><p>8 Like most current systems, the IDC operates by detection of arriving signals at each sensor station (the station processing stage) and then grouping multiple detections together to form events (the network processing stage). [sent-19, score-1.209]
</p><p>9 1 The time and location of each event are found by various search methods including grid search [2], the double-difference algorithm [3], and the intersection method [4]. [sent-20, score-0.446]
</p><p>10 In the words of [5], “Seismic event location is—at its core—a minimization of the difference between observed and predicted arrival times. [sent-21, score-0.584]
</p><p>11 ” Although the mathematics of seismic event detection and 1  Network processing is thus a data association problem similar to those arising in multitarget tracking [1]. [sent-22, score-0.924]
</p><p>12 In simple terms, let X be a random variable ranging over all possible collections of events, with each event deﬁned by time, location, magnitude, and type (natural or man-made). [sent-27, score-0.388]
</p><p>13 ) NET-VISA computes a single most-likely explanation: a set of hypothesized events with their associated detections, marking all other detections as noise. [sent-39, score-0.529]
</p><p>14 All such waves occur in a variety of types [7]—body waves that travel through the earth’s interior and surface waves that travel on the surface. [sent-51, score-0.61]
</p><p>15 Each particular wave type generated by a given event is called a phase. [sent-55, score-0.423]
</p><p>16 These waves are picked up in seismic stations as ground vibrations. [sent-56, score-0.622]
</p><p>17 Typically, seismic stations have either a single 3-axis detector or an array of vertical-axis detectors spread over a scale of many kilometers. [sent-57, score-0.448]
</p><p>18 Various parameters of the detection are measured—onset time, azimuth (direction from the station to the source of the wave), slowness (related to the angle of declination of the signal path), amplitude, etc. [sent-61, score-0.634]
</p><p>19 2  Based on these parameters, a phase label may be assigned to the detection based on the standard IASPEI phase catalog [7]. [sent-62, score-0.411]
</p><p>20 The parameters of an event are its longitude, latitude, depth, time, and magnitude (mb or body-wave magnitude). [sent-65, score-0.433]
</p><p>21 We compute the accuracy of an event history hypothesis by comparison to a chosen ground-truth history. [sent-68, score-0.41]
</p><p>22 An edge is added between a predicted and a true event that are at most 5 degrees in distance2 and 50 seconds in time apart. [sent-70, score-0.419]
</p><p>23 We report 3 quantities from this matching— precision (percentage of predicted events that are matched), recall (percentage of true events that are matched), and average error (average distance in kilometers between matched events). [sent-73, score-0.558]
</p><p>24 3  Generative Probabilistic Model  Our generative model for seismic events and detections follows along the lines of the aircraft detection model in [8, Figure 3]. [sent-74, score-1.029]
</p><p>25 In our model, there is an unknown number of seismic events with unknown parameters (location, time, etc. [sent-75, score-0.588]
</p><p>26 These events produce 14 different types of seismic waves or phases. [sent-77, score-0.726]
</p><p>27 A phase from an event may or may not be detected by a station. [sent-78, score-0.547]
</p><p>28 If a phase is detected at a station, a corresponding detection is generated. [sent-79, score-0.31]
</p><p>29 Additionally, an unknown number of noise detections are generated at each station. [sent-81, score-0.29]
</p><p>30 For NET-VISA, the evidence Y = y consists only of each station’s set of detections and their parameters. [sent-82, score-0.29]
</p><p>31 If e is the set of events (of size |e|), λe is the rate of event generation, and T is the time period under consideration, we have Pθ (|e|) =  (λe · T )|e| exp (−λe · T ) . [sent-85, score-0.617]
</p><p>32 (1)  The longitude and latitude of the ith event, ei are drawn from an event location density, pl (el ) l on the surface of the earth. [sent-87, score-0.816]
</p><p>33 The depth of the event, ei is uniformly distributed up to a maximum d depth D (700 km in our experiments). [sent-88, score-0.37]
</p><p>34 Similarly, the time of the event ei is uniformly distributed t between 0 and T . [sent-89, score-0.608]
</p><p>35 The magnitude of the event, ei , is drawn from what seismologists refer to as the m Gutenberg-Richter distribution, which is in fact an exponential distribution with rate λm : Pθ (ei ) = pl (ei ) l  1 1 λm exp −λm ei . [sent-90, score-0.594]
</p><p>36 ·  |e| i  i=1  Pθ (e ) = exp (−λe · T )  pl (ei ) l i=1  1 λe λm exp −λm ei . [sent-92, score-0.326]
</p><p>37 m D  (3)  Maximum likelihood estimates of λe and λm may be easily determined from historical event frequencies and magnitudes. [sent-93, score-0.388]
</p><p>38 3  Figure 1: Heat map (large values in red, small in blue) of the prior event location density log pl (el ). [sent-97, score-0.5]
</p><p>39 2  Correct Detections  The probability that an event’s j th phase, 1 ≤ j ≤ J, is detected by a station k, 1 ≤ k ≤ K, depends on the wave type or phase, the station, and the event’s magnitude, depth, and distance to the station. [sent-104, score-0.369]
</p><p>40 Let dijk be a binary indicator variable for such a detection of event i, and ∆ik the distance between event i and station k. [sent-105, score-1.434]
</p><p>41 Then we have Pφ (dijk = 1 | ei ) = pjk (ei , ei , ∆ik ). [sent-106, score-0.503]
</p><p>42 m d d  (5)  If an event phase is detected at a station, i. [sent-107, score-0.547]
</p><p>43 dijk = 1, our model speciﬁes probability distribution for the attributes of that detection, aijk . [sent-109, score-0.499]
</p><p>44 The arrival time, aijk , is assigned a Laplacian distribut tion whose mean consists of two parts. [sent-110, score-0.417]
</p><p>45 The ﬁrst is the IASPEI travel time prediction for that phase, which depends only on the event depth and the distance between the event and station. [sent-111, score-0.945]
</p><p>46 The second is a learned station-speciﬁc correction which accounts for inhomogeneities in the earth’s crust, which allow seismic waves to travel faster or slower than the IASPEI prediction. [sent-112, score-0.605]
</p><p>47 The station-speciﬁc correction also accounts for any systematic biases in picking seismic onsets from waveforms. [sent-113, score-0.385]
</p><p>48 Let µjk t be the location of this Laplacian (a function of the event time, depth, and distance to the station) and let bjk be its scale. [sent-114, score-0.532]
</p><p>49 Truncating this Laplacian to the range of possible arrival times produces a t jk normalization constant Zt , so that Pφ (aijk | dijk = 1, ei ) = t  1 jk Zt  exp −  |aijk − µjk (ei , ei , ∆ik )| t t t d bjk t  . [sent-115, score-1.038]
</p><p>50 (6)  Similarly, the arrival azimuth and slowness follow a Laplacian distribution. [sent-116, score-0.332]
</p><p>51 The location aijk of the z arrival azimuth depends only on the location of the event, while the location aijk of the arrival slows ness depends only on the event depth and distance to the station. [sent-117, score-1.564]
</p><p>52 The scales of all these Laplacians are ﬁxed for a given phase and station, so that Pφ (aijk | dijk = 1, ei ) = z Pφ (aijk | dijk = 1, ei ) = s  1 jk Zz  1 jk Zs  exp − exp −  |aijk − µjk (ei )| z z l  , bjk z |aijk − µjk (ei , ∆ik )| s s d bjk s  4  (7) . [sent-118, score-1.315]
</p><p>53 (8)  The arrival amplitud aijk is similar to the detection probability in that it depends only on the event a magnitude, depth, and distance to the station. [sent-119, score-0.963]
</p><p>54 We model the log of the amplitude via a linear regression model with Gaussian noise: Pφ (aijk | dijk = 1, ei ) = √ a  1 jk 2πσa  exp −  (log(aijk ) − µjk (ei , ei , ∆ik ))2 a a m d jk 2σa  . [sent-120, score-0.87]
</p><p>55 2  (9)  Finally, the phase label aijk automatically assigned to the detection follows a multinomial distribuh tion whose parameters depends on the true phase, j: Pφ (aijk | dijk = 1, ei ) = pjk (aijk ). [sent-121, score-1.045]
</p><p>56 Because phase labels indicate among other things the general physical path taken from an event to a station, a distinct set of features were learned from the event characteristics for each phase. [sent-123, score-0.895]
</p><p>57 To estimate the individual station weights αwjk for each phase j and feature w, a hierarchical model was speciﬁed in which each station-speciﬁc weight is independently drawn from a feature-dependent global Normal 2 distribution, so that αwjk ∼ N (µwj , σwj ). [sent-124, score-0.384]
</p><p>58 Detection probability at station 6 for P phase, surface event  1. [sent-129, score-0.685]
</p><p>59 10  Time Residuals around IASPEI prediction for P phase at station 6  model 3. [sent-131, score-0.384]
</p><p>60 00  180  −6  −4  −2  0 Time  2  4  6  Figure 2: Conditional detection probabilities and arrival time distributions (relative to the IASPEI prediction) for the P phase at Station 6. [sent-142, score-0.408]
</p><p>61 3  False Detections  Each station, k, also generates a set of false detections f k through a time-homogeneous Poisson process with rate λk : f k  k  Pφ (|f |) =  (λk · T )|f | exp −λk · T f f  . [sent-144, score-0.408]
</p><p>62 (11)  kl kl The time ftkl , azimuth fz , and slowness fs of these false detections are generated uniformly over kl their respective ranges. [sent-146, score-0.858]
</p><p>63 The amplitude fa of the false detection is generated from a mixture of two kl kl Gaussians, pk (fa ). [sent-147, score-0.518]
</p><p>64 Finally, the phase label fh assigned to the false detection follows a multinomial a k kl distribution, ph (fh ). [sent-148, score-0.515]
</p><p>65 If the azimuth and slowness take values on ranges of length Mz and Ms , respectively, then the probability of the lth false detection is given by 1 1 1 k kl k kl Pφ (f kl ) = p (f )p (f ) . [sent-149, score-0.719]
</p><p>66 (12) T M z Ms a a h h Since the false detections at a station are exchangeable, we have l=|f k |  l=|f k | k  k  k  Pφ (f ) = Pφ (|f |) · |f |! [sent-150, score-0.647]
</p><p>67 kl  Pφ (f ) = exp l=1  5  −λk f  ·T  l=1  λk f pk (f kl )pk (f kl ) . [sent-151, score-0.341]
</p><p>68 Since detections from real seismic sensors are observed incrementally and roughly in time-ascending order, our inference algorithm also produces an incremental hypothesis which advances with time. [sent-155, score-0.742]
</p><p>69 Our starting hypothesis is that all detections in our detection-window are false detections and there are no events. [sent-159, score-0.694]
</p><p>70 Any new detections added to the detection window are again assumed to be false detections. [sent-161, score-0.557]
</p><p>71 As the windows move forward the events older than t0 − MT become stable: none of the moves modify either the event or detections associated with them. [sent-162, score-0.913]
</p><p>72 We deﬁne the score Se of an event as the probability ratio of two hypotheses: one in which the event exists, and another in which the event doesn’t exist and all of its associated detections are noise. [sent-167, score-1.503]
</p><p>73 If an event has score less than 1, an alternative hypothesis in which the event is deleted clearly has higher probability. [sent-168, score-0.847]
</p><p>74 Critically, this event score is unaffected by other events in the current hypothesis. [sent-169, score-0.64]
</p><p>75 (3) and (13) we have pl (ei )λe λm l Se (ei ) = exp −λm ei m D    Pφ (dijk j,k   Pφ (aijk | dijk , ei )  | ei ) δ(dijk, 0) + δ(dijk, 1) k . [sent-171, score-0.953]
</p><p>76 λf kl pk (f kl )pk (fh ) h Mz Ms a a  Note that the ﬁnal fraction is a likelihood ratio comparing interpretations of the same detection as either the detection of event i’s j th phase at station k, or the lth false detection at station k. [sent-172, score-1.803]
</p><p>77 (15)  By deﬁnition, any detection with score less than 1 is more likely to be a false detection. [sent-175, score-0.292]
</p><p>78 Also, the score of an individual detection is independent of other detections and unassociated events in the hypothesis. [sent-176, score-0.693]
</p><p>79 Birth Move We randomly pick a detection, invert it into an event location (using the detection’s time, azimuth, and slowness), and sample an event in a 10 degree by 100 second ball around this inverted location. [sent-178, score-0.834]
</p><p>80 The depth of the event is ﬁxed at 0, and the magnitude is uniformly sampled. [sent-179, score-0.491]
</p><p>81 Improve Detections Move For each detection in the detection window, we consider all possible phases j of all events i up to MT seconds earlier. [sent-180, score-0.533]
</p><p>82 We then associate the best event-phase for this detection that is not already assigned to a detection with higher score at the same station k. [sent-181, score-0.638]
</p><p>83 If this best event-phase has score Sd (dijk ) < 1, the detection is changed to a false detection. [sent-182, score-0.292]
</p><p>84 Improve Events Move For each event ei , we consider 10 points chosen uniformly at random in a small ball around the event (2 degrees in longitude and latitude, 100 km in depth, 5 seconds in time, and 2 units of magnitude), and choose those attributes with the highest score Se (ei ). [sent-183, score-1.171]
</p><p>85 Death Move Any event ei with score Se (ei ) < 1 is deleted, and all of its currently associated detections are marked as false alarms. [sent-199, score-1.039]
</p><p>86 Final Pruning Before outputting event hypotheses, we perform a ﬁnal round of pruning to remove some duplicate events. [sent-200, score-0.388]
</p><p>87 In particular, we delete any event for which there is another higher-scoring event within 5 degrees distance and 50 seconds time. [sent-201, score-0.836]
</p><p>88 Such spurious, or shadow, event hypotheses arise because real seismic events generate many more phases than we currently model. [sent-202, score-1.028]
</p><p>89 In addition, a single phase may sometimes generate multiple detections due to waveform processing, or “pick”, errors. [sent-203, score-0.458]
</p><p>90 These additional unmodeled detections, when taken together, often suggest an additional event at about the same location and time as the original event. [sent-204, score-0.446]
</p><p>91 Note that the birth move is not a greedy move: the proposed event will almost always have a score Se (ei ) < 1 until some number of detections are assigned in subsequent moves. [sent-205, score-0.819]
</p><p>92 Also in this ﬁgure, we show a precision-recall curve for SEL3 using scores from an SVM trained to classify true and false SEL3 events [10] (SEL3 extrapolation). [sent-211, score-0.295]
</p><p>93 Since the NEIC has many more sensors in the United States than the IMS, it is considered a more reliable summary of seismic activity in this region. [sent-215, score-0.43]
</p><p>94 The table on the left shows a break-down by LEB event magnitude. [sent-218, score-0.388]
</p><p>95 3 103  Figure 4: Recall and error (km) broken down by LEB event magnitude and azimuth gap (degrees). [sent-240, score-0.558]
</p><p>96 Large gaps indicate that the event location is under-constrained. [sent-242, score-0.446]
</p><p>97 For example, if all stations are to the southwest of an event, the gap is greater than 270 degrees and the event will be poorly localized along a line running from southwest to northeast. [sent-243, score-0.54]
</p><p>98 By using evidence about missed detections ignored by SEL3, NET-VISA reduces this uncertainty and performs much better. [sent-244, score-0.318]
</p><p>99 6  Conclusions and Further Work  Our results demonstrate that a Bayesian approach to seismic monitoring can improve signiﬁcantly on the performance of classical systems. [sent-253, score-0.436]
</p><p>100 Given that the difﬁculty of seismic monitoring was cited as one of the principal reasons for non-ratiﬁcation of the CTBT by the United States Senate in 1999, one hopes that improvements in monitoring may increase the chances of ﬁnal ratiﬁcation and entry into force. [sent-255, score-0.487]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.388), ('seismic', 0.385), ('detections', 0.29), ('station', 0.265), ('aijk', 0.257), ('ei', 0.22), ('dijk', 0.213), ('events', 0.203), ('leb', 0.157), ('detection', 0.151), ('waves', 0.138), ('arrival', 0.138), ('ims', 0.128), ('azimuth', 0.125), ('phase', 0.119), ('idc', 0.1), ('earth', 0.097), ('kl', 0.094), ('false', 0.092), ('iaspei', 0.086), ('jk', 0.082), ('travel', 0.082), ('slowness', 0.069), ('stations', 0.063), ('pjk', 0.063), ('depth', 0.058), ('location', 0.058), ('bjk', 0.057), ('earthquake', 0.057), ('mz', 0.057), ('pl', 0.054), ('recall', 0.054), ('monitoring', 0.051), ('score', 0.049), ('ik', 0.049), ('waveform', 0.049), ('sensors', 0.045), ('magnitude', 0.045), ('ctbt', 0.043), ('explosions', 0.043), ('livermore', 0.043), ('neic', 0.043), ('visa', 0.043), ('se', 0.042), ('mb', 0.041), ('err', 0.041), ('mt', 0.04), ('detected', 0.04), ('precision', 0.04), ('analysts', 0.038), ('birth', 0.038), ('fh', 0.037), ('ground', 0.036), ('hypothesized', 0.036), ('days', 0.036), ('sensor', 0.035), ('wave', 0.035), ('km', 0.034), ('pk', 0.033), ('surface', 0.032), ('latitude', 0.032), ('longitude', 0.032), ('move', 0.032), ('degrees', 0.031), ('automated', 0.031), ('ms', 0.03), ('bulletin', 0.029), ('distance', 0.029), ('attributes', 0.029), ('continental', 0.029), ('crust', 0.029), ('kilometers', 0.029), ('nimar', 0.029), ('radians', 0.029), ('seismological', 0.029), ('seismologists', 0.029), ('southwest', 0.029), ('storchak', 0.029), ('missed', 0.028), ('phases', 0.028), ('amplitude', 0.027), ('united', 0.027), ('berkeley', 0.027), ('fa', 0.027), ('exp', 0.026), ('attenuation', 0.025), ('stage', 0.025), ('russell', 0.025), ('signal', 0.024), ('nuclear', 0.024), ('window', 0.024), ('hypotheses', 0.024), ('myers', 0.023), ('exchangeable', 0.023), ('extrapolation', 0.023), ('wj', 0.023), ('sd', 0.022), ('hypothesis', 0.022), ('assigned', 0.022), ('laplacian', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="107-tfidf-1" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>Author: Nimar Arora, Stuart Russell, Paul Kidwell, Erik B. Sudderth</p><p>Abstract: The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. 1</p><p>2 0.1126477 <a title="107-tfidf-2" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>3 0.085686058 <a title="107-tfidf-3" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>4 0.078431748 <a title="107-tfidf-4" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>Author: Peter Jones, Venkatesh Saligrama, Sanjoy Mitter</p><p>Abstract: Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence. Keywords: Bayesian Methods, Information Theory, consistency</p><p>5 0.061760526 <a title="107-tfidf-5" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>6 0.061498079 <a title="107-tfidf-6" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>7 0.060435701 <a title="107-tfidf-7" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>8 0.057238862 <a title="107-tfidf-8" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>9 0.05155535 <a title="107-tfidf-9" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>10 0.045926407 <a title="107-tfidf-10" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>11 0.045138426 <a title="107-tfidf-11" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>12 0.043413304 <a title="107-tfidf-12" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>13 0.042895742 <a title="107-tfidf-13" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>14 0.042197935 <a title="107-tfidf-14" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>15 0.041088413 <a title="107-tfidf-15" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>16 0.040002707 <a title="107-tfidf-16" href="./nips-2010-Occlusion_Detection_and_Motion_Estimation_with_Convex_Optimization.html">187 nips-2010-Occlusion Detection and Motion Estimation with Convex Optimization</a></p>
<p>17 0.039657395 <a title="107-tfidf-17" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>18 0.035582379 <a title="107-tfidf-18" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>19 0.035558522 <a title="107-tfidf-19" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>20 0.035295084 <a title="107-tfidf-20" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.11), (1, 0.021), (2, -0.044), (3, -0.008), (4, -0.021), (5, 0.026), (6, -0.043), (7, -0.005), (8, 0.005), (9, -0.01), (10, 0.013), (11, -0.012), (12, -0.042), (13, 0.009), (14, 0.022), (15, 0.051), (16, 0.052), (17, 0.006), (18, -0.003), (19, -0.021), (20, -0.056), (21, 0.059), (22, 0.051), (23, 0.035), (24, 0.1), (25, -0.053), (26, 0.031), (27, -0.025), (28, 0.024), (29, 0.024), (30, -0.06), (31, 0.009), (32, 0.018), (33, -0.123), (34, -0.06), (35, -0.066), (36, 0.016), (37, -0.024), (38, -0.031), (39, -0.008), (40, 0.005), (41, 0.029), (42, 0.067), (43, 0.077), (44, 0.145), (45, -0.043), (46, 0.017), (47, 0.049), (48, 0.006), (49, -0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95179236 <a title="107-lsi-1" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>Author: Nimar Arora, Stuart Russell, Paul Kidwell, Erik B. Sudderth</p><p>Abstract: The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. 1</p><p>2 0.62227809 <a title="107-lsi-2" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>3 0.5972563 <a title="107-lsi-3" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>Author: Peter Jones, Venkatesh Saligrama, Sanjoy Mitter</p><p>Abstract: Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence. Keywords: Bayesian Methods, Information Theory, consistency</p><p>4 0.5595566 <a title="107-lsi-4" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>5 0.48969889 <a title="107-lsi-5" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>Author: Ping Li, Arnd Konig, Wenhao Gui</p><p>Abstract: Computing1 two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b ≥ 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simpliﬁed estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance. 1</p><p>6 0.44152033 <a title="107-lsi-6" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>7 0.43663591 <a title="107-lsi-7" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>8 0.42090017 <a title="107-lsi-8" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>9 0.41577649 <a title="107-lsi-9" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>10 0.40727586 <a title="107-lsi-10" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>11 0.38858825 <a title="107-lsi-11" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>12 0.37355167 <a title="107-lsi-12" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>13 0.36924157 <a title="107-lsi-13" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>14 0.36835888 <a title="107-lsi-14" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>15 0.36779428 <a title="107-lsi-15" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>16 0.36653858 <a title="107-lsi-16" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>17 0.35123807 <a title="107-lsi-17" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>18 0.34825999 <a title="107-lsi-18" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>19 0.34287781 <a title="107-lsi-19" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>20 0.34000474 <a title="107-lsi-20" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.028), (17, 0.026), (27, 0.059), (29, 0.375), (30, 0.042), (35, 0.01), (45, 0.175), (50, 0.043), (52, 0.026), (60, 0.026), (77, 0.052), (78, 0.01), (90, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75507897 <a title="107-lda-1" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>Author: Nimar Arora, Stuart Russell, Paul Kidwell, Erik B. Sudderth</p><p>Abstract: The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. 1</p><p>2 0.58544487 <a title="107-lda-2" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: The goal of inverse reinforcement learning is to ﬁnd a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well deﬁned. 1</p><p>3 0.53507781 <a title="107-lda-3" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>Author: Umar Syed, Ben Taskar</p><p>Abstract: We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efﬁcient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines. 1</p><p>4 0.48327237 <a title="107-lda-4" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>5 0.47844923 <a title="107-lda-5" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>6 0.47844422 <a title="107-lda-6" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>7 0.47792551 <a title="107-lda-7" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>8 0.47737065 <a title="107-lda-8" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>9 0.47663438 <a title="107-lda-9" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>10 0.47636005 <a title="107-lda-10" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>11 0.4762966 <a title="107-lda-11" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>12 0.47531071 <a title="107-lda-12" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>13 0.47497228 <a title="107-lda-13" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>14 0.47491303 <a title="107-lda-14" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>15 0.47473699 <a title="107-lda-15" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>16 0.47465131 <a title="107-lda-16" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>17 0.47455344 <a title="107-lda-17" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>18 0.47448194 <a title="107-lda-18" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>19 0.4743275 <a title="107-lda-19" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>20 0.47421455 <a title="107-lda-20" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
