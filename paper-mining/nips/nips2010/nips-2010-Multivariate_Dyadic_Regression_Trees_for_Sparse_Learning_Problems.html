<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-178" href="#">nips2010-178</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</h1>
<br/><p>Source: <a title="nips-2010-178-pdf" href="http://papers.nips.cc/paper/4178-multivariate-dyadic-regression-trees-for-sparse-learning-problems.pdf">pdf</a></p><p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>Reference: <a title="nips-2010-178-reference" href="../nips2010_reference/nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Multivariate Dyadic Regression Trees for Sparse Learning Problems Han Liu and Xi Chen School of Computer Science, Carnegie Mellon University Pittsburgh, PA 15213  Abstract We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). [sent-1, score-0.487]
</p><p>2 Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. [sent-2, score-0.403]
</p><p>3 Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. [sent-3, score-0.322]
</p><p>4 Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. [sent-4, score-0.081]
</p><p>5 predicting multi-channel signals within a time frame, predicting concentrations of several chemical constitutes using the mass spectra of a sample, or predicting expression levels of many genes using a common set of phenotype variables. [sent-9, score-0.087]
</p><p>6 These problems can be naturally formulated in terms of multivariate regression. [sent-10, score-0.087]
</p><p>7 Moreover, we denote the jth dimension of y 1 n by yj = (yj , . [sent-18, score-0.103]
</p><p>8 Without loss of generality, k k we assume X = [0, 1]d and the true model on yj is : i yj = fj (xi ) + ϵi , i = 1, . [sent-25, score-0.201]
</p><p>9 , n, (1) j d where fj : R → R is a smooth function. [sent-28, score-0.129]
</p><p>10 j This is a general setting of the nonparametric multivariate regression. [sent-38, score-0.142]
</p><p>11 , fp lie in a d-dimensional Sobolev ball with order α and radius C, the best convergence rate for the minimax risk is p · n−2α/(2α+d) . [sent-43, score-0.183]
</p><p>12 However, in many real world applications, the true regression function f may depend only on a small set of variables. [sent-45, score-0.088]
</p><p>13 If S has been given, the minimax lower bound can be improved to be p · n−2α/(2α+r) , which is the best possible rate can be expected. [sent-53, score-0.096]
</p><p>14 For sparse learning problems, our task is to develop an estimator, which adaptively achieves this faster rate of convergence without knowing S in advance. [sent-54, score-0.097]
</p><p>15 1  Previous research on these problems can be roughly divided into three categories: (i) parametric linear models, (ii) nonparametric additive models, and (iii) nonparametric tree models. [sent-55, score-0.308]
</p><p>16 Recently, signiﬁcant progress has been made on inferring nonparametric additive models with joint sparsity constraints [7, 10]. [sent-59, score-0.144]
</p><p>17 For additive models, each fj (x) is assumed to have an additive form: ∑d fj (x) = k=1 fjk (xk ). [sent-60, score-0.378]
</p><p>18 A family of more ﬂexible nonparametric methods are based on tree models. [sent-62, score-0.193]
</p><p>19 One of the most popular tree methods is the classiﬁcation and regression tree (CART) [2]. [sent-63, score-0.364]
</p><p>20 It ﬁrst grows a full tree by orthogonally splitting the axes at locally optimal splitting points, then prunes back the full tree to form a subtree. [sent-64, score-0.328]
</p><p>21 In contrast to CART, dyadic decision trees (DDTs) are restricted to only axis-orthogonal dyadic splits, i. [sent-66, score-0.456]
</p><p>22 each dimension can only be split at its midpoint. [sent-68, score-0.092]
</p><p>23 For a broad range of classiﬁcation problems, [15] showed that DDTs using a special penalty can attain nearly optimal rate of convergence in a minimax sense. [sent-69, score-0.201]
</p><p>24 [1] proposed a dynamic programming algorithm for constructing DDTs when the penalty term has an additive form, i. [sent-70, score-0.104]
</p><p>25 the penalty of the tree can be written as the sum of penalties on all terminal nodes. [sent-72, score-0.37]
</p><p>26 Though intensively studied for classiﬁcation problems, the dyadic decision tree idea has not drawn much attention in the regression settings. [sent-73, score-0.425]
</p><p>27 One of the closest results we are aware of is [4], in which a single response dyadic regression procedure is considered for non-sparse learning problems. [sent-74, score-0.287]
</p><p>28 Another interesting tree model, “Bayesian Additive Regression Trees (BART)”, is proposed under Bayesian framework [6], which is essentially a “sum-of-trees” model. [sent-75, score-0.138]
</p><p>29 Most of the existing work adopt the number of terminal nodes as the penalty. [sent-76, score-0.215]
</p><p>30 Such penalty cannot lead to sparse models since a tree with a small number of terminal nodes might still involve too many variables. [sent-77, score-0.461]
</p><p>31 To obtain sparse models, we propose a new nonparametric method based on multivariate dyadic regression trees (MDRTs). [sent-78, score-0.526]
</p><p>32 Our contributions are two-fold: (i) Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rate of convergence for the class of (α, C)smooth functions. [sent-81, score-0.294]
</p><p>33 (ii) Empirically, to avoid computationally prohibitive exhaustive search in high dimensions, we propose a two-stage greedy algorithm and its randomized version that achieve good performance in both function estimation and variable selection. [sent-82, score-0.115]
</p><p>34 Note that our theory and algorithm can be straightforwardly adapted to univariate sparse regression problem, which is a special case of the multivariate one. [sent-83, score-0.214]
</p><p>35 To the best of our knowledge, this is the ﬁrst time such a sparsity-inducing penalty is equipped to tree models for solving sparse regression problems. [sent-84, score-0.309]
</p><p>36 A MDRT T is a multivariate regression tree that recursively divides the input space X by means of axis-orthogonal dyadic splits. [sent-92, score-0.512]
</p><p>37 If a node is associated ∏d to the cell B = j=1 [aj , bj ], after being dyadically split on the dimension k, the two children are associated to the subcells B k,1 and B k,2 : } { ak + bk B k,1 = xi ∈ B | xi ≤ and B k,2 = B \ B k,1 . [sent-95, score-0.369]
</p><p>38 k 2 The set of terminal nodes of a MDRT T is denoted as term(T ). [sent-96, score-0.215]
</p><p>39 Let Bt be the cell in X induced by a terminal node t, the partition induced by term(T ) can be denoted as π(T ) = {Bt |t ∈ term(T )}. [sent-97, score-0.311]
</p><p>40 2  For each terminal node t, we can ﬁt a multivariate m-th order polynomial regression on data points falling in Bt . [sent-98, score-0.529]
</p><p>41 Instead of using all covariates, such a polynomial regression is only ﬁtted on a set of active variables, which is denoted as A(t). [sent-99, score-0.154]
</p><p>42 For each node b ∈ T (not necessarily a terminal node), A(b) can be an arbitrary subset of {1, . [sent-100, score-0.282]
</p><p>43 If a node is dyadically split perpendicular to the axis k, k must belong to the active sets of its two children. [sent-104, score-0.247]
</p><p>44 For any node b, let par(b) be its parent node, then A(par(b)) ⊂ A(b). [sent-106, score-0.094]
</p><p>45 m For a MDRT T , we deﬁne FT to be the class of p-valued measurable m-th order polynomials corresponding to π(T ). [sent-107, score-0.08]
</p><p>46 Furthermore, for a dyadic integer N = 2L , let TN be the collection of all MDRTs such that no terminal cell has a side length smaller than 2−L . [sent-108, score-0.416]
</p><p>47 The latter one penalizing the number of terminal nodes |π(T )| has been commonly adopted in the existing tree literature. [sent-116, score-0.353]
</p><p>48 In the next section, we will show that this sparsity-inducing term is derived by bounding the VC-dimension of the underlying subgraph of regression functions. [sent-119, score-0.088]
</p><p>49 To evaluate the algorithm performance, we use the L2 -risk with respect to the Lebesgue measure ∑p ∫ µ(·), which is deﬁned as R(f , f ) = E j=1 X |fj (x) − fj (x)|2 dµ(x), where f is the function estimate constructed from n observed samples. [sent-123, score-0.129]
</p><p>50 2 of [9] shows that the lower minimax rate of convergence for class D(α, C) is exactly the same as that for class of d-dimensional Sobolev ball with order α and radius C. [sent-154, score-0.166]
</p><p>51 ,fp ∈D(α,C) Therefore, the lower minimax rate of convergence is p · n−2α/(2α+d) . [sent-160, score-0.12]
</p><p>52 Similarly, if the problem is jointly sparse with the index set S and r = |S| ≪ d, the best rate of convergence can be improved to p · n−2α/(2α+r) when S is given. [sent-161, score-0.097]
</p><p>53 Gaussian random j √ variables, this assumption easily holds with γ = O( log n), which only contributes a logarithmic term to the ﬁnal rate of convergence. [sent-171, score-0.082]
</p><p>54 The next assumption speciﬁes the scaling of the relevant dimension r and ambient dimension d with respect to the sample size n. [sent-172, score-0.159]
</p><p>55 On the other hand, the ambient dimension d can increase exponentially fast with the sample size, which is a realistic scaling for high dimensional settings. [sent-177, score-0.114]
</p><p>56 Remark 1 As discussed in Proposition 1, the obtained rate of convergence in (5) is nearly optimal up to a logarithmic term. [sent-180, score-0.119]
</p><p>57 Remark 2 Since the estimator deﬁned in (2) does not need to know the smoothness α and the sparsity level r in advance, MDRTs are simultaneously adaptive to the unknown smoothness and sparsity level. [sent-181, score-0.171]
</p><p>58 Our analysis closely follows the least squares regression analysis in [9] and some speciﬁc coding scheme of trees in [15]. [sent-183, score-0.177]
</p><p>59 m Let ST be the class of scalar-valued measurable m-th order polynomials corresponding to π(T ), m m and let GT be the class of all subgraphs of functions of ST , i. [sent-186, score-0.103]
</p><p>60 , fp ) be the true regression function, there exists a set of piecewise m polynomials h1 , . [sent-200, score-0.235]
</p><p>61 Sketch of Proof: This is a standard approximation result using multivariate piecewise polynomials. [sent-207, score-0.114]
</p><p>62 The main idea is based on a multivariate Taylor expansion of the function fj at a given point x0 . [sent-208, score-0.216]
</p><p>63 ∑p ∫ First, we deﬁne R(g, f ) = j=1 X |gj (x) − fj (x)|2 dµ(x), Lemma 3 [9] Choose  ( ) [[T ]] log 2 γ4 m log(120eγ 4 n)VGT + n 2 ∑ for some preﬁx code [[T ]] > 0 satisfying T ∈TN 2−[[T ]] ≤ 1. [sent-214, score-0.153]
</p><p>64 To make MDRTs scalable for high dimensional massive datasets, using similar ideas as CARTs, we propose a two-stage procedure: (1) we grow a full tree in a greedy manner; (2) we prune back the full tree to from the ﬁnal tree. [sent-224, score-0.338]
</p><p>65 Given a MDRT T , denote the corresponding multivariate m-th order polynomial ﬁt on π(T ) by m fT = {ftm }t∈π(T ) , where ftm is the m-th order polynomial regression ﬁt on the partition Bt . [sent-226, score-0.313]
</p><p>66 For 5  each xi falling in Bt , let ftm (xi , A(t)) be the predicted function value for xi . [sent-227, score-0.192]
</p><p>67 We denote the the local squared error (LSE) on node t by Rm (t, A(t)): 1 ∑ Rm (t, A(t)) = ∥yi − ftm (xi , A(t))∥2 . [sent-228, score-0.164]
</p><p>68 The total MSE of the tree R(T ) can then be computed by the following equation: ∑ R(T ) = Rm (t, A(t)). [sent-230, score-0.138]
</p><p>69 (11) Our goal is to ﬁnd the tree structure with the polynomial regression on each terminal node that can minimize the total cost. [sent-232, score-0.542]
</p><p>70 The ﬁrst stage is tree growing, in which a terminal node t is ﬁrst selected in each step. [sent-233, score-0.448]
</p><p>71 We then perform one of two actions a1 and a2: a1: adding another dimension k ̸∈ A(t) to A(t), and reﬁt the regression model on all data points falling in Bt ; a2: dyadically splitting t perpendicular to the dimension k ∈ A(t). [sent-234, score-0.382]
</p><p>72 In each tree growing step, we need to decide which action to perform. [sent-235, score-0.204]
</p><p>73 For action a2, let sl(t(k) ) be the side length of Bt on dimension k ∈ A(t). [sent-237, score-0.108]
</p><p>74 If sl(t(k) ) > 2−L , the (k) (k) dimension k of Bt can then be dyadically split. [sent-238, score-0.137]
</p><p>75 In this case, let tL and tR be the left and right child of node t. [sent-239, score-0.094]
</p><p>76 For each terminal node t, we greedily perform the action a∗ on the dimension k ∗ , which are determined by m (14) (a∗ , k ∗ ) = argmax ∆Ra (t, k). [sent-241, score-0.39]
</p><p>77 d}  In high dimensional setting, the above greedy procedure may not lead to the optimal tree since successively locally optimal splits cannot guarantee the global optimum. [sent-245, score-0.224]
</p><p>78 Once an irrelevant dimension has been added in or split, the greedy procedure can never ﬁx the mistake. [sent-246, score-0.145]
</p><p>79 Instead of greedily performing the action on the dimension that leads the maximum drop in LSE, we randomly choose which action to perform according to a multinomial distribution. [sent-248, score-0.172]
</p><p>80 The action a∗ is then performed on the dimension k ∗ . [sent-251, score-0.108]
</p><p>81 In general, when the randomized scheme is adopted, we need to repeat our algorithm many times to pick the best tree. [sent-252, score-0.136]
</p><p>82 For each step, we either merge a pair of terminal nodes or remove a variable from the active set of a terminal node such that the resulted tree has the smaller cost. [sent-254, score-0.667]
</p><p>83 We repeat this process until the tree becomes a single root node with an empty active set. [sent-255, score-0.264]
</p><p>84 The tree with the minimum cost in this process is returned as the ﬁnal tree. [sent-256, score-0.138]
</p><p>85 The pseudocode for the growing stage and cost complexity pruning stage are presented in the Appendix. [sent-257, score-0.081]
</p><p>86 Moreover, to avoid a cell with too few data points, we pre-deﬁne a quantity nmax . [sent-258, score-0.108]
</p><p>87 Let n(t) be the the number of data points fall into Bt , if n(t) ≤ nmax , Bt will no longer be split. [sent-259, score-0.079]
</p><p>88 In addition, whenever we perform the mth order polynomial regression on the active set of a node, we need to make sure it is not rank deﬁcient. [sent-261, score-0.154]
</p><p>89 For randomized scheme, we run 50 random trials and pick the minimum cost tree. [sent-264, score-0.105]
</p><p>90 As for CART, we adopt the MATLAB package from [12], which ﬁts piecewise constant on each p terminal node with the cost complexity criterion: C(T ) = R(T ) + ρ n |π(T )|, where ρ is the tuning parameter playing the same role as λ in (3). [sent-265, score-0.309]
</p><p>91 For more detailed experiment protocols, we set nmax = 5 and L = 6. [sent-283, score-0.079]
</p><p>92 The tree with the minimum MSE on the validation set is then picked as the best tree. [sent-285, score-0.138]
</p><p>93 For criterion (i), if the variables involved in the best tree are exactly the ﬁrst four variables, the variable selection task for this design is deemed as successful. [sent-286, score-0.168]
</p><p>94 Moreover, the performance of randomized scheme is slightly better than its deterministic version in variable selection. [sent-295, score-0.106]
</p><p>95 nmax is set to be 5 for the ﬁrst dataset and 20 for the latter two. [sent-415, score-0.079]
</p><p>96 Moreover, for randomized scheme, we run 50 random trials and pick the minimum cost tree. [sent-417, score-0.105]
</p><p>97 Moreover, randomized scheme does improve the performance compared to the deterministic counterpart. [sent-449, score-0.106]
</p><p>98 6 Conclusions We propose a novel sparse learning method based on multivariate dyadic regression trees (MDRTs). [sent-459, score-0.471]
</p><p>99 Our approach adopts a new sparsity-inducing penalty that simultaneously conduct function estimation and variable selection. [sent-460, score-0.095]
</p><p>100 To the best of our knowledge, it is the ﬁrst time that such a penalty is introduced in the tree literature for high dimensional sparse learning problems. [sent-462, score-0.243]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mdrt', 0.674), ('mdrts', 0.337), ('dyadic', 0.199), ('cart', 0.199), ('terminal', 0.188), ('tree', 0.138), ('pen', 0.135), ('fj', 0.129), ('bt', 0.122), ('ddts', 0.119), ('node', 0.094), ('regression', 0.088), ('multivariate', 0.087), ('nmax', 0.079), ('rt', 0.078), ('randomized', 0.075), ('ftm', 0.07), ('dyadically', 0.07), ('housing', 0.07), ('lse', 0.07), ('dimension', 0.067), ('nt', 0.067), ('fp', 0.063), ('xs', 0.062), ('minimax', 0.062), ('additive', 0.06), ('vgt', 0.06), ('mse', 0.059), ('rm', 0.059), ('trees', 0.058), ('polynomials', 0.057), ('nonparametric', 0.055), ('han', 0.052), ('penalty', 0.044), ('xi', 0.042), ('gt', 0.041), ('action', 0.041), ('greedy', 0.04), ('carts', 0.04), ('chem', 0.04), ('jerome', 0.039), ('sparse', 0.039), ('tn', 0.039), ('ft', 0.038), ('irrelevant', 0.038), ('falling', 0.038), ('ga', 0.038), ('nearly', 0.037), ('yj', 0.036), ('rate', 0.034), ('polynomial', 0.034), ('lemma', 0.032), ('active', 0.032), ('leo', 0.032), ('bart', 0.032), ('scheme', 0.031), ('smoothness', 0.03), ('selection', 0.03), ('xk', 0.03), ('sobolev', 0.03), ('inf', 0.03), ('pick', 0.03), ('predicting', 0.029), ('simultaneously', 0.029), ('cell', 0.029), ('sparsity', 0.029), ('breiman', 0.028), ('worthwhile', 0.028), ('larry', 0.028), ('stage', 0.028), ('nodes', 0.027), ('designs', 0.027), ('sl', 0.027), ('st', 0.027), ('synthetic', 0.027), ('piecewise', 0.027), ('remark', 0.026), ('perpendicular', 0.026), ('tl', 0.026), ('liu', 0.026), ('exible', 0.026), ('splitting', 0.026), ('involve', 0.025), ('ambient', 0.025), ('split', 0.025), ('constants', 0.025), ('growing', 0.025), ('splits', 0.024), ('convergence', 0.024), ('estimator', 0.024), ('logarithmic', 0.024), ('log', 0.024), ('ra', 0.024), ('par', 0.024), ('generic', 0.024), ('drop', 0.023), ('class', 0.023), ('tk', 0.023), ('dimensional', 0.022), ('conduct', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="178-tfidf-1" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>2 0.18821783 <a title="178-tfidf-2" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>Author: Han Liu, Xi Chen, Larry Wasserman, John D. Lafferty</p><p>Abstract: Undirected graphical models encode in a graph G the dependency structure of a random vector Y . In many applications, it is of interest to model Y given another random vector X as input. We refer to the problem of estimating the graph G(x) of Y conditioned on X = x as “graph-valued regression”. In this paper, we propose a semiparametric method for estimating G(x) that builds a tree on the X space just as in CART (classiﬁcation and regression trees), but at each leaf of the tree estimates a graph. We call the method “Graph-optimized CART”, or GoCART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data. 1</p><p>3 0.12305571 <a title="178-tfidf-3" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>Author: Samy Bengio, Jason Weston, David Grangier</p><p>Abstract: Multi-class classiﬁcation becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classiﬁers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster. 1</p><p>4 0.08470694 <a title="178-tfidf-4" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>5 0.082701668 <a title="178-tfidf-5" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We develop a theory of online learning by deﬁning several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting. 1</p><p>6 0.081245176 <a title="178-tfidf-6" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>7 0.073982835 <a title="178-tfidf-7" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>8 0.065769948 <a title="178-tfidf-8" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>9 0.064617284 <a title="178-tfidf-9" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>10 0.063176662 <a title="178-tfidf-10" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>11 0.061648302 <a title="178-tfidf-11" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>12 0.060451739 <a title="178-tfidf-12" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>13 0.060114421 <a title="178-tfidf-13" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>14 0.056406241 <a title="178-tfidf-14" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>15 0.055767011 <a title="178-tfidf-15" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>16 0.054899503 <a title="178-tfidf-16" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>17 0.054073215 <a title="178-tfidf-17" href="./nips-2010-Block_Variable_Selection_in_Multivariate_Regression_and_High-dimensional_Causal_Inference.html">41 nips-2010-Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference</a></p>
<p>18 0.052311637 <a title="178-tfidf-18" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>19 0.050782867 <a title="178-tfidf-19" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>20 0.050160564 <a title="178-tfidf-20" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, 0.019), (2, 0.086), (3, 0.051), (4, -0.028), (5, -0.014), (6, -0.055), (7, 0.03), (8, -0.013), (9, 0.046), (10, -0.036), (11, 0.035), (12, -0.113), (13, -0.005), (14, 0.017), (15, -0.081), (16, 0.005), (17, -0.076), (18, -0.069), (19, -0.023), (20, -0.082), (21, -0.053), (22, -0.013), (23, -0.058), (24, -0.034), (25, 0.02), (26, -0.054), (27, 0.125), (28, 0.024), (29, -0.033), (30, 0.066), (31, 0.05), (32, -0.058), (33, 0.074), (34, 0.043), (35, 0.066), (36, 0.081), (37, -0.171), (38, 0.016), (39, -0.024), (40, 0.083), (41, 0.037), (42, -0.046), (43, 0.013), (44, -0.055), (45, -0.06), (46, 0.004), (47, -0.025), (48, -0.0), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90476608 <a title="178-lsi-1" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>2 0.76948351 <a title="178-lsi-2" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>Author: Han Liu, Xi Chen, Larry Wasserman, John D. Lafferty</p><p>Abstract: Undirected graphical models encode in a graph G the dependency structure of a random vector Y . In many applications, it is of interest to model Y given another random vector X as input. We refer to the problem of estimating the graph G(x) of Y conditioned on X = x as “graph-valued regression”. In this paper, we propose a semiparametric method for estimating G(x) that builds a tree on the X space just as in CART (classiﬁcation and regression trees), but at each leaf of the tree estimates a graph. We call the method “Graph-optimized CART”, or GoCART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data. 1</p><p>3 0.59612787 <a title="178-lsi-3" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>Author: Samy Bengio, Jason Weston, David Grangier</p><p>Abstract: Multi-class classiﬁcation becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a treestructure of classiﬁers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster. 1</p><p>4 0.55801958 <a title="178-lsi-4" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>Author: Vibhav Gogate, William Webb, Pedro Domingos</p><p>Abstract: We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context-speciﬁc independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed-form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature and its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is bounded by a constant k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efﬁcient. Experiments on a variety of domains show that our approach outperforms many state-of-the-art Markov network structure learners. 1</p><p>5 0.54554331 <a title="178-lsi-5" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>6 0.515728 <a title="178-lsi-6" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>7 0.51101154 <a title="178-lsi-7" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>8 0.49278089 <a title="178-lsi-8" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>9 0.48715365 <a title="178-lsi-9" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>10 0.48397222 <a title="178-lsi-10" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>11 0.46852896 <a title="178-lsi-11" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>12 0.441017 <a title="178-lsi-12" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>13 0.4305256 <a title="178-lsi-13" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>14 0.42889312 <a title="178-lsi-14" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>15 0.42869988 <a title="178-lsi-15" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>16 0.40403447 <a title="178-lsi-16" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>17 0.40378097 <a title="178-lsi-17" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>18 0.40082359 <a title="178-lsi-18" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>19 0.39678785 <a title="178-lsi-19" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>20 0.39606553 <a title="178-lsi-20" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.048), (27, 0.049), (30, 0.087), (35, 0.018), (45, 0.219), (50, 0.054), (52, 0.025), (60, 0.028), (77, 0.031), (78, 0.01), (90, 0.343)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95025045 <a title="178-lda-1" href="./nips-2010-Permutation_Complexity_Bound_on_Out-Sample_Error.html">205 nips-2010-Permutation Complexity Bound on Out-Sample Error</a></p>
<p>Author: Malik Magdon-Ismail</p><p>Abstract: We deﬁne a data dependent permutation complexity for a hypothesis set H, which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based (like the maximum discrepancy) on dependent sampling. We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efﬁciently estimated.</p><p>2 0.93144405 <a title="178-lda-2" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>3 0.91685772 <a title="178-lda-3" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>4 0.91144967 <a title="178-lda-4" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>same-paper 5 0.85305232 <a title="178-lda-5" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>6 0.79726571 <a title="178-lda-6" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>7 0.77204341 <a title="178-lda-7" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>8 0.76990193 <a title="178-lda-8" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>9 0.74099821 <a title="178-lda-9" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>10 0.7328071 <a title="178-lda-10" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>11 0.7295146 <a title="178-lda-11" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>12 0.72355866 <a title="178-lda-12" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>13 0.71700579 <a title="178-lda-13" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>14 0.71521789 <a title="178-lda-14" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>15 0.70366079 <a title="178-lda-15" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>16 0.70292044 <a title="178-lda-16" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>17 0.70216256 <a title="178-lda-17" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>18 0.69937575 <a title="178-lda-18" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>19 0.69618803 <a title="178-lda-19" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>20 0.69071323 <a title="178-lda-20" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
