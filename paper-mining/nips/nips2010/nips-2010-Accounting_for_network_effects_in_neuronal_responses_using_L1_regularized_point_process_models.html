<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-21" href="#">nips2010-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</h1>
<br/><p>Source: <a title="nips-2010-21-pdf" href="http://papers.nips.cc/paper/4050-accounting-for-network-effects-in-neuronal-responses-using-l1-regularized-point-process-models.pdf">pdf</a></p><p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>Reference: <a title="nips-2010-21-reference" href="../nips2010_reference/nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Accounting for network effects in neuronal responses using L1 regularized point process models  Ryan C. [sent-1, score-0.21]
</p><p>2 This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. [sent-13, score-0.363]
</p><p>3 We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. [sent-15, score-0.288]
</p><p>4 We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. [sent-16, score-0.192]
</p><p>5 This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. [sent-17, score-0.123]
</p><p>6 1  Introduction  One of the most striking features of spike trains is their variability – that is, the same visual stimulus does not elicit the same spike pattern on repeated presentations. [sent-18, score-0.752]
</p><p>7 1  correlation structure of this population [1]. [sent-25, score-0.131]
</p><p>8 However, in cerebral cortex, recording a full population of individual neurons in a region is currently impossible, and large scale recordings in vivo have been rare. [sent-26, score-0.283]
</p><p>9 Classical methods attempt to explain the activity of neurons only in terms of stimulus ﬁlters or kernels, ignoring sources unrelated to the stimulus. [sent-28, score-0.476]
</p><p>10 An increasing number of groups have modeled spiking with point process models [2, 3, 4] to assess the relative contributions of speciﬁc sources. [sent-29, score-0.106]
</p><p>11 [3] used these methods to model retinal ganglion cells, and they showed that the responses of cells could be predicted to a large extent using the activity of nearby cells. [sent-31, score-0.497]
</p><p>12 We apply this technique to model spike trains in macaque V1 in vivo using L1 regularized point process models, which for discrete time become Generalized Linear Models (GLMs) [5]. [sent-32, score-0.248]
</p><p>13 In addition to incorporating the spike trains of nearby cells, we incorporated a meaningful summary of local network activity, the local ﬁeld potential (LFP), and show that it also can explain an important part of the neuronal variability. [sent-33, score-0.404]
</p><p>14 This is a data constraint: there simply are not enough spikes to locate the true parameters. [sent-35, score-0.118]
</p><p>15 In general, a point process may be represented in terms of a conditional intensity function and, assuming the data (the spike times) are in sufﬁciently small time bins, the resulting likelihood function may be approximated by a Poisson regression likelihood function. [sent-38, score-0.168]
</p><p>16 For ease of notation we leave the spiking history and other covariates implicit and write the conditional intensity (ﬁring rate) at time t as µ(t). [sent-39, score-0.134]
</p><p>17 We then model the log of µ(t) as a linear summation of other factors: N (t)  θj vj = θV (t)  log µ(t) =  (1)  j  where vj is a feature of the data and θj is the corresponding parameter to be ﬁt, and θ = {θ1 , . [sent-40, score-0.124]
</p><p>18 , vN , which are the collection of observables, including input stimulus and measured neural responses. [sent-46, score-0.266]
</p><p>19 yT , with yt ∈ {0, 1} as the observed binary spike train for the cell being modeled, and let µt = µ(t). [sent-50, score-0.35]
</p><p>20 The likelihood of the entire spike train is given by: T  P (Y = y1 . [sent-51, score-0.168]
</p><p>21 1  Regularization path  To choose efﬁciently a penalty that avoids over-ﬁtting, we implement a regularization path algorithm [6, 5]. [sent-72, score-0.183]
</p><p>22 As the regularization parameter λ is decreased, the ﬁtted models begin by under-ﬁtting the data (with large λ) and progress through the regularization path to over-ﬁtting (with small λ). [sent-90, score-0.141]
</p><p>23 An alternative and natural metric is the likelihood value, and the peak of the regularization path was very similar between AUC and likelihood. [sent-96, score-0.128]
</p><p>24 The models here contain combinations of stimulus effects (spatio-temporal receptive ﬁelds), coupling effects (history terms and past 3  spikes from other cells), and network effects (given by the LFP). [sent-100, score-0.81]
</p><p>25 We ﬁnd that cells had different degrees of contributions from the different terms, ranging from entirely stimulus-dependent cells to entirely network-dependent cells. [sent-101, score-0.574]
</p><p>26 1  Methods  The details of the array insertion have been described elsewhere [8]. [sent-103, score-0.103]
</p><p>27 6 mm into cortex using a pneumatic insertion device [9], which led to recordings conﬁned mostly to layers 2–3 of parafoveal V1 (receptive ﬁelds within 5◦ of the fovea) in an anesthetized and paralyzed macaque (sufentanil anesthesia). [sent-105, score-0.132]
</p><p>28 We studied the responses of cells to visual stimuli, presented on a computer screen. [sent-110, score-0.318]
</p><p>29 The average noise correlation between pairs of cells was 0. [sent-116, score-0.326]
</p><p>30 To reduce the problem size, we binned the spiking observations at 10 ms instead of 1 ms. [sent-119, score-0.211]
</p><p>31 The penalty in the regularization path with the largest average area across all the cross validation runs was considered the optimal penalty. [sent-123, score-0.132]
</p><p>32 If we were to use pixel intensities over the last 150 ms (15 observations), the 320 × 320 movie would have 1 536 000 parameters, a number far too large for the ﬁtting method and data. [sent-126, score-0.126]
</p><p>33 Then, we transformed the stimulus space with overlapping Gaussian bump ﬁlters, which are very similar to basis functions. [sent-128, score-0.306]
</p><p>34 Thus, sxy (t − τ ) corresponds to the convolution of a small Gaussian bump indexed by x, y, τ with the recent stimulus frames. [sent-131, score-0.38]
</p><p>35 Figure 2A shows the k parameters for some example cells transformed back to the original pixel space, with the corresponding STAs alongside for comparison. [sent-134, score-0.287]
</p><p>36 Figure 2D shows the population results for these models. [sent-136, score-0.131]
</p><p>37 The distribution of AUC values is generally low, with many cells near chance (. [sent-137, score-0.287]
</p><p>38 5  7  41  λ  172  714  λ=7  Figure 1: Example of ﬁtting a GLM with stimulus terms for a single cell. [sent-145, score-0.266]
</p><p>39 2  {γi} [cell at (3,5)]  Cell at (3,4)  Electrode  LFP models  {γi} [cell at (3,5)]  Cell at (7,2)  D  C  Spike coupling models  Cell at (7,6)  F  LFP model AUC 0. [sent-165, score-0.111]
</p><p>40 A: 4 example stimulus models, with the STAs shown for reference. [sent-174, score-0.266]
</p><p>41 B: 3 example cells ﬁt with spike coupling models. [sent-176, score-0.566]
</p><p>42 The coefﬁcients are shown with respect to the cell location on the array. [sent-177, score-0.134]
</p><p>43 If multiple cells were isolated on the same electrode, the square is divided into 2 or 3 parts. [sent-178, score-0.287]
</p><p>44 As in B, nearby electrodes carry more information about spiking. [sent-181, score-0.141]
</p><p>45 These are plots of the AUCs for the 57 cells modeled. [sent-183, score-0.287]
</p><p>46 In addition, there is an effect of electrode location, with cells with the highest AUC located on the left side of the array. [sent-185, score-0.385]
</p><p>47 3  Spike coupling effects  For the coupling terms, we used the history of ﬁring for the other cells recording in the array as well as the history for the cell being modeled. [sent-187, score-0.834]
</p><p>48 These take the form: M 100  γi ri (t − τ )  log µCOUP (t) = i  (12)  τ =1  with γi being the coupling strength/coefﬁcient, and ri (t − τ ) being the activity of the ith neuron τ ms earlier, and M being the number of neurons. [sent-188, score-0.37]
</p><p>49 Thus the inﬂuence from a surrounding neuron is computed based on its spike count in the last 100ms. [sent-189, score-0.323]
</p><p>50 As expected, nearby cells generally had the largest coefﬁcients (Figure 2B), indicating that cells in closer proximity tend to have more correlation in their spike trains. [sent-190, score-0.794]
</p><p>51 Thus, the units which were well predicted by the other ﬁring in the population also did not require a large number of parameters to achieve the best AUC possible. [sent-194, score-0.131]
</p><p>52 The models described above had one parameter per cell in the population, with each parameter corresponding to the ﬁring over a 100 ms past window. [sent-196, score-0.211]
</p><p>53 We also ﬁt models with 3 parameters per cell in the population, corresponding to the spikes in three non-overlapping temporal epochs (120 ms, 21-50 ms, 51-100 ms). [sent-197, score-0.252]
</p><p>54 We did not attempt to model effects on very short timescales, since we binned the spikes at 10 ms. [sent-201, score-0.21]
</p><p>55 4  Network models  The spiking of cells in the population serves to help predict spiking very well for many cells, but the cause of this relationship remains undetermined. [sent-203, score-0.63]
</p><p>56 The speciﬁc timing of spikes may play a large role in predicting spikes, but alternatively the general network ﬂuctuations could be the primary cause. [sent-204, score-0.192]
</p><p>57 Figure 2C shows the model coefﬁcients of several cells when {xi } are the LFP values at time t. [sent-206, score-0.287]
</p><p>58 Across the population, the AUC values for the cells are almost the same as in the spike coupling models (Figure 2F), and consequently the spatial pattern of AUC on the array is almost identical. [sent-208, score-0.637]
</p><p>59 With these models, the AUC distributions were remarkably similar to the models built with spike coupling terms (Figure 2E). [sent-210, score-0.279]
</p><p>60 The LFP reﬂects activity over a very broad region, and thus for these data the connectivity between most pairs in the population do not generally have much more predictive power than the more broad network dynamics. [sent-211, score-0.326]
</p><p>61 This suggests that much of the power of the spike coupling terms above is a direct result of both cells being driven by the underlying network dynamics, rather than by a direct connection between the two cells unrelated 6  A  1  Full model AUC  0. [sent-212, score-0.963]
</p><p>62 8 µPSTH AUC  1  Figure 3: Scatter plots of the AUC values for the population under different models and conditions. [sent-240, score-0.131]
</p><p>63 A,B: The full model improves upon the individual LFP or stimulus models. [sent-241, score-0.293]
</p><p>64 C: For most cells, trial shufﬂing the spike trains destroys the effectiveness of the models. [sent-242, score-0.255]
</p><p>65 D: Taking the network state and cell spikes into account generally yields a larger AUC than µPSTH . [sent-243, score-0.326]
</p><p>66 Models of spike coupling with more precise timing (< 10 ms) may reﬂect information that these LFP terms would fail to capture. [sent-245, score-0.279]
</p><p>67 The simplest conception is that each of these cells has an independent source of intrinsic noise, and to recover the underlying ﬁring rate function we can simply repeat a stimulus many times. [sent-247, score-0.553]
</p><p>68 We have shown above that for many cells, a portion of the noise is not independent from the rest of the network and is related to other cells and the LFP. [sent-248, score-0.4]
</p><p>69 The population included a distribution of cells, and the GLMs showed that some cells included mostly network terms, and other cells included mostly stimulus terms. [sent-249, score-1.045]
</p><p>70 From Figure 3A and 3B we can see that the inclusion of network terms does indeed explain more of the spikes than the stimulus model alone. [sent-251, score-0.458]
</p><p>71 It is theoretically possible that the LFP or spikes from other cells are reﬂecting higher order terms of the stimulus-response relationship that the linear model fails to capture, and the GLM is harnessing these effects to increase AUC. [sent-252, score-0.469]
</p><p>72 Since the stimulus was repeated we were able to shufﬂe trials. [sent-254, score-0.297]
</p><p>73 Any stimulus information is present on every trial of this repeated stimulus, and so if the AUC improvement is entirely due to the network terms capturing stimulus information, there should be no decrease in AUC in the trial-shufﬂed condition. [sent-255, score-0.686]
</p><p>74 This means that the network terms are not merely capturing extraneous stimulus effects. [sent-257, score-0.34]
</p><p>75 [11] show that when taking the network state into account with a very simple GLM, the signal to noise in the stimulus-response relationship was improved. [sent-259, score-0.113]
</p><p>76 The PSTH is typically used as a proxy for the stimulus effects. [sent-260, score-0.266]
</p><p>77 The idea is that any noise terms are averaged out after many trials to the same repeated stimulus. [sent-261, score-0.101]
</p><p>78 Figure 3D shows the comparison: for almost every cell the full model is better at predicting the spikes than the PSTH itself, even though the stimulus component of the model is merely a linear ﬁlter. [sent-267, score-0.545]
</p><p>79 The stimulus model predicted the PSTH 7  A  LFP model, R2 = 0. [sent-273, score-0.266]
</p><p>80 Taking the network state into account yields a closer estimate to the PSTH, indicating that the PSTH contains effects unrelated to the stimulus. [sent-289, score-0.174]
</p><p>81 well for some cells, but for most others the stimulus model alone cannot match the full model’s performance, indicating a corruption of the PSTH by network effects. [sent-292, score-0.367]
</p><p>82 5  Conclusions  In this paper we have implemented a L1 regularized point process model to account for stimulus effects, neuronal interactions and network state effects for explaining the spiking activity of V1 neurons. [sent-293, score-0.703]
</p><p>83 Using this model, we have shown that activity of cells in the surrounding population can account for a signiﬁcant amount of the variance in the ﬁring of many neurons. [sent-296, score-0.607]
</p><p>84 We found that the LFP, a broad indicator of the synaptic activity of many cells across a large region (the network state), can account for a large share of these inﬂuences from the surrounding cells. [sent-297, score-0.55]
</p><p>85 This suggests that these spikes are due to the general network state rather than precise spike timing or individual true synaptic connections between a pair of cells. [sent-298, score-0.36]
</p><p>86 This is consistent with earlier observations that the spiking activity of a neuron is linked to ongoing population activity as measured with optical imaging [12] and LFP [13]. [sent-299, score-0.54]
</p><p>87 This link to the state of the local population is an inﬂuential force affecting the variability in a cell’s spiking behavior. [sent-300, score-0.287]
</p><p>88 These state transitions occur in sleeping and anesthetized animals, in cortical slices [15], as well as in awake animal [16, 17] and awake human patients [18, 19], and might be responsible for generating much of the slow time scale correlation. [sent-302, score-0.133]
</p><p>89 By directly modeling these sources of variability, this method begins to allow us to obtain better encoding models and more accurately isolate the elements of the stimulus that are truly driving the cells’ responses. [sent-304, score-0.266]
</p><p>90 By attributing portions of ﬁring to network state effects (as indicated by the LFP), this approach can obtain more accurate estimates of the underlying connectivity among neurons in cortical circuits. [sent-305, score-0.228]
</p><p>91 Spatio-temporal correlations and visual signalling in a complete neuronal population. [sent-313, score-0.103]
</p><p>92 Spatial and temporal scales of neuronal correlation in primary visual cortex. [sent-330, score-0.103]
</p><p>93 A method for pneumatically inserting an array of penetrating electrodes into cortical tissue. [sent-333, score-0.197]
</p><p>94 Robust, automatic spike sorting using mixtures of multivariate t-distributions. [sent-336, score-0.168]
</p><p>95 Comparison of recordings from microelectrode arrays and single electrodes in the visual cortex. [sent-339, score-0.187]
</p><p>96 Linking spontaneous activity of single cortical neurons and the underlying functional architecture. [sent-342, score-0.256]
</p><p>97 Development and plasticity of spontaneous activity and up states in cortical organotypic slices. [sent-351, score-0.203]
</p><p>98 Very slow activity ﬂuctuations in monkey visual cortex: implications for functional brain imaging. [sent-354, score-0.152]
</p><p>99 o o a Sequential structure of neocortical spontaneous activity in vivo. [sent-357, score-0.166]
</p><p>100 Interhemispheric correlations of slow spontaneous neuronal ﬂuctuations revealed in human sensory cortex. [sent-363, score-0.117]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('auc', 0.373), ('lfp', 0.373), ('psth', 0.332), ('cells', 0.287), ('stimulus', 0.266), ('spike', 0.168), ('cell', 0.134), ('population', 0.131), ('ring', 0.131), ('sta', 0.129), ('activity', 0.121), ('spikes', 0.118), ('coupling', 0.111), ('kxy', 0.111), ('spiking', 0.106), ('neurosci', 0.102), ('electrode', 0.098), ('electrodes', 0.089), ('ms', 0.077), ('network', 0.074), ('drq', 0.074), ('kohn', 0.074), ('sxy', 0.074), ('glm', 0.073), ('neuronal', 0.072), ('array', 0.071), ('surrounding', 0.068), ('effects', 0.064), ('vj', 0.062), ('neuron', 0.061), ('glms', 0.06), ('jan', 0.057), ('aucs', 0.055), ('kelly', 0.055), ('roc', 0.053), ('neurons', 0.053), ('nearby', 0.052), ('path', 0.051), ('variability', 0.05), ('trial', 0.049), ('movie', 0.049), ('receptive', 0.049), ('kass', 0.049), ('tai', 0.049), ('yt', 0.048), ('adam', 0.047), ('cognition', 0.047), ('coef', 0.047), ('matthew', 0.046), ('pittsburgh', 0.045), ('spontaneous', 0.045), ('regularization', 0.045), ('shuf', 0.045), ('tting', 0.044), ('poisson', 0.043), ('coordinate', 0.042), ('vivo', 0.042), ('cients', 0.041), ('bump', 0.04), ('noise', 0.039), ('uctuations', 0.039), ('cortex', 0.038), ('trains', 0.038), ('cortical', 0.037), ('coup', 0.037), ('ganglion', 0.037), ('jonathon', 0.037), ('litke', 0.037), ('microelectrode', 0.037), ('rck', 0.037), ('shlens', 0.037), ('stas', 0.037), ('stim', 0.037), ('unrelated', 0.036), ('penalty', 0.036), ('nat', 0.035), ('carnegie', 0.032), ('mellon', 0.032), ('anesthetized', 0.032), ('acad', 0.032), ('awake', 0.032), ('insertion', 0.032), ('mas', 0.032), ('nei', 0.032), ('peak', 0.032), ('trials', 0.031), ('repeated', 0.031), ('visual', 0.031), ('recordings', 0.03), ('active', 0.03), ('pillow', 0.03), ('aug', 0.03), ('natl', 0.03), ('sing', 0.03), ('binned', 0.028), ('discontinuity', 0.028), ('history', 0.028), ('full', 0.027), ('pa', 0.026), ('count', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="21-tfidf-1" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>2 0.21901914 <a title="21-tfidf-2" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>3 0.20621674 <a title="21-tfidf-3" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>4 0.20223609 <a title="21-tfidf-4" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>5 0.16846067 <a title="21-tfidf-5" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>6 0.15047333 <a title="21-tfidf-6" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>7 0.11645449 <a title="21-tfidf-7" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>8 0.11574437 <a title="21-tfidf-8" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>9 0.11569991 <a title="21-tfidf-9" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>10 0.1123183 <a title="21-tfidf-10" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>11 0.10831404 <a title="21-tfidf-11" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>12 0.10668322 <a title="21-tfidf-12" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>13 0.10509641 <a title="21-tfidf-13" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>14 0.10315236 <a title="21-tfidf-14" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>15 0.1008034 <a title="21-tfidf-15" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>16 0.091694325 <a title="21-tfidf-16" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>17 0.090645619 <a title="21-tfidf-17" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>18 0.090440288 <a title="21-tfidf-18" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>19 0.087880388 <a title="21-tfidf-19" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>20 0.083601885 <a title="21-tfidf-20" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, 0.059), (2, -0.264), (3, 0.26), (4, 0.106), (5, 0.179), (6, -0.043), (7, 0.005), (8, 0.057), (9, -0.049), (10, 0.023), (11, 0.051), (12, 0.019), (13, 0.107), (14, 0.067), (15, -0.022), (16, -0.02), (17, -0.099), (18, 0.013), (19, 0.055), (20, 0.038), (21, -0.046), (22, 0.116), (23, 0.055), (24, -0.036), (25, 0.051), (26, -0.049), (27, 0.074), (28, 0.027), (29, 0.039), (30, -0.042), (31, -0.162), (32, 0.048), (33, 0.055), (34, -0.027), (35, -0.05), (36, -0.02), (37, -0.02), (38, 0.031), (39, -0.009), (40, 0.004), (41, 0.023), (42, -0.052), (43, 0.015), (44, -0.029), (45, 0.008), (46, 0.032), (47, 0.01), (48, 0.004), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96309912 <a title="21-lsi-1" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>2 0.80992711 <a title="21-lsi-2" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>3 0.73547864 <a title="21-lsi-3" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>4 0.727709 <a title="21-lsi-4" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>5 0.70563877 <a title="21-lsi-5" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>6 0.67953134 <a title="21-lsi-6" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>7 0.62629783 <a title="21-lsi-7" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>8 0.62521452 <a title="21-lsi-8" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>9 0.60810554 <a title="21-lsi-9" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>10 0.55975521 <a title="21-lsi-10" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>11 0.50825334 <a title="21-lsi-11" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>12 0.50679308 <a title="21-lsi-12" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>13 0.49452841 <a title="21-lsi-13" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>14 0.48804885 <a title="21-lsi-14" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>15 0.4850356 <a title="21-lsi-15" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>16 0.46119657 <a title="21-lsi-16" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>17 0.4512282 <a title="21-lsi-17" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>18 0.42555207 <a title="21-lsi-18" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>19 0.42430407 <a title="21-lsi-19" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>20 0.41661602 <a title="21-lsi-20" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.03), (17, 0.016), (27, 0.18), (28, 0.044), (30, 0.053), (35, 0.034), (45, 0.127), (50, 0.053), (52, 0.071), (60, 0.026), (77, 0.075), (90, 0.04), (97, 0.147)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84787059 <a title="21-lda-1" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>2 0.82862759 <a title="21-lda-2" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>3 0.80031997 <a title="21-lda-3" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>4 0.79645699 <a title="21-lda-4" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>5 0.79255724 <a title="21-lda-5" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>6 0.79162008 <a title="21-lda-6" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>7 0.78393185 <a title="21-lda-7" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>8 0.78213465 <a title="21-lda-8" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>9 0.78199095 <a title="21-lda-9" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>10 0.77882367 <a title="21-lda-10" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>11 0.76287389 <a title="21-lda-11" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>12 0.75333738 <a title="21-lda-12" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>13 0.7513721 <a title="21-lda-13" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>14 0.74579525 <a title="21-lda-14" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>15 0.74439502 <a title="21-lda-15" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>16 0.74042219 <a title="21-lda-16" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>17 0.73746449 <a title="21-lda-17" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>18 0.73318547 <a title="21-lda-18" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>19 0.73247671 <a title="21-lda-19" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>20 0.73146635 <a title="21-lda-20" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
