<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-122" href="#">nips2010-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</h1>
<br/><p>Source: <a title="nips-2010-122-pdf" href="http://papers.nips.cc/paper/4037-improving-the-asymptotic-performance-of-markov-chain-monte-carlo-by-inserting-vortices.pdf">pdf</a></p><p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>Reference: <a title="nips-2010-122-reference" href="../nips2010_reference/nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch  Abstract We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. [sent-4, score-1.562]
</p><p>2 The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. [sent-5, score-1.347]
</p><p>3 Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. [sent-6, score-0.954]
</p><p>4 An MCMC estimator can be based on any ergodic Markov chain with the distribution of interest as its stationary distribution. [sent-8, score-0.575]
</p><p>5 However, the choice of Markov chain greatly affects the performance of the estimator, in particular the accuracy achieved with a pre-speciﬁed number of samples [4]. [sent-9, score-0.341]
</p><p>6 In general, the efﬁciency of an MCMC estimator is determined by two factors: i) how fast the chain converges to its stationary distribution, i. [sent-10, score-0.469]
</p><p>7 , the mixing rate [9], and ii) once the chain reaches its stationary distribution, how much the estimates ﬂuctuate based on trajectories of ﬁnite length, which is characterized by the asymptotic variance. [sent-12, score-0.676]
</p><p>8 Previous theory concerned with reducing asymptotic variance has followed two main tracks. [sent-14, score-0.309]
</p><p>9 The ﬁrst focuses on reversible chains, and is mostly based on the theorems of Peskun [10] and Tierney [11], which state that if a reversible Markov chain is modiﬁed so that the probability of staying in the same state is reduced, then the asymptotic variance can be decreased. [sent-15, score-1.693]
</p><p>10 A number of methods have been proposed, particularly in the context of Metropolis-Hastings method, to encourage the Markov chain to move away from the current state, or its adjacency in the continuous case [12, 13]. [sent-16, score-0.349]
</p><p>11 Neal proved in [4] that starting from any ﬁnite-state reversible chain, the asymptotic variance of a related nonreversible chain, with reduced probability of back-tracking to the immediately previous state, will not increase, and typically decrease. [sent-18, score-0.818]
</p><p>12 1  Neal’s result suggests that non-reversible chains may be fundamentally better than reversible ones in terms of the asymptotic performance. [sent-20, score-0.921]
</p><p>13 Our method is applicable to any non-reversible chain whose state transition graph contains loops, including those whose probability of staying in the same state is zero and thus cannot be improved using Peskun’s theorem. [sent-22, score-0.678]
</p><p>14 The method also admits an interesting graphical interpretation which amounts to inserting ‘vortices’ into the state transition graph of the original chain. [sent-23, score-0.35]
</p><p>15 Our result suggests a new and interesting direction for improving the asymptotic performance of MCMC. [sent-24, score-0.257]
</p><p>16 Let A be a transition operator of an ergodic1 Markov chain with stationary distribution π, i. [sent-27, score-0.613]
</p><p>17 , π (x) A (x → y) = π (y) B (y → x) , ∀x, y ∈ S,  (1)  where B is the reverse operator as deﬁned in [5]. [sent-29, score-0.155]
</p><p>18 The expectation can then be estimated through the MCMC estimator T 1 µT = f (xt ) , (2) t=1 T where x1 , · · · , xT is a trajectory sampled from the Markov chain. [sent-30, score-0.086]
</p><p>19 The asymptotic variance of µT , with respect to transition operator A and function f is deﬁned as 2 σA (f ) = lim T V [µT ] ,  (3)  T →∞  2 where V [µT ] denotes the variance of µT . [sent-31, score-0.621]
</p><p>20 Since the chain is ergodic, σA (f ) is well-deﬁned following the central limit theorem, and does not depend on the distribution of the initial point. [sent-32, score-0.323]
</p><p>21 Roughly speaking, asymptotic variance has the meaning that the mean square error of the estimates based on 1 2 T consecutive states of the chain would be approximately T σA (f ), after a sufﬁciently long period of ”burn in” such that the chain is close enough to its stationary distribution. [sent-33, score-1.141]
</p><p>22 Asymptotic variance can be used to compare the asymptotic performance of MCMC estimators based on different chains with the same stationary distribution, where smaller asymptotic variance indicates that, asymptotically, the MCMC estimator requires fewer samples to reach a speciﬁed accuracy. [sent-34, score-1.06]
</p><p>23 Note that σA (f ) depends on both A and its reverse 2 2 operator B, and σA (f ) = σB (f ) since A is also the reverse operator of B by deﬁnition. [sent-36, score-0.31]
</p><p>24 , S = {1, · · · , S}, so that the transition operators A and B, the stationary distribution π, and the function f can all be written in matrix form. [sent-39, score-0.254]
</p><p>25 The asymptotic variance can thus be written as 2 σA (f ) = V [f ] +  ∞ τ =1  QAτ + QB τ − 2ππ  f  f,  1 Strictly speaking, the ergodic assumption is not necessary for the MCMC estimator to work, see [4]. [sent-41, score-0.498]
</p><p>26 Also, from the ergodic assumption, lim Aτ = lim B τ = R, τ →∞  τ →∞  where R = 1π is a square matrix in which every row is π . [sent-45, score-0.2]
</p><p>27 3  Improving the asymptotic variance  It is clear from Eq. [sent-47, score-0.309]
</p><p>28 5 that the transition operator A affects the asymptotic variance only through term [Λ− ]H . [sent-48, score-0.532]
</p><p>29 If the chain is reversible, then J is symmetric, so that Λ is also symmetric, and therefore comparing the asymptotic variance of two MCMC estimators becomes a matter of comparing their 2 2 J, namely, if2 J J = QA , then σA (f ) ≤ σA (f ), for any f . [sent-49, score-0.709]
</p><p>30 In the case where the Markov chain is non-reversible, i. [sent-51, score-0.323]
</p><p>31 1, which transforms the comparison of asymptotic variance based on arbitrary ﬁnite Markov chains into a matrix ordering problem, using a result from matrix analysis. [sent-55, score-0.596]
</p><p>32 2, a special case is identiﬁed, in which the asymptotic variance of a reversible chain is compared to that of a nonreversible one whose joint distribution over consecutive states is that of the reversible chain plus a skew-Hermitian matrix. [sent-57, score-1.993]
</p><p>33 We prove that the resulting non-reversible chain has smaller asymptotic variance, and provide a necessary and sufﬁcient condition for the existence of such non-zero skewHermitian matrices. [sent-58, score-0.625]
</p><p>34 5 we know that comparing the asymptotic variances of two MCMC estimators is equivalent to comparing their [Λ− ]H . [sent-63, score-0.301]
</p><p>35 From Lemma 1, it follows immediately that in the discrete case, the comparison of MCMC estimators based on two Markov chains with the same stationary distribution can be cast as a different problem of matrix comparison, as stated in the following proposition. [sent-66, score-0.386]
</p><p>36 Proposition 1 Let A, A be two transition operators of ergodic Markov chains with stationary distribution π. [sent-67, score-0.55]
</p><p>37 This amounts to the case where the second chain is reversible, and its transition operator is the average of the transition operator of the ﬁrst chain and the associated reverse operator. [sent-79, score-1.158]
</p><p>38 Corollary 1 Let T be a reversible transition operator of a Markov chain with stationary distribution π. [sent-81, score-1.041]
</p><p>39 Denote A = T + Q− H, B = T − Q− H, then 1) A preserves π, and B is the reverse operator of A. [sent-85, score-0.155]
</p><p>40 2 2 4) If Aε = T + (1 + ε) Q− H is valid transition matrix, ε > 0, then σAε (f ) ≤ σA (f ). [sent-88, score-0.142]
</p><p>41 Moreover QA = QT + H = (QT − H) = Q T − Q− H thus B is the reverse operator of A. [sent-91, score-0.155]
</p><p>42 Corollary 1 shows that starting from a reversible Markov chain, as long as one can ﬁnd a nonzero H satisfying Conditions I and II, then the asymptotic performance of the MCMC estimator is guaranteed to improve. [sent-105, score-0.757]
</p><p>43 The following proposition shows that any H satisfying this condition can be constructed systematically. [sent-108, score-0.161]
</p><p>44 H satisﬁes Condition I if and only if H can be written 1 as the linear combination of 2 (S − 1) (S − 2) matrices, with each matrix of the form Ui,j = ui uj − uj ui , 1 ≤ i < j ≤ S − 1. [sent-110, score-0.239]
</p><p>45 Here u1 , · · · , uS−1 are S − 1 non-zero linearly independent vectors satisfying us 1 = 0. [sent-111, score-0.094]
</p><p>46 On one hand, any S-by-S skew-Hermitian matrix can be written as the linear combination of 1 S (S − 1) matrices of the form 2  Vi,j : {Vi,j }m,n = δ (m, i) δ (n, j) − δ (n, i) δ (m, j) , where δ is the standard delta function such that δ (i, j) = 1 if i = j and 0 otherwise. [sent-118, score-0.101]
</p><p>47 Since all Ui,j are linearly independent, it follows that Ui,j must also be linearly independent. [sent-126, score-0.12]
</p><p>48 It turns out that not all reversible Markov chains admit a non-zero H satisfying both Condition I and II. [sent-132, score-0.709]
</p><p>49 For example, consider a Markov chain with only two states. [sent-133, score-0.323]
</p><p>50 to 1 0 The next proposition gives the sufﬁcient and necessary condition for the existence of a non-zero H satisfying both I and II. [sent-135, score-0.188]
</p><p>51 In particular, it shows an interesting link between the existence of such H and the connectivity of the states in the reversible chain. [sent-136, score-0.517]
</p><p>52 Proposition 3 Assume a reversible ergodic Markov chain with transition matrix T and let J = QT . [sent-137, score-1.004]
</p><p>53 The state transition graph GT is deﬁned as the undirected graph with node set S = {1, · · · , S} and edge set {(i, j) : Ji,j > 0, 1 ≤ i < j ≤ S}. [sent-138, score-0.248]
</p><p>54 Sufﬁciency: Without loss of generality, assume the loop is made of states 1, 2, · · · , N and edges (1, 2) , · · · , (N − 1, N ) , (N, 1), with N ≥ 3. [sent-141, score-0.11]
</p><p>55 Necessity: Assume there are no loops in GT , then all states in the chain must be organized in a tree, following the ergodic assumption. [sent-147, score-0.543]
</p><p>56 Since rs ± hs ≥ 0, all except the s -th elements in hs must be 0. [sent-154, score-0.147]
</p><p>57 But since hs 1 = 0, the whole s-th row, thus the s-th column of H must be 0. [sent-155, score-0.092]
</p><p>58 Having set the s-th column and row of H to 0, one can consider the reduced Markov chain with one state less, and repeat with another leaf node. [sent-156, score-0.46]
</p><p>59 The indication of Proposition 3 together with 2 is that all reversible chains can be improved in terms of asymptotic variance using Corollary 1, except those whose transition graphs are trees. [sent-158, score-1.096]
</p><p>60 In practice, the non-tree constraint is not a problem because almost all current methods of constructing reversible chains generate chains with loops. [sent-159, score-0.902]
</p><p>61 3  Graphical interpretation  In this subsection we provide a graphical interpretation of the Starting from a simple case, consider a reversible Markov chain Let u1 = [1, 0, −1] and u2 = [0, 1, −1] . [sent-161, score-0.822]
</p><p>62 The left hand side is a state transition graph of a reversible Markov chain with S = 9 states, with a vortex 3 → 8 → 6 → 5 → 4 of strength ε inserted. [sent-166, score-1.415]
</p><p>63 We start from the vortex 8 → 6 → 9 → 8, and add one vortex a time. [sent-168, score-0.888]
</p><p>64 The dotted lines correspond to edges on which the ﬂows cancel out when a new vortex is added. [sent-169, score-0.494]
</p><p>65 For example, when vortex 6 → 5 → 9 → 6 is added, edge 9 → 6 cancels edge 6 → 9 in the previous vortex, resulting in a larger vortex with four states. [sent-170, score-0.888]
</p><p>66 Note that in this way one can construct vortices which do not include state 9, although each Ui,j is a vortex involving 9. [sent-171, score-0.776]
</p><p>67 Write U1,2 and J + H in explicit form, U1,2 =  0 −1 1  1 −1 0 1 −1 0  p1,1 p2,1 − ε p3,1 + ε  , J +H =  p1,2 + ε p2,2 p3,2 − ε  p1,3 − ε p2,3 + ε p3,3  ,  with pi,j being the probability of the consecutive states being i, j. [sent-173, score-0.101]
</p><p>68 Intuitively, this amounts to adding a ‘vortex’ of direction 1 → 2 → 3 → 1 in the state transition. [sent-175, score-0.129]
</p><p>69 Similarly, the joint probability matrix for the reverse operator is J −H, which adds a vortex in the opposite direction. [sent-176, score-0.624]
</p><p>70 This simple case also gives an explanation of why adding or subtracting non-zero H can only be done where a loop already exists, since the operation requires subtracting ε from all entries in J corresponding to edges in the loop. [sent-177, score-0.145]
</p><p>71 Therefore, adding or subtracting an H to J is equivalent to inserting a number of vortices into the state transition map. [sent-181, score-0.572]
</p><p>72 4  An example  Adding vortices to the state transition graph forces the Markov chain to move in loops following pre-speciﬁed directions. [sent-182, score-0.863]
</p><p>73 Consider a reversible Markov chain with S states forming a ring, namely from state s one can only jump to s⊕1 or s 1, with ⊕ and being the mod-S summation and subtraction. [sent-184, score-0.883]
</p><p>74 The only possible non-zero H S−1 in this example is of form ε s=1 Us,s+1 , corresponding to vortices on the large ring. [sent-185, score-0.262]
</p><p>75 1 We assume uniform stationary distribution π (s) = S . [sent-186, score-0.085]
</p><p>76 In this case, any reversible chain behaves like a random walk. [sent-187, score-0.751]
</p><p>77 The chain which achieves minimal asymptotic variance is the one with the probability of both jumping forward and backward being 1 . [sent-188, score-0.632]
</p><p>78 The expected number of steps for 2 2 this chain to reach the state S edges away is S . [sent-189, score-0.435]
</p><p>79 However, adding the vortex reduces this number to 2 4  7  1. [sent-190, score-0.473]
</p><p>80 4  b  With vortex  c  Figure 2: Demonstration of the vortex effect: (a) and (b) show two different, reversible Markov chains, each containing 128 states connected in a ring. [sent-198, score-1.378]
</p><p>81 The equilibrium distribution of the chains is depicted by the gray inner circles; darker shades correspond to higher probability. [sent-199, score-0.262]
</p><p>82 The equilibrium distribution of chain (a) is uniform, while that of (b) contains two peaks half a ring apart. [sent-200, score-0.391]
</p><p>83 In addition, the chains are constructed such that the probability of staying in the same state is zero. [sent-201, score-0.372]
</p><p>84 In each case, two trajectories, of length 1000, are generated from the chain with and without the vortex, starting from the state pointed to by the arrow. [sent-202, score-0.393]
</p><p>85 The length of the bar radiating out from a given state represents the relative frequency of visits to that state, with red and blue bars corresponding to chains with and without vortex, respectively. [sent-203, score-0.307]
</p><p>86 It is clear from the graph that trajectories sampled from reversible chains spread much slower, with only 1/5 of the states reached in (a) and 1/3 in (b), and the trajectory in (b) does not escape from the current peak. [sent-204, score-0.844]
</p><p>87 On the other hand, with vortices added, trajectories of the same length spread over all the states, and effectively explore both peaks of the stationary distribution in (b). [sent-205, score-0.435]
</p><p>88 Here we take s the Markov chains from (b) and use function f (s) = cos 4π · 128 . [sent-207, score-0.237]
</p><p>89 When vortices are added, not only do the absolute values of the correlations go down signiﬁcantly, but also their signs alternate, indicating that these correlations tend to cancel out in the sum of Eq. [sent-208, score-0.29]
</p><p>90 S roughly 2ε for large S, suggesting that it is much easier for the non-reversible chain to reach faraway 1 states, especially for large S. [sent-210, score-0.343]
</p><p>91 In the extreme case, when ε = 2 , the chain cycles deterministically, reducing asymptotic variance to zero. [sent-211, score-0.632]
</p><p>92 Also note that the reversible chain here has zero probability of staying in the current state, thus cannot be further improved using Peskun’s theorem. [sent-212, score-0.816]
</p><p>93 Our intuition about why adding vortices helps is that chains with vortices move faster than the reversible ones, making the function values of the trajectories less correlated. [sent-213, score-1.288]
</p><p>94 5  Conclusion  In this paper, we have presented a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with the theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. [sent-215, score-1.562]
</p><p>95 The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. [sent-216, score-1.347]
</p><p>96 The results conﬁrm that non-reversible chains are fundamentally better than reversible ones. [sent-217, score-0.697]
</p><p>97 The general framework of Proposition 1 suggests further improvements of MCMC’s asymptotic performance, by applying other results from matrix analysis to asymptotic variance reduction. [sent-218, score-0.558]
</p><p>98 Which combinations of vortices yield optimal improvements for a given chain? [sent-220, score-0.262]
</p><p>99 Neal, ”Improving asymptotic variance of MCMC estimators: Non-reversible chains are better”, Technical Report No. [sent-242, score-0.546]
</p><p>100 Murray, ”Advances in Markov chain Monte Carlo methods”, M. [sent-246, score-0.323]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vortex', 0.444), ('reversible', 0.428), ('chain', 0.323), ('vortices', 0.262), ('chains', 0.237), ('asymptotic', 0.224), ('mcmc', 0.168), ('hes', 0.159), ('peskun', 0.141), ('qa', 0.138), ('transition', 0.122), ('qf', 0.107), ('hx', 0.107), ('ergodic', 0.106), ('markov', 0.105), ('variance', 0.085), ('stationary', 0.085), ('qt', 0.084), ('operator', 0.083), ('nonreversible', 0.081), ('reverse', 0.072), ('state', 0.07), ('proposition', 0.066), ('staying', 0.065), ('states', 0.062), ('neal', 0.062), ('estimator', 0.061), ('galleria', 0.061), ('hermitian', 0.061), ('manno', 0.061), ('usn', 0.061), ('inserting', 0.055), ('qb', 0.053), ('tierney', 0.053), ('idsia', 0.053), ('xt', 0.053), ('hs', 0.051), ('condition', 0.051), ('linearly', 0.05), ('uj', 0.048), ('satisfying', 0.044), ('trajectories', 0.044), ('gt', 0.042), ('kenney', 0.04), ('switzerland', 0.04), ('estimators', 0.039), ('consecutive', 0.039), ('converting', 0.037), ('subtracting', 0.034), ('ui', 0.033), ('improving', 0.033), ('verify', 0.033), ('fundamentally', 0.032), ('loops', 0.032), ('combination', 0.03), ('carlo', 0.03), ('amounts', 0.03), ('speaking', 0.03), ('adding', 0.029), ('monte', 0.029), ('write', 0.029), ('corollary', 0.029), ('graph', 0.028), ('cancel', 0.028), ('existence', 0.027), ('ea', 0.026), ('loop', 0.026), ('tree', 0.026), ('interpretation', 0.026), ('symmetric', 0.026), ('move', 0.026), ('trajectory', 0.025), ('jumps', 0.025), ('rs', 0.025), ('graphically', 0.025), ('equilibrium', 0.025), ('murray', 0.025), ('matrix', 0.025), ('row', 0.025), ('peaks', 0.024), ('matrices', 0.024), ('bases', 0.023), ('lim', 0.022), ('edges', 0.022), ('written', 0.022), ('asymmetric', 0.022), ('ii', 0.021), ('column', 0.021), ('leaf', 0.021), ('spread', 0.02), ('toronto', 0.02), ('reach', 0.02), ('must', 0.02), ('valid', 0.02), ('comparing', 0.019), ('college', 0.019), ('graphical', 0.019), ('ring', 0.019), ('affects', 0.018), ('rms', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="122-tfidf-1" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>2 0.11884424 <a title="122-tfidf-2" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>3 0.10102394 <a title="122-tfidf-3" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>4 0.094624378 <a title="122-tfidf-4" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>Author: Mohammad Ghavamzadeh, Alessandro Lazaric, Odalric Maillard, Rémi Munos</p><p>Abstract: We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a highdimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm. 1</p><p>5 0.093313754 <a title="122-tfidf-5" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>6 0.092154369 <a title="122-tfidf-6" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>7 0.084541559 <a title="122-tfidf-7" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>8 0.084483542 <a title="122-tfidf-8" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>9 0.075474799 <a title="122-tfidf-9" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>10 0.074665204 <a title="122-tfidf-10" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>11 0.064252183 <a title="122-tfidf-11" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>12 0.062534437 <a title="122-tfidf-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.054892376 <a title="122-tfidf-13" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>14 0.050649427 <a title="122-tfidf-14" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>15 0.050365031 <a title="122-tfidf-15" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>16 0.046809275 <a title="122-tfidf-16" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>17 0.045266721 <a title="122-tfidf-17" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>18 0.04492116 <a title="122-tfidf-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.043468669 <a title="122-tfidf-19" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>20 0.042465236 <a title="122-tfidf-20" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, -0.035), (2, 0.042), (3, 0.058), (4, -0.036), (5, 0.004), (6, 0.006), (7, 0.009), (8, -0.036), (9, 0.08), (10, -0.031), (11, -0.007), (12, -0.031), (13, 0.021), (14, -0.043), (15, -0.01), (16, 0.024), (17, -0.025), (18, 0.044), (19, 0.085), (20, -0.055), (21, 0.073), (22, -0.023), (23, 0.019), (24, -0.006), (25, 0.07), (26, 0.039), (27, -0.012), (28, -0.035), (29, -0.007), (30, -0.028), (31, -0.004), (32, -0.114), (33, -0.022), (34, -0.017), (35, 0.116), (36, -0.034), (37, 0.019), (38, -0.113), (39, 0.04), (40, 0.169), (41, 0.04), (42, -0.058), (43, -0.038), (44, 0.072), (45, -0.005), (46, -0.094), (47, -0.038), (48, 0.016), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94587374 <a title="122-lsi-1" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>2 0.56136024 <a title="122-lsi-2" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>3 0.55093503 <a title="122-lsi-3" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>4 0.53268701 <a title="122-lsi-4" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>Author: Pradeep Shenoy, Angela J. Yu, Rajesh P. Rao</p><p>Abstract: Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems. 1</p><p>5 0.48089075 <a title="122-lsi-5" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>6 0.47711927 <a title="122-lsi-6" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>7 0.46490127 <a title="122-lsi-7" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>8 0.45379862 <a title="122-lsi-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.45345688 <a title="122-lsi-9" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>10 0.44356096 <a title="122-lsi-10" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>11 0.44277215 <a title="122-lsi-11" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>12 0.43959501 <a title="122-lsi-12" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>13 0.4218666 <a title="122-lsi-13" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>14 0.40584123 <a title="122-lsi-14" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>15 0.3967399 <a title="122-lsi-15" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>16 0.39143512 <a title="122-lsi-16" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>17 0.38765588 <a title="122-lsi-17" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>18 0.3831085 <a title="122-lsi-18" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>19 0.37808713 <a title="122-lsi-19" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>20 0.37482551 <a title="122-lsi-20" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.048), (27, 0.062), (30, 0.064), (35, 0.021), (45, 0.159), (50, 0.072), (52, 0.054), (60, 0.053), (77, 0.046), (78, 0.017), (84, 0.274), (90, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75962591 <a title="122-lda-1" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>2 0.6093623 <a title="122-lda-2" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>3 0.60849655 <a title="122-lda-3" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>4 0.6073969 <a title="122-lda-4" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>5 0.59874654 <a title="122-lda-5" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Many statistical M -estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer. We analyze the convergence rates of ﬁrst-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We deﬁne appropriately restricted versions of these conditions, and show that they are satisﬁed with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov’s ﬁrst-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ∗ and the optimal solution θ. This globally linear rate is substantially faster than previous analyses of global convergence for speciﬁc methods that yielded only sublinear rates. Our analysis applies to a wide range of M -estimators and statistical models, including sparse linear regression using Lasso ( 1 regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational eﬃciency in high-dimensional estimation. 1</p><p>6 0.59841096 <a title="122-lda-6" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>7 0.59837443 <a title="122-lda-7" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>8 0.59655124 <a title="122-lda-8" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>9 0.59603083 <a title="122-lda-9" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>10 0.59584796 <a title="122-lda-10" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>11 0.59480715 <a title="122-lda-11" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>12 0.59447825 <a title="122-lda-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.59369487 <a title="122-lda-13" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>14 0.59340835 <a title="122-lda-14" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>15 0.59235954 <a title="122-lda-15" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>16 0.59085965 <a title="122-lda-16" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>17 0.59046632 <a title="122-lda-17" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>18 0.59013218 <a title="122-lda-18" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>19 0.58989704 <a title="122-lda-19" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>20 0.58949614 <a title="122-lda-20" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
