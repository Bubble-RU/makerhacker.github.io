<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-122" href="#">nips2010-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</h1>
<br/><p>Source: <a title="nips-2010-122-pdf" href="http://papers.nips.cc/paper/4037-improving-the-asymptotic-performance-of-markov-chain-monte-carlo-by-inserting-vortices.pdf">pdf</a></p><p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>Reference: <a title="nips-2010-122-reference" href="../nips2010_reference/nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vortex', 0.487), ('chain', 0.474), ('revers', 0.306), ('vort', 0.288), ('asymptot', 0.216), ('mcmc', 0.185), ('hes', 0.175), ('peskun', 0.155), ('qa', 0.152), ('qf', 0.117), ('hx', 0.117), ('transit', 0.117), ('markov', 0.114), ('ergod', 0.112), ('qt', 0.093), ('nonrevers', 0.089), ('proposit', 0.077), ('traject', 0.071), ('galler', 0.066), ('hermit', 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="122-tfidf-1" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>2 0.1229848 <a title="122-tfidf-2" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>3 0.12118868 <a title="122-tfidf-3" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>4 0.11113397 <a title="122-tfidf-4" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>5 0.10144509 <a title="122-tfidf-5" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>6 0.094358787 <a title="122-tfidf-6" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>7 0.094131932 <a title="122-tfidf-7" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>8 0.089662194 <a title="122-tfidf-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.088319145 <a title="122-tfidf-9" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>10 0.086276084 <a title="122-tfidf-10" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<p>11 0.082022503 <a title="122-tfidf-11" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>12 0.076441318 <a title="122-tfidf-12" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>13 0.061271787 <a title="122-tfidf-13" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>14 0.056079 <a title="122-tfidf-14" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>15 0.055254214 <a title="122-tfidf-15" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>16 0.054577962 <a title="122-tfidf-16" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>17 0.049252473 <a title="122-tfidf-17" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>18 0.047567628 <a title="122-tfidf-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.046340317 <a title="122-tfidf-19" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>20 0.045875531 <a title="122-tfidf-20" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, 0.016), (2, 0.019), (3, -0.081), (4, 0.044), (5, 0.016), (6, 0.02), (7, -0.097), (8, -0.032), (9, 0.089), (10, -0.035), (11, 0.03), (12, -0.006), (13, 0.063), (14, 0.019), (15, 0.042), (16, 0.046), (17, -0.04), (18, 0.032), (19, 0.079), (20, 0.09), (21, -0.002), (22, 0.036), (23, 0.05), (24, -0.029), (25, -0.019), (26, -0.019), (27, -0.01), (28, -0.048), (29, 0.089), (30, 0.086), (31, -0.038), (32, -0.022), (33, -0.009), (34, 0.031), (35, 0.131), (36, -0.024), (37, 0.036), (38, 0.026), (39, -0.105), (40, 0.042), (41, -0.027), (42, -0.096), (43, -0.02), (44, -0.064), (45, -0.048), (46, -0.005), (47, 0.052), (48, -0.035), (49, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93157297 <a title="122-lsi-1" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>2 0.57481229 <a title="122-lsi-2" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>Author: Hariharan Narayanan, Alexander Rakhlin</p><p>Abstract: We propose a computationally efﬁcient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efﬁcient method for implementing mixture forecasting strategies. 1</p><p>3 0.51970655 <a title="122-lsi-3" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>4 0.49560341 <a title="122-lsi-4" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>Author: José Pereira, Morteza Ibrahimi, Andrea Montanari</p><p>Abstract: We consider linear models for stochastic dynamics. To any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. We tackle the problem of learning such a network from observation of the system trajectory over a time interval T . We analyze the ℓ1 -regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufﬁciently high. This result substantiates the notion of a well deﬁned ‘time complexity’ for the network inference problem. keywords: Gaussian processes, model selection and structure learning, graphical models, sparsity and feature selection. 1 Introduction and main results Let G = (V, E) be a directed graph with weight A0 ∈ R associated to the directed edge (j, i) from ij j ∈ V to i ∈ V . To each node i ∈ V in this network is associated an independent standard Brownian motion bi and a variable xi taking values in R and evolving according to A0 xj (t) dt + dbi (t) , ij dxi (t) = j∈∂+ i where ∂+ i = {j ∈ V : (j, i) ∈ E} is the set of ‘parents’ of i. Without loss of generality we shall take V = [p] ≡ {1, . . . , p}. In words, the rate of change of xi is given by a weighted sum of the current values of its neighbors, corrupted by white noise. In matrix notation, the same system is then represented by dx(t) = A0 x(t) dt + db(t) , p (1) 0 p×p with x(t) ∈ R , b(t) a p-dimensional standard Brownian motion and A ∈ R a matrix with entries {A0 }i,j∈[p] whose sparsity pattern is given by the graph G. We assume that the linear system ij x(t) = A0 x(t) is stable (i.e. that the spectrum of A0 is contained in {z ∈ C : Re(z) < 0}). Further, ˙ we assume that x(t = 0) is in its stationary state. More precisely, x(0) is a Gaussian random variable 1 independent of b(t), distributed according to the invariant measure. Under the stability assumption, this a mild restriction, since the system converges exponentially to stationarity. A portion of time length T of the system trajectory {x(t)}t∈[0,T ] is observed and we ask under which conditions these data are sufﬁcient to reconstruct the graph G (i.e., the sparsity pattern of A0 ). We are particularly interested in computationally efﬁcient procedures, and in characterizing the scaling of the learning time for large networks. Can the network structure be learnt in a time scaling linearly with the number of its degrees of freedom? As an example application, chemical reactions can be conveniently modeled by systems of nonlinear stochastic differential equations, whose variables encode the densities of various chemical species [1, 2]. Complex biological networks might involve hundreds of such species [3], and learning stochastic models from data is an important (and challenging) computational task [4]. Considering one such chemical reaction network in proximity of an equilibrium point, the model (1) can be used to trace ﬂuctuations of the species counts with respect to the equilibrium values. The network G would represent in this case the interactions between different chemical factors. Work in this area focused so-far on low-dimensional networks, i.e. on methods that are guaranteed to be correct for ﬁxed p, as T → ∞, while we will tackle here the regime in which both p and T diverge. Before stating our results, it is useful to stress a few important differences with respect to classical graphical model learning problems: (i) Samples are not independent. This can (and does) increase the sample complexity. (ii) On the other hand, inﬁnitely many samples are given as data (in fact a collection indexed by the continuous parameter t ∈ [0, T ]). Of course one can select a ﬁnite subsample, for instance at regularly spaced times {x(i η)}i=0,1,... . This raises the question as to whether the learning performances depend on the choice of the spacing η. (iii) In particular, one expects that choosing η sufﬁciently large as to make the conﬁgurations in the subsample approximately independent can be harmful. Indeed, the matrix A0 contains more information than the stationary distribution of the above process (1), and only the latter can be learned from independent samples. (iv) On the other hand, letting η → 0, one can produce an arbitrarily large number of distinct samples. However, samples become more dependent, and intuitively one expects that there is limited information to be harnessed from a given time interval T . Our results conﬁrm in a detailed and quantitative way these intuitions. 1.1 Results: Regularized least squares Regularized least squares is an efﬁcient and well-studied method for support recovery. We will discuss relations with existing literature in Section 1.3. In the present case, the algorithm reconstructs independently each row of the matrix A0 . The rth row, A0 , is estimated by solving the following convex optimization problem for Ar ∈ Rp r minimize L(Ar ; {x(t)}t∈[0,T ] ) + λ Ar 1 , (2) where the likelihood function L is deﬁned by L(Ar ; {x(t)}t∈[0,T ] ) = 1 2T T 0 (A∗ x(t))2 dt − r 1 T T 0 (A∗ x(t)) dxr (t) . r (3) (Here and below M ∗ denotes the transpose of matrix/vector M .) To see that this likelihood function is indeed related to least squares, one can formally write xr (t) = dxr (t)/dt and complete the square ˙ for the right hand side of Eq. (3), thus getting the integral (A∗ x(t) − xr (t))2 dt − xr (t)2 dt. ˙ ˙ r The ﬁrst term is a sum of square residuals, and the second is independent of A. Finally the ℓ1 regularization term in Eq. (2) has the role of shrinking to 0 a subset of the entries Aij thus effectively selecting the structure. Let S 0 be the support of row A0 , and assume |S 0 | ≤ k. We will refer to the vector sign(A0 ) as to r r the signed support of A0 (where sign(0) = 0 by convention). Let λmax (M ) and λmin (M ) stand for r 2 the maximum and minimum eigenvalue of a square matrix M respectively. Further, denote by Amin the smallest absolute value among the non-zero entries of row A0 . r When stable, the diffusion process (1) has a unique stationary measure which is Gaussian with covariance Q0 ∈ Rp×p given by the solution of Lyapunov’s equation [5] A0 Q0 + Q0 (A0 )∗ + I = 0. (4) Our guarantee for regularized least squares is stated in terms of two properties of the covariance Q0 and one assumption on ρmin (A0 ) (given a matrix M , we denote by ML,R its submatrix ML,R ≡ (Mij )i∈L,j∈R ): (a) We denote by Cmin ≡ λmin (Q0 0 ,S 0 ) the minimum eigenvalue of the restriction of Q0 to S the support S 0 and assume Cmin > 0. (b) We deﬁne the incoherence parameter α by letting |||Q0 (S 0 )C ,S 0 Q0 S 0 ,S 0 and assume α > 0. (Here ||| · |||∞ is the operator sup norm.) −1 |||∞ = 1 − α, ∗ (c) We deﬁne ρmin (A0 ) = −λmax ((A0 + A0 )/2) and assume ρmin (A0 ) > 0. Note this is a stronger form of stability assumption. Our main result is to show that there exists a well deﬁned time complexity, i.e. a minimum time interval T such that, observing the system for time T enables us to reconstruct the network with high probability. This result is stated in the following theorem. Theorem 1.1. Consider the problem of learning the support S 0 of row A0 of the matrix A0 from a r sample trajectory {x(t)}t∈[0,T ] distributed according to the model (1). If T > 104 k 2 (k ρmin (A0 )−2 + A−2 ) 4pk min log , 2 α2 ρmin (A0 )Cmin δ (5) then there exists λ such that ℓ1 -regularized least squares recovers the signed support of A0 with r probability larger than 1 − δ. This is achieved by taking λ = 36 log(4p/δ)/(T α2 ρmin (A0 )) . The time complexity is logarithmic in the number of variables and polynomial in the support size. Further, it is roughly inversely proportional to ρmin (A0 ), which is quite satisfying conceptually, since ρmin (A0 )−1 controls the relaxation time of the mixes. 1.2 Overview of other results So far we focused on continuous-time dynamics. While, this is useful in order to obtain elegant statements, much of the paper is in fact devoted to the analysis of the following discrete-time dynamics, with parameter η > 0: x(t) = x(t − 1) + ηA0 x(t − 1) + w(t), t ∈ N0 . (6) Here x(t) ∈ Rp is the vector collecting the dynamical variables, A0 ∈ Rp×p speciﬁes the dynamics as above, and {w(t)}t≥0 is a sequence of i.i.d. normal vectors with covariance η Ip×p (i.e. with independent components of variance η). We assume that consecutive samples {x(t)}0≤t≤n are given and will ask under which conditions regularized least squares reconstructs the support of A0 . The parameter η has the meaning of a time-step size. The continuous-time model (1) is recovered, in a sense made precise below, by letting η → 0. Indeed we will prove reconstruction guarantees that are uniform in this limit as long as the product nη (which corresponds to the time interval T in the previous section) is kept constant. For a formal statement we refer to Theorem 3.1. Theorem 1.1 is indeed proved by carefully controlling this limit. The mathematical challenge in this problem is related to the fundamental fact that the samples {x(t)}0≤t≤n are dependent (and strongly dependent as η → 0). Discrete time models of the form (6) can arise either because the system under study evolves by discrete steps, or because we are subsampling a continuous time system modeled as in Eq. (1). Notice that in the latter case the matrices A0 appearing in Eq. (6) and (1) coincide only to the zeroth order in η. Neglecting this technical complication, the uniformity of our reconstruction guarantees as η → 0 has an appealing interpretation already mentioned above. Whenever the samples spacing is not too large, the time complexity (i.e. the product nη) is roughly independent of the spacing itself. 3 1.3 Related work A substantial amount of work has been devoted to the analysis of ℓ1 regularized least squares, and its variants [6, 7, 8, 9, 10]. The most closely related results are the one concerning high-dimensional consistency for support recovery [11, 12]. Our proof follows indeed the line of work developed in these papers, with two important challenges. First, the design matrix is in our case produced by a stochastic diffusion, and it does not necessarily satisﬁes the irrepresentability conditions used by these works. Second, the observations are not corrupted by i.i.d. noise (since successive conﬁgurations are correlated) and therefore elementary concentration inequalities are not sufﬁcient. Learning sparse graphical models via ℓ1 regularization is also a topic with signiﬁcant literature. In the Gaussian case, the graphical LASSO was proposed to reconstruct the model from i.i.d. samples [13]. In the context of binary pairwise graphical models, Ref. [11] proves high-dimensional consistency of regularized logistic regression for structural learning, under a suitable irrepresentability conditions on a modiﬁed covariance. Also this paper focuses on i.i.d. samples. Most of these proofs builds on the technique of [12]. A naive adaptation to the present case allows to prove some performance guarantee for the discrete-time setting. However the resulting bounds are not uniform as η → 0 for nη = T ﬁxed. In particular, they do not allow to prove an analogous of our continuous time result, Theorem 1.1. A large part of our effort is devoted to producing more accurate probability estimates that capture the correct scaling for small η. Similar issues were explored in the study of stochastic differential equations, whereby one is often interested in tracking some slow degrees of freedom while ‘averaging out’ the fast ones [14]. The relevance of this time-scale separation for learning was addressed in [15]. Let us however emphasize that these works focus once more on system with a ﬁxed (small) number of dimensions p. Finally, the related topic of learning graphical models for autoregressive processes was studied recently in [16, 17]. The convex relaxation proposed in these papers is different from the one developed here. Further, no model selection guarantee was proved in [16, 17]. 2 Illustration of the main results It might be difﬁcult to get a clear intuition of Theorem 1.1, mainly because of conditions (a) and (b), which introduce parameters Cmin and α. The same difﬁculty arises with analogous results on the high-dimensional consistency of the LASSO [11, 12]. In this section we provide concrete illustration both via numerical simulations, and by checking the condition on speciﬁc classes of graphs. 2.1 Learning the laplacian of graphs with bounded degree Given a simple graph G = (V, E) on vertex set V = [p], its laplacian ∆G is the symmetric p × p matrix which is equal to the adjacency matrix of G outside the diagonal, and with entries ∆G = ii −deg(i) on the diagonal [18]. (Here deg(i) denotes the degree of vertex i.) It is well known that ∆G is negative semideﬁnite, with one eigenvalue equal to 0, whose multiplicity is equal to the number of connected components of G. The matrix A0 = −m I + ∆G ﬁts into the setting of Theorem 1.1 for m > 0. The corresponding model (1.1) describes the over-damped dynamics of a network of masses connected by springs of unit strength, and connected by a spring of strength m to the origin. We obtain the following result. Theorem 2.1. Let G be a simple connected graph of maximum vertex degree k and consider the model (1.1) with A0 = −m I + ∆G where ∆G is the laplacian of G and m > 0. If k+m 5 4pk T ≥ 2 · 105 k 2 , (7) (k + m2 ) log m δ then there exists λ such that ℓ1 -regularized least squares recovers the signed support of A0 with r probability larger than 1 − δ. This is achieved by taking λ = 36(k + m)2 log(4p/δ)/(T m3 ). In other words, for m bounded away from 0 and ∞, regularized least squares regression correctly reconstructs the graph G from a trajectory of time length which is polynomial in the degree and logarithmic in the system size. Notice that once the graph is known, the laplacian ∆G is uniquely determined. Also, the proof technique used for this example is generalizable to other graphs as well. 4 2800 Min. # of samples for success prob. = 0.9 1 0.9 p = 16 p = 32 0.8 Probability of success p = 64 0.7 p = 128 p = 256 0.6 p = 512 0.5 0.4 0.3 0.2 0.1 0 0 50 100 150 200 250 300 T=nη 350 400 2600 2400 2200 2000 1800 1600 1400 1200 1 10 450 2 3 10 10 p Figure 1: (left) Probability of success vs. length of the observation interval nη. (right) Sample complexity for 90% probability of success vs. p. 2.2 Numerical illustrations In this section we present numerical validation of the proposed method on synthetic data. The results conﬁrm our observations in Theorems 1.1 and 3.1, below, namely that the time complexity scales logarithmically with the number of nodes in the network p, given a constant maximum degree. Also, the time complexity is roughly independent of the sampling rate. In Fig. 1 and 2 we consider the discrete-time setting, generating data as follows. We draw A0 as a random sparse matrix in {0, 1}p×p with elements chosen independently at random with P(A0 = 1) = k/p, k = 5. The ij process xn ≡ {x(t)}0≤t≤n is then generated according to Eq. (6). We solve the regularized least 0 square problem (the cost function is given explicitly in Eq. (8) for the discrete-time case) for different values of n, the number of observations, and record if the correct support is recovered for a random row r using the optimum value of the parameter λ. An estimate of the probability of successful recovery is obtained by repeating this experiment. Note that we are estimating here an average probability of success over randomly generated matrices. The left plot in Fig.1 depicts the probability of success vs. nη for η = 0.1 and different values of p. Each curve is obtained using 211 instances, and each instance is generated using a new random matrix A0 . The right plot in Fig.1 is the corresponding curve of the sample complexity vs. p where sample complexity is deﬁned as the minimum value of nη with probability of success of 90%. As predicted by Theorem 2.1 the curve shows the logarithmic scaling of the sample complexity with p. In Fig. 2 we turn to the continuous-time model (1). Trajectories are generated by discretizing this stochastic differential equation with step δ much smaller than the sampling rate η. We draw random matrices A0 as above and plot the probability of success for p = 16, k = 4 and different values of η, as a function of T . We used 211 instances for each curve. As predicted by Theorem 1.1, for a ﬁxed observation interval T , the probability of success converges to some limiting value as η → 0. 3 Discrete-time model: Statement of the results Consider a system evolving in discrete time according to the model (6), and let xn ≡ {x(t)}0≤t≤n 0 be the observed portion of the trajectory. The rth row A0 is estimated by solving the following r convex optimization problem for Ar ∈ Rp minimize L(Ar ; xn ) + λ Ar 0 where L(Ar ; xn ) ≡ 0 1 2η 2 n 1 , (8) n−1 2 t=0 {xr (t + 1) − xr (t) − η A∗ x(t)} . r (9) Apart from an additive constant, the η → 0 limit of this cost function can be shown to coincide with the cost function in the continuous time case, cf. Eq. (3). Indeed the proof of Theorem 1.1 will amount to a more precise version of this statement. Furthermore, L(Ar ; xn ) is easily seen to be the 0 log-likelihood of Ar within model (6). 5 1 1 0.9 0.95 0.9 0.7 Probability of success Probability of success 0.8 η = 0.04 η = 0.06 0.6 η = 0.08 0.5 η = 0.1 0.4 η = 0.14 0.3 η = 0.22 η = 0.18 0.85 0.8 0.75 0.7 0.65 0.2 0.6 0.1 0 50 100 150 T=nη 200 0.55 0.04 250 0.06 0.08 0.1 0.12 η 0.14 0.16 0.18 0.2 0.22 Figure 2: (right)Probability of success vs. length of the observation interval nη for different values of η. (left) Probability of success vs. η for a ﬁxed length of the observation interval, (nη = 150) . The process is generated for a small value of η and sampled at different rates. As before, we let S 0 be the support of row A0 , and assume |S 0 | ≤ k. Under the model (6) x(t) has r a Gaussian stationary state distribution with covariance Q0 determined by the following modiﬁed Lyapunov equation A0 Q0 + Q0 (A0 )∗ + ηA0 Q0 (A0 )∗ + I = 0 . (10) It will be clear from the context whether A0 /Q0 refers to the dynamics/stationary matrix from the continuous or discrete time system. We assume conditions (a) and (b) introduced in Section 1.1, and adopt the notations already introduced there. We use as a shorthand notation σmax ≡ σmax (I +η A0 ) where σmax (.) is the maximum singular value. Also deﬁne D ≡ 1 − σmax /η . We will assume D > 0. As in the previous section, we assume the model (6) is initiated in the stationary state. Theorem 3.1. Consider the problem of learning the support S 0 of row A0 from the discrete-time r trajectory {x(t)}0≤t≤n . If nη > 4pk 104 k 2 (kD−2 + A−2 ) min log , 2 DC 2 α δ min (11) then there exists λ such that ℓ1 -regularized least squares recovers the signed support of A0 with r probability larger than 1 − δ. This is achieved by taking λ = (36 log(4p/δ))/(Dα2 nη). In other words the discrete-time sample complexity, n, is logarithmic in the model dimension, polynomial in the maximum network degree and inversely proportional to the time spacing between samples. The last point is particularly important. It enables us to derive the bound on the continuoustime sample complexity as the limit η → 0 of the discrete-time sample complexity. It also conﬁrms our intuition mentioned in the Introduction: although one can produce an arbitrary large number of samples by sampling the continuous process with ﬁner resolutions, there is limited amount of information that can be harnessed from a given time interval [0, T ]. 4 Proofs In the following we denote by X ∈ Rn×p the matrix whose (t + 1)th column corresponds to the conﬁguration x(t), i.e. X = [x(0), x(1), . . . , x(n − 1)]. Further ∆X ∈ Rn×p is the matrix containing conﬁguration changes, namely ∆X = [x(1) − x(0), . . . , x(n) − x(n − 1)]. Finally we write W = [w(1), . . . , w(n − 1)] for the matrix containing the Gaussian noise realization. Equivalently, The r th row of W is denoted by Wr . W = ∆X − ηA X . In order to lighten the notation, we will omit the reference to xn in the likelihood function (9) and 0 simply write L(Ar ). We deﬁne its normalized gradient and Hessian by G = −∇L(A0 ) = r 1 ∗ XWr , nη Q = ∇2 L(A0 ) = r 6 1 XX ∗ . n (12) 4.1 Discrete time In this Section we outline our prove for our main result for discrete-time dynamics, i.e., Theorem 3.1. We start by stating a set of sufﬁcient conditions for regularized least squares to work. Then we present a series of concentration lemmas to be used to prove the validity of these conditions, and ﬁnally we sketch the outline of the proof. As mentioned, the proof strategy, and in particular the following proposition which provides a compact set of sufﬁcient conditions for the support to be recovered correctly is analogous to the one in [12]. A proof of this proposition can be found in the supplementary material. Proposition 4.1. Let α, Cmin > 0 be be deﬁned by λmin (Q0 0 ,S 0 ) ≡ Cmin , S |||Q0 0 )C ,S 0 Q0 0 ,S 0 S (S −1 |||∞ ≡ 1 − α . (13) If the following conditions hold then the regularized least square solution (8) correctly recover the signed support sign(A0 ): r λα Amin Cmin G ∞≤ , GS 0 ∞ ≤ − λ, (14) 3 4k α Cmin α Cmin √ , √ . |||QS 0 ,S 0 − Q0 0 ,S 0 |||∞ ≤ (15) |||Q(S 0 )C ,S 0 − Q0 0 )C ,S 0 |||∞ ≤ S (S 12 k 12 k Further the same statement holds for the continuous model 3, provided G and Q are the gradient and the hessian of the likelihood (3). The proof of Theorem 3.1 consists in checking that, under the hypothesis (11) on the number of consecutive conﬁgurations, conditions (14) to (15) will hold with high probability. Checking these conditions can be regarded in turn as concentration-of-measure statements. Indeed, if expectation is taken with respect to a stationary trajectory, we have E{G} = 0, E{Q} = Q0 . 4.1.1 Technical lemmas In this section we will state the necessary concentration lemmas for proving Theorem 3.1. These are non-trivial because G, Q are quadratic functions of dependent random variables the samples {x(t)}0≤t≤n . The proofs of Proposition 4.2, of Proposition 4.3, and Corollary 4.4 can be found in the supplementary material provided. Our ﬁrst Proposition implies concentration of G around 0. Proposition 4.2. Let S ⊆ [p] be any set of vertices and ǫ < 1/2. If σmax ≡ σmax (I + η A0 ) < 1, then 2 P GS ∞ > ǫ ≤ 2|S| e−n(1−σmax ) ǫ /4 . (16) We furthermore need to bound the matrix norms as per (15) in proposition 4.1. First we relate bounds on |||QJS − Q0 JS |||∞ with bounds on |Qij − Q0 |, (i ∈ J, i ∈ S) where J and S are any ij subsets of {1, ..., p}. We have, P(|||QJS − Q0 )|||∞ > ǫ) ≤ |J||S| max P(|Qij − Q0 | > ǫ/|S|). JS ij i,j∈J (17) Then, we bound |Qij − Q0 | using the following proposition ij Proposition 4.3. Let i, j ∈ {1, ..., p}, σmax ≡ σmax (I + ηA0 ) < 1, T = ηn > 3/D and 0 < ǫ < 2/D where D = (1 − σmax )/η then, P(|Qij − Q0 )| > ǫ) ≤ 2e ij n − 32η2 (1−σmax )3 ǫ2 . (18) Finally, the next corollary follows from Proposition 4.3 and Eq. (17). Corollary 4.4. Let J, S (|S| ≤ k) be any two subsets of {1, ..., p} and σmax ≡ σmax (I + ηA0 ) < 1, ǫ < 2k/D and nη > 3/D (where D = (1 − σmax )/η) then, P(|||QJS − Q0 |||∞ > ǫ) ≤ 2|J|ke JS 7 n − 32k2 η2 (1−σmax )3 ǫ2 . (19) 4.1.2 Outline of the proof of Theorem 3.1 With these concentration bounds we can now easily prove Theorem 3.1. All we need to do is to compute the probability that the conditions given by Proposition 4.1 hold. From the statement of the theorem we have that the ﬁrst two conditions (α, Cmin > 0) of Proposition 4.1 hold. In order to make the ﬁrst condition on G imply the second condition on G we assume that λα/3 ≤ (Amin Cmin )/(4k) − λ which is guaranteed to hold if λ ≤ Amin Cmin /8k. (20) We also combine the two last conditions on Q, thus obtaining the following |||Q[p],S 0 − Q0 0 |||∞ ≤ [p],S α Cmin √ , 12 k (21) since [p] = S 0 ∪ (S 0 )C . We then impose that both the probability of the condition on Q failing and the probability of the condition on G failing are upper bounded by δ/2 using Proposition 4.2 and Corollary 4.4. It is shown in the supplementary material that this is satisﬁed if condition (11) holds. 4.2 Outline of the proof of Theorem 1.1 To prove Theorem 1.1 we recall that Proposition 4.1 holds provided the appropriate continuous time expressions are used for G and Q, namely G = −∇L(A0 ) = r 1 T T x(t) dbr (t) , 0 Q = ∇2 L(A0 ) = r 1 T T x(t)x(t)∗ dt . (22) 0 These are of course random variables. In order to distinguish these from the discrete time version, we will adopt the notation Gn , Qn for the latter. We claim that these random variables can be coupled (i.e. deﬁned on the same probability space) in such a way that Gn → G and Qn → Q almost surely as n → ∞ for ﬁxed T . Under assumption (5), it is easy to show that (11) holds for all n > n0 with n0 a sufﬁciently large constant (for a proof see the provided supplementary material). Therefore, by the proof of Theorem 3.1, the conditions in Proposition 4.1 hold for gradient Gn and hessian Qn for any n ≥ n0 , with probability larger than 1 − δ. But by the claimed convergence Gn → G and Qn → Q, they hold also for G and Q with probability at least 1 − δ which proves the theorem. We are left with the task of showing that the discrete and continuous time processes can be coupled in such a way that Gn → G and Qn → Q. With slight abuse of notation, the state of the discrete time system (6) will be denoted by x(i) where i ∈ N and the state of continuous time system (1) by x(t) where t ∈ R. We denote by Q0 the solution of (4) and by Q0 (η) the solution of (10). It is easy to check that Q0 (η) → Q0 as η → 0 by the uniqueness of stationary state distribution. The initial state of the continuous time system x(t = 0) is a N(0, Q0 ) random variable independent of b(t) and the initial state of the discrete time system is deﬁned to be x(i = 0) = (Q0 (η))1/2 (Q0 )−1/2 x(t = 0). At subsequent times, x(i) and x(t) are assumed are generated by the respective dynamical systems using the same matrix A0 using common randomness provided by the standard Brownian motion {b(t)}0≤t≤T in Rp . In order to couple x(t) and x(i), we construct w(i), the noise driving the discrete time system, by letting w(i) ≡ (b(T i/n) − b(T (i − 1)/n)). The almost sure convergence Gn → G and Qn → Q follows then from standard convergence of random walk to Brownian motion. Acknowledgments This work was partially supported by a Terman fellowship, the NSF CAREER award CCF-0743978 and the NSF grant DMS-0806211 and by a Portuguese Doctoral FCT fellowship. 8 References [1] D.T. Gillespie. Stochastic simulation of chemical kinetics. Annual Review of Physical Chemistry, 58:35–55, 2007. [2] D. Higham. Modeling and Simulating Chemical Reactions. SIAM Review, 50:347–368, 2008. [3] N.D.Lawrence et al., editor. Learning and Inference in Computational Systems Biology. MIT Press, 2010. [4] T. Toni, D. Welch, N. Strelkova, A. Ipsen, and M.P.H. Stumpf. Modeling and Simulating Chemical Reactions. J. R. Soc. Interface, 6:187–202, 2009. [5] K. Zhou, J.C. Doyle, and K. Glover. Robust and optimal control. Prentice Hall, 1996. [6] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288, 1996. [7] D.L. Donoho. For most large underdetermined systems of equations, the minimal l1-norm near-solution approximates the sparsest near-solution. Communications on Pure and Applied Mathematics, 59(7):907–934, 2006. [8] D.L. Donoho. For most large underdetermined systems of linear equations the minimal l1norm solution is also the sparsest solution. Communications on Pure and Applied Mathematics, 59(6):797–829, 2006. [9] T. Zhang. Some sharp performance bounds for least squares regression with L1 regularization. Annals of Statistics, 37:2109–2144, 2009. [10] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1constrained quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183–2202, 2009. [11] M.J. Wainwright, P. Ravikumar, and J.D. Lafferty. High-Dimensional Graphical Model Selection Using l-1-Regularized Logistic Regression. Advances in Neural Information Processing Systems, 19:1465, 2007. [12] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2541–2563, 2006. [13] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432, 2008. [14] K. Ball, T.G. Kurtz, L. Popovic, and G. Rempala. Modeling and Simulating Chemical Reactions. Ann. Appl. Prob., 16:1925–1961, 2006. [15] G.A. Pavliotis and A.M. Stuart. Parameter estimation for multiscale diffusions. J. Stat. Phys., 127:741–781, 2007. [16] J. Songsiri, J. Dahl, and L. Vandenberghe. Graphical models of autoregressive processes. pages 89–116, 2010. [17] J. Songsiri and L. Vandenberghe. Topology selection in graphical models of autoregressive processes. Journal of Machine Learning Research, 2010. submitted. [18] F.R.K. Chung. Spectral Graph Theory. CBMS Regional Conference Series in Mathematics, 1997. [19] P. Ravikumar, M.J. Wainwright, and J. Lafferty. High-dimensional Ising model selection using l1-regularized logistic regression. Annals of Statistics, 2008. 9</p><p>5 0.47441709 <a title="122-lsi-5" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>Author: George Konidaris, Scott Kuindersma, Roderic Grupen, Andre S. Barreto</p><p>Abstract: We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further reﬁned through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction. 1</p><p>6 0.47321281 <a title="122-lsi-6" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>7 0.47216514 <a title="122-lsi-7" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>8 0.45874101 <a title="122-lsi-8" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>9 0.45779246 <a title="122-lsi-9" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>10 0.4410395 <a title="122-lsi-10" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>11 0.43773964 <a title="122-lsi-11" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>12 0.43517068 <a title="122-lsi-12" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>13 0.43411595 <a title="122-lsi-13" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>14 0.43397808 <a title="122-lsi-14" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>15 0.43064654 <a title="122-lsi-15" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>16 0.41713434 <a title="122-lsi-16" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>17 0.41284147 <a title="122-lsi-17" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>18 0.41206205 <a title="122-lsi-18" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>19 0.41084269 <a title="122-lsi-19" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>20 0.410101 <a title="122-lsi-20" href="./nips-2010-Empirical_Bernstein_Inequalities_for_U-Statistics.html">74 nips-2010-Empirical Bernstein Inequalities for U-Statistics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.123), (27, 0.239), (30, 0.08), (32, 0.064), (34, 0.158), (45, 0.134), (48, 0.011), (68, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77335316 <a title="122-lda-1" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>2 0.73663545 <a title="122-lda-2" href="./nips-2010-Identifying_Patients_at_Risk_of_Major_Adverse_Cardiovascular_Events_Using_Symbolic_Mismatch.html">116 nips-2010-Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch</a></p>
<p>Author: Zeeshan Syed, John V. Guttag</p><p>Abstract: Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have beneﬁted from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratiﬁcation. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identiﬁed using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We ﬁrst describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classiﬁer, and hierarchical clustering to improve risk stratiﬁcation. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically signiﬁcant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically signiﬁcantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days. 1</p><p>3 0.73163378 <a title="122-lda-3" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>Author: Remi Cuingnet, Marie Chupin, Habib Benali, Olivier Colliot</p><p>Abstract: Support vector machines (SVM) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data. Moreover, when the kernel is linear, SVMs can be used to localize spatial patterns of discrimination between two groups of subjects. However, the features’ spatial distribution is not taken into account. As a consequence, the optimal margin hyperplane is often scattered and lacks spatial coherence, making its anatomical interpretation difﬁcult. This paper introduces a framework to spatially regularize SVM for brain image analysis. We show that Laplacian regularization provides a ﬂexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3D brain images. The proposed framework is applied to the classiﬁcation of MR images based on gray matter concentration maps and cortical thickness measures from 30 patients with Alzheimer’s disease and 30 elderly controls. The results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classiﬁer. 1</p><p>4 0.71730942 <a title="122-lda-4" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>5 0.70297176 <a title="122-lda-5" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>Author: Peter Jones, Venkatesh Saligrama, Sanjoy Mitter</p><p>Abstract: Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence. Keywords: Bayesian Methods, Information Theory, consistency</p><p>6 0.70185679 <a title="122-lda-6" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>7 0.70091277 <a title="122-lda-7" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>8 0.70017797 <a title="122-lda-8" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>9 0.69596809 <a title="122-lda-9" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>10 0.69301057 <a title="122-lda-10" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>11 0.69169712 <a title="122-lda-11" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>12 0.68687397 <a title="122-lda-12" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>13 0.68373811 <a title="122-lda-13" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>14 0.68176657 <a title="122-lda-14" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>15 0.68156159 <a title="122-lda-15" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>16 0.6810441 <a title="122-lda-16" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>17 0.68080586 <a title="122-lda-17" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>18 0.68080425 <a title="122-lda-18" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>19 0.68014622 <a title="122-lda-19" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>20 0.67977113 <a title="122-lda-20" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
