<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-55" href="#">nips2010-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</h1>
<br/><p>Source: <a title="nips-2010-55-pdf" href="http://papers.nips.cc/paper/4153-cross-species-expression-analysis-using-a-dirichlet-process-mixture-model-with-latent-matchings.pdf">pdf</a></p><p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>Reference: <a title="nips-2010-55-reference" href="../nips2010_reference/nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. [sent-5, score-1.685]
</p><p>2 To perform such comparisons researchers need to match genes across species. [sent-6, score-0.478]
</p><p>3 Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. [sent-8, score-0.379]
</p><p>4 Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. [sent-9, score-1.152]
</p><p>5 Our method uses a Dirichlet process mixture model which includes a latent data matching variable. [sent-10, score-0.226]
</p><p>6 Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. [sent-12, score-0.804]
</p><p>7 1  Introduction  Researchers have been increasingly relying on cross species analysis to understand how biological systems operate. [sent-13, score-0.564]
</p><p>8 Sequence based methods have been successfully applied to identify and characterize coding and functional non coding regions in multiple species [1]. [sent-14, score-0.498]
</p><p>9 More recent studies attempt to integrate sequence and gene expression data from multiple species [2, 3, 4]. [sent-16, score-0.747]
</p><p>10 Unlike sequence, expression levels are dynamic and differ across time and conditions. [sent-17, score-0.172]
</p><p>11 By combining expression and sequence data researchers were able to identify both ”core” and ”divergent” genes. [sent-18, score-0.236]
</p><p>12 ”Core” genes are similarly expressed across species and are useful for constructing models of conserved systems, for example the cell cycle [2]. [sent-19, score-0.964]
</p><p>13 ”Divergent” genes are similar in sequence but differ in expression across species. [sent-20, score-0.582]
</p><p>14 These are useful for identifying species speciﬁc responses, for example why some pathogens are resistant to drugs while others are not [3]. [sent-21, score-0.498]
</p><p>15 While useful, cross species analysis of expression data is challenging. [sent-22, score-0.632]
</p><p>16 ) when comparing expression levels across species researchers need to match genes across species. [sent-24, score-1.112]
</p><p>17 For most genes the correct match in another species (known as ortholog) is not known. [sent-25, score-0.895]
</p><p>18 Such an assignment can be used to concatenate the expression vectors for matched genes across species and then cluster the resulting vectors. [sent-28, score-1.293]
</p><p>19 These were used to cluster the data from multiple species to identify conserved and divergent patterns. [sent-31, score-0.814]
</p><p>20 [6] deﬁned one of the species (species A) as a reference and ﬁrst clustered genes in A. [sent-33, score-0.838]
</p><p>21 They then used matched genes in the second species (B) as starting points for clustering 1  genes in B. [sent-34, score-1.355]
</p><p>22 When the clustering algorithm converges in B, genes that remain in the cluster are considered ”core” whereas genes that are removed are ”divergent”. [sent-35, score-0.949]
</p><p>23 [4] used a mixture of Gaussians model, which takes as input the expression data of orthologous genes and a phylogenetic tree connecting the species, to reconstruct the expression proﬁles as well as detecting divergent links in the phylogeny. [sent-37, score-0.851]
</p><p>24 combined experiments from multiple species by using Markov Random Fields [7] and Gaussian Random Fields [8] in which edges represent sequence similarity and potential functions constrain similar genes across species to have a similar expression pattern. [sent-40, score-1.506]
</p><p>25 In many cases the top sequence match is not the correct ortholog and a deterministic assignment may lead to wrong conclusions about the conservation of genes. [sent-42, score-0.188]
</p><p>26 Here we present a new method that uses soft assignments to allow comparison and clustering across species of arbitrary expression data without requiring prior knowledge on the phylogeny. [sent-44, score-0.787]
</p><p>27 Our method takes as input expression datasets in two species and a prior on matches between homologous genes in these species (derived from sequence data). [sent-45, score-1.573]
</p><p>28 The method simultaneously clusters the expression values for both species while computing a posterior for the assignment of orthologs for genes. [sent-46, score-0.786]
</p><p>29 We have tested our method on simulated and immune response data. [sent-48, score-0.256]
</p><p>30 While the method was developed for, and applied to, biological data, it is general and can be used to address other problems including matchings of captions to images (see Section 5). [sent-50, score-0.249]
</p><p>31 2  Problem deﬁnition  In this section, we ﬁrst describe in details the cross species analysis problem for gene expression data. [sent-51, score-0.751]
</p><p>32 Using microarrays or new sequencing techniques researchers can monitor the expression levels of genes under certain conditions or at speciﬁc time points. [sent-53, score-0.571]
</p><p>33 For each such measurement we obtain a vector whose elements are the expression values for all genes (there are usually thousands of entries in each vector). [sent-54, score-0.535]
</p><p>34 We assume that the input consists of microarray experiments from two species and each species has a different set of genes. [sent-55, score-0.924]
</p><p>35 While the exact matches between genes in both species are not known for most genes, we have a prior for gene pairs (one from each species) which is derived from sequence data [9]. [sent-56, score-1.098]
</p><p>36 Our goal is to simultaneously cluster the genes in both species. [sent-57, score-0.529]
</p><p>37 Such clustering can identify coherent and divergent responses between the species. [sent-58, score-0.189]
</p><p>38 In addition, we would like to infer for each gene in one species whether there exists a homolog that is similarly expressed in the other species and if so, who. [sent-59, score-1.043]
</p><p>39 In addition, let M be a sparse non-negative nx × ny matrix that encodes prior information regarding the matching of samples in x and y. [sent-68, score-0.45]
</p><p>40 We deﬁne the match probability between xi and yj as follows: p(xi and yj are matched) =  M(i, j) = πi,j Ni  n  p(xi is not matched) =  1 = πi,0 Ni  (1)  y where Ni = 1 + j=1 M(i, j). [sent-69, score-0.279]
</p><p>41 πi,0 is the prior probability that xi is not matched to any element in Y . [sent-70, score-0.188]
</p><p>42 Our goal is to infer both, the latent variables mj ’s and cluster membership for pairs of samples (xi , ymi )’s. [sent-82, score-0.317]
</p><p>43 We use this approach to develop a nonparametric model for clustering and matching cross species expression data. [sent-95, score-0.728]
</p><p>44 Our model, termed Dirichlet Process Mixture Model with Latent Matchings (DPMMLM) extends the popular Dirichlet Process Mixture Model to cases where priors are provided to matchings between vectors to be clustered. [sent-96, score-0.219]
</p><p>45 2  Dirichlet Process Mixture Model (DPMM)  Dirichlet process has been used as a nonparametric prior on the parameters of a mixture model. [sent-108, score-0.191]
</p><p>46 Using the stick-breaking construction in (2), the Dirichlet process mixture model is given by G ∼ DP(α, G0 )  zi , η i | G ∼ G  xi | zi , ηi ∼ F (ηi )  (3)  where F (ηi ) denotes the distribution of the observation xi given parameter ηi . [sent-111, score-0.489]
</p><p>47 3  Dirichlet Process Mixture Model with Latent Matchings (DPMMLM)  In this section, we describe the new mixture model based on DP with latent variables for data matching between x and y. [sent-113, score-0.183]
</p><p>48 Also, let zi be the mixture membership of the sample pair (xi , ymi ). [sent-116, score-0.329]
</p><p>49 In other words the assignment of x to a cluster depends on both, its own expression levels and the levels of the y component to which it is matched. [sent-118, score-0.318]
</p><p>50 5  Variational Inference for DPMMLM  Although the DP mixture model is an ”inﬁnite” mixture model, it is intractable to solve the optimization problem when allowing for inﬁnitely many variables. [sent-126, score-0.204]
</p><p>51 Following the variational inference framework for conjugate-exponential graphical models [15] we choose the distribution that factorizes over {mi , zi }i=1,. [sent-133, score-0.19]
</p><p>52 ,K−1 as follows: ny  nx  j=0  i=1  K  K−1 j  qθi,j (zi )mi  qφi (mi )  q(m, z, v, η) =  qλk (ηk )  qγk (vk ) k=1  (7)  k=1  where qφi (mi ) and qθi,j (zi ) are multinomial distributions and qγk (vk ) are beta distributions. [sent-142, score-0.352]
</p><p>53 • Update for qγk (vk ): nx  ny  nx  γk,1 = 1 +  ψi,j,k  ny  K  γk,2 = α +  i=1 j=0  ψi,j,t i=1 j=0 t=k+1  • Update for qθi,j (zi ) and qφi (mi ): k−1  E[log(1 − Vk )] + E[log Vk ]  θi,j,k ∝ exp ρi,j,k + k=1  k−1  K  φi,j ∝ exp log πi,j +  k=1  k=1  3. [sent-157, score-0.736]
</p><p>54 • Update for posterior distribution of µX , ΛX : κX = κX0 + nX  mX =  −1 −1 SX = SX0 + VX +  κX0 nX (x − mX0 )(x − mX0 )T κX0 + nX  ny  nx  where nX nx  νX = νX0 + nX ny  nx  1 ψi,j,k , x = = nX n=1 j=0  1 (κX0 mX0 + nX x) κX  ny  j=1  i=1  ψi,j,k (xi −b+W∗ yj −x)(xi −b+W∗ yj −x)T . [sent-177, score-1.322]
</p><p>55 ψi,0,k (xi −x)(xi −x)T +  VX =  ψi,j,k (xi − b + W∗ yj ) and  ψi,0,k xi +  j=1  i=1 ∗  ∗  ∗  ∗  • Update for W , b : We ﬁnd W , b that maximizes the log likelihood. [sent-178, score-0.18]
</p><p>56 Taking the derivative with respect to W∗ and solving for W∗ , we get nx  ny  ∗  nx  ψi,j,k (xi − mX −  W =  T ψi,j,k yj yj  i=1 j=1 nx ny  b =−  nx  ny  ψi,j,k (xi − mX + W yj ) / i=1 j=1  4. [sent-179, score-1.634]
</p><p>57 1  −1  i=1 j=1 ∗  ∗  4  ny  T b)yj  ψi,j,k i=1 j=1  Experiments and Results Simulated data  We demonstrate the performance of the model in identifying data matchings as well as cluster membership of datapoints using simulated data. [sent-180, score-0.579]
</p><p>58 To generate a simulated dataset, we sample 120 datapoints from a mixture of three 5-dimensional Gaussians with separation coefﬁcient = 2 leading to well separated mixtures1 . [sent-181, score-0.188]
</p><p>59 We compare the performance of our model(DPMMLM) with a standard Dirichlet Process Mixture Model where each component in x is matched based on the highest prior: {(xi , yj ∗ ) | i = 1, . [sent-199, score-0.2]
</p><p>60 Figure 1(a) presents the percentage of correct matchings inferred by DPMMLM and the highest prior matching. [sent-204, score-0.294]
</p><p>61 For DPMMLM, a datapoint xi is matched to the datapoint yj with the largest posterior probability φi,j . [sent-205, score-0.363]
</p><p>62 As can be seen, while the percentage of correct matchings decreased with the added noise, DPMMLM still achieves high NMI of 0. [sent-208, score-0.248]
</p><p>63 In conclusion, by relying on matchings of points DPMMLM can still performs very well in terms of its ability to identify correct clusters even with the high noise levels. [sent-211, score-0.381]
</p><p>64 1 Following [16], a Gaussian mixture is c-separated if for each pair (i, j) of components, mi − mj c2 D max(λmax , λmax ) , where λmax denotes the maximum eigenvalue of their covariance. [sent-212, score-0.245]
</p><p>65 i j  6  2  ≥  Normalized Mutual Information  % of correct matchings  90  DPMMLM Top matches  80 70 60 50 40 30 20 0  5  10  15  20  Number of random entries per row (t)  (a) The % of correct matchings. [sent-213, score-0.365]
</p><p>66 We compared human and mouse immune response datasets to identify similar and divergent genes. [sent-237, score-0.613]
</p><p>67 We selected two experiments that studied immune response to gram negative bacteria. [sent-238, score-0.221]
</p><p>68 The second looked at mouse response to Yersinia enterocolitica with and without treatment by IFN-γ [19]. [sent-242, score-0.263]
</p><p>69 We used BLASTN to compute the sequence similarity (bit-score) between all human and mouse genes. [sent-243, score-0.281]
</p><p>70 For each species we selected the most varying 500 genes and expanded the gene list to include all matched genes in the other species with a bit score greater than 75. [sent-244, score-1.892]
</p><p>71 This led to a set of 1476 human and 1967 mouse genes which we compared using our model. [sent-245, score-0.623]
</p><p>72 The M matrix is the bit scores between human and mouse genes thresholded at 75. [sent-246, score-0.623]
</p><p>73 In that ﬁgure, the ﬁrst ﬁve dimensions are human expression values and each gene in human is matched to the mouse gene with the highest posterior. [sent-248, score-0.779]
</p><p>74 Human genes which are not matched to any mouse gene in the cluster have a blank line on the mouse side of the ﬁgure. [sent-249, score-1.109]
</p><p>75 Clusters 1, 4 and 5 display a similar expression pattern in human and mouse with genes either up or down regulated in response to the infection. [sent-251, score-0.879]
</p><p>76 Genes in cluster 2 differ between the two species being mostly down regulated in humans while slightly upregulated in mouse. [sent-252, score-0.747]
</p><p>77 Human genes in cluster 3 also differ from their mouse orthologs. [sent-253, score-0.711]
</p><p>78 While they are strongly upregulated in humans, the corresponding mouse genes do not change much. [sent-254, score-0.647]
</p><p>79 001  GO term description regulation of apoptosis regulation of cell death protein binding regulation of programmed cell death positive regulation of cellular process positive regulation of biological process response to chemical stimulus cytoplasm  P value Corrected P 5. [sent-271, score-1.014]
</p><p>80 org) to calculate the enrichment of functional categories in each cluster based on the hypergeometric distribution. [sent-298, score-0.18]
</p><p>81 Genes in cluster 1 (Table 1) are associated with immune and stress responses. [sent-299, score-0.325]
</p><p>82 When clustering the two datasets independently the p-value for this category is greatly reduced indicating that accurate matchings can lead to better identiﬁcation of core pathways (see Appendix). [sent-303, score-0.291]
</p><p>83 Cluster 4 contains the most coherent set of upregulated genes across the two species. [sent-304, score-0.505]
</p><p>84 In contrast to clusters in which mouse and human genes are similarly expressed, cluster 3 genes are strongly upregulated in human cells while not changing in mouse. [sent-308, score-1.397]
</p><p>85 This cluster is enriched for ribosomal proteins (corrected p-value <0. [sent-309, score-0.224]
</p><p>86 There are studies that show that pathogens can upregulate the synthesis of ribosomal genes (which are required for translation) [21] whereas other studies indicate that ribosomal genes may not change much, or may even be reduced, following infection [22]. [sent-312, score-1.008]
</p><p>87 The results of our analysis indicate that while following Salmonella infection in human cells ribosomal genes are upregulated, they are not activated following Yarsinia infection in mouse. [sent-313, score-0.696]
</p><p>88 We have also analyzed the matchings obtained using sequence data alone (prior) and by combining sequence and expression data (posterior) using our method. [sent-314, score-0.419]
</p><p>89 The top posterior gene is the same as the top prior gene in most cases (905 of the 1476 human genes). [sent-315, score-0.409]
</p><p>90 293 human genes are not matched to any mouse gene in the cluster they are assigned to indicating that they are expressed in a species dependent manner. [sent-317, score-1.454]
</p><p>91 Additionally, for 278 human genes the top posterior and prior mouse gene differ. [sent-318, score-0.848]
</p><p>92 5  Conclusions  We have developed a new model for simultaneously clustering and matching genes across species. [sent-322, score-0.512]
</p><p>93 We have also demonstrated the power of our method on simulated data and immune response dataset. [sent-325, score-0.256]
</p><p>94 While the method was presented in the context of expression data it is general and can be used for other matching tasks in which a prior can be obtained. [sent-326, score-0.23]
</p><p>95 Sequencing and comparison of yeast species to identify genes and regulatory elements. [sent-340, score-0.874]
</p><p>96 A mixture model for the evolution of gene expression in non-homogeneous datasets. [sent-366, score-0.353]
</p><p>97 Identifying cycling genes by combining sequence homology and expression data. [sent-384, score-0.542]
</p><p>98 Host microarray analysis reveals a role for the Salmonella response regulator phoP in human macrophage cell death. [sent-450, score-0.178]
</p><p>99 Role of strain differences on host resistance and the transcriptional response of macrophages to infection with Yersinia enterocolitica. [sent-461, score-0.224]
</p><p>100 [Activation of transcription of ribosome genes following human embryo ﬁbroblast infection with cytomegalovirus in vitro]. [sent-480, score-0.519]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('species', 0.462), ('genes', 0.376), ('nx', 0.269), ('dpmmlm', 0.231), ('matchings', 0.219), ('mouse', 0.182), ('cluster', 0.153), ('immune', 0.14), ('expression', 0.132), ('zi', 0.127), ('fx', 0.124), ('dirichlet', 0.123), ('gene', 0.119), ('regulation', 0.113), ('divergent', 0.109), ('mi', 0.108), ('salmonella', 0.107), ('yj', 0.103), ('mixture', 0.102), ('vk', 0.099), ('matched', 0.097), ('dpmm', 0.094), ('apoptosis', 0.089), ('upregulated', 0.089), ('ny', 0.083), ('response', 0.081), ('infection', 0.078), ('fy', 0.072), ('ribosomal', 0.071), ('dp', 0.066), ('human', 0.065), ('clusters', 0.063), ('variational', 0.063), ('ymi', 0.062), ('matches', 0.061), ('corrected', 0.061), ('posterior', 0.06), ('gw', 0.057), ('conserved', 0.054), ('yersinia', 0.053), ('matching', 0.052), ('datapoints', 0.051), ('cellular', 0.049), ('rand', 0.049), ('prior', 0.046), ('xi', 0.045), ('clustering', 0.044), ('xy', 0.044), ('process', 0.043), ('regulated', 0.043), ('mx', 0.043), ('go', 0.042), ('across', 0.04), ('host', 0.038), ('cross', 0.038), ('membership', 0.038), ('identify', 0.036), ('bergmann', 0.036), ('ortholog', 0.036), ('orthologs', 0.036), ('pathogens', 0.036), ('quon', 0.036), ('rosenfeld', 0.036), ('simulated', 0.035), ('mj', 0.035), ('adjusted', 0.035), ('sequence', 0.034), ('researchers', 0.034), ('assignments', 0.034), ('relying', 0.034), ('assignment', 0.033), ('log', 0.032), ('cell', 0.032), ('stress', 0.032), ('nmi', 0.031), ('font', 0.031), ('infected', 0.031), ('jensen', 0.031), ('lu', 0.03), ('biological', 0.03), ('soft', 0.029), ('coupling', 0.029), ('latent', 0.029), ('correct', 0.029), ('vx', 0.029), ('sequencing', 0.029), ('datapoint', 0.029), ('uppercase', 0.029), ('core', 0.028), ('marginal', 0.028), ('cells', 0.028), ('match', 0.028), ('deterministic', 0.028), ('entries', 0.027), ('transcriptional', 0.027), ('stuart', 0.027), ('lowercase', 0.027), ('enrichment', 0.027), ('ni', 0.026), ('death', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="55-tfidf-1" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>2 0.10406904 <a title="55-tfidf-2" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>3 0.10382982 <a title="55-tfidf-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.10133035 <a title="55-tfidf-4" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>5 0.096655697 <a title="55-tfidf-5" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>Author: Feiping Nie, Heng Huang, Xiao Cai, Chris H. Ding</p><p>Abstract: Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efﬁcient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint 2,1 -norm minimization on both loss function and regularization. The 2,1 -norm based loss function is robust to outliers in data points and the 2,1 norm regularization selects features across all data points with joint sparsity. An efﬁcient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efﬁcient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method. 1</p><p>6 0.090973109 <a title="55-tfidf-6" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>7 0.088191092 <a title="55-tfidf-7" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>8 0.086380906 <a title="55-tfidf-8" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>9 0.081600852 <a title="55-tfidf-9" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>10 0.077054769 <a title="55-tfidf-10" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>11 0.076560922 <a title="55-tfidf-11" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>12 0.075693667 <a title="55-tfidf-12" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>13 0.075378492 <a title="55-tfidf-13" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>14 0.075134255 <a title="55-tfidf-14" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>15 0.07223843 <a title="55-tfidf-15" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>16 0.071871862 <a title="55-tfidf-16" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>17 0.069410831 <a title="55-tfidf-17" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>18 0.066100322 <a title="55-tfidf-18" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>19 0.063925996 <a title="55-tfidf-19" href="./nips-2010-Adaptive_Multi-Task_Lasso%3A_with_Application_to_eQTL_Detection.html">26 nips-2010-Adaptive Multi-Task Lasso: with Application to eQTL Detection</a></p>
<p>20 0.062794194 <a title="55-tfidf-20" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.054), (2, -0.017), (3, 0.049), (4, -0.143), (5, -0.001), (6, 0.039), (7, 0.02), (8, 0.028), (9, -0.048), (10, -0.017), (11, -0.072), (12, 0.02), (13, -0.056), (14, 0.106), (15, -0.032), (16, 0.038), (17, 0.12), (18, 0.096), (19, 0.06), (20, -0.007), (21, 0.025), (22, 0.065), (23, 0.079), (24, -0.109), (25, 0.18), (26, -0.038), (27, -0.1), (28, -0.024), (29, 0.055), (30, -0.024), (31, 0.1), (32, -0.022), (33, -0.009), (34, 0.074), (35, -0.1), (36, 0.023), (37, 0.01), (38, 0.034), (39, 0.122), (40, -0.135), (41, 0.038), (42, 0.077), (43, -0.061), (44, 0.039), (45, -0.023), (46, 0.064), (47, -0.107), (48, 0.072), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95017809 <a title="55-lsi-1" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>2 0.64605105 <a title="55-lsi-2" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>3 0.60410839 <a title="55-lsi-3" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>Author: Dirk Husmeier, Frank Dondelinger, Sophie Lebre</p><p>Abstract: Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have recently been proposed, allowing the network structure to change with time. However, unless time series are very long, this ﬂexibility leads to the risk of overﬁtting and inﬂated inference uncertainty. In the present paper we investigate three regularization schemes based on inter-segment information sharing, choosing different prior distributions and different coupling schemes between nodes. We apply our method to gene expression time series obtained during the Drosophila life cycle, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known in vivo regulatory network of ﬁve genes in yeast. 1</p><p>4 0.58916795 <a title="55-lsi-4" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>Author: Bela Frigyik, Maya Gupta, Yihua Chen</p><p>Abstract: Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data. 1 Modeling Probabilities for Machine Learning Modeling probability mass functions (pmfs) as random is useful in solving many real-world problems. A common random model for pmfs is the Dirichlet distribution [1]. The Dirichlet is conjugate to the multinomial and hence mathematically convenient for Bayesian inference, and the number of parameters is conveniently linear in the size of the sample space. However, the Dirichlet is a distribution over the entire probability simplex, and for many problems this is simply the wrong domain if there is application-speciﬁc prior knowledge that the pmfs come from a restricted subset of the simplex. For example, in natural language modeling, it is common to regularize a pmf over n-grams by some generic language model distribution q0 , that is, the pmf to be modeled is assumed to have the form θ = λq + (1 − λ)q0 for some q in the simplex, λ ∈ (0, 1) and a ﬁxed generic model q0 [2]. But once q0 and λ are ﬁxed, the pmf θ can only come from a subset of the simplex. Another natural language processing example is modeling the probability of keywords in a dictionary where some words are related, such as espresso and latte, and evidence for the one is to some extent evidence for the other. This relationship can be captured with a bounded variation model that would constrain the modeled probability of espresso to be within some of the modeled probability of latte. We show that such bounds on the variation between pmf components also restrict the domain of the pmf to a subset of the simplex. As a third example of restricting the domain, the similarity discriminant analysis classiﬁer estimates class-conditional pmfs that are constrained to be monotonically increasing over an ordered sample space of discrete similarity values [3]. In this paper we propose a simple variant of the Dirichlet whose support is a subset of the simplex, explore its properties, and show how to learn the model from data. We ﬁrst discuss the alternative solution of renormalizing the Dirichlet over the desired subset of the simplex, and other related work. Then we propose the shadow Dirichlet distribution; explain how to construct a shadow Dirichlet for three types of restricted domains: the regularized pmf case, bounded variation between pmf components, and monotonic pmfs; and discuss the most general case. We show how to use the expectation-maximization (EM) algorithm to estimate the shadow Dirichlet parameter α, and present simulation results for the estimation. 1 Dirichlet Shadow Dirichlet Renormalized Dirichlet Figure 1: Dirichlet, shadow Dirichlet, and renormalized Dirichlet for α = [3.94 2.25 2.81]. 2 Related Work One solution to modeling pmfs on only a subset of the simplex is to simply restrict the support of ˜ ˜ the Dirichlet to the desired support S, and renormalize the Dirichlet over S (see Fig. 1 for an example). This renormalized Dirichlet has the advantage that it is still a conjugate distribution for the multinomial. Nallapati et al.considered the renormalized Dirichlet for language modeling, but found it difﬁcult to use because the density requires numerical integration to compute the normalizer [4] . In addition, there is no closed form solution for the mean, covariance, or peak of the renormalized Dirichlet, making it difﬁcult to work with. Table 1 summarizes these properties. Additionally, generating samples from the renormalized Dirichlet is inefﬁcient: one draws samples from the stan˜ dard Dirichlet, then rejects realizations that are outside S. For high-dimensional sample spaces, this could greatly increase the time to generate samples. Although the Dirichlet is a classic and popular distribution on the simplex, Aitchison warns it “is totally inadequate for the description of the variability of compositional data,” because of its “implied independence structure and so the Dirichlet class is unlikely to be of any great use for describing compositions whose components have even weak forms of dependence” [5]. Aitchison instead championed a logistic normal distribution with more parameters to control covariance between components. A number of variants of the Dirichlet that can capture more dependence have been proposed and analyzed. For example, the scaled Dirichlet enables a more ﬂexible shape for the distribution [5], but does not change the support. The original Dirichlet(α1 , α2 , . . . αd ) can be derived as Yj / j Yj where Yj ∼ Γ(αj , β), whereas the scaled Dirichlet is derived from Yj ∼ Γ(αj , βj ), resulting in α density p(θ) = γ j ( α −1 βj j θ j j βi θi )α1 +···+αd i , where β, α ∈ Rd are parameters, and γ is the normalizer. + Another variant is the generalized Dirichlet [6] which also has parameters β, α ∈ Rd , and allows + greater control of the covariance structure, again without changing the support. As perhaps ﬁrst noted by Karl Pearson [7] and expounded upon by Aitchison [5], correlations of proportional data can be very misleading. Many Dirichlet variants have been generalizations of the Connor-Mossiman variant, Dirichlet process variants, other compound Dirichlet models, and hierarchical Dirichlet models. Ongaro et al. [8] propose the ﬂexible Dirichlet distribution by forming a re-parameterized mixture of Dirichlet distributions. Rayens and Srinivasan [9] considered the dependence structure for the general Dirichlet family called the generalized Liouville distributions. In contrast to prior efforts, the shadow Dirichlet manipulates the support to achieve various kinds of dependence that arise frequently in machine learning problems. 3 Shadow Dirichlet Distribution We introduce a new distribution that we call the shadow Dirichlet distribution. Let S be the prob˜ ability (d − 1)-simplex, and let Θ ∈ S be a random pmf drawn from a Dirichlet distribution with density pD and unnormalized parameter α ∈ Rd . Then we say the random pmf Θ ∈ S is distributed + ˜ according to a shadow Dirichlet distribution if Θ = M Θ for some ﬁxed d × d left-stochastic (that ˜ is, each column of M sums to 1) full-rank (and hence invertible) matrix M , and we call Θ the gen2 erating Dirichlet of Θ, or Θ’s Dirichlet shadow. Because M is a left-stochastic linear map between ﬁnite-dimensional spaces, it is a continuous map from the convex and compact S to a convex and compact subset of S that we denote SM . The shadow Dirichlet has two parameters: the generating Dirichlet’s parameter α ∈ Rd , and the + d × d matrix M . Both α and M can be estimated from data. However, as we show in the following subsections, the matrix M can be proﬁtably used as a design parameter that is chosen based on application-speciﬁc knowledge or side-information to specify the restricted domain SM , and in that way impose dependency between the components of the random pmfs. The shadow Dirichlet density p(θ) is the normalized pushforward of the Dirichlet density, that is, it is the composition of the Dirichlet density and M −1 with the Jacobian: 1 α −1 p(θ) = (M −1 θ)j j , (1) B(α) |det(M )| j Γ(αj ) d j is the standard Dirichlet normalizer, and α0 = j=1 αj is the standard where B(α) Γ(α0 ) Dirichlet precision factor. Table 1 summarizes the basic properties of the shadow Dirichlet. Fig. 1 shows an example shadow Dirichlet distribution. Generating samples from the shadow Dirichlet is trivial: generate samples from its generating Dirichlet (for example, using stick-breaking or urn-drawing) and multiply each sample by M to create the corresponding shadow Dirichlet sample. Table 1: Table compares and summarizes the Dirichlet, renormalized Dirichlet, and shadow Dirichlet distributions. Dirichlet(α) Density p(θ) Mean 1 B(α) d j=1 α −1 θj j Shadow Dirichlet (α, M ) 1 B(α)|det(M )| α α0 Renormalized ˜ Dirichlet (α, S) d −1 αj −1 θ)j j=1 (M 1 ˜ S M d j=1 α α0 αj −1 qj dq d j=1 α −1 θj j ˜ θp(θ)dθ S ¯ ¯ − θ)(θ − θ)T p(θ)dθ Cov(Θ) M Cov(Θ)M T αj −1 α0 −d j M α0 −d max p(θ) stick-breaking, urn-drawing draw from Dirichlet(α), multiply by M draw from Dirichlet(α), ˜ reject if not in S ML Estimate iterative (simple functions) iterative (simple functions) unknown complexity ML Compound Estimate iterative (simple functions) iterative (numerical integration) unknown complexity Covariance Mode (if α > 1) How to Sample 3.1 α −1 ˜ (θ S ˜ θ∈S Example: Regularized Pmfs The shadow Dirichlet can be designed to specify a distribution over a set of regularized pmfs SM = ˜ ˘ ˜ ˘ ˘ {θ θ = λθ + (1 − λ)θ, θ ∈ S}, for speciﬁc values of λ and θ. In general, for a given λ and θ ∈ S, the following d × d matrix M will change the support to the desired subset SM by mapping the extreme points of S to the extreme points of SM : ˘ M = (1 − λ)θ1T + λI, (2) where I is the d × d identity matrix. In Section 4 we show that the M given in (2) is optimal in a maximum entropy sense. 3 3.2 Example: Bounded Variation Pmfs We describe how to use the shadow Dirichlet to model a random pmf that has bounded variation such that |θk − θl | ≤ k,l for any k, ∈ {1, 2, . . . , d} and k,l > 0. To construct speciﬁed bounds on the variation, we ﬁrst analyze the variation for a given M . For any d × d left stochastic matrix T d d ˜ ˜ ˜ M, θ = Mθ = M1j θj . . . Mdj θj , so the difference between any two entries is j=1 j=1 ˜ |Mkj − Mlj | θj . ˜ (Mkj − Mlj )θj ≤ |θk − θl | = (3) j j Thus, to obtain a distribution over pmfs with bounded |θk − θ | ≤ k,l for any k, components, it is sufﬁcient to choose components of the matrix M such that |Mkj − Mlj | ≤ k,l for all j = 1, . . . , d ˜ because θ in (3) sums to 1. One way to create such an M is using the regularization strategy described in Section 3.1. For this ˜ ˜ ˘ case, the jth component of θ is θj = M θ = λθj + (1 − λ)θj , and thus the variation between the j ith and jth component of any pmf in SM is: ˜ ˘ ˜ ˘ ˜ ˜ ˘ ˘ |θi − θj | = λθi + (1 − λ)θi − λθj − (1 − λ)θj ≤ λ θi − θj + (1 − λ) θi − θj ˘ ˘ ≤ λ + (1 − λ) max θi − θj . (4) i,j ˘ Thus by choosing an appropriate λ and regularizing pmf θ, one can impose the bounded variation ˘ to be the uniform pmf, and choose any λ ∈ (0, 1), then the matrix given by (4). For example, set θ M given by (2) will guarantee that the difference between any two entries of any pmf drawn from the shadow Dirichlet (M, α) will be less than or equal to λ. 3.3 Example: Monotonic Pmfs For pmfs over ordered components, it may be desirable to restrict the support of the random pmf distribution to only monotonically increasing pmfs (or to only monotonically decreasing pmfs). A d × d left-stochastic matrix M that will result in a shadow Dirichlet that generates only monotonically increasing d × 1 pmfs has kth column [0 . . . 0 1/(d − k + 1) . . . 1/(d − k + 1)]T , we call this the monotonic M . It is easy to see that with this M only monotonic θ’s can be produced, 1˜ 1˜ 1 ˜ because θ1 = d θ1 which is less than or equal to θ2 = d θ1 + d−1 θ2 and so on. In Section 4 we show that the monotonic M is optimal in a maximum entropy sense. Note that to provide support over both monotonically increasing and decreasing pmfs with one distribution is not achievable with a shadow Dirichlet, but could be achieved by a mixture of two shadow Dirichlets. 3.4 What Restricted Subsets are Possible? Above we have described solutions to construct M for three kinds of dependence that arise in machine learning applications. Here we consider the more general question: What subsets of the simplex can be the support of the shadow Dirichlet, and how to design a shadow Dirichlet for a particular support? For any matrix M , by the Krein-Milman theorem [10], SM = M S is the convex hull of its extreme points. If M is injective, the extreme points of SM are easy to specify, as a d × d matrix M will have d extreme points that occur for the d choices of θ that have only one nonzero component, as the rest of the θ will create a non-trivial convex combination of the columns of M , and therefore cannot result in extreme points of SM by deﬁnition. That is, the extreme points of SM are the d columns of M , and one can design any SM with d extreme points by setting the columns of M to be those extreme pmfs. However, if one wants the new support to be a polytope in the probability (d − 1)-simplex with m > d extreme points, then one must use a fat M with d × m entries. Let S m denote the probability 4 (m − 1)-simplex, then the domain of the shadow Dirichlet will be M S m , which is the convex hull of the m columns of M and forms a convex polytope in S with at most m vertices. In this case M cannot be injective, and hence it is not bijective between S m and M S m . However, a density on M S m can be deﬁned as: p(θ) = 1 B(α) ˜ {θ ˜α −1 ˜ θj j dθ. ˜ M θ=θ} j (5) On the other hand, if one wants the support to be a low-dimensional polytope subset of a higherdimensional probability simplex, then a thin d × m matrix M , where m < d, can be used to implement this. If M is injective, then it has a left inverse M ∗ that is a matrix of dimension m × d, and the normalized pushforward of the original density can be used as a density on the image M S m : p(θ) = 1 α −1 1/2 B(α) |det(M T M )| (M ∗ θ)j j , j If M is not injective then one way to determine a density is to use (5). 4 Information-theoretic Properties In this section we note two information-theoretic properties of the shadow Dirichlet. Let Θ be drawn ˜ from shadow Dirichlet density pM , and let its generating Dirichlet Θ be drawn from pD . Then the differential entropy of the shadow Dirichlet is h(pM ) = log |det(M )| + h(pD ), where h(pD ) is the differential entropy of its generating Dirichlet. In fact, the shadow Dirichlet always has less entropy than its Dirichlet shadow because log |det(M )| ≤ 0, which can be shown as a corollary to the following lemma (proof not included due to lack of space): Lemma 4.1. Let {x1 , . . . , xn } and {y1 , . . . , yn } be column vectors in Rn . If each yj is a convex n n combination of the xi ’s, i.e. yj = i=1 γji xi , i=1 γji = 1, γjk ≥ 0, ∀j, k ∈ {1, . . . , n} then |det[y1 , . . . , yn ]| ≤ |det[x1 , . . . , xn ]|. It follows from Lemma 4.1 that the constructive solutions for M given in (2) and the monotonic M are optimal in the sense of maximizing entropy: Corollary 4.1. Let Mreg be the set of left-stochastic matrices M that parameterize shadow Dirichlet ˜ ˘ ˜ ˘ distributions with support in {θ θ = λθ + (1 − λ)θ, θ ∈ S}, for a speciﬁc choice of λ and θ. Then the M given in (2) results in the shadow Dirichlet with maximum entropy, that is, (2) solves arg maxM ∈Mreg h(pM ). Corollary 4.2. Let Mmono be the set of left-stochastic matrices M that parameterize shadow Dirichlet distributions that generate only monotonic pmfs. Then the monotonic M given in Section 3.3 results in the shadow Dirichlet with maximum entropy, that is, the monotonic M solves arg maxM ∈Mmono h(pM ). 5 Estimating the Distribution from Data In this section, we discuss the estimation of α for the shadow Dirichlet and compound shadow Dirichlet, and the estimation of M . 5.1 Estimating α for the Shadow Dirichlet Let matrix M be speciﬁed (for example, as described in the subsections of Section 3), and let q be a d × N matrix where the ith column qi is the ith sample pmf for i = 1 . . . N , and let (qi )j be the jth component of the ith sample pmf for j = 1, . . . , d. Then ﬁnding the maximum likelihood estimate 5 of α for the shadow Dirichlet is straightforward:  N 1 arg max log + log  p(qi |α) ≡ arg max log B(α) |det(M )| α∈Rk α∈Rk + + i=1   1 αj −1  (˜i )j q , ≡ arg max log  B(α)N i j α∈Rk +  N α −1 (M −1 qi )j j  i j (6) where q = M −1 q. Note (6) is the maximum likelihood estimation problem for the Dirichlet dis˜ tribution given the matrix q , and can be solved using the standard methods for that problem (see ˜ e.g. [11, 12]). 5.2 Estimating α for the Compound Shadow Dirichlet For many machine learning applications the given data are modeled as samples from realizations of a random pmf, and given these samples one must estimate the random pmf model’s parameters. We refer to this case as the compound shadow Dirichlet, analogous to the compound Dirichlet (also called the multivariate P´ lya distribution). Assuming one has already speciﬁed M , we ﬁrst discuss o method of moments estimation, and then describe an expectation-maximization (EM) method for computing the maximum likelihood estimate α. ˘ One can form an estimate of α by the method of moments. For the standard compound Dirichlet, one treats the samples of the realizations as normalized empirical histograms, sets the normalized α parameter equal to the empirical mean of the normalized histograms, and uses the empirical variances to determine the precision α0 . By deﬁnition, this estimate will be less likely than the maximum likelihood estimate, but may be a practical short-cut in some cases. For the compound shadow Dirichlet, we believe the method of moments estimator will be a poorer estimate in general. The problem is that if one draws samples from a pmf θ from a restricted subset SM of the simplex, ˘ then the normalized empirical histogram θ of those samples may not be in SM . For example given a monotonic pmf, the histogram of ﬁve samples drawn from it may not be monotonic. Then the empirical mean of such normalized empirical histograms may not be in SM , and so setting the shadow Dirichlet mean M α equal to the empirical mean may lead to an infeasible estimate (one that is outside SM ). A heuristic solution is to project the empirical mean into SM ﬁrst, for example, by ﬁnding the nearest pmf in SM in squared error or relative entropy. As with the compound Dirichlet, this may still be a useful approach in practice for some problems. Next we state an EM method to ﬁnd the maximum likelihood estimate α. Let s be a d × N matrix ˘ of sample histograms from different experiments, such that the ith column si is the ith histogram for i = 1, . . . , N , and (si )j is the number of times we have observed the jth event from the ith pmf vi . Then the maximum log-likelihood estimate of α solves arg max log p(s|α) for α ∈ Rk . + If the random pmfs are drawn from a Dirichlet distribution, then ﬁnding this maximum likelihood estimate requires an iterative procedure, and can be done in several ways including a gradient descent (ascent) approach. However, if the random pmfs are drawn from a shadow Dirichlet distribution, then a direct gradient descent approach is highly inconvenient as it requires taking derivatives of numerical integrals. However, it is practical to apply the expectation-maximization (EM) algorithm [13][14], as we describe in the rest of this section. Code to perform the EM estimation of α can be downloaded from idl.ee.washington.edu/publications.php. We assume that the experiments are independent and therefore p(s|α) = p({si }|α) = and hence arg maxα∈Rk log p(s|α) = arg maxα∈Rk i log p(si |α). + + i p(si |α) To apply the EM method, we consider the complete data to be the sample histograms s and the pmfs that generated them (s, v1 , v2 , . . . , vN ), whose expected log-likelihood will be maximized. Speciﬁcally, because of the assumed independence of the {vi }, the EM method requires one to repeatedly maximize the Q-function such that the estimate of α at the (m + 1)th iteration is: N α(m+1) = arg max α∈Rk + Evi |si ,α(m) [log p(vi |α)] . i=1 6 (7) Like the compound Dirichlet likelihood, the compound shadow Dirichlet likelihood is not necessarily concave. However, note that the Q-function given in (7) is concave, because log p(vi |α) = − log |det(M )| + log pD,α M −1 vi , where pD,α is the Dirichlet distribution with parameter α, and by a theorem of Ronning [11], log pD,α is a concave function, and adding a constant does not change the concavity. The Q-function is a ﬁnite integration of such concave functions and hence also concave [15]. We simplify (7) without destroying the concavity to yield the equivalent problem α(m+1) = d d arg max g(α) for α ∈ Rk , where g(α) = log Γ(α0 ) − j=1 log Γ(αj ) + j=1 βj αj , and + βj = N tij i=1 zi , 1 N where tij and zi are integrals we compute with Monte Carlo integration: d (s ) log(M −1 vi )j γi tij = SM (vi )k i k pM (vi |α(m) )dvi k=1 d zi = (vi )j k(si )k pM (vi |α(m) )dvi , γi SM k=1 where γi is the normalization constant for the multinomial with histogram si . We apply the Newton method [16] to maximize g(α), where the gradient g(α) has kth component ψ0 (α0 ) − ψ0 (α1 ) + β1 , where ψ0 denotes the digamma function. Let ψ1 denote the trigamma function, then the Hessian matrix of g(α) is: H = ψ1 (α0 )11T − diag (ψ1 (α1 ), . . . , ψ1 (αd )) . Note that because H has a very simple structure, the inversion of H required by the Newton step is greatly simpliﬁed by using the Woodbury identity [17]: H −1 = − diag(ξ1 , . . . , ξd ) − 1 1 1 [ξi ξj ]d×d , where ξ0 = ψ1 (α0 ) and ξj = ψ1 (αj ) , j = 1, . . . , d. ξ − d ξ 0 5.3 j=1 j Estimating M for the Shadow Dirichlet Thus far we have discussed how to construct M to achieve certain desired properties and how to interpret a given M ’s effect on the support. In some cases it may be useful to estimate M directly from data, for example, ﬁnding the maximum likelihood M . In general, this is a non-convex problem because the set of rank d − 1 matrices is not convex. However, we offer two approximations. First, note that as in estimating the support of a uniform distribution, the maximum likelihood M will correspond to a support that is no larger than needed to contain the convex hull of sample pmfs. Second, the mean of the empirical pmfs will be in the support, and thus a heuristic is to set the kth column of M (which corresponds to the kth vertex of the support) to be a convex combination of the kth vertex of the standard probability simplex and the empirical mean pmf. We provide code that ﬁnds the d optimal such convex combinations such that a speciﬁced percentage of the sample pmfs are within the support, which reduces the non-convex problem of ﬁnding the maximum likelihood d × d matrix M to a d-dimensional convex relaxation. 6 Demonstrations It is reasonable to believe that if the shadow Dirichlet better matches the problem’s statistics, it will perform better in practice, but an open question is how much better? To motivate the reader to investigate this question further in applications, we provide two small demonstrations. 6.1 Verifying the EM Estimation We used a broad suite of simulations to test and verify the EM estimation. Here we include a simple visual conﬁrmation that the EM estimation works: we drew 100 i.i.d. pmfs from a shadow Dirichlet with monotonic M for d = 3 and α = [3.94 2.25 2.81] (used in [18]). From each of the 100 pmfs, we drew 100 i.i.d. samples. Then we applied the EM algorithm to ﬁnd the α for both the standard compound Dirichlet, and the compound shadow Dirichlet with the correct M . Fig. 2 shows the true distribution and the two estimated distributions. 7 True Distribution (Shadow Dirichlet) Estimated Shadow Dirichlet Estimated Dirichlet Figure 2: Samples were drawn from the true distribution and the given EM method was applied to form the estimated distributions. 6.2 Estimating Proportions from Sales Manufacturers often have constrained manufacturing resources, such as equipment, inventory of raw materials, and employee time, with which to produce multiple products. The manufacturer must decide how to proportionally allocate such constrained resources across their product line based on their estimate of proportional sales. Manufacturer Artifact Puzzles gave us their past retail sales data for the 20 puzzles they sold during July 2009 through Dec 2009, which we used to predict the proportion of sales expected for each puzzle. These estimates were then tested on the next ﬁve months of sales data, for January 2010 through April 2010. The company also provided a similarity between puzzles S, where S(A, B) is the proportion of times an order during the six training months included both puzzle A and B if it included puzzle A. We compared treating each of the six training months of sales data as a sample from a compound Dirichlet versus or a compound shadow Dirichlet. For the shadow Dirichlet, we normalized each column of the similarity matrix S to sum to one so that it was left-stochastic, and used that as the M matrix; this forces puzzles that are often bought together to have closer estimated proportions. We estimated each α parameter by EM to maximize the likelihood of the past sales data, and then estimated the future sales proportions to be the mean of the estimated Dirichlet or shadow Dirichlet distribution. We also compared with treating all six months of sales data as coming from one multinomial which we estimated as the maximum likelihood multinomial, and to taking the mean of the six empirical pmfs. Table 2: Squared errors between estimates and actual proportional sales. Jan. Feb. Mar. Apr. 7 Multinomial .0129 .0185 .0231 .0240 Mean Pmf .0106 .0206 .0222 .0260 Dirichlet .0109 .0172 .0227 .0235 Shadow Dirichlet .0093 .0164 .0197 .0222 Summary In this paper we have proposed a variant of the Dirichlet distribution that naturally captures some of the dependent structure that arises often in machine learning applications. We have discussed some of its theoretical properties, and shown how to specify the distribution for regularized pmfs, bounded variation pmfs, monotonic pmfs, and for any desired convex polytopal domain. We have derived the EM method and made available code to estimate both the shadow Dirichlet and compound shadow Dirichlet from data. Experimental results demonstrate that the EM method can estimate the shadow Dirichlet effectively, and that the shadow Dirichlet may provide worthwhile advantages in practice. 8 References [1] B. Frigyik, A. Kapila, and M. R. Gupta, “Introduction to the Dirichlet distribution and related processes,” Tech. Rep., University of Washington, 2010. [2] C. Zhai and J. Lafferty, “A study of smoothing methods for language models applied to information retrieval,” ACM Trans. on Information Systems, vol. 22, no. 2, pp. 179–214, 2004. [3] Y. Chen, E. K. Garcia, M. R. Gupta, A. Rahimi, and L. Cazzanti, “Similarity-based classiﬁcation: Concepts and algorithms,” Journal of Machine Learning Research, vol. 10, pp. 747–776, March 2009. [4] R. Nallapati, T. Minka, and S. Robertson, “The smoothed-Dirichlet distribution: a building block for generative topic models,” Tech. Rep., Microsoft Research, Cambridge, 2007. [5] Aitchison, Statistical Analysis of Compositional Data, Chapman Hall, New York, 1986. [6] R. J. Connor and J. E. Mosiman, “Concepts of independence for proportions with a generalization of the Dirichlet distibution,” Journal of the American Statistical Association, vol. 64, pp. 194–206, 1969. [7] K. Pearson, “Mathematical contributions to the theory of evolution–on a form of spurious correlation which may arise when indices are used in the measurement of organs,” Proc. Royal Society of London, vol. 60, pp. 489–498, 1897. [8] A. Ongaro, S. Migliorati, and G. S. Monti, “A new distribution on the simplex containing the Dirichlet family,” Proc. 3rd Compositional Data Analysis Workshop, 2008. [9] W. S. Rayens and C. Srinivasan, “Dependence properties of generalized Liouville distributions on the simplex,” Journal of the American Statistical Association, vol. 89, no. 428, pp. 1465– 1470, 1994. [10] Walter Rudin, Functional Analysis, McGraw-Hill, New York, 1991. [11] G. Ronning, “Maximum likelihood estimation of Dirichlet distributions,” Journal of Statistical Computation and Simulation, vol. 34, no. 4, pp. 215221, 1989. [12] T. Minka, “Estimating a Dirichlet distribution,” Tech. Rep., Microsoft Research, Cambridge, 2009. [13] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from incomplete data via the EM algorithm,” Journal of the Royal Statistical Society: Series B (Methodological), vol. 39, no. 1, pp. 1–38, 1977. [14] M. R. Gupta and Y. Chen, Theory and Use of the EM Method, Foundations and Trends in Signal Processing, Hanover, MA, 2010. [15] R. T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ, 1970. [16] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Cambridge, 2004. [17] K. B. Petersen and M. S. Pedersen, Matrix Cookbook, 2009, Available at matrixcookbook.com. [18] R. E. Madsen, D. Kauchak, and C. Elkan, “Modeling word burstiness using the Dirichlet distribution,” in Proc. Intl. Conf. Machine Learning, 2005. 9</p><p>5 0.5788638 <a title="55-lsi-5" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>6 0.52602422 <a title="55-lsi-6" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>7 0.4955408 <a title="55-lsi-7" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>8 0.46108568 <a title="55-lsi-8" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>9 0.45518759 <a title="55-lsi-9" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>10 0.43002117 <a title="55-lsi-10" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>11 0.415988 <a title="55-lsi-11" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>12 0.41380236 <a title="55-lsi-12" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>13 0.41318035 <a title="55-lsi-13" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>14 0.40784657 <a title="55-lsi-14" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>15 0.40080237 <a title="55-lsi-15" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>16 0.40064624 <a title="55-lsi-16" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>17 0.39932683 <a title="55-lsi-17" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>18 0.39347994 <a title="55-lsi-18" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>19 0.39298061 <a title="55-lsi-19" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>20 0.39235398 <a title="55-lsi-20" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.03), (17, 0.012), (27, 0.138), (30, 0.059), (35, 0.054), (45, 0.138), (50, 0.07), (52, 0.024), (60, 0.04), (77, 0.031), (78, 0.012), (88, 0.271), (90, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74938864 <a title="55-lda-1" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>2 0.64503479 <a title="55-lda-2" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>Author: Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua B. Tenenbaum</p><p>Abstract: We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning. 1</p><p>3 0.61752921 <a title="55-lda-3" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>4 0.6170488 <a title="55-lda-4" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>Author: Diederik P. Kingma, Yann L. Cun</p><p>Abstract: Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difﬁculty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with ﬁnite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.</p><p>5 0.61450374 <a title="55-lda-5" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>Author: Georg Langs, Yanmei Tie, Laura Rigolo, Alexandra Golby, Polina Golland</p><p>Abstract: Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difﬁcult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We ﬁrst embed each brain into a functional map that reﬂects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions. 1</p><p>6 0.61311942 <a title="55-lda-6" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>7 0.60795242 <a title="55-lda-7" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>8 0.60643059 <a title="55-lda-8" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>9 0.59948051 <a title="55-lda-9" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>10 0.59861022 <a title="55-lda-10" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>11 0.59527689 <a title="55-lda-11" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>12 0.59520227 <a title="55-lda-12" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>13 0.59427023 <a title="55-lda-13" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>14 0.59317559 <a title="55-lda-14" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>15 0.59286702 <a title="55-lda-15" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>16 0.58985317 <a title="55-lda-16" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>17 0.58838165 <a title="55-lda-17" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>18 0.58826941 <a title="55-lda-18" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>19 0.58805436 <a title="55-lda-19" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>20 0.58556938 <a title="55-lda-20" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
