<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-254" href="#">nips2010-254</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</h1>
<br/><p>Source: <a title="nips-2010-254-pdf" href="http://papers.nips.cc/paper/3966-stability-approach-to-regularization-selection-stars-for-high-dimensional-graphical-models.pdf">pdf</a></p><p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>Reference: <a title="nips-2010-254-reference" href="../nips2010_reference/nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. [sent-4, score-0.274]
</p><p>2 The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. [sent-5, score-0.469]
</p><p>3 Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i. [sent-7, score-0.319]
</p><p>4 with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. [sent-9, score-0.401]
</p><p>5 Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. [sent-10, score-0.15]
</p><p>6 Such problems require us to infer an undirected graph from i. [sent-15, score-0.366]
</p><p>7 Each node in this graph corresponds to a random variable and the existence of an edge between a pair of nodes represent their conditional independence relationship. [sent-19, score-0.366]
</p><p>8 Gaussian graphical models [4, 23, 5, 9] are by far the most popular approach for learning high dimensional undirected graph structures. [sent-20, score-0.503]
</p><p>9 Under the Gaussian assumption, the graph can be estimated using the sparsity pattern of the inverse covariance matrix. [sent-21, score-0.421]
</p><p>10 To handle this challenge, the graphical lasso or glasso [7, 24, 2] is rapidly becoming a popular method for estimating sparse undirected graphs. [sent-24, score-0.308]
</p><p>11 To use this method, however, the user must specify a regularization parameter λ that controls the sparsity of the graph. [sent-25, score-0.14]
</p><p>12 Other methods for estimating high dimensional graphs include [11, 14, 10]. [sent-27, score-0.255]
</p><p>13 They also require the user to specify a regularization parameter. [sent-28, score-0.103]
</p><p>14 The standard methods for choosing the regularization parameter are AIC [1], BIC [19] and cross validation [6]. [sent-29, score-0.14]
</p><p>15 The idea, as we develop it, is based on subsampling [15] and builds on the approach of Meinshausen and B¨ hlmann [12]. [sent-35, score-0.157]
</p><p>16 We draw many random subsamples and construct a u graph from each subsample (unlike K-fold cross-validation, these subsamples are overlapping). [sent-36, score-0.502]
</p><p>17 We choose the regularization parameter so that the obtained graph is sparse and there is not too much variability across subsamples. [sent-37, score-0.49]
</p><p>18 More precisely, we start with a large regularization which corresponds to an empty, and hence highly stable, graph. [sent-38, score-0.103]
</p><p>19 We gradually reduce the amount of regularization until there is a small but acceptable amount of variability of the graph across subsamples. [sent-39, score-0.445]
</p><p>20 For graph selection, Meinshausen and B¨ hlmann [12] also used a stability criterion; however, their u approach differs from StARS in its fundamental conception. [sent-47, score-0.467]
</p><p>21 They use subsampling to produce a new and more stable regularization path then select a regularization parameter from this newly created path, whereas we propose to use subsampling to directly select one regularization parameter from the original path. [sent-48, score-0.555]
</p><p>22 Our aim is to ensure that the selected graph is sparse, but inclusive, while they aim to control the familywise type I errors. [sent-49, score-0.295]
</p><p>23 As a consequence, their goal is contrary to ours: instead of selecting a larger graph that contains the true graph, they try to select a smaller graph that is contained in the true graph. [sent-50, score-0.671]
</p><p>24 As we will discuss in Section 3, in speciﬁc application domains like gene regulatory network analysis, our goal for graph selection is more natural. [sent-51, score-0.428]
</p><p>25 The undirected graph G = (V, E) associated with P has vertices V = {X(1), . [sent-56, score-0.366]
</p><p>26 In this paper, we also interchangeably use E to denote the adjacency matrix of the graph G. [sent-60, score-0.295]
</p><p>27 The edge corresponding to X(j) and X(k) is absent if X(j) and X(k) are conditionally independent given the other coordinates of X. [sent-61, score-0.118]
</p><p>28 The graph estimation problem is to infer E from i. [sent-62, score-0.321]
</p><p>29 Hence, to estimate the graph we only need to estimate the sparsity pattern of Ω. [sent-74, score-0.314]
</p><p>30 A popular approach is the graphical lasso or glasso [7, 24, 2]. [sent-76, score-0.172]
</p><p>31 With a positive regularization parameter λ, the glasso estimator Ω(λ) is obtained by minimizing the regularized negative log-likelihood { } Ω(λ) = arg min −ℓ(Ω) + λ||Ω||1 (1) Ω≻0  where ||Ω||1 =  ∑ j,k  |Ωjk | is the elementwise ℓ1 -norm of Ω. [sent-78, score-0.236]
</p><p>32 The estimated graph G(λ) =  (V, E(λ)) is then easily obtained from Ω(λ): for i ̸= j, an edge (i, j) ∈ E(λ) if and only if the corresponding entry in Ω(λ) is nonzero. [sent-79, score-0.417]
</p><p>33 The resulting regularization path Ω(λ) for all λs has been shown to have excellent theoretical properties [18, 16]. [sent-83, score-0.121]
</p><p>34 [16] show that, if the regularization parameter λ satisﬁes a certain rate, the corresponding estimator Ω(λ) could recover the true graph with high probability. [sent-85, score-0.492]
</p><p>35 They are not practical enough to guide the choice of the regularization parameter λ in ﬁnite-sample settings. [sent-87, score-0.121]
</p><p>36 Larger values of λ tend to yield sparser graphs and smaller values of λ yield denser graphs. [sent-89, score-0.171]
</p><p>37 In particular, Λ = 0 corresponds to the empty graph with no edges. [sent-91, score-0.32]
</p><p>38 Given a grid of regularization parameters Gn = {Λ1 , . [sent-92, score-0.127]
</p><p>39 , ΛK }, our goal of graph regularization parameter selection is to choose one Λ ∈ Gn , such that the true graph E is contained in E(Λ) with high probability. [sent-95, score-0.841]
</p><p>40 Speciﬁcally, it is acceptable that an edge presents but the two genes corresponding to this edge do not really interact with each other. [sent-99, score-0.208]
</p><p>41 There is also a tradeoff: we want to select a denser graph which contains the true graph with high probability. [sent-102, score-0.704]
</p><p>42 At the same time, we want the graph to be as sparse as possible so that important information will not be buried by massive false positives. [sent-103, score-0.395]
</p><p>43 In the following, we start with an overview of several state-of-the-art regularization parameter selection methods for graphs. [sent-105, score-0.173]
</p><p>44 1 Existing Methods The regularization parameter is often chosen using AIC or BIC. [sent-108, score-0.121]
</p><p>45 As we will see in our experiments, AIC and BIC tend to select overly dense graphs in high dimensions. [sent-115, score-0.252]
</p><p>46 For each Λ ∈ Gn , we estimate a graph on the K − 1 training sets and evaluate the negative log-likelihood on the retained validation set. [sent-119, score-0.338]
</p><p>47 Our experiments will conﬁrm this is true for graph estimation as well. [sent-123, score-0.349]
</p><p>48 When Λ is 0, the graph is empty and two datasets from P would both yield the same graph. [sent-126, score-0.339]
</p><p>49 As we increase Λ, the variability of the graph increases and hence the stability decreases. [sent-127, score-0.412]
</p><p>50 We increase Λ just until the point where the graph becomes variable as measured by the stability. [sent-128, score-0.295]
</p><p>51 For each Λ ∈ Gn , we construct a graph using the glasso for each subsample. [sent-143, score-0.392]
</p><p>52 This b b results in N estimated edge matrices E1 (Λ), . [sent-144, score-0.122]
</p><p>53 Let ψ Λ (·) denote the glasso algorithm with the regularization parameter Λ. [sent-149, score-0.218]
</p><p>54 For any Λ Λ subsample Sj let ψst (Sj ) = 1 if the algorithm puts an edge and ψst (Sj ) = 0 if the algorithm does b Λ b not put an edge between (s, t). [sent-150, score-0.193]
</p><p>55 N j=1 st 3  b b b b b b Now deﬁne the parameter ξst (Λ) = 2θst (Λ)(1 − θst (Λ)) and let ξst (Λ) = 2θst (Λ)(1 − θst (Λ)) be b its estimate. [sent-156, score-0.366]
</p><p>56 s  ϵ) (12) s < 1/2, the estimated total stability Db (Λ) still converges to its mean Db (Λ) uniformly over the whole grid Gn . [sent-163, score-0.17]
</p><p>57 We now discuss the implication of Theorem 1 to graph regularization selection problems. [sent-164, score-0.475]
</p><p>58 Due to the generality of StARS, we provide theoretical justiﬁcations for a whole family of graph estimation procedures satisfying certain conditions. [sent-165, score-0.339]
</p><p>59 We denote E b (Λ) as the estimated edge set using the regularization parameter Λ by applying ψ on a subsampled dataset with block size b. [sent-167, score-0.278]
</p><p>60 To establish graph selection result, we start with two technical assumptions: (A1) ∃Λo ∈ Gn , such that maxΛ≤Λo ∧Λ∈Gn Db (Λ) ≤ β/2 for large enough n. [sent-168, score-0.347]
</p><p>61 (A2) requires that all estimated graphs using regularization parameters Λ ≥ Λo contain the true graph with high probability. [sent-172, score-0.647]
</p><p>62 Both assumptions are mild and should be satisﬁed by most graph estimation algorithm with reasonable behaviors. [sent-173, score-0.345]
</p><p>63 The next theorem provides the graph selection performance of StARS: Theorem 2. [sent-179, score-0.347]
</p><p>64 (Partial Sparsistency): Let ψ to be a graph estimation algorithm. [sent-180, score-0.321]
</p><p>65 Let Λs ∈ Gn be the selected regularization parameter using the StARS procedure with a constant cutting point β. [sent-182, score-0.121]
</p><p>66 We ﬁrst 5  quantitatively evaluate these methods on two types of synthetic datasets, where the true graphs are known. [sent-196, score-0.223]
</p><p>67 We then illustrate StARS on a microarray dataset that records the gene expression levels from immortalized B cells of human subjects. [sent-197, score-0.175]
</p><p>68 On all high dimensional synthetic datasets, StARS signiﬁcantly outperforms its competitors. [sent-198, score-0.118]
</p><p>69 On the microarray dataset, StARS obtains a remarkably simple graph while all competing methods select what appear to be overly dense graphs. [sent-199, score-0.463]
</p><p>70 1  Synthetic Data  To quantitatively evaluate the graph estimation performance, we adapt the criteria including precision, recall, and F1 -score from the information retrieval literature. [sent-201, score-0.34]
</p><p>71 Let G = (V, E) be a pdimensional graph and let G = (V, E) be an estimated graph. [sent-202, score-0.346]
</p><p>72 We deﬁne precision = |E ∩ E|/|E|, recall = |E ∩ E|/|E|, and F1 -score = 2 · precision · recall/(precision + recall). [sent-203, score-0.178]
</p><p>73 On the synthetic data where we know the true graphs, we also compare the previous methods with an oracle procedure which selects the optimal regularization parameter by minimizing the total number of different edges between the estimated and true graphs along the full regularization path. [sent-205, score-0.636]
</p><p>74 To make the comparison fair, once the regularization parameters are selected, √ estimate the oracle and StARS graphs only based on a subsampled we dataset with size b(n) = ⌊10 n⌋. [sent-208, score-0.359]
</p><p>75 In contrast, the K-CV, BIC, and AIC graphs are estimated using the full dataset. [sent-209, score-0.191]
</p><p>76 We generate data from sparse Gaussian graphs, neighborhood graphs and hub graphs, which mimic characteristics of real-wolrd biological networks. [sent-211, score-0.411]
</p><p>77 Table 1: Quantitative comparison of different methods on the datasets from the neighborhood and hub graphs. [sent-240, score-0.232]
</p><p>78 In high dimensional settings, however, StARS clearly outperforms all 6  the competing methods for both neighborhood and hub graphs. [sent-362, score-0.319]
</p><p>79 At ﬁrst sight, it might be surprising that for data from low-dimensional neighborhood graphs, BIC and AIC even outperform the oracle procedure! [sent-364, score-0.14]
</p><p>80 This is due to the fact that both BIC and AIC graphs are estimated using all the n = 800 data points, while the oracle graph is estimated using √ only the subsampled dataset with size b(n) = ⌊10 · n⌋ = 282. [sent-365, score-0.653]
</p><p>81 The estimated graphs for different methods in the setting n = 400, p = 100 are provided in Figures 1 and 2, from which we see that the StARS graph is almost as good as the oracle, while the K-CV, BIC, and AIC graphs are overly too dense. [sent-368, score-0.657]
</p><p>82 (a) True graph  (b) Oracle graph  (c) StARS graph  (d) K-CV graph  (e) BIC graph  (f) AIC graph  Figure 1: Comparison of different methods on the data from the neighborhood graphs (n = 400, p = 100). [sent-369, score-1.969]
</p><p>83 2  Microarray Data  We apply StARS to a dataset based on Affymetrix GeneChip microarrays for the gene expression levels from immortalized B cells of human subjects. [sent-371, score-0.13]
</p><p>84 Using a sub-pathway subset of 324 correlated genes, we study the estimated graphs obtained from each method under investigation. [sent-374, score-0.191]
</p><p>85 The StARS and BIC graphs are provided in Figure 3. [sent-375, score-0.14]
</p><p>86 We see that the StARS graph is remarkably simple and informative, exhibiting some cliques and hub genes. [sent-376, score-0.449]
</p><p>87 In contrast, the BIC graph is very dense and possible useful association information is buried in the large number of estimated edges. [sent-377, score-0.411]
</p><p>88 The selected graphs using AIC and K-CV are even more dense than the BIC graph and will be reported elsewhere. [sent-378, score-0.461]
</p><p>89 A full treatment of the biological implication of these two graphs validated by enrichment analysis will be provided in the full version of this paper. [sent-379, score-0.191]
</p><p>90 Casting the problem in the context of a regularized optimization has led to some success, but the choice of the regularization parameter is critical. [sent-381, score-0.121]
</p><p>91 We present a new method, StARS, for choosing this parameter in high dimensional inference for undirected graphs. [sent-382, score-0.171]
</p><p>92 (a) StARS graph  (b) BIC graph  Figure 3: Microarray data example. [sent-384, score-0.59]
</p><p>93 The StARS graph is more informative graph than the BIC graph. [sent-385, score-0.59]
</p><p>94 For graphical models, we choose the regularization parameter directly based on the edge stability. [sent-387, score-0.267]
</p><p>95 However, even without these conditions, StARS has a simple interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. [sent-389, score-0.469]
</p><p>96 The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. [sent-427, score-0.179]
</p><p>97 High dimensional graphs and variable selection u with the Lasso. [sent-430, score-0.244]
</p><p>98 Coexpression network based on natural variation in human gene expression reveals gene interactions and functions. [sent-439, score-0.148]
</p><p>99 Model selection in Gaussian graphical models: High-dimensional consistency of ℓ1 -regularized MLE. [sent-450, score-0.107]
</p><p>100 Model selection and estimation in the Gaussian graphical model. [sent-479, score-0.133]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stars', 0.59), ('st', 0.348), ('bic', 0.303), ('graph', 0.295), ('aic', 0.221), ('gn', 0.2), ('hub', 0.154), ('db', 0.149), ('graphs', 0.14), ('regularization', 0.103), ('glasso', 0.097), ('stability', 0.095), ('meinshausen', 0.083), ('oracle', 0.081), ('subsampling', 0.08), ('subsamples', 0.078), ('hlmann', 0.077), ('undirected', 0.071), ('edge', 0.071), ('precision', 0.066), ('microarray', 0.062), ('neighborhood', 0.059), ('gene', 0.056), ('graphical', 0.055), ('selection', 0.052), ('dimensional', 0.052), ('estimated', 0.051), ('subsample', 0.051), ('edges', 0.048), ('recall', 0.046), ('larry', 0.041), ('buried', 0.039), ('immortalized', 0.039), ('nicolai', 0.039), ('politis', 0.039), ('replicable', 0.039), ('underselect', 0.039), ('covariance', 0.038), ('sj', 0.038), ('synthetic', 0.036), ('subsampled', 0.035), ('kathryn', 0.034), ('estimating', 0.033), ('sparse', 0.032), ('instability', 0.031), ('denser', 0.031), ('overly', 0.031), ('high', 0.03), ('false', 0.029), ('clustering', 0.028), ('true', 0.028), ('wasserman', 0.026), ('estimation', 0.026), ('biological', 0.026), ('dense', 0.026), ('acceptable', 0.025), ('cv', 0.025), ('implication', 0.025), ('regulatory', 0.025), ('han', 0.025), ('empty', 0.025), ('select', 0.025), ('ji', 0.024), ('mild', 0.024), ('robert', 0.024), ('competing', 0.024), ('conditionally', 0.024), ('peter', 0.024), ('grid', 0.024), ('retained', 0.024), ('absent', 0.023), ('variability', 0.022), ('positives', 0.022), ('really', 0.022), ('justi', 0.021), ('criterion', 0.021), ('methodological', 0.021), ('choose', 0.02), ('lasso', 0.02), ('jk', 0.02), ('ki', 0.02), ('interpretation', 0.02), ('datasets', 0.019), ('validation', 0.019), ('ravikumar', 0.019), ('regression', 0.019), ('quantitatively', 0.019), ('ik', 0.019), ('sparsity', 0.019), ('partitioned', 0.019), ('genes', 0.019), ('parameter', 0.018), ('expression', 0.018), ('interactions', 0.018), ('inverse', 0.018), ('theoretical', 0.018), ('tradeoff', 0.018), ('estimator', 0.018), ('joe', 0.017), ('microarrays', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="254-tfidf-1" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>2 0.24460511 <a title="254-tfidf-2" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>3 0.14085256 <a title="254-tfidf-3" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>4 0.12028517 <a title="254-tfidf-4" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>Author: Emile Richard, Nicolas Baskiotis, Theodoros Evgeniou, Nicolas Vayatis</p><p>Abstract: We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices. We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We show experiments with both simulated and real data which reveal the interest of our methodology. 1</p><p>5 0.10800975 <a title="254-tfidf-5" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>Author: Han Liu, Xi Chen, Larry Wasserman, John D. Lafferty</p><p>Abstract: Undirected graphical models encode in a graph G the dependency structure of a random vector Y . In many applications, it is of interest to model Y given another random vector X as input. We refer to the problem of estimating the graph G(x) of Y conditioned on X = x as “graph-valued regression”. In this paper, we propose a semiparametric method for estimating G(x) that builds a tree on the X space just as in CART (classiﬁcation and regression trees), but at each leaf of the tree estimates a graph. We call the method “Graph-optimized CART”, or GoCART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data. 1</p><p>6 0.10297649 <a title="254-tfidf-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.10225606 <a title="254-tfidf-7" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>8 0.095662802 <a title="254-tfidf-8" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>9 0.089583158 <a title="254-tfidf-9" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>10 0.087522097 <a title="254-tfidf-10" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>11 0.083942905 <a title="254-tfidf-11" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>12 0.082273699 <a title="254-tfidf-12" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>13 0.080197856 <a title="254-tfidf-13" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>14 0.077820987 <a title="254-tfidf-14" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>15 0.077241451 <a title="254-tfidf-15" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>16 0.076107353 <a title="254-tfidf-16" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>17 0.07494764 <a title="254-tfidf-17" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>18 0.068959273 <a title="254-tfidf-18" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>19 0.067613848 <a title="254-tfidf-19" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>20 0.065525658 <a title="254-tfidf-20" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, 0.024), (2, 0.052), (3, 0.126), (4, -0.042), (5, -0.135), (6, -0.012), (7, -0.007), (8, 0.002), (9, 0.031), (10, -0.17), (11, 0.008), (12, -0.039), (13, 0.064), (14, 0.056), (15, -0.082), (16, 0.052), (17, -0.115), (18, -0.072), (19, 0.12), (20, -0.088), (21, 0.074), (22, -0.169), (23, 0.193), (24, 0.057), (25, 0.065), (26, 0.041), (27, 0.093), (28, 0.062), (29, 0.183), (30, 0.079), (31, -0.014), (32, -0.085), (33, 0.034), (34, -0.014), (35, 0.165), (36, 0.032), (37, -0.1), (38, 0.027), (39, -0.033), (40, 0.096), (41, -0.083), (42, 0.107), (43, 0.07), (44, 0.031), (45, -0.019), (46, -0.049), (47, 0.056), (48, 0.012), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96683967 <a title="254-lsi-1" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>2 0.75258029 <a title="254-lsi-2" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>Author: Rina Foygel, Mathias Drton</p><p>Abstract: Gaussian graphical models with sparsity in the inverse covariance matrix are of signiﬁcant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n. 1</p><p>3 0.67589635 <a title="254-lsi-3" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>4 0.59598094 <a title="254-lsi-4" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>Author: Han Liu, Xi Chen, Larry Wasserman, John D. Lafferty</p><p>Abstract: Undirected graphical models encode in a graph G the dependency structure of a random vector Y . In many applications, it is of interest to model Y given another random vector X as input. We refer to the problem of estimating the graph G(x) of Y conditioned on X = x as “graph-valued regression”. In this paper, we propose a semiparametric method for estimating G(x) that builds a tree on the X space just as in CART (classiﬁcation and regression trees), but at each leaf of the tree estimates a graph. We call the method “Graph-optimized CART”, or GoCART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data. 1</p><p>5 0.58106148 <a title="254-lsi-5" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>6 0.55560011 <a title="254-lsi-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.53156477 <a title="254-lsi-7" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>8 0.53017664 <a title="254-lsi-8" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>9 0.52223152 <a title="254-lsi-9" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>10 0.50068545 <a title="254-lsi-10" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>11 0.48209643 <a title="254-lsi-11" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>12 0.43230549 <a title="254-lsi-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.42833418 <a title="254-lsi-13" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>14 0.42786935 <a title="254-lsi-14" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>15 0.41163477 <a title="254-lsi-15" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>16 0.38533503 <a title="254-lsi-16" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>17 0.37730011 <a title="254-lsi-17" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>18 0.37715277 <a title="254-lsi-18" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>19 0.37395483 <a title="254-lsi-19" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>20 0.36443591 <a title="254-lsi-20" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.046), (17, 0.013), (27, 0.076), (30, 0.052), (35, 0.043), (45, 0.221), (50, 0.057), (52, 0.031), (59, 0.235), (60, 0.033), (77, 0.059), (90, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88158506 <a title="254-lda-1" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>Author: Alan Fern, Prasad Tadepalli</p><p>Abstract: We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We ﬁrst introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in ﬁnite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant’s action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution. 1</p><p>2 0.85538137 <a title="254-lda-2" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>same-paper 3 0.83198124 <a title="254-lda-3" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>4 0.79847217 <a title="254-lda-4" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>5 0.7882061 <a title="254-lda-5" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>6 0.75904083 <a title="254-lda-6" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>7 0.73785824 <a title="254-lda-7" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>8 0.73645467 <a title="254-lda-8" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>9 0.7325294 <a title="254-lda-9" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>10 0.73165029 <a title="254-lda-10" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>11 0.73158699 <a title="254-lda-11" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>12 0.73134869 <a title="254-lda-12" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>13 0.73103291 <a title="254-lda-13" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>14 0.73091233 <a title="254-lda-14" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>15 0.7298491 <a title="254-lda-15" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>16 0.72929573 <a title="254-lda-16" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>17 0.72813296 <a title="254-lda-17" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>18 0.72745645 <a title="254-lda-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.72730565 <a title="254-lda-19" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>20 0.72683793 <a title="254-lda-20" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
