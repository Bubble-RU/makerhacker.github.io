<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 nips-2010-Random Projections for $k$-means Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-221" href="#">nips2010-221</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>221 nips-2010-Random Projections for $k$-means Clustering</h1>
<br/><p>Source: <a title="nips-2010-221-pdf" href="http://papers.nips.cc/paper/3901-random-projections-for-k-means-clustering.pdf">pdf</a></p><p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>Reference: <a title="nips-2010-221-reference" href="../nips2010_reference/nips-2010-Random_Projections_for_%24k%24-means_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. [sent-3, score-0.119]
</p><p>2 A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results. [sent-4, score-0.114]
</p><p>3 1 Introduction The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last ﬁfty years [20]. [sent-5, score-0.18]
</p><p>4 This paper focuses on the application of the random projection method (see Section 2. [sent-7, score-0.06]
</p><p>5 3) to the k-means clustering problem (see Deﬁnition ˜ 1). [sent-8, score-0.146]
</p><p>6 Formally, assuming as input a set of n points in d dimensions, our goal is to randomly project the points into d ˜ ≪ d, and then apply a k-means clustering algorithm (see Deﬁnition 2) on the projected points. [sent-9, score-0.251]
</p><p>7 Of dimensions, with d course, one should be able to compute the projection fast without distorting signiﬁcantly the “clusters” of the original point set. [sent-10, score-0.072]
</p><p>8 Our algorithm (see Algorithm 1) satisﬁes both conditions by computing the embedding in time linear in the size of the input and by distorting the “clusters” of the dataset by a factor of at most 2 + ε, for some ε ∈ (0, 1/3) (see Theorem 1). [sent-11, score-0.217]
</p><p>9 We believe that the high dimensionality of modern data will render our algorithm useful and attractive in many practical applications [9]. [sent-12, score-0.086]
</p><p>10 Let A be an n × d matrix containing n d-dimensional points (A(i) denotes the i-th point of the set), and let k be the number of clusters (see also Section 2. [sent-14, score-0.174]
</p><p>11 an embedding with image A ∈ Rn×c containing (rescaled) columns from A, such that with constant probability the clustering structure is preserved within a factor of 2 + ε. [sent-22, score-0.358]
</p><p>12 In the RP methods the construction is done with random sign matrices and the mailman algorithm (see Sections 2. [sent-26, score-0.341]
</p><p>13 2 Preliminaries We start by formally deﬁning the k-means clustering problem using matrix notation. [sent-32, score-0.245]
</p><p>14 Later in this section, we precisely describe the approximability framework adopted in the k-means clustering literature and ﬁx the notation. [sent-33, score-0.186]
</p><p>15 [T HE K - MEANS CLUSTERING PROBLEM ] Given a set of n points in d dimensions (rows in an n × d matrix A) and a positive integer k denoting the number of clusters, ﬁnd the n × k indicator matrix Xopt such that Xopt = arg min A − XX ⊤ A X∈X  2 F  . [sent-35, score-0.394]
</p><p>16 (1) 2  Here X denotes the set of all n × k indicator matrices X. [sent-36, score-0.077]
</p><p>17 An n × k indicator matrix has exactly one non-zero element per row, which denotes cluster membership. [sent-38, score-0.172]
</p><p>18 , k, the i-th point belongs to the j-th cluster if √ and only if Xij = 1/ zj , where zj denotes the number of points in the corresponding cluster. [sent-45, score-0.079]
</p><p>19 1 Approximation Algorithms for k-means clustering Finding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation algorithms for k-means clustering. [sent-48, score-0.146]
</p><p>20 [ K - MEANS APPROXIMATION ALGORITHM ] An algorithm is a “γ-approximation” for the k-means clustering problem (γ ≥ 1) if it takes inputs A and k, and returns an indicator matrix Xγ that satisﬁes with probability at least 1 − δγ , ⊤ A − Xγ Xγ A  2 F  ≤ γ min A − XX ⊤ A X∈X  2 F  . [sent-51, score-0.389]
</p><p>21 For our discussion, we ﬁx the γ-approximation algorithm to be the one presented in [14], which guarantees γ = 1 + ε′ ′ O(1) for any ε′ ∈ (0, 1] with running time O(2(k/ε ) dn). [sent-53, score-0.094]
</p><p>22 2 Notation Given an n × d matrix A and an integer k with k < min{n, d}, let Uk ∈ Rn×k (resp. [sent-55, score-0.125]
</p><p>23 Vk ∈ Rd×k ) be the matrix of the top k left (resp. [sent-56, score-0.099]
</p><p>24 right) singular vectors of A, and let Σk ∈ Rk×k be a diagonal matrix containing the top 2  k singular values of A in non-increasing order. [sent-57, score-0.238]
</p><p>25 A F and A 2 denote the Frobenius and the spectral norm of a matrix A, respectively. [sent-65, score-0.099]
</p><p>26 the unique d × n matrix satisfying A = AA† A, A† AA† = A† , (AA† )⊤ = AA† , and (A† A)⊤ = A† A. [sent-68, score-0.099]
</p><p>27 A useful property of matrix norms is that for any two matrices C and T of appropriate dimensions, CT F ≤ C F T 2 ; this is a stronger version of the standard submultiplicavity property. [sent-70, score-0.126]
</p><p>28 We call P a projector matrix if it is square and P 2 = P . [sent-71, score-0.151]
</p><p>29 Subsequent research simpliﬁed the proof of the above result by showing that such a projection can be generated using a d × t random√ Gaussian matrix R, i. [sent-82, score-0.139]
</p><p>30 (3)  ˜ Notice that such an embedding A = AR preserves the metric structure of the point-set, so it also preserves, within a factor of 1 + ε, the optimal value of the k-means objective function of A. [sent-89, score-0.185]
</p><p>31 Achlioptas proved that even a (rescaled) random sign matrix sufﬁces in order to get the same guarantees as above [1], an approach that we adopt here (see step two in Algorithm 1). [sent-90, score-0.192]
</p><p>32 3 A random-projection-type k-means algorithm Algorithm 1 takes as inputs the matrix A ∈ Rn×d , the number of clusters k, an error parameter ε ∈ (0, 1/3), and some γ-approximation k-means algorithm. [sent-92, score-0.212]
</p><p>33 It returns an indicator matrix Xγ determining a k-partition of the rows of A. [sent-93, score-0.213]
</p><p>34 ˜ Input: n × d matrix A (n points, d features), number of clusters k, error parameter ε ∈ (0, 1/3), and γ-approximation k-means algorithm. [sent-94, score-0.137]
</p><p>35 Output: Indicator matrix Xγ determining a k-partition on the rows of A. [sent-95, score-0.163]
</p><p>36 Run the γ-approximation algorithm on A to obtain Xγ ; Return the indicator matrix Xγ ˜ ˜ Algorithm 1: A random projection algorithm for k-means clustering. [sent-110, score-0.277]
</p><p>37 1 Running time analysis Algorithm 1 reduces the dimensions of A by post-multiplying it with a random sign matrix R. [sent-112, score-0.245]
</p><p>38 If R is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and 3  compute the product AR in O(nd⌈ε−2 k/ log(d)⌉) time. [sent-114, score-0.436]
</p><p>39 Indeed, the mailman algorithm computes (after preprocessing ) a matrix-vector product of any d-dimensional vector (row of A) with an d × log(d) sign matrix in O(d) time. [sent-115, score-0.419]
</p><p>40 By partitioning the columns of our d × t matrix R into ⌈t/ log(d)⌉ blocks, the claim follows. [sent-116, score-0.099]
</p><p>41 The latter assumption is reasonable in our setting since the need for dimension reduction in k-means clustering arises usually in high-dimensional data (large d). [sent-118, score-0.233]
</p><p>42 Other choices of R would give the same approximation results; the time complexity to compute the embedding would √ be different though. [sent-119, score-0.096]
</p><p>43 A matrix where each entry is a random Gaussian variable with zero mean and variance 1/ t would imply an O(knd/ε2 ) time complexity (naive multiplication). [sent-120, score-0.119]
</p><p>44 In our experiments in Section 5 we experiment with the matrix R described in Algorithm 1 and employ MatLab’s matrix-matrix BLAS implementation to proceed in the third step of the algorithm. [sent-121, score-0.144]
</p><p>45 We also experimented with a novel MatLab/C implementation of the mailman algorithm but, in the general case, we were not able to outperform MatLab’s built-in routines (see section 5. [sent-122, score-0.272]
</p><p>46 Using, for example, the algorithm of [14] with γ = 1 + ε would result in an algorithm that preserves the clustering within a factor of O(1) 2 + ε, for any ε ∈ (0, 1/3), running in time O(nd⌈ε−2 k/ log(d)⌉ + 2(k/ε) kn/ε2 ). [sent-125, score-0.363]
</p><p>47 We thus employ the Lloyd algorithm for our experimental evaluation of our algorithm in Section 5. [sent-127, score-0.092]
</p><p>48 Note that, after using the proposed dimensionality reduction method, the cost of the Lloyd heuristic is only O(nk 2 /ε2 ) per iteration. [sent-128, score-0.13]
</p><p>49 Let the n × d matrix A and the positive integer k < min{n, d} be the inputs of the k-means clustering problem. [sent-135, score-0.312]
</p><p>50 Run Algorithm 1 with inputs A, k, ε, and the γ-approximation algorithm in order to construct an indicator matrix Xγ . [sent-137, score-0.224]
</p><p>51 Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Deﬁnition 1 and Lemma 5 in [19]. [sent-142, score-0.099]
</p><p>52 1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisﬁes Deﬁnition 1 and Lemma 5 in [19]. [sent-144, score-0.162]
</p><p>53 Assume that the matrix R is constructed by using Algorithm 1 with inputs A, k and ε. [sent-146, score-0.14]
</p><p>54 1  Reading the input d × log d sign matrix requires O(d log d) time. [sent-158, score-0.198]
</p><p>55 However, in our case we only consider multiplication with a random sign matrix, therefore we can avoid the preprocessing step by directly computing a random correspondence matrix as discussed in [15, Preprocessing Section]. [sent-159, score-0.27]
</p><p>56 Let Φ = Vk⊤ R; note that Φ is a k × t matrix and the SV D of Φ is Φ = UΦ ΣΦ VΦ , where UΦ and ΣΦ are k × k † matrices, and VΦ is a t × k matrix. [sent-166, score-0.099]
</p><p>57 By taking the SVD of (Vk⊤ R) and (Vk⊤ R)⊤ we get †  (Vk⊤ R) − (Vk⊤ R)⊤  ⊤ ⊤ VΦ Σ−1 UΦ − VΦ ΣΦ UΦ Φ  =  2  ⊤ VΦ (Σ−1 − ΣΦ )UΦ Φ  =  2  =  2  Σ−1 − ΣΦ Φ  2  ,  ⊤ since VΦ and UΦ can be dropped without changing any unitarily invariant norm. [sent-167, score-0.155]
</p><p>58 Assuming that, for all i ∈ [k], σi (Φ) and τi (Ψ) denote the i-th largest singular value of Φ and the i-th diagonal element of Ψ, respectively, it is  τi (Ψ) =  1 − σi (Φ)σk+1−i (Φ) . [sent-169, score-0.069]
</p><p>59 Under the same assumptions as in Lemma 2 and for any n × d matrix C w. [sent-173, score-0.099]
</p><p>60 Then, setting Z = CR 2 , using F the third statement of Lemma 2, the fact that k ≥ 1, and Chebyshev’s inequality we get P |Z − E [Z] | ≥ ε C  2 F  ≤  Var [Z] ε2 C  ≤  4 F  4 F 4 C F  2 C tε2  2 ≤ 0. [sent-179, score-0.115]
</p><p>61 97, †  Ak = (AR)(Vk⊤ R) Vk⊤ + E, where E is an n × d matrix with E  F  ≤ 4ε A − Ak  (7)  F. [sent-187, score-0.099]
</p><p>62 Then, setting A = Ak + Aρ−k , and using the triangle inequality we get E  F  ≤  †  ⊤ Ak − Ak R(Vk⊤ R) Vk  F  †  ⊤ Aρ−k R(Vk R) Vk⊤  +  F  . [sent-190, score-0.1]
</p><p>63 A well-known property connects the SVD of a matrix and k-means clustering. [sent-213, score-0.099]
</p><p>64 Recall Deﬁnition 1, and ⊤ notice that Xopt Xopt A is a matrix of rank at most k. [sent-214, score-0.147]
</p><p>65 Since I − Xγ Xγ is a projector matrix, it can be dropped without ˜ ˜ increasing a unitarily invariant norm. [sent-223, score-0.177]
</p><p>66 (12) we used Lemma 5, the triangle inequality, and the fact that I − Xγ Xγ is a projector matrix and can be dropped without increasing a unitarily invariant norm. [sent-229, score-0.306]
</p><p>67 To better understand this step, notice that Xγ gives a γ-approximation to the ˜ optimal k-means clustering of the matrix AR, and any other n × k indicator matrix (for example, the matrix Xopt ) satisﬁes 2 2 2 ⊤ ⊤ I − Xγ Xγ AR F ≤ γ min (I − XX ⊤ )AR F ≤ γ I − Xopt Xopt AR F . [sent-235, score-0.56]
</p><p>68 1  0  50  100 150 200 number of dimensions t  250  300  0. [sent-264, score-0.083]
</p><p>69 35  0  50  100 150 200 number of dimensions t  250  0  300  0  50  100 150 200 number of dimensions t  250  300  Figure 1: The results of our experiments after running Algorithm 1 with k = 40 on the face images collection. [sent-265, score-0.319]
</p><p>70 5 Experiments This section describes an empirical evaluation of Algorithm 1 on a face images collection. [sent-274, score-0.093]
</p><p>71 We implemented our algorithm in MatLab and compared it against other prominent dimensionality reduction techniques such as the Local Linear Embedding (LLE) algorithm and the Laplacian scores for feature selection. [sent-275, score-0.232]
</p><p>72 Our empirical ﬁndings are very promising indicating that our algorithm and implementation could be very useful in real applications involving clustering of large-scale data. [sent-278, score-0.201]
</p><p>73 1 An application of Algorithm 1 on a face images collection We experiment with a face images collection. [sent-280, score-0.186]
</p><p>74 This collection contains 400 face images of dimensions 64 × 64 corresponding to 40 different people. [sent-282, score-0.176]
</p><p>75 These images form 40 groups each one containing exactly 10 different images of the same person. [sent-283, score-0.109]
</p><p>76 After vectorizing each 2-D image and putting it as a row vector in an appropriate matrix, one can construct a 400 × 4096 image-by-pixel matrix A. [sent-284, score-0.099]
</p><p>77 In this matrix, objects are the face images of the ORL collection while features are the pixel values of the images. [sent-285, score-0.093]
</p><p>78 To apply the Lloyd’s heuristic on A, we employ MatLab’s function kmeans with the parameter determining the maximum number of repetitions setting to 30. [sent-286, score-0.1]
</p><p>79 whenever we call kmeans with inputs a matrix A ∈ R400×d , with d ≥ 1, and the integer k = 40, ˜ respectively. [sent-289, score-0.198]
</p><p>80 , 391-th rows of A, corresponds to picking images from the forty different groups of the available collection, since the images of every group are stored sequentially in A. [sent-293, score-0.128]
</p><p>81 We evaluate the clustering outcome from two different perspectives. [sent-294, score-0.146]
</p><p>82 First, we measure and report the objective function F of the k-means clustering problem. [sent-295, score-0.146]
</p><p>83 Second, we report the mis-classiﬁcation accuracy of the clustering result. [sent-299, score-0.146]
</p><p>84 9, for example, implies that 90% of the objects were assigned to the correct cluster after the application of the clustering algorithm. [sent-301, score-0.169]
</p><p>85 In the sequel, we ﬁrst perform experiments by running Algorithm 1 with everything ﬁxed but t, which denotes the dimensionality of the projected data. [sent-302, score-0.147]
</p><p>86 Then, for four representative values of t, we compare Algorithm 1 with three other dimensionality reduction methods as well with the approach of running the Lloyd’s heuristic on the original high dimensional data. [sent-303, score-0.19]
</p><p>87 First, the normalized objective function F is a ˜ is large in the ﬁrst few choices piece-wise non-increasing function of the number of dimensions t. [sent-310, score-0.083]
</p><p>88 ˜ of t; then, increasing the number of dimensions t of the projected data decreases F by a smaller value. [sent-352, score-0.118]
</p><p>89 Finally, we report the running time T of the algorithm which includes only the clustering step. [sent-358, score-0.24]
</p><p>90 Compare, for example, the one second running time that is needed to solve the k-means problem when t = 275 against the 10 seconds that are necessary to solve the problem on the high dimensional data. [sent-362, score-0.095]
</p><p>91 To our beneﬁt, in this case, the multiplication AR takes only 0. [sent-363, score-0.062]
</p><p>92 We now compare our algorithm against other dimensionality reduction techniques. [sent-366, score-0.146]
</p><p>93 In terms of computational complexity, for example t = 50, the time (in seconds) needed for all ﬁve methods (only the dimension reduction step) are TSV D = 5. [sent-369, score-0.087]
</p><p>94 2 A note on the mailman algorithm for matrix-matrix and matrix-vector multiplication In this section, we compare three different implementations of the third step of Algorithm 1. [sent-376, score-0.313]
</p><p>95 1, the mailman algorithm is asymptotically faster than naively multiplying the two matrices A and R. [sent-378, score-0.278]
</p><p>96 In this section we want to understand whether this asymptotic behavior of the mailman algorithm is indeed achieved in a practical implementation. [sent-379, score-0.251]
</p><p>97 We observed that when A is a vector (n = 1), then the mailman algorithm is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15]. [sent-383, score-0.251]
</p><p>98 On the other hand, our best implementation of the mailman algorithm for matrix-matrix operations is inferior to both (MM1) and (MM2) for any 10 ≤ n ≤ 10, 000. [sent-385, score-0.272]
</p><p>99 Random projection in dimensionality reduction: applications to image and text data. [sent-407, score-0.092]
</p><p>100 A simple linear time (1+ε)-approximation algorithm for k-means clustering in any dimensions. [sent-468, score-0.18]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xopt', 0.612), ('vk', 0.531), ('mailman', 0.217), ('ak', 0.162), ('clustering', 0.146), ('ar', 0.131), ('lloyd', 0.111), ('lemma', 0.104), ('matrix', 0.099), ('svd', 0.097), ('embedding', 0.096), ('dimensions', 0.083), ('matlab', 0.067), ('cr', 0.064), ('multiplication', 0.062), ('running', 0.06), ('reduction', 0.06), ('boutsidis', 0.059), ('lle', 0.059), ('unitarily', 0.059), ('factor', 0.055), ('projector', 0.052), ('dimensionality', 0.052), ('singular', 0.051), ('rr', 0.05), ('indicator', 0.05), ('rp', 0.05), ('uk', 0.049), ('ik', 0.048), ('notice', 0.048), ('face', 0.048), ('drineas', 0.048), ('dropped', 0.047), ('aa', 0.047), ('images', 0.045), ('statement', 0.045), ('sign', 0.043), ('preserved', 0.042), ('inputs', 0.041), ('xx', 0.041), ('inequality', 0.04), ('symposium', 0.04), ('projection', 0.04), ('approximability', 0.04), ('ccf', 0.04), ('rpi', 0.04), ('submultiplicativity', 0.04), ('rows', 0.038), ('clusters', 0.038), ('projections', 0.038), ('rn', 0.036), ('projected', 0.035), ('petros', 0.035), ('christos', 0.035), ('orl', 0.035), ('seconds', 0.035), ('algorithm', 0.034), ('preserves', 0.034), ('focs', 0.032), ('kmeans', 0.032), ('distorting', 0.032), ('extraction', 0.032), ('get', 0.03), ('laplacian', 0.03), ('var', 0.03), ('triangle', 0.03), ('savings', 0.028), ('guyon', 0.028), ('log', 0.028), ('dimension', 0.027), ('matrices', 0.027), ('hd', 0.027), ('preprocessing', 0.026), ('determining', 0.026), ('integer', 0.026), ('scores', 0.026), ('losing', 0.026), ('feature', 0.026), ('stoc', 0.024), ('employ', 0.024), ('cluster', 0.023), ('ls', 0.023), ('implementation', 0.021), ('rescaled', 0.021), ('foundations', 0.021), ('concludes', 0.021), ('neighbors', 0.02), ('random', 0.02), ('nition', 0.02), ('johnson', 0.019), ('zj', 0.019), ('iii', 0.019), ('theorem', 0.019), ('invariant', 0.019), ('containing', 0.019), ('min', 0.019), ('heuristic', 0.018), ('diagonal', 0.018), ('points', 0.018), ('union', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="221-tfidf-1" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>2 0.19877 <a title="221-tfidf-2" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>Author: Hongjing Lu, Tungyou Lin, Alan Lee, Luminita Vese, Alan L. Yuille</p><p>Abstract: It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More speciﬁcally, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, ﬁrst-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our ﬁrst experimental session we estimate the weights of the three terms for each functional form to maximize the ﬁt to human performance. We then measured human performance for motion tasks and found that we obtained better ﬁt for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better ﬁt to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best ﬁt models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction – ranging from 3% to 8% for ﬁve human subjects over ten experimental conditions – and give further support that the human visual system uses an L1-norm (Laplace) prior.</p><p>3 0.1614279 <a title="221-tfidf-3" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>4 0.15645291 <a title="221-tfidf-4" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>5 0.13658966 <a title="221-tfidf-5" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>Author: Alex Kulesza, Ben Taskar</p><p>Abstract: We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random ﬁelds and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems. 1</p><p>6 0.13277674 <a title="221-tfidf-6" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>7 0.12860404 <a title="221-tfidf-7" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>8 0.09728577 <a title="221-tfidf-8" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>9 0.088561803 <a title="221-tfidf-9" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>10 0.082639486 <a title="221-tfidf-10" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>11 0.08035551 <a title="221-tfidf-11" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>12 0.077278726 <a title="221-tfidf-12" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>13 0.076560922 <a title="221-tfidf-13" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>14 0.075771518 <a title="221-tfidf-14" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>15 0.070549905 <a title="221-tfidf-15" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>16 0.06277746 <a title="221-tfidf-16" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>17 0.061186008 <a title="221-tfidf-17" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>18 0.058888018 <a title="221-tfidf-18" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>19 0.057049733 <a title="221-tfidf-19" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>20 0.056997743 <a title="221-tfidf-20" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.168), (1, 0.014), (2, 0.063), (3, 0.028), (4, 0.013), (5, -0.125), (6, 0.046), (7, 0.006), (8, 0.055), (9, -0.017), (10, 0.126), (11, -0.161), (12, 0.081), (13, -0.031), (14, 0.273), (15, -0.026), (16, 0.092), (17, 0.034), (18, -0.014), (19, 0.047), (20, 0.096), (21, 0.017), (22, -0.008), (23, -0.025), (24, -0.09), (25, 0.051), (26, 0.036), (27, -0.139), (28, 0.035), (29, -0.125), (30, 0.08), (31, -0.088), (32, -0.053), (33, 0.033), (34, 0.116), (35, -0.039), (36, 0.129), (37, -0.081), (38, -0.031), (39, -0.072), (40, 0.035), (41, -0.064), (42, 0.117), (43, -0.022), (44, 0.015), (45, -0.093), (46, -0.005), (47, -0.065), (48, -0.08), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93414909 <a title="221-lsi-1" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>2 0.65337336 <a title="221-lsi-2" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>3 0.60082918 <a title="221-lsi-3" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>Author: Margareta Ackerman, Shai Ben-David, David Loker</p><p>Abstract: Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill deﬁned problem - given a data set, it is not clear what a “correct” clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some ﬁrst steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms. In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg’s famous impossibility result, while providing a simpler proof. 1</p><p>4 0.56099856 <a title="221-lsi-4" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>5 0.53438658 <a title="221-lsi-5" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>Author: Jacob Bien, Ya Xu, Michael W. Mahoney</p><p>Abstract: The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.</p><p>6 0.51305664 <a title="221-lsi-6" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>7 0.49591568 <a title="221-lsi-7" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>8 0.46784669 <a title="221-lsi-8" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>9 0.42780483 <a title="221-lsi-9" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>10 0.41356823 <a title="221-lsi-10" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>11 0.39355385 <a title="221-lsi-11" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>12 0.38585362 <a title="221-lsi-12" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>13 0.38561812 <a title="221-lsi-13" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>14 0.3793011 <a title="221-lsi-14" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>15 0.37596267 <a title="221-lsi-15" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>16 0.37285525 <a title="221-lsi-16" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>17 0.36931032 <a title="221-lsi-17" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>18 0.36799878 <a title="221-lsi-18" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>19 0.3342545 <a title="221-lsi-19" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>20 0.31738177 <a title="221-lsi-20" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.321), (17, 0.013), (27, 0.033), (30, 0.138), (35, 0.037), (45, 0.135), (50, 0.049), (52, 0.043), (60, 0.049), (77, 0.027), (78, 0.018), (90, 0.021), (95, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91758746 <a title="221-lda-1" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>same-paper 2 0.89225888 <a title="221-lda-2" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>Author: Christos Boutsidis, Anastasios Zouzias, Petros Drineas</p><p>Abstract: This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ Rn×d ) can be projected into t = Ω(k/ε2 ) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2 k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + √ The projection is done ε. √ by post-multiplying A with a d × t random matrix R having entries +1/ t or −1/ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p><p>3 0.88511908 <a title="221-lda-3" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>Author: Andrey Bernstein, Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the online binary classiﬁcation problem, where we are given m classiﬁers. At each stage, the classiﬁers map the input to the probability that the input belongs to the positive class. An online classiﬁcation meta-algorithm is an algorithm that combines the outputs of the classiﬁers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classiﬁers. In this paper, we use sensitivity and speciﬁcity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisﬁes the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classiﬁers that satisﬁes fprate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classiﬁers, we show that this algorithm takes a very simple form. To our best knowledge, this is the ﬁrst algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting. 1</p><p>4 0.88002306 <a title="221-lda-4" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>Author: Jacob Bien, Ya Xu, Michael W. Mahoney</p><p>Abstract: The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach, whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. We show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We also observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.</p><p>5 0.84445935 <a title="221-lda-5" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>Author: Pranjal Awasthi, Reza B. Zadeh</p><p>Abstract: Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efﬁcient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will ﬁnd the target clustering under the strongest property.</p><p>6 0.83714914 <a title="221-lda-6" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>7 0.83588582 <a title="221-lda-7" href="./nips-2010-Learning_Multiple_Tasks_using_Manifold_Regularization.html">146 nips-2010-Learning Multiple Tasks using Manifold Regularization</a></p>
<p>8 0.75230616 <a title="221-lda-8" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>9 0.73611653 <a title="221-lda-9" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>10 0.70477813 <a title="221-lda-10" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>11 0.69873047 <a title="221-lda-11" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>12 0.69798219 <a title="221-lda-12" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>13 0.68893629 <a title="221-lda-13" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>14 0.68736058 <a title="221-lda-14" href="./nips-2010-Guaranteed_Rank_Minimization_via_Singular_Value_Projection.html">110 nips-2010-Guaranteed Rank Minimization via Singular Value Projection</a></p>
<p>15 0.68706387 <a title="221-lda-15" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>16 0.6772067 <a title="221-lda-16" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>17 0.67162645 <a title="221-lda-17" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>18 0.66637629 <a title="221-lda-18" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>19 0.6660009 <a title="221-lda-19" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>20 0.65971732 <a title="221-lda-20" href="./nips-2010-Online_Learning_in_The_Manifold_of_Low-Rank_Matrices.html">195 nips-2010-Online Learning in The Manifold of Low-Rank Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
