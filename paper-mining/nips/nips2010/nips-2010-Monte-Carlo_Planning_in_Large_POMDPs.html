<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2010-Monte-Carlo Planning in Large POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-168" href="#">nips2010-168</a> knowledge-graph by maker-knowledge-mining</p><h1>168 nips-2010-Monte-Carlo Planning in Large POMDPs</h1>
<br/><p>Source: <a title="nips-2010-168-pdf" href="http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf">pdf</a></p><p>Author: David Silver, Joel Veness</p><p>Abstract: This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent’s belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, MonteCarlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan eﬀectively in signiﬁcantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 × 10 battleship and partially observable PacMan, with approximately 1018 and 1056 states respectively. Our MonteCarlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the ﬁrst general purpose planner to achieve high performance in such large and unfactored POMDPs. 1</p><p>Reference: <a title="nips-2010-168-reference" href="../nips2010_reference/nips-2010-Monte-Carlo_Planning_in_Large_POMDPs_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pomcp', 0.545), ('rollout', 0.311), ('pomdp', 0.291), ('uct', 0.251), ('ha', 0.237), ('partic', 0.189), ('rocksampl', 0.179), ('pocm', 0.177), ('ht', 0.16), ('prefer', 0.148), ('plan', 0.129), ('hist', 0.128), ('sim', 0.127), ('search', 0.125), ('ghost', 0.11), ('ship', 0.107), ('policy', 0.096), ('ninit', 0.095), ('believ', 0.093), ('hao', 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="168-tfidf-1" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>2 0.25581887 <a title="168-tfidf-2" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>3 0.17473559 <a title="168-tfidf-3" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>4 0.12392755 <a title="168-tfidf-4" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>5 0.12350427 <a title="168-tfidf-5" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>6 0.11476408 <a title="168-tfidf-6" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>7 0.10784107 <a title="168-tfidf-7" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>8 0.10511076 <a title="168-tfidf-8" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>9 0.093673207 <a title="168-tfidf-9" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>10 0.093182586 <a title="168-tfidf-10" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>11 0.086364403 <a title="168-tfidf-11" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>12 0.07428962 <a title="168-tfidf-12" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>13 0.074097164 <a title="168-tfidf-13" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>14 0.073126346 <a title="168-tfidf-14" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>15 0.072872385 <a title="168-tfidf-15" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>16 0.070523754 <a title="168-tfidf-16" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>17 0.069422953 <a title="168-tfidf-17" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>18 0.069230013 <a title="168-tfidf-18" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>19 0.067222826 <a title="168-tfidf-19" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>20 0.065196767 <a title="168-tfidf-20" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.163), (2, -0.013), (3, 0.018), (4, 0.025), (5, -0.043), (6, -0.017), (7, -0.051), (8, -0.007), (9, 0.06), (10, -0.014), (11, 0.066), (12, -0.023), (13, 0.056), (14, 0.099), (15, -0.049), (16, -0.068), (17, 0.021), (18, 0.015), (19, 0.009), (20, -0.034), (21, 0.045), (22, 0.018), (23, 0.016), (24, 0.067), (25, -0.021), (26, 0.008), (27, 0.027), (28, 0.108), (29, -0.008), (30, 0.067), (31, 0.097), (32, -0.061), (33, -0.003), (34, 0.064), (35, -0.031), (36, -0.016), (37, 0.033), (38, -0.117), (39, -0.013), (40, -0.05), (41, 0.102), (42, 0.038), (43, -0.047), (44, 0.218), (45, 0.026), (46, -0.054), (47, 0.073), (48, -0.013), (49, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90634894 <a title="168-lsi-1" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>2 0.74035883 <a title="168-lsi-2" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>3 0.69727385 <a title="168-lsi-3" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>4 0.62764823 <a title="168-lsi-4" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>5 0.55540371 <a title="168-lsi-5" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>6 0.55521101 <a title="168-lsi-6" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>7 0.55497754 <a title="168-lsi-7" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>8 0.5190413 <a title="168-lsi-8" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>9 0.51513481 <a title="168-lsi-9" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>10 0.47458628 <a title="168-lsi-10" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>11 0.43693259 <a title="168-lsi-11" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>12 0.4207378 <a title="168-lsi-12" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>13 0.37017977 <a title="168-lsi-13" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>14 0.36073968 <a title="168-lsi-14" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>15 0.35820001 <a title="168-lsi-15" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>16 0.35799438 <a title="168-lsi-16" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>17 0.3529996 <a title="168-lsi-17" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>18 0.34755239 <a title="168-lsi-18" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>19 0.34511822 <a title="168-lsi-19" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>20 0.33918691 <a title="168-lsi-20" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.04), (30, 0.025), (32, 0.088), (34, 0.068), (45, 0.609), (68, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89914709 <a title="168-lda-1" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>2 0.88492316 <a title="168-lda-2" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>3 0.85564816 <a title="168-lda-3" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>4 0.82283169 <a title="168-lda-4" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>5 0.808869 <a title="168-lda-5" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>6 0.77712929 <a title="168-lda-6" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>7 0.76832604 <a title="168-lda-7" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>8 0.75336844 <a title="168-lda-8" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>9 0.63909215 <a title="168-lda-9" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>10 0.62517315 <a title="168-lda-10" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>11 0.61152047 <a title="168-lda-11" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>12 0.60282993 <a title="168-lda-12" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>13 0.6010468 <a title="168-lda-13" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>14 0.57752317 <a title="168-lda-14" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>15 0.5701918 <a title="168-lda-15" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>16 0.56554043 <a title="168-lda-16" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>17 0.54550159 <a title="168-lda-17" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>18 0.5446859 <a title="168-lda-18" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>19 0.54055214 <a title="168-lda-19" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>20 0.53688854 <a title="168-lda-20" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<br/><br/><br/></body>
</html>
