<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-30" href="#">nips2010-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</h1>
<br/><p>Source: <a title="nips-2010-30-pdf" href="http://papers.nips.cc/paper/4110-an-inverse-power-method-for-nonlinear-eigenproblems-with-applications-in-1-spectral-clustering-and-sparse-pca.pdf">pdf</a></p><p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><p>Reference: <a title="nips-2010-30-reference" href="../nips2010_reference/nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. [sent-4, score-0.258]
</p><p>2 In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. [sent-5, score-0.148]
</p><p>3 We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. [sent-6, score-0.372]
</p><p>4 We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. [sent-7, score-0.52]
</p><p>5 Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. [sent-9, score-0.392]
</p><p>6 However, considering the eigenproblem from a variational point of view using Courant-Fischer-theory, the objective is a ratio of quadratic functions, which is quite restrictive from a modeling perspective. [sent-11, score-0.32]
</p><p>7 We show in this paper that using a ratio of p-homogeneous functions leads quite naturally to a nonlinear eigenvalue problem, associated to a certain nonlinear operator. [sent-12, score-0.486]
</p><p>8 Clearly, such a generalization is only interesting if certain properties of the standard problem are preserved and efﬁcient algorithms for the computation of nonlinear eigenvectors are available. [sent-13, score-0.263]
</p><p>9 In this paper we present an efﬁcient generalization of the inverse power method (IPM) to nonlinear eigenvalue problems and study the relation to the standard problem. [sent-14, score-0.456]
</p><p>10 In prior work [5] we proposed p-spectral clustering based on the graph p-Laplacian, a nonlinear operator on graphs which reduces to the standard graph Laplacian for p = 2. [sent-17, score-0.449]
</p><p>11 For p close to one, we obtained much better cuts than standard spectral clustering, at the cost of higher runtime. [sent-18, score-0.189]
</p><p>12 However, opposed to the suggested method in [19] our IPM is guaranteed to converge to an eigenvector of the 1-Laplacian. [sent-21, score-0.503]
</p><p>13 The motivation for sparse PCA is that the largest PCA component is difﬁcult to interpret as usually all components are nonzero. [sent-23, score-0.169]
</p><p>14 We show that also sparse PCA has a natural formulation as a nonlinear eigenvalue problem and can be efﬁciently solved with the IPM. [sent-26, score-0.351]
</p><p>15 2  Nonlinear Eigenproblems  The standard eigenproblem for a symmetric matric A ∈ Rn×n is of the form Af − λf = 0,  (1)  n  where f ∈ R and λ ∈ R. [sent-28, score-0.217]
</p><p>16 It is a well-known result from linear algebra that for symmetric matrices A, the eigenvectors of A can be characterized as critical points of the functional FStandard (f ) =  f, Af f  2 2  . [sent-29, score-0.272]
</p><p>17 This restriction however can be overcome using nonlinear eigenproblems. [sent-32, score-0.148]
</p><p>18 In this paper we consider functionals F of the form R(f ) , (3) F (f ) = S(f ) where with R+ = {x ∈ R | x ≥ 0} we assume R : Rn → R+ , S : Rn → R+ to be convex, Lipschitz continuous, even and positively p-homogeneous1 with p ≥ 1. [sent-33, score-0.12]
</p><p>19 The condition that R and S are p-homogeneous and even will imply for any eigenvector v that also αv for α ∈ R is an eigenvector. [sent-35, score-0.45]
</p><p>20 It is easy to see that the functional of the standard eigenvalue problem in Equation (2) is a special case of the general functional in (3). [sent-36, score-0.208]
</p><p>21 Then it holds for every critical point f ∗ of F , F (f ∗ ) = 0  ⇐⇒  R(f ∗ ) −  R(f ∗ ) · S(f ∗ )  S(f ∗ ) = 0 . [sent-38, score-0.109]
</p><p>22 If R and S are both quadratic, r and s are linear operators and one gets back the standard eigenproblem (1). [sent-40, score-0.225]
</p><p>23 Before we proceed to the general nondifferentiable case, we have to introduce some important concepts from nonsmooth analysis. [sent-41, score-0.113]
</p><p>24 A characterization of critical points of nonsmooth functionals is as follows. [sent-45, score-0.233]
</p><p>25 1 ([7]) A point f ∈ Rn is a critical point of F , if 0 ∈ ∂F . [sent-47, score-0.109]
</p><p>26 This generalizes the well-known fact that the gradient of a differentiable function vanishes at a critical point. [sent-48, score-0.133]
</p><p>27 We now show that the nonlinear eigenproblem (4) is a necessary condition for a critical point and in some cases even sufﬁcient. [sent-49, score-0.449]
</p><p>28 1 ([21]) Let R : Rn → R be a positively p-homogeneous and convex continuous function. [sent-52, score-0.104]
</p><p>29 1  A function G : Rn → R is positively homogeneous of degree p if G(γx) = γ p G(x) for all γ ≥ 0. [sent-54, score-0.092]
</p><p>30 2  The next theorem characterizes the relation between nonlinear eigenvectors and critical points of F . [sent-55, score-0.348]
</p><p>31 Then a necessary condition for f ∗ being a critical point of F is 0 ∈ ∂R(f ∗ ) − λ∗ ∂S(f ∗ ),  where  λ∗ =  R(f ∗ ) . [sent-58, score-0.109]
</p><p>32 Finally, the deﬁnition of the associated nonlinear operators in the nonsmooth case is a bit tricky as r and s can be set-valued. [sent-60, score-0.255]
</p><p>33 3  The inverse power method for nonlinear Eigenproblems  A standard technique to obtain the smallest eigenvalue of a positive semi-deﬁnite symmetric matrix A is the inverse power method [12]. [sent-62, score-0.655]
</p><p>34 Its main building block is the fact that the iterative scheme Af k+1 = f k  (6)  converges to the smallest eigenvector of A. [sent-63, score-0.478]
</p><p>35 Transforming (6) into the optimization problem f k+1 = arg min u  1 u, A u − u, f k 2  (7)  is the motivation for the general IPM. [sent-64, score-0.116]
</p><p>36 The direct generalization tries to solve 0 ∈ r(f k+1 ) − s(f k )  or equivalently  f k+1 = arg min R(u) − u, s(f k ) ,  (8)  u  where r(f ) ∈ ∂R(f ) and s(f ) ∈ ∂S(f ). [sent-65, score-0.113]
</p><p>37 Moreover, the introduction of λk in Algorithm 1 is necessary to guarantee descent whereas in Algorithm 2 it would just yield a rescaled solution of the problem in the inner loop (called inner problem in the following). [sent-69, score-0.35]
</p><p>38 2 is a necessary condition for a critical point of F and often also sufﬁcient. [sent-71, score-0.109]
</p><p>39 Note that we cannot guarantee convergence to the smallest eigenvector even though our experiments suggest that we often do so. [sent-74, score-0.548]
</p><p>40 However, as the method is fast one can afford to run it multiple times with different initializations and use the eigenvector with smallest eigenvalue. [sent-75, score-0.544]
</p><p>41 4:  The inner optimization problem is convex for both algorithms. [sent-77, score-0.16]
</p><p>42 In turns out that both for 1-spectral clustering and sparse PCA the inner problem can be solved very efﬁciently, for sparse PCA it has even a closed form solution. [sent-78, score-0.393]
</p><p>43 In [4] they propose an inverse power method specially tailored towards the continuous p-Laplacian for p > 1, which can be seen as a special case of Algorithm 2. [sent-82, score-0.17]
</p><p>44 In [15] a generalized power method has been proposed which will be discussed in Section 5. [sent-83, score-0.136]
</p><p>45 Finally, both methods can be easily adapted to compute the largest nonlinear eigenvalue, which however we have to omit due to space constraints. [sent-84, score-0.178]
</p><p>46 1 The sequences f k produced by Algorithms 1 and 2 converge to an eigenvector f ∗ with eigenvalue λ∗ ∈ 0, F (f 0 ) in the sense that it solves the nonlinear eigenproblem (5). [sent-89, score-0.904]
</p><p>47 If S is continuously differentiable at f ∗ , then F has a critical point at f ∗ . [sent-90, score-0.155]
</p><p>48 1, descent in F is not only guaranteed for the optimal solution of the inner problem, but for any vector u which has inner objective value 1 Φf k (u) < 0 = Φf k (f k ) for Alg. [sent-92, score-0.336]
</p><p>49 First, for the convergence of the IPM, it is sufﬁcient to use a vector u satisfying the above conditions instead of the optimal solution of the inner problem. [sent-96, score-0.179]
</p><p>50 In particular, in an early stage where one is far away from the limit, it makes no sense to invest much effort to solve the inner problem accurately. [sent-97, score-0.126]
</p><p>51 Second, if the inner problem is solved by a descent method, a good initialization for the inner problem at step k + 1 is given by f k in the case of Alg. [sent-98, score-0.327]
</p><p>52 4  Application 1: 1-spectral clustering and Cheeger cuts  Spectral clustering is a graph-based clustering method (see [20] for an overview) based on a relaxation of the NP-hard problem of ﬁnding the optimal balanced cut of an undirected graph. [sent-101, score-0.653]
</p><p>53 The spectral relaxation has as its solution the second eigenvector of the graph Laplacian and the ﬁnal partition is found by optimal thresholding. [sent-102, score-0.729]
</p><p>54 While usually spectral clustering is understood as relaxation of the so called ratio/normalized cut, it can be equally seen as relaxation of the ratio/normalized Cheeger cut, see [5]. [sent-103, score-0.336]
</p><p>55 In [5] we proposed p-spectral clustering, a generalization of spectral clustering based on the second eigenvector of the nonlinear graph p-Laplacian (the graph Laplacian is recovered for p = 2). [sent-106, score-1.064]
</p><p>56 While the inequality is quite loose for spectral clustering (p = 2), it becomes tight for p → 1. [sent-108, score-0.309]
</p><p>57 Indeed in [5] much better cuts than standard spectral clustering were obtained, at the expense of higher runtime. [sent-109, score-0.326]
</p><p>58 In [19] the idea was taken up and they considered directly the variational characterization of the ratio Cheeger cut, see also [8], hRCC = minf nonconstant  1 2  n i,j=1  wij |fi − fj |  f − median(f )1  1  = min f nonconstant  1 2  n i,j=1  median(f)=0  wij |fi − fj | f  . [sent-110, score-0.733]
</p><p>59 In this paper we consider the functional associated to the 1-Laplacian ∆1 , F1 (f ) =  1 2  n i,j=1  wij |fi − fj | f  1  =  f, ∆1 f , f 1  (10)  where n  wij uij | uij = −uji , uij ∈ sign(fi − fj ) and sign(x) =  (∆1 f )i = j=1  −1, [−1, 1], 1,  x < 0, x = 0, x > 0. [sent-114, score-0.543]
</p><p>60 and study its associated nonlinear eigenproblem 0 ∈ ∆1 f − λ sign(f ). [sent-115, score-0.34]
</p><p>61 1 Any non-constant eigenvector f ∗ of the 1-Laplacian has median zero. [sent-117, score-0.513]
</p><p>62 Moreover, let λ2 be the second eigenvalue of the 1-Laplacian, then if G is connected it holds λ2 = hRCC . [sent-118, score-0.114]
</p><p>63 For the computation of the second eigenvector we have to modify the IPM which is discussed in the next section. [sent-119, score-0.45]
</p><p>64 1  Modiﬁcation of the IPM for computing the second eigenvector of the 1-Laplacian  The direct minimization of (10) would be compatible with the IPM, but the global minimizer is the ﬁrst eigenvector which is constant. [sent-121, score-0.9]
</p><p>65 For computing the second eigenvector note that, unlike in the case p = 2, we cannot simply project on the space orthogonal to the constant eigenvector, since mutual orthogonality of the eigenvectors does not hold in the nonlinear case. [sent-122, score-0.689]
</p><p>66 Algorithm 3 is a modiﬁcation of Algorithm 1 which computes a nonconstant eigenvector of the 1k+1 k+1 k+1 Laplacian. [sent-123, score-0.596]
</p><p>67 This condition ensures that the inner objective is invariant under addition of a constant and thus not affected by the subtraction of the median. [sent-126, score-0.126]
</p><p>68 Opposite to [19] we can prove convergence to a nonconstant eigenvector of the 1-Laplacian. [sent-127, score-0.622]
</p><p>69 1 The sequence f k produced by Algorithm 3 converges to an eigenvector f ∗ of the 1Laplacian with eigenvalue λ∗ ∈ hRCC , F1 (f 0 ) . [sent-131, score-0.564]
</p><p>70 2  Quality guarantee for 1-spectral clustering  Even though we cannot guarantee that we obtain the optimal ratio Cheeger cut, we can guarantee that 1-spectral clustering always leads to a ratio Cheeger cut at least as good as the one found by ∗ ∗ standard spectral clustering. [sent-134, score-0.779]
</p><p>71 Let (Cf , Cf ) be the partition of V obtained by optimal thresholding of ∗ t t t f , where Cf = arg mint RCC(Cf , Cf ), and for t ∈ R, Cf = {i ∈ V | fi > t}. [sent-135, score-0.154]
</p><p>72 2 Let u denote the second eigenvector of the standard graph Laplacian, and f denote 1 ∗ ∗ the result of Algorithm 3 after initializing with the vector |C| 1C , where C = arg min{|Cu |, |Cu |}. [sent-145, score-0.588]
</p><p>73 3  Solution of the inner problem  The inner problem is convex, thus a solution can be computed by any standard method for solving convex nonsmooth programs, e. [sent-148, score-0.41]
</p><p>74 However, in this particular case we can exploit the structure of the problem and use the equivalent dual formulation of the inner problem. [sent-151, score-0.15]
</p><p>75 3 Let E ⊂ V × V denote the set of edges and A : RE → RV be deﬁned as (Aα)i = j | (i,j)∈E wij αij . [sent-153, score-0.109]
</p><p>76 The inner problem is equivalent to min{α∈RE |  α  ∞ ≤1,  αij =−αji }  Ψ(α) := Aα − F (f k )v k  The Lipschitz constant of the gradient of Ψ is upper bounded by 2 maxr  2 2  n s=1  . [sent-154, score-0.126]
</p><p>77 Moreover, it can be efﬁciently solved using FISTA ([2]), a two-step subgradient method with guaranteed convergence 1 rate O( k2 ) where k is the number of steps. [sent-157, score-0.116]
</p><p>78 FISTA provides a good solution in a few steps which guarantees descent in functional (9) and thus makes the modiﬁed IPM very fast. [sent-159, score-0.101]
</p><p>79 For k = 1, given a data matrix X ∈ Rn×p where each column has mean 0, in PCA one computes f ∗ = arg max f ∈Rp  f, X T Xf f  2 2  ,  (11)  where the maximizer f ∗ is the largest eigenvector of the covariance matrix Σ = X T X ∈ Rp×p . [sent-163, score-0.506]
</p><p>80 For instance, in the case of gene expression data one would like the principal components to consist only of a few signiﬁcant genes, making it easy to interpret by a human. [sent-166, score-0.126]
</p><p>81 The ﬁrst approaches performed simple thresholding of the principal components which was shown to be misleading [6]. [sent-171, score-0.169]
</p><p>82 Since then several methods have been proposed, mainly based on penalizing the L1 norm of the principal components, including SCoTLASS [14] and SPCA [22]. [sent-172, score-0.093]
</p><p>83 f, Σf Xf 2 f ∈Rp  f ∗ = arg min f ∈Rp  In order to enforce sparsity we use instead of the L2 -norm a convex combination of an L1 norm and L2 norm in the enumerator, which yields the functional F (f ) =  (1 − α) f 2 + α f Xf 2  1  ,  (12)  with sparsity controlling parameter α ∈ [0, 1]. [sent-183, score-0.24]
</p><p>84 The inner problem of the IPM becomes g k+1 = arg min (1 − α) f f  2  2 ≤1  +α f  1  − λk f, µk ,  µk =  where  Σf k f k , Σf k  . [sent-186, score-0.215]
</p><p>85 + i  As s is just a scaling factor, we can omit it and obtain the simple and efﬁcient scheme to compute sparse principal components shown in Algorithm 4. [sent-191, score-0.191]
</p><p>86 The subtle difference is that in our formulation the thresholding parameter of the inner problem depends on the current eigenvalue estimate whereas it is ﬁxed in [15]. [sent-193, score-0.307]
</p><p>87 1 [5] as well as standard spectral clustering with optimal 7  thresholding the second eigenvector of the graph Laplacian (p = 2). [sent-196, score-0.853]
</p><p>88 The following table shows the average ratio Cheeger cut (RCC) and error (classiﬁcation as in [5]) for 100 draws of a two-moons dataset with 2000 points. [sent-198, score-0.187]
</p><p>89 In the case of the IPM, we use the best result of 10 runs with random initializations and one run initialized with the second eigenvector of the unnormalized graph Laplacian. [sent-199, score-0.618]
</p><p>90 For [19] we initialize once with the second eigenvector of the normalized graph Laplacian as proposed in [19] and 10 times randomly. [sent-200, score-0.532]
</p><p>91 0200)  Figure 1: Left and middle: Second eigenvector of the 1-Laplacian and 2-Laplacian, respectively. [sent-223, score-0.45]
</p><p>92 Next we perform unnormalized 1-spectral clustering on the full USPS and MNIST-datasets (9298 resp. [sent-225, score-0.18]
</p><p>93 As clustering criterion we use the multicut version of RCut, given as K  RCut(C1 , . [sent-227, score-0.137]
</p><p>94 In each substep the eigenvector obtained on the subgraph is thresholded such that the multi-cut criterion is minimized. [sent-232, score-0.489]
</p><p>95 As in the previous experiment, we perform one run initialized with the thresholded second eigenvector of the unnormalized graph Laplacian in the case of the IPM and with the second eigenvector of the normalized graph Laplacian in the case of [19]. [sent-234, score-1.146]
</p><p>96 1686  Again the three nonlinear eigenvector methods clearly outperform standard spectral clustering. [sent-256, score-0.739]
</p><p>97 2 for bi-partitions one achieves a cut at least as good as the one of standard spectral clustering if one initializes with the thresholded 2nd eigenvector of the 2-Laplacian. [sent-260, score-0.909]
</p><p>98 Computing the ﬁrst eigenvalue of the p-Laplacian via the inverse power method. [sent-289, score-0.261]
</p><p>99 Loading and correlations in the interpretation of principal components. [sent-301, score-0.093]
</p><p>100 A modiﬁed principal component technique based on the LASSO. [sent-347, score-0.137]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eigenvector', 0.45), ('ipm', 0.362), ('rcc', 0.267), ('cheeger', 0.255), ('pca', 0.195), ('eigenproblem', 0.192), ('cf', 0.154), ('nonlinear', 0.148), ('hrcc', 0.146), ('nonconstant', 0.146), ('cut', 0.142), ('spectral', 0.141), ('clustering', 0.137), ('inner', 0.126), ('rcut', 0.121), ('eigenvalue', 0.114), ('wij', 0.109), ('critical', 0.109), ('principal', 0.093), ('eigenvectors', 0.091), ('cu', 0.085), ('power', 0.085), ('rn', 0.083), ('graph', 0.082), ('xf', 0.078), ('laplacian', 0.074), ('nonsmooth', 0.074), ('fik', 0.073), ('positively', 0.07), ('sparse', 0.065), ('eigenproblems', 0.064), ('median', 0.063), ('inverse', 0.062), ('fj', 0.061), ('arg', 0.056), ('fi', 0.055), ('fista', 0.055), ('uij', 0.052), ('functionals', 0.05), ('enumerator', 0.049), ('moghaddam', 0.049), ('initialization', 0.048), ('cuts', 0.048), ('functional', 0.047), ('sign', 0.046), ('ratio', 0.045), ('component', 0.044), ('guarantee', 0.044), ('thresholding', 0.043), ('initializations', 0.043), ('unnormalized', 0.043), ('saarland', 0.043), ('szlam', 0.043), ('lipschitz', 0.041), ('rp', 0.041), ('nondifferentiable', 0.039), ('omnipress', 0.039), ('thresholded', 0.039), ('subgradient', 0.037), ('euler', 0.037), ('aspremont', 0.037), ('maxi', 0.036), ('sparsity', 0.035), ('di', 0.035), ('convex', 0.034), ('min', 0.033), ('components', 0.033), ('operators', 0.033), ('vertex', 0.032), ('af', 0.032), ('quite', 0.031), ('repeat', 0.03), ('guaranteed', 0.03), ('adapted', 0.03), ('lemma', 0.029), ('quadratic', 0.029), ('usps', 0.029), ('relaxation', 0.029), ('smallest', 0.028), ('generalized', 0.028), ('solution', 0.027), ('descent', 0.027), ('motivation', 0.027), ('runtime', 0.027), ('convergence', 0.026), ('wants', 0.026), ('ci', 0.026), ('maximal', 0.025), ('tv', 0.025), ('symmetric', 0.025), ('bregman', 0.025), ('differentiable', 0.024), ('generalization', 0.024), ('formulation', 0.024), ('method', 0.023), ('variational', 0.023), ('homogeneous', 0.022), ('variation', 0.022), ('gi', 0.022), ('continuously', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="30-tfidf-1" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><p>2 0.16108847 <a title="30-tfidf-2" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>3 0.12999673 <a title="30-tfidf-3" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>4 0.11509532 <a title="30-tfidf-4" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>Author: Margareta Ackerman, Shai Ben-David, David Loker</p><p>Abstract: Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill deﬁned problem - given a data set, it is not clear what a “correct” clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some ﬁrst steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms. In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg’s famous impossibility result, while providing a simpler proof. 1</p><p>5 0.10543805 <a title="30-tfidf-5" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>6 0.10305879 <a title="30-tfidf-6" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>7 0.085791931 <a title="30-tfidf-7" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>8 0.078828737 <a title="30-tfidf-8" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>9 0.076361679 <a title="30-tfidf-9" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>10 0.074009337 <a title="30-tfidf-10" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>11 0.07180886 <a title="30-tfidf-11" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>12 0.07169544 <a title="30-tfidf-12" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>13 0.065332994 <a title="30-tfidf-13" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>14 0.064349324 <a title="30-tfidf-14" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>15 0.064216062 <a title="30-tfidf-15" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>16 0.060484454 <a title="30-tfidf-16" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>17 0.060129568 <a title="30-tfidf-17" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>18 0.057408508 <a title="30-tfidf-18" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>19 0.056966018 <a title="30-tfidf-19" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>20 0.055308893 <a title="30-tfidf-20" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, 0.046), (2, 0.082), (3, 0.092), (4, 0.026), (5, -0.148), (6, 0.031), (7, -0.011), (8, 0.066), (9, 0.022), (10, 0.034), (11, -0.031), (12, 0.061), (13, -0.021), (14, 0.128), (15, -0.107), (16, 0.085), (17, 0.033), (18, -0.022), (19, 0.01), (20, 0.127), (21, -0.028), (22, -0.084), (23, 0.031), (24, 0.02), (25, -0.015), (26, 0.09), (27, 0.028), (28, 0.039), (29, 0.019), (30, 0.004), (31, 0.006), (32, 0.044), (33, -0.045), (34, -0.073), (35, -0.001), (36, -0.084), (37, -0.026), (38, 0.021), (39, -0.03), (40, -0.04), (41, -0.023), (42, 0.026), (43, -0.078), (44, 0.006), (45, -0.065), (46, 0.012), (47, -0.019), (48, 0.01), (49, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94392383 <a title="30-lsi-1" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><p>2 0.74332106 <a title="30-lsi-2" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>3 0.74159646 <a title="30-lsi-3" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>4 0.66981775 <a title="30-lsi-4" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>Author: Margareta Ackerman, Shai Ben-David, David Loker</p><p>Abstract: Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill deﬁned problem - given a data set, it is not clear what a “correct” clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some ﬁrst steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms. In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg’s famous impossibility result, while providing a simpler proof. 1</p><p>5 0.65620023 <a title="30-lsi-5" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>Author: Jason Lee, Ben Recht, Nathan Srebro, Joel Tropp, Ruslan Salakhutdinov</p><p>Abstract: The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative ﬁltering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable ﬁrst-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative ﬁltering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1</p><p>6 0.63512379 <a title="30-lsi-6" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>7 0.62662089 <a title="30-lsi-7" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>8 0.59059095 <a title="30-lsi-8" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>9 0.55991215 <a title="30-lsi-9" href="./nips-2010-Sparse_Inverse_Covariance_Selection_via_Alternating_Linearization_Methods.html">248 nips-2010-Sparse Inverse Covariance Selection via Alternating Linearization Methods</a></p>
<p>10 0.54567111 <a title="30-lsi-10" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>11 0.54461324 <a title="30-lsi-11" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>12 0.51195151 <a title="30-lsi-12" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>13 0.4927755 <a title="30-lsi-13" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>14 0.49094811 <a title="30-lsi-14" href="./nips-2010-CUR_from_a_Sparse_Optimization_Viewpoint.html">45 nips-2010-CUR from a Sparse Optimization Viewpoint</a></p>
<p>15 0.47921526 <a title="30-lsi-15" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>16 0.47600314 <a title="30-lsi-16" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>17 0.46449816 <a title="30-lsi-17" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>18 0.46362513 <a title="30-lsi-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.46171486 <a title="30-lsi-19" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>20 0.45529512 <a title="30-lsi-20" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.241), (13, 0.095), (27, 0.056), (30, 0.055), (35, 0.024), (45, 0.181), (50, 0.057), (52, 0.04), (60, 0.031), (77, 0.068), (78, 0.016), (90, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89519364 <a title="30-lda-1" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>same-paper 2 0.78519988 <a title="30-lda-2" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>Author: Matthias Hein, Thomas Bühler</p><p>Abstract: Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to ﬁnding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems. 1</p><p>3 0.69877398 <a title="30-lda-3" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>4 0.68951243 <a title="30-lda-4" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>5 0.68625975 <a title="30-lda-5" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>Author: Mohsen Bayati, José Pereira, Andrea Montanari</p><p>Abstract: We consider the problem of learning a coefﬁcient vector x0 ∈ RN from noisy linear observation y = Ax0 + w ∈ Rn . In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator x. In this case, a popular approach consists in solving an ℓ1 -penalized least squares problem known as the LASSO or Basis Pursuit DeNoising (BPDN). For sequences of matrices A of increasing dimensions, with independent gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the ﬁrst rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efﬁcient algorithm, that is inspired from graphical models ideas. Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.</p><p>6 0.68266886 <a title="30-lda-6" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>7 0.68113118 <a title="30-lda-7" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>8 0.67764217 <a title="30-lda-8" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>9 0.67689276 <a title="30-lda-9" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>10 0.67600459 <a title="30-lda-10" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>11 0.67509907 <a title="30-lda-11" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>12 0.67491436 <a title="30-lda-12" href="./nips-2010-A_Dirty_Model_for_Multi-task_Learning.html">5 nips-2010-A Dirty Model for Multi-task Learning</a></p>
<p>13 0.6749025 <a title="30-lda-13" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>14 0.67440611 <a title="30-lda-14" href="./nips-2010-Large-Scale_Matrix_Factorization_with_Missing_Data_under_Additional_Constraints.html">136 nips-2010-Large-Scale Matrix Factorization with Missing Data under Additional Constraints</a></p>
<p>15 0.67398846 <a title="30-lda-15" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>16 0.67376381 <a title="30-lda-16" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>17 0.67338026 <a title="30-lda-17" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>18 0.67301917 <a title="30-lda-18" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>19 0.67282814 <a title="30-lda-19" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>20 0.67178416 <a title="30-lda-20" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
