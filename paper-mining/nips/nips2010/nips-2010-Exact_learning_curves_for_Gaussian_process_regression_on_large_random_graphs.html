<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-85" href="#">nips2010-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</h1>
<br/><p>Source: <a title="nips-2010-85-pdf" href="http://papers.nips.cc/paper/3981-exact-learning-curves-for-gaussian-process-regression-on-large-random-graphs.pdf">pdf</a></p><p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>Reference: <a title="nips-2010-85-reference" href="../nips2010_reference/nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Exact learning curves for Gaussian process regression on large random graphs  Peter Sollich Department of Mathematics King’s College London London, WC2R 2LS, U. [sent-1, score-0.379]
</p><p>2 uk  Abstract We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. [sent-12, score-0.238]
</p><p>3 Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. [sent-13, score-0.352]
</p><p>4 These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. [sent-14, score-0.462]
</p><p>5 Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. [sent-15, score-0.255]
</p><p>6 We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. [sent-16, score-0.203]
</p><p>7 Qualitatively, GP learning curves are relatively well understood for the scenario where the inputs x come from a continuous space, typically Rn [3, 4, 5, 6, 7, 8, 9, 10, 11]. [sent-20, score-0.158]
</p><p>8 However, except in the limit of large n, or for very speciﬁc situations like one-dimensional inputs [3], the learning curves cannot be calculated exactly. [sent-21, score-0.158]
</p><p>9 Here we show that this is possible for discrete input spaces where similarity between input points can be represented as a graph whose edges connect similar points, inspired by work at last year’s NIPS that developed simple approximations for this scenario [12]. [sent-22, score-0.282]
</p><p>10 There are many potential application domains where learning of such functions of discrete inputs x could be relevant, for example if x is a research paper whose impact f (x) we would like to predict; the similarity graph could then be constructed on the basis of shared authorship. [sent-23, score-0.204]
</p><p>11 Or we could be trying to learn functions on generic symbol strings x, for example ones characterizing protein amino acid sequences, and the similarity graph would have edges between homologous molecules. [sent-24, score-0.148]
</p><p>12 We focus on large sparse random graphs, where each node is connected only to a ﬁnite number of other nodes even though the overall number of nodes in the graph is large. [sent-26, score-0.416]
</p><p>13 1  In section 2 we give a brief overview of GP regression and summarize the approximation for the learning curves used in previous work [4, 8, 12]. [sent-27, score-0.242]
</p><p>14 Because for sparse random graphs typical loop lengths grow with the graph size, the belief propagation equations and hence our learning curve predictions should become exact for large graphs. [sent-29, score-0.578]
</p><p>15 Section 4 compares the predictions with simulation results for Poisson (Erdos-Renyi) graphs, where each edge is independently present with some small probability, and random regular graphs, where each node has the same degree (number of neighbours). [sent-30, score-0.292]
</p><p>16 2  GP regression and approximate learning curves  Gaussian processes have become a well known machine learning technique used in a wide range of areas, see e. [sent-35, score-0.237]
</p><p>17 If the graph has V nodes, the covariance function is then just a V × V matrix. [sent-50, score-0.229]
</p><p>18 A number of possible forms for covariance functions on graphs have been proposed. [sent-51, score-0.252]
</p><p>19 We will focus on the relatively ﬂexible random walk covariance function [22], 1 ((1 − a−1 )I + a−1 D −1/2 AD −1/2 )p a ≥ 2, p ≥ 0 (2) κ Here A is the adjacency matrix of the graph, with Aij = 1 if nodes i and j are connected by an edge, and 0 otherwise; D = diag{d1 , . [sent-52, score-0.202]
</p><p>20 , dV } is a diagonal matrix containing the degrees of the nodes in the graph (di = j Aij ). [sent-55, score-0.215]
</p><p>21 One can easily see the relationship to a random walk: the unnormalised covariance function is a (symmetrised) p-step ‘lazy’ random walk, with probability a−1 of moving to a neighbouring node at each step. [sent-56, score-0.222]
</p><p>22 The prior thus assumes that function values up to a distance p along the graph are correlated with each other, to an extent determined by the hyperparameter a−1 . [sent-57, score-0.148]
</p><p>23 C=  Our main concern in this paper are GP learning curves in discrete input spaces. [sent-59, score-0.209]
</p><p>24 The learning curve describes how the average generalisation error (mean square error) decreases with the number of examples N . [sent-60, score-0.254]
</p><p>25 The generalisation error on an ensemble of graphs is given by =  1 V  ¯ (fi − fi )2  f |D,D,graphs  (3)  i  1 We focus on the zero prior mean case throughout. [sent-62, score-0.455]
</p><p>26 It is worth noting that the generalisation error for a graph ensemble contains an additional average over this ensemble. [sent-65, score-0.376]
</p><p>27 As is standard in the study of learning curves we have assumed a matched scenario where the posterior P (f |D) for our predictions is also the posterior over the underlying target functions. [sent-66, score-0.274]
</p><p>28 The generalisation error is then the Bayes error, and is given by the average posterior variance. [sent-67, score-0.175]
</p><p>29 In the next section we will show that this shortcoming can be overcome by the cavity method (belief propagation) which explicitly takes advantage of the sparse structure of the underlying graph. [sent-71, score-0.742]
</p><p>30 This will give an accurate approximation for the learning curves in a broad range of ensembles of sparse random graphs. [sent-72, score-0.307]
</p><p>31 3  Accurate predictions with the cavity method  The cavity method was developed in statistical physics [18] but is closely related to belief propagation; for a good overview of these and other mean ﬁeld methods, see e. [sent-73, score-1.475]
</p><p>32 Because we only need the posterior variance in the matched case considered here, we ¯ can shift f so that f = 0; fi is then the deviation of the function value at node i from the posterior mean. [sent-77, score-0.257]
</p><p>33 In this notation, the Bayes error is 1 = df fi2 P (f |D) D,graphs (5) V i where P (f |D) now contains in the exponent only the terms from (1) that are quadratic in f . [sent-78, score-0.106]
</p><p>34 It will be more useful to write this as a sum over all nodes: if ni counts the 2 number of examples seen at node i, then µ fi2 = i ni fi . [sent-81, score-0.456]
</p><p>35 To eliminate the inverse of the covariance function we therefore perform a Fourier transform on the ﬁrst term in the exponent, exp(− 1 f T C −1 f ) ∝ dh exp(− 1 hT Ch + i i hi fi ). [sent-83, score-0.195]
</p><p>36 To solve this we introduce additional variables hq , deﬁned recursively via hq = (D −1/2 AD −1/2 )hq−1 for q ≥ 1 and h0 = h. [sent-89, score-0.59]
</p><p>37 These deﬁnitions are enforced via Dirac delta-functions, each i and q ≥ 1 −1/2 −1/2 q−1 −1/2 −1/2 q−1 ˆ ˆ giving a factor δ(hq −di hj ) ∝ dhq exp[ihq (hq −di hj )]. [sent-90, score-0.199]
</p><p>38 i i i i j Aij dj j Aij dj Substituting this into equation (8) gives the key advantage that now the adjacency matrix appears only linearly in the exponent, so that we have interactions only across edges of the graph. [sent-91, score-0.275]
</p><p>39 By differentiating log Z with respect to λ, keeping track of λ-dependent prefactors not written above, one ﬁnds that the Bayes error is, 1 λ→0 V  = lim  i  1 ni /σ 2 + λ  1−  di (h0 )2 i ni /σ 2 + λ  (10)  and so we need the marginal distributions of the h0 . [sent-93, score-0.33]
</p><p>40 This is where the cavity method enters: for a i large random graph the structure is locally treelike, so that if node i were eliminated the corresponding subgraphs (locally trees) rooted at the neighbours j ∈ N (i) of i would become independent [17]. [sent-94, score-1.013]
</p><p>41 By performing the Gaussian integrals in the cavity update equations (11) explicitly, these equations then take the rather simple form (i)  Vj  (j)  XVk X)−1  = (Oj −  (12)  k∈N (j)\i  where we have deﬁned the (2p + 1) × (2p + 1) matrices   1 1 c0 + ni /σ2 +λ 2 c1 . [sent-96, score-0.895]
</p><p>42 0p,p  0  Finally we need to translate these equations to an ensemble of large sparse graphs. [sent-126, score-0.164]
</p><p>43 Each ensemble is characterised by the distribution p(d) of the degrees di , with every graph that has the desired degree distribution being assigned the same probability. [sent-127, score-0.376]
</p><p>44 Instead of individual cavity covariance 4  (i)  matrices Vj , we need to consider their probability distribution W (V ) across all edges of the graph. [sent-128, score-0.767]
</p><p>45 Picking at random an edge (i, j) of a graph, the probability that node j will have degree ¯ dj is then p(dj )dj /d, because such a node has dj “chances” of being picked. [sent-129, score-0.491]
</p><p>46 ) Using again the locally treelike structure, the incoming (to node j) (j) cavity covariances Vk will be i. [sent-131, score-0.829]
</p><p>47 The average is over the distribution over the number of examples n ≡ nj at node j in the dataset D. [sent-136, score-0.144]
</p><p>48 In general equation (13) – which can also be formally derived using the replica approach [24] – cannot be solved analytically, but we can solve it numerically using a standard population dynamics method [25]. [sent-138, score-0.12]
</p><p>49 They allow us to predict learning curves as a function of the number of examples per node, ν, for arbitrary degree distributions p(d) of our random graph ensemble providing the graphs are sparse, and for arbitrary noise level σ 2 and covariance function hyperparameters p and a. [sent-149, score-0.738]
</p><p>50 We note brieﬂy that in graphs with isolated nodes (d = 0), one has to be slightly careful as already in the deﬁnition of the covariance function (2) one should replace D → D + δI to avoid division by 1 zero, taking δ → 0 at the end. [sent-150, score-0.342]
</p><p>51 5  4  Results  We will begin by comparing the performance of our new cavity prediction (equation (16)) against the eigenvalue approximation (equation (4)) from [4, 7], for random regular graphs with degree 3 (so that p(d) = δd,3 ). [sent-153, score-1.108]
</p><p>52 Figure 1: (Left) A comparison of the cavity prediction (solid line with triangles) against the eigenvalue approximation (dashed line) for the learning curves for random regular graphs of degree 3, and against simulation results for graphs with V = 500 nodes (solid line with circles). [sent-155, score-1.528]
</p><p>53 (Bottom) Similarly for Poisson (Erdos-Renyi) graphs with c = 3. [sent-158, score-0.171]
</p><p>54 As can be seen in ﬁgure 1 (left) & (right) the cavity approach is accurate along the entire learning curve, to the point where the prediction is visually almost indistinguishable from the numerical simulation results. [sent-159, score-0.782]
</p><p>55 Importantly, the cavity approach predicts even the midsection of the learning curve for intermediate values of ν, where the eigenvalue prediction clearly fails. [sent-160, score-0.939]
</p><p>56 The deviations between cavity theory and the eigenvalue predictions are largest in this central part because at this point ﬂuctuations in the number examples seen at each node have the greatest effect. [sent-161, score-0.946]
</p><p>57 For large ν, the dataset typically contains many examples for each node and Poisson ﬂuctuations around the average value n = ν are small. [sent-165, score-0.144]
</p><p>58 The ﬂuctuation effects for intermediate ν are suppressed when the noise level σ 2 is large, because then the generalisation error in the range of intermediate ν is still fairly close to its initial value (ν = 0). [sent-166, score-0.2]
</p><p>59 But for the smaller noise levels ﬂuctuations in the number of examples for each node can have a large effect, and correspondingly the eigenvalue prediction becomes very poor for intermediate ν. [sent-167, score-0.281]
</p><p>60 Comparing ﬁgure 1 (left) and (right), it can also be seen that unlike the eigenvalue-based approximation, the cavity prediction for the learning curve does not deteriorate as p is varied towards lower values. [sent-170, score-0.805]
</p><p>61 Figure 1 (bottom) shows the performance of our cavity prediction for this graph ensemble with c = 3 for a GP with p = 10, a = 2, in comparison to simulation results for V = 500. [sent-175, score-0.96]
</p><p>62 The cavity prediction clearly outperforms the eigenvalue-based approximation and again remains accurate even in the central part of the learning curve. [sent-176, score-0.792]
</p><p>63 Taken together, the results for random regular and Poisson graphs clearly conﬁrm our expectation that the cavity prediction for the learning curve that we have derived should be exact for large graphs. [sent-177, score-1.026]
</p><p>64 It is worth noting that our new cavity prediction will work for arbitrary degree distributions and is limited only by the assumption of graph sparsity. [sent-178, score-0.961]
</p><p>65 1  Why the eigenvalue approximation fails  The derivation of the eigenvalue approximation (4) by Opper in [8] gives some insight into when and how this approximation breaks down. [sent-180, score-0.234]
</p><p>66 Opper takes equation (6) and uses the replica trick to 1 write log Z D = limn→0 n log Z n D . [sent-181, score-0.12]
</p><p>67 For small noise levels, on the other hand, the Gaussian variational approach clearly does not capture all the details of the ﬂuctuations in the numbers of examples ni . [sent-186, score-0.175]
</p><p>68 By comparison, in this paper, using the cavity method we are able to retain the average over D explicitly, without the need to approximate the distribution of the ni . [sent-187, score-0.801]
</p><p>69 The result of this is that the section of the learning curve where ﬂuctuations in numbers of examples play a large role is captured accurately, while the Gaussian variational (eigenvalue) approach can give wildly inaccurate results there. [sent-188, score-0.176]
</p><p>70 5  Conclusions and further work  In this paper we have studied the learning curves of GP regression on large random graphs. [sent-189, score-0.208]
</p><p>71 In a signiﬁcant advance on the work of [12], we showed that the approximations for learning curves proposed by Sollich [4] and Opper [7] for continuous input spaces can be greatly improved upon in the graph case, by using the cavity method. [sent-190, score-1.075]
</p><p>72 Section 3 derived the learning curve approximation using the cavity method for arbitrary degree distributions. [sent-192, score-0.856]
</p><p>73 We deﬁned a generating function Z (equation (6)) from which the generalisation error can be obtained by differentiation. [sent-193, score-0.142]
</p><p>74 We then rewrote this using Fourier transforms (equation (7)) and introduced additional variables (equation (9)) to get Z into the required form for a cavity approach: the partition function of a complex-valued Gaussian graphical model. [sent-194, score-0.686]
</p><p>75 By standard arguments we then derived the cavity update equations for a ﬁxed graph (equation (12)). [sent-195, score-0.881]
</p><p>76 Finally we generalised from these to graph ensembles (equation (13)), taking the limit of large graph size. [sent-196, score-0.356]
</p><p>77 The resulting prediction for the generalisation error (equation (16)) has an intuitively appealing interpretation, where each node in the graph learns subject to an effective (and data-dependent) Gaussian prior provided by its neighbours. [sent-197, score-0.441]
</p><p>78 In section 4 we compared our new prediction to the eigenvalue approximation results in [12]. [sent-198, score-0.142]
</p><p>79 We showed that our new method is far more accurate in the challenging midsection of the learning curves than the eigenvalue version, both for random regular and Poisson graph ensembles (ﬁgure 1). [sent-199, score-0.551]
</p><p>80 1 discusses why the older approximation, derived from a replica perspective in [7], is inaccurate compared to the cavity method. [sent-201, score-0.784]
</p><p>81 To retain tractable averages in continuous input spaces, it has to approximate ﬂuctuations in the dataset of the number of examples for each node, thus resulting in the inaccurate predictions seen in ﬁgure 1. [sent-202, score-0.148]
</p><p>82 On graphs one is able to perform this average explicitly when calculating cavity updates and the resulting Bayes error, giving a far more accurate prediction of the learning curves. [sent-203, score-0.96]
</p><p>83 Indeed, an important assumption in the current work is that small loops are rare whilst in community graphs, where nodes exhibit preferential attachment, there can be many small loops. [sent-205, score-0.094]
</p><p>84 We are in the process of analysing GP learning on such graphs using the approach of Rogers et al. [sent-206, score-0.171]
</p><p>85 [27], where community graphs are modelled as having a sparse superstructure joining clusters of densely connected nodes. [sent-207, score-0.196]
</p><p>86 Following previous studies [12], we have in this paper set the scale of the covariance function by normalising the average prior covariance over all nodes. [sent-208, score-0.162]
</p><p>87 For the Poisson graph case our learning curve simulations then show, however, that there can be large variations in the local prior variances Cii , while from the Bayesian modelling point of view it would seem more plausible to use covariance functions where all Cii = 1. [sent-209, score-0.306]
</p><p>88 This could be achieved by pre- and post-multiplying the random walk covariance matrix by an appropriate diagonal matrix. [sent-210, score-0.135]
</p><p>89 We hope to study this modiﬁed covariance function in future, and to extend the cavity prediction for the learning curves to this case. [sent-211, score-0.967]
</p><p>90 This was studied for continuous input spaces in [10]; equally interesting would be a study of mismatch with a ﬁxed target function as analysed by Opper et al. [sent-213, score-0.107]
</p><p>91 This is relevant because frequently in real world learning one will have only partial knowledge of the graph structure, for instance in metabolic networks when not all of the pathways have been discovered, or social networks where friendships are continuously being made and broken. [sent-216, score-0.148]
</p><p>92 One would hope that, as seen with the learning curves for single output GPs in this paper, input domains deﬁned by graphs might allow simpliﬁcations in the analysis and provide more accurate bounds or even exact predictions. [sent-218, score-0.412]
</p><p>93 Finally, it would be worth extending the study of graph mismatch to the case of evolving graphs and functions. [sent-219, score-0.395]
</p><p>94 Here spatio-temporal GP regression could be employed to predict functions changing over time, perhaps including a model based approach as in [29] to account for the evolving graph structure. [sent-220, score-0.224]
</p><p>95 Learning curves for gaussian processes regression: A framework for good approximations. [sent-256, score-0.249]
</p><p>96 Learning curves for Gaussian process regression: Approximations and bounds. [sent-268, score-0.158]
</p><p>97 Kernels and learning curves for Gaussian process regression on random graphs. [sent-285, score-0.208]
</p><p>98 o e [27] Tim Rogers, Conrad P´ rez Vicente, Koujin Takeda, and Isaac P´ rez Castillo. [sent-366, score-0.104]
</p><p>99 Spectral density of random e e graphs with topological constraints. [sent-367, score-0.171]
</p><p>100 Generalization errors and learning curves for regression with multi-task Gaussian processes. [sent-370, score-0.208]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cavity', 0.686), ('hq', 0.295), ('graphs', 0.171), ('curves', 0.158), ('opper', 0.155), ('gp', 0.15), ('graph', 0.148), ('ni', 0.115), ('generalisation', 0.112), ('poisson', 0.109), ('node', 0.109), ('dj', 0.107), ('sollich', 0.098), ('fi', 0.082), ('covariance', 0.081), ('curve', 0.077), ('uctuations', 0.072), ('di', 0.07), ('hj', 0.07), ('xvk', 0.069), ('nodes', 0.067), ('eigenvalue', 0.066), ('gaussian', 0.062), ('equation', 0.061), ('ensemble', 0.06), ('ensembles', 0.06), ('bayes', 0.06), ('cq', 0.059), ('replica', 0.059), ('cii', 0.059), ('dhq', 0.059), ('dvk', 0.059), ('degree', 0.059), ('walk', 0.054), ('vk', 0.054), ('rez', 0.052), ('fia', 0.052), ('gps', 0.052), ('regular', 0.05), ('regression', 0.05), ('ht', 0.05), ('mismatch', 0.05), ('predictions', 0.05), ('equations', 0.047), ('neighbours', 0.044), ('rogers', 0.044), ('df', 0.044), ('prediction', 0.042), ('aij', 0.041), ('characterised', 0.039), ('curran', 0.039), ('dhk', 0.039), ('koujin', 0.039), ('midsection', 0.039), ('reimer', 0.039), ('takeda', 0.039), ('inaccurate', 0.039), ('exp', 0.035), ('london', 0.035), ('examples', 0.035), ('tim', 0.034), ('herbster', 0.034), ('treelike', 0.034), ('urry', 0.034), ('approximation', 0.034), ('posterior', 0.033), ('propagation', 0.033), ('vj', 0.033), ('spaces', 0.033), ('dh', 0.032), ('translate', 0.032), ('exponent', 0.032), ('oj', 0.032), ('mismatched', 0.032), ('neighbouring', 0.032), ('spin', 0.032), ('explicitly', 0.031), ('accurate', 0.03), ('error', 0.03), ('domains', 0.029), ('intermediate', 0.029), ('processes', 0.029), ('ad', 0.028), ('whilst', 0.027), ('ch', 0.027), ('discrete', 0.027), ('belief', 0.027), ('predict', 0.026), ('approximations', 0.026), ('marginals', 0.026), ('mit', 0.026), ('worth', 0.026), ('physics', 0.026), ('subgraphs', 0.026), ('variational', 0.025), ('king', 0.025), ('sparse', 0.025), ('simulation', 0.024), ('input', 0.024), ('isolated', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="85-tfidf-1" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>2 0.11065662 <a title="85-tfidf-2" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>3 0.10627601 <a title="85-tfidf-3" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>4 0.10275682 <a title="85-tfidf-4" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>5 0.096902058 <a title="85-tfidf-5" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>6 0.093267746 <a title="85-tfidf-6" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>7 0.087522097 <a title="85-tfidf-7" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>8 0.083385423 <a title="85-tfidf-8" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>9 0.077238992 <a title="85-tfidf-9" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>10 0.076129943 <a title="85-tfidf-10" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>11 0.070019729 <a title="85-tfidf-11" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>12 0.066446796 <a title="85-tfidf-12" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>13 0.065417506 <a title="85-tfidf-13" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>14 0.064319707 <a title="85-tfidf-14" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>15 0.064251214 <a title="85-tfidf-15" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>16 0.063416436 <a title="85-tfidf-16" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<p>17 0.062728196 <a title="85-tfidf-17" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>18 0.062301226 <a title="85-tfidf-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.061691601 <a title="85-tfidf-19" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>20 0.060607005 <a title="85-tfidf-20" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.043), (2, 0.011), (3, 0.118), (4, -0.064), (5, -0.043), (6, 0.047), (7, 0.008), (8, 0.018), (9, -0.003), (10, -0.12), (11, -0.038), (12, 0.006), (13, 0.001), (14, 0.028), (15, -0.053), (16, 0.034), (17, -0.057), (18, -0.007), (19, 0.108), (20, -0.045), (21, 0.021), (22, -0.079), (23, -0.01), (24, 0.027), (25, 0.056), (26, 0.055), (27, 0.089), (28, 0.041), (29, 0.027), (30, 0.143), (31, 0.042), (32, 0.092), (33, -0.033), (34, -0.042), (35, -0.075), (36, -0.035), (37, 0.015), (38, -0.009), (39, -0.098), (40, 0.02), (41, 0.059), (42, -0.003), (43, -0.104), (44, 0.09), (45, 0.046), (46, 0.046), (47, 0.032), (48, -0.084), (49, -0.172)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93433475 <a title="85-lsi-1" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>2 0.70624727 <a title="85-lsi-2" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>3 0.70275187 <a title="85-lsi-3" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>4 0.6225037 <a title="85-lsi-4" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>5 0.62098521 <a title="85-lsi-5" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>Author: Seth Myers, Jure Leskovec</p><p>Abstract: In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1 -like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.</p><p>6 0.60279465 <a title="85-lsi-6" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>7 0.59643108 <a title="85-lsi-7" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>8 0.59018642 <a title="85-lsi-8" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>9 0.53517324 <a title="85-lsi-9" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>10 0.51290363 <a title="85-lsi-10" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>11 0.51025367 <a title="85-lsi-11" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>12 0.50898916 <a title="85-lsi-12" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>13 0.5001412 <a title="85-lsi-13" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>14 0.45453626 <a title="85-lsi-14" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>15 0.45196873 <a title="85-lsi-15" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>16 0.44989929 <a title="85-lsi-16" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>17 0.43952292 <a title="85-lsi-17" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>18 0.42713213 <a title="85-lsi-18" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>19 0.4232052 <a title="85-lsi-19" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>20 0.42202604 <a title="85-lsi-20" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.056), (17, 0.019), (25, 0.225), (27, 0.076), (30, 0.07), (35, 0.017), (45, 0.208), (50, 0.071), (52, 0.04), (60, 0.027), (77, 0.056), (78, 0.016), (79, 0.012), (90, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90691948 <a title="85-lda-1" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>2 0.87429672 <a title="85-lda-2" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>same-paper 3 0.82352734 <a title="85-lda-3" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>4 0.81127524 <a title="85-lda-4" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>5 0.7493757 <a title="85-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.74569261 <a title="85-lda-6" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>7 0.74275285 <a title="85-lda-7" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>8 0.74014437 <a title="85-lda-8" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>9 0.73934603 <a title="85-lda-9" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>10 0.73841959 <a title="85-lda-10" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>11 0.73750871 <a title="85-lda-11" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>12 0.73680371 <a title="85-lda-12" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>13 0.73616821 <a title="85-lda-13" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>14 0.73533881 <a title="85-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.73417604 <a title="85-lda-15" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>16 0.73415208 <a title="85-lda-16" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>17 0.73331958 <a title="85-lda-17" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>18 0.73164183 <a title="85-lda-18" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>19 0.73155195 <a title="85-lda-19" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>20 0.73150027 <a title="85-lda-20" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
