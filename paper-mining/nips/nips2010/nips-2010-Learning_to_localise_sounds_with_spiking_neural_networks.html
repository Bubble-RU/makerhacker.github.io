<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 nips-2010-Learning to localise sounds with spiking neural networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-157" href="#">nips2010-157</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 nips-2010-Learning to localise sounds with spiking neural networks</h1>
<br/><p>Source: <a title="nips-2010-157-pdf" href="http://papers.nips.cc/paper/4127-learning-to-localise-sounds-with-spiking-neural-networks.pdf">pdf</a></p><p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>Reference: <a title="nips-2010-157-reference" href="../nips2010_reference/nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sound', 0.411), ('bina', 0.385), ('neuron', 0.295), ('delay', 0.222), ('fl', 0.197), ('assembl', 0.197), ('hrtfs', 0.193), ('acoust', 0.186), ('fr', 0.183), ('hardwir', 0.178), ('intera', 0.178), ('azimu', 0.169), ('synchrony', 0.157), ('spik', 0.124), ('cochlear', 0.117), ('elev', 0.116), ('lter', 0.111), ('itd', 0.104), ('mona', 0.104), ('channel', 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="157-tfidf-1" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>2 0.24723729 <a title="157-tfidf-2" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>3 0.19175169 <a title="157-tfidf-3" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>4 0.17571506 <a title="157-tfidf-4" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>5 0.17422597 <a title="157-tfidf-5" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>6 0.15950668 <a title="157-tfidf-6" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>7 0.15343082 <a title="157-tfidf-7" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>8 0.13240984 <a title="157-tfidf-8" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>9 0.12733036 <a title="157-tfidf-9" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>10 0.11728136 <a title="157-tfidf-10" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>11 0.11357478 <a title="157-tfidf-11" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>12 0.11157547 <a title="157-tfidf-12" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>13 0.10562025 <a title="157-tfidf-13" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>14 0.10506117 <a title="157-tfidf-14" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>15 0.089860037 <a title="157-tfidf-15" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>16 0.077550665 <a title="157-tfidf-16" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>17 0.066823706 <a title="157-tfidf-17" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>18 0.063431948 <a title="157-tfidf-18" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>19 0.061252873 <a title="157-tfidf-19" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>20 0.059558336 <a title="157-tfidf-20" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.059), (2, -0.341), (3, 0.001), (4, -0.008), (5, -0.066), (6, 0.029), (7, 0.021), (8, -0.013), (9, 0.004), (10, 0.019), (11, 0.042), (12, -0.008), (13, -0.004), (14, -0.008), (15, -0.056), (16, -0.029), (17, -0.002), (18, -0.035), (19, -0.008), (20, -0.017), (21, 0.013), (22, 0.041), (23, 0.03), (24, -0.08), (25, -0.01), (26, -0.005), (27, -0.05), (28, 0.068), (29, -0.021), (30, -0.033), (31, -0.003), (32, -0.038), (33, 0.003), (34, -0.02), (35, 0.057), (36, 0.019), (37, 0.028), (38, 0.062), (39, -0.013), (40, 0.03), (41, 0.099), (42, -0.02), (43, 0.117), (44, 0.022), (45, 0.018), (46, -0.021), (47, -0.025), (48, -0.072), (49, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94538498 <a title="157-lsi-1" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>2 0.86139995 <a title="157-lsi-2" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>3 0.84911811 <a title="157-lsi-3" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>4 0.84541053 <a title="157-lsi-4" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>Author: Anand Singh, Renaud Jolivet, Pierre Magistretti, Bruno Weber</p><p>Abstract: Sodium entry during an action potential determines the energy efﬁciency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefﬁcient in that regard with about 4 times more charges ﬂowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efﬁciency and that the dynamics of their voltage-gated channels is signiﬁcantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of HodgkinHuxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models. 1</p><p>5 0.74711502 <a title="157-lsi-5" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>6 0.74500734 <a title="157-lsi-6" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>7 0.74058652 <a title="157-lsi-7" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>8 0.69933248 <a title="157-lsi-8" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>9 0.60805142 <a title="157-lsi-9" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>10 0.58627707 <a title="157-lsi-10" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>11 0.57392895 <a title="157-lsi-11" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>12 0.54696167 <a title="157-lsi-12" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>13 0.53209496 <a title="157-lsi-13" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>14 0.51350647 <a title="157-lsi-14" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>15 0.48138869 <a title="157-lsi-15" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>16 0.47519654 <a title="157-lsi-16" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>17 0.46736881 <a title="157-lsi-17" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>18 0.44634813 <a title="157-lsi-18" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>19 0.44629288 <a title="157-lsi-19" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>20 0.39260799 <a title="157-lsi-20" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.08), (12, 0.408), (30, 0.045), (32, 0.083), (34, 0.109), (39, 0.018), (45, 0.043), (52, 0.012), (68, 0.087), (71, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72315848 <a title="157-lda-1" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>2 0.58019835 <a title="157-lda-2" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>Author: Kaiming Li, Lei Guo, Carlos Faraco, Dajiang Zhu, Fan Deng, Tuo Zhang, Xi Jiang, Degang Zhang, Hanbo Chen, Xintao Hu, Steve Miller, Tianming Liu</p><p>Abstract: Functional segregation and integration are fundamental characteristics of the human brain. Studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest. A very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or ROIs (regions of interests) for individuals. Essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the ROIs. This paper presents a novel methodology to optimize the locations of an individual's ROIs in the working memory system. Our strategy is to formulate the individual ROI optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints. The optimization problem is solved via the simulated annealing approach. Our experimental results show that the optimized ROIs have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles. 1 Int ro ducti o n The human brainâ&euro;&trade;s function is segregated into distinct regions and integrated via axonal fibers [1]. Studying the connectivity among these regions and modeling their dynamics and interactions has drawn increasing interest and effort from the brain imaging and neuroscience communities [2-6]. For example, recently, the Human Connectome Project [7] and the 1000 Functional Connectomes Project [8] have embarked to elucidate large-scale connectivity patterns in the human brain. For traditional connectivity analysis, a variety of models including DCM (dynamics causal modeling), GCM (Granger causality modeling) and MVA (multivariate autoregressive modeling) are proposed [6, 9-10] to model the interactions of the ROIs. A fundamental issue in these studies is how to accurately identify the ROIs, which are the structural substrates for measuring connectivity. Currently, this is still an open, urgent, yet challenging problem in many brain imaging applications. From our perspective, the major challenges come from uncertainties in ROI boundary definition, the tremendous variability across individuals, and high nonlinearities within and around ROIs. Current approaches for identifying brain ROIs can be broadly classified into four categories. The first is manual labeling by experts using their domain knowledge. The second is a data-driven clustering of ROIs from the brain image itself. For instance, the ReHo (regional homogeneity) algorithm [11] has been used to identify regional homogeneous regions as ROIs. The third is to predefine ROIs in a template brain, and warp them back to the individual space using image registration [12]. Lastly, ROIs can be defined from the activated regions observed during a task-based fMRI paradigm. While fruitful results have been achieved using these approaches, there are various limitations. For instance, manual labeling is difficult to implement for large datasets and may be vulnerable to inter-subject and intra-subject variation; it is difficult to build correspondence across subjects using data-driven clustering methods; warping template ROIs back to individual space is subject to the accuracy of warping techniques and the anatomical variability across subjects. Even identifying ROIs using task-based fMRI paradigms, which is regarded as the standard approach for ROI identification, is still an open question. It was reported in [13] that many imaging-related variables including scanner vender, RF coil characteristics (phase array vs. volume coil), k-space acquisition trajectory, reconstruction algorithms, susceptibility -induced signal dropout, as well as field strength differences, contribute to variations in ROI identification. Other researchers reported that spatial smoothing, a common preprocessing technique in fMRI analysis to enhance SNR, may introduce artificial localization shift s (up to 12.1mm for Gaussian kernel volumetric smoothing) [14] or generate overly smoothed activation maps that may obscure important details [15]. For example, as shown in Fig.1a, the local maximum of the ROI was shifted by 4mm due to the spatial smoothing process. Additionally, its structural profile (Fig.1b) was significantly altered. Furthermore, group-based activation maps may show different patterns from an individual's activation map; Fig.1c depicts such differences. The top panel is the group activation map from a working memory study, while the bottom panel is the activation map of one subject in the study. As we can see from the highlighted boxes, the subject has less activated regions than the group analysis result. In conclusion, standard analysis of task-based fMRI paradigm data is inadequate to accurately localize ROIs for each individual. Fig.1. (a): Local activation map maxima (marked by the cross) shift of one ROI due to spatial volumetric smoothing. The top one was detected using unsmoothed data while the bottom one used smoothed data (FWHM: 6.875mm). (b): The corresponding fibers for the ROIs in (a). The ROIs are presented using a sphere (radius: 5mm). (c): Activation map differences between the group (top) and one subject (bottom). The highlighted boxes show two of the missing activated ROIs found from the group analysis. Without accurate and reliable individualized ROIs, the validity of brain connectivity analysis, and computational modeling of dynamics and interactions among brain networks , would be questionable. In response to this fundamental issue, this paper presents a novel computational methodology to optimize the locations of an individual's ROIs initialized from task-based fMRI. We use the ROIs identified in a block-based working memory paradigm as a test bed application to develop and evaluate our methodology. The optimization of ROI locations was formulated as an energy minimization problem, with the goal of jointly maximizing the group-wise consistency of functional and structural connectivity patterns and anatomic profiles. The optimization problem is solved via the well-established simulated annealing approach. Our experimental results show that the optimized ROIs achieved our optimization objectives and demonstrated promising results. 2 Mat eria l s a nd Metho ds 2.1 Data acquisition and preprocessing Twenty-five university students were recruited to participate in this study. Each participant performed an fMRI modified version of the OSPAN task (3 block types: OSPAN, Arithmetic, and Baseline) while fMRI data was acquired. DTI scans were also acquired for each participant. FMRI and DTI scans were acquired on a 3T GE Signa scanner. Acquisition parameters were as follows : fMRI: 64x64 matrix, 4mm slice thickness, 220mm FOV, 30 slices, TR=1.5s, TE=25ms, ASSET=2; DTI: 128x128 matrix, 2mm slice thickness, 256mm FOV, 60 slices, TR=15100ms, TE= variable, ASSET=2, 3 B0 images, 30 optimized gradient directions, b-value=1000). Each participantâ&euro;&trade;s fMRI data was analyzed using FSL. Individual activation map Fig.2. working memory reflecting the OSPAN (OSPAN > Baseline) contrast was used. In ROIs mapped on a total, we identified the 16 highest activated ROIs, including left WM/GM surface and right insula, left and right medial frontal gyrus, left and right precentral gyrus, left and right paracingulate gyrus, left and right dorsolateral prefrontal cortex, left and right inferior parietal lobule, left occipital pole, right frontal pole, right lateral occipital gyrus, and left and right precuneus. Fig.2 shows the 16 ROIs mapped onto a WM(white matter)/GM(gray matter) cortical surface. For some individuals, there may be missing ROIs on their activation maps. Under such condition, we adapted the group activation map as a guide to find these ROIs using linear registration. DTI pre-processing consisted of skull removal, motion correction, and eddy current correction. After the pre-processing, fiber tracking was performed using MEDINRIA (FA threshold: 0.2; minimum fiber length: 20). Fibers were extended along their tangent directions to reach into the gray matter when necessary. Brain tissue segmentation was conducted on DTI data by the method in [16] and the cortical surface was reconstructed from the tissue maps using the marching cubes algorithm. The cortical surface was parcellated into anatomical regions using the HAMMER tool [17]. DTI space was used as the standard space from which to generate the GM (gray matter) segmentation and from which to report the ROI locations on the cortical surface. Since the fMRI and DTI sequences are both EPI (echo planar imaging) sequences, their distortions tend to be similar and the misalignment between DTI and fMRI images is much less than that between T1 and fMRI images [18]. Co-registration between DTI and fMRI data was performed using FSL FLIRT [12]. The activated ROIs and tracked fibers were then mapped onto the cortical surface for joint modeling. 2.2 Joint modeling of anatomical, structural and functional profiles Despite the high degree of variability across subjects, there are several aspects of regularity on which we base the proposed solution. Firstly, across subjects, the functional ROIs should have similar anatomical locations, e.g., similar locations in the atlas space. Secondly, these ROIs should have similar structural connectivity profiles across subjects. In other words, fibers penetrating the same functional ROIs should have at least similar target regions across subjects. Lastly, individual networks identified by task-based paradigms, like the working memory network we adapted as a test bed in this paper, should have similar functional connectivity pattern across subjects. The neuroscience bases of the above premises include: 1) structural and functional brain connectivity are closely related [19], and cortical gyrification and axongenesis processes are closely coupled [20]; Hence, it is reasonable to put these three types of information in a joint modeling framework. 2) Extensive studies have already demonstrated the existence of a common structural and functional architecture of the human brain [21, 22], and it makes sense to assume that the working memory network has similar structural and functional connectivity patterns across individuals. Based on these premises, we proposed to optimize the locations of individual functional ROIs by jointly modeling anatomic profiles, structural connectivity patterns, and functional connectivity patterns, as illustrated in Fig 3. The Fig.3. ROIs optimization scheme. goal was to minimize the group-wise variance (or maximize group-wise consistency) of these jointly modeled profiles. Mathematically, we modeled the group-wise variance as energy E as follows. A ROI from fMRI analysis was mapped onto the surface, and is represented by a center vertex and its neighborhood. Suppose đ?&lsquo;&hellip; đ?&lsquo;&ndash;đ?&lsquo;&mdash; is the ROI region j on the cortical surface of subject i identified in Section 2.1; we find a corresponding surface ROI region đ?&lsquo;&dagger; đ?&lsquo;&ndash;đ?&lsquo;&mdash; so that the energy E (contains energy from n subjects, each with m ROIs) is minimized: đ??¸ = đ??¸ đ?&lsquo;Ž (đ?&oelig;&dagger; đ??¸ đ?&lsquo;? â&circ;&rsquo;đ?&lsquo;&euro; đ??¸ đ?&lsquo;? đ?&oelig;Ž đ??¸đ?&lsquo;? + (1 â&circ;&rsquo; đ?&oelig;&dagger;) đ??¸ đ?&lsquo;&ldquo; â&circ;&rsquo;đ?&lsquo;&euro; đ??¸ đ?&lsquo;&ldquo; đ?&oelig;Žđ??¸đ?&lsquo;&ldquo; ) (1) where Ea is the anatomical constraint; Ec is the structural connectivity constraint, M Ec and ď ł E are the mean and standard deviation of Ec in the searching space; E f is the functional c connectivity constraint, M E f and ď ł E f are the mean and standard deviation of E f respectively; and ď Ź is a weighting parameter between 0 and 1. If not specified, and m is the number of ROIs in this paper. The details of these energy terms are provided in the following sections. 2.2.1 n is the number of subjects, Anatomical constraint energy Anatomical constraint energy Ea is defined to ensure that the optimized ROIs have similar anatomical locations in the atlas space (Fig.4 shows an example of ROIs of 15 randomly selected subjects in the atlas space). We model the locations for all ROIs in the atlas space using a Gaussian model (mean: đ?&lsquo;&euro; đ?&lsquo;&lsaquo; đ?&lsquo;&mdash; ,and standard deviation: ď ł X j for ROI j ). The model parameters were estimated using the initial locations obtained from Section 2.1. Let X ij be the center coordinate of region Sij in the atlas space, then Ea is expressed as đ??¸đ?&lsquo;Ž = { 1 đ?&lsquo;&rsquo; đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľâ&circ;&rsquo;1 Fig.4. ROI distributions in Atlas space. (đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľâ&permil;¤1) (đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľ>1) (2) â&euro;&ndash; , 1 â&permil;¤ đ?&lsquo;&ndash; â&permil;¤ đ?&lsquo;&rsaquo;; 1 â&permil;¤ đ?&lsquo;&mdash; â&permil;¤ đ?&lsquo;&scaron;. } (3) where đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľ = đ?&lsquo;&euro;đ?&lsquo;Žđ?&lsquo;Ľ { â&euro;&ndash; đ?&lsquo;&lsaquo; đ?&lsquo;&ndash;đ?&lsquo;&mdash; â&circ;&rsquo;đ?&lsquo;&euro; đ?&lsquo;&lsaquo; đ?&lsquo;&mdash; 3đ?&oelig;Ž đ?&lsquo;&lsaquo; đ?&lsquo;&mdash; Under the above definition, if any X ij is within the range of 3s X from the distribution model j center M X , the anatomical constraint energy will always be one; if not, there will be an j exponential increase of the energy which punishes the possible involvement of outliers. In other words, this energy factor will ensure the optimized ROIs will not significantly deviate away from the original ROIs. 2.2.2 Structural connectivity constraint energy Structural connectivity constraint energy Ec is defined to ensure the group has similar structural connectivity profiles for each functional ROI, since similar functional regions should have the similar structural connectivity patterns [19], n m Ec ď&euro;˝ ď&fnof;Ľď&fnof;Ľ (Cij ď&euro;­ M C j )Covc ď&euro;­1 (Ci j ď&euro;­ M C j )T (4) i ď&euro;˝1 j ď&euro;˝1 where Cij is the connectivity pattern vector for ROI j of subject i , M C j is the group mean ď&euro;­1 for ROI j , and Covc is the inverse of the covariance matrix. The connectivity pattern vector Cij is a fiber target region distribution histogram. To obtain this histogram, we first parcellate all the cortical surfaces into nine regions ( as shown in Fig.5a, four lobes for each hemisphere, and the subcortical region) using the HAMMER algorithm [17]. A finer parcellation is available but not used due to the relatively lower parcellation accuracy, which might render the histogram too sensitive to the parcellation result. Then, we extract fibers penetrating region Sij , and calculate the distribution of the fibersâ&euro;&trade; target cortical regions. Fig.5 illustrates the ideas. Fig.5. Structural connectivity pattern descriptor. (a): Cortical surface parcellation using HAMMER [17]; (b): Joint visualization of the cortical surface, two ROIs (blue and green spheres), and fibers penetrating the ROIs (in red and yellow, respectively); (c): Corresponding target region distribution histogram of ROIs in Fig.5b. There are nine bins corresponding to the nine cortical regions. Each bin contains the number of fibers that penetrate the ROI and are connected to the corresponding cortical region. Fiber numbers are normalized across subjects. 2.2.3 Functional connectivity constraint energy Functional connectivity constraint energy E f is defined to ensure each individual has similar functional connectivity patterns for the working memory system, assuming the human brain has similar functional architecture across individuals [21]. đ?&lsquo;&rsaquo; đ??¸ đ?&lsquo;&ldquo; = â&circ;&lsquo; đ?&lsquo;&ndash;=1â&euro;&ndash;đ??šđ?&lsquo;&ndash; â&circ;&rsquo; đ?&lsquo;&euro; đ??š â&euro;&ndash; (5) Here, Fi is the functional connectivity matrix for subject i , and M F is the group mean of the dataset. The connectivity between each pair of ROIs is defined using the Pearson correlation. The matrix distance used here is the Frobenius norm. 2.3 Energy minimization solution The minimization of the energy defined in Section 2.2 is known as a combinatorial optimization problem. Traditional optimization methods may not fit this problem, since there are two noticeable characteristics in this application. First, we do not know how the energy changes with the varying locations of ROIs. Therefore, techniques like Newtonâ&euro;&trade;s method cannot be used. Second, the structure of search space is not smooth, which may lead to multiple local minima during optimization. To address this problem, we adopt the simulated annealing (SA) algorithm [23] for the energy minimization. The idea of the SA algorithm is based on random walk through the space for lower energies. In these random walks, the probability of taking a step is determined by the Boltzmann distribution, - (E - E )/ ( KT ) p = e i+ 1 i (6) if Ei ď&euro;Ť1 ď&euro;ž Ei , and p ď&euro;˝ 1 when Ei ď&euro;Ť1 ď&sbquo;Ł Ei . Here, đ??¸ đ?&lsquo;&ndash; and đ??¸ đ?&lsquo;&ndash;+1 are the system energies at solution configuration đ?&lsquo;&ndash; and đ?&lsquo;&ndash; + 1 respectively; đ??ž is the Boltzmann constant; and đ?&lsquo;&Dagger; is the system temperature. In other words, a step will be taken when a lower energy is found. A step will also be taken with probability p if a higher energy is found. This helps avoid the local minima in the search space. 3 R esult s Compared to structural and functional connectivity patterns, anatomical profiles are more easily affected by variability across individuals. Therefore, the anatomical constraint energy is designed to provide constraint only to ROIs that are obviously far away from reasonableness. The reasonable range was statistically modeled by the localizations of ROIs warped into the atlas space in Section 2.2.1. Our focus in this paper is the structural and functional profiles. 3.1 Optimization using anatomical and structural connectivity profile s In this section, we use only anatomical and structural connectivity profiles to optimize the locations of ROIs. The goal is to check whether the structural constraint energy Ec works as expected. Fig.6 shows the fibers penetrating the right precuneus for eight subjects before (top panel) and after optimization (bottom panel). The ROI is highlighted in a red sphere for each subject. As we can see from the figure (please refer to the highlighted yellow arrows), after optimization, the third and sixth subjects have significantly improved consistency with the rest of the group than before optimization, which proves the validity of the energy function Eq.(4). Fig.6. Comparison of structural profiles before and after optimization. Each column shows the corresponding before-optimization (top) and after-optimization (bottom) fibers of one subject. The ROI (right precuneus) is presented by the red sphere. 3.2 Optimization using anatomical and functional connectivity profiles In this section, we optimize the locations of ROIs using anatomical and functional profiles, aiming to validate the definition of functional connectivity constraint energy E f . If this energy constraint worked well, the functional connectivity variance of the working memory system across subjects would decrease. Fig.7 shows the comparison of the standard derivation for functional connectivity before (left) and after (right) optimization. As we can see, the variance is significantly reduced after optimization. This demonstrated the effectiveness of the defined functional connectivity constraint energy. Fig.7. Comparison of the standard derivation for functional connectivity before and after the optimization. Lower values mean more consistent connectivity pattern cross subjects. 3.3 Consistency between optimization of functional profiles and structural profiles Fig.8. Optimization consistency between functional and structural profiles. Top: Functional profile energy drop along with structural profile optimization; Bottom: Structural profile energy drop along with functional profile optimization. Each experiment was repeated 15 times with random initial ROI locations that met the anatomical constraint. The relationship between structure and function has been extensively studied [24], and it is widely believed that they are closely related. In this section, we study the relationship between functional profiles and structural profiles by looking at how the energy for one of them changes while the energy of the other decreases. The optimization processes in Section 3.1 and 3.2 were repeated 15 times respectively with random initial ROI locations that met the anatomical constraint. As shown in Fig.8, in general, the functional profile energies and structural profile energies are closely related in such a way that the functional profile energies tend to decrease along with the structural profile optimization process, while the structural profile energies also tend to decrease as the functional profile is optimized. This positively correlated decrease of functional profile energy and structural profile energy not only proves the close relationship between functional and structural profiles, but also demonstrates the consistency between functional and structural optimization, laying down the foundation of the joint optimiza tion, whose results are detailed in the following section. 3.4 Optimization connectivity profiles using anatomical, structural and functional In this section, we used all the constraints in Eq. (1) to optimize the individual locations of all ROIs in the working memory system. Ten runs of the optimization were performed using random initial ROI locations that met the anatomical constraint. Weighting parameter ď Ź equaled 0.5 for all these runs. Starting and ending temperatures for the simulated annealing algorithm are 8 and 0.05; Boltzmann constant K ď&euro;˝ 1 . As we can see from Fig.9, most runs started to converge at step 24, and the convergence energy is quite close for all runs. This indicates that the simulated annealing algorithm provides a valid solution to our problem. By visual inspection, most of the ROIs move to more reasonable and consistent locations after the joint optimization. As an example, Fig.10 depicts the location movements of the ROI in Fig. 6 for eight subjects. As we can see, the ROIs for these subjects share a similar anatomical landmark, which appears to be the tip of the upper bank of the parieto-occipital sulcus. If the initial ROI was not at this landmark, it moved to the landmark after the optimization, which was the case for subjects 1, 4 and 7. The structural profiles of these ROIs are very similar to Fig.6. The results in Fig. 10 indicate the significant improvement of ROI locations achieved by the joint optimization procedure. Fig.9. Convergence performance of the simulated annealing . Each run has 28 temperature conditions. Fig.10. The movement of right precuneus before (in red sphere) and after (in green sphere) optimization for eight subjects. The</p><p>3 0.41804168 <a title="157-lda-3" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>4 0.41524661 <a title="157-lda-4" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>Author: Yi Sun, Jürgen Schmidhuber, Faustino J. Gomez</p><p>Abstract: We present a new way of converting a reversible ﬁnite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result conﬁrms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC. 1</p><p>5 0.413863 <a title="157-lda-5" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>Author: Alex Kulesza, Ben Taskar</p><p>Abstract: We present a novel probabilistic model for distributions over sets of structures— for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random ﬁelds and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems. 1</p><p>6 0.41350773 <a title="157-lda-6" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>7 0.41341251 <a title="157-lda-7" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>8 0.41247633 <a title="157-lda-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.41240358 <a title="157-lda-9" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>10 0.41183835 <a title="157-lda-10" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>11 0.41167384 <a title="157-lda-11" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>12 0.41150111 <a title="157-lda-12" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>13 0.41124722 <a title="157-lda-13" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>14 0.4111042 <a title="157-lda-14" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>15 0.41051617 <a title="157-lda-15" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>16 0.41039553 <a title="157-lda-16" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>17 0.40992832 <a title="157-lda-17" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>18 0.40934825 <a title="157-lda-18" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>19 0.40926048 <a title="157-lda-19" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>20 0.40890524 <a title="157-lda-20" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
