<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-225" href="#">nips2010-225</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</h1>
<br/><p>Source: <a title="nips-2010-225-pdf" href="http://papers.nips.cc/paper/3936-relaxed-clipping-a-global-training-method-for-robust-regression-and-classification.pdf">pdf</a></p><p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>Reference: <a title="nips-2010-225-reference" href="../nips2010_reference/nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. [sent-4, score-0.501]
</p><p>2 A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. [sent-6, score-1.158]
</p><p>3 We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. [sent-7, score-0.62]
</p><p>4 We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. [sent-8, score-0.621]
</p><p>5 1  Introduction  Robust statistics is a well established ﬁeld that analyzes the sensitivity of common estimators to outliers and provides alternative estimators that achieve improved robustness [11, 13, 17, 23]. [sent-9, score-0.707]
</p><p>6 Unfortunately, the state-of-the-art in robust statistics relies on non-convex training criteria that have yet to yield efﬁcient global solution methods [13, 17, 23]. [sent-12, score-0.362]
</p><p>7 Although many robust regression methods have been proposed in the classical literature, Mestimators continue to be a dominant approach [13, 17]. [sent-13, score-0.356]
</p><p>8 These correspond to the standard machine learning approach of minimizing a sum of prediction errors under a given loss function (assuming a ﬁxed scaling). [sent-14, score-0.37]
</p><p>9 M-estimation is reasonably well understood, analytically tractable, and provides a simple framework for trading off between robustness against outliers and data efﬁciency on inliers [13, 17]. [sent-15, score-0.478]
</p><p>10 Unfortunately, robustness in this context comes with a cost: when minimizing a convex loss, even a single data point can dominate the result. [sent-16, score-0.354]
</p><p>11 That is, any (non-constant) convex loss function exhibits necessarily unbounded sensitivity to even a single outlier [17, §5. [sent-17, score-0.933]
</p><p>12 Although unbounded sensitivity can obviously be mitigated by imposing prior bounds on the domain and range of the data [5, 6], such is not always possible in practice. [sent-20, score-0.214]
</p><p>13 Instead, the classical literature achieves bounded outlier sensitivity by considering redescending loss functions (see [17, §2. [sent-21, score-0.926]
</p><p>14 2] for a deﬁnition), or more restrictively, bounded loss functions, both of which are inherently non-convex. [sent-22, score-0.412]
</p><p>15 Robust regression has also been extensively investigated in computer vision [2, 26], where a similar conclusion has been reached that bounded loss functions are necessary to counteract the types of outliers created by edge discontinuities, multiple motions, and specularities in image data. [sent-23, score-0.853]
</p><p>16 The attempt to avoid outlier sensitivity has led many to propose bounded loss functions [8, 15, 18, 19, 25] to replace the standard convex, unbounded losses deployed in support vector machines and boosting [9] respectively. [sent-25, score-1.051]
</p><p>17 In fact, [16] has shown that minimizing 1  any convex margin loss cannot achieve robustness to random misclassiﬁcation noise. [sent-26, score-0.733]
</p><p>18 In particular, we present a general model for bounding any convex loss function, via a process of “loss clipping”, that ensures bounded sensitivity to outliers. [sent-29, score-0.702]
</p><p>19 Although the resulting optimization problem is not, by itself, convex, we demonstrate an efﬁcient convex relaxation and rounding procedure that guarantees bounded response to data—a guarantee that cannot be established for any convex loss minimization on its own. [sent-30, score-0.973]
</p><p>20 The approach we propose is generic and can be applied to any standard loss function, be it for regression or classiﬁcation. [sent-31, score-0.43]
</p><p>21 Our work is inspired by a number of studies that have investigated robust estimators in computer vision and machine learning [2, 26, 27, 30]. [sent-32, score-0.328]
</p><p>22 However, these previous attempts were either hampered by local optimization or restricted to special cases; none had guarantees of global training and outlier insensitivity. [sent-33, score-0.37]
</p><p>23 For example, work on “robust optimization” [28, 29] considers minimizing the worst case loss achieved given prespeciﬁed bounds on the maximum data deviation that will be considered. [sent-35, score-0.37]
</p><p>24 Although interesting, these results do not directly bear on the question at hand since we explicitly do not bound the magnitude of the outliers (i. [sent-36, score-0.306]
</p><p>25 We also do not focus on asymptotic or inﬁnitesimal notions from robust statistics, such as inﬂuence functions [11], nor impose boundedness assumptions on the domain and range of the data or the predictor [5, 6]. [sent-42, score-0.334]
</p><p>26 Normally the loss function L is chosen to be convex in θ so that the minimization problem can be solved efﬁciently. [sent-47, score-0.519]
</p><p>27 Although convexity is important for computational tractability, it has the undesired side-effect of causing unbounded outlier sensitivity, as mentioned. [sent-48, score-0.352]
</p><p>28 Nevertheless, our goal in this paper will be to eliminate unbounded sensitivity for convex loss functions while retaining a scalable computational approach. [sent-50, score-0.771]
</p><p>29 1 Standard Convex Loss Functions: Our general construction applies to arbitrary convex losses, but we will demonstrate our methods on standard loss functions employed in regression and classiﬁcation. [sent-51, score-0.659]
</p><p>30 A standard example is Bregman divergences, which are deﬁned by taking a strongly convex differentiable potential Φ then taking the difference between the potential and its ﬁrst order Taylor approximation, obtaining a loss LΦ (ˆ y) = Φ(ˆ) − Φ(y) − φ(y)(ˆ − y), where y y y φ(y) = Φ′ (y) [1, 14]. [sent-52, score-0.581]
</p><p>31 Several natural loss functions can be deﬁned this way, including least squares LΦ (ˆ y) = (ˆ − y)2 /2, using the potential Φ(y) = y 2 /2, and forward KL-divergence LΦ (ˆ y) = y y y ˆ y y ln y + (1 − y ) ln 1−ˆ , using the potential Φ(y) = y ln y + (1 − y) ln(1 − y) for 0 ≤ y ≤ 1. [sent-53, score-0.663]
</p><p>32 2  A related construction is matching losses [14], which are determined by taking a strictly increasing differentiable transfer function f to be used in prediction via y = f (z) where z = x⊤ θ. [sent-55, score-0.305]
</p><p>33 Then, given ˆ z ˆ a transfer f , a loss can be deﬁned by LF (ˆ z) = z f (ζ) − f (z) dζ = F (ˆ) − F (z) − f (z)(ˆ − z) z z z such that F satisﬁes F ′ (z) = f (z). [sent-56, score-0.4]
</p><p>34 These two loss constructions are related by the equality LΦ (y y ) = LF (ˆ z) where F is the Legendre-Fenchel ˆ z conjugate of Φ [4, §3. [sent-58, score-0.331]
</p><p>35 For example, the ˆ y y 1−y y post-prediction KL-divergence y ln y + (1 − y) ln 1−ˆ is equal to the convex pre-prediction loss ˆ y ˆ z LF (ˆ z) = ln(ez + 1) − ln(ez + 1) − σ(z)(ˆ − z) via the transfer y = σ(ˆ) = (1 + e−ˆ)−1 . [sent-60, score-0.716]
</p><p>36 Such z z ˆ z losses are prevalent in regression and probabilistic classiﬁcation settings. [sent-61, score-0.26]
</p><p>37 Although a step ˆ ˆ transfer does not admit the matching loss construction, a surrogate margin loss can be obtained by taking a nonincreasing function l such that limm→∞ l(m) = 0, then deﬁning Ll (ˆ, y) = l(y y ). [sent-63, score-0.779]
</p><p>38 If the margin y y loss is furthermore chosen to be convex, efﬁcient minimization can be attained. [sent-66, score-0.411]
</p><p>39 In each case, the loss is convex in the parameters θ. [sent-68, score-0.487]
</p><p>40 Note that by their very convexity these losses cannot be robust: all admit unbounded sensitivity to a single outlier (the same is also true for L1 loss when applied to regression). [sent-69, score-0.978]
</p><p>41 Bounded loss functions: As observed, non-convex loss functions are necessary to bound the effects of outliers [17]. [sent-70, score-1.0]
</p><p>42 Black and Rangarajan [2] provide a useful catalog of bounded and redescending loss functions for robust regression, of which a representative example is the Geman and McClure loss L(y, y ) = (ˆ − y)2 /(τ + (ˆ − y)2 ) for τ > 0; see Figure 1. [sent-71, score-1.048]
</p><p>43 It therefore appears that bounded loss functions achieve robustness at the cost of losing global training guarantees. [sent-73, score-0.7]
</p><p>44 Our goal is to show that robustness and efﬁcient global training are not mutually exclusive. [sent-74, score-0.256]
</p><p>45 Despite extensive research on regression and classiﬁcation, almost no work we are aware of (save perhaps [30] in a limited way) attempts to reconcile robustness to outliers with global training algorithms. [sent-75, score-0.629]
</p><p>46 3  Loss Clipping  Adapting the ideas of [2, 27, 30], given any convex loss ℓ(y, x⊤ θ) deﬁne the clipped loss as ℓc (y, x⊤ θ)  = min(1, ℓ(y, x⊤ θ)). [sent-76, score-1.038]
</p><p>47 (2)  Figure 1 demonstrates loss clipping for some standard loss functions. [sent-77, score-0.754]
</p><p>48 Given a clipped loss, a robust form of training problem (1) can be written as min θ  γ θ 2  n 2 2  ℓc (yi , Xi: θ). [sent-78, score-0.538]
</p><p>49 To make progress on the computational question we exploit a key observation: for any loss function, its corresponding clipped loss can be indirectly expressed by an auxiliary optimization of a smooth objective (if the original loss function itself was smooth). [sent-81, score-1.213]
</p><p>50 That is, given a loss ℓ(y, x⊤ θ) deﬁne the corresponding ρ-relaxed loss to be ℓρ (y, x⊤ θ)  =  ρℓ(y, x⊤ θ) + 1 − ρ  (4)  for 0 ≤ ρ ≤ 1; see Figure 1. [sent-82, score-0.662]
</p><p>51 This construction is an instance of an outlier process as described in [2] and is motivated by a special case hinge-loss construction originally proposed in [30]. [sent-83, score-0.314]
</p><p>52 The 3  loss 0 y−y ˆ  0 yy ˆ  0 yy ˆ  Figure 1: Comparing standard losses (dashed) with corresponding “clipped” losses (solid), ρ-relaxed losses (dotted), and non-convex robust losses (dash-dotted). [sent-84, score-1.421]
</p><p>53 Left: squared loss (dashed), clipped (solid), 1/3-relaxed (dotted), robust Geman and McClure loss [2] (dash-dotted). [sent-85, score-1.104]
</p><p>54 Center: SVM hinge loss (dashed), clipped [27, 30] (solid), 1/2-relaxed (upper dotted), robust 1 − tanh(y y ) loss [19] (dash-dotted). [sent-86, score-1.134]
</p><p>55 Right: Adaboost exponential ˆ loss (dashed), clipped (solid), 1/2-relaxed (upper dotted), robust 1 − tanh(y y ) loss [19] (dash-dotted). [sent-87, score-1.104]
</p><p>56 ˆ ρ-relaxation provides a convenient characterization of any clipped loss, since it can be shown in general that minimizing a corresponding ρ-relaxed loss is equivalent to minimizing the clipped loss. [sent-88, score-0.849]
</p><p>57 Proposition 1 For any loss function ℓ(y, x⊤ θ), we have ℓc (y, x⊤ θ) = min0≤ρ≤1 ℓρ (y, x⊤ θ). [sent-89, score-0.331]
</p><p>58 (5)  Unfortunately, the resulting problem is not jointly convex in ρ and θ even though it is convex in each given the other. [sent-92, score-0.312]
</p><p>59 Such marginal convexity might suggest that an alternating minimization strategy, however the proof of Proposition 1 shows that each minimization over ρ will result in ρi = 0 for losses greater than 1, or ρi = 1 for losses less than 1. [sent-93, score-0.426]
</p><p>60 4  A Convex Relaxation  One contribution of this paper is to derive an exact reformulation of (5) that admits a convex relaxation and rounding scheme that retain bounded sensitivity to outliers. [sent-95, score-0.628]
</p><p>61 We ﬁrst show how the relaxation can be efﬁciently solved by a scalable algorithm that eliminates any need for semideﬁnite programming, then provide a guarantee of bounded outlier sensitivity in Section 5. [sent-96, score-0.584]
</p><p>62 (8)  θ  By construction, ℓ∗ (y, α) is guaranteed to be convex in α since it is a pointwise maximum over linear functions [4, §3. [sent-101, score-0.22]
</p><p>63 Lemma 1 For any convex differentiable loss function ℓ(y, x⊤ θ) such that the level sets of ℓα (v) = αx⊤ (θ − v) + ℓ(y, x⊤ v) are bounded, we have ℓ(y, x⊤ θ)  =  sup αx⊤ θ − ℓ∗ (y, α). [sent-103, score-0.583]
</p><p>64 Next enforce the constraint δ(M ) = n+1 1 with a Lagrange multiplier λ: (14)  =  sup α, λ  =  M  min  0, tr(M )=1  −(n + 1) tr(M T (α)) + λ⊤ (1 − (n + 1)δ(M ))  sup λ⊤ 1 − (n + 1)  α, λ  M  max  0, tr(M )=1  tr M (T (α) + ∆(λ)) . [sent-120, score-0.256]
</p><p>65 (15) (16)  This relaxed formulation (16) is now amenable to efﬁcient global optimization: The outer problem is jointly concave in α and λ, since it is a pointwise minimum of concave functions. [sent-121, score-0.276]
</p><p>66 Although the outer problem is not smooth, many effective methods exist for nonsmooth convex optimization [20, 31]. [sent-126, score-0.258]
</p><p>67 Unfortunately, the relaxation step taken in (13) means that the solution to (14) (recovered from the ν that solves (17)) does not necessarily solve (10): the inner solution ν in (17) might not be unique. [sent-133, score-0.263]
</p><p>68 More typically, however, the maximum eigenvector is not unique at (α∗ , λ∗ ), meaning that a gap has been introduced—this occurs if and only if the inner solution M ∗ to (14) is not rank 1. [sent-135, score-0.209]
</p><p>69 5  Bounding Outlier Sensitivity  Thus far we have proposed a robust training objective, provided an efﬁcient convex relaxation that establishes a lower bound, and proposed a simple rounding method for recovering an approximate solution. [sent-151, score-0.653]
</p><p>70 The question remains as to whether the approximate solution retains bounded sensitivity to outliers (or to leverage points [23, §1. [sent-152, score-0.574]
</p><p>71 Nevertheless one would still like to guarantee that the gap stays bounded in the presence of arbitrary outliers and leverage points. [sent-157, score-0.442]
</p><p>72 Furthermore, if the unclipped loss ℓ(y, y ) is b-Lipschitz in y for b < ∞ and either y or ˆ ˆ K remains bounded, then there exists a c < ∞ such that R(ˆ , θ) ≤ c. [sent-159, score-0.331]
</p><p>73 ρ ˆ That is, the ρ-relaxed loss obtained by the rounded solution stays bounded in this case, even when accounting for the proposed relaxation and rounding procedure and data perturbation. [sent-160, score-0.749]
</p><p>74 That is, the tightest convex upper bound for any convex loss function is simply given by the function itself, which corresponds to setting ρi = 1 for every training example. [sent-164, score-0.733]
</p><p>75 The resulting upper bound on the clipped loss (hence on the misclassiﬁcation error in the margin loss case) is much tighter than that achieved by simply minimizing a convex loss. [sent-166, score-1.157]
</p><p>76 Test error rates (RMSE) on clean data (average ± standard deviations) at different outlier probabilities p, 20 repeats. [sent-215, score-0.318]
</p><p>77 The bottom row shows the relative gap obtained between the ρ-relaxed loss of the rounded solution and the computed lower bound (16). [sent-216, score-0.528]
</p><p>78 We compare the behaviours of standard regression losses: least-squares (L2), L1 (L1), the Huber minimax loss (HuberM) [13, 17], and the robust Geman and McClure loss (GM) [2]. [sent-221, score-0.983]
</p><p>79 To these we compare the proposed relaxed method (ClipRelax), along with an alternating minimizer of the clipped loss (ClipAlt). [sent-222, score-0.616]
</p><p>80 Note that the robust GM loss ﬁnds two different minima, corresponding to that of L2 and ClipRelax respectively, hence it was not depicted in the plot. [sent-227, score-0.553]
</p><p>81 We then seed the data set with outliers 4 by randomly re-sampling each yi (and Xi: ) from N (0, 105 ) and N (0, 102 ) respectively, governed by an outlier probability p. [sent-232, score-0.562]
</p><p>82 Clearly, the outliers signiﬁcantly affect the performance of least squares. [sent-237, score-0.274]
</p><p>83 Interestingly, this experiment shows that the relative gap between the ρ-robust loss obtained by the proposed method and the lower bound on the optimal ρ-robust loss (16) remains remarkably small, indicating our robust relaxation (almost) optimally minimizes the original non-convex clipped loss. [sent-239, score-1.28]
</p><p>84 We chose three data sets: astronomy data containing outliers from [23], and two UCI data sets, seeding the the UCI data sets with outliers. [sent-308, score-0.319]
</p><p>85 For UCI data, outliers were added by resampling Xi: and yi from N (0, 1000), with 5% outliers. [sent-310, score-0.33]
</p><p>86 Here we estimated the scale using the mean absolute deviation, a robust approach commonly used in the robust statistics literature [17]. [sent-313, score-0.474]
</p><p>87 Unsurprisingly, the classical robust loss functions, L1 and HuberM, perform better than L2 in the presence of outliers, but not as well as ClipRelax. [sent-316, score-0.588]
</p><p>88 the logit, or binomial deviance loss [12]) and the robust 1 − tanh loss [19] in a classiﬁcation context. [sent-319, score-1.047]
</p><p>89 From these results one can conclude that ClipRelax is more robust than standard logit training. [sent-323, score-0.292]
</p><p>90 Training with logit loss is slightly better than the tanh loss algorithm in terms of training loss, but not very signiﬁcantly. [sent-324, score-0.871]
</p><p>91 It is interesting to see that when the prediction error is measured on clean labels ClipRelax generalizes signiﬁcantly better than the robust 1−tanh loss. [sent-325, score-0.308]
</p><p>92 This implies that the classiﬁcation model produced by ClipRelax is closer to the true model despite of the presence of outliers, demonstrating that the proposed method can be robust in a simple classiﬁcation context. [sent-326, score-0.222]
</p><p>93 7  Conclusion  We have proposed a robust estimation method for regression and classiﬁcation based on a notion of “loss-clipping”. [sent-327, score-0.321]
</p><p>94 The key beneﬁt is competitive (or better) estimation quality than the state-of-the-art in robust estimation, while ensuring provable robustness to outliers and computable bounds on the optimality gap. [sent-329, score-0.655]
</p><p>95 It would be interesting to investigate whether the techniques developed can also be applied to other forms of robust estimators from the classical literature, including GM, MM, L, R and S estimators [11, 13, 17, 23]. [sent-331, score-0.397]
</p><p>96 On the uniﬁcation of line processes, outlier rejection, and robust statistics with applications in early vision. [sent-344, score-0.454]
</p><p>97 On robustness properties of convex risk minimization methods for pattern recognition. [sent-361, score-0.347]
</p><p>98 Consistency and robustness of kernel-based regression in convex risk minimization. [sent-366, score-0.414]
</p><p>99 On the design of loss functions for classiﬁcation: theory, robustness to outliers, and SavageBoost. [sent-434, score-0.522]
</p><p>100 Robust support vector machine training via convex outlier ablation. [sent-501, score-0.446]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cliprelax', 0.332), ('loss', 0.331), ('outliers', 0.274), ('outlier', 0.232), ('clipalt', 0.23), ('robust', 0.222), ('clipped', 0.22), ('losses', 0.161), ('robustness', 0.159), ('convex', 0.156), ('huberm', 0.153), ('sensitivity', 0.134), ('rounding', 0.118), ('yy', 0.112), ('gm', 0.11), ('regression', 0.099), ('relaxation', 0.099), ('tr', 0.094), ('clipping', 0.092), ('clean', 0.086), ('tanh', 0.081), ('bounded', 0.081), ('ln', 0.08), ('unbounded', 0.08), ('inner', 0.078), ('rounded', 0.077), ('mcclure', 0.077), ('supplement', 0.074), ('lf', 0.073), ('outer', 0.071), ('logit', 0.07), ('estimators', 0.07), ('transfer', 0.069), ('bregman', 0.065), ('sup', 0.062), ('ll', 0.058), ('training', 0.058), ('yi', 0.056), ('huber', 0.055), ('classi', 0.054), ('concave', 0.051), ('deviance', 0.051), ('redescending', 0.051), ('rousseeuw', 0.051), ('deviations', 0.05), ('adaboost', 0.049), ('geman', 0.049), ('margin', 0.048), ('misclassi', 0.047), ('boundedness', 0.047), ('stability', 0.046), ('inliers', 0.045), ('astronomy', 0.045), ('christmann', 0.045), ('gap', 0.045), ('solution', 0.043), ('eigenvector', 0.043), ('xu', 0.043), ('leverage', 0.042), ('solid', 0.042), ('unfortunately', 0.041), ('construction', 0.041), ('tt', 0.041), ('local', 0.041), ('convexity', 0.04), ('reformulation', 0.04), ('dotted', 0.04), ('minimizing', 0.039), ('global', 0.039), ('ez', 0.039), ('trapped', 0.039), ('scalable', 0.038), ('min', 0.038), ('caramanis', 0.037), ('investigated', 0.036), ('dashed', 0.036), ('minima', 0.036), ('classical', 0.035), ('divergences', 0.035), ('wiley', 0.035), ('differentiable', 0.034), ('recover', 0.034), ('duality', 0.033), ('predictor', 0.033), ('minimizer', 0.033), ('pointwise', 0.032), ('minimization', 0.032), ('functions', 0.032), ('bound', 0.032), ('relaxed', 0.032), ('solvable', 0.031), ('nonsmooth', 0.031), ('binomial', 0.031), ('uci', 0.031), ('understood', 0.031), ('literature', 0.03), ('cation', 0.03), ('recovered', 0.03), ('hinge', 0.03), ('potential', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="225-tfidf-1" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>2 0.27097338 <a title="225-tfidf-2" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Sujay Sanghavi</p><p>Abstract: Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efﬁciently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative ﬁltering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efﬁcient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisﬁed, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identiﬁes the corrupted points. Such identiﬁcation of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and ﬁnancial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.</p><p>3 0.1831174 <a title="225-tfidf-3" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>4 0.16568916 <a title="225-tfidf-4" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>Author: Nathan Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: √ ˜ We establish an excess risk bound of O HR2 + HL∗ Rn for ERM with an H-smooth loss n function and a hypothesis class with Rademacher complexity Rn , where L∗ is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = R/n, this translates to ˜ ˜ a learning rate of O (RH/n) in the separable (L∗ = 0) case and O RH/n + L∗ RH/n more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective. 1</p><p>5 0.14267041 <a title="225-tfidf-5" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>6 0.11818224 <a title="225-tfidf-6" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>7 0.10788964 <a title="225-tfidf-7" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>8 0.10125393 <a title="225-tfidf-8" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>9 0.098705955 <a title="225-tfidf-9" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>10 0.096990541 <a title="225-tfidf-10" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>11 0.092310116 <a title="225-tfidf-11" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>12 0.089363642 <a title="225-tfidf-12" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>13 0.087861098 <a title="225-tfidf-13" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>14 0.087713115 <a title="225-tfidf-14" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>15 0.085507311 <a title="225-tfidf-15" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>16 0.085258186 <a title="225-tfidf-16" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>17 0.081371762 <a title="225-tfidf-17" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>18 0.081261314 <a title="225-tfidf-18" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>19 0.080918916 <a title="225-tfidf-19" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>20 0.079288229 <a title="225-tfidf-20" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.05), (2, 0.166), (3, -0.022), (4, 0.09), (5, 0.019), (6, -0.083), (7, -0.002), (8, -0.047), (9, -0.016), (10, 0.011), (11, -0.027), (12, 0.152), (13, 0.097), (14, -0.024), (15, 0.068), (16, 0.068), (17, 0.13), (18, 0.054), (19, -0.019), (20, 0.156), (21, -0.063), (22, 0.07), (23, -0.075), (24, 0.032), (25, -0.006), (26, 0.109), (27, 0.148), (28, 0.196), (29, 0.08), (30, -0.113), (31, 0.031), (32, -0.076), (33, -0.09), (34, 0.134), (35, 0.02), (36, -0.154), (37, -0.039), (38, 0.061), (39, -0.126), (40, 0.035), (41, 0.036), (42, -0.056), (43, -0.113), (44, -0.103), (45, -0.015), (46, 0.036), (47, -0.008), (48, 0.157), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95577055 <a title="225-lsi-1" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>2 0.75410169 <a title="225-lsi-2" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>3 0.7303952 <a title="225-lsi-3" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>4 0.72805136 <a title="225-lsi-4" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Sujay Sanghavi</p><p>Abstract: Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efﬁciently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative ﬁltering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efﬁcient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisﬁed, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identiﬁes the corrupted points. Such identiﬁcation of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and ﬁnancial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.</p><p>5 0.63878715 <a title="225-lsi-5" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>Author: Uwe Dick, Peter Haider, Thomas Vanck, Michael Brückner, Tobias Scheffer</p><p>Abstract: We study a setting in which Poisson processes generate sequences of decisionmaking events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efﬁciently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service. 1</p><p>6 0.58551913 <a title="225-lsi-6" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>7 0.56730753 <a title="225-lsi-7" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>8 0.53384173 <a title="225-lsi-8" href="./nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">191 nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>9 0.52927577 <a title="225-lsi-9" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>10 0.52673697 <a title="225-lsi-10" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>11 0.51966649 <a title="225-lsi-11" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>12 0.50928235 <a title="225-lsi-12" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>13 0.50846589 <a title="225-lsi-13" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>14 0.47748333 <a title="225-lsi-14" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>15 0.47579771 <a title="225-lsi-15" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>16 0.47279412 <a title="225-lsi-16" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>17 0.47045109 <a title="225-lsi-17" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>18 0.46848392 <a title="225-lsi-18" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>19 0.44869816 <a title="225-lsi-19" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>20 0.4469263 <a title="225-lsi-20" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.043), (17, 0.011), (27, 0.047), (30, 0.062), (35, 0.021), (45, 0.235), (50, 0.057), (52, 0.101), (60, 0.057), (77, 0.066), (78, 0.015), (90, 0.05), (97, 0.164)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92769927 <a title="225-lda-1" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>Author: Fabian L. Wauthier, Michael I. Jordan</p><p>Abstract: Heavy-tailed distributions are often used to enhance the robustness of regression and classiﬁcation methods to outliers in output space. Often, however, we are confronted with “outliers” in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed stochastic processes (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classiﬁcation estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs when the marginals of the heavy-tailed process have sufﬁciently heavy tails. The analysis is complemented by experiments on biological data which indicate signiﬁcant improvements of estimates in sparse regions while producing competitive results in dense regions. 1</p><p>2 0.90501857 <a title="225-lda-2" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>Author: Gilbert Leung, Novi Quadrianto, Kostas Tsioutsiouliklis, Alex J. Smola</p><p>Abstract: We present a fast online solver for large scale parametric max-ﬂow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-ﬂow problems by performing stochastic gradient descent on a set of ﬂows. We apply the algorithm to optimize tier arrangement of over 84 million web pages on a layered set of caches to serve an incoming query stream optimally. 1</p><p>same-paper 3 0.87519711 <a title="225-lda-3" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>4 0.85068524 <a title="225-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.84837896 <a title="225-lda-5" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Many statistical M -estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer. We analyze the convergence rates of ﬁrst-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We deﬁne appropriately restricted versions of these conditions, and show that they are satisﬁed with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov’s ﬁrst-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ∗ and the optimal solution θ. This globally linear rate is substantially faster than previous analyses of global convergence for speciﬁc methods that yielded only sublinear rates. Our analysis applies to a wide range of M -estimators and statistical models, including sparse linear regression using Lasso ( 1 regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational eﬃciency in high-dimensional estimation. 1</p><p>6 0.83862537 <a title="225-lda-6" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>7 0.83829153 <a title="225-lda-7" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>8 0.83742708 <a title="225-lda-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.83731544 <a title="225-lda-9" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>10 0.83722585 <a title="225-lda-10" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>11 0.83686638 <a title="225-lda-11" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>12 0.83555615 <a title="225-lda-12" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>13 0.83336747 <a title="225-lda-13" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>14 0.83315521 <a title="225-lda-14" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>15 0.83143467 <a title="225-lda-15" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>16 0.83015579 <a title="225-lda-16" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>17 0.82718492 <a title="225-lda-17" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>18 0.82682145 <a title="225-lda-18" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>19 0.82673377 <a title="225-lda-19" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>20 0.8253932 <a title="225-lda-20" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
