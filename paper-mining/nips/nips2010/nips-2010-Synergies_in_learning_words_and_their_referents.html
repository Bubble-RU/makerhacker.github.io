<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>264 nips-2010-Synergies in learning words and their referents</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-264" href="#">nips2010-264</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>264 nips-2010-Synergies in learning words and their referents</h1>
<br/><p>Source: <a title="nips-2010-264-pdf" href="http://papers.nips.cc/paper/3946-synergies-in-learning-words-and-their-referents.pdf">pdf</a></p><p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>Reference: <a title="nips-2010-264-reference" href="../nips2010_reference/nips-2010-Synergies_in_learning_words_and_their_referents_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. [sent-14, score-0.503]
</p><p>2 These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. [sent-16, score-0.444]
</p><p>3 We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. [sent-18, score-0.327]
</p><p>4 1  Introduction  Conventional views of language acquisition often assume that human language learners initially use a single source of information to acquire one component of language, which they then use to leverage the acquisition of other linguistic components. [sent-19, score-0.5]
</p><p>5 For example, Kuhl [1] presents a standard “bootstrapping” view of early language acquisition in which successively more difﬁcult tasks are addressed by learners, beginning with phoneme inventory and progressing to word segmentation and word learning. [sent-20, score-1.273]
</p><p>6 , Graf Estes et al [2], who showed that infants were more successful in mapping novel objects to novel words after those words had been successfully segmented from the speech stream. [sent-23, score-0.469]
</p><p>7 Computationally speaking, an interactive account views language acquisition as a joint inference problem for all components of language simultaneously, rather than a discrete sequence of inference problems for individual language components. [sent-25, score-0.499]
</p><p>8 (We are thus using “interactive” to refer to the way that language acquisition is formulated as an inference problem, rather than a speciﬁc mechanism or architecture as in [3]). [sent-26, score-0.296]
</p><p>9 , where improvements in the acquisition of component A also improves the acqui1  PIG|DOG  i z D & t D e p I g PIG  Figure 1: The photograph indicates non-linguistic context containing the (toy) pig and dog for the utterance Is that the pig? [sent-31, score-0.588]
</p><p>10 The objects in the non-linguistic context are indicated by the preﬁx “PIG|DOG”, which is followed by the unsegmented phonemicised input. [sent-34, score-0.18]
</p><p>11 The possible word segmentation points are indicated by separators between the phonemes. [sent-35, score-0.604]
</p><p>12 The correct word segmentation is indicated by the ﬁlled blue word separators, and the mapping between words and non-linguistic objects is indicated by the underbrace subscript. [sent-37, score-1.17]
</p><p>13 sition of component B, and improvements in the acquisition of component B also improves the acquisition of component A. [sent-38, score-0.302]
</p><p>14 In this paper we focus on the acquisition of two of the simpler aspects of language: (i) segmenting sentences into words (thereby identifying their pronunciations), and (ii) the relationship between words and the objects they refer to. [sent-40, score-0.504]
</p><p>15 The acquisition of word pronunciations is viewed as a segmentation problem as follows. [sent-43, score-0.762]
</p><p>16 Following Elman [4] and Brent [5, 6], a corpus of child-directed speech is “phonemicised” by looking each word up in a pronouncing dictionary and concatenating those pronunciations. [sent-44, score-0.452]
</p><p>17 For example, the mother’s utterance Is that the pig is mapped to the broad phonemic representation Iz D&t; D6 pIg (in an ASCII-based broad phonemic encoding), which are then concatenated to form IzD&tD6pIg. [sent-45, score-0.348]
</p><p>18 ; The word segmentation task is to segment a corpus of such unsegmented utterance representations into words, thus identifying the pronunciations of the words in the corpus. [sent-46, score-0.94]
</p><p>19 We study the acquisition of the relationship between words and the objects they refer to using the framework proposed by Frank et al [7]. [sent-47, score-0.55]
</p><p>20 Here each utterance in the corpus is labelled with the contextually-relevant objects that the speaker might be referring to. [sent-48, score-0.34]
</p><p>21 For example, in the context of Figure 1, the utterance would be labelled with the two contextually-relevant objects PIG and DOG. [sent-50, score-0.29]
</p><p>22 Jones et al [8] combined the word segmentation and word reference tasks into a single inference task, where the goal is to simultaneously segment the utterance into words, and to map a subset of the words of each utterance to the utterance’s contextually-relevant objects. [sent-52, score-1.564]
</p><p>23 The next section summarises previous work on word segmentation and learning the relationship between words and their referents. [sent-55, score-0.71]
</p><p>24 Section 3 introduces Adaptor Grammars, explains how they can be used for word segmentation and topic modelling, and presents the Adaptor Grammars that will be used in this paper. [sent-56, score-0.621]
</p><p>25 Section 4 presents experimental 2  results showing synergistic interactions between word segmentation and learning the relationship between words and the objects they refer to, while section 5 summarises and concludes the paper. [sent-57, score-0.841]
</p><p>26 2  Previous work  Word segmentation has been studied using a wide variety of computational perspectives. [sent-58, score-0.196]
</p><p>27 Elman [4] and Brent [5, 6] introduced the basic word segmentation paradigm investigated here. [sent-59, score-0.578]
</p><p>28 Johnson et al [11] introduced a generalisation of Probabilistic Context-Free Grammars (PCFGs) called Adaptor Grammars (AGs) as a framework for specifying HDPs for linguistic applications (because this paper relies heavily on AGs we describe them in more detail in section 3 below). [sent-68, score-0.2]
</p><p>29 Johnson [12] investigated AGs for word segmentation that capture a range of different kinds of generalisations. [sent-69, score-0.578]
</p><p>30 The unigram AG replicates the unigram segmentation model of Goldwater et al, and suffers from the same undersegmentation problems. [sent-70, score-0.496]
</p><p>31 The acquisition of the mapping between words and the objects they refer to was studied by Frank et al [7]. [sent-72, score-0.514]
</p><p>32 They used a modiﬁed version of the LDA topic model [13] where the “topics” are contextually-relevant objects that words in the utterance can refer to, so the mapping from “topics” to words effectively speciﬁes which words refer to these contextually-salient objects. [sent-73, score-0.59]
</p><p>33 Jones et al [8] integrated the Frank et al “topic” model of the word-object relationship with the unigram model of Goldwater et al to obtain a joint model that both performs word segmentation and also learns which words refer to which contextually-salient objects. [sent-74, score-1.378]
</p><p>34 We use this reduction to express Frank et al models [7] of the word to object relationship as AGs which also incorporate Johnson’s [12] models of word segmentation. [sent-76, score-1.01]
</p><p>35 The resulting AGs can express a wide range of joint HDP models of word segmentation and the word-object relationship, including the model proposed by Jones et al [8], as well as several generalisations. [sent-77, score-0.744]
</p><p>36 Informally, θA→α is the probability of a node labelled A expanding to a sequence of nodes labelled α, and the probability of a tree is the product of the probabilities of the rules used to construct each non-leaf node in it. [sent-95, score-0.228]
</p><p>37 Kurihara et al [17] describe a Variational Bayes algorithm for inferring PCFGs using a mean-ﬁeld approximation, while Johnson et al [18] describe a Markov Chain Monte Carlo algorithm based on Gibbs sampling. [sent-126, score-0.332]
</p><p>38 2  Modelling word-object reference using PCFGs  This section presents a novel encoding of a Frank et al [7] model for identifying word-object relationships as a PCFG. [sent-128, score-0.211]
</p><p>39 Because the Frank et al [7] model of the word-object relationship is very similiar to an LDA topic model, we can use the same techniques to design Bayesian PCFGs that infer word-object relationships. [sent-131, score-0.307]
</p><p>40 The models we investigate in this paper assume that the words in a single sentence refer to at most one non-linguistic object (although it would be easy to relax this restriction). [sent-132, score-0.376]
</p><p>41 Let O = O ∪ {∅}, where ∅ is a distinguished “null object” not in O, and let the nonterminals N = {S} ∪ {Ao , Bo : o ∈ O }, where Ao and Bo are nonterminals indexed by the o ∈ O. [sent-136, score-0.198]
</p><p>42 Informally, a nonterminal Bo expanding to word w ∈ V indicates that w refers to object o, while a B∅ expanding to w indicates that w is non-referential. [sent-137, score-0.61]
</p><p>43 The set of objects in the non-linguistic context of an utterance is indicated by preﬁxing the utterance with a context identiﬁer associated with those objects, such as “PIG|DOG” in Figure 1. [sent-138, score-0.394]
</p><p>44 Then the terminals of the 4  S Apig XX Apig Bpig XX Apig B∅ pig XX Apig B∅ the XX Apig B∅ that PIG|DOG  is  Figure 2: A tree generated by the reference PCFG encoding a Frank et al [7] model of the wordobject relationship. [sent-143, score-0.475]
</p><p>45 The yield of this tree corresponds to the sentence Is that the pig, and the context identiﬁer is “PIG|DOG”. [sent-144, score-0.266]
</p><p>46 This grammar generates sentences consisting of a context identiﬁer followed by a sequence of words; e. [sent-147, score-0.223]
</p><p>47 Informally, the rule expanding S picks an object o that the words in the object can refer to (if o = ∅ then all words in the sentence are non-referential). [sent-150, score-0.592]
</p><p>48 The ﬁrst rule expanding Ao ensures that o is a member of that sentence’s non-linguistic context, the second rule generates a Bo that will ultimately generate a word w (which we take to indicate that w refers to o), while the third rule generates a word associated with the null object ∅. [sent-151, score-1.09]
</p><p>49 A slightly more complicated PCFG, which we call the reference1 grammar, can enforce the requirement that there is at most one referential word in each sentence. [sent-152, score-0.494]
</p><p>50 S → S B∅ S→c c∈C S → Ao B o o∈O (3) Ao → c c ∈ C, o ∈ c Ao → Ao B ∅ o ∈ O Bo → w o ∈ O ,w ∈ V In this grammar the nonterminal labels function as states that record not just which object a referential word refers to, but also whether that referential word has been generated or not. [sent-157, score-1.237]
</p><p>51 Viewed top-down, the switch from S to Ao indicates that a word from Bo has just been generated (i. [sent-158, score-0.382]
</p><p>52 3  Adaptor grammars  This subsection brieﬂy reviews adaptor grammars; for more detail see [11]. [sent-163, score-0.474]
</p><p>53 An Adaptor Grammar (AG) is a septuple (N, W, R, S, θ, A, C) consisting of a PCFG (N, W, R, S, θ) in which a subset A ⊆ N of the nonterminals are identiﬁed as adapted, and where each adapted nonterminal X ∈ A has an associated adaptor CX . [sent-164, score-0.514]
</p><p>54 An adaptor CX for X is a function that maps a distribution over trees TX to a distribution over distributions over TX . [sent-165, score-0.322]
</p><p>55 Generating a tree associated with an adapted nonterminal involves either reusing an already generated tree from the cache, or else generating a “fresh” tree as in a PCFG. [sent-186, score-0.234]
</p><p>56 4  Word segmentation with adaptor grammars  AGs can be used as models of word segmentation, which we brieﬂy review here; see Johnson [12] for more details. [sent-188, score-1.052]
</p><p>57 (with its correct segmentation indicated in blue) is as follows: i z D & t D e p I g We can represent any possible segmentation of any possible sentence as a tree generated by the following unigram AG. [sent-191, score-0.781]
</p><p>58 The trees generated by this adaptor grammar are the same as the trees generated by the CFG rules. [sent-195, score-0.465]
</p><p>59 For example, the following skeletal parse in which all but the Word nonterminals are suppressed (the others are deterministically inferrable) shows the parse that corresponds to the correct segmentation of the string above. [sent-197, score-0.404]
</p><p>60 (Word i z) (Word D & t) (Word D e) (Word p I g) Because the Word nonterminal in the AG is adapted (indicated here by underlining) the adaptor grammar learns the probability of the entire Word subtrees (e. [sent-198, score-0.522]
</p><p>61 This AG implements the unigram segmentation model of Goldwater et al [9], and as explained in section 2, it has the same tendancy to undersegment as the original unigram model. [sent-201, score-0.624]
</p><p>62 The collocation AG (5) produces a more accurate segmentation because it models (and therefore “explain away”) some of the inter-word dependencies. [sent-202, score-0.395]
</p><p>63 The collocation AG is a hierarchical process, where the base distribution for the Colloc (collocation) nonterminal adaptor is generated from the Word distribution. [sent-206, score-0.583]
</p><p>64 The collocation AG generates a sentence as a sequence of Colloc (collocation) nonterminals, each of which is a sequence of Word nonterminals. [sent-207, score-0.454]
</p><p>65 5  Adaptor grammars for joint segmentation and word-object acquisition  This section explains how to combine the word-object reference PCFGs presented in section 3. [sent-211, score-0.58]
</p><p>66 2 with the word segmentation AGs presented in section 3. [sent-212, score-0.578]
</p><p>67 Combining the word-object reference PCFGs (2) or (3) with the unigram AG (4) is relatively straight-forward; all we need to do is replace the last rule Bo → w in these grammars with Bo → Phoneme+ , i. [sent-214, score-0.397]
</p><p>68 , the Bo nonterminals expand to an arbitray sequence of phonemes, and the Bo nonterminals are adapted, so these subtrees are cached and reused as appropriate. [sent-216, score-0.198]
</p><p>69 This grammar generates a skeletal parses such as the following: (B∅ i z) (B∅ D & t) (B∅ D e) (BPIG p I g) The unigram-reference1 AG is similiar to the unigram-reference AG, except that it stipulates that at most one word per sentence is associated with a (non-null) object. [sent-218, score-0.869]
</p><p>70 It is also possible to combine the word-object reference PCFGs with the collocation AG. [sent-219, score-0.244]
</p><p>71 The collocationreference AG is a combination of the collocation AG for word segmentation and the reference PCFG for modelling the word-object relationship. [sent-221, score-0.822]
</p><p>72 It permits an arbitrary number of words in a sentence to be referential. [sent-222, score-0.293]
</p><p>73 Interestingly, there are two different reasonable ways of combining the collocation AG with the reference1 PCFG. [sent-223, score-0.199]
</p><p>74 The collocation-reference1 AG requires that at most one word in a sentence is referential, just like the reference1 PCFG (3). [sent-224, score-0.579]
</p><p>75 The collocation-referenceC1 AG is similiar to the collocation-reference1 AG, except that it requires that at most one word in a collocation is referential. [sent-225, score-0.643]
</p><p>76 This means that the collocation-referenceC1 AG permits multiple referential words in a sentence (but they must all refer to the same object). [sent-226, score-0.444]
</p><p>77 This AG is linguistically plausible because a collocation often consists of a content word, which may be referential, surrounded by function words, which are generally not referential. [sent-227, score-0.199]
</p><p>78 4  Experimental results  We used the same training corpus as Jones et al [8], which was based on the corpus collected by Fernald et al [19] annotated with the objects in the non-linguistic context by Frank et al [7]. [sent-228, score-0.686]
</p><p>79 For each sentence in each sample we extracted the word segmentation and the word-object relationships the parse implies, so we obtained 2,000 sample analyses for each sentence in the corpus. [sent-234, score-1.01]
</p><p>80 Perhaps the most basic question is: does non-linguistic context help word segmentation? [sent-238, score-0.416]
</p><p>81 Jones et al [8] investigated this question by comparing analyses from what we are calling the unigram and unigram-reference models, and failed to ﬁnd any overall effect of the non-linguistic context (although they did show that it improves the segmentation accuracy of referential words). [sent-240, score-0.639]
</p><p>82 However, as the following table shows, we do see a marked 7  improvement in word segmentation f-score when we combine non-linguistic context with the more accurate collocation models. [sent-241, score-0.811]
</p><p>83 Model unigram unigram-reference unigram-reference1 collocation collocation-reference collocation-reference1 collocation-referenceC1  word segmentation f-score 0. [sent-242, score-0.908]
</p><p>84 750  We can also ask the converse question: does better word segmentation improve sentence referent identiﬁcation? [sent-249, score-0.837]
</p><p>85 Here we measure how well the models identify which object, if any, this sentence refers to, and does not directly evaluate word segmentation accuracy. [sent-250, score-0.775]
</p><p>86 The baseline model here assigns each sentence the “null” ∅ object, achieving an accuracy of 0. [sent-251, score-0.197]
</p><p>87 We can also measure the f-score with which the models identify non-∅ sentence referents; now the trivial baseline model achieves 0 f-score. [sent-254, score-0.197]
</p><p>88 Model unigram unigram-reference unigram-reference1 collocation collocation-reference collocation-reference1 collocation-referenceC1  sentence referent accuracy 0. [sent-255, score-0.589]
</p><p>89 747  We see a marked improvement in sentence referent accuracy and sentence referent f-score with the collocation-referenceC1 AG. [sent-267, score-0.518]
</p><p>90 This is a single number that indicates how good the models are at identifying referring words and the words that they refer to. [sent-270, score-0.269]
</p><p>91 Model unigram unigram-reference unigram-reference1 colloc collocation-reference collocation-reference1 collocation-referenceC1  topical word f-score 0 0. [sent-271, score-0.612]
</p><p>92 636  Again, we ﬁnd that the collocation-referenceC1 AG identiﬁes referring words and the objects they refer to more accurately than the other models. [sent-276, score-0.235]
</p><p>93 5  Conclusion  This paper has used Adaptor Grammars (AGs) to formulate a variety of models that jointly segment utterances into words and identify the objects in the non-linguistic context that some of these words refer to. [sent-277, score-0.362]
</p><p>94 The AGs differed in the kinds of generalisations they are capable of learning, and in the relationship between word segmentation and word reference that they assume. [sent-278, score-1.041]
</p><p>95 An efﬁcient, probabilistically sound algorithm for segmentation and word discovery. [sent-307, score-0.578]
</p><p>96 Using speakers’ referential intentions to model early cross-situational word learning. [sent-311, score-0.494]
</p><p>97 A Bayesian framework for word segmentation: Exploring the effects of context. [sent-321, score-0.382]
</p><p>98 Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. [sent-340, score-0.699]
</p><p>99 PCFGs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. [sent-350, score-0.561]
</p><p>100 Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. [sent-354, score-0.864]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('word', 0.382), ('ag', 0.31), ('adaptor', 0.286), ('ao', 0.24), ('pig', 0.229), ('pcfg', 0.218), ('collocation', 0.199), ('sentence', 0.197), ('segmentation', 0.196), ('grammars', 0.188), ('ags', 0.153), ('acquisition', 0.151), ('unigram', 0.131), ('pcfgs', 0.131), ('al', 0.128), ('utterance', 0.119), ('referential', 0.112), ('bo', 0.11), ('grammar', 0.107), ('colloc', 0.099), ('nonterminals', 0.099), ('nonterminal', 0.098), ('words', 0.096), ('language', 0.082), ('phoneme', 0.08), ('gx', 0.08), ('labelled', 0.075), ('jones', 0.073), ('johnson', 0.073), ('frank', 0.062), ('objects', 0.062), ('apig', 0.062), ('referent', 0.062), ('similiar', 0.062), ('goldwater', 0.06), ('generates', 0.058), ('dog', 0.055), ('cfg', 0.055), ('interactive', 0.054), ('referents', 0.05), ('tx', 0.049), ('corpus', 0.046), ('mark', 0.045), ('reference', 0.045), ('linguistics', 0.044), ('collocations', 0.044), ('object', 0.044), ('expanding', 0.043), ('topic', 0.043), ('synergies', 0.04), ('refer', 0.039), ('association', 0.039), ('et', 0.038), ('referring', 0.038), ('parse', 0.038), ('sharon', 0.038), ('hx', 0.038), ('lda', 0.037), ('trees', 0.036), ('relationship', 0.036), ('segment', 0.035), ('tree', 0.035), ('context', 0.034), ('linguistic', 0.034), ('gs', 0.034), ('rule', 0.033), ('skeletal', 0.033), ('unsegmented', 0.033), ('brent', 0.033), ('pronunciations', 0.033), ('cx', 0.033), ('adapted', 0.031), ('synergistic', 0.03), ('parses', 0.03), ('informally', 0.03), ('north', 0.029), ('technologies', 0.029), ('dirichlet', 0.029), ('phonemes', 0.028), ('noun', 0.027), ('strings', 0.027), ('indicated', 0.026), ('xx', 0.026), ('bigram', 0.025), ('bevan', 0.025), ('bpig', 0.025), ('elman', 0.025), ('estes', 0.025), ('fernald', 0.025), ('infants', 0.025), ('macquarie', 0.025), ('nsw', 0.025), ('phonemicised', 0.025), ('tda', 0.025), ('tdx', 0.025), ('identi', 0.025), ('speech', 0.024), ('sentences', 0.024), ('null', 0.024), ('inference', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="264-tfidf-1" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>2 0.2353427 <a title="264-tfidf-2" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>3 0.15789838 <a title="264-tfidf-3" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>4 0.1556434 <a title="264-tfidf-4" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>Author: Yariv Maron, Elie Bienenstock, Michael James</p><p>Abstract: Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a hig hdimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks. 1 In trod u cti on The embedding of objects in a low-dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10, 13]. Most methods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3], embed objects of heterogeneous types, based on their co-occurrence statistics. In this paper we demonstrate that the latter can be successfully applied to unsupervised part-of-speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1, 4, 5, 6, 7]. The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features). The induction task is fully unsupervised, i.e., it uses no annotations. This task has been addressed in the past using a variety of methods. Some approaches, such as [1], combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [7, 2, 5]. Using a latent-descriptor clustering approach, [15] obtain the best results to date for distributional-only unsupervised POS tagging of the widely-used WSJ corpus. Using a heterogeneous-data embedding approach for this task, we define separate embedding functions for the objects</p><p>5 0.15346174 <a title="264-tfidf-5" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>Author: Noah A. Smith, Shay B. Cohen</p><p>Abstract: Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a ﬁxed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting. 1</p><p>6 0.089927785 <a title="264-tfidf-6" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>7 0.087344207 <a title="264-tfidf-7" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>8 0.083299972 <a title="264-tfidf-8" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>9 0.073825419 <a title="264-tfidf-9" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>10 0.064638264 <a title="264-tfidf-10" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>11 0.06231967 <a title="264-tfidf-11" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>12 0.054656353 <a title="264-tfidf-12" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>13 0.049099516 <a title="264-tfidf-13" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>14 0.048431139 <a title="264-tfidf-14" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>15 0.043949295 <a title="264-tfidf-15" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>16 0.043753672 <a title="264-tfidf-16" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>17 0.042934548 <a title="264-tfidf-17" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>18 0.041813035 <a title="264-tfidf-18" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>19 0.041495871 <a title="264-tfidf-19" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>20 0.041293714 <a title="264-tfidf-20" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.103), (1, 0.04), (2, -0.033), (3, -0.042), (4, -0.201), (5, 0.05), (6, 0.075), (7, -0.018), (8, 0.005), (9, 0.051), (10, 0.094), (11, 0.033), (12, -0.084), (13, 0.05), (14, -0.015), (15, 0.017), (16, 0.114), (17, -0.011), (18, -0.086), (19, -0.125), (20, -0.03), (21, 0.041), (22, 0.121), (23, 0.06), (24, 0.28), (25, -0.207), (26, 0.096), (27, -0.072), (28, -0.012), (29, 0.034), (30, 0.075), (31, -0.016), (32, -0.133), (33, 0.062), (34, -0.079), (35, -0.125), (36, 0.091), (37, -0.06), (38, -0.075), (39, -0.105), (40, 0.098), (41, 0.063), (42, 0.05), (43, -0.06), (44, -0.059), (45, 0.082), (46, -0.003), (47, -0.003), (48, 0.095), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97989696 <a title="264-lsi-1" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>2 0.89706635 <a title="264-lsi-2" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>3 0.70069432 <a title="264-lsi-3" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>Author: Yariv Maron, Elie Bienenstock, Michael James</p><p>Abstract: Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a hig hdimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks. 1 In trod u cti on The embedding of objects in a low-dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10, 13]. Most methods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3], embed objects of heterogeneous types, based on their co-occurrence statistics. In this paper we demonstrate that the latter can be successfully applied to unsupervised part-of-speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1, 4, 5, 6, 7]. The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features). The induction task is fully unsupervised, i.e., it uses no annotations. This task has been addressed in the past using a variety of methods. Some approaches, such as [1], combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [7, 2, 5]. Using a latent-descriptor clustering approach, [15] obtain the best results to date for distributional-only unsupervised POS tagging of the widely-used WSJ corpus. Using a heterogeneous-data embedding approach for this task, we define separate embedding functions for the objects</p><p>4 0.62580448 <a title="264-lsi-4" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>5 0.56533545 <a title="264-lsi-5" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>6 0.38671499 <a title="264-lsi-6" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>7 0.37376001 <a title="264-lsi-7" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>8 0.34913352 <a title="264-lsi-8" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>9 0.32838079 <a title="264-lsi-9" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>10 0.29883757 <a title="264-lsi-10" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>11 0.28502989 <a title="264-lsi-11" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>12 0.27993914 <a title="264-lsi-12" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>13 0.26026571 <a title="264-lsi-13" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>14 0.25850236 <a title="264-lsi-14" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>15 0.24485971 <a title="264-lsi-15" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>16 0.2371489 <a title="264-lsi-16" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>17 0.23648058 <a title="264-lsi-17" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>18 0.22931384 <a title="264-lsi-18" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>19 0.22591037 <a title="264-lsi-19" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>20 0.21492784 <a title="264-lsi-20" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.017), (22, 0.012), (27, 0.057), (30, 0.608), (45, 0.105), (50, 0.029), (52, 0.014), (77, 0.013), (78, 0.011), (90, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93948609 <a title="264-lda-1" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>Author: Hariharan Narayanan, Sanjoy Mitter</p><p>Abstract: The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of ﬁtting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume log 1 and curvature is unavoidable. Whether the known lower bound of O( k + 2 δ ) 2 for the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of 2 log 1 log4 k log 1 O( k2 + 2 δ ) to O k min k, 2 + 2 δ . Based on these results, we 2 devise a simple algorithm for k−means and another that uses a family of convex programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional data, where the sample complexity is independent of the ambient dimension. 1</p><p>same-paper 2 0.9178403 <a title="264-lda-2" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>3 0.86293292 <a title="264-lda-3" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>Author: Alexandre Bouchard-côté, Michael I. Jordan</p><p>Abstract: Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. 1</p><p>4 0.75469714 <a title="264-lda-4" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>Author: Ronny Luss, Saharon Rosset, Moni Shahar</p><p>Abstract: A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. These network ﬂow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints.</p><p>5 0.7235108 <a title="264-lda-5" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>Author: Tian Lan, Yang Wang, Weilong Yang, Greg Mori</p><p>Abstract: We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predeﬁned structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can signiﬁcantly improve activity recognition performance. 1</p><p>6 0.68009168 <a title="264-lda-6" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>7 0.63596141 <a title="264-lda-7" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>8 0.55582619 <a title="264-lda-8" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>9 0.5424704 <a title="264-lda-9" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>10 0.51999831 <a title="264-lda-10" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>11 0.51977384 <a title="264-lda-11" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>12 0.51618081 <a title="264-lda-12" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>13 0.50894952 <a title="264-lda-13" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>14 0.49463207 <a title="264-lda-14" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>15 0.4880898 <a title="264-lda-15" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>16 0.48329553 <a title="264-lda-16" href="./nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">274 nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<p>17 0.47262725 <a title="264-lda-17" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>18 0.47204566 <a title="264-lda-18" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>19 0.47146735 <a title="264-lda-19" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>20 0.47068417 <a title="264-lda-20" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
