<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 nips-2010-Direct Loss Minimization for Structured Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-61" href="#">nips2010-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 nips-2010-Direct Loss Minimization for Structured Prediction</h1>
<br/><p>Source: <a title="nips-2010-61-pdf" href="http://papers.nips.cc/paper/4069-direct-loss-minimization-for-structured-prediction.pdf">pdf</a></p><p>Author: Tamir Hazan, Joseph Keshet, David A. McAllester</p><p>Abstract: In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classiÔ¨Åcation one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptronlike learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem. 1</p><p>Reference: <a title="nips-2010-61-reference" href="../nips2010_reference/nips-2010-Direct_Loss_Minimization_for_Structured_Prediction_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yw', 0.753), ('ydirect', 0.337), ('phonem', 0.284), ('wt', 0.173), ('loss', 0.141), ('ywt', 0.139), ('timit', 0.12), ('align', 0.112), ('lim', 0.105), ('speech', 0.102), ('xt', 0.098), ('yt', 0.086), ('yk', 0.078), ('argmax', 0.074), ('output', 0.065), ('rw', 0.064), ('sw', 0.064), ('scor', 0.06), ('bleu', 0.059), ('ut', 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="61-tfidf-1" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>2 0.17301528 <a title="61-tfidf-2" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>3 0.096609935 <a title="61-tfidf-3" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>4 0.086033344 <a title="61-tfidf-4" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>5 0.083951585 <a title="61-tfidf-5" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>6 0.081030197 <a title="61-tfidf-6" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>7 0.076623045 <a title="61-tfidf-7" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>8 0.072991006 <a title="61-tfidf-8" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>9 0.065657824 <a title="61-tfidf-9" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>10 0.065619074 <a title="61-tfidf-10" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>11 0.06438607 <a title="61-tfidf-11" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>12 0.062940523 <a title="61-tfidf-12" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>13 0.062765896 <a title="61-tfidf-13" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>14 0.061892524 <a title="61-tfidf-14" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>15 0.060419314 <a title="61-tfidf-15" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>16 0.059965562 <a title="61-tfidf-16" href="./nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">188 nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>17 0.058721062 <a title="61-tfidf-17" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>18 0.058310039 <a title="61-tfidf-18" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>19 0.057882495 <a title="61-tfidf-19" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>20 0.05754539 <a title="61-tfidf-20" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.137), (1, -0.009), (2, 0.042), (3, -0.008), (4, -0.017), (5, -0.046), (6, 0.07), (7, -0.05), (8, 0.046), (9, 0.044), (10, -0.014), (11, 0.036), (12, -0.051), (13, 0.017), (14, -0.132), (15, 0.013), (16, 0.029), (17, 0.026), (18, -0.074), (19, 0.017), (20, 0.127), (21, -0.007), (22, -0.022), (23, 0.046), (24, -0.168), (25, -0.045), (26, -0.009), (27, -0.025), (28, -0.072), (29, -0.16), (30, -0.055), (31, -0.027), (32, -0.1), (33, -0.027), (34, -0.037), (35, -0.002), (36, 0.096), (37, 0.014), (38, 0.012), (39, 0.077), (40, 0.08), (41, -0.08), (42, -0.008), (43, 0.07), (44, 0.109), (45, -0.115), (46, -0.044), (47, 0.016), (48, -0.181), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85068578 <a title="61-lsi-1" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>2 0.6905179 <a title="61-lsi-2" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>3 0.51876205 <a title="61-lsi-3" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>4 0.50840288 <a title="61-lsi-4" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>5 0.4782317 <a title="61-lsi-5" href="./nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">188 nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>6 0.46521935 <a title="61-lsi-6" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>7 0.45731646 <a title="61-lsi-7" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>8 0.44024053 <a title="61-lsi-8" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>9 0.42017618 <a title="61-lsi-9" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>10 0.41457403 <a title="61-lsi-10" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>11 0.40816593 <a title="61-lsi-11" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>12 0.39981237 <a title="61-lsi-12" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>13 0.3965016 <a title="61-lsi-13" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>14 0.38989037 <a title="61-lsi-14" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>15 0.37032214 <a title="61-lsi-15" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>16 0.36838937 <a title="61-lsi-16" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>17 0.36227539 <a title="61-lsi-17" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>18 0.35032848 <a title="61-lsi-18" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>19 0.34113497 <a title="61-lsi-19" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>20 0.33715799 <a title="61-lsi-20" href="./nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">274 nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.012), (7, 0.114), (30, 0.032), (32, 0.124), (34, 0.127), (45, 0.047), (49, 0.011), (68, 0.102), (95, 0.296)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.75102949 <a title="61-lda-1" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>same-paper 2 0.72346276 <a title="61-lda-2" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>3 0.71371239 <a title="61-lda-3" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>4 0.71181124 <a title="61-lda-4" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>5 0.64229321 <a title="61-lda-5" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>6 0.62609184 <a title="61-lda-6" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>7 0.62572545 <a title="61-lda-7" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>8 0.62528205 <a title="61-lda-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.62514889 <a title="61-lda-9" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>10 0.62402481 <a title="61-lda-10" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>11 0.62367588 <a title="61-lda-11" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>12 0.62323064 <a title="61-lda-12" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>13 0.62272245 <a title="61-lda-13" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>14 0.62265128 <a title="61-lda-14" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>15 0.62252641 <a title="61-lda-15" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>16 0.62217641 <a title="61-lda-16" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>17 0.62192094 <a title="61-lda-17" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>18 0.62131673 <a title="61-lda-18" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>19 0.6209923 <a title="61-lda-19" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>20 0.62083226 <a title="61-lda-20" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
