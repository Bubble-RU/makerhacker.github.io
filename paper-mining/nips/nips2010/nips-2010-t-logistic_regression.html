<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>290 nips-2010-t-logistic regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-290" href="#">nips2010-290</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>290 nips-2010-t-logistic regression</h1>
<br/><p>Source: <a title="nips-2010-290-pdf" href="http://papers.nips.cc/paper/3928-t-logistic-regression.pdf">pdf</a></p><p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>Reference: <a title="nips-2010-290-reference" href="../nips2010_reference/nips-2010-t-logistic_regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract  We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. [sent-7, score-0.465]
</p><p>2 This gives rise to a regularized risk minimization problem with a non-convex loss function. [sent-8, score-0.254]
</p><p>3 Because of the nature of the loss function, our algorithm is tolerant to label noise. [sent-10, score-0.268]
</p><p>4 Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. [sent-11, score-0.189]
</p><p>5 1  Introduction  Many machine learning algorithms minimize a regularized risk [1]: m  J(θ) = Ω(θ) + Remp (θ), where Remp (θ) =  1 � l(xi , yi , θ). [sent-13, score-0.205]
</p><p>6 m i=1  (1)  Here, Ω is a regularizer which penalizes complex θ; and Remp , the empirical risk, is obtained by averaging the loss l over the training dataset {(x1 , y1 ), . [sent-14, score-0.165]
</p><p>7 If we deﬁne the margin of a training example (x, y) as u(x, y, θ) := y �φ(x), θ�, then many popular loss functions for binary classiﬁcation can be written as functions of the margin. [sent-19, score-0.223]
</p><p>8 (2) (3) (4) (5)  The 0 − 1 loss is non-convex and difﬁcult to handle; it has been shown that it is NP-hard to even approximately minimize the regularized risk with the 0 − 1 loss [2]. [sent-22, score-0.422]
</p><p>9 Therefore, other loss functions can be viewed as convex proxies of the 0 − 1 loss. [sent-23, score-0.265]
</p><p>10 Hinge loss leads to support vector machines (SVMs), exponential loss is used in Adaboost, and logistic regression uses the logistic loss. [sent-24, score-0.957]
</p><p>11 Convexity is a very attractive property because it ensures that the regularized risk minimization problem has a unique global optimum [3]. [sent-25, score-0.153]
</p><p>12 However, as was recently shown by Long and Servedio [4], learning algorithms based on convex loss functions are not robust to noise2 . [sent-26, score-0.3]
</p><p>13 Intuitively, the convex loss functions grows at least linearly with slope |l� (0)| as u ∈ (−∞, 0), which introduces the overwhelming impact from the data with u � 0. [sent-27, score-0.29]
</p><p>14 There has been some recent and some notso-recent work on using non-convex loss functions to alleviate the above problem. [sent-28, score-0.157]
</p><p>15 Although, the analysis of [4] is carried out in the context of boosting, we believe, the results hold for a larger class of algorithms which minimize a regularized risk with a convex loss function. [sent-31, score-0.399]
</p><p>16 By loss extending logistic regression from the exLogistic exp ponential family to the t-exponential fam6 ily, a natural extension of exponential family Hinge of distributions studied in statistical physics [6–10], we obtain the t-logistic regression 4 algorithm. [sent-33, score-1.059]
</p><p>17 Furthermore, we show that a simple block coordinate descent scheme can be used to solve the resultant regularized 2 0-1 loss risk minimization problem. [sent-34, score-0.349]
</p><p>18 Analysis of this procedure also intuitively explains why tmargin logistic regression is able to handle label -4 -2 0 2 4 noise. [sent-35, score-0.488]
</p><p>19 Figure 1: Some commonly used loss functions for binary  Our paper is structured as follows: In sec- classiﬁcation. [sent-36, score-0.157]
</p><p>20 The hinge, expotion 2 we brieﬂy review logistic regression nential, and logistic losses are convex upper bounds of the especially in the context of exponential fam- 0-1 loss. [sent-38, score-0.803]
</p><p>21 In section 3, we review t-exponential families, which form the basis for our proposed t-logistic regression algorithm introduced in section 4. [sent-40, score-0.15]
</p><p>22 In section 5 we utilize ideas from convex multiplicative programming to design an optimization strategy. [sent-41, score-0.156]
</p><p>23 2  Logistic Regression  Since we build upon the probabilistic underpinnings of logistic regression, we brieﬂy review some salient concepts. [sent-44, score-0.244]
</p><p>24 Given a family of conditional distributions parameterized by θ, using Bayes rule, and making a standard iid assumption about the data allows us to write p(θ | X, Y) = p(θ)  m �  i=1  p(yi | xi ; θ)/p(Y | X) ∝ p(θ)  m �  i=1  p(yi | xi ; θ)  (6)  where p(Y | X) is clearly independent of θ. [sent-50, score-0.208]
</p><p>25 To model p(yi | xi ; θ), consider the conditional exponential family of distributions p(y| x; θ) = exp (�φ(x, y), θ� − g(θ | x)) ,  (7)  g(θ | x) = log (exp (�φ(x, +1), θ�) + exp (�φ(x, −1), θ�)) . [sent-51, score-0.361]
</p><p>26 (8)  with the log-partition function g(θ | x) given by If we choose the feature map φ(x, y) = that p(y| x; θ) is the logistic function p(y| x; θ) =  y 2 φ(x),  and denote u = y �φ(x), θ� then it is easy to see  1 exp(u/2) = . [sent-52, score-0.244]
</p><p>27 2 i=1  (10)  Logistic regression computes a maximum a-posteriori (MAP) estimate for θ by minimizing (10) as a function of θ. [sent-55, score-0.15]
</p><p>28 Comparing (1) and (10) it is easy to see that the regularizer employed in logistic 2 regression is λ �θ� , while the loss function is the negative log-likelihood − log p(y| x; θ), which 2 thanks to (9) can be identiﬁed with the logistic loss (5). [sent-56, score-0.9]
</p><p>29 2  3  t-Exponential family of Distributions  In this section we will look at generalizations of the log and exp functions which were ﬁrst introduced in statistical physics [6–9]. [sent-57, score-0.227]
</p><p>30 The t-exponential function expt for (0 < t < 2) is deﬁned as follows: � exp(x) if t = 1 expt (x) := 1/(1−t) otherwise. [sent-60, score-1.02]
</p><p>31 Clearly, expt generalizes the usual exp function, which is recovered in the limit as t → 1. [sent-63, score-0.61]
</p><p>32 Furthermore, many familiar properties of exp are preserved: expt functions are convex, non-decreasing, non-negative and satisfy expt (0) = 1 [9]. [sent-64, score-1.121]
</p><p>33 But expt does not preserve one very important property of exp, namely expt (a + b) �= expt (a) · expt (b). [sent-65, score-2.04]
</p><p>34 One can also deﬁne the inverse of expt namely logt as � log(x) if t = 1 � (12) logt (x) := � 1−t − 1 /(1 − t) otherwise. [sent-66, score-1.076]
</p><p>35 From Figure 2, it is clear that expt decays towards 0 more slowly than the exp function for 1 < t < 2. [sent-68, score-0.585]
</p><p>36 9 3 x 0 2 -1 1 2 3 4 5 6 7 0-1 loss 1 -2 -3 -2 -1 0 1 2 x -3 -4 -2  loss 6 4 2 0  2  4  margin  Figure 2: Left: expt and Middle: logt for various values of t indicated. [sent-77, score-1.095]
</p><p>37 The right ﬁgure depicts the t-logistic loss functions for different values of t. [sent-78, score-0.157]
</p><p>38 When t = 1, we recover the logistic loss Analogous to the exponential family of distributions, the t-exponential family of distributions is deﬁned as [9, 13]: p(x; θ) := expt (�φ(x), θ� − gt (θ)) . [sent-79, score-1.465]
</p><p>39 (13)  qt (x; θ) := p(x; θ)t /Z(θ)  (14)  A prominent member of the t-exponential family is the Student’s-t distribution [14]. [sent-80, score-0.16]
</p><p>40 Just like in the exponential family case, gt the log-partition function ensures that p(x; θ) is normalized. [sent-81, score-0.484]
</p><p>41 However, no closed form solution exists for computing gt exactly in general. [sent-82, score-0.32]
</p><p>42 A closely related distribution, which often appears when working with t-exponential families is the so-called escort distribution [9, 13]: where Z(θ) = integrates to 1. [sent-83, score-0.237]
</p><p>43 �  p(x; θ) dx is the normalizing constant which ensures that the escort distribution t  Although gt (θ) is not the cumulant function of the t-exponential family, it still preserves convexity. [sent-84, score-0.468]
</p><p>44 In addition, it is very close to being a moment generating function ∇θ gt (θ) = Eqt (x;θ) [φ(x)] . [sent-85, score-0.296]
</p><p>45 8 in Sears [13] and a version specialized to the generalized exponential families appears as Proposition 5. [sent-88, score-0.179]
</p><p>46 The main difference from ∇θ g(θ) of the normal exponential family is that now ∇θ gt (θ) is equal to the expectation of its escort distribution qt (x; θ) instead of p(x; θ). [sent-90, score-0.655]
</p><p>47 (17) Even though no closed form solution exists, one can compute gt given θ and x using numerical techniques efﬁciently. [sent-92, score-0.32]
</p><p>48 If we select t satisfying −(v + 1)/2 = 1/(1 − t) and denote, �−2/(v+1) � Γ((v + 1)/2) , Ψ= √ vπΓ(v/2)σ 1/2 then by some simple but tedious calculation (included in the supplementary material) ˜ St(x|µ, σ, v) = exp (−λ(x − µ)2 /2 − gt ) ˜ (19) t  2Ψ ˜ where λ = (t − 1)vσ  and gt = ˜  Ψ−1 . [sent-95, score-0.75]
</p><p>49 (20)  Here, the degree of freedom for Student’s-t distribution is chosen such that it also belongs to the expt family, which in turn yields v = (3 − t)/(t − 1). [sent-97, score-0.51]
</p><p>50 As before, if we let φ(x, y) = y φ(x) and plot the negative log-likelihood − log p(y| x; θ), then we 2 no longer obtain a convex loss function (see Figure 2). [sent-100, score-0.262]
</p><p>51 Using (13), (18), and (11), we can further write m d ��� � � �y �� i ˜ 2 ˆ φ(xi ), θ − gt (θ | xi )) +const. [sent-104, score-0.324]
</p><p>52 ˜ 1 + (1 − t)(−λθj /2 − gt ) 1 + (1 − t)( J(θ) ∝ 2 �� �� � i=1 � � j=1 � rj (θ)  =  d �  j=1  rj (θ)  m �  li (θ)  li (θ) + const. [sent-106, score-0.378]
</p><p>53 4  Since t > 1, it is easy to see that rj (θ) > 0 is a convex function of θ. [sent-109, score-0.149]
</p><p>54 On the other hand, since gt ˆ is convex and t > 1 it follows that li (θ) > 0 is also a convex function of θ. [sent-110, score-0.512]
</p><p>55 5  Convex Multiplicative Programming  In convex multiplicative programming [18] we are interested in the following optimization problem: N � min P(θ) � zn (θ) s. [sent-113, score-0.454]
</p><p>56 θ ∈ Rd , (23) θ  n=1  where zn (θ) are positive convex functions. [sent-115, score-0.406]
</p><p>57 Clearly, (22) can be identiﬁed with (23) by setting N = d+m and identifying zn (θ) = rn (θ) for n = 1, . [sent-116, score-0.298]
</p><p>58 (24) min min MP(θ, ξ) � ξ  θ  n=1  n=1  The optimization problem in (24) is very reminiscent of logistic regression. [sent-128, score-0.244]
</p><p>59 In logistic regression, � n � �� n � � ln (θ) = − y2 φ(xn ), θ + g(θ | xn ), while here ln (θ) = 1 + (1 − t) y2 φ(xn ), θ − gt (θ | xn ) . [sent-129, score-0.766]
</p><p>60 The key difference is that in t-logistic regression each data point xn has a weight (or inﬂuence) ξn associated with it. [sent-130, score-0.223]
</p><p>61 ξ-Step: Assume that θ is ﬁxed, and denote zn = zn (θ) to rewrite (24) as: ˜ min ξ  N �  ξn zn s. [sent-135, score-0.919]
</p><p>62 Since the objective function is linear in ξ and the feasible region is a convex set, (25) is a convex optimization problem. [sent-138, score-0.242]
</p><p>63 By introducing a non-negative Lagrange multiplier γ ≥ 0, the partial Lagrangian and its gradient with respect to ξn� can be written as � � N N � � (26) L(ξ, γ) = ξ n zn + γ · 1 − ˜ ξn n=1  � ∂ L(ξ, γ) = zn� − γ ˜ ξn . [sent-139, score-0.298]
</p><p>64 ˜N  (28)  n=1  Recall that ξn in (24) is the weight (or inﬂuence) of each term zn (θ). [sent-153, score-0.298]
</p><p>65 The above analysis shows that γ = zn (θ)ξn remains constant for all n. [sent-154, score-0.298]
</p><p>66 If zn (θ) becomes very large then its inﬂuence ξn ˜ ˜ is reduced. [sent-155, score-0.298]
</p><p>67 Therefore, points with very large loss have their inﬂuence capped and this makes the algorithm robust to outliers. [sent-156, score-0.166]
</p><p>68 This step is essentially the same as logistic regression, except that each component has a weight ξ here. [sent-158, score-0.244]
</p><p>69 This requires us to compute the gradient ∇θ zn (θ):  ˜ ∇θ zn (θ) = ∇θ rn (θ) = (t − 1)λθn · en �y � n φ(xn ) − ∇θ gt (θ | xn ) for n = 1, . [sent-164, score-0.996]
</p><p>70 , m ∇θ zn+d (θ) = ∇θ ln (θ) = (1 − t) 2 �y �� �y n n = (1 − t) φ(xn ) − Eqt (yn | xn ;θ) φ(xn ) , 2 2 where en denotes the d dimensional vector with one at the n-th coordinate and zeros elsewhere (n-th unit vector). [sent-167, score-0.19]
</p><p>71 qt (y| x; θ) is the escort distribution of p(y| x; θ) (16): for n = 1, . [sent-168, score-0.201]
</p><p>72 3) How much overhead does t-logistic regression incur as compared to logistic regression? [sent-176, score-0.394]
</p><p>73 The Long-Servedio dataset is an artiﬁcially constructed dataset to show that algorithms which minimize a differentiable convex loss are not tolerant to label noise Long and Servedio [4]. [sent-180, score-0.511]
</p><p>74 The Mease-Wyner is another synthetic dataset to test the effect of label noise. [sent-191, score-0.153]
</p><p>75 Our comparators are logistic regression, linear SVMs4 , and an algorithm (the probit) which employs the probit loss, L(u) = 1 − erf (2u), used in BrownBoost/RobustBoost [5]. [sent-201, score-0.462]
</p><p>76 L-BFGS is also used to train logistic regression and the probit loss based algorithms. [sent-203, score-0.718]
</p><p>77 Label noise is added by randomly choosing 10% of the labels in the training set and ﬂipping them; each dataset is tested with and without label noise. [sent-204, score-0.158]
</p><p>78 The optimal parameters namely λ for t-logistic and � logistic regression and C for SVMs is chosen by performing a grid search over the � parameter space 2−7,−6,. [sent-206, score-0.394]
</p><p>79 In the latter case, the test error of t-logistic regression is very similar to logistic regression and Linear SVM (with 0% test error in 4 We also experimented with RampSVM [20], however, the results are worser than the other algorithms. [sent-215, score-0.594]
</p><p>80 9 probit SVM  Figure 3: The test error rate of various algorithms on six datasets (left to right, top: Long-Servedio, Mease-Wyner, Mushroom; bottom: USPS-N, Adult, Web) with and without 10% label noise. [sent-253, score-0.363]
</p><p>81 The blue (light) bar denotes a clean dataset while the magenta (dark) bar are the results with label noise added. [sent-255, score-0.292]
</p><p>82 When label noise is added, t-logistic regression (especially with t = 1. [sent-258, score-0.274]
</p><p>83 One can clearly observe that the ξ of the noisy data is much smaller than that of the clean data, which indicates that the algorithm is able to effectively identify these points and cap their inﬂuence. [sent-265, score-0.166]
</p><p>84 From left to right, the ﬁrst spike corresponds to the noisy large margin examples, the second spike represents the noisy pullers, the third spike denotes the clean pullers, while the rightmost spike corresponds to the clean large margin examples. [sent-267, score-0.568]
</p><p>85 Clearly, the noisy large margin examples and the noisy pullers are assigned a low value of ξ thus capping their inﬂuence and leading to the perfect classiﬁcation of the test set. [sent-268, score-0.323]
</p><p>86 On the other hand, logistic regression is unable to discriminate between clean and noisy training samples which leads to bad performance on noisy datasets. [sent-269, score-0.594]
</p><p>87 In a nutshell, t-logistic regression takes longer to train than either logistic regression or the probit. [sent-271, score-0.567]
</p><p>88 First, there is no closed form expression for gt (θ | x). [sent-273, score-0.32]
</p><p>89 When it does converge, it is often faster than the convex multiplicative programming algorithm. [sent-277, score-0.156]
</p><p>90 Just like logistic regression which uses a convex loss and hence converges to the same solution independent of the initialization, the solution obtained 5  We provide the signiﬁcance test results in Table 2 of supplementary material. [sent-284, score-0.741]
</p><p>91 0  ξ  ξ  Figure 4: The distribution of ξ obtained after training t-logistic regression with t = 1. [sent-321, score-0.15]
</p><p>92 by t-logistic regression seems fairly independent of the initial value of θ. [sent-327, score-0.173]
</p><p>93 On the other hand, the performance of the probit ﬂuctuates widely with different initial values of θ. [sent-328, score-0.193]
</p><p>94 7  Discussion and Outlook  In this paper, we generalize logistic regression to t-logistic regression by using the t-exponential family. [sent-351, score-0.544]
</p><p>95 On Long-Servedio experiment, if the label noise is increased signiﬁcantly beyond 10%, the performance of t-logistic regression may degrade (see Fig. [sent-355, score-0.274]
</p><p>96 It will be interesting to investigate if t-logistic regression can be married with graphical models to yield t-conditional random ﬁelds. [sent-358, score-0.15]
</p><p>97 We will also focus on better numerical techniques to accelerate the θ-step, especially a faster way to compute gt . [sent-359, score-0.296]
</p><p>98 Generalized thermostatistics based on deformed exponential and logarithmic functions. [sent-402, score-0.16]
</p><p>99 Estimators, escort proabilities, and φ-exponential families in statistical physics. [sent-410, score-0.213]
</p><p>100 An outer approximation method for minimizing the product of several convex functions on a convex set. [sent-450, score-0.242]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('expt', 0.51), ('zn', 0.298), ('gt', 0.296), ('logt', 0.283), ('logistic', 0.244), ('probit', 0.193), ('regression', 0.15), ('escort', 0.142), ('mushroom', 0.142), ('testerror', 0.142), ('loss', 0.131), ('student', 0.118), ('pullers', 0.113), ('convex', 0.108), ('family', 0.101), ('label', 0.094), ('supplementary', 0.083), ('clean', 0.08), ('exp', 0.075), ('physica', 0.074), ('xn', 0.073), ('families', 0.071), ('risk', 0.071), ('adult', 0.067), ('remp', 0.064), ('noisy', 0.06), ('qt', 0.059), ('exponential', 0.057), ('eqt', 0.057), ('servedio', 0.057), ('thermostatistics', 0.057), ('regularized', 0.052), ('spike', 0.052), ('datasets', 0.051), ('kuno', 0.05), ('uence', 0.048), ('multiplicative', 0.048), ('coordinate', 0.046), ('outlook', 0.046), ('deformed', 0.046), ('yi', 0.045), ('hinge', 0.044), ('tolerant', 0.043), ('rj', 0.041), ('logarithms', 0.041), ('margin', 0.04), ('ln', 0.04), ('minimize', 0.037), ('cyan', 0.036), ('libsvm', 0.036), ('svm', 0.036), ('robust', 0.035), ('dataset', 0.034), ('coordinates', 0.034), ('convexity', 0.034), ('bundle', 0.033), ('isotropic', 0.033), ('dark', 0.031), ('en', 0.031), ('svms', 0.03), ('answer', 0.03), ('noise', 0.03), ('ensures', 0.03), ('st', 0.029), ('springer', 0.028), ('xi', 0.028), ('generalized', 0.027), ('bar', 0.027), ('functions', 0.026), ('ym', 0.026), ('boosting', 0.026), ('clearly', 0.026), ('objective', 0.026), ('descent', 0.025), ('distributions', 0.025), ('usual', 0.025), ('questions', 0.025), ('xm', 0.025), ('rewrite', 0.025), ('test', 0.025), ('physics', 0.025), ('phil', 0.025), ('jylanki', 0.025), ('vanhatalo', 0.025), ('timothy', 0.025), ('capping', 0.025), ('erf', 0.025), ('rocco', 0.025), ('roderick', 0.025), ('overwhelming', 0.025), ('empirically', 0.025), ('block', 0.024), ('appears', 0.024), ('plotted', 0.024), ('closed', 0.024), ('fairly', 0.023), ('monotonically', 0.023), ('frequency', 0.023), ('longer', 0.023), ('rmly', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="290-tfidf-1" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>2 0.14267041 <a title="290-tfidf-2" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>3 0.1381449 <a title="290-tfidf-3" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>4 0.13673285 <a title="290-tfidf-4" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>Author: Armand Joulin, Jean Ponce, Francis R. Bach</p><p>Abstract: Dimensionality reduction is commonly used in the setting of multi-label supervised classiﬁcation to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression, and show that this model provides a probabilistic interpretation of discriminative clustering methods with added beneﬁts in terms of number of hyperparameters and optimization. While the expectation-maximization (EM) algorithm is commonly used to learn these probabilistic models, it usually leads to local maxima because it relies on a non-convex cost function. To avoid this problem, we introduce a local approximation of this cost function, which in turn leads to a quadratic non-convex optimization problem over a product of simplices. In order to maximize quadratic functions, we propose an efﬁcient algorithm based on convex relaxations and lowrank representations of the data, capable of handling large-scale problems. Experiments on text document classiﬁcation show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods. 1</p><p>5 0.12965347 <a title="290-tfidf-5" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>Author: James Petterson, Tibério S. Caetano</p><p>Abstract: Multi-label classiﬁcation is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classiﬁcation and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classiﬁcation admit relaxations that can be efﬁciently optimised. We optimise these relaxations with standard algorithms and compare our results with several stateof-the-art methods, showing excellent performance. 1</p><p>6 0.11218139 <a title="290-tfidf-6" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>7 0.10625445 <a title="290-tfidf-7" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>8 0.088671163 <a title="290-tfidf-8" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>9 0.087395489 <a title="290-tfidf-9" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>10 0.084840305 <a title="290-tfidf-10" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>11 0.083108127 <a title="290-tfidf-11" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>12 0.07475885 <a title="290-tfidf-12" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>13 0.070952386 <a title="290-tfidf-13" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>14 0.070045061 <a title="290-tfidf-14" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>15 0.068899669 <a title="290-tfidf-15" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>16 0.067341313 <a title="290-tfidf-16" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>17 0.065727778 <a title="290-tfidf-17" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>18 0.065308943 <a title="290-tfidf-18" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>19 0.063955322 <a title="290-tfidf-19" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>20 0.063409761 <a title="290-tfidf-20" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.053), (2, 0.102), (3, -0.026), (4, 0.039), (5, 0.08), (6, -0.089), (7, 0.004), (8, -0.043), (9, -0.01), (10, -0.038), (11, -0.007), (12, 0.1), (13, 0.069), (14, -0.039), (15, 0.06), (16, 0.021), (17, 0.122), (18, 0.071), (19, -0.108), (20, 0.037), (21, -0.091), (22, -0.005), (23, -0.052), (24, 0.045), (25, 0.071), (26, 0.048), (27, 0.182), (28, 0.089), (29, 0.05), (30, -0.042), (31, -0.009), (32, 0.002), (33, 0.02), (34, 0.009), (35, 0.04), (36, -0.021), (37, -0.06), (38, -0.069), (39, 0.086), (40, 0.045), (41, 0.089), (42, -0.002), (43, -0.032), (44, 0.055), (45, 0.038), (46, -0.028), (47, 0.01), (48, 0.054), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93994117 <a title="290-lsi-1" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>2 0.77716327 <a title="290-lsi-2" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>Author: James Petterson, Tibério S. Caetano</p><p>Abstract: Multi-label classiﬁcation is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classiﬁcation and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classiﬁcation admit relaxations that can be efﬁciently optimised. We optimise these relaxations with standard algorithms and compare our results with several stateof-the-art methods, showing excellent performance. 1</p><p>3 0.76234752 <a title="290-lsi-3" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The problem of controlling the margin of a classiﬁer is studied. A detailed analytical study is presented on how properties of the classiﬁcation risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classiﬁcation margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the ﬁxed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter. 1</p><p>4 0.74557668 <a title="290-lsi-4" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<p>Author: Min Yang, Linli Xu, Martha White, Dale Schuurmans, Yao-liang Yu</p><p>Abstract: Robust regression and classiﬁcation are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of “loss clipping” can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classiﬁcation problems. 1</p><p>5 0.70920295 <a title="290-lsi-5" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>Author: Uwe Dick, Peter Haider, Thomas Vanck, Michael Brückner, Tobias Scheffer</p><p>Abstract: We study a setting in which Poisson processes generate sequences of decisionmaking events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efﬁciently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service. 1</p><p>6 0.63921785 <a title="290-lsi-6" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>7 0.60539114 <a title="290-lsi-7" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>8 0.52633303 <a title="290-lsi-8" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>9 0.52571958 <a title="290-lsi-9" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>10 0.52306783 <a title="290-lsi-10" href="./nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">191 nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>11 0.52254647 <a title="290-lsi-11" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>12 0.51780999 <a title="290-lsi-12" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>13 0.50794822 <a title="290-lsi-13" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>14 0.50434166 <a title="290-lsi-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.50358033 <a title="290-lsi-15" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>16 0.50296175 <a title="290-lsi-16" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>17 0.48644033 <a title="290-lsi-17" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>18 0.48602238 <a title="290-lsi-18" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>19 0.48024911 <a title="290-lsi-19" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>20 0.47823387 <a title="290-lsi-20" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.059), (17, 0.018), (27, 0.051), (30, 0.083), (35, 0.022), (45, 0.213), (49, 0.186), (50, 0.047), (52, 0.05), (53, 0.016), (60, 0.044), (77, 0.052), (78, 0.024), (90, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90747201 <a title="290-lda-1" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>Author: Barnabás Póczos, Csaba Szepesvári, David Tax</p><p>Abstract: We present simple and computationally efﬁcient nonparametric estimators of R´ nyi entropy and mutual information based on an i.i.d. sample drawn from an e unknown, absolutely continuous distribution over Rd . The estimators are calculated as the sum of p-th powers of the Euclidean lengths of the edges of the ‘generalized nearest-neighbor’ graph of the sample and the empirical copula of the sample respectively. For the ﬁrst time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis. 1</p><p>2 0.8676573 <a title="290-lda-2" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>Author: Hongbo Zhou, Qiang Cheng</p><p>Abstract: Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact deﬁnition for a novel representation called sparse grouping representation (SGR), and prove a set of sufﬁcient conditions for generating such group level sparsity. Under these sufﬁcient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classiﬁcation setting. 1</p><p>3 0.8651697 <a title="290-lda-3" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>Author: Kamiya Motwani, Nagesh Adluru, Chris Hinrichs, Andrew Alexander, Vikas Singh</p><p>Abstract: We study the problem of segmenting speciﬁc white matter structures of interest from Diffusion Tensor (DT-MR) images of the human brain. This is an important requirement in many Neuroimaging studies: for instance, to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images. Typically, interactive expert guided segmentation has been the method of choice for such applications, but this is tedious for large datasets common today. To address this problem, we endow an image segmentation algorithm with “advice” encoding some global characteristics of the region(s) we want to extract. This is accomplished by constructing (using expert-segmented images) an epitome of a speciﬁc region – as a histogram over a bag of ‘words’ (e.g., suitable feature descriptors). Now, given such a representation, the problem reduces to segmenting a new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-speciﬁed histogram over features. We present combinatorial approximation algorithms to incorporate such domain speciﬁc constraints for Markov Random Field (MRF) segmentation. Making use of recent results on image co-segmentation, we derive effective solution strategies for our problem. We provide an analysis of solution quality, and present promising experimental evidence showing that many structures of interest in Neuroscience can be extracted reliably from 3-D brain image volumes using our algorithm. 1</p><p>same-paper 4 0.85674518 <a title="290-lda-4" href="./nips-2010-t-logistic_regression.html">290 nips-2010-t-logistic regression</a></p>
<p>Author: Nan Ding, S.v.n. Vishwanathan</p><p>Abstract: We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss function. An efﬁcient block coordinate descent optimization scheme can be derived for estimating the parameters. Because of the nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex loss functions, our algorithm is fairly robust to the choice of initial values. We verify both these observations empirically on a number of synthetic and real datasets. 1</p><p>5 0.80312979 <a title="290-lda-5" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>6 0.79917377 <a title="290-lda-6" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>7 0.79863292 <a title="290-lda-7" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>8 0.79678357 <a title="290-lda-8" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>9 0.7963475 <a title="290-lda-9" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>10 0.79609537 <a title="290-lda-10" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>11 0.79597372 <a title="290-lda-11" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>12 0.79476416 <a title="290-lda-12" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>13 0.79452205 <a title="290-lda-13" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>14 0.79401273 <a title="290-lda-14" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>15 0.79384893 <a title="290-lda-15" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>16 0.79287678 <a title="290-lda-16" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>17 0.79185921 <a title="290-lda-17" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>18 0.79169196 <a title="290-lda-18" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>19 0.79166651 <a title="290-lda-19" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>20 0.78992468 <a title="290-lda-20" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
