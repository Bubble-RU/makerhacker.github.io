<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-227" href="#">nips2010-227</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</h1>
<br/><p>Source: <a title="nips-2010-227-pdf" href="http://papers.nips.cc/paper/3967-rescaling-thinning-or-complementing-on-goodness-of-fit-procedures-for-point-process-models-and-generalized-linear-models.pdf">pdf</a></p><p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>Reference: <a title="nips-2010-227-reference" href="../nips2010_reference/nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 On goodness-of-ﬁt procedures for point process models and Generalized Linear Models Wulfram Gerstner Brain Mind Institute Ecole Polytechnique F´ d´ rale de Lausanne e e 1015 Lausanne EPFL, Switzerland wulfram. [sent-2, score-0.232]
</p><p>2 ch  Abstract Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. [sent-6, score-0.413]
</p><p>3 They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e. [sent-7, score-0.178]
</p><p>4 Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. [sent-11, score-0.329]
</p><p>5 Furthermore, two additional tests based on thinning and complementing point processes are introduced. [sent-12, score-0.78]
</p><p>6 They augment the instruments available for checking model adequacy of point processes as well as discretized models. [sent-13, score-0.215]
</p><p>7 An observation is a sequence of spike times and their stochastic properties are captured by a single function, the conditional intensity [1]. [sent-16, score-0.788]
</p><p>8 In practice, neural data is binned such that a spike train is represented as a sequence of spike counts per time bin. [sent-19, score-0.935]
</p><p>9 Such discretized models of time series have mostly been seen as an approximation to continuous point processes and hence, the time-rescaling theorem was also applied to such models [4, 5, 6, 7, 8]. [sent-21, score-0.343]
</p><p>10 We review the approximations necessary for the transition to discrete time and point out a procedure to create surrogate point processes even when these approximations do not hold (section 2). [sent-23, score-0.293]
</p><p>11 Two novel tests based on two different operations on point processes are introduced: random thinning and random complementing. [sent-24, score-0.571]
</p><p>12 This supports the representation of neural activity as a point process in which each spike is assumed to be a singular event in time. [sent-29, score-0.558]
</p><p>13 (C) When time is divided into large bins, the spike train is represented as a time series of discrete counts. [sent-30, score-0.548]
</p><p>14 (D) If the bin width is chosen small enough, the spike train corresponds to a binary time series, indicating the presence of a single spike inside a given time bin. [sent-31, score-1.145]
</p><p>15 1  Methods Representations of neural activity  We characterize a neuron by its response in terms of trains of action potentials using the theory of point processes (Figures 1A and 1B). [sent-33, score-0.202]
</p><p>16 The stochastic properties of a point process are characterized by its conditional intensity function λ(t|H(t)), deﬁned as [1]: P [spike in (t, t + ∆)|Ht ] , (1) ∆ where Ht is the history of the stochastic process up to time t and possibly includes other covariates of interest. [sent-36, score-0.661]
</p><p>17 For ﬁtting and evaluating different parameter sets of the conditional intensity function, a maximum-likelihood approach is followed [10, 11]. [sent-37, score-0.346]
</p><p>18 log λ(ui |Hui ) −  log L(point process) =  (2)  0  i=1  One possibility are binning-free models (like renewal processes or other parametric models). [sent-39, score-0.205]
</p><p>19 Inside the bin, the process locally behaves like a Poisson process with constant rate λk = λ(tk |Hk ) with tk = ∆k and Hk = Htk . [sent-42, score-0.254]
</p><p>20 Using the number of spikes ck per bin as a representation of the observation, the discretized version of Equation 2 is equivalent to the log-likelihood of a series of Poisson samples (apart from terms that are not dependent on λ(t|Ht )). [sent-43, score-0.424]
</p><p>21 A complementary approach to the point process framework is to see spike trains as time series, e. [sent-47, score-0.673]
</p><p>22 For Poisson-GLMs, a sequence of Poisson-distributed count variables ci is modeled and the linear sum of covariates is linked to the expected mean of the Poisson distribution µi . [sent-50, score-0.171]
</p><p>23 Deﬁned this way, the likelihood for an observed sequence bi given a pk particular model of pi is given by log L(Bernoulli) = k bk log 1−pk + k log(1 − pk ). [sent-53, score-0.23]
</p><p>24 Moreover, using the same approximation, it is possible to link the Bernoulli series to the conditional intensity function λ(t|Ht ) via λi ≈ pi /∆ . [sent-55, score-0.497]
</p><p>25 Traditionally, this path was chosen to relate the time series to the theory of point processes and to be able to use goodness-of-ﬁt analyses available for such point processes [9]. [sent-56, score-0.336]
</p><p>26 (A) Using the time-rescaling theorem, the time of each spike is rescaled according to the integral of the conditional intensity function. [sent-58, score-0.815]
</p><p>27 (B) Assuming that the conditional intensity function has a lower limit B, spikes of the original spike train are thinned by keeping a spike only with probability Bλ−1 . [sent-59, score-1.464]
</p><p>28 (C) Assuming that i the conditional intensity function has an upper limit C, a complementary process λC = C − λ can be constructed. [sent-60, score-0.499]
</p><p>29 Adding samples from this inhomogeneous Poisson process to the observed spikes results in a homogeneous Poisson process with rate C. [sent-61, score-0.524]
</p><p>30 2  Goodness-of-ﬁt tests for point processes  Statistical tests are usually evaluated using two measures: The speciﬁcity (fraction of correct models that pass the test) and the sensitivity or test power (fraction of wrong models that are properly rejected by the test). [sent-63, score-0.644]
</p><p>31 The sensitivity of a given test depends on the strength of the departure from the modeled intensity function to the true intensity. [sent-65, score-0.514]
</p><p>32 It states that if {ui } is a realization of events from a point process with conditional intensity λ(t|Ht ), u then rescaling via the transformation ui = 0 i λ(t|Ht )dt will yield a unit-rate Poisson process. [sent-69, score-0.758]
</p><p>33 ı j The spike time ui falling into bin j, is transformed into: ui = k=1 pk . [sent-71, score-0.837]
</p><p>34 2  Thinning point processes  It is well known that an inhomogeneous point process can be simulated by generating a homogeneous Poisson process with constant intensity C with C ≥ max λ(t) (the so-called dominant process) and keeping every spike at time ti with probability p = λ(ti ) [13, 2]. [sent-74, score-1.33]
</p><p>35 In reverse, this C can be used to do model-checking [14]: Let B be a lower bound of the ﬁtted conditional intensity λ(t|H(t)). [sent-75, score-0.346]
</p><p>36 Now take λ(t|H(t)) as the dominant process with samples ui . [sent-76, score-0.183]
</p><p>37 Thin the process by keepB ing a spike with probability λ(ti |Ht ) . [sent-77, score-0.515]
</p><p>38 For a correctly speciﬁed model λ(t|Ht ), the thinned process will be a homogeneous Poisson process with rate B (Figure 2B). [sent-78, score-0.399]
</p><p>39 3  ¯ Typically, B = min λ(t) λ(t) (due to absolute refractoriness in most renewal process models and GLMs), such that the thinned process will have a prohibitively low rate and only very few spikes will be selected. [sent-79, score-0.579]
</p><p>40 After applying the thinning procedure on all spikes of the stitched process, the thinned process should be a Poisson process with rate B ∗ . [sent-83, score-0.799]
</p><p>41 Stretching each thinned process by a factor of B ∗ creates a set of K unit-rate processes. [sent-85, score-0.206]
</p><p>42 It tests the global null hypothesis that all tested sub-hypotheses are true against the alternative hypothesis that at least one hypothesis is false. [sent-89, score-0.196]
</p><p>43 3  Complementing point processes  The idea of thinning might also be used the other way round. [sent-101, score-0.468]
</p><p>44 Assume the observations ui have been generated by thinning a homogeneous Poisson process with rate C using the modeled conditional intensity λ(t|Ht ) as the lower bound. [sent-102, score-1.003]
</p><p>45 Then we can deﬁne a complementary process λc (t) = C − λ(t|Ht ) such that adding spikes from the complementary point process to the observed spikes, the resulting process will be a homogeneous Poisson process with rate C. [sent-103, score-0.779]
</p><p>46 This algorithm is a straightforward inversion of the thinning algorithms discussed in [2, 1]. [sent-104, score-0.329]
</p><p>47 It might happen that the upper bound C of the modeled intensity is much larger than the average λ(t). [sent-105, score-0.342]
</p><p>48 In that case, the observed spike pattern would be distorted with high number of Poisson spikes from the complementary process and the test power would be low. [sent-106, score-0.854]
</p><p>49 To avoid this, a similar technique as for the thinning procedure can be employed. [sent-107, score-0.329]
</p><p>50 Deﬁne a threshold C ∗ ≤ C and consider only the region of the spike train for which λ(t|H(t)) < C ∗ . [sent-108, score-0.466]
</p><p>51 Apply the complementing procedure on these parts of the spike train to obtain a point process with rate C ∗ when concatenating the intervals. [sent-109, score-0.847]
</p><p>52 3  Creating surrogate point processes from time series  Since the time-rescaling theorem can only be used when λ(t|Ht ) the exact spike times {ui } are known, it is not a priori clear how it applies to discretized time-series models. [sent-113, score-0.828]
</p><p>53 For such cases, we propose to generate surrogate point process samples that are equivalent to the observed time series. [sent-114, score-0.256]
</p><p>54 To apply the time-rescaling theorem on discretized models such as GLMs, the integral of the time transformation is replaced by a discrete sum over bins (the na¨ve time-rescaling). [sent-115, score-0.215]
</p><p>55 Hence, the modeled process can be regarded as a piecewise-constant intensity function. [sent-122, score-0.444]
</p><p>56 The expected number of spikes of a Poisson process is related to its intensity via µi = λi ∆ such that we can construct the conditional intensity function as 1 The K tests contain overlapping regions of the same spike train, hence, we expect the statistical tests to be correlated. [sent-123, score-1.49]
</p><p>57 For bin-free point process models for which the spike times and a conditional intensity λ(t|H(t)) is available, goodness-of-ﬁt tests for point processes can be readily applied. [sent-126, score-1.197]
</p><p>58 For Poisson-GLMs, exact spike times are drawn inside each bin for the speciﬁed number of spikes that were observed. [sent-127, score-0.795]
</p><p>59 The piece-wise constant conditional intensity function is linked to the modeled number of counts per bin via λi = ∆−1 µi . [sent-128, score-0.65]
</p><p>60 For BernoulliGLMs, the probability of obtaining at least one spike per bin pi is modeled. [sent-129, score-0.709]
</p><p>61 For each bin with spikes (bi = 1) – assuming a local Poisson process – a sample ci from a biased Poisson distribution with mean µi = − ln(1 − pi ) is drawn together with corresponding spike times. [sent-130, score-1.002]
</p><p>62 Finally, point-process based goodness-of-ﬁt tests may be applied to this surrogate spike train. [sent-131, score-0.603]
</p><p>63 Conditioned on the number of spikes that occurred in a homogeneous Poisson process of rate λi , the exact spike times are uniformly distributed inside bin i. [sent-133, score-0.988]
</p><p>64 A surrogate point process can be constructed from a Poisson-GLM by generating random spike times (i − 1 + U nif (0, 1))∆ for each spike within bin i (1 ≤ i ≤ N ) for all bins with ci > 0. [sent-134, score-1.364]
</p><p>65 One can then proceed to the point-process-based goodness-of-ﬁt tools using the surrogate spike train and its conditional intensity λi . [sent-135, score-0.899]
</p><p>66 Bernoulli-GLMs: Based on the observed binary spike train {bi }, the sequence of probabilities pi of spiking within bin i is modeled. [sent-136, score-0.814]
</p><p>67 We can relate this to the point process framework using the following observations: Assume that pi denotes the probability of ﬁnding at least one spike within (poisson) (X ≥ each bin2 and that locally, the process behaves like a Poisson process. [sent-137, score-0.777]
</p><p>68 The conditional intensity is given by λi = ∆ µi = 1) = 1 − Pµi −∆−1 ln(1 − pi ). [sent-139, score-0.463]
</p><p>69 In practice, for each bin with bi = 1, we draw the amount of spikes within the (poisson) (X = k|k ≥ 1) and sample exact spike times bin by ﬁrst sampling from the distribution Pµi uniformly as in the case of the Poisson-GLMs. [sent-140, score-0.974]
</p><p>70 For the thinning and complementing procedure, K = 10 partitions were chosen (see section 2. [sent-142, score-0.538]
</p><p>71 The Poisson hypothesis in the proposed procedures is tested by a Kolmogorov-Smirnov test on the inter-spike intervals of the transformed process. [sent-147, score-0.169]
</p><p>72 The process was simulated over a length of T = 20 s and the intensity was discretized with ∆ = 1 ms. [sent-150, score-0.466]
</p><p>73 2  0 0  5  10 time [s]  15  20  (a) intensity function  test power  no jitter medium jitter high jitter  test power  intensity function  150  0 0  10  20 30 jitter strength  0. [sent-159, score-2.422]
</p><p>74 (A) Sample intensity functions for an undistorted intensity (black line) and two models with jitters in the coefﬁcients (β = 12, medium jitter and β = 30, large jitter). [sent-164, score-1.076]
</p><p>75 (B) The test power of each test as a function of the jitter strength. [sent-165, score-0.525]
</p><p>76 The dashed line indicates the level of the medium jitter strength (red line in ﬁgure A). [sent-166, score-0.639]
</p><p>77 (C) ROC curve analysis for an intermediate jitter strength of β = 12. [sent-167, score-0.498]
</p><p>78 The intersection of the curves with the dashed line corresponds to the test power at a signiﬁcance level of α = . [sent-168, score-0.214]
</p><p>79 A binary spike train was generated by calculating the probability of at least one spike in each time bin as pi = 1 − exp(−λ(ti )∆) and drawing samples from a Bernoulli distribution with speciﬁed probabilities pi . [sent-171, score-1.316]
</p><p>80 For evaluating the different algorithms, wrong models for the intensity were created with jittered coefﬁcients uk = uk + βUnif(−1, 1) where β indicates the strength of the deviation from the true model. [sent-172, score-0.471]
</p><p>81 For each jitter strength, N = 1000 spike trains were generated from the true model and λ(t|Ht ) was constructed using the wrong model (Figure 4A). [sent-173, score-0.864]
</p><p>82 For any β > 0, the fraction of rejected models deﬁnes the sensitivity or test power. [sent-174, score-0.179]
</p><p>83 Notably, the complementing and thinning procedures detect a departure from the correct model earlier than the classical rescaling (Figure 4B). [sent-178, score-0.73]
</p><p>84 For models with an intermediate jitter strength (β = 12), ROC curves were constructed. [sent-184, score-0.49]
</p><p>85 It can be seen that especially for intermediate jitter strengths, complementing and thinning outperform time-rescaling (Figure 4C), independent of the chosen signiﬁcance level. [sent-186, score-0.933]
</p><p>86 2  Example: Renewal process  In a second example, we consider renewal processes, i. [sent-188, score-0.189]
</p><p>87 In this case, the conditional intensity is given by p(t−t∗ ) λ(t|Ht ) = where t∗ denotes the time of the last spike prior to time t. [sent-194, score-0.807]
</p><p>88 For this t−t∗ 1−  0  p(u)du  example, we chose the Gamma distribution as it is commonly used to model real spike trains [4, 3, 7]. [sent-195, score-0.453]
</p><p>89 The spike train was generated from a true model, following a Gamma distribution with scale param− ∆t  A eter A = 0. [sent-196, score-0.466]
</p><p>90 For each jitter strength, N = 1000 data sets of length T = 20 s were generated from the true model and the wrong model and the tests were applied. [sent-202, score-0.514]
</p><p>91 15 inter−spike interval [s]  (a) intensity function  0. [sent-211, score-0.317]
</p><p>92 5  (b) test power  test power  20  sample ISI distribution no jitter medium jitter high jitter  test power  probability density function  30  0. [sent-214, score-1.509]
</p><p>93 (B) The test power of each test as a function of the jitter strength. [sent-224, score-0.525]
</p><p>94 The dashed line indicates the level of the medium jitter strength (red line in ﬁgure A). [sent-225, score-0.639]
</p><p>95 (C) ROC curve analysis for an intermediate jitter strength of β = 0. [sent-226, score-0.498]
</p><p>96 The intersection of the curves with the dashed line corresponds to the test power at a signiﬁcance level of α = . [sent-228, score-0.214]
</p><p>97 The analysis of test power for each test and the ROC curve analysis for an intermediate jitter strength reveal that time-rescaling is slightly superior to thinning and complementing (Figure 5B and C). [sent-230, score-1.203]
</p><p>98 3  Example: Inhomogeneous Spike Response Model  We model an inhomogeneous spike response model with escape noise using a Bernoulli-GLM [21]. [sent-234, score-0.507]
</p><p>99 The spiking probability is modulated by an inhomogeneous rate r(t). [sent-235, score-0.173]
</p><p>100 The rate function is modeled like in the ﬁrst J=40  sin(2πf (t − j T ))  i J with f = 1 Hz example as a band-limited function rti = r(ti ) = j j=1 uj π(ti − J T ) and J = 40 coefﬁcients that were randomly drawn from a uniform distribution on the interval [−0. [sent-237, score-0.184]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.413), ('jitter', 0.358), ('thinning', 0.329), ('poisson', 0.301), ('intensity', 0.288), ('complementing', 0.209), ('ht', 0.181), ('bin', 0.179), ('rescaling', 0.138), ('spikes', 0.135), ('pi', 0.117), ('spiketrain', 0.104), ('thinned', 0.104), ('tests', 0.103), ('process', 0.102), ('processes', 0.096), ('inhomogeneous', 0.094), ('surrogate', 0.087), ('renewal', 0.087), ('ui', 0.081), ('medium', 0.081), ('cance', 0.081), ('glms', 0.079), ('discretized', 0.076), ('strength', 0.073), ('power', 0.069), ('homogeneous', 0.064), ('rejected', 0.062), ('ti', 0.061), ('lausanne', 0.06), ('conditional', 0.058), ('roc', 0.057), ('ci', 0.056), ('modeled', 0.054), ('wrong', 0.053), ('train', 0.053), ('city', 0.052), ('spiking', 0.052), ('complementary', 0.051), ('test', 0.049), ('na', 0.047), ('bernoulli', 0.047), ('kp', 0.045), ('point', 0.043), ('bins', 0.042), ('intervals', 0.041), ('trains', 0.04), ('inside', 0.039), ('rale', 0.039), ('rti', 0.039), ('simes', 0.039), ('undistorted', 0.039), ('bi', 0.039), ('linked', 0.039), ('intermediate', 0.037), ('pk', 0.037), ('hz', 0.035), ('uj', 0.035), ('dashed', 0.035), ('specificity', 0.035), ('jittered', 0.035), ('distorted', 0.035), ('epfl', 0.035), ('series', 0.034), ('counts', 0.032), ('isi', 0.032), ('polytechnique', 0.032), ('rescaled', 0.032), ('line', 0.031), ('hypothesis', 0.031), ('curve', 0.03), ('level', 0.03), ('naive', 0.03), ('ks', 0.03), ('times', 0.029), ('interval', 0.029), ('departure', 0.028), ('unif', 0.028), ('cients', 0.027), ('rate', 0.027), ('procedures', 0.026), ('glm', 0.026), ('switzerland', 0.026), ('theorem', 0.026), ('ve', 0.025), ('ms', 0.025), ('transformation', 0.025), ('fraction', 0.024), ('time', 0.024), ('discretization', 0.024), ('events', 0.023), ('action', 0.023), ('tk', 0.023), ('ecole', 0.023), ('coef', 0.023), ('covariates', 0.022), ('history', 0.022), ('sensitivity', 0.022), ('transformed', 0.022), ('models', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="227-tfidf-1" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>2 0.30374026 <a title="227-tfidf-2" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>3 0.25291041 <a title="227-tfidf-3" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>4 0.16824614 <a title="227-tfidf-4" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>5 0.15047333 <a title="227-tfidf-5" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>6 0.14243117 <a title="227-tfidf-6" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>7 0.11253555 <a title="227-tfidf-7" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>8 0.10728881 <a title="227-tfidf-8" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>9 0.10108102 <a title="227-tfidf-9" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>10 0.088219419 <a title="227-tfidf-10" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>11 0.081029139 <a title="227-tfidf-11" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>12 0.071975075 <a title="227-tfidf-12" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>13 0.070155665 <a title="227-tfidf-13" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>14 0.065417506 <a title="227-tfidf-14" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>15 0.054307617 <a title="227-tfidf-15" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>16 0.052691005 <a title="227-tfidf-16" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>17 0.050184153 <a title="227-tfidf-17" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>18 0.050021645 <a title="227-tfidf-18" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>19 0.048344359 <a title="227-tfidf-19" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>20 0.048270684 <a title="227-tfidf-20" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.137), (1, 0.02), (2, -0.12), (3, 0.192), (4, 0.045), (5, 0.248), (6, -0.039), (7, 0.101), (8, 0.112), (9, -0.088), (10, -0.014), (11, 0.09), (12, 0.011), (13, 0.146), (14, 0.105), (15, 0.035), (16, 0.083), (17, -0.04), (18, 0.119), (19, -0.187), (20, -0.083), (21, 0.113), (22, -0.177), (23, -0.053), (24, -0.06), (25, 0.079), (26, -0.051), (27, -0.046), (28, 0.067), (29, 0.094), (30, -0.046), (31, 0.063), (32, -0.029), (33, 0.022), (34, 0.019), (35, -0.092), (36, -0.062), (37, -0.056), (38, 0.024), (39, -0.133), (40, 0.062), (41, -0.047), (42, -0.083), (43, 0.018), (44, 0.056), (45, 0.01), (46, 0.088), (47, 0.003), (48, -0.05), (49, -0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97055089 <a title="227-lsi-1" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>2 0.84934437 <a title="227-lsi-2" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>3 0.81543142 <a title="227-lsi-3" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>4 0.65459043 <a title="227-lsi-4" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>5 0.57912731 <a title="227-lsi-5" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>6 0.51427829 <a title="227-lsi-6" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>7 0.48962891 <a title="227-lsi-7" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>8 0.45703921 <a title="227-lsi-8" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>9 0.43250078 <a title="227-lsi-9" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>10 0.42814839 <a title="227-lsi-10" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>11 0.37956703 <a title="227-lsi-11" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>12 0.36802608 <a title="227-lsi-12" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>13 0.36001223 <a title="227-lsi-13" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>14 0.34757316 <a title="227-lsi-14" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>15 0.29330668 <a title="227-lsi-15" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>16 0.26351002 <a title="227-lsi-16" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>17 0.25657082 <a title="227-lsi-17" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>18 0.2539162 <a title="227-lsi-18" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>19 0.24765387 <a title="227-lsi-19" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>20 0.24358371 <a title="227-lsi-20" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.03), (17, 0.015), (27, 0.044), (30, 0.039), (35, 0.02), (45, 0.122), (50, 0.048), (52, 0.52), (60, 0.017), (77, 0.035), (90, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83666509 <a title="227-lda-1" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Sujay Sanghavi</p><p>Abstract: Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efﬁciently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative ﬁltering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efﬁcient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisﬁed, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identiﬁes the corrupted points. Such identiﬁcation of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and ﬁnancial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.</p><p>same-paper 2 0.82676744 <a title="227-lda-2" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>3 0.81610769 <a title="227-lda-3" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of Rd . Examples include SVMs for the analysis of histograms or colored images, SVMs for text classiÄ?Ĺš cation and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1</p><p>4 0.78105015 <a title="227-lda-4" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: Divisive normalization (DN) has been advocated as an effective nonlinear efﬁcient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justiﬁes DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm DN as an effective efﬁcient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small. 1</p><p>5 0.73517907 <a title="227-lda-5" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>6 0.48371053 <a title="227-lda-6" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>7 0.45929372 <a title="227-lda-7" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>8 0.44950765 <a title="227-lda-8" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>9 0.44655392 <a title="227-lda-9" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>10 0.44330263 <a title="227-lda-10" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>11 0.43947068 <a title="227-lda-11" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>12 0.43728414 <a title="227-lda-12" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>13 0.43713918 <a title="227-lda-13" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>14 0.4359943 <a title="227-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.43125924 <a title="227-lda-15" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>16 0.42846859 <a title="227-lda-16" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>17 0.42366496 <a title="227-lda-17" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>18 0.42013466 <a title="227-lda-18" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>19 0.41833213 <a title="227-lda-19" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>20 0.41524658 <a title="227-lda-20" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
