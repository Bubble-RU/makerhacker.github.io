<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>261 nips-2010-Supervised Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-261" href="#">nips2010-261</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>261 nips-2010-Supervised Clustering</h1>
<br/><p>Source: <a title="nips-2010-261-pdf" href="http://papers.nips.cc/paper/4115-supervised-clustering.pdf">pdf</a></p><p>Author: Pranjal Awasthi, Reza B. Zadeh</p><p>Abstract: Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efﬁcient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will ﬁnd the target clustering under the strongest property.</p><p>Reference: <a title="nips-2010-261-reference" href="../nips2010_reference/nips-2010-Supervised_Clustering_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('teach', 0.448), ('clust', 0.428), ('hi', 0.323), ('request', 0.308), ('merg', 0.272), ('unmark', 0.221), ('interv', 0.204), ('hj', 0.199), ('query', 0.148), ('target', 0.145), ('split', 0.139), ('hypercub', 0.108), ('ai', 0.106), ('rectangl', 0.101), ('strict', 0.079), ('balc', 0.077), ('conceiv', 0.071), ('sl', 0.069), ('aj', 0.067), ('lef', 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="261-tfidf-1" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>Author: Pranjal Awasthi, Reza B. Zadeh</p><p>Abstract: Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efﬁcient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will ﬁnd the target clustering under the strongest property.</p><p>2 0.33469278 <a title="261-tfidf-2" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>Author: Margareta Ackerman, Shai Ben-David, David Loker</p><p>Abstract: Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill deﬁned problem - given a data set, it is not clear what a “correct” clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some ﬁrst steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms. In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg’s famous impossibility result, while providing a simpler proof. 1</p><p>3 0.20453057 <a title="261-tfidf-3" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>4 0.18213266 <a title="261-tfidf-4" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>Author: Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua B. Tenenbaum</p><p>Abstract: We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We extend the inﬁnite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p><p>5 0.17259255 <a title="261-tfidf-5" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>Author: Kamalika Chaudhuri, Sanjoy Dasgupta</p><p>Abstract: For a density f on Rd , a high-density cluster is any connected component of {x : f (x) ≥ λ}, for some λ > 0. The set of all high-density clusters form a hierarchy called the cluster tree of f . We present a procedure for estimating the cluster tree given samples from f . We give ﬁnite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem. 1</p><p>6 0.1411135 <a title="261-tfidf-6" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>7 0.13265431 <a title="261-tfidf-7" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>8 0.12966305 <a title="261-tfidf-8" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>9 0.12234879 <a title="261-tfidf-9" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>10 0.10198237 <a title="261-tfidf-10" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>11 0.10105345 <a title="261-tfidf-11" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>12 0.099867515 <a title="261-tfidf-12" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>13 0.099648654 <a title="261-tfidf-13" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>14 0.097927511 <a title="261-tfidf-14" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>15 0.096483126 <a title="261-tfidf-15" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>16 0.093236431 <a title="261-tfidf-16" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>17 0.089999162 <a title="261-tfidf-17" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>18 0.083290532 <a title="261-tfidf-18" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>19 0.083212204 <a title="261-tfidf-19" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>20 0.078813486 <a title="261-tfidf-20" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, -0.042), (2, 0.076), (3, -0.104), (4, 0.121), (5, -0.105), (6, -0.253), (7, 0.288), (8, 0.094), (9, 0.08), (10, 0.163), (11, -0.076), (12, 0.05), (13, -0.059), (14, 0.092), (15, -0.113), (16, 0.006), (17, -0.088), (18, -0.035), (19, 0.118), (20, 0.004), (21, -0.016), (22, -0.041), (23, -0.066), (24, -0.081), (25, -0.098), (26, -0.002), (27, 0.003), (28, 0.092), (29, 0.031), (30, -0.037), (31, -0.03), (32, -0.034), (33, -0.056), (34, -0.035), (35, -0.023), (36, 0.127), (37, 0.086), (38, 0.0), (39, -0.101), (40, 0.026), (41, 0.035), (42, -0.073), (43, 0.026), (44, -0.082), (45, 0.025), (46, -0.082), (47, 0.006), (48, -0.014), (49, -0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96425635 <a title="261-lsi-1" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>Author: Pranjal Awasthi, Reza B. Zadeh</p><p>Abstract: Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efﬁcient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will ﬁnd the target clustering under the strongest property.</p><p>2 0.85956842 <a title="261-lsi-2" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>Author: Margareta Ackerman, Shai Ben-David, David Loker</p><p>Abstract: Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill deﬁned problem - given a data set, it is not clear what a “correct” clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some ﬁrst steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms. In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinberg’s famous impossibility result, while providing a simpler proof. 1</p><p>3 0.81984425 <a title="261-lsi-3" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>Author: Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua B. Tenenbaum</p><p>Abstract: We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We extend the inﬁnite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p><p>4 0.74214154 <a title="261-lsi-4" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>5 0.68289614 <a title="261-lsi-5" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>Author: Kamalika Chaudhuri, Sanjoy Dasgupta</p><p>Abstract: For a density f on Rd , a high-density cluster is any connected component of {x : f (x) ≥ λ}, for some λ > 0. The set of all high-density clusters form a hierarchy called the cluster tree of f . We present a procedure for estimating the cluster tree given samples from f . We give ﬁnite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem. 1</p><p>6 0.65452987 <a title="261-lsi-6" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>7 0.60875231 <a title="261-lsi-7" href="./nips-2010-Minimum_Average_Cost_Clustering.html">166 nips-2010-Minimum Average Cost Clustering</a></p>
<p>8 0.53596228 <a title="261-lsi-8" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>9 0.52472609 <a title="261-lsi-9" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>10 0.41671243 <a title="261-lsi-10" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>11 0.41157347 <a title="261-lsi-11" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>12 0.39929542 <a title="261-lsi-12" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>13 0.37832037 <a title="261-lsi-13" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>14 0.36735135 <a title="261-lsi-14" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>15 0.36278316 <a title="261-lsi-15" href="./nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</a></p>
<p>16 0.3526662 <a title="261-lsi-16" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>17 0.34422475 <a title="261-lsi-17" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>18 0.34208953 <a title="261-lsi-18" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>19 0.33874062 <a title="261-lsi-19" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>20 0.3384563 <a title="261-lsi-20" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.116), (30, 0.068), (32, 0.155), (34, 0.152), (42, 0.214), (45, 0.075), (68, 0.103)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83832502 <a title="261-lda-1" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>Author: Pranjal Awasthi, Reza B. Zadeh</p><p>Abstract: Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class in that model. Our algorithm is query-efﬁcient in the sense that it involves only a small amount of interaction with the teacher. We also present and study two natural generalizations of the model. The model assumes that the teacher response to the algorithm is perfect. We eliminate this limitation by proposing a noisy model and give an algorithm for clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees a random subset of the points. Finally, for datasets satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class of clustering functions containing Single-Linkage will ﬁnd the target clustering under the strongest property.</p><p>2 0.79414386 <a title="261-lda-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.78696233 <a title="261-lda-3" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Many statistical M -estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer. We analyze the convergence rates of ﬁrst-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions— namely, strong convexity and smoothness conditions—that underlie classical optimization analysis. We deﬁne appropriately restricted versions of these conditions, and show that they are satisﬁed with high probability for various statistical models. Under these conditions, our theory guarantees that Nesterov’s ﬁrst-order method [12] has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter θ∗ and the optimal solution θ. This globally linear rate is substantially faster than previous analyses of global convergence for speciﬁc methods that yielded only sublinear rates. Our analysis applies to a wide range of M -estimators and statistical models, including sparse linear regression using Lasso ( 1 regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization. Overall, this result reveals an interesting connection between statistical precision and computational eﬃciency in high-dimensional estimation. 1</p><p>4 0.78667152 <a title="261-lda-4" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Sparse methods for supervised learning aim at ﬁnding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the ℓ1 -norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lov´ sz extension, a common tool in submodua lar analysis. This deﬁnes a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting speciﬁc submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also deﬁne new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p><p>5 0.78627908 <a title="261-lda-5" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We develop a theory of online learning by deﬁning several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting. 1</p><p>6 0.78580236 <a title="261-lda-6" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>7 0.78479123 <a title="261-lda-7" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>8 0.78461576 <a title="261-lda-8" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>9 0.78443646 <a title="261-lda-9" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>10 0.78307819 <a title="261-lda-10" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>11 0.78271681 <a title="261-lda-11" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>12 0.78168499 <a title="261-lda-12" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>13 0.78015029 <a title="261-lda-13" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>14 0.78008533 <a title="261-lda-14" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>15 0.77965462 <a title="261-lda-15" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>16 0.77938151 <a title="261-lda-16" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>17 0.77922088 <a title="261-lda-17" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>18 0.7791993 <a title="261-lda-18" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>19 0.77888834 <a title="261-lda-19" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>20 0.77840775 <a title="261-lda-20" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
