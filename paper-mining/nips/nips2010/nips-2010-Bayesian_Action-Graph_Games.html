<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2010-Bayesian Action-Graph Games</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-39" href="#">nips2010-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 nips-2010-Bayesian Action-Graph Games</h1>
<br/><p>Source: <a title="nips-2010-39-pdf" href="http://papers.nips.cc/paper/4171-bayesian-action-graph-games.pdf">pdf</a></p><p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>Reference: <a title="nips-2010-39-reference" href="../nips2010_reference/nips-2010-Bayesian_Action-Graph_Games_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gam', 0.665), ('play', 0.278), ('equilibr', 0.256), ('nash', 0.246), ('bag', 0.237), ('ai', 0.205), ('util', 0.185), ('bayes', 0.164), ('pro', 0.161), ('ag', 0.142), ('nod', 0.127), ('ui', 0.122), ('le', 0.108), ('coff', 0.079), ('shop', 0.079), ('aj', 0.074), ('pxy', 0.074), ('govind', 0.065), ('strategy', 0.063), ('mix', 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="39-tfidf-1" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>2 0.20902058 <a title="39-tfidf-2" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>Author: Jacob D. Abernethy, Manfred K. Warmuth</p><p>Abstract: We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player’s best mixed strategy with knowledge of this budget. We show that, for a general class of normal-form games, the minimax strategy is indeed efﬁciently computable and relies on a “random playout” technique. We give three diverse applications of this new algorithmic template: a cost-sensitive “Hedge” setting, a particular problem in Metrical Task Systems, and the design of combinatorial prediction markets. 1</p><p>3 0.20260447 <a title="39-tfidf-3" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>4 0.18716289 <a title="39-tfidf-4" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We develop a theory of online learning by deﬁning several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting. 1</p><p>5 0.15182135 <a title="39-tfidf-5" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>Author: Jean-philippe Vert, Kevin Bleakley</p><p>Abstract: We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results. 1</p><p>6 0.14878699 <a title="39-tfidf-6" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>7 0.14798424 <a title="39-tfidf-7" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>8 0.14314154 <a title="39-tfidf-8" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>9 0.11404254 <a title="39-tfidf-9" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>10 0.1083054 <a title="39-tfidf-10" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>11 0.092733257 <a title="39-tfidf-11" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>12 0.075777374 <a title="39-tfidf-12" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>13 0.074350305 <a title="39-tfidf-13" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>14 0.072304495 <a title="39-tfidf-14" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>15 0.071491309 <a title="39-tfidf-15" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>16 0.070540741 <a title="39-tfidf-16" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>17 0.066741072 <a title="39-tfidf-17" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>18 0.066436835 <a title="39-tfidf-18" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>19 0.064868361 <a title="39-tfidf-19" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>20 0.064118974 <a title="39-tfidf-20" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, -0.003), (2, 0.031), (3, -0.085), (4, 0.049), (5, -0.091), (6, -0.008), (7, -0.033), (8, 0.1), (9, 0.13), (10, -0.017), (11, 0.052), (12, -0.057), (13, 0.08), (14, 0.167), (15, -0.065), (16, -0.103), (17, -0.044), (18, -0.151), (19, -0.099), (20, 0.175), (21, -0.179), (22, 0.252), (23, 0.007), (24, 0.167), (25, 0.158), (26, 0.089), (27, 0.094), (28, 0.055), (29, 0.01), (30, -0.045), (31, -0.099), (32, -0.056), (33, -0.005), (34, -0.069), (35, -0.052), (36, -0.147), (37, 0.028), (38, -0.06), (39, 0.063), (40, 0.027), (41, 0.168), (42, 0.021), (43, 0.01), (44, 0.079), (45, 0.168), (46, 0.029), (47, -0.005), (48, 0.155), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96941513 <a title="39-lsi-1" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>2 0.67298174 <a title="39-lsi-2" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>Author: Jacob D. Abernethy, Manfred K. Warmuth</p><p>Abstract: We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player’s best mixed strategy with knowledge of this budget. We show that, for a general class of normal-form games, the minimax strategy is indeed efﬁciently computable and relies on a “random playout” technique. We give three diverse applications of this new algorithmic template: a cost-sensitive “Hedge” setting, a particular problem in Metrical Task Systems, and the design of combinatorial prediction markets. 1</p><p>3 0.41854641 <a title="39-lsi-3" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>4 0.41250911 <a title="39-lsi-4" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>Author: Andrey Bernstein, Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the online binary classiﬁcation problem, where we are given m classiﬁers. At each stage, the classiﬁers map the input to the probability that the input belongs to the positive class. An online classiﬁcation meta-algorithm is an algorithm that combines the outputs of the classiﬁers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classiﬁers. In this paper, we use sensitivity and speciﬁcity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm that satisﬁes the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold; and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classiﬁers that satisﬁes fprate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classiﬁers, we show that this algorithm takes a very simple form. To our best knowledge, this is the ﬁrst algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting. 1</p><p>5 0.39948434 <a title="39-lsi-5" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We develop a theory of online learning by deﬁning several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fatshattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting. 1</p><p>6 0.3696475 <a title="39-lsi-6" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>7 0.35949621 <a title="39-lsi-7" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>8 0.35055637 <a title="39-lsi-8" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>9 0.34507707 <a title="39-lsi-9" href="./nips-2010-Avoiding_False_Positive_in_Multi-Instance_Learning.html">36 nips-2010-Avoiding False Positive in Multi-Instance Learning</a></p>
<p>10 0.33031854 <a title="39-lsi-10" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>11 0.31909996 <a title="39-lsi-11" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>12 0.299501 <a title="39-lsi-12" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>13 0.2847963 <a title="39-lsi-13" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>14 0.27970138 <a title="39-lsi-14" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>15 0.26341978 <a title="39-lsi-15" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>16 0.25317249 <a title="39-lsi-16" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>17 0.24855672 <a title="39-lsi-17" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>18 0.24055533 <a title="39-lsi-18" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>19 0.23526455 <a title="39-lsi-19" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>20 0.2315685 <a title="39-lsi-20" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.072), (30, 0.07), (32, 0.092), (34, 0.15), (45, 0.125), (64, 0.265), (68, 0.091), (96, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73260516 <a title="39-lda-1" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>2 0.71535075 <a title="39-lda-2" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>3 0.69340873 <a title="39-lda-3" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>4 0.67152959 <a title="39-lda-4" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>Author: Tim Rogers, Chuck Kalish, Joseph Harrison, Xiaojin Zhu, Bryan R. Gibson</p><p>Abstract: When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary. 1</p><p>5 0.67049354 <a title="39-lda-5" href="./nips-2010-Moreau-Yosida_Regularization_for_Grouped_Tree_Structure_Learning.html">170 nips-2010-Moreau-Yosida Regularization for Grouped Tree Structure Learning</a></p>
<p>Author: Jun Liu, Jieping Ye</p><p>Abstract: We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-deﬁned tree structure is based on a group-Lasso penalty, where one group is deﬁned for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efﬁcient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efﬁcient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efﬁciency and effectiveness of the proposed algorithm.</p><p>6 0.65537345 <a title="39-lda-6" href="./nips-2010-Structured_sparsity-inducing_norms_through_submodular_functions.html">258 nips-2010-Structured sparsity-inducing norms through submodular functions</a></p>
<p>7 0.64906716 <a title="39-lda-7" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>8 0.64835799 <a title="39-lda-8" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>9 0.64808124 <a title="39-lda-9" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>10 0.64772582 <a title="39-lda-10" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>11 0.64404482 <a title="39-lda-11" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>12 0.643269 <a title="39-lda-12" href="./nips-2010-Practical_Large-Scale_Optimization_for_Max-norm_Regularization.html">210 nips-2010-Practical Large-Scale Optimization for Max-norm Regularization</a></p>
<p>13 0.64005238 <a title="39-lda-13" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>14 0.63979626 <a title="39-lda-14" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>15 0.63869905 <a title="39-lda-15" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>16 0.63632852 <a title="39-lda-16" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>17 0.63628954 <a title="39-lda-17" href="./nips-2010-Predicting_Execution_Time_of_Computer_Programs_Using_Sparse_Polynomial_Regression.html">211 nips-2010-Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression</a></p>
<p>18 0.63589633 <a title="39-lda-18" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>19 0.63493025 <a title="39-lda-19" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>20 0.63192976 <a title="39-lda-20" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
