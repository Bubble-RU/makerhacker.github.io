<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-263" href="#">nips2010-263</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</h1>
<br/><p>Source: <a title="nips-2010-263-pdf" href="http://papers.nips.cc/paper/4172-switching-state-space-model-for-simultaneously-estimating-state-transitions-and-nonstationary-firing-rates.pdf">pdf</a></p><p>Author: Ken Takiyama, Masato Okada</p><p>Abstract: 019 020 We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary ﬁring rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis. 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053</p><p>Reference: <a title="nips-2010-263-reference" href="../nips2010_reference/nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e. [sent-2, score-0.751]
</p><p>2 We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. [sent-5, score-0.309]
</p><p>3 Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. [sent-6, score-0.452]
</p><p>4 We also analyze neural data that were recorded from the medial temporal area. [sent-7, score-0.158]
</p><p>5 The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. [sent-8, score-0.286]
</p><p>6 Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. [sent-9, score-0.424]
</p><p>7 Recent studies have suggested that cortical neuron activities transit among neural states in response to applied sensory stimuli[1-3]. [sent-12, score-0.198]
</p><p>8 detected state transitions among neural states using a hidden Markov model whose output distribution is multivariate Poisson distribution (multivariate-Poisson hidden Markov model(mPHMM))[1]. [sent-14, score-0.441]
</p><p>9 indicated the correspondence relationship between the time of the state transitions and the time when input properties change[2]. [sent-16, score-0.231]
</p><p>10 They also suggested that the number of neural states corresponds to the number of input properties. [sent-17, score-0.102]
</p><p>11 Assessing neural states and their transitions thus play a signiﬁcant role in elucidating neural encoding. [sent-18, score-0.307]
</p><p>12 Firing rates have state-dependent properties because mean and temporal correlations are signiﬁcantly different among all neural states[1]. [sent-19, score-0.316]
</p><p>13 We call the times of state transitions as change points. [sent-20, score-0.287]
</p><p>14 Change points are those times when the time-series data statistics change signiﬁcantly and cause nonstationarity in time-series data. [sent-21, score-0.056]
</p><p>15 In this study, stationarity means that time-series data have temporally uniform statistical properties. [sent-22, score-0.056]
</p><p>16 By this deﬁnition, data that do not have stationarity have nonstationarity. [sent-23, score-0.056]
</p><p>17 Previous studies have detected change points on the basis of discontinuous changes in mean ﬁring rates using an mPHMM. [sent-24, score-0.428]
</p><p>18 In this model, ﬁring rates in each neural state take a constant value. [sent-25, score-0.276]
</p><p>19 However, actually in motor cortex, average ﬁring rates and preferred direction change dynamically in motor planning and execution[4]. [sent-26, score-0.289]
</p><p>20 This makes it necessary to estimate state-dependent, instantaneous ﬁring rates. [sent-27, score-0.053]
</p><p>21 Medial temporal (MT) area neurons show oscillatory ﬁring rates when the target speed is modulated in the manner of a sinusoidal function[6]. [sent-29, score-0.258]
</p><p>22 These results indicate that change points also need to be detected when the temporal proﬁles of ﬁring rates change discontinuously. [sent-30, score-0.422]
</p><p>23 One solution is to simultaneously estimate both change points and instantaneous ﬁring rates. [sent-31, score-0.134]
</p><p>24 A switching state space model(SSSM)[7] can model nonstationary time-series data that include change points. [sent-32, score-0.379]
</p><p>25 It can model nonstationary time-series data while switching system models at change points. [sent-34, score-0.334]
</p><p>26 Each system model estimates stationary state variables in the region that it handles. [sent-35, score-0.093]
</p><p>27 Recent studies have been focusing on constructing algorithms for estimating ﬁring rates using single-trial data to consider trial-by-trial variations in neural activities [8]. [sent-36, score-0.306]
</p><p>28 However, these previous methods assume ﬁring rate stationarity within a trial. [sent-37, score-0.088]
</p><p>29 They cannot estimate nonstationary ﬁring rates that include change points. [sent-38, score-0.4]
</p><p>30 An SSSM may be used to estimate nonstationary ﬁring rates using single-trial data. [sent-39, score-0.344]
</p><p>31 We propose an algorithm for simultaneously estimating state transitions among neural states and nonstationary ﬁring rates using an SSSM. [sent-40, score-0.725]
</p><p>32 We expect to be able to estimate change points when not only mean ﬁring rates but also temporal proﬁles of ﬁring rates change discontinuously. [sent-41, score-0.573]
</p><p>33 Our algorithm consists of a non-Gaussian SSSM, whose non-Gaussian property is caused by binary spike events. [sent-42, score-0.118]
</p><p>34 Learning and estimation algorithms consist of variational Bayes[9,10] and local variational methods[11,12]. [sent-43, score-0.382]
</p><p>35 Automatic relevance determination (ARD) induced by the variational Bayes method[13] enables us to estimate the number of neural states after pruning redundant ones. [sent-44, score-0.345]
</p><p>36 Although many studies have discussed state transitions by analyzing multi-neuron data, some of them have suggested that single-neuron activities reﬂect state transitions in a recurrent neural network[14]. [sent-46, score-0.574]
</p><p>37 Note that we can easily extend our algorithm to multi-neuron analysis using the often-used assumption that change points are common among recorded neurons[1-3]. [sent-47, score-0.109]
</p><p>38 1 Likelihood Function Observation time T consists of K time bins of widths ∆ (ms), and each bin includes at most one spike (∆ 1). [sent-49, score-0.34]
</p><p>39 We deﬁne ηk such that ηk = +1 if the kth bin includes a spike and ηk = −1 otherwise (k = 1, . [sent-54, score-0.325]
</p><p>40 The likelihood function is deﬁned by the Bernoulli distribution 1+ηk 1−ηk ∏K p(t|λ) = k=1 (λk ∆) 2 (1 − λk ∆) 2 , (1) where λ = {λ1 , . [sent-58, score-0.034]
</p><p>41 The product of ﬁring rates and bin width corresponds to the spike-occurrence probability and λk ∆ ∈ [0, 1) since ∆ 1. [sent-62, score-0.305]
</p><p>42 The logit λk transformation of exp(2xk ) = 1−λ∆∆ (xk ∈ (−∞, ∞)) lets us consider the nonnegativity of ﬁring k rates in detail[11]. [sent-63, score-0.216]
</p><p>43 Observation time T consists of M coarse bins of widths r = C∆ (ms). [sent-70, score-0.111]
</p><p>44 A coarse bin includes many spikes and the ﬁring rate in each bin is constant. [sent-71, score-0.393]
</p><p>45 The likelihood function which is obtained by applying the logit transformation and the coarse graining to eq. [sent-72, score-0.165]
</p><p>46 (1) is ∏M p(t|x) = m=1 [exp(ˆm xm − C log 2 cosh xm )], η (2) ∑C where ηm = u=1 η(m−1)C+u . [sent-73, score-0.377]
</p><p>47 2  Switching State Space Model  1  2  M  1 x1  Firing rate  1 x2 z2  1 xM zM  ^ η2  ^ ηM  An SSSM consists of N system models; for each model, we den ﬁne a prior distribution. [sent-75, score-0.056]
</p><p>48 We deﬁne label variables zm such that Label z 1 n zm = 1 if the nth system model generates an observation in the variable n mth bin and zm = 0 otherwise (n = 1, . [sent-76, score-1.792]
</p><p>49 108 109  We call N the number of labels and the nth system model the nth label. [sent-84, score-0.449]
</p><p>50 The likelihood function, including label variables, is given by ∏N ∏M n p(t|x, z) = n=1 m=1 [exp(ˆm xn − C log 2 cosh xn )]zm . [sent-100, score-0.745]
</p><p>51 The prior distributions of ﬁring rates are Gaussian ∏N ∏N √ |β n Λ| n p(x) = n=1 p(xn |β n , µn ) = n=1 (2π)M exp(− β2 (xn − µn )T Λ(xn − µn )), (7) where β n , µn respectively mean the temporal correlation and the mean values of the nth-label ﬁring rates (n = 1, . [sent-102, score-0.479]
</p><p>52 Here for simplicity, we introduced Λ, which is the structure of the temporal ∏ n correlation satisfying p(xn |β n , µn ) ∝ m exp(− β2 ((xm − µm ) − (xm−1 − µm−1 ))2 ). [sent-106, score-0.077]
</p><p>53 Ghahramani & Hinton (2000) did not introduce a priori knowledge about the label switching frequencies. [sent-108, score-0.239]
</p><p>54 However, in many cases, the time scale of state transitions is probably slower than that of the temporal variation of ﬁring rates. [sent-109, score-0.333]
</p><p>55 C(γ n ) and C(γ nk ) correspond to the n nk normalization constants of p(π|γ ) and p(a|γ ), respectively. [sent-117, score-0.528]
</p><p>56 γ n , γ nk are hyperparameters to control the probability that the nth label is selected at the initial time and that the nth label switches to the kth. [sent-119, score-0.871]
</p><p>57 We deﬁne the prior distributions of µn and β n using non-informative priors. [sent-120, score-0.04]
</p><p>58 Since we do not have a priori knowledge about neural states, µ and β, which characterize each neural state, should be estimated from scratch. [sent-121, score-0.092]
</p><p>59 3  Estimation and Learning of non-Gaussian SSSM  It is generally computationally difﬁcult to calculate the marginal posterior distribution in an SSSM[6]. [sent-122, score-0.072]
</p><p>60 The variational free energy satisﬁes log p(t) = −F[q] + KL(q(w)q(θ) p(w, θ|t)),  (11)  where KL(q(w)q(θ) p(w, θ|t)) is the Kullback-Leibler divergence between test distributions and ∫ q(y) a posterior distribution p(w, θ|t) deﬁned by KL(q(y) p(y|t)) = dyq(y) log p(y|t) . [sent-125, score-0.406]
</p><p>61 Since the marginal likelihood log p(t) takes a constant value, the minimization of variational free energy indirectly minimizes Kullback-Leibler divergence. [sent-126, score-0.363]
</p><p>62 The variational Bayes method requires conjugacy between the likelihood function (eq. [sent-127, score-0.262]
</p><p>63 (4) and (7) are not conjugate to each other because of the binary spike events. [sent-131, score-0.118]
</p><p>64 The local variational method enables us to construct a variational Bayes algorithm for a non-Gaussian SSSM. [sent-132, score-0.434]
</p><p>65 1  Local Variational Method  The local variational method, which was proposed by Jaakola & Jordan[11], approximately transforms a non-Gaussian distribution into a quadratic-form distribution by introducing variational parameters. [sent-134, score-0.382]
</p><p>66 have proven the effectiveness of this method in estimating stationary ﬁring rates[12]. [sent-136, score-0.035]
</p><p>67 (4) includes f (xn ) = log 2 cosh xn , which is a m m concave function of y = (xn )2 . [sent-138, score-0.451]
</p><p>68 The concavity can be conﬁrmed by showing the negativity of the m second-order derivative of f (xn ) with respect to (xn )2 for all xn . [sent-139, score-0.205]
</p><p>69 (4) m m m ∏N ∏M n tanh ξ n n n pξ (t|x, z) = n=1 m=1 [exp(ˆm xn − C 2ξn m ((xn )2 − (ξm )2 )) − C log 2 cosh ξm )]zm , (12) η m m m  181 182  is a variational parameter. [sent-141, score-0.66]
</p><p>70 (12) and (7) enables us to construct the variational Bayes algorithm. [sent-147, score-0.243]
</p><p>71 (12), we ﬁnd that the variational free energy ∫∫ q(w)q(θ) Fξ [q] = dwdθq(w)q(θ) log pξ (t,w,θ) = Uξ [q] − S[q] (13) ∫∫ satisﬁes the inequality Fξ [q] ≥ F[q], where Uξ [q] = − dwdθqξ (w)qξ (θ) log pξ (s, w, θ). [sent-149, score-0.335]
</p><p>72 Since the inequality log p(t, x, z) ≥ −F[q] ≥ −Fξ [q] is satisﬁed, the test distributions that minimize Fξ [q] can indirectly minimize F[q] which is analytically intractable. [sent-150, score-0.102]
</p><p>73 Using the EM algorithm to estimate variational parameters improves the approximation accuracy of Fξ [q][16]. [sent-151, score-0.213]
</p><p>74 Under constraints ∫ ∫ ∑ dxq(x|µ, β) = 1, z q(z) = 1, dπq(π) = 1 and daq(a) = 1, we can obtain the test distributions of hidden variables xn , z that minimize eq. [sent-160, score-0.268]
</p><p>75 n In the calculation of q(xn ), zm controls the effective variance of the likelihood function. [sent-164, score-0.487]
</p><p>76 A higher n n zm means the data are reliable for the nth label in the mth bin and lower zm means the data are ∑N n unreliable. [sent-165, score-1.34]
</p><p>77 Under the constraint n=1 zm = 1, all labels estimate their ﬁring rates on the basis n of divide-and-conquer principle of data reliability. [sent-166, score-0.678]
</p><p>78 Using the equality (ξm )2 = (xn )2 that will be m ( ˆn = ηm xn − C log 2 cosh xn − C log 2 cosh 1 + developed in the next section, we obtain bm ˆ m m 2 ) −1 (W n )(m,m) / xn 2 in eq. [sent-167, score-1.091]
</p><p>79 When the mth bin includes many (few) spikes, the nth label tends m to be selected if it estimates the highest (lowest) ﬁring rate among the labels. [sent-169, score-0.579]
</p><p>80 But the variance of the nth label (W n )−1 (m,m) penalizes that label’s selection probability. [sent-170, score-0.286]
</p><p>81 We can also obtain the test distribution of parameters π, a as ∏N ∑N ˆn q(π) = C(ˆ n ) n=1 (π n )γ −1 δ( n=1 π n − 1), γ ] ∏N [ ∏N ∑N nk γ nk −1 ˆ q(a) = γ nk δ( k=1 ank − 1) , n=1 C(ˆ ) k=1 (a ) where C(ˆ n ) = γ  PN  Γ( n=1 γ n ) ˆ , Γ(ˆ 1 ). [sent-171, score-0.947]
</p><p>82 Γ(ˆ N ) γ γ  C(ˆ nk ) = γ  normalization constants of q(π) and q(a),  PN  Γ( k=1 γ nk ) ˆ . [sent-174, score-0.528]
</p><p>83 Γ(ˆ nN ) γ γ ∑M −1 n n 1 nk and γ = z1 + γ , γ = m=1 ˆ ˆ  4  (16) (17)  correspond to the n k zm zm+1 + γ nk . [sent-178, score-0.956]
</p><p>84 216 217 218 219 220 221  We can see γ n in γ n controls the probability that the nth label is selected at the initial time, and ˆ γ nk in γ nk biases the probability of the transition from the nth label to the kth label. [sent-179, score-1.152]
</p><p>85 A forwardˆ backward algorithm enables us to calculate the ﬁrst- and second-order statistics of q(z). [sent-180, score-0.093]
</p><p>86 Since an SSSM involves many local solutions, we search for a global one using deterministic annealing, which is proven to be effective for estimating and learning in an SSSM [7]. [sent-181, score-0.035]
</p><p>87 3 EM algorithm  224 225  The EM algorithm enables us to estimate variational parameters ξ and parameters µ and β. [sent-183, score-0.265]
</p><p>88 In the EM algorithm, the calculation of the Q function is computationally difﬁcult because it requires us to calculate averages using the true posterior distribution. [sent-184, score-0.097]
</p><p>89 We thus calculate the Q function using test distributions instead of the true posterior distributions as follows: ∫ ˜ Q(µ, β, ξ µ(t ) , β (t ) , ξ (t ) ) = dxq(x|µ(t ) , β (t ) )q(z)q(π)q(a) log pξ (t, x, z, π, a|µ, β). [sent-185, score-0.186]
</p><p>90 (18)  226 227 228 229 230 231  ˜ Since Q(µ, β, ξ µ(t ) , β (t ) , ξ(t ) ) = −U[q]ξ , maximizing the Q function with respect to µ, β, ξ is equivalent to minimizing the variational free energy (eq. [sent-186, score-0.267]
</p><p>91 The update rules  232 233 234 235 236 237 238 239 240  n (ξm )2 = (xn )2 , m  249 250 251 252 253 254 255 256 257 258 259 260 261 262 263  βn =  M Tr[Λ((Wn )−1 +( xn −µn )( xn −µn )T )]  (19)   Summary of our algorithm Set γ 1 and γ nk . [sent-188, score-0.674]
</p><p>92 t ← t + 1  4    Results  ˜ The estimated ﬁring rate in the mth bin is deﬁned by xm = xnm , where nm satisﬁes nm = ˜ ˜ ˜ m n n k arg maxn zm . [sent-199, score-0.814]
</p><p>93 The estimated change points mr = mC∆ satisﬁes zm > zm (∀ k = n) ˜ ˜ ˜ ˜ n k ˜ ˜ and zm+1 < zm+1 (∃ k = n). [sent-200, score-0.912]
</p><p>94 The estimated number of labels N is given by N = ˜ ˜ n N − (the number of pruned labels), where we assume that the nth label is pruned out if zm < 10−5 (∀ m). [sent-201, score-0.805]
</p><p>95 We call our algorithm “the variational Bayes switching state space model” (VB-SSSM). [sent-202, score-0.373]
</p><p>96 1  Synthetic data analysis and Comparison with previous methods  We artiﬁcially generate spike trains from arbitrarily set ﬁring rates with an inhomogeneous gamma process. [sent-204, score-0.351]
</p><p>97 Throughout this study, we set κ which means the spike irregularity to 2. [sent-205, score-0.118]
</p><p>98 We additionally conﬁrmed that the following results are invariant if we generate spikes using inhomogeneous Poisson or inverse Gaussian process. [sent-207, score-0.069]
</p><p>99 The hyperparameters γ nk represent the a priori knowledge where the time scale of transitions among labels is sufﬁciently slower than that of ﬁring-rate variations. [sent-212, score-0.523]
</p><p>100 241 242  245 246  µn = xn , m m  Accuracy of change-point detections  This section discusses the comparative results between the VB-SSSM and mPHMM regarding the accuracy of change-point detections and number-of-labels estimation. [sent-218, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zm', 0.428), ('ring', 0.402), ('sssm', 0.284), ('nk', 0.264), ('xn', 0.205), ('nth', 0.2), ('variational', 0.191), ('rates', 0.181), ('cosh', 0.181), ('transitions', 0.162), ('ank', 0.155), ('nonstationary', 0.141), ('dwd', 0.129), ('bin', 0.124), ('spike', 0.118), ('switching', 0.113), ('firing', 0.111), ('mphmm', 0.103), ('label', 0.086), ('xm', 0.081), ('bayes', 0.079), ('temporal', 0.077), ('mth', 0.074), ('discontinuous', 0.071), ('state', 0.069), ('em', 0.063), ('pn', 0.058), ('change', 0.056), ('stationarity', 0.056), ('states', 0.054), ('kth', 0.052), ('detected', 0.052), ('dxq', 0.052), ('graining', 0.052), ('enables', 0.052), ('tanh', 0.049), ('wn', 0.046), ('bm', 0.046), ('coarse', 0.044), ('activities', 0.041), ('calculate', 0.041), ('les', 0.041), ('priori', 0.04), ('distributions', 0.04), ('exp', 0.039), ('elucidating', 0.039), ('widths', 0.039), ('free', 0.038), ('spikes', 0.038), ('energy', 0.038), ('conjugacy', 0.037), ('switches', 0.035), ('logit', 0.035), ('estimating', 0.035), ('log', 0.034), ('likelihood', 0.034), ('detections', 0.034), ('medial', 0.034), ('pruned', 0.033), ('ms', 0.032), ('among', 0.032), ('kl', 0.032), ('rate', 0.032), ('posterior', 0.031), ('includes', 0.031), ('inhomogeneous', 0.031), ('hz', 0.031), ('instantaneous', 0.031), ('pro', 0.03), ('indirectly', 0.028), ('bins', 0.028), ('nn', 0.027), ('rmed', 0.026), ('nm', 0.026), ('neural', 0.026), ('motor', 0.026), ('simultaneously', 0.025), ('calculation', 0.025), ('probably', 0.025), ('labels', 0.025), ('system', 0.024), ('poisson', 0.024), ('changes', 0.023), ('hidden', 0.023), ('studies', 0.023), ('burst', 0.023), ('tridiagonal', 0.023), ('cln', 0.023), ('xnm', 0.023), ('abeles', 0.023), ('kemere', 0.023), ('sustained', 0.023), ('liation', 0.023), ('watanabe', 0.023), ('satis', 0.023), ('basis', 0.022), ('estimate', 0.022), ('suggested', 0.022), ('recorded', 0.021), ('gamma', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="263-tfidf-1" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>Author: Ken Takiyama, Masato Okada</p><p>Abstract: 019 020 We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary ﬁring rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis. 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053</p><p>2 0.1322466 <a title="263-tfidf-2" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>3 0.10977393 <a title="263-tfidf-3" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>4 0.10728881 <a title="263-tfidf-4" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>5 0.10351508 <a title="263-tfidf-5" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>6 0.10315236 <a title="263-tfidf-6" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>7 0.09623412 <a title="263-tfidf-7" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>8 0.095714383 <a title="263-tfidf-8" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>9 0.094760142 <a title="263-tfidf-9" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>10 0.086750925 <a title="263-tfidf-10" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>11 0.085399896 <a title="263-tfidf-11" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>12 0.081941657 <a title="263-tfidf-12" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>13 0.07654655 <a title="263-tfidf-13" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>14 0.076383956 <a title="263-tfidf-14" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>15 0.074390925 <a title="263-tfidf-15" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>16 0.069253325 <a title="263-tfidf-16" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>17 0.067357622 <a title="263-tfidf-17" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>18 0.066080943 <a title="263-tfidf-18" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>19 0.063983664 <a title="263-tfidf-19" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>20 0.060731288 <a title="263-tfidf-20" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.029), (2, -0.066), (3, 0.133), (4, -0.024), (5, 0.186), (6, -0.007), (7, 0.044), (8, 0.06), (9, -0.063), (10, -0.029), (11, -0.005), (12, 0.046), (13, 0.046), (14, -0.001), (15, -0.036), (16, 0.02), (17, 0.114), (18, 0.127), (19, -0.061), (20, -0.017), (21, -0.046), (22, 0.017), (23, 0.037), (24, 0.054), (25, 0.037), (26, -0.058), (27, 0.049), (28, -0.094), (29, 0.009), (30, -0.027), (31, -0.016), (32, 0.042), (33, 0.1), (34, 0.01), (35, 0.002), (36, -0.097), (37, 0.048), (38, -0.029), (39, 0.1), (40, -0.051), (41, 0.054), (42, 0.069), (43, 0.045), (44, 0.08), (45, -0.054), (46, -0.073), (47, 0.042), (48, -0.017), (49, -0.14)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95862722 <a title="263-lsi-1" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>Author: Ken Takiyama, Masato Okada</p><p>Abstract: 019 020 We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary ﬁring rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis. 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053</p><p>2 0.56716007 <a title="263-lsi-2" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>3 0.56155312 <a title="263-lsi-3" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>Author: Peter Jones, Venkatesh Saligrama, Sanjoy Mitter</p><p>Abstract: Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence. Keywords: Bayesian Methods, Information Theory, consistency</p><p>4 0.54981166 <a title="263-lsi-4" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>5 0.54835427 <a title="263-lsi-5" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>Author: Bo Thiesson, Chong Wang</p><p>Abstract: Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-speciﬁc data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is signiﬁcantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data. 1</p><p>6 0.52029252 <a title="263-lsi-6" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>7 0.51707286 <a title="263-lsi-7" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>8 0.48982367 <a title="263-lsi-8" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>9 0.48700115 <a title="263-lsi-9" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>10 0.47043335 <a title="263-lsi-10" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>11 0.46469837 <a title="263-lsi-11" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>12 0.4603067 <a title="263-lsi-12" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>13 0.45180333 <a title="263-lsi-13" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>14 0.41805127 <a title="263-lsi-14" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>15 0.41740263 <a title="263-lsi-15" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>16 0.41405183 <a title="263-lsi-16" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>17 0.40613556 <a title="263-lsi-17" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>18 0.39765465 <a title="263-lsi-18" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>19 0.38700345 <a title="263-lsi-19" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>20 0.38444895 <a title="263-lsi-20" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.026), (17, 0.032), (27, 0.073), (30, 0.069), (45, 0.176), (47, 0.288), (50, 0.054), (52, 0.063), (60, 0.064), (77, 0.032), (78, 0.02), (90, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74127001 <a title="263-lda-1" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>Author: Ken Takiyama, Masato Okada</p><p>Abstract: 019 020 We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary ﬁring rates using a switching state space model (SSSM). This algorithm enables us to detect state transitions on the basis of not only the discontinuous changes of mean ﬁring rates but also discontinuous changes in temporal proﬁles of ﬁring rates, e.g., temporal correlation. We construct a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events. Synthetic data analysis reveals that our algorithm has the high performance for estimating state transitions, the number of neural states, and nonstationary ﬁring rates compared to previous methods. We also analyze neural data that were recorded from the medial temporal area. The statistically detected neural states probably coincide with transient and sustained states that have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition on the basis of discontinuous changes in the temporal correlation of ﬁring rates, which transitions previous methods cannot detect. This result suggests that our algorithm is advantageous in real-data analysis. 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053</p><p>2 0.66007787 <a title="263-lda-2" href="./nips-2010-Multitask_Learning_without_Label_Correspondences.html">177 nips-2010-Multitask Learning without Label Correspondences</a></p>
<p>Author: Novi Quadrianto, James Petterson, Tibério S. Caetano, Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efﬁciently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces, such as integrating Yahoo! and DMOZ web directories. 1</p><p>3 0.59629816 <a title="263-lda-3" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>Author: Barnabás Póczos, Csaba Szepesvári, David Tax</p><p>Abstract: We present simple and computationally efﬁcient nonparametric estimators of R´ nyi entropy and mutual information based on an i.i.d. sample drawn from an e unknown, absolutely continuous distribution over Rd . The estimators are calculated as the sum of p-th powers of the Euclidean lengths of the edges of the ‘generalized nearest-neighbor’ graph of the sample and the empirical copula of the sample respectively. For the ﬁrst time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis. 1</p><p>4 0.59552258 <a title="263-lda-4" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>5 0.59535885 <a title="263-lda-5" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>6 0.59532648 <a title="263-lda-6" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>7 0.59206265 <a title="263-lda-7" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>8 0.59053206 <a title="263-lda-8" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>9 0.59005064 <a title="263-lda-9" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>10 0.58918792 <a title="263-lda-10" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>11 0.58890635 <a title="263-lda-11" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>12 0.58889663 <a title="263-lda-12" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>13 0.58782482 <a title="263-lda-13" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>14 0.587731 <a title="263-lda-14" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>15 0.58724499 <a title="263-lda-15" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>16 0.58685189 <a title="263-lda-16" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>17 0.58675718 <a title="263-lda-17" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>18 0.58664757 <a title="263-lda-18" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>19 0.58648884 <a title="263-lda-19" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>20 0.58641261 <a title="263-lda-20" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
