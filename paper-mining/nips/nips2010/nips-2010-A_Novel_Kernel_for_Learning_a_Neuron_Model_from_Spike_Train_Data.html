<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-10" href="#">nips2010-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</h1>
<br/><p>Source: <a title="nips-2010-10-pdf" href="http://papers.nips.cc/paper/4155-a-novel-kernel-for-learning-a-neuron-model-from-spike-train-data.pdf">pdf</a></p><p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>Reference: <a title="nips-2010-10-reference" href="../nips2010_reference/nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. [sent-3, score-1.824]
</p><p>2 We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. [sent-4, score-0.705]
</p><p>3 Many neuron models have been proposed to understand the dynamics of individual and populations of neurons. [sent-10, score-0.29]
</p><p>4 Although these models vary in complexity, at a fundamental level they are mechanisms which transform input spike trains into an output spike train. [sent-11, score-1.295]
</p><p>5 This view has found expression in the Quantitative Single-Neuron Modeling competition where submitted models compete on how accurately they can predict the output spike train of a biological neuron given an input current [2]. [sent-12, score-0.986]
</p><p>6 Since the vast majority of neurons receive input from chemical synapses [3], a stricter stipulation would be to predict output spikes based on input spike trains at the various synapses of the neuron. [sent-13, score-1.145]
</p><p>7 There are advantages to this variation of the problem: complicated subthreshold ﬂuctuations in the membrane potential need not be modeled, since models are now judged strictly on the basis of their performance at predicting the timing of output spikes. [sent-14, score-0.466]
</p><p>8 Not only does the model better represent the functional complexity of the input/output transformation of a neuron, comparisons to the real neuron can be conducted in a non-invasive manner. [sent-16, score-0.29]
</p><p>9 In this paper we learn a Spike Response Model 0 (SRM0 )[4] approximation of a neuron by only considering the timing of all afferent (incoming) and efferent (outgoing) spikes of the neuron over a bounded past. [sent-17, score-0.916]
</p><p>10 We begin by formulating the problem in a classiﬁcation based supervised learning framework where spike train data is labeled according to whether the neuron is about to spike, or has recently spiked. [sent-18, score-0.936]
</p><p>11 We then derive a novel kernel on spike trains which is computed from a dictionary of post-synaptic potential (PSP) and after-hyperpolarizing potential (AHP) like functions. [sent-20, score-0.968]
</p><p>12 For a complementary approach to learning a neuron model from spike train data, see [5]. [sent-22, score-0.936]
</p><p>13 Second, SRM0 is a relatively simple neuron model, and therefore is likely to display better generalizability on unseen input. [sent-25, score-0.353]
</p><p>14 Finally, the disparity between the learned neuron model and the actual neuron could shed light on the various operational modes of biological neurons. [sent-26, score-0.646]
</p><p>15 It is conceivable that the learned SRM0 model accurately predicts the behavior of the neuron a majority of the time. [sent-27, score-0.29]
</p><p>16 In such a case, the neuron can be seen as operating in two different modes, one SRM0 like, and the other not. [sent-29, score-0.29]
</p><p>17 Multiple models could then be learned to model the neuron in its various operational modes. [sent-30, score-0.31]
</p><p>18 We denote the arrival times of spikes at synapse j using the vector tj = tj , tj . [sent-33, score-0.626]
</p><p>19 tj j , where Nj is bounded from above by the number of spikes that can be present 1 2 N in an Υ window of time. [sent-36, score-0.273]
</p><p>20 t0 represents the output spike train of the neuron and vectors t1 . [sent-37, score-0.965]
</p><p>21 tj represents the time that has elapsed since that spike i was generated or received by the neuron. [sent-41, score-0.684]
</p><p>22 We can then formalize the membrane potential function P : RN → R, where N = j=0 Nj . [sent-43, score-0.344]
</p><p>23 , t ) is deﬁned over the space of all spike trains and reports the present membrane potential of the neuron. [sent-47, score-1.014]
</p><p>24 For notational simplicity, we deﬁne the spike conﬁguration, s ∈ RN , which represents the timing of all afferent and efferent spikes within the window of length Υ. [sent-52, score-0.952]
</p><p>25 The neuron generates a spike when P (s) = Θ, dP/dt ≥ 0. [sent-57, score-0.909]
</p><p>26 The SRM0 model uses a bounded past history as described above to calculate the present membrane potential of the ˆ neuron. [sent-59, score-0.372]
</p><p>27 The present membrane potential P is calculated as shown in Equation 1. [sent-60, score-0.344]
</p><p>28 j represents the response of the neuron to a presynaptic spike at synapse j, the PSP. [sent-62, score-1.083]
</p><p>29 At any given time, the neuron generates ˆ ˆ a spike if the membrane potential crosses the threshold from below (i. [sent-64, score-1.253]
</p><p>30 This ˆ problem is equivalent to classifying subthreshold spike conﬁgurations (P (s) < Θ) from suprathreshˆ old spike conﬁgurations (P (s) ≥ Θ), which leads to the classiﬁcation problem shown in Equation 2. [sent-69, score-1.24]
</p><p>31 It should be noted that the true membrane potential function, P , is a feasible solution to this problem since P (s) < Θ ∀s ∈ S − and P (s) ≥ Θ ∀s ∈ S + . [sent-70, score-0.344]
</p><p>32 P (s) − Θ ≥ 1 ∀s ∈ S + AND P (s) − Θ ≤ −1 ∀s ∈ S −  (2)  To generate training data which belong to S + and S − , we provide the spike conﬁgurations which occur at a ﬁxed inﬁnitesimal time differential before and after the neuron generates a spike, as illustrated in Figure 1(a). [sent-74, score-0.931]
</p><p>33 The spike train at the instant the neuron generated a spike is shown by the solid lines. [sent-75, score-1.532]
</p><p>34 We shift the spike window inﬁnitesimally into the past (future) to produce a spike conﬁguration s ∈ S − (S + ), shown by the up (down) arrows. [sent-76, score-1.26]
</p><p>35 Notice that the spike which is currently 2  generated in the output spike train, t0 , emphasized by the dashed circle, is not included in either spike conﬁguration s. [sent-77, score-1.817]
</p><p>36 First, the spike would induce an AHP effect which would cause the membrane potential to fall below the threshold. [sent-80, score-0.971]
</p><p>37 Second, if it were included, this would cause the classiﬁer to only consider whether or not that particular spike existed when classifying a given spike conﬁguration as a member of S + or S − . [sent-81, score-1.192]
</p><p>38 Although this method would work well for the training data, it would not generalize to unseen live spike train data. [sent-83, score-0.677]
</p><p>39 5  β  β=15  10  3  5  5 0  τ  0 0  100 β=0  1  0  t0  33  33  Time (ms)  66  100  Figure 1: Figure (a) depicts the spike conﬁgurations used in the classiﬁcation problem. [sent-88, score-0.596]
</p><p>40 Producing a hypersurface which can separate the supra-threshold spike conﬁgurations from the subthreshold spike conﬁgurations within the spike time feature space, would be extremely difﬁcult. [sent-91, score-1.836]
</p><p>41 As discussed above, if we could map a given spike conﬁguration s to its corresponding membrane potential P (s), then the classiﬁcation problem is trivial. [sent-92, score-0.94]
</p><p>42 Although we do not have access to the membrane potential function, we can use a linear combination of functions from a dictionary to reproduce an approximation to the membrane potential function P . [sent-93, score-0.829]
</p><p>43 The SRM0 model is an additively separable model [8], that is, the membrane potential is a sum Nj ˆ m ˆ of functions of the individual spikes of the spike conﬁguration (P (s) = j=0 i=1 Pij (tj )). [sent-96, score-1.176]
</p><p>44 This i feature lends itself well to modeling the membrane potential using a linear combination of dictionary elements. [sent-97, score-0.448]
</p><p>45 The dictionary used here was one derived from a function used by MacGregor and Lewis for neuron modeling [9]. [sent-98, score-0.394]
</p><p>46 4  Approximation of the membrane potential function  We would like to combine members of the chosen dictionary of functions to construct an approximation of the membrane potential function, P , which will yield a solution to the classiﬁcation problem posed in Equation 2. [sent-101, score-0.835]
</p><p>47 Following this we will discuss a continuous formulation, in which we combine elements drawn from an inﬁnite continuous range of β and τ parametrized dictionary functions to model P . [sent-103, score-0.199]
</p><p>48 In the discrete and continuous formulation, we will ﬁrst model the effect of a single spike for simplicity. [sent-106, score-0.668]
</p><p>49 We will conclude this section by extending the continuous formulation to the case of multiple spikes on a single synapse, and the case of multiple spikes on multiple synapses. [sent-107, score-0.393]
</p><p>50 1  Discrete Formulation  In the discrete formulation, we wish to approximate the membrane potential function using a linear combination of a ﬁnite, predeﬁned set of functions from the REEF dictionary. [sent-109, score-0.382]
</p><p>51 Focusing on the single spike case, our goal is to model the effect of a single spike on the membrane potential. [sent-110, score-1.487]
</p><p>52 We ˆ denote this effect on the membrane potential by P and it is deﬁned as a linear combination of 1 parametrized REEF functions as shown in Equation 4. [sent-111, score-0.426]
</p><p>53 , (βM , τN )} are used to construct a P that can best reproduce the effect of the spike on the membrane potential. [sent-119, score-0.909]
</p><p>54 We are concerned with ﬁnding a threshold dependent classiﬁcation ˆ ˆ ˆ function P , such that P (t) ≥ Θ + 1 when the spike t ∈ S + and P (t) ≤ Θ − 1 when t ∈ S − . [sent-128, score-0.596]
</p><p>55 Therefore, if ˆ ft (β, τ ) ∈ L2 , then P (t) is ﬁnite by the Cauchy-Schwartz inequality since α(β, τ ), ft (β, τ ) ≤ α(β, τ ) · ft (β, τ ) < ∞ if both α(β, τ ) < ∞ and ft (β, τ ) < ∞. [sent-131, score-0.46]
</p><p>56 To show that ft (β, τ ) ∈ L2 we must show ft (β, τ ), ft (β, τ ) < ∞. [sent-132, score-0.345]
</p><p>57 For ease of readability we shall henceforth suppress the domain variables in ft (β, τ ) and α(β, τ ) and refer to them as ft and α. [sent-133, score-0.23]
</p><p>58 1  Proof ∞  ∞  fx , fy = 0  = Therefore ft , ft =  0  1 β exp − τ x  exp −  x τ  1 β exp − τ y  xy (x + y)2 t·t (t+t)2  =  exp −  y dβdτ τ  (6) (7)  1 4  < ∞ ∀t ∈ [ , ∞) for some > 0. [sent-136, score-0.318]
</p><p>59 We must note here that by deﬁning the membrane potential function in this manner, we have formulated a problem which yields a solution which is different from the solution to the discrete problem. [sent-137, score-0.363]
</p><p>60 However, our deﬁnition in Equation 5 deﬁnes the “point evaluation” of our membrane potential function. [sent-140, score-0.344]
</p><p>61 M , and ym is the corresponding classiﬁcation for spike time tm (that is, ym = +1 if tm ∈ S + and ym = −1 if tm ∈ S − ). [sent-145, score-1.166]
</p><p>62 νM ∈ R, the solution to Equation 8 can be written in the form M  α=  νk ftk  (9)  k=1  Proof We consider the subspace of L2 spanned by the REEF functions evaluated at the times of the given training data points (span{ ftk : 1 ≤ k ≤ M }). [sent-159, score-0.403]
</p><p>63 α  λ k 1 − yk  +  α , ftk + α⊥ , ftk − Θ  (10)  k=1 M  Min. [sent-164, score-0.384]
</p><p>64 ym  k=1 M  νk ftk , ftm  νk K(tk , tm ) − Θ  νi νj K(ti , tj ) s. [sent-177, score-0.54]
</p><p>65 M }  ∞  K(ti , tj ) = fti , ftj =  fti ftj dβ dτ = 0  4. [sent-186, score-0.204]
</p><p>66 3  −Θ  (13)  k=1 M  0  ti tj (ti + tj )2  (14) (15)  Single Synapse  We are now in a position to extend the framework to multiple spikes on a single synapse. [sent-187, score-0.372]
</p><p>67 Since we are learning an SRM0 approximation of a neuron, we assume that the effects of spikes are additively separable [8] and that each spike’s effect on the membrane potential for the given synapse is identical. [sent-188, score-0.812]
</p><p>68 We ﬁrst deﬁne the threshold dependent classiﬁcation function for a single spike in a manner identical to that of the single spike formulation shown in Equation 5. [sent-190, score-1.233]
</p><p>69 This will be the “stereotyped” effect that a spike arriving at this synapse has on the membrane potential. [sent-191, score-1.117]
</p><p>70 Note that the AHP effect of the output spike train can be modeled seamlessly (as a virtual synapse) in this framework. [sent-192, score-0.726]
</p><p>71 1  Primal Problem  We now consider the additive effects of multiple spikes arriving at a synapse. [sent-195, score-0.217]
</p><p>72 , tmm to be the mth data point, which consists of Nm spikes, represented by 1 2 N their spike times. [sent-199, score-0.63]
</p><p>73 Instead of the superscript repeatedly referring to the synapse in question, it now refers to the data point. [sent-201, score-0.197]
</p><p>74 M }  (17)  h=1 N  k The Representer theorem states that the optimal α must lie in span{ i=1 ftk : 1 ≤ k ≤ M }. [sent-215, score-0.212]
</p><p>75 2  ftk i  (18)  i=1  k=1  Dual Problem  Substituting back Equation 18 yields the dual problem Equation 19, which can be solved given the positive deﬁnite kernel in Equation 20. [sent-219, score-0.256]
</p><p>76 2  Nk  M  ftk i  k=1 Np  p  q  K(t , t ) =  Nm  i=1  −Θ  ftm h  ftk , i  Nq  f ,  f  tq k  =  Np Nq  f ,f tp i  tq k  i=1 k=1  k=1  ≥ 1 m = {1 . [sent-221, score-0.598]
</p><p>77 Therefore, we keep the effects of each synapse on the membrane potential separate by assigning each synapse its own α function. [sent-228, score-0.761]
</p><p>78 1  Primal Problem  Since each synapse and the output has its own α function, this simply adds another summation term over the S synapses and the output (indexed by 0). [sent-231, score-0.381]
</p><p>79 S is the number of synapses, Nm,s is the number of spikes on the sth synapse of the mth data point, and tm,s is the timing of the hth spike on the sth h synapse of the mth data point. [sent-233, score-1.346]
</p><p>80 We ﬁrst considered a simplistic neuron which only received spikes on a single synapse. [sent-266, score-0.455]
</p><p>81 We then increased the complexity of the neuron, by introducing AHP effects as well as different types (excitatory and inhibitory) of afferent synapses with varying synaptic weights. [sent-267, score-0.245]
</p><p>82 Although we learned neurons with varying complexity, for want of space, we discuss here the case of a single neuron that received input spike trains from 4 excitatory synapses and 1 inhibitory synapse to mimic the ratio of connections observed in the cortex [12]. [sent-269, score-1.39]
</p><p>83 The stereotyped PSP for the excitatory and inhibitory synapses differed in their rise and fall times. [sent-270, score-0.243]
</p><p>84 We ﬁrst trained the classiﬁer using 100,000 seconds of spike train data. [sent-278, score-0.646]
</p><p>85 Only the spike conﬁgurations occurring at ﬁxed differentials before and after the neuron emitted a spike were considered. [sent-279, score-1.482]
</p><p>86 The input spike trains were generated using an inhomogeneous Poisson process, where the rate was varied sinusoidally around the intended mean spike rate in order to produce a more general set of training data. [sent-280, score-1.286]
</p><p>87 All spike conﬁgurations were considered when testing, regardless of temporal proximity to spike generation. [sent-283, score-1.229]
</p><p>88 5 0  −15 20  40  60  Time (ms)  80  100  −20 0  20  40  60  Time (ms)  80  100  Figure 2: Figure (a) shows histograms of the difference in time between the actual and predicted spike time by the learned model. [sent-292, score-0.615]
</p><p>89 Figure (b) shows the various PSP approximations (gray) in comparison to the PSP functions used by the neuron (black). [sent-293, score-0.331]
</p><p>90 Figure (c) depicts the AHP approximation (gray) and the AHP function used by the neuron (black). [sent-294, score-0.29]
</p><p>91 We also calculated a histogram of how close the spike predictions were. [sent-299, score-0.615]
</p><p>92 For every spike produced by the neuron, we determined the temporal proximity of the closest spike time predicted by the model. [sent-300, score-1.229]
</p><p>93 From the histograms, we see that the vast majority of spikes were predicted correctly (with a temporal proximity of 0 ms) and that out of the mispredicted spike times, the temporal proximity of all predicted spikes fell within 70 ms of the actual spike time. [sent-306, score-1.67]
</p><p>94 To calculate the classiﬁcation model’s approximated PSP we artiﬁcially send a single spike across each input synapse. [sent-308, score-0.596]
</p><p>95 We artiﬁcially generate a spike to produce the AHP approximation. [sent-309, score-0.616]
</p><p>96 By considering the distance of the single spike data point from the classiﬁer’s margin as the spike ages, we can get a scaled and translated version of the PSP and AHP. [sent-310, score-1.192]
</p><p>97 The technique used is noninvasive in the sense that it only requires the timing of afferent and efferent spikes within a certain bounded past. [sent-320, score-0.336]
</p><p>98 The REEF dictionary was chosen due to its similarity to PSP and AHP functions used in a neuron model proposed by MacGregor and Lewis [9]. [sent-321, score-0.413]
</p><p>99 The spike response model: a framework to predict neuronal spike trains. [sent-350, score-1.192]
</p><p>100 Generalized integrate-and-ﬁre models of neuronal activity approximate spike trains of a detailed model to a high degree of accuracy. [sent-365, score-0.67]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.596), ('neuron', 0.29), ('psp', 0.272), ('membrane', 0.264), ('ahp', 0.256), ('synapse', 0.197), ('ftk', 0.192), ('spikes', 0.165), ('reef', 0.16), ('synapses', 0.126), ('ft', 0.115), ('dictionary', 0.104), ('tm', 0.101), ('ym', 0.089), ('equation', 0.088), ('tj', 0.088), ('potential', 0.08), ('ms', 0.074), ('trains', 0.074), ('representer', 0.071), ('afferent', 0.07), ('ftm', 0.07), ('grbf', 0.064), ('efferent', 0.056), ('gurations', 0.055), ('tq', 0.052), ('train', 0.05), ('subthreshold', 0.048), ('macgregor', 0.048), ('psps', 0.048), ('timing', 0.045), ('jolivet', 0.042), ('formulation', 0.041), ('excitatory', 0.04), ('lewis', 0.04), ('tp', 0.04), ('stereotyped', 0.039), ('sth', 0.039), ('classi', 0.039), ('inhibitory', 0.038), ('span', 0.038), ('proximity', 0.037), ('guration', 0.035), ('gerstner', 0.034), ('mth', 0.034), ('kernel', 0.034), ('spiking', 0.033), ('nm', 0.033), ('parametrized', 0.032), ('fti', 0.032), ('generalizability', 0.032), ('urest', 0.032), ('additively', 0.031), ('unseen', 0.031), ('ti', 0.031), ('effect', 0.031), ('device', 0.03), ('primal', 0.03), ('dual', 0.03), ('nq', 0.029), ('arriving', 0.029), ('output', 0.029), ('neurons', 0.029), ('past', 0.028), ('con', 0.027), ('synaptic', 0.026), ('ftj', 0.026), ('kimeldorf', 0.026), ('modes', 0.025), ('nj', 0.025), ('np', 0.025), ('posed', 0.024), ('quadratic', 0.023), ('effects', 0.023), ('generates', 0.023), ('voltage', 0.023), ('exp', 0.022), ('belong', 0.022), ('continuous', 0.022), ('approximations', 0.022), ('biological', 0.021), ('separable', 0.021), ('nk', 0.02), ('window', 0.02), ('mixing', 0.02), ('modeled', 0.02), ('cations', 0.02), ('produce', 0.02), ('lie', 0.02), ('operational', 0.02), ('gray', 0.019), ('histograms', 0.019), ('discrete', 0.019), ('functions', 0.019), ('versatile', 0.019), ('substitute', 0.019), ('histogram', 0.019), ('tailored', 0.018), ('occurred', 0.018), ('reproduce', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="10-tfidf-1" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>2 0.30374026 <a title="10-tfidf-2" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>3 0.26328206 <a title="10-tfidf-3" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>Author: Sohan Seth, Park Il, Austin Brockmeier, Mulugeta Semework, John Choi, Joseph Francis, Jose Principe</p><p>Abstract: Hypothesis testing on point processes has several applications such as model ﬁtting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean ﬁring rate and time varying rate function. However, these statistics do not fully describe a point process, and therefore, the conclusions drawn by these tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. A divergence measure compares the full probability structure and, therefore, leads to a more robust test of hypothesis. We extend the traditional Kolmogorov–Smirnov and Cram´ r–von-Mises tests to the space of spike trains via stratiﬁcation, and e show that these statistics can be consistently estimated from data without any free parameter. We demonstrate an application of the proposed divergences as a cost function to ﬁnd optimally matched point processes. 1</p><p>4 0.26248118 <a title="10-tfidf-4" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>5 0.22354807 <a title="10-tfidf-5" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>6 0.19587915 <a title="10-tfidf-6" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>7 0.17551766 <a title="10-tfidf-7" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>8 0.16846067 <a title="10-tfidf-8" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>9 0.13176024 <a title="10-tfidf-9" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>10 0.11534947 <a title="10-tfidf-10" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>11 0.10455762 <a title="10-tfidf-11" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>12 0.10351508 <a title="10-tfidf-12" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>13 0.10004307 <a title="10-tfidf-13" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>14 0.076955594 <a title="10-tfidf-14" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>15 0.063418902 <a title="10-tfidf-15" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>16 0.060322993 <a title="10-tfidf-16" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>17 0.052956931 <a title="10-tfidf-17" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>18 0.051046975 <a title="10-tfidf-18" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>19 0.049362823 <a title="10-tfidf-19" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>20 0.04809479 <a title="10-tfidf-20" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, 0.042), (2, -0.184), (3, 0.238), (4, 0.138), (5, 0.335), (6, -0.06), (7, 0.117), (8, 0.132), (9, -0.079), (10, 0.006), (11, 0.128), (12, 0.064), (13, 0.195), (14, 0.109), (15, 0.031), (16, 0.068), (17, -0.113), (18, 0.039), (19, -0.228), (20, -0.005), (21, 0.087), (22, -0.216), (23, -0.055), (24, -0.071), (25, -0.029), (26, -0.02), (27, -0.063), (28, 0.022), (29, 0.039), (30, 0.035), (31, 0.007), (32, -0.034), (33, 0.05), (34, 0.023), (35, -0.001), (36, -0.005), (37, -0.013), (38, -0.002), (39, -0.036), (40, -0.025), (41, 0.04), (42, 0.053), (43, 0.057), (44, 0.015), (45, 0.012), (46, 0.037), (47, -0.035), (48, 0.025), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97081721 <a title="10-lsi-1" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>2 0.83343995 <a title="10-lsi-2" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>3 0.77874404 <a title="10-lsi-3" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>4 0.77614081 <a title="10-lsi-4" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>5 0.77407104 <a title="10-lsi-5" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>6 0.77051848 <a title="10-lsi-6" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>7 0.64348722 <a title="10-lsi-7" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>8 0.63600397 <a title="10-lsi-8" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>9 0.60294318 <a title="10-lsi-9" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>10 0.4926568 <a title="10-lsi-10" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>11 0.48273656 <a title="10-lsi-11" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>12 0.36037943 <a title="10-lsi-12" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>13 0.30295062 <a title="10-lsi-13" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>14 0.25181562 <a title="10-lsi-14" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>15 0.22699437 <a title="10-lsi-15" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>16 0.22669165 <a title="10-lsi-16" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>17 0.19853324 <a title="10-lsi-17" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>18 0.1790709 <a title="10-lsi-18" href="./nips-2010-Switched_Latent_Force_Models_for_Movement_Segmentation.html">262 nips-2010-Switched Latent Force Models for Movement Segmentation</a></p>
<p>19 0.16297941 <a title="10-lsi-19" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>20 0.15843093 <a title="10-lsi-20" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.051), (17, 0.027), (27, 0.06), (30, 0.029), (35, 0.011), (45, 0.148), (50, 0.042), (52, 0.111), (60, 0.028), (77, 0.136), (78, 0.024), (90, 0.035), (99, 0.212)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78937793 <a title="10-lda-1" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>2 0.69681782 <a title="10-lda-2" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>3 0.69244254 <a title="10-lda-3" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>Author: Rob Fergus, George Williams, Ian Spiro, Christoph Bregler, Graham W. Taylor</p><p>Abstract: This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that realworld performance can be improved through the use of synthetic data. 1</p><p>4 0.68232733 <a title="10-lda-4" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>5 0.6732167 <a title="10-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.67309916 <a title="10-lda-6" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>7 0.67266858 <a title="10-lda-7" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>8 0.66957968 <a title="10-lda-8" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>9 0.66849148 <a title="10-lda-9" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>10 0.66843814 <a title="10-lda-10" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>11 0.6640119 <a title="10-lda-11" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>12 0.66272461 <a title="10-lda-12" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>13 0.65873486 <a title="10-lda-13" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>14 0.65860045 <a title="10-lda-14" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>15 0.65670156 <a title="10-lda-15" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>16 0.65657014 <a title="10-lda-16" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>17 0.65580899 <a title="10-lda-17" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>18 0.65382272 <a title="10-lda-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.65093309 <a title="10-lda-19" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>20 0.64985341 <a title="10-lda-20" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
