<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-175" href="#">nips2010-175</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</h1>
<br/><p>Source: <a title="nips-2010-175-pdf" href="http://papers.nips.cc/paper/4034-multiparty-differential-privacy-via-aggregation-of-locally-trained-classifiers.pdf">pdf</a></p><p>Author: Manas Pathak, Shantanu Rane, Bhiksha Raj</p><p>Abstract: As increasing amounts of sensitive personal information ﬁnds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classiﬁer using classiﬁers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classiﬁer. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classiﬁer and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset. 1</p><p>Reference: <a title="nips-2010-175-reference" href="../nips2010_reference/nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract As increasing amounts of sensitive personal information ﬁnds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. [sent-7, score-0.353]
</p><p>2 Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. [sent-8, score-0.457]
</p><p>3 In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classiﬁer using classiﬁers trained locally by separate mutually untrusting parties. [sent-9, score-0.92]
</p><p>4 The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classiﬁer. [sent-10, score-1.33]
</p><p>5 We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classiﬁer and a bound on the excess risk introduced by the perturbation. [sent-11, score-1.021]
</p><p>6 In the process, however, they risk compromising the privacy of the individuals by releasing sensitive information such as their medical or ﬁnancial records, addresses and telephone numbers, preferences of various kinds which the individuals may not want exposed. [sent-15, score-0.445]
</p><p>7 In this paper, we address the problem of learning a classiﬁer from a multi-party collection of such private data. [sent-17, score-0.229]
</p><p>8 The conditions we impose are that (a) None of the parties are willing to share the data with one another or with any third party (e. [sent-30, score-0.693]
</p><p>9 Within SMC individual parties use a combination of cryptographic techniques and oblivious transfer to jointly compute a function of their private data [3, 4, 5]. [sent-35, score-0.782]
</p><p>10 The techniques typically provide guarantees that none of the parties learn anything about the individual data besides what may be inferred from the ﬁnal result of the computation. [sent-36, score-0.577]
</p><p>11 Moreover, for all but the simplest computational problems, SMC protocols tend to be highly expensive, requiring iterated encryption and decryption and repeated communication of encrypted partial results between participating parties. [sent-39, score-0.213]
</p><p>12 An alternative theoretical model for protecting the privacy of individual data instances is differential privacy [6]. [sent-40, score-0.821]
</p><p>13 A mechanism evaluated over a database is said to satisfy differential privacy if the probability of the mechanism producing a particular output is almost the same regardless of the presence or absence of any individual data instance in the database. [sent-42, score-0.597]
</p><p>14 Differential privacy provides statistical guarantees that the output of the computation does not carry information about individual data instances. [sent-43, score-0.335]
</p><p>15 We provide an alternative solution: within our approach the individual parties locally compute an optimal classiﬁer with their data. [sent-45, score-0.577]
</p><p>16 The individual classiﬁers are then averaged to obtain the ﬁnal aggregate classiﬁer. [sent-46, score-0.306]
</p><p>17 The aggregation is performed through a secure protocol that also adds a stochastic component to the averaged classiﬁer, such that the resulting aggregate classiﬁer is differentially private, i. [sent-47, score-0.691]
</p><p>18 We prove that the addition of the noise does indeed result in a differentially private classiﬁer. [sent-54, score-0.438]
</p><p>19 We also provide a bound on the true excess risk of the differentially private averaged classiﬁer compared to the optimal classiﬁer trained on the combined data. [sent-55, score-0.682]
</p><p>20 2  Differential Privacy  In this paper, we consider the differential privacy model introduced by Dwork [6]. [sent-57, score-0.43]
</p><p>21 Given any two databases D and D differing by one element, which we will refer to as adjacent databases, a randomized query function M is said to be differentially private if the probability that M produces a response S on D is close to the probability that M produces the same response S on D . [sent-58, score-0.427]
</p><p>22 Deﬁnition A randomized function M with a well-deﬁned probability density P satisﬁes differential privacy if, for all adjacent databases D and D and for any S ∈ range(M ), log  P (M (D) = S) P (M (D ) = S)  ≤ . [sent-60, score-0.457]
</p><p>23 A classiﬁer satisfying differential privacy implies that no additional details about the individual training data instances can be obtained with certainty from output of the learning algorithm, beyond the a priori background knowledge. [sent-62, score-0.588]
</p><p>24 Differential privacy provides an ad omnia guarantee as opposed to most other models that provide ad hoc guarantees against a speciﬁc set of attacks and adversarial behaviors. [sent-63, score-0.286]
</p><p>25 By evaluating the differentially private classiﬁer over a large number of test instances, an adversary cannot learn the exact form of the training data. [sent-64, score-0.452]
</p><p>26 [7] proposed the exponential mechanism for creating functions satisfying differential privacy by adding a perturbation term from the Laplace distribution scaled by the sensitivity of the function. [sent-67, score-0.591]
</p><p>27 Chaudhuri and Monteleoni [8] use the exponential mechanism [7] to create a differentially private logistic regression classiﬁer by perturbing the estimated parameters with multivariate Laplacian noise scaled by the sensitivity of the classiﬁer. [sent-68, score-0.538]
</p><p>28 They also propose another method to learn classiﬁers satisfying differential privacy by adding a linear perturbation term to the objective function which is scaled by Laplacian noise. [sent-69, score-0.514]
</p><p>29 [9] show we can create a differentially private function by adding noise from Laplace distribution scaled by the smooth sensitivity of the function. [sent-71, score-0.506]
</p><p>30 They also propose the sample and aggregate framework for replacing the original function with a related function for which the smooth sensitivity can be easily computed. [sent-73, score-0.302]
</p><p>31 Smith [10] presents a method for differentially private unbiased MLE using this framework. [sent-74, score-0.4]
</p><p>32 All the previous methods are inherently designed for the case where a single curator has access to the entire data and is interested in releasing a differentially private function computed over the data. [sent-75, score-0.541]
</p><p>33 To the best of our knowledge and belief, ours is the ﬁrst method designed for releasing a differentially private classiﬁer computed over training data owned by different parties who do not wish to disclose the data to each other. [sent-76, score-1.024]
</p><p>34 Our technique was principally motivated by the sample and aggregate framework, where we considered the samples to be owned by individual parties. [sent-77, score-0.364]
</p><p>35 Similar to [10], we choose a simple average as the aggregation function and the parties together release the perturbed aggregate classiﬁer which satisﬁes differential privacy. [sent-78, score-1.065]
</p><p>36 In the multi-party case, however, adding the perturbation to the classiﬁer is no longer straightforward and it is necessary to provide a secure protocol to do this. [sent-79, score-0.269]
</p><p>37 3  Multiparty Classiﬁcation Protocol  The problem we address is as follows: a number of parties P1 , . [sent-80, score-0.504]
</p><p>38 1  Training Local Classiﬁers on Individual Datasets  Each party Pj uses their data set (x, y)|j to learn an 2 regularized logistic regression classiﬁer with ˆ weights wj . [sent-89, score-0.245]
</p><p>39 The parties then collaborate 1 ˆ ˆ to compute an aggregate classiﬁer given by ws = K j wj + η, where η is a d-dimensional 2 random variable sampled from a Laplace distribution scaled with the parameter n(1) λ and n(1) = minj nj . [sent-94, score-1.237]
</p><p>40 As we shall see later, composing an aggregate classiﬁer in this manner incurs only a wellbounded excess risk over training a classiﬁer directly on the union of all data while enabling the parties to maintain their privacy. [sent-95, score-0.978]
</p><p>41 1 that the noise term η ensures that ˆ the classiﬁer ws satisﬁes differential privacy, i. [sent-97, score-0.475]
</p><p>42 , that individual data instances cannot be discerned from the aggregate classiﬁer. [sent-99, score-0.362]
</p><p>43 We note that the parties Pj cannot simply take 3  ˆ their individually trained classiﬁers wj , perturb them with a noise vector and publish the perturbed classiﬁers, because aggregating such classiﬁers will not give the correct η ∼ Lap 2/(n(1) λ) in general. [sent-103, score-0.814]
</p><p>44 Since individual parties cannot simply add noise to their classiﬁers to impose differential privacy, the actual averaging operation must be performed such that the individual parties do not expose their own classiﬁers or the number of data instances they possess. [sent-104, score-1.344]
</p><p>45 We therefore use a private multiparty protocol, interacting with an untrusted curator “Charlie” to perform the averaging. [sent-105, score-0.429]
</p><p>46 The outcome of the protocol is such that each of the parties obtain additive shares of the ﬁnal classiﬁer ˆ ˆ ws , such that these shares must be added to obtain ws . [sent-106, score-1.5]
</p><p>47 Privacy-Preserving Protocol We use asymmetric key additively homomorphic encryption [11]. [sent-108, score-0.213]
</p><p>48 For an additively homomorphic encryption function ξ(·), ξ(a) ξ(b) = ξ(a + b), ξ(a)b = ξ(ab). [sent-110, score-0.213]
</p><p>49 For the ensuing protocol, encryption keys are considered public and decryption keys are privately owned by the speciﬁed parties. [sent-114, score-0.231]
</p><p>50 Assuming the parties to be honest-but-curious, the steps of the protocol are as follows. [sent-115, score-0.632]
</p><p>51 Each party Pj computes nj = aj + bj , where aj and bj are integers representing additive shares of the database lengths nj for j = 1, 2, . [sent-119, score-0.634]
</p><p>52 The parties Pj mutually agree on a permutation π1 on the index vector (1, 2, . [sent-125, score-0.564]
</p><p>53 Then, each party Pj sends its share aj to party Pπ1 (j) , and sends its share bj to Charlie with the index changed according to the permutation. [sent-130, score-0.535]
</p><p>54 Thus, after this step, the parties have permuted additive shares given by π1 (a) while Charlie has permuted additive shares π1 (b). [sent-131, score-0.958]
</p><p>55 The parties Pj generate a key pair (pk,sk) where pk is a public key for homomorphic encryption and sk is the secret decryption key known only to the parties but not to Charlie. [sent-133, score-1.349]
</p><p>56 The parties send ξ(π1 (a)) = π1 (ξ(a)) to Charlie. [sent-135, score-0.504]
</p><p>57 He sends ξ(π2 (π1 (a) + r)) to the individual parties in the following order: First element to P1 , second element to P2 ,. [sent-142, score-0.579]
</p><p>58 Note also that, adding the vector collectively owned by the parties and the vector owned by Charlie would give π2 (π1 (a) + r) + π2 (π1 (b) − r) = π2 (π1 (a + b)) = π2 (π1 (n)). [sent-153, score-0.62]
</p><p>59 , K}, these comparisons ˜ ˜ b b can be solved by any implementation of a secure millionaire protocol [2]. [sent-161, score-0.234]
</p><p>60 Charlie holds only an additive share of minj nj and thus cannot know the true length of the smallest database. [sent-164, score-0.229]
</p><p>61 He generates a key-pair (pk ,sk ) for additive homomorphic function ζ(·) where only the encryption −1 key pk is publicly available to the parties Pj . [sent-170, score-0.806]
</p><p>62 Charlie then transmits ζ(π2 (u)) = −1 π2 (ζ(u)) to the parties Pj . [sent-171, score-0.527]
</p><p>63 The parties mutually obtain a permuted vector π1 (π2 (ζ(u))) = ζ(v) where π1 inverts the permutation π1 originally applied by the parties Pj in Stage I. [sent-173, score-1.115]
</p><p>64 However, since the parties Pj cannot decrypt ζ(·), they cannot ﬁnd out this index. [sent-175, score-0.504]
</p><p>65 All parties Pj now compute a d-dimensional noise vector ψ such that, for i = 1, . [sent-186, score-0.542]
</p><p>66 At this stage, the parties and Charlie have the following d-dimensional vectors: Charlie has K(η + s), ˆ ˆ P1 has w1 − Ks, and all other parties Pj , j = 2, . [sent-207, score-1.008]
</p><p>67 None of the K + 1 participants can share this data for fear of compromising differential privacy. [sent-211, score-0.226]
</p><p>68 Finally, Charlie and the K database-owning parties run a simple secure function evaluation protocol [13], at the end of which each of the K + 1 participants obtains an ˆ additive share of K ws . [sent-213, score-1.191]
</p><p>69 This protocol is provably private against honest but curious participants when there are no collisions. [sent-214, score-0.39]
</p><p>70 3  Testing Phase  A test participant Dave having a test data instance x ∈ Rd is interested in applying the trained ˆ classiﬁer adds the published shares and divides by K to get the differentially private classiﬁer ws . [sent-219, score-0.855]
</p><p>71 1  Proof of Differential Privacy  We show that the perturbed aggregate classiﬁer satisﬁes differential privacy. [sent-222, score-0.532]
</p><p>72 This would imply a change in one element in the training dataset of one party ˆj and thereby a change in the corresponding learned vector ws . [sent-231, score-0.508]
</p><p>73 Assuming that the change is in the ˆ dataset of the party Pj , the change in the learned vector is only going to be in wj ; let denote the new ˆ ˆ ˆ ˆ classiﬁer by wj . [sent-232, score-0.356]
</p><p>74 1, we bound the sensitivity of wj as wj − wj 1 ≤ nj2 λ . [sent-234, score-0.337]
</p><p>75 ˆ We ﬁrst establish a bound on the 2 norm of the difference between the aggregate classiﬁer w and the classiﬁer w∗ trained over the entire training data. [sent-239, score-0.381]
</p><p>76 Given the aggregate classiﬁer w, the classiﬁer w∗ trained over the entire training data and n(1) is the size of the smallest training dataset, ˆ w − w∗  2  6  ≤  K −1 . [sent-245, score-0.421]
</p><p>77 The largest possible n ˆ value for n(1) is K in which case all parties having an equal amount of training data and w will be closest to w∗ . [sent-248, score-0.531]
</p><p>78 In the one party case for K = 1, the bound indicates that norm of the difference ˆ would be upper bounded by zero, which is a valid sanity check as the aggregate classiﬁer w is the same as w∗ . [sent-249, score-0.458]
</p><p>79 We use this result to establish a bound on the empirical risk of the perturbed aggregate classiﬁer ˆ ˆ ws = w + η over the empirical risk of the unperturbed classiﬁer w∗ in the following theorem. [sent-250, score-0.892]
</p><p>80 The bound increases for smaller values of implying a tighter deﬁnition of differential privacy, indicating a clear trade-off between privacy and utility. [sent-256, score-0.47]
</p><p>81 The bound is also inversely proportional to n2 implying (1) an increase in excess risk when the parties have training datasets of disparate sizes. [sent-257, score-0.799]
</p><p>82 In the limiting case → ∞, we are adding a perturbation term η sampled from a Laplacian distribution of inﬁnitesimally small variance resulting in the perturbed classiﬁer being almost as same as ˆ using the unperturbed aggregate classiﬁer w satisfying a very loose deﬁnition of differential privacy. [sent-258, score-0.646]
</p><p>83 2, the excess error in using an aggregate classiﬁer is inversely proportional to the size of the smallest dataset n(1) and in the one party case K = 1, the bound ˆ becomes zero as the aggregate classiﬁer w is the same as w∗ . [sent-261, score-0.94]
</p><p>84 Also, for a small value of in the one party case K = 1 and n(1) = n, our bound reduces to that in Lemma 3 of [8], ˆ J(ws ) ≤ J(w∗ ) +  2d2 (λ + 1) log2 n2 2 λ 2  d δ  . [sent-262, score-0.201]
</p><p>85 (4)  While the previous theorem gives us a bound on the empirical excess risk over a given training ˆ dataset, it is important to consider a bound on the true excess risk of ws over w∗ . [sent-263, score-0.726]
</p><p>86 Let us denote the ˜ ˆ ˆ ˆ true risk of the classiﬁer ws by J(ws ) = E[J(ws )] and similarly, the true risk of the classiﬁer w∗ by ˜ J(w∗ ) = E[J(w∗ )]. [sent-264, score-0.411]
</p><p>87 In the following theorem, we apply the result from [14] which uses the bound on the empirical excess risk to form a bound on the true excess risk. [sent-265, score-0.347]
</p><p>88 Experiments  We perform an empirical evaluation of the proposed differentially private classiﬁer to obtain a characterization of the increase in the error due to perturbation. [sent-270, score-0.4]
</p><p>89 5  non-private all data DP all data DP aggregate n(1)=6512 DP aggregate n(1)=4884 DP aggregate n(1)=3256  0. [sent-272, score-0.771]
</p><p>90 4  ε  ˆ Figure 2: Classiﬁer performance evaluated for w∗ , w∗ + η, and ws for different data splits vs. [sent-286, score-0.293]
</p><p>91 The choice of the dataset is motivated as a realistic example for application of data privacy techniques. [sent-288, score-0.313]
</p><p>92 1 In Figure 2, we compare the test error of perturbed aggregate classiﬁers trained over data from ﬁve parties for different values of . [sent-293, score-0.949]
</p><p>93 We also compare with the error of the classiﬁer trained using combined training data and its perturbed version satisfying differential privacy. [sent-295, score-0.407]
</p><p>94 The perturbed aggregate classiﬁer which is trained using maximum n(1) = 6512 does consistently better than for lower values of n(1) which is same as our theory suggested. [sent-297, score-0.445]
</p><p>95 Also, the test error for all perturbed aggregate classiﬁers drops with , but comparatively faster for even split and converges to the test error of the classiﬁer trained over the combined data. [sent-298, score-0.467]
</p><p>96 As expected, the differentially private classiﬁer trained over the entire training data does much better than the perturbed aggregate classiﬁers with an error equal to the unperturbed classiﬁer except for small values of . [sent-299, score-0.925]
</p><p>97 The lower error of this classiﬁer is at the cost of the loss in privacy of the parties as they would need to share the data in order to train the classiﬁer over combined data. [sent-300, score-0.84]
</p><p>98 6  Conclusion  We proposed a method for composing an aggregate classiﬁer satisfying -differential privacy from classiﬁers locally trained by multiple untrusting parties. [sent-301, score-0.704]
</p><p>99 The upper bound on the excess risk of the perturbed aggregate classifer as compared to the optimal classiﬁer trained over the complete data without privacy constraints is inversely proportional to the privacy parameter , suggesting an inherent tradeoff between privacy and utility. [sent-302, score-1.547]
</p><p>100 In future work, we seek to generalize the theoretical analysis of the perturbed aggregate classiﬁer to the setting in which each party has data generated from a different distribution. [sent-305, score-0.549]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parties', 0.504), ('ws', 0.293), ('privacy', 0.286), ('aggregate', 0.257), ('charlie', 0.24), ('private', 0.229), ('er', 0.208), ('differentially', 0.171), ('party', 0.161), ('classi', 0.149), ('differential', 0.144), ('perturbed', 0.131), ('protocol', 0.128), ('encryption', 0.12), ('pj', 0.12), ('curator', 0.106), ('secure', 0.106), ('shares', 0.105), ('excess', 0.104), ('wj', 0.084), ('additive', 0.072), ('homomorphic', 0.067), ('multiparty', 0.067), ('risk', 0.059), ('ers', 0.058), ('owned', 0.058), ('secret', 0.058), ('trained', 0.057), ('instances', 0.056), ('database', 0.054), ('smallest', 0.053), ('decryption', 0.053), ('smc', 0.053), ('unperturbed', 0.053), ('permuted', 0.05), ('individual', 0.049), ('aj', 0.049), ('nj', 0.046), ('sensitivity', 0.045), ('pk', 0.043), ('stage', 0.042), ('inversely', 0.041), ('bound', 0.04), ('encrypted', 0.04), ('jaideep', 0.04), ('vaidya', 0.04), ('noise', 0.038), ('releasing', 0.035), ('nissim', 0.035), ('perturbation', 0.035), ('laplace', 0.033), ('participants', 0.033), ('dwork', 0.032), ('mechanism', 0.032), ('adult', 0.032), ('permutation', 0.03), ('minj', 0.03), ('index', 0.03), ('aggregation', 0.029), ('share', 0.028), ('obtains', 0.027), ('composing', 0.027), ('dataset', 0.027), ('training', 0.027), ('databases', 0.027), ('atallah', 0.027), ('bhiksha', 0.027), ('decrypts', 0.027), ('inverts', 0.027), ('kantarcioglu', 0.027), ('kobbi', 0.027), ('plaintext', 0.027), ('rane', 0.027), ('restated', 0.027), ('turbed', 0.027), ('untrusted', 0.027), ('untrusting', 0.027), ('additively', 0.026), ('sends', 0.026), ('satisfying', 0.026), ('bj', 0.026), ('adversary', 0.025), ('dp', 0.025), ('possess', 0.024), ('none', 0.024), ('datasets', 0.024), ('personal', 0.024), ('locally', 0.024), ('murat', 0.023), ('repositories', 0.023), ('obliviously', 0.023), ('cynthia', 0.023), ('obfuscated', 0.023), ('transmits', 0.023), ('scaled', 0.023), ('please', 0.023), ('combined', 0.022), ('individuals', 0.022), ('uci', 0.022), ('compromising', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="175-tfidf-1" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>Author: Manas Pathak, Shantanu Rane, Bhiksha Raj</p><p>Abstract: As increasing amounts of sensitive personal information ﬁnds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classiﬁer using classiﬁers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classiﬁer. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classiﬁer and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset. 1</p><p>2 0.34919572 <a title="175-tfidf-2" href="./nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">216 nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>Author: Oliver Williams, Frank Mcsherry</p><p>Abstract: We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy deﬁnition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We ﬁnd that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured. 1</p><p>3 0.1039341 <a title="175-tfidf-3" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>4 0.097097903 <a title="175-tfidf-4" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>Author: Yangqing Jia, Mathieu Salzmann, Trevor Darrell</p><p>Abstract: Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multiview learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation. 1</p><p>5 0.083017275 <a title="175-tfidf-5" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>Author: Nathan Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: √ ˜ We establish an excess risk bound of O HR2 + HL∗ Rn for ERM with an H-smooth loss n function and a hypothesis class with Rademacher complexity Rn , where L∗ is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = R/n, this translates to ˜ ˜ a learning rate of O (RH/n) in the separable (L∗ = 0) case and O RH/n + L∗ RH/n more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective. 1</p><p>6 0.067646869 <a title="175-tfidf-6" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>7 0.06409429 <a title="175-tfidf-7" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>8 0.062042717 <a title="175-tfidf-8" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>9 0.060110517 <a title="175-tfidf-9" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>10 0.057217196 <a title="175-tfidf-10" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>11 0.055952031 <a title="175-tfidf-11" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>12 0.054854859 <a title="175-tfidf-12" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>13 0.054137919 <a title="175-tfidf-13" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>14 0.053774267 <a title="175-tfidf-14" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>15 0.049343556 <a title="175-tfidf-15" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>16 0.045701481 <a title="175-tfidf-16" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>17 0.042242471 <a title="175-tfidf-17" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>18 0.04194406 <a title="175-tfidf-18" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>19 0.041842248 <a title="175-tfidf-19" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>20 0.040402781 <a title="175-tfidf-20" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, 0.04), (2, 0.041), (3, -0.056), (4, 0.03), (5, 0.063), (6, -0.08), (7, -0.017), (8, -0.051), (9, -0.069), (10, -0.013), (11, 0.01), (12, 0.007), (13, -0.027), (14, -0.045), (15, 0.036), (16, 0.034), (17, 0.154), (18, -0.034), (19, -0.016), (20, -0.047), (21, -0.022), (22, -0.11), (23, -0.026), (24, 0.023), (25, 0.01), (26, 0.03), (27, 0.143), (28, -0.054), (29, -0.062), (30, 0.218), (31, -0.18), (32, -0.042), (33, -0.117), (34, -0.129), (35, -0.112), (36, 0.159), (37, -0.001), (38, -0.219), (39, -0.206), (40, -0.229), (41, -0.178), (42, -0.051), (43, 0.093), (44, -0.106), (45, -0.081), (46, -0.039), (47, -0.265), (48, 0.109), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92211699 <a title="175-lsi-1" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>Author: Manas Pathak, Shantanu Rane, Bhiksha Raj</p><p>Abstract: As increasing amounts of sensitive personal information ﬁnds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classiﬁer using classiﬁers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classiﬁer. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classiﬁer and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset. 1</p><p>2 0.80241835 <a title="175-lsi-2" href="./nips-2010-Probabilistic_Inference_and_Differential_Privacy.html">216 nips-2010-Probabilistic Inference and Differential Privacy</a></p>
<p>Author: Oliver Williams, Frank Mcsherry</p><p>Abstract: We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy deﬁnition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We ﬁnd that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured. 1</p><p>3 0.35247204 <a title="175-lsi-3" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>4 0.3488867 <a title="175-lsi-4" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>Author: Jenna Wiens, John V. Guttag</p><p>Abstract: While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success in applying supervised machine learning to the same task. The problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients. We address these difﬁculties using active learning to perform patient-adaptive and task-adaptive heartbeat classiﬁcation. When tested on a benchmark database of cardiologist annotated ECG recordings, our method had considerably better performance than other recently proposed methods on the two primary classiﬁcation tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90% less patient-speciﬁc training data than the methods to which we compared it. 1</p><p>5 0.30902204 <a title="175-lsi-5" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>6 0.29501072 <a title="175-lsi-6" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>7 0.28727648 <a title="175-lsi-7" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>8 0.28543541 <a title="175-lsi-8" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>9 0.27712563 <a title="175-lsi-9" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<p>10 0.2740244 <a title="175-lsi-10" href="./nips-2010-Factorized_Latent_Spaces_with_Structured_Sparsity.html">89 nips-2010-Factorized Latent Spaces with Structured Sparsity</a></p>
<p>11 0.27055201 <a title="175-lsi-11" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>12 0.26930377 <a title="175-lsi-12" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>13 0.25379601 <a title="175-lsi-13" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>14 0.24901913 <a title="175-lsi-14" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>15 0.23209655 <a title="175-lsi-15" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>16 0.23138791 <a title="175-lsi-16" href="./nips-2010-Permutation_Complexity_Bound_on_Out-Sample_Error.html">205 nips-2010-Permutation Complexity Bound on Out-Sample Error</a></p>
<p>17 0.2243751 <a title="175-lsi-17" href="./nips-2010-A_New_Probabilistic_Model_for_Rank_Aggregation.html">9 nips-2010-A New Probabilistic Model for Rank Aggregation</a></p>
<p>18 0.2217242 <a title="175-lsi-18" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>19 0.22119188 <a title="175-lsi-19" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>20 0.21487407 <a title="175-lsi-20" href="./nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">278 nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.037), (17, 0.011), (27, 0.06), (30, 0.049), (35, 0.02), (45, 0.15), (50, 0.059), (52, 0.019), (60, 0.029), (77, 0.033), (78, 0.018), (81, 0.349), (90, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73824018 <a title="175-lda-1" href="./nips-2010-Multiparty_Differential_Privacy_via_Aggregation_of_Locally_Trained_Classifiers.html">175 nips-2010-Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers</a></p>
<p>Author: Manas Pathak, Shantanu Rane, Bhiksha Raj</p><p>Abstract: As increasing amounts of sensitive personal information ﬁnds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classiﬁer using classiﬁers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classiﬁer. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classiﬁer and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset. 1</p><p>2 0.48284101 <a title="175-lda-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.48085138 <a title="175-lda-3" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>Author: Han Liu, Xi Chen</p><p>Abstract: We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs). Unlike traditional dyadic decision trees (DDTs) or classiﬁcation and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty. Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of (α, C)-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets. 1</p><p>4 0.48069519 <a title="175-lda-4" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>5 0.47985765 <a title="175-lda-5" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>6 0.47827211 <a title="175-lda-6" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>7 0.47625029 <a title="175-lda-7" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>8 0.47502697 <a title="175-lda-8" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>9 0.47472101 <a title="175-lda-9" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>10 0.4737756 <a title="175-lda-10" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>11 0.47091511 <a title="175-lda-11" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>12 0.4705449 <a title="175-lda-12" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>13 0.46933603 <a title="175-lda-13" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>14 0.46923319 <a title="175-lda-14" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>15 0.46915838 <a title="175-lda-15" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>16 0.4689756 <a title="175-lda-16" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>17 0.46843877 <a title="175-lda-17" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>18 0.46839619 <a title="175-lda-18" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>19 0.46829391 <a title="175-lda-19" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>20 0.46803787 <a title="175-lda-20" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
