<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-79" href="#">nips2010-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</h1>
<br/><p>Source: <a title="nips-2010-79-pdf" href="http://papers.nips.cc/paper/4120-estimating-spatial-layout-of-rooms-using-volumetric-reasoning-about-objects-and-surfaces.pdf">pdf</a></p><p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>Reference: <a title="nips-2010-79-reference" href="../nips2010_reference/nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract There has been a recent push in extraction of 3D spatial layout of scenes. [sent-4, score-0.519]
</p><p>2 In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. [sent-6, score-0.675]
</p><p>3 We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. [sent-7, score-0.635]
</p><p>4 Understanding such a complex scene not only involves visual recognition of objects but also requires extracting the 3D spatial layout of the room (ceiling, ﬂoor and walls). [sent-9, score-1.38]
</p><p>5 Extraction of the spatial layout of a room provides crucial geometric context required for visual recognition. [sent-10, score-1.031]
</p><p>6 There has been a recent push to extract spatial layout of the room by classiﬁers which predict qualitative surface orientation labels (ﬂoor, ceiling, left, right, center wall and object) from appearance features and then ﬁt a parametric model of the room. [sent-11, score-1.493]
</p><p>7 However, such an approach is limited in that it does not use the additional information conveyed by the conﬁguration of objects in the room and, therefore, it fails to use all of the available cues for estimating the spatial layout. [sent-12, score-0.821]
</p><p>8 In this paper, we propose to incorporate an explicit volumetric representation of objects in 3D for spatial interpretation process. [sent-13, score-0.738]
</p><p>9 Unlike previous approaches which model objects by their projection in the image plane, we propose a parametric representation of the 3D volumes occupied by objects in the scene. [sent-14, score-0.604]
</p><p>10 We show that such a parametric representation of the volume occupied by an object can provide crucial evidence for estimating the spatial layout of the rooms. [sent-15, score-0.862]
</p><p>11 This evidence comes from volumetric reasoning between the objects in the room and the spatial layout of the room. [sent-16, score-1.706]
</p><p>12 We propose to augment the existing structured classiﬁcation approaches with volumetric reasoning in 3D for extracting the spatial layout of the room. [sent-17, score-1.142]
</p><p>13 Figure 1 shows an example of a case where volumetric reasoning is crucial in estimating the surface layout of the room. [sent-18, score-1.136]
</p><p>14 Figure 1(b) shows the estimated spatial layout for the room (overlaid on surface orientation labels predicted by a classiﬁer) when no reasoning about the objects is performed. [sent-19, score-1.594]
</p><p>15 In this paper, we propose a method to perform volumetric reasoning by combining classical constrained search techniques and current structured prediction techniques. [sent-23, score-0.672]
</p><p>16 (b) Estimate of the spatial layout of the room without object reasoning. [sent-25, score-1.088]
</p><p>17 approach leads to substantially improved performance on standard datasets with the added beneﬁt of a more complete scene description that includes objects in addition to surface layout. [sent-33, score-0.571]
</p><p>18 However, these approaches are only concerned with dominant directions in the 3D world and do not attempt extract three dimensional information of the room and the objects in the room. [sent-42, score-0.644]
</p><p>19 However, since this approach samples possible spatial layout hypothesis without clutter, it is prone to errors caused by the occlusion and tend to ﬁt rooms in which the walls coincide with the object surfaces. [sent-49, score-0.909]
</p><p>20 [12] uses an appearance based clutter classiﬁer and computes visual features only from the regions classiﬁed as “non-clutter”, while parameterizing the 3D structure of the scene by a box. [sent-51, score-0.44]
</p><p>21 They use structured approaches to estimate the best ﬁtting room box to the image. [sent-52, score-0.498]
</p><p>22 In these methods, however, the modeling of interactions between clutter and spatial-layout of the room is only done in the image plane and the 3D interactions between room and clutter are not considered. [sent-55, score-1.206]
</p><p>23 [14] have also modeled objects as three dimensional cuboids and considered the volumetric intersection with the room structure. [sent-57, score-1.106]
</p><p>24 Their primary goal is to improve object detection, such as beds, by using information of scene geometry, whereas our goal is to improve scene understanding by proposing a control structure that incorporates volumetric constraints. [sent-59, score-1.091]
</p><p>25 Therefore, we are able to improve the estimate of the room by estimating the objects and vice versa, whereas in their work information ﬂows in only one direction (from scene to objects). [sent-60, score-0.905]
</p><p>26 [15], qualitative reasoning of scene geometry was done by modeling objects as “blocks” for outdoor scenes. [sent-62, score-0.729]
</p><p>27 In contrast, we use stronger parameteric models for rooms and objects in indoor scenes, which are more structured, that allows us to do more explicit and exact 3D volumetric reasoning. [sent-63, score-0.737]
</p><p>28 2  Overview  Our goal is to jointly extract the spatial layout of the room and the conﬁguration of objects in the scene. [sent-64, score-1.123]
</p><p>29 We model the spatial layout of the room by 3D boxes and we model the objects as solids which occupy 3D volumes in the free space deﬁned by the room walls. [sent-65, score-1.609]
</p><p>30 Given a set of room hypotheses and object hypotheses, our goal is to search the space of scene conﬁgurations and select the conﬁguration that best matches the local surface geometry estimated from image cues and satisﬁes the volumetric constraints of the physical world. [sent-66, score-1.829]
</p><p>31 This implies that the volumetric intersection between two objects should be empty. [sent-70, score-0.623]
</p><p>32 • Containment: Every object should be contained in the free space deﬁned by the walls of the room (i. [sent-71, score-0.719]
</p><p>33 e, none of the objects should be outside the room walls). [sent-72, score-0.621]
</p><p>34 The vanishing points deﬁne the orientation of the major surfaces in the scene [6, 11, 12] and hence constrain the layout of ceilings, ﬂoor and walls of the room. [sent-75, score-1.031]
</p><p>35 Using the line segments labeled by their orientations, we then generate multiple hypotheses for rooms and objects (Figure 2(e)(f)). [sent-76, score-0.518]
</p><p>36 A hypothesis of a room is a 3D parametric representation of the layout of major surfaces of the scene, such as ﬂoor, left wall, center wall, right wall, and ceiling. [sent-77, score-1.037]
</p><p>37 A hypothesis of an object is a 3D parametric representation of an object in the scene, approximated as a cuboid. [sent-78, score-0.462]
</p><p>38 The room and cuboid hypotheses are then combined to form the set of possible conﬁgurations of the entire scene (Figure 2(h)). [sent-79, score-0.956]
</p><p>39 The conﬁguration of the entire scene is represented as one sample of the room hypothesis along with some subset of object hypotheses. [sent-80, score-0.932]
</p><p>40 The number of possible scene conﬁgurations is exponential in the number of object hypotheses 1 . [sent-81, score-0.571]
</p><p>41 However, not all cuboid and room subsets are compatible with each other. [sent-82, score-0.552]
</p><p>42 We use simple 3D spatial reasoning to enforce the volumetric constraints described above (See Figure 2(g)). [sent-83, score-0.764]
</p><p>43 We therefore test each room-object pair and each object-object pair for their 3D volumetric compatibility, so that we allow only the scene conﬁgurations which have no room-object and no object-object volumetric intersection. [sent-84, score-1.092]
</p><p>44 Finally, we evaluate the scene conﬁgurations created by combinations of room hypotheses and object hypotheses to ﬁnd the scene conﬁguration that best matches the image (Figure 2(i)). [sent-85, score-1.515]
</p><p>45 Since it is computationally expensive to test exhaustive combinations of scene conﬁgurations in practice, we use beam-search to sample the scene conﬁgurations that are volumetrically-compatible (Section 5. [sent-88, score-0.532]
</p><p>46 The estimated surface geometry is therefore used as features in a scoring function that evaluates a given scene conﬁguration. [sent-93, score-0.493]
</p><p>47 4  Generating Scene Conﬁguration Hypothesis  Given the local surface geometry and the oriented line segments extracted from the image, we now create multiple hypotheses for possible spatial layout of the room and object layout in the room. [sent-101, score-1.888]
</p><p>48 These hypotheses are then combined to produce scene conﬁguration layout such that all the objects occupy exclusive 3D volumes and the objects are inside the freespace of the room deﬁned by the walls. [sent-102, score-1.63]
</p><p>49 1  Generating Room Hypotheses  A room hypothesis encodes the position and orientation of walls, ﬂoor, and ceiling. [sent-104, score-0.662]
</p><p>50 In this paper, we represent a room hypothesis by a parametric box model [12]. [sent-105, score-0.597]
</p><p>51 They examine exhaustive combinations of line segments and check which of the resulting combinations deﬁne physically valid room models. [sent-108, score-0.627]
</p><p>52 Only the minimum number of line segments to deﬁne the parametric room model are sampled. [sent-110, score-0.585]
</p><p>53 (ii) Object hypothesis generation: we use the orientation maps to generate object hypotheses by ﬁnding convex edges. [sent-113, score-0.539]
</p><p>54 2  Generating Object Hypotheses  Our goal is to extract the 3D geometry of the clutter objects to perform 3D spatial reasoning. [sent-115, score-0.532]
</p><p>55 However, our goal is to perform coarse 3D reasoning about the spatial layout of rooms and spatial layout of objects in the room. [sent-117, score-1.405]
</p><p>56 We only need to model a subset of objects in the scene to provide enough constraints for volumetric reasoning. [sent-118, score-0.87]
</p><p>57 Furthermore, by modeling objects by a parametric model of a cuboid, we can determine the location and dimensions in 3D up to scale, which allows volumetric reasoning about the 3D interaction between objects and the room. [sent-121, score-1.021]
</p><p>58 We generate object hypotheses from the orientation map described above. [sent-122, score-0.506]
</p><p>59 We can see from the ﬁgure that the distribution of surfaces on the objects estimated by the orientation map suggests the presence of a cuboidal object. [sent-125, score-0.542]
</p><p>60 This is achieved by checking the estimated orientation of the regions and the spatial location of the regions with respect to the vanishing points. [sent-128, score-0.465]
</p><p>61 Figure 3(ii)(d) shows a sample of a cuboidal object hypothesis generated from the given orientation map. [sent-133, score-0.442]
</p><p>62 3  Volumetric Compatibility of Scene Conﬁguration  Given a room conﬁguration and a set of candidate objects, a key operation is to evaluate whether the resulting combination satisﬁes the three fundamental volumetric compatibility constraints described in Section 2. [sent-135, score-0.924]
</p><p>63 The problem of estimating the three dimensional layout of a scene from a single image is inherently ambiguous because any measurement from a single image can only be determined up to scale. [sent-136, score-0.764]
</p><p>64 In order to test the volumetric compatibility of room-object hypotheses pairs and objectobject hypotheses pairs, we make the assumption that all objects rest on the ﬂoor. [sent-137, score-0.946]
</p><p>65 This assumption ﬁxes the scale ambiguity between room and object hypotheses and allows us to reason about their 3D location. [sent-138, score-0.761]
</p><p>66 To test whether an object is contained within the free space of a room, we check whether the projection of the bottom surface of the object onto the image is completely contained within the projection of the ﬂoor surface of the room. [sent-139, score-0.706]
</p><p>67 If the projection of the bottom surface of the object is not completely 5  within the ﬂoor surface, the corresponding 3D object model must be protruding into the walls of the room. [sent-140, score-0.584]
</p><p>68 Similarly, to test whether the volume occupied by two objects is exclusive, we assume that the two objects rest on the same ﬂoor plane and we compare the projection of their bottom surfaces onto the image. [sent-142, score-0.603]
</p><p>69 1  Inference  Given an image x, a set of room hypotheses {r1 , r2 , . [sent-146, score-0.664]
</p><p>70 , om }, our goal is to ﬁnd the best scene conﬁguration y = (yr , yo ), where yr = 1 n 1 m i (yr , . [sent-152, score-0.484]
</p><p>71 yr = 1 if room hypothesis ri is used in the scene conﬁguration i i and yr = 0 otherwise, and yo = 1 if object hypothesis oi is present in the scene conﬁguration and i i yo = 0 otherwise. [sent-159, score-1.637]
</p><p>72 Note that i yr = 1 as only one room hypothesis is needed to deﬁne the scene conﬁguration. [sent-160, score-0.848]
</p><p>73 Finding the best scene conﬁguration y∗ = arg maxy f (x, y) through testing all possible scene conﬁgurations requires n · 2m evaluations of the score function. [sent-162, score-0.524]
</p><p>74 In the ﬁrst level of the search tree, scene conﬁgurations with a room hypothesis and no object hypothesis are evaluated. [sent-164, score-1.04]
</p><p>75 2  Learning the Score Function  T We set the score function to f (x, y) = wT ψ(x, y) + wφ φ(y), where ψ(x, y) is a feature vector for a given image x and measures the compatibility of the scene conﬁguration y with the estimated surface geometry. [sent-170, score-0.544]
</p><p>76 φ(y) is the penalty term for incompatible conﬁgurations and penalizes the room and object conﬁgurations which violate volumetric constraints. [sent-171, score-1.084]
</p><p>77 Feature Vector: The feature vector ψ(x, y) is computed by measuring how well each surface in the scene conﬁguration y is supported by the orientation map and the geometric context. [sent-181, score-0.65]
</p><p>78 6  Input image  Orientation map  Geometric context  Room only  Room and objects  Figure 4: Two qualitative examples showing how 3D volumetric reasoning aids estimation of the spatial layout of the room. [sent-184, score-1.423]
</p><p>79 OM+GC No object reasoning Volumetric reasoning  OM  GC  18. [sent-185, score-0.539]
</p><p>80 2%  Table 1: Percentage of pixels with correct estimate of room surfaces. [sent-191, score-0.47]
</p><p>81 Second row is our approach with 3D volumetric reasoning of objects. [sent-193, score-0.606]
</p><p>82 is computed for each of the six surfaces in the scene conﬁguration (ﬂoor, left wall, center wall, right wall, ceiling, object) as the relative area which the orientation map or the geometric context correctly explains the attribute of the surface. [sent-197, score-0.658]
</p><p>83 For example, the feature for the ﬂoor surface in the scene conﬁguration is computed by the relative area which the orientation map predicts a horizontal surface, and the area which the geometric context predicts a ﬂoor label. [sent-199, score-0.711]
</p><p>84 Volumetric Penalty: The penalty term φ(y) measures how much the volumetric constraints are violated. [sent-200, score-0.458]
</p><p>85 (1) The ﬁrst term φ(yr , yo ) measures the volumetric intersection between the volume deﬁned by room walls and objects. [sent-201, score-1.136]
</p><p>86 It penalizes the conﬁgurations where the object hypothesis lie outside the room volume and the penalty is proportional to the volume outside the room. [sent-202, score-0.763]
</p><p>87 The dataset consists of 314 images, and the ground-truth consists of the marked spatial layout of the room and the clutter layouts. [sent-206, score-1.03]
</p><p>88 If no 3D clutter reasoning is used and the room box is ﬁtted to the orientation map and geometric context, the box gets ﬁt to the object surfaces and therefore leads to substantial error in the spatial layout estimation. [sent-210, score-1.797]
</p><p>89 However, if we use 3D object reasoning walls get pushed due to the containment constraint and the spatial layout estimation improves. [sent-211, score-0.995]
</p><p>90 We can also see from the examples that extracting a subset of objects in the scene is enough for reasoning and improving the spatial layout estimation. [sent-212, score-1.124]
</p><p>91 Figure 5 and 6 shows more examples of the spatial layout and the estimated clutter objects in the images. [sent-213, score-0.786]
</p><p>92 Quantitative Evaluation: We evaluate the performance of our approach in estimating the spatial layout of the room. [sent-215, score-0.511]
</p><p>93 We use the pixel-based measure introduced in [12] which counts the percentage of pixels on the room surfaces that disagree with the ground truth. [sent-216, score-0.572]
</p><p>94 Dotted lines represent the room estimate without object reasoning. [sent-223, score-0.609]
</p><p>95 approach which uses a parametric model of clutter and simple 3D volumetric reasoning outperforms both the approaches and has an error of 16. [sent-227, score-0.772]
</p><p>96 When we only use the surface layout estimates from [8] as features of the cost function, our approach has an error rate of 20. [sent-230, score-0.48]
</p><p>97 7  Conclusion  In this paper, we have proposed the use of volumetric reasoning between objects and surfaces of room layout to recover the spatial layout of a scene. [sent-236, score-2.148]
</p><p>98 By parametrically representing the 3D volume of objects and rooms, we can apply constraints for volumetric reasoning, such as spatial exclusion and containment. [sent-237, score-0.82]
</p><p>99 Our experiments show that volumetric reasoning improves the estimate of the room layout and provides a richer interpretation about objects in the scene. [sent-238, score-1.567]
</p><p>100 The rich geometric information provided by our method can provide crucial information for object recognition and eventually aid in complete scene understanding. [sent-239, score-0.508]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('room', 0.442), ('volumetric', 0.42), ('layout', 0.34), ('scene', 0.252), ('reasoning', 0.186), ('objects', 0.179), ('object', 0.167), ('hypotheses', 0.152), ('orientation', 0.149), ('oor', 0.147), ('surface', 0.14), ('spatial', 0.139), ('guration', 0.124), ('wall', 0.112), ('walls', 0.11), ('cuboid', 0.11), ('clutter', 0.109), ('yo', 0.108), ('surfaces', 0.102), ('gurations', 0.095), ('yr', 0.083), ('geometry', 0.082), ('rooms', 0.082), ('con', 0.08), ('vanishing', 0.078), ('occupied', 0.077), ('hypothesis', 0.071), ('geometric', 0.071), ('image', 0.07), ('segments', 0.06), ('parametric', 0.057), ('indoor', 0.056), ('couch', 0.055), ('cuboidal', 0.055), ('hedau', 0.055), ('ceiling', 0.048), ('beam', 0.044), ('compatibility', 0.043), ('volumes', 0.042), ('om', 0.041), ('cuboids', 0.041), ('kosecka', 0.041), ('regions', 0.04), ('map', 0.038), ('search', 0.037), ('incompatible', 0.036), ('plane', 0.034), ('manhattan', 0.033), ('volume', 0.032), ('estimating', 0.032), ('rectangular', 0.032), ('containment', 0.031), ('exclusion', 0.031), ('qualitative', 0.03), ('gc', 0.029), ('hoiem', 0.029), ('cues', 0.029), ('structured', 0.029), ('pixels', 0.028), ('extracting', 0.028), ('combinations', 0.028), ('box', 0.027), ('blockworld', 0.027), ('eccv', 0.026), ('line', 0.026), ('occupy', 0.025), ('center', 0.025), ('intersection', 0.024), ('cubes', 0.024), ('quadrilaterals', 0.024), ('extract', 0.023), ('check', 0.022), ('pushed', 0.022), ('invalid', 0.022), ('commitment', 0.022), ('configuration', 0.022), ('drawings', 0.022), ('extraction', 0.022), ('context', 0.021), ('orientations', 0.021), ('gupta', 0.021), ('camera', 0.021), ('lee', 0.021), ('physically', 0.021), ('cube', 0.021), ('parameterizing', 0.021), ('predicts', 0.02), ('failure', 0.02), ('score', 0.02), ('constraints', 0.019), ('estimated', 0.019), ('penalty', 0.019), ('labeled', 0.019), ('cluttered', 0.019), ('exclusive', 0.019), ('hebert', 0.019), ('appearance', 0.018), ('push', 0.018), ('tree', 0.018), ('crucial', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="79-tfidf-1" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>2 0.23420642 <a title="79-tfidf-2" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>3 0.19663212 <a title="79-tfidf-3" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>4 0.1439812 <a title="79-tfidf-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>5 0.14390692 <a title="79-tfidf-5" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>6 0.11886198 <a title="79-tfidf-6" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>7 0.10785339 <a title="79-tfidf-7" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>8 0.10089908 <a title="79-tfidf-8" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>9 0.099888101 <a title="79-tfidf-9" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>10 0.09278217 <a title="79-tfidf-10" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>11 0.087215982 <a title="79-tfidf-11" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>12 0.077540971 <a title="79-tfidf-12" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>13 0.076527826 <a title="79-tfidf-13" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>14 0.075310409 <a title="79-tfidf-14" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>15 0.0710214 <a title="79-tfidf-15" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>16 0.066365406 <a title="79-tfidf-16" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>17 0.061736461 <a title="79-tfidf-17" href="./nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">141 nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>18 0.061366547 <a title="79-tfidf-18" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>19 0.055382732 <a title="79-tfidf-19" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>20 0.055364691 <a title="79-tfidf-20" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, 0.078), (2, -0.163), (3, -0.211), (4, -0.001), (5, -0.047), (6, -0.083), (7, -0.008), (8, 0.056), (9, 0.049), (10, 0.039), (11, 0.042), (12, -0.161), (13, 0.035), (14, 0.042), (15, 0.045), (16, 0.127), (17, -0.101), (18, 0.153), (19, 0.086), (20, 0.043), (21, -0.024), (22, -0.001), (23, 0.036), (24, -0.003), (25, -0.11), (26, -0.055), (27, 0.089), (28, 0.03), (29, -0.022), (30, -0.068), (31, -0.026), (32, 0.04), (33, -0.054), (34, -0.041), (35, 0.001), (36, 0.004), (37, 0.049), (38, 0.001), (39, -0.058), (40, -0.017), (41, 0.001), (42, 0.064), (43, -0.065), (44, 0.12), (45, 0.023), (46, -0.004), (47, -0.076), (48, 0.021), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97547215 <a title="79-lsi-1" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>2 0.82866901 <a title="79-lsi-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.80275261 <a title="79-lsi-3" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>4 0.75576651 <a title="79-lsi-4" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>Author: Mario Fritz, Kate Saenko, Trevor Darrell</p><p>Abstract: Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata ﬁelds specifying camera intrinstics. We develop an efﬁcient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata. 1</p><p>5 0.63763642 <a title="79-lsi-5" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>6 0.60085773 <a title="79-lsi-6" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>7 0.58829498 <a title="79-lsi-7" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>8 0.57515168 <a title="79-lsi-8" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>9 0.56079108 <a title="79-lsi-9" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>10 0.54808044 <a title="79-lsi-10" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>11 0.5377602 <a title="79-lsi-11" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>12 0.5306415 <a title="79-lsi-12" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>13 0.49745262 <a title="79-lsi-13" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>14 0.49230847 <a title="79-lsi-14" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>15 0.46055314 <a title="79-lsi-15" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>16 0.4531135 <a title="79-lsi-16" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>17 0.42140356 <a title="79-lsi-17" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>18 0.3751846 <a title="79-lsi-18" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>19 0.35627246 <a title="79-lsi-19" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>20 0.34394047 <a title="79-lsi-20" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.028), (17, 0.012), (25, 0.293), (27, 0.076), (30, 0.051), (35, 0.021), (45, 0.205), (50, 0.056), (52, 0.019), (60, 0.016), (77, 0.04), (78, 0.016), (90, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83579552 <a title="79-lda-1" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>same-paper 2 0.81431544 <a title="79-lda-2" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>3 0.72487128 <a title="79-lda-3" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>4 0.71402758 <a title="79-lda-4" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>5 0.64105642 <a title="79-lda-5" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>6 0.63318974 <a title="79-lda-6" href="./nips-2010-Multivariate_Dyadic_Regression_Trees_for_Sparse_Learning_Problems.html">178 nips-2010-Multivariate Dyadic Regression Trees for Sparse Learning Problems</a></p>
<p>7 0.631935 <a title="79-lda-7" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>8 0.62967587 <a title="79-lda-8" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>9 0.62947524 <a title="79-lda-9" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>10 0.62609172 <a title="79-lda-10" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>11 0.62593138 <a title="79-lda-11" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>12 0.62407792 <a title="79-lda-12" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>13 0.62375391 <a title="79-lda-13" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>14 0.62317443 <a title="79-lda-14" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>15 0.62287599 <a title="79-lda-15" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>16 0.6223951 <a title="79-lda-16" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>17 0.62176222 <a title="79-lda-17" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>18 0.62151814 <a title="79-lda-18" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>19 0.62098306 <a title="79-lda-19" href="./nips-2010-Probabilistic_latent_variable_models_for_distinguishing_between_cause_and_effect.html">218 nips-2010-Probabilistic latent variable models for distinguishing between cause and effect</a></p>
<p>20 0.62047994 <a title="79-lda-20" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
