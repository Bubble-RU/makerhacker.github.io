<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2010-Fractionally Predictive Spiking Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-96" href="#">nips2010-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 nips-2010-Fractionally Predictive Spiking Neurons</h1>
<br/><p>Source: <a title="nips-2010-96-pdf" href="http://papers.nips.cc/paper/3983-fractionally-predictive-spiking-neurons.pdf">pdf</a></p><p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>Reference: <a title="nips-2010-96-reference" href="../nips2010_reference/nips-2010-Fractionally_Predictive_Spiking_Neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl  Abstract Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. [sent-9, score-0.843]
</p><p>2 Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. [sent-10, score-1.063]
</p><p>3 A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. [sent-11, score-0.451]
</p><p>4 Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. [sent-12, score-0.942]
</p><p>5 For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. [sent-13, score-0.595]
</p><p>6 As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. [sent-14, score-1.234]
</p><p>7 1  Introduction  A key issue in computational neuroscience is the interpretation of neural signaling, as expressed by a neuron’s sequence of action potentials. [sent-15, score-0.026]
</p><p>8 An emerging notion is that neurons may in fact encode information at multiple timescales simultaneously [1, 2, 3, 4]: the precise timing of spikes may be conveying high-frequency information, and slower measures, like the rate of spiking, may be relating low-frequency information. [sent-16, score-0.699]
</p><p>9 Such multi-timescale encoding comes naturally, at least for sensory neurons, as the statistics of the outside world often exhibit self-similar multi-timescale features [5] and the magnitude of natural signals can extend over several orders. [sent-17, score-0.465]
</p><p>10 Since neurons are limited in the rate and resolution with which they can emit spikes, the mapping of large dynamic-range signals into spike-trains is an integral part of attempts at understanding neural coding. [sent-18, score-0.461]
</p><p>11 Experiments have extensively demonstrated that neurons adapt their response when facing persistent changes in signal magnitude. [sent-19, score-0.575]
</p><p>12 Typically, adaptation changes the relation between the magnitude of the signal and the neuron’s discharge rate. [sent-20, score-0.433]
</p><p>13 Since adaptation thus naturally relates to neural coding, it has been extensively scrutinized [6, 7, 8]. [sent-21, score-0.169]
</p><p>14 Tying the notions of self-similar multi-scale natural signals and adaptive neural coding together, it has recently been suggested that neuronal adaptation allows neuronal spiking to communicate a fractional derivative of the actual computed signal [10, 4]. [sent-25, score-1.844]
</p><p>15 Fractional derivatives are a generalization of standard ‘integer’ derivatives (‘ﬁrst order’, ‘second order’), to real valued derivatives (e. [sent-26, score-0.261]
</p><p>16 A key feature of such derivatives is that they are non-local, and rather convey information over essentially a large part of the signal spectrum [10]. [sent-30, score-0.402]
</p><p>17 1  Here, we show how neural spikes can encode temporal signals when the spike-train itself is taken as the fractional derivative of the signal. [sent-31, score-1.4]
</p><p>18 We show that this is the case for a signal approximated by a sum of shifted power-law kernels starting at respective times ti and decaying proportional to 1/(t − ti )β . [sent-32, score-0.853]
</p><p>19 Then, the fractional derivative of this approximated signal corresponds to a sum of spikes at times ti , provided that the order of fractional differentiation α is equal to 1 − β: a spiketrain is the α = 0. [sent-33, score-2.098]
</p><p>20 2 fractional derivative of a signal approximated by a sum of power-law kernels with exponent β = 0. [sent-34, score-1.347]
</p><p>21 Such signal encoding with power-law kernels can be carried out for example with simple standard thresholding spiking neurons with a refractory reset following a power-law. [sent-36, score-0.997]
</p><p>22 As fractional derivatives contain information over many time-ranges, they are naturally suited for predicting signals. [sent-37, score-0.61]
</p><p>23 This links to notions of predictive coding, where neurons communicate deviations from expected signals rather than the signal itself. [sent-38, score-1.036]
</p><p>24 Predictive coding has been suggested as a key feature of neuronal processing in e. [sent-39, score-0.182]
</p><p>25 For self-similar scale-free signals, future signals may be inﬂuenced by past signals over very extended time-ranges: so-called longmemory. [sent-42, score-0.568]
</p><p>26 For example, fractional Brownian motion (fBm) can exhibit long-memory, depending on their Hurst-parameter H. [sent-43, score-0.555]
</p><p>27 5 fBM models which exhibit long-range dependence (longmemory) where the autocorrelation-function follows a power-law decay [12]. [sent-45, score-0.12]
</p><p>28 The long-memory nature of signals approximated with sums of power-law kernels naturally extends this signal approximation into the future along the autocorrelation of the signal, at least for self-similar 1/f γ like signals. [sent-46, score-1.038]
</p><p>29 The key “predictive” assumption we make is that a neuron’s spike-train up to time t contains all the information that the past signal contributes to the future signal t > t. [sent-47, score-0.614]
</p><p>30 The correspondence between a spike-train as a fractional derivative and a signal approximated as a sum of power-law kernels is only exact when spike-trains are taken as a sum of Dirac-δ functions and the power-law kernels as 1/tβ . [sent-48, score-1.538]
</p><p>31 As both responses are singular, neurons would only be able to approximate this. [sent-49, score-0.188]
</p><p>32 We show empirically how sums of (approximated) 1/tβ power-law kernels can accurately approximate long-memory fBm signals via simple difference thresholding, in an online greedy fashion. [sent-50, score-0.5]
</p><p>33 Thus encodings signals, we show that the power-law kernels approximate synthesized signals with about half the number of spikes to obtain the same Signal-to-Noise-Ratio, when compared to the same encoding method using similar but exponentially decaying kernels. [sent-51, score-1.026]
</p><p>34 We further demonstrate the approximation of sine wave modulated white-noise signals with sums of power-law kernels. [sent-52, score-0.525]
</p><p>35 We ﬁnd the effect is stronger when encoding the actual sine wave envelope, mimicking the difference between thalamic and cortical neurons reported in [4]. [sent-54, score-0.636]
</p><p>36 This may suggest that these cortical neurons are more concerned with encoding the sine wave envelope. [sent-55, score-0.507]
</p><p>37 The power-law approximation also allows for the transparent and straightforward implementation of temporal signal ﬁltering by a post-synaptic, receiving neuron. [sent-56, score-0.514]
</p><p>38 Since neural decoding by a receiving neuron corresponds to adding a power-law kernel for each received spike, modifying this receiving power-law kernel then corresponds to a temporal ﬁltering operation, effectively exploiting the wide-spectrum nature of power-law kernels. [sent-57, score-0.619]
</p><p>39 This is particularly relevant, since, as has been amply noted [9, 14], power-law dynamics can be closely approximated by a weighted sum or cascade of exponential kernels. [sent-58, score-0.238]
</p><p>40 Temporal ﬁltering would then correspond to simply tuning the weights for this sum or cascade. [sent-59, score-0.06]
</p><p>41 We illustrate this notion with an encoding/decoding example for both a high-pass and low-pass ﬁlter. [sent-60, score-0.03]
</p><p>42 2  Power-law Signal Encoding  Neural processing can often be reduced to a Linear-Non-Linear (LNL) ﬁltering operation on incoming signals [15] (ﬁgure 1), where inputs are linearly weighted and then passed through a non-linearity to yield the neural activation. [sent-61, score-0.368]
</p><p>43 As this computation yields analog activations, and neurons communicate through spikes, the additional problem faced by spiking neurons is to decode the incoming signal and then encode the computed LNL ﬁlter again into a spike-train. [sent-62, score-0.985]
</p><p>44 The standard spiking neuron model is that of Linear-Nonlinear-Poisson spiking, where spikes have a stochastic relationship to the computed activation [16]. [sent-63, score-0.695]
</p><p>45 Here, we interpret the spike encoding and decoding in the light of processing and communicating signals with fractional derivatives [10]. [sent-64, score-1.1]
</p><p>46 and summing these kernels [17]; keeping track of doublets and triplet spikes allows for even greater ﬁdelity. [sent-66, score-0.567]
</p><p>47 This approach however only worked for signals with a frequency response lacking low frequencies [17]. [sent-67, score-0.347]
</p><p>48 Low-frequency changes lead to “adaptation”, where the kernel is adapted to ﬁt the signal again [18]. [sent-68, score-0.38]
</p><p>49 For long-range predictive coding, the absence of low frequencies leaves little to predict, as the effective correlation time of the signals is then typically very short as well [17]. [sent-69, score-0.448]
</p><p>50 Using the notion of predictive coding in the context of (possible) long-range dependencies, we deﬁne the goal of signal encoding as follows: let a signal xj (t) be the result of the continuous-time computation in neuron j up to time t, and let neuron j have emitted spikes tj up to time t. [sent-70, score-1.719]
</p><p>51 These spikes should be emitted such that the signal xj (t ) for t < t is decoded up to some signal-to-noise ratio, and these spikes should be predictive for xj (t ) for t > t in the sense that no additional spikes are needed at times t > t to convey the predictive information up to time t. [sent-71, score-1.86]
</p><p>52 Taking kernels as a signal ﬁlter of ﬁxed width, as in the general approach in [17] has the important drawback that the signal reconstruction incurs a delay for the duration of the ﬁlter: its detection cannot be communicated until the ﬁlter is actually matched to the signal. [sent-72, score-0.784]
</p><p>53 Alternatively, a predictive coding approach could rely on only on a very short backward looking ﬁlter, minimizing the delay in the system, and continuously computing a forward predictive signal. [sent-74, score-0.485]
</p><p>54 At any time in the future then, only deviations of the actual signal from this expectation are communicated. [sent-75, score-0.372]
</p><p>55 1  Spike-trains as fractional derivative  As recent work has highlighted the possibility that neurons encode fractional derivatives, it is noteworthy that the non-local nature of fractional calculus offers a natural framework for predictive coding. [sent-77, score-2.102]
</p><p>56 The fractional derivative r(t) of a signal x(t) is denoted as Dα x(t), and intuitively expresses: r(t) =  dα x(t), dtα  where α is the fractional order, e. [sent-79, score-1.416]
</p><p>57 We assume that neurons carry out predictive coding by emitting spikes such that all predictive information is contained in the current spikes, and no more spikes will be ﬁred if the signal follows this prediction. [sent-84, score-1.669]
</p><p>58 Approximating spikes by Dirac-δ functions, we take the spike-train up to some time t0 to be the fractional derivative of the past signal and be fully predictive for the expected inﬂuence the 3  Fractionally Predicting Spikes  a)  x(t) r(t)  0  c)  0. [sent-85, score-1.507]
</p><p>59 3  time (s)  Non−singular kernels  b)  x(t) r(t) α-exp(τ=10ms)  0  0. [sent-88, score-0.172]
</p><p>60 c) Approximated 1/tβ power-law kernel for different values of k from eq. [sent-95, score-0.06]
</p><p>61 d) The approximated 1/tβ power-law kernel (blue line) can be decomposed as a weighted sum of α-functions with various decay time-constants (dashed lines). [sent-97, score-0.353]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fractional', 0.49), ('spikes', 0.368), ('signal', 0.27), ('signals', 0.247), ('neurons', 0.188), ('kernels', 0.172), ('neuron', 0.17), ('predictive', 0.169), ('derivative', 0.166), ('spiking', 0.157), ('approximated', 0.148), ('encoding', 0.121), ('fbm', 0.112), ('lnl', 0.112), ('coding', 0.102), ('sine', 0.099), ('communicate', 0.092), ('receiving', 0.089), ('derivatives', 0.087), ('sums', 0.081), ('adaptation', 0.081), ('fractionally', 0.075), ('transparent', 0.075), ('decoding', 0.074), ('ti', 0.073), ('lter', 0.07), ('wave', 0.069), ('ltering', 0.066), ('powerlaw', 0.066), ('cwi', 0.066), ('exhibit', 0.065), ('timed', 0.06), ('sum', 0.06), ('kernel', 0.06), ('decaying', 0.057), ('decay', 0.055), ('decoded', 0.054), ('amsterdam', 0.054), ('encode', 0.052), ('suppression', 0.051), ('spike', 0.051), ('temporal', 0.051), ('changes', 0.05), ('emitted', 0.049), ('refractory', 0.049), ('neuronal', 0.049), ('netherlands', 0.047), ('delay', 0.045), ('convey', 0.045), ('past', 0.044), ('actual', 0.043), ('exponent', 0.041), ('notions', 0.041), ('thresholding', 0.04), ('response', 0.038), ('incoming', 0.038), ('life', 0.038), ('carry', 0.035), ('fourier', 0.035), ('naturally', 0.033), ('bohte', 0.033), ('timescales', 0.033), ('encodings', 0.033), ('rombouts', 0.033), ('spiketrain', 0.033), ('magnitude', 0.032), ('frequencies', 0.032), ('ms', 0.031), ('suggested', 0.031), ('weighted', 0.03), ('mimicking', 0.03), ('thalamic', 0.03), ('exponentials', 0.03), ('signaling', 0.03), ('communicating', 0.03), ('noteworthy', 0.03), ('lacking', 0.03), ('future', 0.03), ('cortical', 0.03), ('notion', 0.03), ('extensively', 0.029), ('deviations', 0.029), ('approximation', 0.029), ('synthesized', 0.028), ('autocorrelation', 0.028), ('conveying', 0.028), ('tying', 0.028), ('singular', 0.028), ('retina', 0.027), ('calculus', 0.027), ('triplet', 0.027), ('communicated', 0.027), ('snr', 0.027), ('exponents', 0.027), ('operation', 0.027), ('dashed', 0.027), ('neural', 0.026), ('stronger', 0.026), ('modulation', 0.026), ('delity', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="96-tfidf-1" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>2 0.17551766 <a title="96-tfidf-2" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>3 0.14596979 <a title="96-tfidf-3" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>4 0.14265709 <a title="96-tfidf-4" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>5 0.135794 <a title="96-tfidf-5" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>6 0.11861367 <a title="96-tfidf-6" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>7 0.11645449 <a title="96-tfidf-7" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>8 0.11356699 <a title="96-tfidf-8" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>9 0.10718781 <a title="96-tfidf-9" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>10 0.10503395 <a title="96-tfidf-10" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>11 0.10258681 <a title="96-tfidf-11" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>12 0.10108102 <a title="96-tfidf-12" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>13 0.098402739 <a title="96-tfidf-13" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>14 0.087655567 <a title="96-tfidf-14" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>15 0.087499008 <a title="96-tfidf-15" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>16 0.084667698 <a title="96-tfidf-16" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>17 0.08214736 <a title="96-tfidf-17" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>18 0.080492489 <a title="96-tfidf-18" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>19 0.07568524 <a title="96-tfidf-19" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>20 0.069372326 <a title="96-tfidf-20" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.047), (2, -0.193), (3, 0.189), (4, 0.161), (5, 0.203), (6, 0.031), (7, 0.099), (8, 0.067), (9, -0.042), (10, 0.021), (11, 0.05), (12, 0.024), (13, 0.069), (14, 0.001), (15, -0.009), (16, 0.006), (17, -0.004), (18, -0.077), (19, -0.042), (20, 0.047), (21, 0.007), (22, -0.039), (23, -0.036), (24, -0.043), (25, -0.093), (26, -0.011), (27, -0.042), (28, 0.024), (29, 0.01), (30, 0.02), (31, 0.067), (32, -0.091), (33, 0.016), (34, 0.01), (35, -0.004), (36, 0.056), (37, 0.033), (38, 0.0), (39, 0.045), (40, 0.082), (41, 0.034), (42, 0.01), (43, 0.015), (44, 0.032), (45, 0.028), (46, -0.063), (47, -0.011), (48, -0.068), (49, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98292226 <a title="96-lsi-1" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>2 0.80941558 <a title="96-lsi-2" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>Author: Aurel A. Lazar, Yevgeniy Slutskiy</p><p>Abstract: In system identiﬁcation both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing ﬁlter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identiﬁcation of the dendritic processing ﬁlter and reconstruct its kernel with arbitrary precision. 1</p><p>3 0.77738881 <a title="96-lsi-3" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>4 0.73484564 <a title="96-lsi-4" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>5 0.73224777 <a title="96-lsi-5" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>6 0.68015033 <a title="96-lsi-6" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>7 0.61205149 <a title="96-lsi-7" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>8 0.54761803 <a title="96-lsi-8" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>9 0.51797116 <a title="96-lsi-9" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>10 0.50957197 <a title="96-lsi-10" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>11 0.49006051 <a title="96-lsi-11" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>12 0.474646 <a title="96-lsi-12" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>13 0.45802471 <a title="96-lsi-13" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>14 0.4198772 <a title="96-lsi-14" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>15 0.39064783 <a title="96-lsi-15" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>16 0.3893545 <a title="96-lsi-16" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>17 0.38111958 <a title="96-lsi-17" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>18 0.37678221 <a title="96-lsi-18" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>19 0.36262259 <a title="96-lsi-19" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>20 0.35667351 <a title="96-lsi-20" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.024), (17, 0.045), (27, 0.102), (30, 0.027), (35, 0.029), (45, 0.166), (50, 0.103), (52, 0.089), (60, 0.019), (77, 0.089), (90, 0.027), (98, 0.196)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86482674 <a title="96-lda-1" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>2 0.76190078 <a title="96-lda-2" href="./nips-2010-Efficient_Minimization_of_Decomposable_Submodular_Functions.html">69 nips-2010-Efficient Minimization of Decomposable Submodular Functions</a></p>
<p>Author: Peter Stobbe, Andreas Krause</p><p>Abstract: Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efÄ?Ĺš ciently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classiÄ?Ĺš cation-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude. 1</p><p>3 0.74072391 <a title="96-lda-3" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>4 0.73269141 <a title="96-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.72617131 <a title="96-lda-5" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>6 0.72089881 <a title="96-lda-6" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>7 0.71908015 <a title="96-lda-7" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>8 0.71690428 <a title="96-lda-8" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>9 0.71564084 <a title="96-lda-9" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>10 0.71207976 <a title="96-lda-10" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>11 0.70987988 <a title="96-lda-11" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>12 0.70784736 <a title="96-lda-12" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>13 0.70665962 <a title="96-lda-13" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>14 0.70616645 <a title="96-lda-14" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>15 0.70041424 <a title="96-lda-15" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>16 0.69917101 <a title="96-lda-16" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>17 0.69875854 <a title="96-lda-17" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>18 0.69640154 <a title="96-lda-18" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>19 0.69476175 <a title="96-lda-19" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>20 0.69168776 <a title="96-lda-20" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
