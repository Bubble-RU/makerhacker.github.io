<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 nips-2010-Online Markov Decision Processes under Bandit Feedback</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-196" href="#">nips2010-196</a> knowledge-graph by maker-knowledge-mining</p><h1>196 nips-2010-Online Markov Decision Processes under Bandit Feedback</h1>
<br/><p>Source: <a title="nips-2010-196-pdf" href="http://papers.nips.cc/paper/4048-online-markov-decision-processes-under-bandit-feedback.pdf">pdf</a></p><p>Author: Gergely Neu, Andras Antos, András György, Csaba Szepesvári</p><p>Abstract: We consider online learning in ﬁnite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and, assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of the new algorithm is O T 2/3 (ln T )1/3 , giving the ﬁrst rigorously proved regret bound for the problem. 1</p><p>Reference: <a title="nips-2010-196-reference" href="../nips2010_reference/nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rt', 0.396), ('policy', 0.355), ('reward', 0.355), ('qt', 0.331), ('vt', 0.26), ('xt', 0.253), ('regret', 0.243), ('ag', 0.216), ('bandit', 0.21), ('ln', 0.166), ('ut', 0.12), ('mdp', 0.104), ('hung', 0.095), ('qn', 0.083), ('neu', 0.08), ('puterm', 0.08), ('onlin', 0.078), ('mdps', 0.075), ('et', 0.071), ('transit', 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="196-tfidf-1" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>2 0.40586215 <a title="196-tfidf-2" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>3 0.36474195 <a title="196-tfidf-3" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>4 0.33902958 <a title="196-tfidf-4" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>5 0.32374457 <a title="196-tfidf-5" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>6 0.31391451 <a title="196-tfidf-6" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>7 0.30723968 <a title="196-tfidf-7" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>8 0.29651222 <a title="196-tfidf-8" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>9 0.29302526 <a title="196-tfidf-9" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>10 0.28154081 <a title="196-tfidf-10" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>11 0.27888405 <a title="196-tfidf-11" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>12 0.27566972 <a title="196-tfidf-12" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>13 0.26084623 <a title="196-tfidf-13" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>14 0.25785664 <a title="196-tfidf-14" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>15 0.24275398 <a title="196-tfidf-15" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>16 0.23810254 <a title="196-tfidf-16" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>17 0.23662378 <a title="196-tfidf-17" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>18 0.23179106 <a title="196-tfidf-18" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>19 0.22630581 <a title="196-tfidf-19" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>20 0.22580105 <a title="196-tfidf-20" href="./nips-2010-Non-Stochastic_Bandit_Slate_Problems.html">183 nips-2010-Non-Stochastic Bandit Slate Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.297), (1, 0.538), (2, -0.004), (3, -0.055), (4, -0.028), (5, -0.033), (6, 0.108), (7, -0.089), (8, 0.123), (9, 0.224), (10, 0.04), (11, 0.179), (12, -0.109), (13, 0.039), (14, 0.136), (15, -0.029), (16, -0.084), (17, -0.054), (18, 0.03), (19, 0.059), (20, -0.085), (21, -0.019), (22, -0.024), (23, -0.008), (24, -0.013), (25, -0.031), (26, -0.007), (27, -0.043), (28, -0.07), (29, -0.012), (30, 0.06), (31, -0.023), (32, -0.009), (33, -0.004), (34, 0.031), (35, 0.088), (36, -0.027), (37, -0.0), (38, 0.03), (39, 0.044), (40, -0.02), (41, -0.087), (42, -0.043), (43, 0.037), (44, -0.037), (45, -0.056), (46, 0.001), (47, 0.011), (48, -0.032), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96964288 <a title="196-lsi-1" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>2 0.82997507 <a title="196-lsi-2" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>3 0.78278804 <a title="196-lsi-3" href="./nips-2010-Non-Stochastic_Bandit_Slate_Problems.html">183 nips-2010-Non-Stochastic Bandit Slate Problems</a></p>
<p>4 0.76396847 <a title="196-lsi-4" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>5 0.73038822 <a title="196-lsi-5" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<p>6 0.67943573 <a title="196-lsi-6" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>7 0.6659112 <a title="196-lsi-7" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>8 0.65155143 <a title="196-lsi-8" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>9 0.64833444 <a title="196-lsi-9" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>10 0.64603901 <a title="196-lsi-10" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>11 0.6250211 <a title="196-lsi-11" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>12 0.61685765 <a title="196-lsi-12" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>13 0.61502409 <a title="196-lsi-13" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>14 0.60489535 <a title="196-lsi-14" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>15 0.59031808 <a title="196-lsi-15" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>16 0.56312907 <a title="196-lsi-16" href="./nips-2010-Basis_Construction_from_Power_Series_Expansions_of_Value_Functions.html">37 nips-2010-Basis Construction from Power Series Expansions of Value Functions</a></p>
<p>17 0.55963916 <a title="196-lsi-17" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>18 0.55199349 <a title="196-lsi-18" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>19 0.54270446 <a title="196-lsi-19" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>20 0.54231071 <a title="196-lsi-20" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.081), (9, 0.016), (30, 0.042), (32, 0.255), (34, 0.128), (45, 0.118), (68, 0.103), (72, 0.144)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92763138 <a title="196-lda-1" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>2 0.92172593 <a title="196-lda-2" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>3 0.90885901 <a title="196-lda-3" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>4 0.90593213 <a title="196-lda-4" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>5 0.90456367 <a title="196-lda-5" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>6 0.9031319 <a title="196-lda-6" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>7 0.89791554 <a title="196-lda-7" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>8 0.89782029 <a title="196-lda-8" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>9 0.89744228 <a title="196-lda-9" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>10 0.89701527 <a title="196-lda-10" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>11 0.89658058 <a title="196-lda-11" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>12 0.89543337 <a title="196-lda-12" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>13 0.89510846 <a title="196-lda-13" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>14 0.89453918 <a title="196-lda-14" href="./nips-2010-Online_Classification_with_Specificity_Constraints.html">192 nips-2010-Online Classification with Specificity Constraints</a></p>
<p>15 0.89436632 <a title="196-lda-15" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>16 0.8942638 <a title="196-lda-16" href="./nips-2010-Efficient_and_Robust_Feature_Selection_via_Joint_%E2%84%932%2C1-Norms_Minimization.html">73 nips-2010-Efficient and Robust Feature Selection via Joint ℓ2,1-Norms Minimization</a></p>
<p>17 0.89329541 <a title="196-lda-17" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>18 0.89189875 <a title="196-lda-18" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>19 0.88938046 <a title="196-lda-19" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>20 0.88795716 <a title="196-lda-20" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<br/><br/><br/></body>
</html>
