<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-244" href="#">nips2010-244</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</h1>
<br/><p>Source: <a title="nips-2010-244-pdf" href="http://papers.nips.cc/paper/3970-sodium-entry-efficiency-during-action-potentials-a-novel-single-parameter-family-of-hodgkin-huxley-models.pdf">pdf</a></p><p>Author: Anand Singh, Renaud Jolivet, Pierre Magistretti, Bruno Weber</p><p>Abstract: Sodium entry during an action potential determines the energy efﬁciency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefﬁcient in that regard with about 4 times more charges ﬂowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efﬁciency and that the dynamics of their voltage-gated channels is signiﬁcantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of HodgkinHuxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models. 1</p><p>Reference: <a title="nips-2010-244-reference" href="../nips2010_reference/nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sodium entry efﬁciency during action potentials: A novel single-parameter family of Hodgkin-Huxley models Renaud Jolivet∗ Institute of Pharmacology and Toxicology University of Z¨ rich, Z¨ rich, Switzerland u u renaud. [sent-1, score-0.489]
</p><p>2 ch  Abstract Sodium entry during an action potential determines the energy efﬁciency of a neuron. [sent-11, score-0.535]
</p><p>3 The classic Hodgkin-Huxley model of action potential generation is notoriously inefﬁcient in that regard with about 4 times more charges ﬂowing through the membrane than the theoretical minimum required to achieve the observed depolarization. [sent-12, score-0.677]
</p><p>4 Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efﬁciency and that the dynamics of their voltage-gated channels is signiﬁcantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. [sent-13, score-0.921]
</p><p>5 Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. [sent-14, score-0.129]
</p><p>6 Here, we introduce a novel family of HodgkinHuxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. [sent-15, score-1.553]
</p><p>7 1  Introduction  Action potentials play the central role in neuron-to-neuron communication. [sent-18, score-0.175]
</p><p>8 At the onset of an action potential, the change in the membrane potential leads to opening of voltage-gated sodium channels, leading to inﬂux of sodium ions. [sent-19, score-1.73]
</p><p>9 Once the membrane is sufﬁciently depolarized, the opening of voltage-gated potassium channels leads to an efﬂux of potassium ions and brings the membrane back to the resting potential. [sent-20, score-1.122]
</p><p>10 During and after this process, the ionic gradients are restored by the Na,K-ATPase electrogenic pump which extrudes 3 sodium ions in exchange for 2 potassium ions and requires 1 ATP molecule per cycle. [sent-21, score-1.034]
</p><p>11 1  There is thus a metabolic cost in terms of ATP molecules to be spent associated with every action potential. [sent-24, score-0.396]
</p><p>12 This metabolic cost can be roughly estimated to be 1/3 of the sodium entry into the neuron. [sent-25, score-0.713]
</p><p>13 A metabolically efﬁcient action potential would have sodium entry restricted to the rising phase of the action potential so that a minimal number of charges is transported to produce the observed voltage change. [sent-26, score-1.856]
</p><p>14 This can be encapsulated into a measure called Sodium Entry Ratio (SER) deﬁned as the integral of the sodium current during the action potential divided by the product of the membrane capacitance by the observed change in membrane voltage. [sent-27, score-1.397]
</p><p>15 A metabolically optimally efﬁcient neuron would have a SER of 1 or close to 1. [sent-28, score-0.151]
</p><p>16 The metabolic efﬁciency critically depends on the gating kinetics of the voltage-dependent channels and on their interaction during the action potential. [sent-29, score-0.668]
</p><p>17 Analyzing the squid giant axon action potential, Hodgkin and Huxley established that the SER is approximately 4, owing to the fact that the sodium channels remain open during the falling phase of the action potential [1]. [sent-31, score-1.628]
</p><p>18 This has led to the idea that action potentials are metabolically inefﬁcient and these numbers were used as key input in a number of studies aiming at establishing an energy budget for brain tissue (see e. [sent-32, score-0.601]
</p><p>19 However, two recent studies have demonstrated that mammalian neurons, having fundamentally similar action potentials as the squid giant axon, are signiﬁcantly more efﬁcient owing to lesser sodium entry during the falling phase of the action potential [5,6]. [sent-35, score-1.83]
</p><p>20 In the ﬁrst study, Alle and colleagues observed that action potentials in mossy ﬁber boutons of hippocampal granule neurons have about 30% extra sodium entry than the theoretical minimum [5] (SER 1. [sent-36, score-1.257]
</p><p>21 In the second study, Carter and Bean expanded this ﬁnding, showing that different central neurons have different SERs [6]. [sent-38, score-0.186]
</p><p>22 More speciﬁcally, they measured that cortical pyramidal neurons are the most efﬁcient with a SER 1. [sent-39, score-0.347]
</p><p>23 2 while pyramidal neurons from the CA1 hippocampus region have a SER 1. [sent-40, score-0.258]
</p><p>24 On the other hand, inhibitory neurons were found to have less efﬁcient action potentials with cerebellar Purkinje neurons having a SER 2 and cortical basket cell interneurons having a SER 2. [sent-42, score-0.911]
</p><p>25 Interestingly, this is postulated to originate in the type or distribution of voltage-gated potassium channels present in each of these cell types. [sent-43, score-0.419]
</p><p>26 Even the less efﬁcient neurons are twice more metabolically efﬁcient than the original Hodgkin-Huxley neuron. [sent-44, score-0.282]
</p><p>27 These recent ﬁndings call for a revision of the original Hodgkin-Huxley model which fails on several accounts to describe accurately central mammalian neurons. [sent-45, score-0.193]
</p><p>28 The aim of the present work is to formulate an in silico model for an accurate description of the sodium and potassium currents underlying the generation of action potentials in central mammalian neurons. [sent-46, score-1.564]
</p><p>29 To this end, we introduce a novel family of Hodgkin-Huxley models HHξ parameterized by a single parameter ξ. [sent-47, score-0.107]
</p><p>30 Varying ξ in a meaningful range allows reproducing the whole range of observations of Carter and Bean [6] providing a very economic modeling strategy that can be used to model a wide range of central neurons from cortical pyramidal neurons to Purkinje cells. [sent-48, score-0.591]
</p><p>31 The third section demonstrates the performances of the novel family of models and characterize its properties. [sent-50, score-0.107]
</p><p>32 1  Model and methods Hodgkin-Huxley model family  In order to develop a novel family of Hodgkin-Huxley models, we started from the original HodgkinHuxley formalism [1]. [sent-53, score-0.187]
</p><p>33 In this formalism, the evolution of the membrane voltage V is governed by C  dV =− dt  Ik + Iext k  2  (1)  with C the membrane capacitance and Iext an externally applied current. [sent-54, score-0.519]
</p><p>34 The terms αx and βx are non-trivial functions of the voltage V . [sent-58, score-0.145]
</p><p>35 The reversal potentials Ex were then adjusted to match known concentrations of the respective ions in and around mammalian cells. [sent-62, score-0.428]
</p><p>36 We then proceeded to explore the behavior of the model and observed that the speciﬁc dynamics of iNa and iK during an action potential is critically dependent on the exact deﬁnition of αn . [sent-63, score-0.522]
</p><p>37 More speciﬁcally, we observed that by varying p5 in a meaningful range, we could reproduce qualitatively the observations of Carter and Bean [6] regarding the dynamics of the sodium current iNa during individual action potentials. [sent-68, score-1.024]
</p><p>38 All the other parameters appearing in the αx and βx functions were then optimized using a standard optimization algorithm so that the model reproduces as closely as possible the values characterizing action potential dynamics as reported in [6]. [sent-72, score-0.499]
</p><p>39 The ﬁnal values for parameters of the novel family of Hodgkin-Huxley models are reported in Table 1. [sent-73, score-0.107]
</p><p>40 Table 1: The novel family of Hodgkin-Huxley models HHξ αx  βx  channel  variable  Na  m  41. [sent-77, score-0.193]
</p><p>41 488  n  gx (mS/cm2 )  K  The voltage V is expressed in mV. [sent-97, score-0.145]
</p><p>42 67  0  voltage [mV]  voltage [mV]  −80  0  −80 30  35  currents [μA/cm2]  40 500  35 time [ms]  40  40 500  0  −500 30  0  30  35  currents [μA/cm2]  voltage [mV]  −80 35  width = 0. [sent-100, score-0.719]
</p><p>43 6) and “width” indicates the width of the action potential measured at the position indicated by the cyan arrow (see “Sodium entry ratio and numerics” subsection). [sent-108, score-0.63]
</p><p>44 2  Sodium entry ratio and numerics  The relevant parameters to compare the novel family of Hodgkin-Huxley models HHξ to the experimental dataset under consideration are: (i) the action potential peak, (ii) the action potential width and (iii) the sodium entry ratio (SER). [sent-116, score-1.799]
</p><p>45 The action potential peak is simply deﬁned as the maximal depolarization reached during the action potential. [sent-117, score-0.852]
</p><p>46 Following [6], the action potential width is measured at half the action potential height, measured as the difference in membrane potential from the peak to the resting potential. [sent-118, score-1.447]
</p><p>47 Finally, still following [6], the SER is deﬁned for an isolated action potential by SER =  iNa/C∆V  (6)  with ∆V the change in voltage during the action potential measured from the action potential threshold ϑ to its peak. [sent-119, score-1.519]
</p><p>48 The action potential threshold ϑ was deﬁned as 1% of the maximal dV /dt. [sent-120, score-0.437]
</p><p>49 3  Results  Recent experimental results suggest that the dynamics of the action potential generating voltagegated channels in the classical Hodgkin-Huxley model do not correctly reproduce what is observed in mammalian neurons [5,6]. [sent-124, score-1.03]
</p><p>50 More speciﬁcally, the Hodgkin-Huxley equations generate a sodium current with a characteristic secondary peak during the action potential decaying phase, leading to a very important inﬂux of sodium ions that counter the effect of potassium ions making the model metabolically inefﬁcient [1]. [sent-125, score-2.319]
</p><p>51 Mammalian neurons display a sodium current with a unique sharp peak or at most a low amplitude secondary peak [5,6]. [sent-126, score-1.045]
</p><p>52 5  Figure 2: Predictions of our model family are compared to the experimentally observed correlation between the action potential width and the SER. [sent-132, score-0.632]
</p><p>53 Data were collected for (from left to right): Purkinje cells, cortical interneurons, CA1 pyramidal neurons and cortical pyramidal neurons. [sent-134, score-0.485]
</p><p>54 In the precedent section, we have introduced a novel family of models HHξ parameterized by the unique parameter ξ (see Table 1). [sent-140, score-0.107]
</p><p>55 We will now show how varying ξ allows reproducing the wide range of dynamics observed experimentally. [sent-141, score-0.113]
</p><p>56 Figure 1 shows the behavior of HHξ during an isolated action potential for three different values of ξ. [sent-142, score-0.474]
</p><p>57 In all three cases, the action potential is triggered by the same unique square pulse of current generating an isolated action potential with roughly the same latency about 4 s after the end of the stimulating pulse. [sent-143, score-0.949]
</p><p>58 For low values of ξ, the sodium current iNa exhibits a single very sharp peak, being almost null after the action potential has peaked. [sent-145, score-1.058]
</p><p>59 At high values of ξ, iNa exhibits a distinctive secondary peak after the action potential has peaked. [sent-146, score-0.596]
</p><p>60 The potassium current iK is also much bigger in the latter case. [sent-147, score-0.288]
</p><p>61 We also observe a negative correlation between ξ and the width of the action potential. [sent-150, score-0.399]
</p><p>62 The width of action potentials decreases when ξ increases. [sent-151, score-0.545]
</p><p>63 Finally action potentials generated at low ξ values return to the resting potential from above while action potentials generated at high ξ values exhibit an after-hyperpolarization. [sent-152, score-1.064]
</p><p>64 These different instances of our family of models HHξ cover all the experimentally observed behaviors as reported in [6] (compare with Figures 1-3 therein). [sent-153, score-0.135]
</p><p>65 Indeed, Carter and Bean observed neurons with low SER, broad action potentials and a single sharp peak in the sodium current dynamics (cortical and CA1 pyramidal neurons). [sent-154, score-1.525]
</p><p>66 They also observed neurons with high SER, narrow action potentials and a distinctive secondary peak in the sodium current dynamics during the action potential decaying phase (cortical interneurons and cerebellar Purkinje cells). [sent-155, score-2.055]
</p><p>67 It clearly demonstrates that by varying ξ, our model family is able to capture the whole range of observed behaviors and quantitatively ﬁts the measured SER and action potential widths. [sent-157, score-0.626]
</p><p>68 We also observe a faint positive correlation between the action potential width and its peak like in [6] (not shown). [sent-158, score-0.637]
</p><p>69 While the dynamics of gating variables is traditionally formulated in terms of αx and βx functions (see Eq. [sent-159, score-0.145]
</p><p>70 each gating variable an equilibrium value x∞ (V ) and a time constant τx (V ). [sent-162, score-0.121]
</p><p>71 Figure 3 shows x∞ and τx for all three gating variables of the model as a function of the membrane voltage V , the variable opening the sodium channel m, the variable closing the sodium channel h and the variable associated with the potassium channel n. [sent-163, score-2.029]
</p><p>72 With increment in the value of ξ, the asymptotic value n∞ shifts towards lower membrane potentials, in other words for the same membrane voltage, the equilibrium value is higher. [sent-164, score-0.364]
</p><p>73 In summary, at low ξ values, the potassium current iK is only activated when the membrane potential is high and it kicks in slowly. [sent-166, score-0.612]
</p><p>74 At high ξ values, iK is activated earlier in the action potential and kicks in faster. [sent-167, score-0.468]
</p><p>75 This supports remarkably well the arguments of Carter and Bean to explain the relative metabolic inefﬁciency of GABAergic neurons. [sent-168, score-0.089]
</p><p>76 Indeed, fast-spiking neurons with narrow action potentials use fast-activating Kv3 channels to repolarize the membrane. [sent-169, score-0.79]
</p><p>77 It is also interesting to note that Kv3 channels enable fast spiking [8]. [sent-171, score-0.142]
</p><p>78 This is supposedly due to incomplete sodium channel inactivation and to earlier recovery, in effect speeding recovery and reducing the refractory period. [sent-172, score-0.69]
</p><p>79 Finally, Figure 4 shows the membrane voltage V when the model is subjected to a constant input as well as the corresponding gain functions or frequency versus current curves. [sent-173, score-0.346]
</p><p>80 The f − I curve has the typical saturating proﬁle observed for many neurons [9] and all the models start spiking at a non-zero frequency. [sent-174, score-0.18]
</p><p>81 In line with the idea that neurons with a sharp action potential and incomplete inactivation of sodium channels can spike faster, the discharge frequency increases with the value of ξ for a given input current. [sent-175, score-1.374]
</p><p>82 4  Discussion  Recent experimental results have highlighted that the original Hodgkin-Huxley model [1] is not particularly well suited to describe the dynamics of sodium and potassium voltage-gated channels during the course of an action potential in mammalian neurons. [sent-176, score-1.604]
</p><p>83 The Hodgkin-Huxley model is also a poor foundation for studies dedicated to computing an energy budget for the mammalian brain since it severely overestimates the metabolic cost associated with action potentials by at least a factor of 2. [sent-177, score-0.729]
</p><p>84 Despite that, the Hodgkin-Huxley model is still widely used and often for modeling projects speciﬁcally targeting the mammalian brain. [sent-178, score-0.164]
</p><p>85 2  xsi = 12 xsi = 13 xsi = 14 xsi = 15 xsi = 16  0. [sent-180, score-0.39]
</p><p>86 The models were stimulated with a constant current input of 5 sec after an initial 30 ms pulse. [sent-191, score-0.117]
</p><p>87 The proposed family of models uses the original equations of Hodgkin and Huxley as they were formulated originally but introduces new expressions for the functions αx and βx that characterize the dynamics of the gating variables m, n and h. [sent-194, score-0.253]
</p><p>88 By varying ξ in a speciﬁc range, our family of models is able to quantitatively reproduce a wide range of dynamics for the voltage-gated sodium and potassium channels during individual action potentials. [sent-196, score-1.463]
</p><p>89 These different behaviors cover neuron types as different as cortical pyramidal neurons, cortical interneurons or Purkinje cells. [sent-198, score-0.335]
</p><p>90 in sodium channel inactivation, may help to explain the differences between different cell types. [sent-202, score-0.635]
</p><p>91 It should also be noted that action potentials as narrow as 250 µs can be as energy-efﬁcient (SER = 1. [sent-203, score-0.491]
</p><p>92 3) [10] as the widest action potentials measured by Carter and Bean [6], suggesting that sodium channel kinetics, in addition to potassium channel kinetics, is also different for different cell types and subcellular compartments. [sent-204, score-1.45]
</p><p>93 Numerous studies have been dedicated to study the energy constraints of the brain from the coding and network design perspective [4,11] or from the channel kinetic perspective [3,5,6,12]. [sent-205, score-0.109]
</p><p>94 Recently it has been argued that energy minimization under functional constraints could be the unifying principle governing the speciﬁc combination of ion channels that each individual neuron expresses [12]. [sent-206, score-0.191]
</p><p>95 In support of this hypothesis, it was demonstrated that some mammalian neurons generate their action potentials with currents that almost reach optimal metabolic efﬁciency [5]. [sent-207, score-0.959]
</p><p>96 So far, these studies have mostly addressed the question of metabolic efﬁciency considering isolated action potentials. [sent-208, score-0.433]
</p><p>97 Moreover, it can be difﬁcult to compare neurons with very different properties. [sent-209, score-0.157]
</p><p>98 Here, we have introduced a new family of biophysical models able to reproduce different action potentials relevant to this debate and their underlying currents [6]. [sent-210, score-0.698]
</p><p>99 It also suggests that it could be possible to design a generic Hodgkin-Huxley-type model family that could encompass a very broad range of different observed behaviors in a similar way than the Izhikevich model does 7  for integrate-and-ﬁre type model neurons [13]. [sent-212, score-0.32]
</p><p>100 Finally we believe that our model family will prove invaluable in studying metabolic questions and in particular in addressing the speciﬁc question: why are inhibitory neurons less metabolically efﬁcient than excitatory neurons? [sent-213, score-0.451]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sodium', 0.549), ('action', 0.307), ('ser', 0.26), ('potassium', 0.25), ('mammalian', 0.164), ('membrane', 0.163), ('neurons', 0.157), ('potentials', 0.146), ('voltage', 0.145), ('channels', 0.142), ('potential', 0.13), ('bean', 0.125), ('carter', 0.125), ('metabolically', 0.125), ('peak', 0.108), ('pyramidal', 0.101), ('currents', 0.096), ('ina', 0.094), ('ions', 0.094), ('purkinje', 0.094), ('width', 0.092), ('metabolic', 0.089), ('channel', 0.086), ('gating', 0.083), ('family', 0.08), ('ms', 0.079), ('iapp', 0.078), ('xsi', 0.078), ('entry', 0.075), ('hh', 0.071), ('ik', 0.069), ('hodgkin', 0.069), ('cortical', 0.063), ('dynamics', 0.062), ('huxley', 0.055), ('iext', 0.055), ('inactivation', 0.055), ('secondary', 0.051), ('interneurons', 0.05), ('mv', 0.049), ('alle', 0.047), ('ena', 0.047), ('gna', 0.047), ('ionic', 0.047), ('kinetics', 0.047), ('laughlin', 0.047), ('pharmacology', 0.047), ('squid', 0.047), ('toxicology', 0.047), ('reproduce', 0.045), ('axon', 0.041), ('giant', 0.041), ('ux', 0.041), ('switzerland', 0.041), ('inef', 0.04), ('equilibrium', 0.038), ('narrow', 0.038), ('current', 0.038), ('isolated', 0.037), ('lausanne', 0.035), ('gl', 0.035), ('rich', 0.034), ('sharp', 0.034), ('phase', 0.034), ('ciency', 0.032), ('opening', 0.032), ('neurophysiol', 0.032), ('behaviors', 0.032), ('el', 0.031), ('atp', 0.031), ('cerebellar', 0.031), ('charges', 0.031), ('hodgkinhuxley', 0.031), ('kicks', 0.031), ('owing', 0.03), ('central', 0.029), ('ek', 0.029), ('range', 0.028), ('originally', 0.028), ('resting', 0.028), ('dv', 0.028), ('novel', 0.027), ('jolivet', 0.027), ('izhikevich', 0.027), ('postulated', 0.027), ('numerics', 0.027), ('gk', 0.026), ('measured', 0.026), ('neuron', 0.026), ('cells', 0.024), ('decaying', 0.024), ('reversal', 0.024), ('capacitance', 0.024), ('biophysical', 0.024), ('externally', 0.024), ('sb', 0.024), ('sejnowski', 0.024), ('generation', 0.023), ('observed', 0.023), ('energy', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="244-tfidf-1" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>Author: Anand Singh, Renaud Jolivet, Pierre Magistretti, Bruno Weber</p><p>Abstract: Sodium entry during an action potential determines the energy efﬁciency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefﬁcient in that regard with about 4 times more charges ﬂowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efﬁciency and that the dynamics of their voltage-gated channels is signiﬁcantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of HodgkinHuxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models. 1</p><p>2 0.12982634 <a title="244-tfidf-2" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>3 0.10004307 <a title="244-tfidf-3" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>4 0.084671237 <a title="244-tfidf-4" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>5 0.079936668 <a title="244-tfidf-5" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>6 0.078565963 <a title="244-tfidf-6" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>7 0.07657823 <a title="244-tfidf-7" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>8 0.076130167 <a title="244-tfidf-8" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>9 0.061177641 <a title="244-tfidf-9" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>10 0.058347423 <a title="244-tfidf-10" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>11 0.056285392 <a title="244-tfidf-11" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>12 0.055725373 <a title="244-tfidf-12" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>13 0.05451531 <a title="244-tfidf-13" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>14 0.053133309 <a title="244-tfidf-14" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>15 0.05235476 <a title="244-tfidf-15" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>16 0.051274844 <a title="244-tfidf-16" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>17 0.05087496 <a title="244-tfidf-17" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>18 0.049062669 <a title="244-tfidf-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.043400962 <a title="244-tfidf-19" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>20 0.043050598 <a title="244-tfidf-20" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.093), (1, -0.018), (2, -0.123), (3, 0.105), (4, 0.037), (5, 0.11), (6, -0.063), (7, 0.01), (8, 0.063), (9, 0.042), (10, 0.017), (11, 0.051), (12, 0.007), (13, 0.025), (14, 0.035), (15, 0.019), (16, -0.013), (17, -0.008), (18, -0.008), (19, -0.04), (20, 0.004), (21, 0.015), (22, -0.049), (23, 0.021), (24, 0.067), (25, -0.018), (26, 0.018), (27, -0.063), (28, -0.019), (29, -0.025), (30, 0.043), (31, 0.035), (32, 0.071), (33, -0.042), (34, 0.137), (35, 0.136), (36, -0.021), (37, -0.066), (38, -0.018), (39, 0.029), (40, 0.016), (41, -0.141), (42, 0.012), (43, 0.028), (44, -0.096), (45, 0.018), (46, 0.059), (47, 0.014), (48, 0.025), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96381319 <a title="244-lsi-1" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>Author: Anand Singh, Renaud Jolivet, Pierre Magistretti, Bruno Weber</p><p>Abstract: Sodium entry during an action potential determines the energy efﬁciency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefﬁcient in that regard with about 4 times more charges ﬂowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efﬁciency and that the dynamics of their voltage-gated channels is signiﬁcantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of HodgkinHuxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models. 1</p><p>2 0.60636979 <a title="244-lsi-2" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>3 0.58848858 <a title="244-lsi-3" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>4 0.51042682 <a title="244-lsi-4" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>Author: Leonid Karlinsky, Michael Dinerstein, Shimon Ullman</p><p>Abstract: This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because instances of different actions may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are anchored to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance. 1</p><p>5 0.50783646 <a title="244-lsi-5" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>6 0.49725804 <a title="244-lsi-6" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>7 0.48897538 <a title="244-lsi-7" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>8 0.48547286 <a title="244-lsi-8" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>9 0.48098296 <a title="244-lsi-9" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>10 0.46443841 <a title="244-lsi-10" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>11 0.43592119 <a title="244-lsi-11" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>12 0.43553245 <a title="244-lsi-12" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>13 0.41744465 <a title="244-lsi-13" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>14 0.41035312 <a title="244-lsi-14" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>15 0.39453059 <a title="244-lsi-15" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>16 0.36430812 <a title="244-lsi-16" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>17 0.35250294 <a title="244-lsi-17" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>18 0.33932897 <a title="244-lsi-18" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>19 0.31781003 <a title="244-lsi-19" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>20 0.31036502 <a title="244-lsi-20" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.024), (27, 0.133), (30, 0.028), (45, 0.088), (50, 0.047), (52, 0.034), (60, 0.016), (63, 0.38), (77, 0.116), (90, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78225666 <a title="244-lda-1" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>Author: Anand Singh, Renaud Jolivet, Pierre Magistretti, Bruno Weber</p><p>Abstract: Sodium entry during an action potential determines the energy efﬁciency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefﬁcient in that regard with about 4 times more charges ﬂowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efﬁciency and that the dynamics of their voltage-gated channels is signiﬁcantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of HodgkinHuxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models. 1</p><p>2 0.48967317 <a title="244-lda-2" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>3 0.42717272 <a title="244-lda-3" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>Author: Kanaka Rajan, L Abbott, Haim Sompolinsky</p><p>Abstract: How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of ﬂuctuations in the input-generated response, by comparing the distribution of evoked and intrinsically generated activity across the different units of a neural network. We develop a complementary approach to principal component analysis in which separate high-variance directions are derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses. 1 1 Motivation Stimulus selectivity in neural networks was historically measured directly from input-driven responses [1], and only later were similar selectivity patterns observed in spontaneous activity across the cortical surface [2, 3]. We argue that it is possible to work in the reverse order, and show that analyzing the distribution of spontaneous activity across the different units in the network can inform us about the selectivity of evoked responses to stimulus features, even when no apparent sensory map exists. Sensory-evoked responses are typically divided into a signal component generated by the stimulus and a noise component corresponding to ongoing activity that is not directly related to the stimulus. Subsequent effort focuses on understanding how the signal depends on properties of the stimulus, while the remaining, irregular part of the response is treated as additive noise. The distinction between external stochastic processes and the noise generated deterministically as a function of intrinsic recurrence has been previously studied in chaotic neural networks [4]. It has also been suggested that internally generated noise is not additive and can be more sensitive to the frequency and amplitude of the input, compared to the signal component of the response [5 - 8]. In this paper, we demonstrate that the interaction between deterministic intrinsic noise and the spatial properties of the external stimulus is also complex and nonlinear. We study the impact of network connectivity on the spatial pattern of input-driven responses by comparing the structure of evoked and spontaneous activity, and show how the unique signature of these dynamics determines the selectivity of networks to spatial features of the stimuli driving them. 2 Model description In this section, we describe the network model and the methods we use to analyze its dynamics. Subsequent sections explore how the spatial patterns of spontaneous and evoked responses are related in terms of the distribution of the activity across the network. Finally, we show how the stimulus selectivity of the network can be inferred from its spontaneous activity patterns. 2.1 Network elements We build a ﬁring rate model of N interconnected units characterized by a statistical description of the underlying circuitry (as N → ∞, the system “self averages” making the description independent of a speciﬁc network architecture, see also [11, 12]). Each unit is characterized by an activation variable xi ∀ i = 1, 2, . . . N , and a nonlinear response function ri which relates to xi through ri = R0 + φ(xi ) where,   R0 tanh x for x ≤ 0 R0 φ(x) = (1) x  (Rmax − R0 ) tanh otherwise. Rmax −R0 Eq. 1 allows us to independently set the maximum ﬁring rate Rmax and the background rate R0 to biologically reasonable values, while retaining a maximum gradient at x = 0 to guarantee the smoothness of the transition to chaos [4]. We introduce a recurrent weight matrix with element Jij equivalent to the strength of the synapse from unit j → unit i. The individual weights are chosen independently and randomly from a Gaus2 sian distribution with mean and variance given by [Jij ]J = 0 and Jij J = g 2 /N , where square brackets are ensemble averages [9 - 11,13]. The control parameter g which scales as the variance of the synaptic weights, is particularly important in determining whether or not the network produces spontaneous activity with non-trivial dynamics (Speciﬁcally, g = 0 corresponds to a completely uncoupled network and a network with g = 1 generates non-trivial spontaneous activity [4, 9, 10]). The activation variable for each unit xi is therefore determined by the relation, N τr dxi = −xi + g Jij rj + Ii , dt j=1 with the time scale of the network set by the single-neuron time constant τr of 10 ms. 2 (2) The amplitude I of an oscillatory external input of frequency f , is always the same for each unit, but in some examples shown in this paper, we introduce a neuron-speciﬁc phase factor θi , chosen randomly from a uniform distribution between 0 and 2π, such that Ii = I cos(2πf t + θi ) ∀ i = 1, 2, . . . N. (3) In visually responsive neurons, this mimics a population of simple cells driven by a drifting grating of temporal frequency f , with the different phases arising from offsets in spatial receptive ﬁeld locations. The randomly assigned phases in our model ensure that the spatial pattern of input is not correlated with the pattern of recurrent connectivity. In our selectivity analysis however (Fig. 3), we replace the random phases with spatial input patterns that are aligned with network connectivity. 2.2 PCA redux Principal component analysis (PCA) has been applied proﬁtably to neuronal recordings (see for example [14]) but these analyses often plot activity trajectories corresponding to different network states using the ﬁxed principal component coordinates derived from combined activities under all stimulus conditions. Our analysis offers a complementary approach whereby separate principal components are derived for each stimulus condition, and the resulting principal angles reveal not only the difference between the shapes of trajectories corresponding to different network states, but also the orientation of the low-dimensional subspaces these trajectories occupy within the full N -dimensional space of neuronal activity. The instantaneous network state can be described by a point in an N -dimensional space with coordinates equal to the ﬁring rates of the N units. Over time, the network activity traverses a trajectory in this N -dimensional space and PCA can be used to delineate the subspace in which this trajectory lies. The analysis is done by diagonalizing the equal-time cross-correlation matrix of network ﬁring rates given by, Dij = (ri (t) − ri )(rj (t) − rj ) , (4) where</p><p>4 0.41906774 <a title="244-lda-4" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>Author: Haefner Ralf, Matthias Bethge</p><p>Abstract: Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and ﬁnd it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We ﬁnd that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions. 1</p><p>5 0.41396889 <a title="244-lda-5" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>Author: K. Wong, He Wang, Si Wu, Chi Fung</p><p>Abstract: Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We ﬁnd that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we ﬁnd that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p><p>6 0.41174859 <a title="244-lda-6" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>7 0.41097787 <a title="244-lda-7" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>8 0.40731576 <a title="244-lda-8" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>9 0.40717092 <a title="244-lda-9" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>10 0.40661007 <a title="244-lda-10" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>11 0.40468541 <a title="244-lda-11" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>12 0.40429002 <a title="244-lda-12" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>13 0.40253299 <a title="244-lda-13" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>14 0.40053344 <a title="244-lda-14" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>15 0.39916259 <a title="244-lda-15" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>16 0.39846137 <a title="244-lda-16" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>17 0.3948732 <a title="244-lda-17" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>18 0.39216241 <a title="244-lda-18" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>19 0.39027408 <a title="244-lda-19" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>20 0.38835403 <a title="244-lda-20" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
