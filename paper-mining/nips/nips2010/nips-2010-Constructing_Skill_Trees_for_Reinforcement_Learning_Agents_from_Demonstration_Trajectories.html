<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-50" href="#">nips2010-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</h1>
<br/><p>Source: <a title="nips-2010-50-pdf" href="http://papers.nips.cc/paper/3903-constructing-skill-trees-for-reinforcement-learning-agents-from-demonstration-trajectories.pdf">pdf</a></p><p>Author: George Konidaris, Scott Kuindersma, Roderic Grupen, Andre S. Barreto</p><p>Abstract: We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further reﬁned through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction. 1</p><p>Reference: <a title="nips-2010-50-reference" href="../nips2010_reference/nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. [sent-3, score-1.197]
</p><p>2 CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. [sent-4, score-1.307]
</p><p>3 The skill chains from each trajectory are then merged to form a skill tree. [sent-5, score-1.773]
</p><p>4 1  Introduction  Hierarchical reinforcement learning [1] offers an appealing family of approaches to scaling up standard reinforcement learning (RL) [2] methods by enabling the use of both low-level primitive actions and higher-level macro-actions (or skills). [sent-7, score-0.197]
</p><p>5 Recently, Konidaris and Barto [3] introduced a general method for skill discovery in continuous RL domains called skill chaining. [sent-9, score-1.653]
</p><p>6 Skill chaining adaptively segments complex policies into skills that can be executed sequentially and that are easier to represent and learn. [sent-10, score-0.519]
</p><p>7 It can be coupled with abstraction selection [4] to select skill-speciﬁc abstractions, which can aid in acquiring policies that are high-dimensional when represented monolithically, but can be broken into subpolicies that can be deﬁned over far fewer variables. [sent-11, score-0.19]
</p><p>8 Unfortunately, performing skill chaining iteratively is slow: it creates skills sequentially, and requires several episodes to learn a new skill policy followed by several further episodes to learn by trial and error where it can be executed successfully. [sent-12, score-2.169]
</p><p>9 We introduce CST, a new skill acquisition method that can build skill trees (with appropriate abstractions) from a set of sample solution trajectories obtained from demonstration, a planner, or a controller. [sent-15, score-1.811]
</p><p>10 CST uses an incremental MAP changepoint detection method [8] to segment each solution trajectory into skills and then merges the resulting skill chains into a skill tree. [sent-16, score-2.23]
</p><p>11 We show that CST can construct a skill tree from human demonstration trajectories in Pinball, a challenging dynamic continuous domain, and that the resulting skills can be reﬁned using RL. [sent-18, score-1.344]
</p><p>12 We further show that it can be used to segment demonstration trajectories from a mobile manipulator into chains of skills, where each skill is assigned an appropriate abstraction. [sent-19, score-1.324]
</p><p>13 Rather than restricting the agent to selecting actions that take a single time step to complete, it models higher-level decision making using options: actions that have their own policies and which may require multiple time steps to complete. [sent-22, score-0.189]
</p><p>14 Options can be added to an agent’s action repertoire alongside its primitive actions, and the agent chooses when to execute them in the same way it chooses when to execute primitive actions. [sent-24, score-0.18]
</p><p>15 Methods for creating new options must determine when to create an option, how to deﬁne its termination condition (skill discovery), how to deﬁne its initiation set, and how to learn its policy. [sent-25, score-0.198]
</p><p>16 Given an option reward function, policy learning can be viewed as just another RL problem. [sent-26, score-0.238]
</p><p>17 Creation and termination are typically performed by the identiﬁcation of option goal states, with an option created to reach a goal state and then terminate. [sent-27, score-0.295]
</p><p>18 Although there are many skill discovery methods for discrete domains, very few exist for continuous domains. [sent-29, score-0.832]
</p><p>19 To the best of our knowledge (see Section 6), skill chaining [3] is the only such method that does not make any assumptions about the domain structure. [sent-30, score-0.96]
</p><p>20 2  Skill Chaining and Abstraction Selection  Skill chaining mirrors an idea present in other control ﬁelds—for example, in robotics a similar idea is known as pre-image backchaining [10, 11], and in control for chaotic systems as adaptive targeting [12]. [sent-32, score-0.204]
</p><p>21 Given a continuous RL problem where the policy is either too difﬁcult to learn directly or too complex to represent monolithically, we construct a skill tree such that we can obtain a trajectory from every start state to a solution state by executing a sequence (or chain) of acquired skills. [sent-33, score-1.088]
</p><p>22 When the agent triggers some target event, To —which occurs when it moves from a state not contained in any event in T to one contained in To —it creates a new option, o, with the goal of reaching To . [sent-37, score-0.2]
</p><p>23 As the agent continues to interact with the environment it learns a policy for o, and adds it to its set of available actions. [sent-38, score-0.185]
</p><p>24 The agent uses these states as training examples and learns Io , the initiation set of o, using a classiﬁer. [sent-41, score-0.177]
</p><p>25 An agent applying this method along a single trajectory will slowly learn a chain of skills that grows backward from the task goal region towards the start region (as depicted in Figure 1). [sent-43, score-0.406]
</p><p>26 More generally, multiple trajectories, noise in control, stochasticity in the environment, or simple variance will result in skill trees rather than skill chains because more than one option will be created to reach some target events. [sent-44, score-1.848]
</p><p>27 (a)  (b)  (c)  (d)  Figure 1: An agent creates options using skill chaining. [sent-47, score-0.97]
</p><p>28 (a) First, the agent encounters a target event and creates an option to reach it. [sent-48, score-0.272]
</p><p>29 (b) Entering the initiation set of this ﬁrst option triggers the creation of a second option whose target is the initiation set of the ﬁrst option. [sent-49, score-0.434]
</p><p>30 (c) Finally, after many trajectories the agent has created a chain of options to reach the original target. [sent-50, score-0.369]
</p><p>31 (d) When multiple options are created to target an initiation set, the chain splits and the agent creates a skill tree. [sent-51, score-1.137]
</p><p>32 2  The major advantage of skill chaining is that it provides a mechanism for the agent to adaptively represent a complex policy using a collection of simpler policies. [sent-52, score-1.125]
</p><p>33 We can take this further and allow each individual option policy to use its own state abstraction. [sent-53, score-0.233]
</p><p>34 Given a library of possible abstractions, and a set of sample trajectories (as, for example, obtained when initially learning an option policy), abstraction selection ﬁnds the abstraction best able to represent the value function inferred from the sample trajectories. [sent-58, score-0.456]
</p><p>35 It can be combined with skill chaining to learn a skill tree where each skill has its own abstraction; in such cases, the initiation set of each skill will be restricted to states where its policy can be well-represented using its abstraction. [sent-59, score-3.539]
</p><p>36 3  Changepoint Detection  Skill chaining learns a segmented policy by creating a new option when either the most suitable abstraction changes, or the value function (and therefore policy) becomes too complex to represent with a single option. [sent-61, score-0.493]
</p><p>37 We would like to segment an entire trajectory at once; the question then becomes: how many options exist along it, and where do they begin and end? [sent-62, score-0.264]
</p><p>38 This can be modeled as a multiple changepoint detection problem [8]. [sent-63, score-0.17]
</p><p>39 Unlike the standard regression setting, in RL our data is sequentially but not necessarily spatially segmented, and we would like to perform changepoint detection online—processing transitions as they occur and then discarding them. [sent-71, score-0.199]
</p><p>40 Fearnhead and Liu [8] introduced online algorithms for both Bayesian and MAP changepoint detection; we use the simpler method that obtains the MAP changepoints and models via an online Viterbi algorithm. [sent-72, score-0.199]
</p><p>41 The probability of an observed data segment starting at time i + 1 and continuing through j using q is P (i, j, q)(1 − G(j − i − 1)), reﬂecting the ﬁt probability and the probability of a segment of at least j − i − 1 steps. [sent-79, score-0.214]
</p><p>42 3  Constructing Skill Trees from Sample Trajectories  We propose using changepoint detection to segment sample trajectories into skills, using return Rt (sum of discounted reward) as the target variable. [sent-91, score-0.456]
</p><p>43 This provides an intuitive mapping to RL since a value function is simply an estimator of return; segmentation based on return thus provides a natural way to segment the value function implied by a trajectory into simpler value functions, or to detect a change in model (and therefore abstraction). [sent-92, score-0.233]
</p><p>44 To do so, we must select an appropriate model of expected skill (segment) length, and an appropriate model for ﬁtting the data. [sent-93, score-0.841]
</p><p>45 We assume a geometric distribution for skill lengths with parameter p, so that g(l) = (1 − p)l−1 p and G(l) = 1 (1 − (1 − p)l ). [sent-94, score-0.791]
</p><p>46 This gives us a natural way to set p since p = k , where k is the expected skill length. [sent-95, score-0.791]
</p><p>47 They can thus be used to produce a value function ﬁt (equivalent to a least-squares Monte Carlo estimate) for the skill segment if so desired; again, without the need to store the trajectory. [sent-105, score-0.898]
</p><p>48 Using this model we can segment a single trajectory into a skill chain; given multiple skill chains from different trajectories, we would like to merge them into a skill tree. [sent-106, score-2.679]
</p><p>49 4  segments by assigning them to the same skill (rather than two distinct skills). [sent-108, score-0.83]
</p><p>50 Since we wish to build skills that can be sequentially executed, we can only consider merging two segments when they have the same target—which means that the segments immediately following each of them have been merged. [sent-109, score-0.35]
</p><p>51 Since we assume that all trajectories have the same ﬁnal goal, we merge two chains by starting at their ﬁnal skill segments. [sent-110, score-1.057]
</p><p>52 For each pair of segments, we determine whether or not they are a good statistical match, and if so merging them, repeating this process until we fail to merge a pair of skill segments, after which the remaining skill chains branch off on their own. [sent-111, score-1.723]
</p><p>53 Segmenting a sample trajectory should be performed using a lower-order function approximator than that used for policy learning, since we see merely a single trajectory sample rather than a dense sample over the state space. [sent-114, score-0.318]
</p><p>54 If we are to merge skills obtained over multiple trajectories into trees, we require the component skills to be aligned, meaning that the changepoints occur in roughly the same places. [sent-117, score-0.69]
</p><p>55 When this is not the case, they may vary since segmentation is then based on function approximation boundaries, and hence two trajectories segmented independently may be poorly aligned. [sent-119, score-0.231]
</p><p>56 Therefore, when segmenting two trajectories sequentially in anticipation of a merge, we may wish to include a bias on changepoint locations in the second trajectory. [sent-120, score-0.366]
</p><p>57 We can include this bias during changepoint detection by multiplying Equation 1 with the resulting PDF evaluated at the current state. [sent-122, score-0.187]
</p><p>58 2 Previous experiments have shown that skill chaining is able to ﬁnd a very good policy while ﬂat learning ﬁnds a poor solution [3]. [sent-124, score-1.044]
</p><p>59 In this section, we evaluate the performance beneﬁts obtained using a skill tree generated from a pair of human-provided solution trajectories. [sent-125, score-0.817]
</p><p>60 We use the Pinball domain instance shown in Figure 3 with 5 pairs (one trajectory in each pair for each start state) of human demonstration trajectories. [sent-131, score-0.24]
</p><p>61 edu/˜ gdk/pinball  5  CST used an expected skill length of 100, δ = 0. [sent-145, score-0.791]
</p><p>62 To obtain a fair comparison to skill chaining, we initialized the CST skill policies using 10 episodes of experience replay of the demonstrated trajectories, rather than using the sufﬁcient statistics to perform a least-squares value function ﬁt. [sent-150, score-1.712]
</p><p>63 Example segmentations and the resulting merged trajectories are shown in Figure 3. [sent-153, score-0.188]
</p><p>64 (a)  (b)  (c)  (d)  Figure 3: The Pinball instance used in our experiment (a), along with segmented skill chains from a pair of sample solution trajectories (b and c), and the assignments obtained when the two chains are merged (d). [sent-154, score-1.157]
</p><p>65 The learning curves obtained using the resulting skill trees to RL agents are shown in Figure 4. [sent-156, score-0.921]
</p><p>66 These results compare the learning curves of CST agents, agents that perform skill chaining from scratch, and agents that are given fully pre-learned skills (obtained over 250 episodes of skill chaining). [sent-157, score-2.186]
</p><p>67 They show that the CST policies are not good enough to use immediately, as the agents do worse than those given pre-learned skills for the ﬁrst few episodes. [sent-158, score-0.381]
</p><p>68 However, very shortly thereafter the CST agents are able to learn excellent policies—immediately performing much better than skill chaining agents, and shortly thereafter even exceeding the performance of agents with pre-learned skills. [sent-159, score-1.142]
</p><p>69 This is likely because the skill tree structure obtained from demonstration has fewer but better skills than that learned incrementally by skill chaining agents. [sent-160, score-2.142]
</p><p>70 In addition, segmenting demonstration trajectories into skills results in much faster learning than attempting to acquire the entire policy by demonstration at once. [sent-161, score-0.775]
</p><p>71 The learning curve for agents that ﬁrst perform experience replay on the overall task value function and then proceed using skill chaining (not shown) is virtually identical to that of agents performing skill chaining from scratch. [sent-162, score-2.115]
</p><p>72 6  5  Acquiring Skills from Human Demonstration on the uBot  In this section we show that CST is able to create skill chains and select appropriate abstractions for each skill from human demonstration on the uBot-5, a dynamically balancing mobile manipulator. [sent-163, score-1.913]
</p><p>73 Demonstration trajectories are obtained from an expert human operator, controlling the uBot as it enters a corridor, approaches a door, pushes the door open, turns right into a new corridor, and ﬁnally approaches and pushes on a panel (illustrated in Figure 5). [sent-164, score-0.224]
</p><p>74 Trajectories Required 2 1 1 2 1 3  Figure 6: A demonstration trajectory segmented into skills, each with an appropriate abstraction. [sent-177, score-0.283]
</p><p>75 We gathered 12 demonstration trajectories from the uBot, of which 3 had to be discarded because the perceptual features were too noisy. [sent-178, score-0.287]
</p><p>76 Of the remaining 9, all segmented sensibly and 8 were able to be merged into a single skill chain. [sent-179, score-0.867]
</p><p>77 This shows that CST is able to segment trajectories obtained from a robot platform, select an appropriate abstraction in each case, and then replay the resulting policies using a small number of sample trajectories. [sent-181, score-0.51]
</p><p>78 6  Related Work  Several methods exist for skill discovery in discrete reinforcement learning domains; the most recent relevant work is by Mehta et al. [sent-182, score-0.878]
</p><p>79 [14], which induces task hierarchies from demonstration trajectories 7  in discrete domains, but assumes a factored MDP with given dynamic Bayes network action models. [sent-183, score-0.31]
</p><p>80 By contrast, we know of very little work on skill acquisition in continuous domains where the skills or action hierarchy are not designed in advance. [sent-184, score-1.109]
</p><p>81 However, they are consequently more complex and computationally intensive than the much simpler changepoint detection method we use, and they have not been used in the context of skill acquisition. [sent-191, score-0.961]
</p><p>82 A great deal of work exists in robotics under the general heading of learning from demonstration [5], where control policies are learned using sample trajectories obtained from a human, robot demonstrator, or a planner. [sent-192, score-0.433]
</p><p>83 Most methods learn an entire single policy from data, although some perform segmentation—for example, Jenkins and Matari´ [20] segment demonstrated data into motion primc itives, and thereby build a motion primitive library. [sent-193, score-0.293]
</p><p>84 Other methods use demonstration to provide an initial policy that is then reﬁned using reinforcement learning (e. [sent-195, score-0.309]
</p><p>85 The ﬁrst is that the demonstrated skills form a tree, when in some cases they may form a more general graph (e. [sent-200, score-0.218]
</p><p>86 A straightforward modiﬁcation of the procedure to merge skill chains could accommodate such cases. [sent-203, score-0.907]
</p><p>87 We also assume that the domain reward function is known and that each option reward can be obtained from it by adding in a termination reward. [sent-204, score-0.218]
</p><p>88 However, this requires segmentation based on policy rather than value function, since rewards are not given at demonstration time. [sent-208, score-0.284]
</p><p>89 Because policies are usually multivariate, this would require a multivariate changepoint detection algorithm, such as that by Xuan and Murphy [18]. [sent-209, score-0.232]
</p><p>90 Finally, we assume that the best model for combining a pair of skills is the model selected for representing both individually. [sent-210, score-0.218]
</p><p>91 This may not always hold—two skills best represented individually by one model may be better represented together using another (perhaps more general) one. [sent-211, score-0.218]
</p><p>92 Each skill is allocated its own abstraction, and therefore can be learned and represented efﬁciently—potentially allowing us to learn higher dimensional, extended policies. [sent-214, score-0.791]
</p><p>93 During learning, an unsuccessful or partial episode can still improve skills whose goals where nevertheless reached. [sent-215, score-0.218]
</p><p>94 Conﬁdence-based learning methods [25] can be applied to each skill individually. [sent-216, score-0.791]
</p><p>95 Finally, skills learned using agent-centric features (such as in our uBot example) can be transferred to new problems [26], and thereby detached from a problem-speciﬁc setting to be more generally useful. [sent-217, score-0.218]
</p><p>96 Taken together, these advantages, in conjunction with the application of CST to bootstrap skill policy acquisition, may prove crucial to scaling up policy learning methods to high-dimensional, continuous domains. [sent-218, score-1.021]
</p><p>97 Skill discovery in continuous reinforcement learning domains using skill chaining. [sent-241, score-0.93]
</p><p>98 Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. [sent-287, score-0.172]
</p><p>99 Performance-derived behavior vocabularies: data-driven acquisition of c skills from motion. [sent-363, score-0.243]
</p><p>100 Conﬁdence-based policy learning from demonstration using Gaussian mixture models. [sent-394, score-0.241]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('skill', 0.791), ('skills', 0.218), ('trajectories', 0.15), ('chaining', 0.149), ('cst', 0.142), ('changepoint', 0.141), ('demonstration', 0.137), ('segment', 0.107), ('abstraction', 0.104), ('policy', 0.104), ('agents', 0.101), ('konidaris', 0.099), ('ubot', 0.099), ('option', 0.098), ('initiation', 0.096), ('pinball', 0.088), ('trajectory', 0.083), ('agent', 0.081), ('abstractions', 0.077), ('options', 0.074), ('chains', 0.07), ('reinforcement', 0.068), ('policies', 0.062), ('changepoints', 0.058), ('barto', 0.058), ('robotics', 0.055), ('rl', 0.05), ('merge', 0.046), ('pjm', 0.044), ('segmentation', 0.043), ('segments', 0.039), ('fearnhead', 0.039), ('segmented', 0.038), ('merged', 0.038), ('door', 0.038), ('primitive', 0.038), ('reward', 0.036), ('episodes', 0.035), ('monolithically', 0.033), ('replay', 0.033), ('fourier', 0.031), ('state', 0.031), ('incrementally', 0.03), ('domains', 0.03), ('segmenting', 0.029), ('trees', 0.029), ('detection', 0.029), ('io', 0.029), ('target', 0.029), ('robot', 0.029), ('sequentially', 0.029), ('ap', 0.028), ('termination', 0.028), ('sequencing', 0.027), ('tree', 0.026), ('acquisition', 0.025), ('autonomous', 0.025), ('merging', 0.025), ('appropriate', 0.025), ('corridor', 0.025), ('particle', 0.025), ('creates', 0.024), ('chain', 0.024), ('acquiring', 0.024), ('actions', 0.023), ('action', 0.023), ('continuous', 0.022), ('executed', 0.022), ('grupen', 0.022), ('kuindersma', 0.022), ('manipulator', 0.022), ('matari', 0.022), ('mugan', 0.022), ('reach', 0.022), ('rt', 0.022), ('motion', 0.022), ('mobile', 0.022), ('viterbi', 0.022), ('domain', 0.02), ('discovery', 0.019), ('argall', 0.019), ('chernova', 0.019), ('mehta', 0.019), ('timestep', 0.019), ('xuan', 0.019), ('basis', 0.019), ('pt', 0.019), ('event', 0.018), ('created', 0.018), ('autonomously', 0.018), ('kinematic', 0.018), ('humanoid', 0.018), ('jenkins', 0.018), ('pushes', 0.018), ('drive', 0.017), ('bias', 0.017), ('triggers', 0.017), ('planner', 0.017), ('approximator', 0.017), ('obstacles', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="50-tfidf-1" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>Author: George Konidaris, Scott Kuindersma, Roderic Grupen, Andre S. Barreto</p><p>Abstract: We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further reﬁned through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction. 1</p><p>2 0.12437362 <a title="50-tfidf-2" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>Author: Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua B. Tenenbaum</p><p>Abstract: We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning. 1</p><p>3 0.10079151 <a title="50-tfidf-3" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>4 0.096183553 <a title="50-tfidf-4" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>Author: Gergely Neu, Andras Antos, András György, Csaba Szepesvári</p><p>Abstract: We consider online learning in ﬁnite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and, assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of the new algorithm is O T 2/3 (ln T )1/3 , giving the ﬁrst rigorously proved regret bound for the problem. 1</p><p>5 0.095515668 <a title="50-tfidf-5" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>6 0.093710855 <a title="50-tfidf-6" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>7 0.092834346 <a title="50-tfidf-7" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>8 0.092825994 <a title="50-tfidf-8" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>9 0.089361869 <a title="50-tfidf-9" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>10 0.085448384 <a title="50-tfidf-10" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>11 0.083579019 <a title="50-tfidf-11" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>12 0.073048517 <a title="50-tfidf-12" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>13 0.069866464 <a title="50-tfidf-13" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>14 0.065614842 <a title="50-tfidf-14" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>15 0.065493569 <a title="50-tfidf-15" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>16 0.064183138 <a title="50-tfidf-16" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>17 0.063138954 <a title="50-tfidf-17" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>18 0.055679243 <a title="50-tfidf-18" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>19 0.05251785 <a title="50-tfidf-19" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>20 0.050308675 <a title="50-tfidf-20" href="./nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">29 nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.166), (2, -0.051), (3, -0.011), (4, -0.025), (5, -0.014), (6, 0.007), (7, -0.004), (8, 0.02), (9, 0.022), (10, -0.006), (11, -0.008), (12, 0.01), (13, -0.004), (14, 0.022), (15, -0.001), (16, -0.023), (17, 0.024), (18, -0.019), (19, 0.008), (20, -0.018), (21, 0.01), (22, 0.025), (23, -0.008), (24, 0.048), (25, -0.013), (26, -0.02), (27, -0.015), (28, 0.027), (29, -0.035), (30, 0.047), (31, 0.067), (32, -0.038), (33, -0.074), (34, -0.065), (35, 0.03), (36, -0.028), (37, 0.01), (38, 0.032), (39, 0.002), (40, 0.003), (41, 0.064), (42, -0.009), (43, 0.014), (44, -0.068), (45, 0.057), (46, 0.055), (47, -0.016), (48, 0.034), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92126161 <a title="50-lsi-1" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>Author: George Konidaris, Scott Kuindersma, Roderic Grupen, Andre S. Barreto</p><p>Abstract: We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further reﬁned through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction. 1</p><p>2 0.75045574 <a title="50-lsi-2" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>Author: Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua B. Tenenbaum</p><p>Abstract: We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning. 1</p><p>3 0.6968869 <a title="50-lsi-3" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>Author: Abdeslam Boularias, Brahim Chaib-draa</p><p>Abstract: We consider the problem of apprenticeship learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efﬁcient tool for generalizing the demonstration, based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features. Most IRL algorithms use a simple Monte Carlo estimation to approximate the expected feature counts under the expert’s policy. In this paper, we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts. To reduce this error, we introduce a novel approach for bootstrapping the demonstration by assuming that: (i), the expert is (near-)optimal, and (ii), the dynamics of the system is known. Empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations. 1</p><p>4 0.65951288 <a title="50-lsi-4" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>5 0.64191121 <a title="50-lsi-5" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>Author: Martha White, Adam White</p><p>Abstract: The reinforcement learning community has explored many approaches to obtaining value estimates and models to guide decision making; these approaches, however, do not usually provide a measure of conﬁdence in the estimate. Accurate estimates of an agent’s conﬁdence are useful for many applications, such as biasing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing conﬁdence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agentenvironment interaction rarely satisﬁes traditional assumptions. Samples of valueestimates are dependent, likely non-normally distributed and often limited, particularly in early learning when conﬁdence estimates are pivotal. In this work, we investigate how to compute robust conﬁdences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute conﬁdence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applicability of our conﬁdence estimation algorithms with experiments on exploration, parameter estimation and tracking. 1</p><p>6 0.61074364 <a title="50-lsi-6" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>7 0.59364307 <a title="50-lsi-7" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>8 0.58462095 <a title="50-lsi-8" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>9 0.57347918 <a title="50-lsi-9" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>10 0.56318432 <a title="50-lsi-10" href="./nips-2010-Monte-Carlo_Planning_in_Large_POMDPs.html">168 nips-2010-Monte-Carlo Planning in Large POMDPs</a></p>
<p>11 0.55418169 <a title="50-lsi-11" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>12 0.5520156 <a title="50-lsi-12" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>13 0.54732019 <a title="50-lsi-13" href="./nips-2010-Mixture_of_time-warped_trajectory_models_for_movement_decoding.html">167 nips-2010-Mixture of time-warped trajectory models for movement decoding</a></p>
<p>14 0.52811104 <a title="50-lsi-14" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>15 0.52538836 <a title="50-lsi-15" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>16 0.52311724 <a title="50-lsi-16" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>17 0.51917839 <a title="50-lsi-17" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>18 0.50900561 <a title="50-lsi-18" href="./nips-2010-An_Approximate_Inference_Approach_to_Temporal_Optimization_in_Optimal_Control.html">29 nips-2010-An Approximate Inference Approach to Temporal Optimization in Optimal Control</a></p>
<p>19 0.47124627 <a title="50-lsi-19" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>20 0.46284011 <a title="50-lsi-20" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.029), (17, 0.023), (27, 0.062), (30, 0.049), (35, 0.019), (39, 0.014), (45, 0.18), (50, 0.049), (52, 0.022), (53, 0.016), (60, 0.046), (77, 0.047), (88, 0.012), (90, 0.036), (92, 0.306)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87930983 <a title="50-lda-1" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>2 0.73487085 <a title="50-lda-2" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>Author: Bela Frigyik, Maya Gupta, Yihua Chen</p><p>Abstract: Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data. 1 Modeling Probabilities for Machine Learning Modeling probability mass functions (pmfs) as random is useful in solving many real-world problems. A common random model for pmfs is the Dirichlet distribution [1]. The Dirichlet is conjugate to the multinomial and hence mathematically convenient for Bayesian inference, and the number of parameters is conveniently linear in the size of the sample space. However, the Dirichlet is a distribution over the entire probability simplex, and for many problems this is simply the wrong domain if there is application-speciﬁc prior knowledge that the pmfs come from a restricted subset of the simplex. For example, in natural language modeling, it is common to regularize a pmf over n-grams by some generic language model distribution q0 , that is, the pmf to be modeled is assumed to have the form θ = λq + (1 − λ)q0 for some q in the simplex, λ ∈ (0, 1) and a ﬁxed generic model q0 [2]. But once q0 and λ are ﬁxed, the pmf θ can only come from a subset of the simplex. Another natural language processing example is modeling the probability of keywords in a dictionary where some words are related, such as espresso and latte, and evidence for the one is to some extent evidence for the other. This relationship can be captured with a bounded variation model that would constrain the modeled probability of espresso to be within some of the modeled probability of latte. We show that such bounds on the variation between pmf components also restrict the domain of the pmf to a subset of the simplex. As a third example of restricting the domain, the similarity discriminant analysis classiﬁer estimates class-conditional pmfs that are constrained to be monotonically increasing over an ordered sample space of discrete similarity values [3]. In this paper we propose a simple variant of the Dirichlet whose support is a subset of the simplex, explore its properties, and show how to learn the model from data. We ﬁrst discuss the alternative solution of renormalizing the Dirichlet over the desired subset of the simplex, and other related work. Then we propose the shadow Dirichlet distribution; explain how to construct a shadow Dirichlet for three types of restricted domains: the regularized pmf case, bounded variation between pmf components, and monotonic pmfs; and discuss the most general case. We show how to use the expectation-maximization (EM) algorithm to estimate the shadow Dirichlet parameter α, and present simulation results for the estimation. 1 Dirichlet Shadow Dirichlet Renormalized Dirichlet Figure 1: Dirichlet, shadow Dirichlet, and renormalized Dirichlet for α = [3.94 2.25 2.81]. 2 Related Work One solution to modeling pmfs on only a subset of the simplex is to simply restrict the support of ˜ ˜ the Dirichlet to the desired support S, and renormalize the Dirichlet over S (see Fig. 1 for an example). This renormalized Dirichlet has the advantage that it is still a conjugate distribution for the multinomial. Nallapati et al.considered the renormalized Dirichlet for language modeling, but found it difﬁcult to use because the density requires numerical integration to compute the normalizer [4] . In addition, there is no closed form solution for the mean, covariance, or peak of the renormalized Dirichlet, making it difﬁcult to work with. Table 1 summarizes these properties. Additionally, generating samples from the renormalized Dirichlet is inefﬁcient: one draws samples from the stan˜ dard Dirichlet, then rejects realizations that are outside S. For high-dimensional sample spaces, this could greatly increase the time to generate samples. Although the Dirichlet is a classic and popular distribution on the simplex, Aitchison warns it “is totally inadequate for the description of the variability of compositional data,” because of its “implied independence structure and so the Dirichlet class is unlikely to be of any great use for describing compositions whose components have even weak forms of dependence” [5]. Aitchison instead championed a logistic normal distribution with more parameters to control covariance between components. A number of variants of the Dirichlet that can capture more dependence have been proposed and analyzed. For example, the scaled Dirichlet enables a more ﬂexible shape for the distribution [5], but does not change the support. The original Dirichlet(α1 , α2 , . . . αd ) can be derived as Yj / j Yj where Yj ∼ Γ(αj , β), whereas the scaled Dirichlet is derived from Yj ∼ Γ(αj , βj ), resulting in α density p(θ) = γ j ( α −1 βj j θ j j βi θi )α1 +···+αd i , where β, α ∈ Rd are parameters, and γ is the normalizer. + Another variant is the generalized Dirichlet [6] which also has parameters β, α ∈ Rd , and allows + greater control of the covariance structure, again without changing the support. As perhaps ﬁrst noted by Karl Pearson [7] and expounded upon by Aitchison [5], correlations of proportional data can be very misleading. Many Dirichlet variants have been generalizations of the Connor-Mossiman variant, Dirichlet process variants, other compound Dirichlet models, and hierarchical Dirichlet models. Ongaro et al. [8] propose the ﬂexible Dirichlet distribution by forming a re-parameterized mixture of Dirichlet distributions. Rayens and Srinivasan [9] considered the dependence structure for the general Dirichlet family called the generalized Liouville distributions. In contrast to prior efforts, the shadow Dirichlet manipulates the support to achieve various kinds of dependence that arise frequently in machine learning problems. 3 Shadow Dirichlet Distribution We introduce a new distribution that we call the shadow Dirichlet distribution. Let S be the prob˜ ability (d − 1)-simplex, and let Θ ∈ S be a random pmf drawn from a Dirichlet distribution with density pD and unnormalized parameter α ∈ Rd . Then we say the random pmf Θ ∈ S is distributed + ˜ according to a shadow Dirichlet distribution if Θ = M Θ for some ﬁxed d × d left-stochastic (that ˜ is, each column of M sums to 1) full-rank (and hence invertible) matrix M , and we call Θ the gen2 erating Dirichlet of Θ, or Θ’s Dirichlet shadow. Because M is a left-stochastic linear map between ﬁnite-dimensional spaces, it is a continuous map from the convex and compact S to a convex and compact subset of S that we denote SM . The shadow Dirichlet has two parameters: the generating Dirichlet’s parameter α ∈ Rd , and the + d × d matrix M . Both α and M can be estimated from data. However, as we show in the following subsections, the matrix M can be proﬁtably used as a design parameter that is chosen based on application-speciﬁc knowledge or side-information to specify the restricted domain SM , and in that way impose dependency between the components of the random pmfs. The shadow Dirichlet density p(θ) is the normalized pushforward of the Dirichlet density, that is, it is the composition of the Dirichlet density and M −1 with the Jacobian: 1 α −1 p(θ) = (M −1 θ)j j , (1) B(α) |det(M )| j Γ(αj ) d j is the standard Dirichlet normalizer, and α0 = j=1 αj is the standard where B(α) Γ(α0 ) Dirichlet precision factor. Table 1 summarizes the basic properties of the shadow Dirichlet. Fig. 1 shows an example shadow Dirichlet distribution. Generating samples from the shadow Dirichlet is trivial: generate samples from its generating Dirichlet (for example, using stick-breaking or urn-drawing) and multiply each sample by M to create the corresponding shadow Dirichlet sample. Table 1: Table compares and summarizes the Dirichlet, renormalized Dirichlet, and shadow Dirichlet distributions. Dirichlet(α) Density p(θ) Mean 1 B(α) d j=1 α −1 θj j Shadow Dirichlet (α, M ) 1 B(α)|det(M )| α α0 Renormalized ˜ Dirichlet (α, S) d −1 αj −1 θ)j j=1 (M 1 ˜ S M d j=1 α α0 αj −1 qj dq d j=1 α −1 θj j ˜ θp(θ)dθ S ¯ ¯ − θ)(θ − θ)T p(θ)dθ Cov(Θ) M Cov(Θ)M T αj −1 α0 −d j M α0 −d max p(θ) stick-breaking, urn-drawing draw from Dirichlet(α), multiply by M draw from Dirichlet(α), ˜ reject if not in S ML Estimate iterative (simple functions) iterative (simple functions) unknown complexity ML Compound Estimate iterative (simple functions) iterative (numerical integration) unknown complexity Covariance Mode (if α > 1) How to Sample 3.1 α −1 ˜ (θ S ˜ θ∈S Example: Regularized Pmfs The shadow Dirichlet can be designed to specify a distribution over a set of regularized pmfs SM = ˜ ˘ ˜ ˘ ˘ {θ θ = λθ + (1 − λ)θ, θ ∈ S}, for speciﬁc values of λ and θ. In general, for a given λ and θ ∈ S, the following d × d matrix M will change the support to the desired subset SM by mapping the extreme points of S to the extreme points of SM : ˘ M = (1 − λ)θ1T + λI, (2) where I is the d × d identity matrix. In Section 4 we show that the M given in (2) is optimal in a maximum entropy sense. 3 3.2 Example: Bounded Variation Pmfs We describe how to use the shadow Dirichlet to model a random pmf that has bounded variation such that |θk − θl | ≤ k,l for any k, ∈ {1, 2, . . . , d} and k,l > 0. To construct speciﬁed bounds on the variation, we ﬁrst analyze the variation for a given M . For any d × d left stochastic matrix T d d ˜ ˜ ˜ M, θ = Mθ = M1j θj . . . Mdj θj , so the difference between any two entries is j=1 j=1 ˜ |Mkj − Mlj | θj . ˜ (Mkj − Mlj )θj ≤ |θk − θl | = (3) j j Thus, to obtain a distribution over pmfs with bounded |θk − θ | ≤ k,l for any k, components, it is sufﬁcient to choose components of the matrix M such that |Mkj − Mlj | ≤ k,l for all j = 1, . . . , d ˜ because θ in (3) sums to 1. One way to create such an M is using the regularization strategy described in Section 3.1. For this ˜ ˜ ˘ case, the jth component of θ is θj = M θ = λθj + (1 − λ)θj , and thus the variation between the j ith and jth component of any pmf in SM is: ˜ ˘ ˜ ˘ ˜ ˜ ˘ ˘ |θi − θj | = λθi + (1 − λ)θi − λθj − (1 − λ)θj ≤ λ θi − θj + (1 − λ) θi − θj ˘ ˘ ≤ λ + (1 − λ) max θi − θj . (4) i,j ˘ Thus by choosing an appropriate λ and regularizing pmf θ, one can impose the bounded variation ˘ to be the uniform pmf, and choose any λ ∈ (0, 1), then the matrix given by (4). For example, set θ M given by (2) will guarantee that the difference between any two entries of any pmf drawn from the shadow Dirichlet (M, α) will be less than or equal to λ. 3.3 Example: Monotonic Pmfs For pmfs over ordered components, it may be desirable to restrict the support of the random pmf distribution to only monotonically increasing pmfs (or to only monotonically decreasing pmfs). A d × d left-stochastic matrix M that will result in a shadow Dirichlet that generates only monotonically increasing d × 1 pmfs has kth column [0 . . . 0 1/(d − k + 1) . . . 1/(d − k + 1)]T , we call this the monotonic M . It is easy to see that with this M only monotonic θ’s can be produced, 1˜ 1˜ 1 ˜ because θ1 = d θ1 which is less than or equal to θ2 = d θ1 + d−1 θ2 and so on. In Section 4 we show that the monotonic M is optimal in a maximum entropy sense. Note that to provide support over both monotonically increasing and decreasing pmfs with one distribution is not achievable with a shadow Dirichlet, but could be achieved by a mixture of two shadow Dirichlets. 3.4 What Restricted Subsets are Possible? Above we have described solutions to construct M for three kinds of dependence that arise in machine learning applications. Here we consider the more general question: What subsets of the simplex can be the support of the shadow Dirichlet, and how to design a shadow Dirichlet for a particular support? For any matrix M , by the Krein-Milman theorem [10], SM = M S is the convex hull of its extreme points. If M is injective, the extreme points of SM are easy to specify, as a d × d matrix M will have d extreme points that occur for the d choices of θ that have only one nonzero component, as the rest of the θ will create a non-trivial convex combination of the columns of M , and therefore cannot result in extreme points of SM by deﬁnition. That is, the extreme points of SM are the d columns of M , and one can design any SM with d extreme points by setting the columns of M to be those extreme pmfs. However, if one wants the new support to be a polytope in the probability (d − 1)-simplex with m > d extreme points, then one must use a fat M with d × m entries. Let S m denote the probability 4 (m − 1)-simplex, then the domain of the shadow Dirichlet will be M S m , which is the convex hull of the m columns of M and forms a convex polytope in S with at most m vertices. In this case M cannot be injective, and hence it is not bijective between S m and M S m . However, a density on M S m can be deﬁned as: p(θ) = 1 B(α) ˜ {θ ˜α −1 ˜ θj j dθ. ˜ M θ=θ} j (5) On the other hand, if one wants the support to be a low-dimensional polytope subset of a higherdimensional probability simplex, then a thin d × m matrix M , where m < d, can be used to implement this. If M is injective, then it has a left inverse M ∗ that is a matrix of dimension m × d, and the normalized pushforward of the original density can be used as a density on the image M S m : p(θ) = 1 α −1 1/2 B(α) |det(M T M )| (M ∗ θ)j j , j If M is not injective then one way to determine a density is to use (5). 4 Information-theoretic Properties In this section we note two information-theoretic properties of the shadow Dirichlet. Let Θ be drawn ˜ from shadow Dirichlet density pM , and let its generating Dirichlet Θ be drawn from pD . Then the differential entropy of the shadow Dirichlet is h(pM ) = log |det(M )| + h(pD ), where h(pD ) is the differential entropy of its generating Dirichlet. In fact, the shadow Dirichlet always has less entropy than its Dirichlet shadow because log |det(M )| ≤ 0, which can be shown as a corollary to the following lemma (proof not included due to lack of space): Lemma 4.1. Let {x1 , . . . , xn } and {y1 , . . . , yn } be column vectors in Rn . If each yj is a convex n n combination of the xi ’s, i.e. yj = i=1 γji xi , i=1 γji = 1, γjk ≥ 0, ∀j, k ∈ {1, . . . , n} then |det[y1 , . . . , yn ]| ≤ |det[x1 , . . . , xn ]|. It follows from Lemma 4.1 that the constructive solutions for M given in (2) and the monotonic M are optimal in the sense of maximizing entropy: Corollary 4.1. Let Mreg be the set of left-stochastic matrices M that parameterize shadow Dirichlet ˜ ˘ ˜ ˘ distributions with support in {θ θ = λθ + (1 − λ)θ, θ ∈ S}, for a speciﬁc choice of λ and θ. Then the M given in (2) results in the shadow Dirichlet with maximum entropy, that is, (2) solves arg maxM ∈Mreg h(pM ). Corollary 4.2. Let Mmono be the set of left-stochastic matrices M that parameterize shadow Dirichlet distributions that generate only monotonic pmfs. Then the monotonic M given in Section 3.3 results in the shadow Dirichlet with maximum entropy, that is, the monotonic M solves arg maxM ∈Mmono h(pM ). 5 Estimating the Distribution from Data In this section, we discuss the estimation of α for the shadow Dirichlet and compound shadow Dirichlet, and the estimation of M . 5.1 Estimating α for the Shadow Dirichlet Let matrix M be speciﬁed (for example, as described in the subsections of Section 3), and let q be a d × N matrix where the ith column qi is the ith sample pmf for i = 1 . . . N , and let (qi )j be the jth component of the ith sample pmf for j = 1, . . . , d. Then ﬁnding the maximum likelihood estimate 5 of α for the shadow Dirichlet is straightforward:  N 1 arg max log + log  p(qi |α) ≡ arg max log B(α) |det(M )| α∈Rk α∈Rk + + i=1   1 αj −1  (˜i )j q , ≡ arg max log  B(α)N i j α∈Rk +  N α −1 (M −1 qi )j j  i j (6) where q = M −1 q. Note (6) is the maximum likelihood estimation problem for the Dirichlet dis˜ tribution given the matrix q , and can be solved using the standard methods for that problem (see ˜ e.g. [11, 12]). 5.2 Estimating α for the Compound Shadow Dirichlet For many machine learning applications the given data are modeled as samples from realizations of a random pmf, and given these samples one must estimate the random pmf model’s parameters. We refer to this case as the compound shadow Dirichlet, analogous to the compound Dirichlet (also called the multivariate P´ lya distribution). Assuming one has already speciﬁed M , we ﬁrst discuss o method of moments estimation, and then describe an expectation-maximization (EM) method for computing the maximum likelihood estimate α. ˘ One can form an estimate of α by the method of moments. For the standard compound Dirichlet, one treats the samples of the realizations as normalized empirical histograms, sets the normalized α parameter equal to the empirical mean of the normalized histograms, and uses the empirical variances to determine the precision α0 . By deﬁnition, this estimate will be less likely than the maximum likelihood estimate, but may be a practical short-cut in some cases. For the compound shadow Dirichlet, we believe the method of moments estimator will be a poorer estimate in general. The problem is that if one draws samples from a pmf θ from a restricted subset SM of the simplex, ˘ then the normalized empirical histogram θ of those samples may not be in SM . For example given a monotonic pmf, the histogram of ﬁve samples drawn from it may not be monotonic. Then the empirical mean of such normalized empirical histograms may not be in SM , and so setting the shadow Dirichlet mean M α equal to the empirical mean may lead to an infeasible estimate (one that is outside SM ). A heuristic solution is to project the empirical mean into SM ﬁrst, for example, by ﬁnding the nearest pmf in SM in squared error or relative entropy. As with the compound Dirichlet, this may still be a useful approach in practice for some problems. Next we state an EM method to ﬁnd the maximum likelihood estimate α. Let s be a d × N matrix ˘ of sample histograms from different experiments, such that the ith column si is the ith histogram for i = 1, . . . , N , and (si )j is the number of times we have observed the jth event from the ith pmf vi . Then the maximum log-likelihood estimate of α solves arg max log p(s|α) for α ∈ Rk . + If the random pmfs are drawn from a Dirichlet distribution, then ﬁnding this maximum likelihood estimate requires an iterative procedure, and can be done in several ways including a gradient descent (ascent) approach. However, if the random pmfs are drawn from a shadow Dirichlet distribution, then a direct gradient descent approach is highly inconvenient as it requires taking derivatives of numerical integrals. However, it is practical to apply the expectation-maximization (EM) algorithm [13][14], as we describe in the rest of this section. Code to perform the EM estimation of α can be downloaded from idl.ee.washington.edu/publications.php. We assume that the experiments are independent and therefore p(s|α) = p({si }|α) = and hence arg maxα∈Rk log p(s|α) = arg maxα∈Rk i log p(si |α). + + i p(si |α) To apply the EM method, we consider the complete data to be the sample histograms s and the pmfs that generated them (s, v1 , v2 , . . . , vN ), whose expected log-likelihood will be maximized. Speciﬁcally, because of the assumed independence of the {vi }, the EM method requires one to repeatedly maximize the Q-function such that the estimate of α at the (m + 1)th iteration is: N α(m+1) = arg max α∈Rk + Evi |si ,α(m) [log p(vi |α)] . i=1 6 (7) Like the compound Dirichlet likelihood, the compound shadow Dirichlet likelihood is not necessarily concave. However, note that the Q-function given in (7) is concave, because log p(vi |α) = − log |det(M )| + log pD,α M −1 vi , where pD,α is the Dirichlet distribution with parameter α, and by a theorem of Ronning [11], log pD,α is a concave function, and adding a constant does not change the concavity. The Q-function is a ﬁnite integration of such concave functions and hence also concave [15]. We simplify (7) without destroying the concavity to yield the equivalent problem α(m+1) = d d arg max g(α) for α ∈ Rk , where g(α) = log Γ(α0 ) − j=1 log Γ(αj ) + j=1 βj αj , and + βj = N tij i=1 zi , 1 N where tij and zi are integrals we compute with Monte Carlo integration: d (s ) log(M −1 vi )j γi tij = SM (vi )k i k pM (vi |α(m) )dvi k=1 d zi = (vi )j k(si )k pM (vi |α(m) )dvi , γi SM k=1 where γi is the normalization constant for the multinomial with histogram si . We apply the Newton method [16] to maximize g(α), where the gradient g(α) has kth component ψ0 (α0 ) − ψ0 (α1 ) + β1 , where ψ0 denotes the digamma function. Let ψ1 denote the trigamma function, then the Hessian matrix of g(α) is: H = ψ1 (α0 )11T − diag (ψ1 (α1 ), . . . , ψ1 (αd )) . Note that because H has a very simple structure, the inversion of H required by the Newton step is greatly simpliﬁed by using the Woodbury identity [17]: H −1 = − diag(ξ1 , . . . , ξd ) − 1 1 1 [ξi ξj ]d×d , where ξ0 = ψ1 (α0 ) and ξj = ψ1 (αj ) , j = 1, . . . , d. ξ − d ξ 0 5.3 j=1 j Estimating M for the Shadow Dirichlet Thus far we have discussed how to construct M to achieve certain desired properties and how to interpret a given M ’s effect on the support. In some cases it may be useful to estimate M directly from data, for example, ﬁnding the maximum likelihood M . In general, this is a non-convex problem because the set of rank d − 1 matrices is not convex. However, we offer two approximations. First, note that as in estimating the support of a uniform distribution, the maximum likelihood M will correspond to a support that is no larger than needed to contain the convex hull of sample pmfs. Second, the mean of the empirical pmfs will be in the support, and thus a heuristic is to set the kth column of M (which corresponds to the kth vertex of the support) to be a convex combination of the kth vertex of the standard probability simplex and the empirical mean pmf. We provide code that ﬁnds the d optimal such convex combinations such that a speciﬁced percentage of the sample pmfs are within the support, which reduces the non-convex problem of ﬁnding the maximum likelihood d × d matrix M to a d-dimensional convex relaxation. 6 Demonstrations It is reasonable to believe that if the shadow Dirichlet better matches the problem’s statistics, it will perform better in practice, but an open question is how much better? To motivate the reader to investigate this question further in applications, we provide two small demonstrations. 6.1 Verifying the EM Estimation We used a broad suite of simulations to test and verify the EM estimation. Here we include a simple visual conﬁrmation that the EM estimation works: we drew 100 i.i.d. pmfs from a shadow Dirichlet with monotonic M for d = 3 and α = [3.94 2.25 2.81] (used in [18]). From each of the 100 pmfs, we drew 100 i.i.d. samples. Then we applied the EM algorithm to ﬁnd the α for both the standard compound Dirichlet, and the compound shadow Dirichlet with the correct M . Fig. 2 shows the true distribution and the two estimated distributions. 7 True Distribution (Shadow Dirichlet) Estimated Shadow Dirichlet Estimated Dirichlet Figure 2: Samples were drawn from the true distribution and the given EM method was applied to form the estimated distributions. 6.2 Estimating Proportions from Sales Manufacturers often have constrained manufacturing resources, such as equipment, inventory of raw materials, and employee time, with which to produce multiple products. The manufacturer must decide how to proportionally allocate such constrained resources across their product line based on their estimate of proportional sales. Manufacturer Artifact Puzzles gave us their past retail sales data for the 20 puzzles they sold during July 2009 through Dec 2009, which we used to predict the proportion of sales expected for each puzzle. These estimates were then tested on the next ﬁve months of sales data, for January 2010 through April 2010. The company also provided a similarity between puzzles S, where S(A, B) is the proportion of times an order during the six training months included both puzzle A and B if it included puzzle A. We compared treating each of the six training months of sales data as a sample from a compound Dirichlet versus or a compound shadow Dirichlet. For the shadow Dirichlet, we normalized each column of the similarity matrix S to sum to one so that it was left-stochastic, and used that as the M matrix; this forces puzzles that are often bought together to have closer estimated proportions. We estimated each α parameter by EM to maximize the likelihood of the past sales data, and then estimated the future sales proportions to be the mean of the estimated Dirichlet or shadow Dirichlet distribution. We also compared with treating all six months of sales data as coming from one multinomial which we estimated as the maximum likelihood multinomial, and to taking the mean of the six empirical pmfs. Table 2: Squared errors between estimates and actual proportional sales. Jan. Feb. Mar. Apr. 7 Multinomial .0129 .0185 .0231 .0240 Mean Pmf .0106 .0206 .0222 .0260 Dirichlet .0109 .0172 .0227 .0235 Shadow Dirichlet .0093 .0164 .0197 .0222 Summary In this paper we have proposed a variant of the Dirichlet distribution that naturally captures some of the dependent structure that arises often in machine learning applications. We have discussed some of its theoretical properties, and shown how to specify the distribution for regularized pmfs, bounded variation pmfs, monotonic pmfs, and for any desired convex polytopal domain. We have derived the EM method and made available code to estimate both the shadow Dirichlet and compound shadow Dirichlet from data. Experimental results demonstrate that the EM method can estimate the shadow Dirichlet effectively, and that the shadow Dirichlet may provide worthwhile advantages in practice. 8 References [1] B. Frigyik, A. Kapila, and M. R. Gupta, “Introduction to the Dirichlet distribution and related processes,” Tech. Rep., University of Washington, 2010. [2] C. Zhai and J. Lafferty, “A study of smoothing methods for language models applied to information retrieval,” ACM Trans. on Information Systems, vol. 22, no. 2, pp. 179–214, 2004. [3] Y. Chen, E. K. Garcia, M. R. Gupta, A. Rahimi, and L. Cazzanti, “Similarity-based classiﬁcation: Concepts and algorithms,” Journal of Machine Learning Research, vol. 10, pp. 747–776, March 2009. [4] R. Nallapati, T. Minka, and S. Robertson, “The smoothed-Dirichlet distribution: a building block for generative topic models,” Tech. Rep., Microsoft Research, Cambridge, 2007. [5] Aitchison, Statistical Analysis of Compositional Data, Chapman Hall, New York, 1986. [6] R. J. Connor and J. E. Mosiman, “Concepts of independence for proportions with a generalization of the Dirichlet distibution,” Journal of the American Statistical Association, vol. 64, pp. 194–206, 1969. [7] K. Pearson, “Mathematical contributions to the theory of evolution–on a form of spurious correlation which may arise when indices are used in the measurement of organs,” Proc. Royal Society of London, vol. 60, pp. 489–498, 1897. [8] A. Ongaro, S. Migliorati, and G. S. Monti, “A new distribution on the simplex containing the Dirichlet family,” Proc. 3rd Compositional Data Analysis Workshop, 2008. [9] W. S. Rayens and C. Srinivasan, “Dependence properties of generalized Liouville distributions on the simplex,” Journal of the American Statistical Association, vol. 89, no. 428, pp. 1465– 1470, 1994. [10] Walter Rudin, Functional Analysis, McGraw-Hill, New York, 1991. [11] G. Ronning, “Maximum likelihood estimation of Dirichlet distributions,” Journal of Statistical Computation and Simulation, vol. 34, no. 4, pp. 215221, 1989. [12] T. Minka, “Estimating a Dirichlet distribution,” Tech. Rep., Microsoft Research, Cambridge, 2009. [13] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from incomplete data via the EM algorithm,” Journal of the Royal Statistical Society: Series B (Methodological), vol. 39, no. 1, pp. 1–38, 1977. [14] M. R. Gupta and Y. Chen, Theory and Use of the EM Method, Foundations and Trends in Signal Processing, Hanover, MA, 2010. [15] R. T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ, 1970. [16] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, Cambridge, 2004. [17] K. B. Petersen and M. S. Pedersen, Matrix Cookbook, 2009, Available at matrixcookbook.com. [18] R. E. Madsen, D. Kauchak, and C. Elkan, “Modeling word burstiness using the Dirichlet distribution,” in Proc. Intl. Conf. Machine Learning, 2005. 9</p><p>3 0.73480517 <a title="50-lda-3" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>Author: Wei Wang, Zhi-hua Zhou</p><p>Abstract: The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be O(log 1 ), contrasting to single-view setting where the polynomial improveǫ ment is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is O( 1 ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise, ǫ contrasting to previous polynomial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. 1</p><p>same-paper 4 0.72666979 <a title="50-lda-4" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>Author: George Konidaris, Scott Kuindersma, Roderic Grupen, Andre S. Barreto</p><p>Abstract: We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further reﬁned through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction. 1</p><p>5 0.57083416 <a title="50-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.56934965 <a title="50-lda-6" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>7 0.56931984 <a title="50-lda-7" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>8 0.56828064 <a title="50-lda-8" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>9 0.56613457 <a title="50-lda-9" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>10 0.56545174 <a title="50-lda-10" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>11 0.56539851 <a title="50-lda-11" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>12 0.56487536 <a title="50-lda-12" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>13 0.56473863 <a title="50-lda-13" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>14 0.56459689 <a title="50-lda-14" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>15 0.56416428 <a title="50-lda-15" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>16 0.56415731 <a title="50-lda-16" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>17 0.56387216 <a title="50-lda-17" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>18 0.56352621 <a title="50-lda-18" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>19 0.56310004 <a title="50-lda-19" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>20 0.56296462 <a title="50-lda-20" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
