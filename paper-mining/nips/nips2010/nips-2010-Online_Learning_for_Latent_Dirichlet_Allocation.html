<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2010-Online Learning for Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-194" href="#">nips2010-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 nips-2010-Online Learning for Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2010-194-pdf" href="http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>Reference: <a title="nips-2010-194-reference" href="../nips2010_reference/nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). [sent-9, score-0.489]
</p><p>2 Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. [sent-10, score-0.413]
</p><p>3 It can handily analyze massive document collections, including those arriving in a stream. [sent-11, score-0.215]
</p><p>4 We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3. [sent-12, score-0.474]
</p><p>5 We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. [sent-14, score-0.718]
</p><p>6 For example, research in probabilistic topic modeling—the application we will focus on in this paper—revolves around ﬁtting complex hierarchical Bayesian models to large collections of documents. [sent-17, score-0.259]
</p><p>7 In a topic model, the posterior distribution reveals latent semantic structure that can be used for many applications. [sent-18, score-0.343]
</p><p>8 For topic models and many other Bayesian models of interest, however, the posterior is intractable to compute and researchers must appeal to approximate posterior inference. [sent-19, score-0.372]
</p><p>9 Optimization approaches are usually based on variational inference, which is called variational Bayes (VB) when used in a Bayesian hierarchical model. [sent-22, score-0.41]
</p><p>10 The per-iteration cost of batch algorithms can quickly become impractical for very large datasets. [sent-27, score-0.244]
</p><p>11 5  105  Documents seen (log scale) 8192  systems systems service road health systems made communication health service service companies announced billion market national language communication west care company language road billion  12288  16384  105. [sent-31, score-0.611]
</p><p>12 3 million unique Wikipedia articles is compared with online VB run on 98,000 Wikipedia articles and with the batch algorithm run on the same 98,000 articles. [sent-37, score-0.797]
</p><p>13 The online algorithms converge much faster than the batch algorithm does. [sent-38, score-0.528]
</p><p>14 Bottom: Evolution of a topic about business as online LDA sees more and more documents. [sent-39, score-0.539]
</p><p>15 to summarize the latent structure of massive document collections that cannot be annotated by hand. [sent-40, score-0.235]
</p><p>16 A central research problem for topic modeling is to efﬁciently ﬁt models to larger corpora [4, 5]. [sent-41, score-0.279]
</p><p>17 To this end, we develop an online variational Bayes algorithm for latent Dirichlet allocation (LDA), one of the simplest topic models and one on which many others are based. [sent-42, score-0.801]
</p><p>18 Our algorithm is based on online stochastic optimization, which has been shown to produce good parameter estimates dramatically faster than batch algorithms on large datasets [6]. [sent-43, score-0.575]
</p><p>19 Online LDA handily analyzes massive collections of documents and, moreover, online LDA need not locally store or collect the documents— each can arrive in a stream and be discarded after one look. [sent-44, score-0.576]
</p><p>20 In the subsequent sections, we derive online LDA and show that it converges to a stationary point of the variational objective function. [sent-45, score-0.576]
</p><p>21 We study the performance of online LDA in several ways, including by ﬁtting a topic model to 3. [sent-46, score-0.474]
</p><p>22 We show that online LDA ﬁnds topic models as good as or better than those found with batch VB, and in a fraction of the time (see ﬁgure 1). [sent-48, score-0.718]
</p><p>23 Online variational Bayes is a practical new method for estimating the posterior of complex hierarchical Bayesian models. [sent-49, score-0.284]
</p><p>24 2  Online variational Bayes for latent Dirichlet allocation  Latent Dirichlet Allocation (LDA) [7] is a Bayesian probabilistic model of text documents. [sent-50, score-0.303]
</p><p>25 ” Each topic deﬁnes a multinomial distribution over the vocabulary and is assumed to have been drawn from a Dirichlet, βk ∼ Dirichlet(η). [sent-52, score-0.281]
</p><p>26 Then, for each word i in the document, draw a topic index zdi ∈ {1, . [sent-55, score-0.377]
</p><p>27 , K} from the topic weights zdi ∼ θd and draw the observed word wdi from the selected topic, wdi ∼ βzdi . [sent-58, score-0.475]
</p><p>28 Note that if we sum over the topic assignments z, then we get p(wdi |θd , β) = k θdk βkw . [sent-60, score-0.264]
</p><p>29 This leads to the “multinomial PCA” interpretation of LDA; we can think of LDA as a probabilistic factorization of the matrix of word counts n (where ndw is the number of times word w appears in document d) into a matrix of topic weights θ and a dictionary of topics β [9]. [sent-61, score-0.667]
</p><p>30 Our work can thus 2  be seen as an extension of online matrix factorization techniques that optimize squared error [10] to more general probabilistic formulations. [sent-62, score-0.285]
</p><p>31 We can analyze a corpus of documents with LDA by examining the posterior distribution of the topics β, topic proportions θ, and topic assignments z conditioned on the documents. [sent-63, score-0.92]
</p><p>32 This posterior cannot be computed directly [7], and is usually approximated using Markov Chain Monte Carlo (MCMC) methods or variational inference. [sent-65, score-0.284]
</p><p>33 Developing scalable approximate inference methods for topic models is an active area of research [3, 4, 5, 11]. [sent-67, score-0.256]
</p><p>34 To this end, we develop online variational inference for LDA, an approximate posterior inference algorithm that can analyze massive collections of documents. [sent-68, score-0.78]
</p><p>35 We ﬁrst review the traditional variational Bayes algorithm for LDA and its objective function, then present our online method, and show that it converges to a stationary point of the same objective function. [sent-69, score-0.621]
</p><p>36 1  Batch variational Bayes for LDA  In Variational Bayesian inference (VB) the true posterior is approximated by a simpler distribution q(z, θ, β), which is indexed by a set of free parameters [12, 13]. [sent-71, score-0.326]
</p><p>37 This step will help us to derive an online inference algorithm. [sent-78, score-0.302]
</p><p>38 This reveals that the variational objective relies only on ndw , the number of times word w appears in document d. [sent-80, score-0.489]
</p><p>39 L can be optimized using coordinate ascent over the variational parameters φ, γ, λ [7]: φdwk ∝ exp{Eq [log θdk ] + Eq [log βkw ]}; γdk = α + w ndw φdwk ; λkw = η + d ndw φdwk . [sent-83, score-0.435]
</p><p>40 2  Online variational inference for LDA  Algorithm 1 has constant memory requirements and empirically converges faster than batch collapsed Gibbs sampling [3]. [sent-95, score-0.585]
</p><p>41 We propose an online variational inference algorithm for ﬁtting λ, the parameters to the variational posterior over the topic distributions β. [sent-98, score-1.029]
</p><p>42 Our algorithm is nearly as simple as the batch VB algorithm, but converges much faster for large datasets. [sent-99, score-0.312]
</p><p>43 A good setting of the topics λ is one for which the ELBO L is as high as possible after ﬁtting the per-document variational parameters γ and φ with the E step deﬁned in algorithm 1. [sent-100, score-0.302]
</p><p>44 Our goal is to set λ to maximize L(n, λ)  d  (nd , γ(nd , λ), φ(nd , λ), λ),  (7)  where (nd , γd , φd , λ) is the dth document’s contribution to the variational bound in equation 4. [sent-102, score-0.228]
</p><p>45 ˜ We then compute λ, the setting of λ that would be optimal (given φt ) if our entire corpus consisted of the single document nt repeated D times. [sent-106, score-0.292]
</p><p>46 (In the true online case D → ∞, corresponding to empirical ˜ Bayes estimation of β. [sent-110, score-0.26]
</p><p>47 3 that online LDA corresponds to a stochastic natural gradient algorithm on the variational objective L [15, 16]. [sent-117, score-0.598]
</p><p>48 This algorithm closely resembles one proposed in [16] for online VB on models with hidden data— the most important difference is that we use an approximate E step to optimize γt and φt , since we cannot compute the conditional distribution p(zt , θt |β, nt , α) exactly. [sent-118, score-0.394]
</p><p>49 In online LDA, this means computing λ using S > 1 observations: ˜ λkw = η +  D S  s  ntsk φtskw ,  (8)  where nts is the sth document in mini-batch t. [sent-121, score-0.343]
</p><p>50 The variational parameters φts and γts for this document are ﬁt with a normal E step. [sent-122, score-0.288]
</p><p>51 Note that we recover batch VB when S = D and κ = 0. [sent-123, score-0.244]
</p><p>52 In batch variational LDA, point estimates of the hyperparameters α and η can be ﬁt given γ and λ using a linear-time Newton-Raphson method [7]. [sent-125, score-0.449]
</p><p>53 end for  incorporate updates for α and η into online LDA: α ← α − ρt α(γt ); ˜  η ← η − ρt η (λ), ˜  (9)  where α(γt ) is the inverse of the Hessian times the gradient α (nt , γt , φt , λ), η (λ) is the inverse ˜ ˜ of the Hessian times the gradient η L, and ρt (τ0 + t)−κ as elsewhere. [sent-131, score-0.342]
</p><p>54 Since variational inference replaces sampling with optimization, we can use results from stochastic optimization to analyze online LDA. [sent-134, score-0.605]
</p><p>55 Thus, since t=0 ρt = ∞ and t=0 ρ2 < t ∞, the analysis in [19] shows both that λ converges and that the gradient λ d (nd , γd , φd , λ) converges to 0, and thus that λ converges to a stationary point. [sent-144, score-0.219]
</p><p>56 In variational inference, an alternative is to use the Fisher information matrix of the variational distribution q (i. [sent-148, score-0.41]
</p><p>57 , the Hessian of the log of the variational probability density function), which corresponds to using 1 Although we use a deterministic procedure to compute γ and φ given n and λ, this analysis can also be applied if γ and φ are optimized using a randomized algorithm. [sent-150, score-0.259]
</p><p>58 The problem is addressed by online coordinate ascent algorithms such as those described in [20, 21, 16, 17, 10]. [sent-160, score-0.26]
</p><p>59 Collapsed Gibbs Sampling (CGS) is a popular MCMC approach that samples from the posterior over topic assignments z by repeatedly sampling the topic assignment zdi conditioned on the data and all other topic assignments [22]. [sent-165, score-0.944]
</p><p>60 One online MCMC approach adapts CGS by sampling topic assignments zdi based on the topic assignments and data for all previously analyzed words, instead of all other words in the corpus [23]. [sent-166, score-1.05]
</p><p>61 Two alternative online MCMC approaches were considered in [24]. [sent-168, score-0.26]
</p><p>62 The ﬁrst, called incremental LDA, periodically resamples the topic assignments for previously analyzed words. [sent-169, score-0.284]
</p><p>63 In a study in [24], none of these three online MCMC algorithms performed as well as batch CGS. [sent-171, score-0.504]
</p><p>64 Instead of online methods, the authors of [4] used parallel computing to apply LDA to large corpora. [sent-172, score-0.282]
</p><p>65 They developed two approximate parallel CGS schemes for LDA that gave similar predictive performance on held-out documents to batch CGS. [sent-173, score-0.431]
</p><p>66 By contrast, batch VB can be implemented using constant memory, and parallelizes easily. [sent-176, score-0.244]
</p><p>67 As we will show in the next section, its online counterpart is even faster. [sent-177, score-0.26]
</p><p>68 4  Experiments  We ran several experiments to evaluate online LDA’s efﬁciency and effectiveness. [sent-178, score-0.283]
</p><p>69 The second set of experiments evaluates online VB in the setting where new documents are constantly being observed. [sent-180, score-0.454]
</p><p>70 2 2 Open-source Python implementations of batch and online LDA can be found at http://www. [sent-183, score-0.504]
</p><p>71 5 64 1030  1500  600  101  102  103  104  101  Time in seconds (log scale)  102  103  104  Time in seconds (log scale)  Figure 2: Held-out perplexity obtained on the Nature (left) and Wikipedia (right) corpora as a function of CPU time. [sent-204, score-0.278]
</p><p>72 For moderately large mini-batch sizes, online LDA ﬁnds solutions as good as those that the batch LDA ﬁnds, but with much less computation. [sent-205, score-0.504]
</p><p>73 When ﬁt to a 10,000-document subset of the training corpus batch LDA’s speed improves, but its performance suffers. [sent-206, score-0.343]
</p><p>74 Perplexity is deﬁned as the geometric mean of the inverse marginal probability of each word in the held-out set of documents: perplexity(ntest , λ, α)  exp −(  i  log p(ntest |α, β))/( i  i,w  ntest ) iw  (15)  where ni test denotes the vector of word counts for the ith document. [sent-208, score-0.338]
</p><p>75 Since we cannot directly compute log p(ntest |α, β), we use a lower bound on perplexity as a proxy: i perplexity(ntest , λ, α) ≤ exp −(  ntest ) . [sent-209, score-0.368]
</p><p>76 iw (16) The per-document parameters γi and φi for the variational distributions q(θi ) and q(zi ) are ﬁt using the E step in algorithm 2. [sent-210, score-0.255]
</p><p>77 The topics λ are ﬁt to a training set of documents and then held ﬁxed. [sent-211, score-0.238]
</p><p>78 i  Eq [log p(ntest , θi , zi |α, β)] − Eq [log q(θi , zi )])( i  i,w  There is some question as to the meaningfulness of perplexity as a metric for comparing different topic models [25]. [sent-214, score-0.427]
</p><p>79 Although online LDA converges to a stationary point for any valid κ, τ0 , and S, the quality of this stationary point and the speed of convergence may depend on how the learning parameters are set. [sent-219, score-0.396]
</p><p>80 We evaluated a range of settings of the learning parameters κ, τ0 , and S on two corpora: 352,549 documents from the journal Nature 3 and 100,000 documents downloaded from the English ver3  For the Nature articles, we removed all words not in a pruned vocabulary of 4,253 words. [sent-220, score-0.43]
</p><p>81 We then ran online LDA for ﬁve hours on the remaining documents from each corpus for κ ∈ {0. [sent-223, score-0.568]
</p><p>82 After ﬁve hours of CPU time, we computed perplexity on the test sets for the topics λ obtained at the end of each ﬁt. [sent-230, score-0.307]
</p><p>83 For mini-batch sizes from 256 to 16384 there is little difference in perplexity scores. [sent-236, score-0.236]
</p><p>84 To compare batch LDA to online LDA, we evaluated held-out perplexity as a function of time on the Nature and Wikipedia corpora above. [sent-241, score-0.782]
</p><p>85 We also evaluated batch LDA ﬁt to a 10,000document subset of the training corpus. [sent-243, score-0.244]
</p><p>86 On the larger Nature corpus, online LDA ﬁnds a solution as good as the batch algorithm’s with much less computation. [sent-247, score-0.504]
</p><p>87 On the smaller Wikipedia corpus, the online algorithm ﬁnds a better solution than the batch algorithm does. [sent-248, score-0.552]
</p><p>88 The batch algorithm converges quickly on the 10,000-document corpora, but makes less accurate predictions on held-out documents. [sent-249, score-0.312]
</p><p>89 To demonstrate the ability of online VB to perform in a true online setting, we wrote a Python script to continually download and analyze mini-batches of articles chosen at random from a list of approximately 3. [sent-251, score-0.733]
</p><p>90 The amount of time needed to download an article and convert it to a vector of word counts is comparable to the amount of time that the online LDA algorithm takes to analyze it. [sent-256, score-0.436]
</p><p>91 Figure 1 shows the evolution of the perplexity obtained on the held-out validation set of 1,000 Wikipedia articles by the online algorithm as a function of number of articles seen. [sent-259, score-0.737]
</p><p>92 Shown for comparison is the perplexity obtained by the online algorithm (with the same parameters) ﬁt to only 98,000 Wikipedia articles, and that obtained by the batch algorithm ﬁt to the same 98,000 articles. [sent-260, score-0.765]
</p><p>93 The online algorithm outperforms the batch algorithm regardless of which training dataset is used, but it does best with access to a constant stream of novel documents. [sent-261, score-0.552]
</p><p>94 The batch algorithm’s failure to outperform the online algorithm on limited data may be due to stochastic gradient’s robustness to local optima [19]. [sent-262, score-0.575]
</p><p>95 The online algorithm converged after analyzing about half of the 3. [sent-263, score-0.284]
</p><p>96 Even one iteration of the batch algorithm over that many articles would have taken days. [sent-265, score-0.388]
</p><p>97 5  Discussion  We have developed online variational Bayes (VB) for LDA. [sent-266, score-0.465]
</p><p>98 This algorithm requires only a few more lines of code than the traditional batch VB of [7], and is handily applied to massive and streaming document collections. [sent-267, score-0.48]
</p><p>99 The approach we used to derive an online version of batch VB for LDA is general (and simple) enough to apply to a wide variety of hierarchical Bayesian models. [sent-269, score-0.504]
</p><p>100 Efﬁcient methods for topic model inference on streaming document collections. [sent-344, score-0.362]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lda', 0.41), ('vb', 0.309), ('kw', 0.264), ('online', 0.26), ('batch', 0.244), ('eq', 0.214), ('topic', 0.214), ('perplexity', 0.213), ('variational', 0.205), ('wikipedia', 0.186), ('documents', 0.165), ('dwk', 0.131), ('articles', 0.12), ('ndw', 0.115), ('nt', 0.11), ('dk', 0.106), ('ntest', 0.101), ('corpus', 0.099), ('zdi', 0.098), ('dirichlet', 0.094), ('mcmc', 0.083), ('document', 0.083), ('service', 0.083), ('elbo', 0.082), ('posterior', 0.079), ('companies', 0.074), ('topics', 0.073), ('industry', 0.066), ('twk', 0.066), ('word', 0.065), ('business', 0.065), ('corpora', 0.065), ('company', 0.06), ('kv', 0.059), ('massive', 0.057), ('bayes', 0.055), ('log', 0.054), ('billion', 0.053), ('latent', 0.05), ('assignments', 0.05), ('cgs', 0.049), ('handily', 0.049), ('wdi', 0.049), ('allocation', 0.048), ('stochastic', 0.047), ('vocabulary', 0.046), ('stationary', 0.046), ('collections', 0.045), ('converges', 0.044), ('blei', 0.043), ('inference', 0.042), ('gradient', 0.041), ('tk', 0.038), ('services', 0.037), ('zd', 0.037), ('em', 0.034), ('settings', 0.034), ('python', 0.034), ('download', 0.034), ('ntv', 0.033), ('ntw', 0.033), ('script', 0.033), ('tvk', 0.033), ('nds', 0.032), ('princeton', 0.031), ('health', 0.03), ('million', 0.029), ('constantly', 0.029), ('bayesian', 0.028), ('hessian', 0.028), ('int', 0.028), ('multiplying', 0.028), ('counts', 0.027), ('iw', 0.026), ('cpu', 0.026), ('analyze', 0.026), ('factorization', 0.025), ('nature', 0.025), ('mimno', 0.025), ('forgotten', 0.025), ('memory', 0.025), ('sampling', 0.025), ('algorithm', 0.024), ('streaming', 0.023), ('equation', 0.023), ('sizes', 0.023), ('ran', 0.023), ('management', 0.022), ('parallel', 0.022), ('tting', 0.022), ('objective', 0.021), ('multinomial', 0.021), ('road', 0.021), ('asuncion', 0.021), ('hours', 0.021), ('chain', 0.021), ('analyzed', 0.02), ('smyth', 0.02), ('market', 0.02), ('words', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="194-tfidf-1" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>2 0.4072302 <a title="194-tfidf-2" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>3 0.28145298 <a title="194-tfidf-3" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>4 0.22214349 <a title="194-tfidf-4" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>5 0.19844104 <a title="194-tfidf-5" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>6 0.19785902 <a title="194-tfidf-6" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>7 0.18208697 <a title="194-tfidf-7" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>8 0.16068709 <a title="194-tfidf-8" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>9 0.1603854 <a title="194-tfidf-9" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>10 0.15415436 <a title="194-tfidf-10" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>11 0.10654242 <a title="194-tfidf-11" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>12 0.094358593 <a title="194-tfidf-12" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>13 0.09273874 <a title="194-tfidf-13" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>14 0.084301755 <a title="194-tfidf-14" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>15 0.083774671 <a title="194-tfidf-15" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>16 0.083245717 <a title="194-tfidf-16" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>17 0.082510673 <a title="194-tfidf-17" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>18 0.079608671 <a title="194-tfidf-18" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>19 0.078904547 <a title="194-tfidf-19" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>20 0.073825419 <a title="194-tfidf-20" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, 0.028), (2, 0.035), (3, -0.003), (4, -0.395), (5, 0.214), (6, 0.305), (7, 0.023), (8, -0.115), (9, 0.052), (10, 0.228), (11, 0.074), (12, 0.026), (13, 0.106), (14, -0.092), (15, 0.069), (16, -0.046), (17, 0.037), (18, -0.053), (19, 0.031), (20, 0.126), (21, -0.126), (22, -0.084), (23, 0.048), (24, -0.073), (25, -0.078), (26, 0.023), (27, 0.002), (28, -0.045), (29, -0.062), (30, 0.022), (31, -0.137), (32, 0.045), (33, -0.049), (34, 0.015), (35, 0.076), (36, -0.132), (37, 0.053), (38, 0.089), (39, -0.018), (40, 0.018), (41, -0.084), (42, 0.09), (43, 0.049), (44, 0.025), (45, -0.089), (46, -0.029), (47, 0.014), (48, -0.058), (49, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96663642 <a title="194-lsi-1" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>2 0.9447037 <a title="194-lsi-2" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>3 0.74898344 <a title="194-lsi-3" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>4 0.64370549 <a title="194-lsi-4" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, Ryota Tomioka</p><p>Abstract: Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only ﬁnd a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.</p><p>5 0.57173282 <a title="194-lsi-5" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>Author: Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David B. Dunson</p><p>Abstract: We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here deﬁned by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efﬁcient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. 1</p><p>6 0.48071083 <a title="194-lsi-6" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>7 0.47019455 <a title="194-lsi-7" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>8 0.45054236 <a title="194-lsi-8" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>9 0.41012669 <a title="194-lsi-9" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>10 0.35256037 <a title="194-lsi-10" href="./nips-2010-Shadow_Dirichlet_for_Restricted_Probability_Modeling.html">237 nips-2010-Shadow Dirichlet for Restricted Probability Modeling</a></p>
<p>11 0.3512328 <a title="194-lsi-11" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>12 0.34933004 <a title="194-lsi-12" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>13 0.34638497 <a title="194-lsi-13" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>14 0.33011958 <a title="194-lsi-14" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>15 0.30138937 <a title="194-lsi-15" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>16 0.2871249 <a title="194-lsi-16" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>17 0.2857534 <a title="194-lsi-17" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>18 0.28326398 <a title="194-lsi-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.28077546 <a title="194-lsi-19" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>20 0.26821563 <a title="194-lsi-20" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.061), (17, 0.016), (27, 0.221), (30, 0.072), (35, 0.013), (45, 0.156), (50, 0.069), (52, 0.028), (60, 0.037), (73, 0.189), (77, 0.021), (78, 0.017), (90, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86121213 <a title="194-lda-1" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>Author: Kaiming Li, Lei Guo, Carlos Faraco, Dajiang Zhu, Fan Deng, Tuo Zhang, Xi Jiang, Degang Zhang, Hanbo Chen, Xintao Hu, Steve Miller, Tianming Liu</p><p>Abstract: Functional segregation and integration are fundamental characteristics of the human brain. Studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest. A very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or ROIs (regions of interests) for individuals. Essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the ROIs. This paper presents a novel methodology to optimize the locations of an individual's ROIs in the working memory system. Our strategy is to formulate the individual ROI optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints. The optimization problem is solved via the simulated annealing approach. Our experimental results show that the optimized ROIs have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles. 1 Int ro ducti o n The human brainâ&euro;&trade;s function is segregated into distinct regions and integrated via axonal fibers [1]. Studying the connectivity among these regions and modeling their dynamics and interactions has drawn increasing interest and effort from the brain imaging and neuroscience communities [2-6]. For example, recently, the Human Connectome Project [7] and the 1000 Functional Connectomes Project [8] have embarked to elucidate large-scale connectivity patterns in the human brain. For traditional connectivity analysis, a variety of models including DCM (dynamics causal modeling), GCM (Granger causality modeling) and MVA (multivariate autoregressive modeling) are proposed [6, 9-10] to model the interactions of the ROIs. A fundamental issue in these studies is how to accurately identify the ROIs, which are the structural substrates for measuring connectivity. Currently, this is still an open, urgent, yet challenging problem in many brain imaging applications. From our perspective, the major challenges come from uncertainties in ROI boundary definition, the tremendous variability across individuals, and high nonlinearities within and around ROIs. Current approaches for identifying brain ROIs can be broadly classified into four categories. The first is manual labeling by experts using their domain knowledge. The second is a data-driven clustering of ROIs from the brain image itself. For instance, the ReHo (regional homogeneity) algorithm [11] has been used to identify regional homogeneous regions as ROIs. The third is to predefine ROIs in a template brain, and warp them back to the individual space using image registration [12]. Lastly, ROIs can be defined from the activated regions observed during a task-based fMRI paradigm. While fruitful results have been achieved using these approaches, there are various limitations. For instance, manual labeling is difficult to implement for large datasets and may be vulnerable to inter-subject and intra-subject variation; it is difficult to build correspondence across subjects using data-driven clustering methods; warping template ROIs back to individual space is subject to the accuracy of warping techniques and the anatomical variability across subjects. Even identifying ROIs using task-based fMRI paradigms, which is regarded as the standard approach for ROI identification, is still an open question. It was reported in [13] that many imaging-related variables including scanner vender, RF coil characteristics (phase array vs. volume coil), k-space acquisition trajectory, reconstruction algorithms, susceptibility -induced signal dropout, as well as field strength differences, contribute to variations in ROI identification. Other researchers reported that spatial smoothing, a common preprocessing technique in fMRI analysis to enhance SNR, may introduce artificial localization shift s (up to 12.1mm for Gaussian kernel volumetric smoothing) [14] or generate overly smoothed activation maps that may obscure important details [15]. For example, as shown in Fig.1a, the local maximum of the ROI was shifted by 4mm due to the spatial smoothing process. Additionally, its structural profile (Fig.1b) was significantly altered. Furthermore, group-based activation maps may show different patterns from an individual's activation map; Fig.1c depicts such differences. The top panel is the group activation map from a working memory study, while the bottom panel is the activation map of one subject in the study. As we can see from the highlighted boxes, the subject has less activated regions than the group analysis result. In conclusion, standard analysis of task-based fMRI paradigm data is inadequate to accurately localize ROIs for each individual. Fig.1. (a): Local activation map maxima (marked by the cross) shift of one ROI due to spatial volumetric smoothing. The top one was detected using unsmoothed data while the bottom one used smoothed data (FWHM: 6.875mm). (b): The corresponding fibers for the ROIs in (a). The ROIs are presented using a sphere (radius: 5mm). (c): Activation map differences between the group (top) and one subject (bottom). The highlighted boxes show two of the missing activated ROIs found from the group analysis. Without accurate and reliable individualized ROIs, the validity of brain connectivity analysis, and computational modeling of dynamics and interactions among brain networks , would be questionable. In response to this fundamental issue, this paper presents a novel computational methodology to optimize the locations of an individual's ROIs initialized from task-based fMRI. We use the ROIs identified in a block-based working memory paradigm as a test bed application to develop and evaluate our methodology. The optimization of ROI locations was formulated as an energy minimization problem, with the goal of jointly maximizing the group-wise consistency of functional and structural connectivity patterns and anatomic profiles. The optimization problem is solved via the well-established simulated annealing approach. Our experimental results show that the optimized ROIs achieved our optimization objectives and demonstrated promising results. 2 Mat eria l s a nd Metho ds 2.1 Data acquisition and preprocessing Twenty-five university students were recruited to participate in this study. Each participant performed an fMRI modified version of the OSPAN task (3 block types: OSPAN, Arithmetic, and Baseline) while fMRI data was acquired. DTI scans were also acquired for each participant. FMRI and DTI scans were acquired on a 3T GE Signa scanner. Acquisition parameters were as follows : fMRI: 64x64 matrix, 4mm slice thickness, 220mm FOV, 30 slices, TR=1.5s, TE=25ms, ASSET=2; DTI: 128x128 matrix, 2mm slice thickness, 256mm FOV, 60 slices, TR=15100ms, TE= variable, ASSET=2, 3 B0 images, 30 optimized gradient directions, b-value=1000). Each participantâ&euro;&trade;s fMRI data was analyzed using FSL. Individual activation map Fig.2. working memory reflecting the OSPAN (OSPAN > Baseline) contrast was used. In ROIs mapped on a total, we identified the 16 highest activated ROIs, including left WM/GM surface and right insula, left and right medial frontal gyrus, left and right precentral gyrus, left and right paracingulate gyrus, left and right dorsolateral prefrontal cortex, left and right inferior parietal lobule, left occipital pole, right frontal pole, right lateral occipital gyrus, and left and right precuneus. Fig.2 shows the 16 ROIs mapped onto a WM(white matter)/GM(gray matter) cortical surface. For some individuals, there may be missing ROIs on their activation maps. Under such condition, we adapted the group activation map as a guide to find these ROIs using linear registration. DTI pre-processing consisted of skull removal, motion correction, and eddy current correction. After the pre-processing, fiber tracking was performed using MEDINRIA (FA threshold: 0.2; minimum fiber length: 20). Fibers were extended along their tangent directions to reach into the gray matter when necessary. Brain tissue segmentation was conducted on DTI data by the method in [16] and the cortical surface was reconstructed from the tissue maps using the marching cubes algorithm. The cortical surface was parcellated into anatomical regions using the HAMMER tool [17]. DTI space was used as the standard space from which to generate the GM (gray matter) segmentation and from which to report the ROI locations on the cortical surface. Since the fMRI and DTI sequences are both EPI (echo planar imaging) sequences, their distortions tend to be similar and the misalignment between DTI and fMRI images is much less than that between T1 and fMRI images [18]. Co-registration between DTI and fMRI data was performed using FSL FLIRT [12]. The activated ROIs and tracked fibers were then mapped onto the cortical surface for joint modeling. 2.2 Joint modeling of anatomical, structural and functional profiles Despite the high degree of variability across subjects, there are several aspects of regularity on which we base the proposed solution. Firstly, across subjects, the functional ROIs should have similar anatomical locations, e.g., similar locations in the atlas space. Secondly, these ROIs should have similar structural connectivity profiles across subjects. In other words, fibers penetrating the same functional ROIs should have at least similar target regions across subjects. Lastly, individual networks identified by task-based paradigms, like the working memory network we adapted as a test bed in this paper, should have similar functional connectivity pattern across subjects. The neuroscience bases of the above premises include: 1) structural and functional brain connectivity are closely related [19], and cortical gyrification and axongenesis processes are closely coupled [20]; Hence, it is reasonable to put these three types of information in a joint modeling framework. 2) Extensive studies have already demonstrated the existence of a common structural and functional architecture of the human brain [21, 22], and it makes sense to assume that the working memory network has similar structural and functional connectivity patterns across individuals. Based on these premises, we proposed to optimize the locations of individual functional ROIs by jointly modeling anatomic profiles, structural connectivity patterns, and functional connectivity patterns, as illustrated in Fig 3. The Fig.3. ROIs optimization scheme. goal was to minimize the group-wise variance (or maximize group-wise consistency) of these jointly modeled profiles. Mathematically, we modeled the group-wise variance as energy E as follows. A ROI from fMRI analysis was mapped onto the surface, and is represented by a center vertex and its neighborhood. Suppose đ?&lsquo;&hellip; đ?&lsquo;&ndash;đ?&lsquo;&mdash; is the ROI region j on the cortical surface of subject i identified in Section 2.1; we find a corresponding surface ROI region đ?&lsquo;&dagger; đ?&lsquo;&ndash;đ?&lsquo;&mdash; so that the energy E (contains energy from n subjects, each with m ROIs) is minimized: đ??¸ = đ??¸ đ?&lsquo;Ž (đ?&oelig;&dagger; đ??¸ đ?&lsquo;? â&circ;&rsquo;đ?&lsquo;&euro; đ??¸ đ?&lsquo;? đ?&oelig;Ž đ??¸đ?&lsquo;? + (1 â&circ;&rsquo; đ?&oelig;&dagger;) đ??¸ đ?&lsquo;&ldquo; â&circ;&rsquo;đ?&lsquo;&euro; đ??¸ đ?&lsquo;&ldquo; đ?&oelig;Žđ??¸đ?&lsquo;&ldquo; ) (1) where Ea is the anatomical constraint; Ec is the structural connectivity constraint, M Ec and ď ł E are the mean and standard deviation of Ec in the searching space; E f is the functional c connectivity constraint, M E f and ď ł E f are the mean and standard deviation of E f respectively; and ď Ź is a weighting parameter between 0 and 1. If not specified, and m is the number of ROIs in this paper. The details of these energy terms are provided in the following sections. 2.2.1 n is the number of subjects, Anatomical constraint energy Anatomical constraint energy Ea is defined to ensure that the optimized ROIs have similar anatomical locations in the atlas space (Fig.4 shows an example of ROIs of 15 randomly selected subjects in the atlas space). We model the locations for all ROIs in the atlas space using a Gaussian model (mean: đ?&lsquo;&euro; đ?&lsquo;&lsaquo; đ?&lsquo;&mdash; ,and standard deviation: ď ł X j for ROI j ). The model parameters were estimated using the initial locations obtained from Section 2.1. Let X ij be the center coordinate of region Sij in the atlas space, then Ea is expressed as đ??¸đ?&lsquo;Ž = { 1 đ?&lsquo;&rsquo; đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľâ&circ;&rsquo;1 Fig.4. ROI distributions in Atlas space. (đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľâ&permil;¤1) (đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľ>1) (2) â&euro;&ndash; , 1 â&permil;¤ đ?&lsquo;&ndash; â&permil;¤ đ?&lsquo;&rsaquo;; 1 â&permil;¤ đ?&lsquo;&mdash; â&permil;¤ đ?&lsquo;&scaron;. } (3) where đ?&lsquo;&lsquo;đ?&lsquo;&scaron;đ?&lsquo;Žđ?&lsquo;Ľ = đ?&lsquo;&euro;đ?&lsquo;Žđ?&lsquo;Ľ { â&euro;&ndash; đ?&lsquo;&lsaquo; đ?&lsquo;&ndash;đ?&lsquo;&mdash; â&circ;&rsquo;đ?&lsquo;&euro; đ?&lsquo;&lsaquo; đ?&lsquo;&mdash; 3đ?&oelig;Ž đ?&lsquo;&lsaquo; đ?&lsquo;&mdash; Under the above definition, if any X ij is within the range of 3s X from the distribution model j center M X , the anatomical constraint energy will always be one; if not, there will be an j exponential increase of the energy which punishes the possible involvement of outliers. In other words, this energy factor will ensure the optimized ROIs will not significantly deviate away from the original ROIs. 2.2.2 Structural connectivity constraint energy Structural connectivity constraint energy Ec is defined to ensure the group has similar structural connectivity profiles for each functional ROI, since similar functional regions should have the similar structural connectivity patterns [19], n m Ec ď&euro;˝ ď&fnof;Ľď&fnof;Ľ (Cij ď&euro;­ M C j )Covc ď&euro;­1 (Ci j ď&euro;­ M C j )T (4) i ď&euro;˝1 j ď&euro;˝1 where Cij is the connectivity pattern vector for ROI j of subject i , M C j is the group mean ď&euro;­1 for ROI j , and Covc is the inverse of the covariance matrix. The connectivity pattern vector Cij is a fiber target region distribution histogram. To obtain this histogram, we first parcellate all the cortical surfaces into nine regions ( as shown in Fig.5a, four lobes for each hemisphere, and the subcortical region) using the HAMMER algorithm [17]. A finer parcellation is available but not used due to the relatively lower parcellation accuracy, which might render the histogram too sensitive to the parcellation result. Then, we extract fibers penetrating region Sij , and calculate the distribution of the fibersâ&euro;&trade; target cortical regions. Fig.5 illustrates the ideas. Fig.5. Structural connectivity pattern descriptor. (a): Cortical surface parcellation using HAMMER [17]; (b): Joint visualization of the cortical surface, two ROIs (blue and green spheres), and fibers penetrating the ROIs (in red and yellow, respectively); (c): Corresponding target region distribution histogram of ROIs in Fig.5b. There are nine bins corresponding to the nine cortical regions. Each bin contains the number of fibers that penetrate the ROI and are connected to the corresponding cortical region. Fiber numbers are normalized across subjects. 2.2.3 Functional connectivity constraint energy Functional connectivity constraint energy E f is defined to ensure each individual has similar functional connectivity patterns for the working memory system, assuming the human brain has similar functional architecture across individuals [21]. đ?&lsquo;&rsaquo; đ??¸ đ?&lsquo;&ldquo; = â&circ;&lsquo; đ?&lsquo;&ndash;=1â&euro;&ndash;đ??šđ?&lsquo;&ndash; â&circ;&rsquo; đ?&lsquo;&euro; đ??š â&euro;&ndash; (5) Here, Fi is the functional connectivity matrix for subject i , and M F is the group mean of the dataset. The connectivity between each pair of ROIs is defined using the Pearson correlation. The matrix distance used here is the Frobenius norm. 2.3 Energy minimization solution The minimization of the energy defined in Section 2.2 is known as a combinatorial optimization problem. Traditional optimization methods may not fit this problem, since there are two noticeable characteristics in this application. First, we do not know how the energy changes with the varying locations of ROIs. Therefore, techniques like Newtonâ&euro;&trade;s method cannot be used. Second, the structure of search space is not smooth, which may lead to multiple local minima during optimization. To address this problem, we adopt the simulated annealing (SA) algorithm [23] for the energy minimization. The idea of the SA algorithm is based on random walk through the space for lower energies. In these random walks, the probability of taking a step is determined by the Boltzmann distribution, - (E - E )/ ( KT ) p = e i+ 1 i (6) if Ei ď&euro;Ť1 ď&euro;ž Ei , and p ď&euro;˝ 1 when Ei ď&euro;Ť1 ď&sbquo;Ł Ei . Here, đ??¸ đ?&lsquo;&ndash; and đ??¸ đ?&lsquo;&ndash;+1 are the system energies at solution configuration đ?&lsquo;&ndash; and đ?&lsquo;&ndash; + 1 respectively; đ??ž is the Boltzmann constant; and đ?&lsquo;&Dagger; is the system temperature. In other words, a step will be taken when a lower energy is found. A step will also be taken with probability p if a higher energy is found. This helps avoid the local minima in the search space. 3 R esult s Compared to structural and functional connectivity patterns, anatomical profiles are more easily affected by variability across individuals. Therefore, the anatomical constraint energy is designed to provide constraint only to ROIs that are obviously far away from reasonableness. The reasonable range was statistically modeled by the localizations of ROIs warped into the atlas space in Section 2.2.1. Our focus in this paper is the structural and functional profiles. 3.1 Optimization using anatomical and structural connectivity profile s In this section, we use only anatomical and structural connectivity profiles to optimize the locations of ROIs. The goal is to check whether the structural constraint energy Ec works as expected. Fig.6 shows the fibers penetrating the right precuneus for eight subjects before (top panel) and after optimization (bottom panel). The ROI is highlighted in a red sphere for each subject. As we can see from the figure (please refer to the highlighted yellow arrows), after optimization, the third and sixth subjects have significantly improved consistency with the rest of the group than before optimization, which proves the validity of the energy function Eq.(4). Fig.6. Comparison of structural profiles before and after optimization. Each column shows the corresponding before-optimization (top) and after-optimization (bottom) fibers of one subject. The ROI (right precuneus) is presented by the red sphere. 3.2 Optimization using anatomical and functional connectivity profiles In this section, we optimize the locations of ROIs using anatomical and functional profiles, aiming to validate the definition of functional connectivity constraint energy E f . If this energy constraint worked well, the functional connectivity variance of the working memory system across subjects would decrease. Fig.7 shows the comparison of the standard derivation for functional connectivity before (left) and after (right) optimization. As we can see, the variance is significantly reduced after optimization. This demonstrated the effectiveness of the defined functional connectivity constraint energy. Fig.7. Comparison of the standard derivation for functional connectivity before and after the optimization. Lower values mean more consistent connectivity pattern cross subjects. 3.3 Consistency between optimization of functional profiles and structural profiles Fig.8. Optimization consistency between functional and structural profiles. Top: Functional profile energy drop along with structural profile optimization; Bottom: Structural profile energy drop along with functional profile optimization. Each experiment was repeated 15 times with random initial ROI locations that met the anatomical constraint. The relationship between structure and function has been extensively studied [24], and it is widely believed that they are closely related. In this section, we study the relationship between functional profiles and structural profiles by looking at how the energy for one of them changes while the energy of the other decreases. The optimization processes in Section 3.1 and 3.2 were repeated 15 times respectively with random initial ROI locations that met the anatomical constraint. As shown in Fig.8, in general, the functional profile energies and structural profile energies are closely related in such a way that the functional profile energies tend to decrease along with the structural profile optimization process, while the structural profile energies also tend to decrease as the functional profile is optimized. This positively correlated decrease of functional profile energy and structural profile energy not only proves the close relationship between functional and structural profiles, but also demonstrates the consistency between functional and structural optimization, laying down the foundation of the joint optimiza tion, whose results are detailed in the following section. 3.4 Optimization connectivity profiles using anatomical, structural and functional In this section, we used all the constraints in Eq. (1) to optimize the individual locations of all ROIs in the working memory system. Ten runs of the optimization were performed using random initial ROI locations that met the anatomical constraint. Weighting parameter ď Ź equaled 0.5 for all these runs. Starting and ending temperatures for the simulated annealing algorithm are 8 and 0.05; Boltzmann constant K ď&euro;˝ 1 . As we can see from Fig.9, most runs started to converge at step 24, and the convergence energy is quite close for all runs. This indicates that the simulated annealing algorithm provides a valid solution to our problem. By visual inspection, most of the ROIs move to more reasonable and consistent locations after the joint optimization. As an example, Fig.10 depicts the location movements of the ROI in Fig. 6 for eight subjects. As we can see, the ROIs for these subjects share a similar anatomical landmark, which appears to be the tip of the upper bank of the parieto-occipital sulcus. If the initial ROI was not at this landmark, it moved to the landmark after the optimization, which was the case for subjects 1, 4 and 7. The structural profiles of these ROIs are very similar to Fig.6. The results in Fig. 10 indicate the significant improvement of ROI locations achieved by the joint optimization procedure. Fig.9. Convergence performance of the simulated annealing . Each run has 28 temperature conditions. Fig.10. The movement of right precuneus before (in red sphere) and after (in green sphere) optimization for eight subjects. The</p><p>same-paper 2 0.82271558 <a title="194-lda-2" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>3 0.82012999 <a title="194-lda-3" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>4 0.81847203 <a title="194-lda-4" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>5 0.81519705 <a title="194-lda-5" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>6 0.81458777 <a title="194-lda-6" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>7 0.80440217 <a title="194-lda-7" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>8 0.79921275 <a title="194-lda-8" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>9 0.79151809 <a title="194-lda-9" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>10 0.79006553 <a title="194-lda-10" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>11 0.76419014 <a title="194-lda-11" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>12 0.75971365 <a title="194-lda-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.75104302 <a title="194-lda-13" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>14 0.74511999 <a title="194-lda-14" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>15 0.73294371 <a title="194-lda-15" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>16 0.72721928 <a title="194-lda-16" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>17 0.72706836 <a title="194-lda-17" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>18 0.72679806 <a title="194-lda-18" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>19 0.72199827 <a title="194-lda-19" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>20 0.72048432 <a title="194-lda-20" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
