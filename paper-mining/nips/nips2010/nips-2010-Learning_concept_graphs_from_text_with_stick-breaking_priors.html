<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>150 nips-2010-Learning concept graphs from text with stick-breaking priors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-150" href="#">nips2010-150</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>150 nips-2010-Learning concept graphs from text with stick-breaking priors</h1>
<br/><p>Source: <a title="nips-2010-150-pdf" href="http://papers.nips.cc/paper/3963-learning-concept-graphs-from-text-with-stick-breaking-priors.pdf">pdf</a></p><p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>Reference: <a title="nips-2010-150-reference" href="../nips2010_reference/nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. [sent-8, score-0.392]
</p><p>2 Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. [sent-9, score-0.229]
</p><p>3 The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. [sent-10, score-0.714]
</p><p>4 Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. [sent-12, score-0.236]
</p><p>5 1  Introduction  We present a generative probabilistic model for learning concept graphs from text. [sent-15, score-0.273]
</p><p>6 We deﬁne a concept graph as a rooted, directed graph where the nodes represent thematic units (called concepts) and the edges represent relationships between concepts. [sent-16, score-0.774]
</p><p>7 Concept graphs are useful for summarizing document collections and providing a visualization of the thematic content and structure of large document sets - a task that is difﬁcult to accomplish using only keyword search. [sent-17, score-0.592]
</p><p>8 An example of a concept graph is Wikipedia’s category graph1 . [sent-18, score-0.439]
</p><p>9 Figure 1 shows a small portion of the Wikipedia category graph rooted at the category M ACHINE LEARNING2 . [sent-19, score-0.447]
</p><p>10 From the graph we can quickly infer that the collection of machine learning articles in Wikipedia focuses primarily on evolutionary algorithms and Markov models with less emphasis on other aspects of machine learning such as Bayesian networks and kernel methods. [sent-20, score-0.413]
</p><p>11 The problem we address in this paper is that of learning a concept graph given a collection of documents where (optionally) we may have concept labels for the documents and an initial graph structure. [sent-21, score-1.124]
</p><p>12 This is particularly suited for document collections like Wikipedia where the set of articles is changing at such a fast rate that an automatic method for updating the concept graph may be preferable to manual editing or re-learning the hierarchy from scratch. [sent-25, score-0.708]
</p><p>13 LDA is a probabilistic model for automatically identifying topics within a document collection where a topic is a probability distribution over words. [sent-27, score-0.436]
</p><p>14 In contrast, methods such as the hierarchical topic model (hLDA) [2] learn a set of topics in the form of a tree structure. [sent-29, score-0.197]
</p><p>15 The restriction to tree structures however is not well suited for large document collections like Wikipedia. [sent-30, score-0.265]
</p><p>16 The hierarchical Pachinko allocation model (hPAM) [3] is able to learn a set of topics arranged in a ﬁxedsized graph with a nonparametric version introduced in [4]. [sent-32, score-0.271]
</p><p>17 In addition, our model provides a formal mechanism for utilizing labeled data and existing concept graph structures. [sent-36, score-0.507]
</p><p>18 Other methods for creating concept graphs include the use of techniques such as hierarchical clustering, pattern mining and formal concept analysis to construct ontologies from document collections [5, 6, 7]. [sent-37, score-0.693]
</p><p>19 Our primary novel contribution is the introduction of a ﬂexible probabilistic framework for learning general graph structures from text that is capable of utilizing both unlabeled documents as well as labeled documents and prior knowledge in the form of existing graph structures. [sent-39, score-1.002]
</p><p>20 We then introduce our generative model and explain how it can be adapted for the case where we have an initial graph structure. [sent-41, score-0.233]
</p><p>21 The probability of sampling a particular cluster from P(·) given the sequences {xj } and {vj } is not equal to the probability of sampling the same cluster given a permutation of the sequences {xσ(j) } and {vσ(j) }. [sent-54, score-0.211]
</p><p>22 We construct a prior on graph structures by specifying a distribution at each node (denoted as Pt ) that governs the probability of transitioning from node t to another node in the graph. [sent-64, score-1.073]
</p><p>23 However we may discover evidence for passing directly to a leaf node such as S TATISTICAL NATURAL L ANGUAGE P ROCESSING (e. [sent-68, score-0.243]
</p><p>24 Second, making a transition to a new node must have non-zero probability. [sent-71, score-0.243]
</p><p>25 For example, we may observe new articles related to the topic of Bioinformatics. [sent-72, score-0.272]
</p><p>26 In this case, we want to add a new node to the graph (B IOINFORMATICS) and assign some probability of transitioning to it from other nodes. [sent-73, score-0.552]
</p><p>27 For each node t we deﬁne a feasible set Ft as the collection of nodes to which t can transition. [sent-79, score-0.494]
</p><p>28 The feasible set may contain the children of node t or possible child nodes of node t (as discussed above). [sent-80, score-0.699]
</p><p>29 We add a special node called the ”exit node” to Ft . [sent-82, score-0.243]
</p><p>30 If we sample the exit node then we exit from the graph instead of transitioning forward. [sent-83, score-0.721]
</p><p>31 We deﬁne Pt as a stick-breaking distribution over the ﬁnite set of nodes Ft where the remaining probability mass is assigned to an inﬁnite set of new nodes (nodes that exist but have not yet been observed). [sent-84, score-0.344]
</p><p>32 |Ft |  Pt (·) =  ∞  πtj δftj (·) + j=1  πtj δxtj (·) j=|Ft |+1  The ﬁrst |Ft | atoms of the stick-breaking distribution are the feasible nodes ftj ∈ Ft . [sent-86, score-0.274]
</p><p>33 The remaining atoms are unidentiﬁable nodes that have yet to be observed (denoted as xtj for simplicity). [sent-87, score-0.226]
</p><p>34 In our experiments, we ﬁrst assign each node to a unique depth and then deﬁne Ft as any node at the next lower depth. [sent-91, score-0.486]
</p><p>35 The choice of Ft determines the type of graph structures that can be learned. [sent-92, score-0.222]
</p><p>36 More generally, one could extend the deﬁnition of Ft to include any node at a lower depth. [sent-95, score-0.243]
</p><p>37 We use a Metropolis-Hastings sampler proposed by [10] to learn the permutation of feasible nodes with the highest likelihood given the data. [sent-114, score-0.363]
</p><p>38 As discussed earlier, each node t is associated with a stick-breaking prior Pt . [sent-120, score-0.243]
</p><p>39 In addition, we associate with each node a multinomial distribution φt over words in the fashion of topic models. [sent-121, score-0.419]
</p><p>40 First, a path through the graph is sampled from the stick-breaking distributions. [sent-123, score-0.306]
</p><p>41 The i + 1st node in the path is sampled from Ppdi (·) which is the stick-breaking distribution at the ith node in the path. [sent-125, score-0.605]
</p><p>42 This process continues until an exit node is sampled. [sent-126, score-0.329]
</p><p>43 Then for each word xi a level in the path, ldi , is sampled from a truncated discrete distribution. [sent-127, score-0.51]
</p><p>44 The word xi is generated by the topic at level ldi of the path pd which we denote as pd [ldi ]. [sent-128, score-1.215]
</p><p>45 In the case where we observe labeled documents and an initial graph structure the paths for document d is restricted to end at the concept label of document d. [sent-129, score-1.091]
</p><p>46 The motivation is to constrain the length distribution to have the same general functional form across documents (in contrast to the relatively unconstrained multinomial), but to allow the parameters of the distribution to be documentspeciﬁc. [sent-132, score-0.197]
</p><p>47 If word xdi has level ldi = 0 then the word is generated by the topic at the last node on the path and successive levels correspond to earlier nodes in the path. [sent-135, score-1.379]
</p><p>48 In the case of labeled documents, this matches our belief that a majority of words in the document should be assigned to the concept label itself. [sent-136, score-0.414]
</p><p>49 We use a collapsed Gibbs sampler [9] to infer the path assignment pd for each document, the level distribution parameter τd for each document, and the level assignment ldi for each word. [sent-138, score-0.908]
</p><p>50 1  Sampling Paths  For each document, we must sample a path pd conditioned on all other paths p−d , the level variables, and the word tokens. [sent-141, score-0.68]
</p><p>51 p(pd |x, l, p−d , τ ) ∝ p(xd |x−d , l, p) · p(pd |p−d )  (1)  The ﬁrst term in Equation 1 is the probability of all words in the document given the path pd . [sent-143, score-0.543]
</p><p>52 We compute this probability by marginalizing over the topic distributions φt : λd  V  Γ(η + Npd [l],v )  p(xd |x−d , l, p) = l=1  v=1  −d Γ(η + Npd [l],v )  Γ(V η +  v  −d Npd [l],v )  Γ(V η +  ∗  v  Npd [l],v )  We use λd to denote the length of path pd . [sent-144, score-0.512]
</p><p>53 The notation Npd [l],v stands for the number of times word type v has been assigned to node pd [l]. [sent-145, score-0.593]
</p><p>54 The superscript −d means we ﬁrst decrement the count Npd [l],v for every word in document d. [sent-146, score-0.298]
</p><p>55 The second term is the conditional probability of the path pd given all other paths p−d . [sent-147, score-0.494]
</p><p>56 We present the sampling equation under the assumption that there is a maximum number of nodes M allowed at each level. [sent-148, score-0.218]
</p><p>57 We ﬁrst consider the probability of sampling a single edge in the path from a node x to one of its feasible nodes {y1 , y2 , . [sent-149, score-0.695]
</p><p>58 , yM } where the node y1 has the ﬁrst position in the stickbreaking permutation, y2 has the second position, y3 the third and so on. [sent-152, score-0.271]
</p><p>59 We denote the number of paths that have gone from x to yi as N(x,yi ) . [sent-153, score-0.223]
</p><p>60 We denote the number of paths that have gone from x to a node with a strictly higher position in the stick-breaking distribution M than yi as N(x,>yi ) . [sent-154, score-0.494]
</p><p>61 The probability of selecting node yi is given by: p(x → yi | p−d ) =  α + N(x,yi ) α + β + N(x,≥yi )  i−1  β + N(x,>yr ) α + β + N(x,≥yr ) r=1  for i = 1 . [sent-157, score-0.419]
</p><p>62 M  If ym is the last node with a nonzero count N(x,ym ) and m << M it is convenient to compute the probability of transitioning to yi , for i ≤ m, and the probability of transitioning to any node higher than ym . [sent-160, score-0.923]
</p><p>63 The probability of transitioning to a node higher than ym is given by M  p(x → yk |p−d ) = ∆ 1 − k=m+1  β M −m α+β  β+N(x,>yr ) m r=1 α+β+N(x,≥yr ) . [sent-161, score-0.425]
</p><p>64 where ∆ = A similar derivation can be used to compute the probability of sampling a node higher than ym when M is equal to inﬁnity. [sent-162, score-0.394]
</p><p>65 Now that we have computed the probability of a single edge, we can compute the probability of an entire path pd : λd  p(pd |p−d ) =  p(pdj → pd,j+1 |p−d ) j=1  4. [sent-163, score-0.402]
</p><p>66 2  Sampling Levels  For the ith word in the dth document we must sample a level ldi conditioned on all other levels l−di , the document paths, the level parameters τ , and the word tokens. [sent-164, score-1.108]
</p><p>67 p(ldi |x, l−di , p, τ ) =  −di η + Npd [ldi ],xdi −di W η + Npd [ldi ],·  ·  (1 − τd )ldi τd (1 − (1 − τd )λd +1 )  The ﬁrst term is the probability of word type xdi given the topic at node pd [ldi ]. [sent-165, score-0.806]
</p><p>68 The second term is the probability of the level ldi given the level parameter τd . [sent-166, score-0.475]
</p><p>69 3  Sampling τ Variables  Finally, we must sample the level distribution τd conditioned on the rest of the level parameters τ −d , the level variables, and the word tokens. [sent-168, score-0.34]
</p><p>70 Consider a node x with feasible nodes {y1 , y2 , . [sent-174, score-0.456]
</p><p>71 We sample two feasible nodes yi and yj from a uniform distribution3 . [sent-178, score-0.352]
</p><p>72 Then the probability of swapping the position of nodes yi and yj is given by N(x,yi ) −1  min 1, k=0  ∗ α + β + N(x,>yi ) + k  α + β + N(x,>yj ) + k  N(x,yj ) −1  · k=0  α + β + N(x,>yj ) + k ∗ α + β + N(x,>yi ) + k  ∗ where N(x,>yi ) = N(x,>yi ) − N(x,yj ) . [sent-180, score-0.327]
</p><p>73 After every new path assignment, we propose one swap for each node in the graph. [sent-182, score-0.362]
</p><p>74 Figure 4(a) shows a simulated concept graph with 10 nodes drawn according to the 3 In [10] feasible nodes are sampled from the prior probability distribution. [sent-187, score-0.795]
</p><p>75 The vocabulary size is 1, 000 words and we generate 4, 000 documents with 250 words each. [sent-191, score-0.197]
</p><p>76 Each edge in the graph is labeled with the number of paths that traverse it. [sent-192, score-0.454]
</p><p>77 Figures 4(b)-(d) show the learned graph structures as the fraction of labeled data increases from 0 labeled and 4, 000 unlabeled documents to all 4, 000 documents being labeled. [sent-193, score-0.784]
</p><p>78 In addition to labeling the edges, we label each node based upon the similarity of the learned topic at the node to the topics of the original graph structure. [sent-194, score-0.87]
</p><p>79 The Gibbs sampler is initialized to a root node when there is no labeled data. [sent-195, score-0.48]
</p><p>80 With labeled data, the Gibbs sampler is initialized with the correct placement of nodes to levels. [sent-196, score-0.44]
</p><p>81 The sampler does not observe the edge structure of the graph nor the correct number of nodes at each level (i. [sent-197, score-0.556]
</p><p>82 With no labeled data, the sampler is unable to recover the relationship between concepts 8 and 10 (due to the relatively small number of documents that contain words from both concepts). [sent-200, score-0.488]
</p><p>83 With 250 labeled documents, the sampler is able to learn the correct placement of both nodes 8 and 10 (although the topics contain some noise). [sent-201, score-0.465]
</p><p>84 For both GraphLDA and hPAM, the number of nodes at each level was set to 25. [sent-206, score-0.219]
</p><p>85 The edge widths correspond to the probability of the edge in the graph  5. [sent-292, score-0.275]
</p><p>86 We use the aforementioned 518 machine-learning Wikipedia articles, along with their category labels, to learn topic distributions for each node in Figure 1. [sent-294, score-0.476]
</p><p>87 The sampler is initialized with the correct placement of nodes and each document is initialized to a random path from the root to its category label. [sent-295, score-0.771]
</p><p>88 After 2, 000 iterations, we ﬁx the path assignments for the Wikipedia articles and introduce a new set of documents. [sent-296, score-0.251]
</p><p>89 We sample paths for the new collection of documents keeping the paths from the Wikipedia articles ﬁxed. [sent-298, score-0.638]
</p><p>90 The sampler was allowed to add new nodes to each level to explain any new concepts that occurred in the ICML text set. [sent-299, score-0.479]
</p><p>91 Figure 5 illustrates a portion of the ﬁnal graph structure. [sent-300, score-0.227]
</p><p>92 The nodes in bold are the original nodes from the Wikipedia category graph. [sent-301, score-0.407]
</p><p>93 The results show that the model is capable of augmenting an existing concept graph with new concepts (e. [sent-302, score-0.459]
</p><p>94 boosting/ensembles are on the same path as the concepts for SVMs and neural networks). [sent-307, score-0.205]
</p><p>95 6  Discussion and Conclusion  Motivated by the increasing availability of large-scale structured collections of documents such as Wikipedia, we have presented a ﬂexible non-parametric Bayesian framework for learning concept graphs from text. [sent-308, score-0.483]
</p><p>96 The proposed approach can combine unlabeled data with prior knowledge in the form of labeled documents and existing graph structures. [sent-309, score-0.468]
</p><p>97 Extensions such as allowing the model to handle multiple paths per document are likely to be worth pursuing. [sent-310, score-0.293]
</p><p>98 Computing the probability of every path during sampling, where the number of graphs is a product over the number of nodes at each level, is a computational bottleneck in the current inference algorithm and will not scale. [sent-312, score-0.374]
</p><p>99 The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies. [sent-323, score-0.242]
</p><p>100 Learning concept hierarchies from text using formal concept analysis. [sent-355, score-0.413]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ldi', 0.321), ('wikipedia', 0.259), ('hpam', 0.257), ('node', 0.243), ('graphlda', 0.236), ('pd', 0.223), ('documents', 0.197), ('graph', 0.187), ('hlda', 0.171), ('npd', 0.171), ('document', 0.171), ('ft', 0.165), ('concept', 0.159), ('nodes', 0.157), ('topic', 0.14), ('articles', 0.132), ('word', 0.127), ('paths', 0.122), ('sampler', 0.121), ('path', 0.119), ('category', 0.093), ('irvine', 0.092), ('transitioning', 0.092), ('exit', 0.086), ('concepts', 0.086), ('achine', 0.086), ('labeled', 0.084), ('beta', 0.08), ('yi', 0.073), ('pt', 0.073), ('graphs', 0.068), ('yr', 0.065), ('pachinko', 0.064), ('level', 0.062), ('sampling', 0.061), ('ym', 0.06), ('collections', 0.059), ('topics', 0.057), ('mallet', 0.056), ('thematic', 0.056), ('feasible', 0.056), ('evolutionary', 0.056), ('gibbs', 0.054), ('text', 0.053), ('simulated', 0.049), ('placement', 0.046), ('hastings', 0.046), ('generative', 0.046), ('cimiano', 0.043), ('classifier', 0.043), ('padhraic', 0.043), ('philosophy', 0.043), ('vtj', 0.043), ('xdi', 0.043), ('xtj', 0.043), ('metropolis', 0.042), ('formal', 0.042), ('dirichlet', 0.041), ('restaurant', 0.041), ('portion', 0.04), ('levels', 0.04), ('smyth', 0.039), ('yj', 0.039), ('collection', 0.038), ('keyword', 0.038), ('ihler', 0.038), ('xj', 0.037), ('genetic', 0.037), ('multinomial', 0.036), ('vj', 0.036), ('structures', 0.035), ('utilizing', 0.035), ('ftj', 0.035), ('ontologies', 0.035), ('porteous', 0.035), ('atom', 0.035), ('chinese', 0.034), ('rooted', 0.034), ('blei', 0.034), ('classification', 0.034), ('geometric', 0.032), ('traverse', 0.032), ('lda', 0.032), ('initialized', 0.032), ('di', 0.031), ('probability', 0.03), ('david', 0.03), ('edge', 0.029), ('ian', 0.029), ('accomplish', 0.029), ('permutation', 0.029), ('language', 0.028), ('gone', 0.028), ('position', 0.028), ('relationships', 0.028), ('andrew', 0.027), ('nonparametric', 0.027), ('sample', 0.027), ('capable', 0.027), ('atoms', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="150-tfidf-1" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>2 0.25583079 <a title="150-tfidf-2" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>3 0.19844104 <a title="150-tfidf-3" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>4 0.19535185 <a title="150-tfidf-4" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>5 0.15995789 <a title="150-tfidf-5" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>Author: Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David B. Dunson</p><p>Abstract: We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here deﬁned by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efﬁcient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. 1</p><p>6 0.1399457 <a title="150-tfidf-6" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>7 0.13588555 <a title="150-tfidf-7" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>8 0.11702031 <a title="150-tfidf-8" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>9 0.10252017 <a title="150-tfidf-9" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>10 0.09839011 <a title="150-tfidf-10" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>11 0.096902058 <a title="150-tfidf-11" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>12 0.093790442 <a title="150-tfidf-12" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>13 0.090895914 <a title="150-tfidf-13" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>14 0.088630497 <a title="150-tfidf-14" href="./nips-2010-New_Adaptive_Algorithms_for_Online_Classification.html">182 nips-2010-New Adaptive Algorithms for Online Classification</a></p>
<p>15 0.084272891 <a title="150-tfidf-15" href="./nips-2010-Multitask_Learning_without_Label_Correspondences.html">177 nips-2010-Multitask Learning without Label Correspondences</a></p>
<p>16 0.083299972 <a title="150-tfidf-16" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>17 0.082273699 <a title="150-tfidf-17" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>18 0.082118131 <a title="150-tfidf-18" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>19 0.077786632 <a title="150-tfidf-19" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>20 0.07553979 <a title="150-tfidf-20" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.212), (1, 0.062), (2, 0.036), (3, 0.028), (4, -0.371), (5, 0.055), (6, 0.156), (7, -0.043), (8, 0.004), (9, 0.031), (10, 0.027), (11, 0.039), (12, -0.079), (13, 0.128), (14, -0.0), (15, -0.007), (16, -0.021), (17, -0.151), (18, -0.18), (19, 0.012), (20, 0.05), (21, -0.002), (22, -0.044), (23, 0.057), (24, 0.006), (25, 0.048), (26, -0.002), (27, 0.053), (28, 0.103), (29, 0.064), (30, 0.059), (31, 0.073), (32, 0.008), (33, 0.044), (34, 0.024), (35, 0.018), (36, 0.038), (37, 0.047), (38, 0.003), (39, -0.036), (40, -0.016), (41, 0.041), (42, 0.007), (43, 0.068), (44, -0.024), (45, 0.104), (46, -0.014), (47, -0.036), (48, -0.005), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95795625 <a title="150-lsi-1" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>2 0.75348181 <a title="150-lsi-2" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>3 0.69744974 <a title="150-lsi-3" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>Author: Eric Wang, Dehong Liu, Jorge Silva, Lawrence Carin, David B. Dunson</p><p>Abstract: We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here deﬁned by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efﬁcient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives. 1</p><p>4 0.67336988 <a title="150-lsi-4" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>5 0.65200466 <a title="150-lsi-5" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>6 0.56264865 <a title="150-lsi-6" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>7 0.50368422 <a title="150-lsi-7" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>8 0.48700696 <a title="150-lsi-8" href="./nips-2010-Optimal_Web-Scale_Tiering_as_a_Flow_Problem.html">198 nips-2010-Optimal Web-Scale Tiering as a Flow Problem</a></p>
<p>9 0.46050921 <a title="150-lsi-9" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>10 0.45735604 <a title="150-lsi-10" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>11 0.44934803 <a title="150-lsi-11" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>12 0.43521547 <a title="150-lsi-12" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>13 0.41995245 <a title="150-lsi-13" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>14 0.40722555 <a title="150-lsi-14" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>15 0.40641469 <a title="150-lsi-15" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>16 0.39965773 <a title="150-lsi-16" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>17 0.39725426 <a title="150-lsi-17" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>18 0.39358395 <a title="150-lsi-18" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>19 0.39008045 <a title="150-lsi-19" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>20 0.3889403 <a title="150-lsi-20" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.046), (22, 0.019), (27, 0.143), (30, 0.07), (35, 0.012), (45, 0.184), (50, 0.064), (52, 0.028), (58, 0.229), (60, 0.055), (77, 0.037), (78, 0.012), (90, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84053868 <a title="150-lda-1" href="./nips-2010-Decoding_Ipsilateral_Finger_Movements_from_ECoG_Signals_in_Humans.html">57 nips-2010-Decoding Ipsilateral Finger Movements from ECoG Signals in Humans</a></p>
<p>Author: Yuzong Liu, Mohit Sharma, Charles Gaona, Jonathan Breshears, Jarod Roland, Zachary Freudenburg, Eric Leuthardt, Kilian Q. Weinberger</p><p>Abstract: Several motor related Brain Computer Interfaces (BCIs) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices. Contralateral primary motor cortex is also the region most severely affected by hemispheric stroke. Recent studies have identiﬁed ipsilateral cortical activity in planning of motor movements and its potential implications for a stroke relevant BCI. The most fundamental functional loss after a hemispheric stroke is the loss of ﬁne motor control of the hand. Thus, whether ipsilateral cortex encodes ﬁnger movements is critical to the potential feasibility of BCI approaches in the future. This study uses ipsilateral cortical signals from humans (using ECoG) to decode ﬁnger movements. We demonstrate, for the ﬁrst time, successful ﬁnger movement detection using machine learning algorithms. Our results show high decoding accuracies in all cases which are always above chance. We also show that signiﬁcant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features are consistent with previous physiological ﬁndings. The results of this study have substantial implications for advancing neuroprosthetic approaches to stroke populations not currently amenable to existing BCI techniques. 1</p><p>same-paper 2 0.80860341 <a title="150-lda-2" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>Author: America Chambers, Padhraic Smyth, Mark Steyvers</p><p>Abstract: We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents—a task that is difﬁcult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs. 1</p><p>3 0.73643625 <a title="150-lda-3" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>4 0.72957706 <a title="150-lda-4" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>5 0.72538292 <a title="150-lda-5" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>6 0.72533345 <a title="150-lda-6" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>7 0.72463697 <a title="150-lda-7" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>8 0.72212452 <a title="150-lda-8" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>9 0.71855122 <a title="150-lda-9" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>10 0.71722442 <a title="150-lda-10" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>11 0.71296227 <a title="150-lda-11" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>12 0.71245396 <a title="150-lda-12" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>13 0.71222556 <a title="150-lda-13" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>14 0.70967048 <a title="150-lda-14" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>15 0.70703459 <a title="150-lda-15" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>16 0.70683771 <a title="150-lda-16" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>17 0.70670873 <a title="150-lda-17" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>18 0.70430648 <a title="150-lda-18" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>19 0.70364314 <a title="150-lda-19" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>20 0.70240122 <a title="150-lda-20" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
