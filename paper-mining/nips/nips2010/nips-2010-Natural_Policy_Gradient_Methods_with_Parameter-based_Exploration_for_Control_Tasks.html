<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-179" href="#">nips2010-179</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</h1>
<br/><p>Source: <a title="nips-2010-179-pdf" href="http://papers.nips.cc/paper/3987-natural-policy-gradient-methods-with-parameter-based-exploration-for-control-tasks.pdf">pdf</a></p><p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>Reference: <a title="nips-2010-179-reference" href="../nips2010_reference/nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. [sent-6, score-0.705]
</p><p>2 Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. [sent-7, score-0.864]
</p><p>3 The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. [sent-8, score-1.298]
</p><p>4 Experimental results show that the proposed method outperforms several policy gradient methods. [sent-9, score-0.623]
</p><p>5 1  Introduction  Reinforcement learning can be used to handle policy search problems in unknown environments. [sent-10, score-0.47]
</p><p>6 Policy gradient methods [22, 20, 5] train parameterized stochastic policies by climbing the gradient of the average reward. [sent-11, score-0.354]
</p><p>7 Policy gradient methods have thus been successfully applied to several practical tasks [11, 21, 16]. [sent-13, score-0.173]
</p><p>8 In the domain of control, a policy is often constructed with a controller and an exploration strategy. [sent-14, score-1.074]
</p><p>9 The controller is represented by a domain-appropriate pre-structured parametric function. [sent-15, score-0.18]
</p><p>10 The exploration strategy is required to seek the parameters of the controller. [sent-16, score-0.52]
</p><p>11 Instead of directly perturbing the parameters of the controller, conventional exploration strategies perturb the resulting control signal. [sent-17, score-0.585]
</p><p>12 However, a signiﬁcant problem with the sampling strategy is that the high variance in their gradient estimates leads to slow convergence. [sent-18, score-0.253]
</p><p>13 Recently, parameter-based exploration [18] strategies that search the controller parameter space by direct parameter perturbation have been proposed, and these have been demonstrated to work more efﬁciently than conventional strategies [17, 18, 13]. [sent-19, score-0.77]
</p><p>14 Another approach to speeding up policy gradient methods is to replace the gradient with the natural gradient [2], the so-called natural policy gradient [9, 4, 15]; this is motivated by the intuition that a change in the policy parameterization should not inﬂuence the result of the policy update. [sent-20, score-2.656]
</p><p>15 The combination of parameter-based exploration strategies and the natural policy gradient is expected to result in improvements in the convergence rate; however, such an algorithm has not yet been proposed. [sent-21, score-1.179]
</p><p>16 However, natural policy gradients with parameter-based exploration strategies have a disadvantage in that the computational cost is high. [sent-22, score-1.101]
</p><p>17 The natural policy gradient requires the computation of the inverse of the Fisher information matrix (FIM) of the policy distribution; this is prohibitively expensive, especially for a high-dimensional policy. [sent-23, score-1.264]
</p><p>18 Unfortunately, parameter-based exploration strategies tend to have higher dimensions than control-based ones. [sent-24, score-0.474]
</p><p>19 1  In this paper, we propose a new reinforcement learning method that combines the natural policy gradient and parameter-based exploration. [sent-26, score-0.799]
</p><p>20 We derive an efﬁcient algorithm for estimating the natural policy gradient with a particular exploration strategy implementation. [sent-27, score-1.203]
</p><p>21 Our algorithm calculates the natural policy gradient using the inverse of the exact FIM and the Monte Carlo-estimated gradient. [sent-28, score-0.782]
</p><p>22 The resulting algorithm, called natural policy gradients with parameter-based exploration (NPGPE), has a computational cost similar to that of conventional policy gradient algorithms. [sent-29, score-1.722]
</p><p>23 Numerical experiments show that the proposed method outperforms several policy gradient methods, including the current state-of-the-art NAC [15] with control-based exploration. [sent-30, score-0.623]
</p><p>24 2 Policy Search Framework We consider the standard reinforcement learning framework in which an agent interacts with a Markov decision process. [sent-31, score-0.229]
</p><p>25 In this section, we review the estimation of policy gradients and describe the difference between control- and parameter-based exploration. [sent-32, score-0.545]
</p><p>26 1  Markov Decision Process Notation  At each discrete time t, the agent observes state st ∈ S, selects action at ∈ A, and then receives an instantaneous reward rt ∈ resulting from a state transition in the environment. [sent-34, score-0.48]
</p><p>27 The objective of the reinforcement learning agent is to construct a policy that maximizes the agent’s performance. [sent-38, score-0.676]
</p><p>28 A parameterized policy π(a|s, θ) is deﬁned as a probability distribution over an action space under a given state with parameters θ. [sent-39, score-0.539]
</p><p>29 A  Policy Gradients  Policy gradient methods update policies by estimating the gradient of the average reward w. [sent-43, score-0.385]
</p><p>30 The exact gradient of the average reward (see [20]) is given by θ η(θ)  =  pD (s|θ) S  π(a|s, θ)  θ  log π(a|s, θ)Qθ (s, a)dads. [sent-51, score-0.254]
</p><p>31 (1)  A  The natural gradient [2] has a basis in information geometry, which studies the Riemannian geometric structure of the manifold of probability distributions. [sent-52, score-0.235]
</p><p>32 Thus, the natural gradient can be computed from the gradient and the FIM, and it tends to converge faster than the conventional gradient. [sent-54, score-0.436]
</p><p>33 Kakade [9] applied the natural gradient to policy search; this was called as the natural policy gradient. [sent-55, score-1.257]
</p><p>34 If the FIM is invertible, the natural policy gradient ˜ θ η(θ) ≡ F−1 θ η(θ) is given by the θ policy gradient premultiplied by the inverse matrix of the FIM Fθ . [sent-56, score-1.451]
</p><p>35 Figure 1: Illustration of the main difference between control-based exploration and parameter-based exploration. [sent-58, score-0.424]
</p><p>36 The controller ψ(u|s, w) is represented by a single-layer perceptron. [sent-59, score-0.18]
</p><p>37 While the controlbased exploration strategy (left) perturbs the resulting control signal, the parameter-based exploration strategy (right) perturbs the parameters of the controller. [sent-60, score-1.163]
</p><p>38 The GPOMDP algorithm [5] instead computes a Monte Carlo approximation of (1): the agent interacts with the environment, producing an observation, action, and reward sequence {s1 , a1 , r1 , s2 , . [sent-63, score-0.192]
</p><p>39 Under mild technical assumptions, the policy gradient approximation is θ η(θ)  ≈  1 T  T  rt zt , t=1  where zt = βzt−1 + θ log π(at |st , θ) is called the eligibility trace [12], θ log π(at |st , θ) is called the characteristic eligibility [22], and β denotes the discount factor (0 ≤ β < 1). [sent-67, score-1.087]
</p><p>40 Therefore, the natural policy θ gradient approximation is ˜ θ η(θ) ≈ 1 T  T  F−1 rt zt = θ t=1  1 T  T  ˜ rt zt ,  (2)  t=1  ˜ where zt = β˜t−1 + ˜ θ log π(at |st , θ). [sent-71, score-1.083]
</p><p>41 To estimate the natural policy gradient, the heuristic sugz gested by Kakade [9] used 1 1 Fθ,t = (1 − )Fθ,t−1 + ( θ log π(at |st , θ) θ log π(at |st , θ)T + λI), (3) t t the online estimate of the FIM, where λ is a small positive constant. [sent-72, score-0.64]
</p><p>42 4  Parameter-based Exploration  In most control tasks, we attempt to have a (deterministic or stochastic) controller ψ(u|s, w) and an exploration strategy, where u ∈ U ⊆ m denotes control and w ∈ W ⊆ n , the parameters of the controller. [sent-74, score-0.73]
</p><p>43 The objective of learning is to seek suitable values of the parameters w, and the exploration strategy is required to carry out stochastic sampling near the current parameters. [sent-75, score-0.546]
</p><p>44 A typical exploration strategy model, we call control-based exploration, would be a normal distribution for the control space (Figure1 (left)). [sent-76, score-0.581]
</p><p>45 In this case, the action of the agent is control, and the policy is represented by 1 1 exp − (u − ψ(s, w))T Σ−1 (u − ψ(s, w)) : S → U, πU (u|s, θ) = 2 (2π)m/2 |Σ|1/2 where Σ is the m × m covariance matrix and the agent seeks θ = w, Σ . [sent-77, score-0.791]
</p><p>46 The control at time t is generated by ˜ ut = ψ(st , w), ut ∼ N (˜ t , Σ). [sent-78, score-0.161]
</p><p>47 u 1 [5] showed that the approximation error is proportional to (1−β)/(1−|κ2 |), where κ2 is the sub-dominant eigenvalue of the Markov chain  3  One useful feature of such a Gaussian unit [22] is that the agent can potentially control its degree of exploratory behavior. [sent-79, score-0.175]
</p><p>48 The control-based exploration strategy samples near the output of the controller. [sent-80, score-0.498]
</p><p>49 Therefore, the sampling strategy generates controls that are not likely to be generated from the current controller, even if the exploration variances decrease. [sent-82, score-0.524]
</p><p>50 This might be one reason why the policy improvement gets stuck. [sent-84, score-0.47]
</p><p>51 [18] introduced a different exploration strategy for policy gradient methods called policy gradients with parameter-based exploration (PGPE). [sent-86, score-2.09]
</p><p>52 In this approach, the action of the agent is the parameters of the controller, and the policy is represented by ˜ πW (w|s, θ) =  1 1 ˜ ˜ ˜ exp − (w − w)T Σ−1 (w − w) ˜ 2 (2π)n/2 |Σ|1/2  : S → W,  ˜ ˜ where Σ is the n × n covariance matrix and the agent seeks θ = w, Σ . [sent-87, score-0.791]
</p><p>53 The controller is included in the dynamics of the environment, and the control at time t is generated by ˜ ˜ wt ∼ N (w, Σ), ˜ ut = ψ(st , wt ). [sent-88, score-0.732]
</p><p>54 GPOMDP-based methods can estimate policy gradients such as partially observable settings, i. [sent-89, score-0.545]
</p><p>55 , the ˜ policy πW (w|s, θ) excludes the observation of the current state. [sent-91, score-0.47]
</p><p>56 Because this exploration strategy directly perturbs the parameters (Figure1 (right)), the samples are generated near the current parameters under small exploration variances. [sent-92, score-0.974]
</p><p>57 Note that the advantage of this framework is that because the gradient is estimated directly by sampling the parameters of the controller, the implementation ∂ of the policy gradient algorithms does not require ∂θ ψ, which is difﬁcult to derive from complex controllers. [sent-93, score-0.802]
</p><p>58 [18] demonstrated that PGPE can yield faster convergence than the control-based exploration strategy in several challenging episodic tasks. [sent-95, score-0.537]
</p><p>59 However, the parameter-based exploration tends to have a higher dimension than the control-based one. [sent-96, score-0.424]
</p><p>60 Therefore, because of the computational cost of the inverse of Fθ calculated by (3), natural policy gradients ﬁnd limited applications. [sent-97, score-0.671]
</p><p>61 3  Natural Policy Gradients with Parameter-based Exploration  In this section, we propose a new algorithm called natural policy gradients with parameter-based exploration (NPGPE) for the efﬁcient estimation of the natural policy gradient. [sent-98, score-1.603]
</p><p>62 2  Inverse of Fisher Information Matrix  Previous natural policy gradient methods [9] use the empirical FIM, which is estimated from a ˜ sample path. [sent-111, score-0.705]
</p><p>63 4  Algorithm 1 Natural Policy Gradient Method with Parameter-based Exploration Require: θ = w, C : policy parameters, ψ(u|s, w): controller, α: step size, β: discount rate, b: baseline. [sent-114, score-0.47]
</p><p>64 do ˜ 3: Draw ξt ∼ N (0, I), compute action wt = CT ξt + w. [sent-119, score-0.271]
</p><p>65 ˜ 4: Execute ut ∼ ψ(ut |st , wt ), obtain observation st+1 and reward rt . [sent-120, score-0.394]
</p><p>66 W  Furthermore, the FIM of this distribution is Fθ =  ˜ µ(w|θ)  pD (s|θ) S  ˜ µ(w|θ)  =  θ  ˜ log µ(w|θ)  θ  ˜ ˜ log µ(w|θ)T dwds  W θ  ˜ log µ(w|θ)  θ  ˜ ˜ log µ(w|θ)T dw. [sent-122, score-0.176]
</p><p>67 Because Fθ is a block-diagonal matrix and C is upper triangular, it is easy to verify that the inverse matrix of the FIM is 1 0 0 0 T F−1 = [0 Ik ] CT − vk vk + C , ¯ k Ik 0 Ik ¯ ¯ 2 where we use T vk C  3. [sent-131, score-0.53]
</p><p>68 ¯ ¯ 0 Ik 0 Ik ¯ ¯  (4)  Natural Policy Gradient  ˜ Now, we derive the eligibility premultiplied by the inverse matrix of the FIM ˜ θ log µ(wt |θ) = ˜ F−1 θ log µ(wt |θ) in the same manner as [1]. [sent-133, score-0.285]
</p><p>69 This is a signiﬁcant improvement over the current natural policy gradient estimation using (2) and (3) with parameter-based exploration, whose complexity is O(n6 ). [sent-165, score-0.705]
</p><p>70 Note that more simple forms for exploration distribution could be used. [sent-166, score-0.424]
</p><p>71 When we use the exploration strategy that is represented as an independent normal distribution for each parameter wi in w, the natural policy gradient is estimated in O(n) time. [sent-167, score-1.223]
</p><p>72 4  An Algorithm  ˜ For a parameterized class of controllers ψ(u|s, w), we can use the exploration strategy µ(w|θ). [sent-170, score-0.498]
</p><p>73 In ˜ ˜ practice, the parameters of the controller wt are generated by wt = CT ξt + w, where ξt ∼ N (0, I) T ˜ ˜ are normal random numbers. [sent-172, score-0.64]
</p><p>74 To reduce the variance of the gradient estimation, we employ variance reduction techniques [6] to adapt the reinforcement baseline b. [sent-174, score-0.247]
</p><p>75 The efﬁciency of parameter-based exploration has been reported for episodic tasks [18]. [sent-177, score-0.483]
</p><p>76 We compare parameter- and control-based exploration strategies with natural gradient and conventional ”vanilla” gradients using a simple continuing task as an example of a linear control problem. [sent-178, score-0.93]
</p><p>77 1  Implementation  We compare two different exploration strategies. [sent-181, score-0.424]
</p><p>78 The ﬁrst is the parameter-based exploration strat˜ egy µ(w|θ) presented in Section 3. [sent-182, score-0.424]
</p><p>79 The parameters of the policy πU (u|s, θ) are θ = w, D to be an [n + m(m + 1)/2]-dimensional column vector consisting of the elements of w and the upper-right elements of D. [sent-185, score-0.47]
</p><p>80 2  Linear Quadratic Regulator  The following linear control problem can serve as a benchmark of delayed reinforcement tasks [10]. [sent-187, score-0.177]
</p><p>81 The dynamics of the environment is st+1 = st + ut + δ, 1  1  where s ∈ , u ∈ , and δ ∼ N (0, 0. [sent-188, score-0.213]
</p><p>82 When the agent chooses an action that does not lie in the range [−4, 4], the action executed in the environment is also truncated. [sent-192, score-0.243]
</p><p>83 The controller is represented by ψ(u|s, w) = s · w, where w ∈ 1 . [sent-193, score-0.18]
</p><p>84 For clariﬁcation, we now write an NPG that employs the natural policy gradient and a VPG that employs the ”vanilla” policy gradient. [sent-195, score-1.213]
</p><p>85 Therefore, NPG(w) and VPG(w) denote the use of the parameterbased exploration strategy, and NPG(u) and VPG(u) denote the use of the control-based exploration strategy. [sent-196, score-0.848]
</p><p>86 We can see that the algorithm using parameter-based exploration had better performance than that using control-based exploration in the continuing task. [sent-199, score-0.883]
</p><p>87 The natural policy gradient also improved the convergence speed, and a combination with parameter-based exploration outperformed all other methods. [sent-200, score-1.129]
</p><p>88 The reason for the acceleration in learning in this case may be the fact that the samples generated by the parameter-based exploration strategy allow effective search. [sent-201, score-0.498]
</p><p>89 Because control-based exploration maintains the sampling area in the control space, the sampling is almost uniform in the parameter space at around s = 0, where the agent visits frequently. [sent-203, score-0.651]
</p><p>90 Therefore, the parameter-based exploration may realize more efﬁcient sampling than the control-based exploration. [sent-204, score-0.45]
</p><p>91 The parameters of the controller are normalized by gain i = j wi,j and weight i,j = wi,j /gain i , where wi,j denotes the j-th parameter of the i-th joint. [sent-217, score-0.199]
</p><p>92 The dimension of the parameters of the policies is dW = n(n + 3)/2 = 27 and dU = n + m(m + 1)/2 = 9 for the parameter- and control-based exploration strategy, respectively. [sent-225, score-0.446]
</p><p>93 NAC is the state-of-the-art policy gradient algorithm [15] that combines natural policy gradients, actor-critic framework, and leastsquares temporal-difference Q-learning. [sent-229, score-1.175]
</p><p>94 NAC computes the inverse of a d × d matrix to estimate the natural steepest ascent direction. [sent-230, score-0.176]
</p><p>95 5  Conclusions  This paper proposed a novel natural policy gradient method combined with parameter-based exploration to cope with high-dimensional reinforcement learning domains. [sent-237, score-1.223]
</p><p>96 The proposed algorithm, NPGPE, is very simple and quickly calculates the estimation of the natural policy gradient. [sent-238, score-0.585]
</p><p>97 Future works will focus on developing actor-critic versions of NPGPE that might encourage performance improvements at an early stage, and on combining other gradient methods such as natural conjugate gradient methods [8]. [sent-240, score-0.388]
</p><p>98 Variance reduction techniques for gradient estimates in reinforcement learning. [sent-266, score-0.247]
</p><p>99 Reinforcement learning for continuous action using stochastic gradient ascent. [sent-284, score-0.204]
</p><p>100 Policy gradient method for reinforcement learning with function approximation. [sent-320, score-0.247]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.47), ('exploration', 0.424), ('npg', 0.363), ('wt', 0.22), ('fim', 0.212), ('controller', 0.18), ('gradient', 0.153), ('vk', 0.144), ('ik', 0.144), ('nac', 0.139), ('npgpe', 0.138), ('st', 0.135), ('vpg', 0.121), ('agent', 0.112), ('reinforcement', 0.094), ('pd', 0.085), ('natural', 0.082), ('ct', 0.076), ('gradients', 0.075), ('yt', 0.075), ('eligibility', 0.074), ('strategy', 0.074), ('kimura', 0.069), ('shigenobu', 0.069), ('triu', 0.069), ('rt', 0.068), ('zt', 0.066), ('control', 0.063), ('reward', 0.057), ('isao', 0.052), ('nagata', 0.052), ('perturbs', 0.052), ('premultiplied', 0.052), ('sehnke', 0.052), ('action', 0.051), ('strategies', 0.05), ('ut', 0.049), ('conventional', 0.048), ('diag', 0.046), ('triangular', 0.045), ('inverse', 0.044), ('log', 0.044), ('peters', 0.042), ('locomotion', 0.042), ('riemannian', 0.041), ('episodic', 0.039), ('ck', 0.037), ('robot', 0.036), ('continuing', 0.035), ('jan', 0.035), ('akimoto', 0.035), ('cvk', 0.035), ('daan', 0.035), ('hajime', 0.035), ('miyamae', 0.035), ('motors', 0.035), ('ono', 0.035), ('pgpe', 0.035), ('promotion', 0.035), ('wierstra', 0.035), ('yuichi', 0.035), ('japan', 0.034), ('calculates', 0.033), ('juergen', 0.03), ('gpomdp', 0.03), ('vanilla', 0.03), ('environment', 0.029), ('characteristic', 0.028), ('fk', 0.028), ('arm', 0.028), ('matrix', 0.027), ('center', 0.027), ('climbing', 0.026), ('kakade', 0.026), ('sampling', 0.026), ('pomdps', 0.023), ('steepest', 0.023), ('cholesky', 0.023), ('stefan', 0.023), ('interacts', 0.023), ('policies', 0.022), ('seek', 0.022), ('fisher', 0.021), ('sun', 0.021), ('observes', 0.021), ('tasks', 0.02), ('normal', 0.02), ('jonathan', 0.02), ('block', 0.02), ('employs', 0.019), ('relation', 0.019), ('gain', 0.019), ('covariance', 0.019), ('evolution', 0.019), ('arrows', 0.018), ('prohibitively', 0.018), ('state', 0.018), ('pages', 0.018), ('stage', 0.018), ('perturbation', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="179-tfidf-1" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>2 0.39982334 <a title="179-tfidf-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>3 0.37028357 <a title="179-tfidf-3" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present policy gradient results within the framework of linearly-solvable MDPs. For the ﬁrst time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the ﬁrst compatible function approximators and natural policy gradients for continuous-time stochastic systems.</p><p>4 0.35165253 <a title="179-tfidf-4" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>5 0.30144405 <a title="179-tfidf-5" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>Author: Finale Doshi-velez, David Wingate, Nicholas Roy, Joshua B. Tenenbaum</p><p>Abstract: We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning. 1</p><p>6 0.29133648 <a title="179-tfidf-6" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>7 0.27967483 <a title="179-tfidf-7" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>8 0.25089094 <a title="179-tfidf-8" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>9 0.24211764 <a title="179-tfidf-9" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>10 0.22057343 <a title="179-tfidf-10" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>11 0.21184239 <a title="179-tfidf-11" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>12 0.18791997 <a title="179-tfidf-12" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>13 0.15547125 <a title="179-tfidf-13" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>14 0.14587985 <a title="179-tfidf-14" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>15 0.1447138 <a title="179-tfidf-15" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>16 0.14041682 <a title="179-tfidf-16" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>17 0.12891723 <a title="179-tfidf-17" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>18 0.11851572 <a title="179-tfidf-18" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>19 0.11700938 <a title="179-tfidf-19" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>20 0.11093578 <a title="179-tfidf-20" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.235), (1, -0.517), (2, -0.08), (3, -0.062), (4, 0.015), (5, -0.067), (6, 0.07), (7, -0.018), (8, -0.004), (9, -0.077), (10, -0.001), (11, -0.038), (12, 0.093), (13, 0.023), (14, 0.029), (15, 0.043), (16, 0.046), (17, -0.041), (18, -0.052), (19, 0.011), (20, -0.03), (21, 0.027), (22, 0.031), (23, 0.061), (24, -0.062), (25, -0.004), (26, 0.027), (27, 0.0), (28, -0.017), (29, 0.033), (30, 0.035), (31, -0.019), (32, 0.048), (33, 0.029), (34, 0.011), (35, 0.06), (36, 0.063), (37, -0.076), (38, 0.011), (39, 0.035), (40, -0.001), (41, -0.03), (42, 0.055), (43, 0.107), (44, -0.007), (45, -0.001), (46, -0.068), (47, -0.034), (48, -0.003), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97919202 <a title="179-lsi-1" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>2 0.88891035 <a title="179-lsi-2" href="./nips-2010-On_a_Connection_between_Importance_Sampling_and_the_Likelihood_Ratio_Policy_Gradient.html">189 nips-2010-On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>
<p>Author: Tang Jie, Pieter Abbeel</p><p>Abstract: Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (i) using the past experience to estimate only the gradient of the expected return U (θ) at the current policy parameterization θ, rather than to obtain a more complete estimate of U (θ), and (ii) using past experience under the current policy only rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines—a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds. 1</p><p>3 0.86898208 <a title="179-lsi-3" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>Author: Alex Strehl, John Langford, Lihong Li, Sham M. Kakade</p><p>Abstract: We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned. The primary challenge in a variety of settings is that the exploration policy, in which “ofﬂine” data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner. The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged. We empirically verify our solution on two reasonably sized sets of real-world data obtained from Yahoo!.</p><p>4 0.86623007 <a title="179-lsi-4" href="./nips-2010-A_Reduction_from_Apprenticeship_Learning_to_Classification.html">14 nips-2010-A Reduction from Apprenticeship Learning to Classification</a></p>
<p>Author: Umar Syed, Robert E. Schapire</p><p>Abstract: We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classiﬁcation algorithm to learn to imitate the expert’s behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classiﬁer has error rate ǫ, the difference between the √ value of the apprentice’s policy and the expert’s policy is O( ǫ). Further, we prove that this difference is only O(ǫ) when the expert’s policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difﬁcult to obtain. 1</p><p>5 0.82842761 <a title="179-lsi-5" href="./nips-2010-Policy_gradients_in_linearly-solvable_MDPs.html">208 nips-2010-Policy gradients in linearly-solvable MDPs</a></p>
<p>Author: Emanuel Todorov</p><p>Abstract: We present policy gradient results within the framework of linearly-solvable MDPs. For the ﬁrst time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the ﬁrst compatible function approximators and natural policy gradients for continuous-time stochastic systems.</p><p>6 0.82105744 <a title="179-lsi-6" href="./nips-2010-Linear_Complementarity_for_Regularized_Policy_Evaluation_and_Improvement.html">160 nips-2010-Linear Complementarity for Regularized Policy Evaluation and Improvement</a></p>
<p>7 0.79376256 <a title="179-lsi-7" href="./nips-2010-Nonparametric_Bayesian_Policy_Priors_for_Reinforcement_Learning.html">184 nips-2010-Nonparametric Bayesian Policy Priors for Reinforcement Learning</a></p>
<p>8 0.75528425 <a title="179-lsi-8" href="./nips-2010-Error_Propagation_for_Approximate_Policy_and_Value_Iteration.html">78 nips-2010-Error Propagation for Approximate Policy and Value Iteration</a></p>
<p>9 0.73323447 <a title="179-lsi-9" href="./nips-2010-Bootstrapping_Apprenticeship_Learning.html">43 nips-2010-Bootstrapping Apprenticeship Learning</a></p>
<p>10 0.66032666 <a title="179-lsi-10" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>11 0.62068886 <a title="179-lsi-11" href="./nips-2010-Constructing_Skill_Trees_for_Reinforcement_Learning_Agents_from_Demonstration_Trajectories.html">50 nips-2010-Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories</a></p>
<p>12 0.61240244 <a title="179-lsi-12" href="./nips-2010-Predictive_State_Temporal_Difference_Learning.html">212 nips-2010-Predictive State Temporal Difference Learning</a></p>
<p>13 0.60568041 <a title="179-lsi-13" href="./nips-2010-Batch_Bayesian_Optimization_via_Simulation_Matching.html">38 nips-2010-Batch Bayesian Optimization via Simulation Matching</a></p>
<p>14 0.58823615 <a title="179-lsi-14" href="./nips-2010-PAC-Bayesian_Model_Selection_for_Reinforcement_Learning.html">201 nips-2010-PAC-Bayesian Model Selection for Reinforcement Learning</a></p>
<p>15 0.55726898 <a title="179-lsi-15" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>16 0.54921746 <a title="179-lsi-16" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>17 0.5402115 <a title="179-lsi-17" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>18 0.48572651 <a title="179-lsi-18" href="./nips-2010-Feature_Construction_for_Inverse_Reinforcement_Learning.html">93 nips-2010-Feature Construction for Inverse Reinforcement Learning</a></p>
<p>19 0.47643486 <a title="179-lsi-19" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>20 0.44421688 <a title="179-lsi-20" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.055), (15, 0.324), (16, 0.019), (27, 0.072), (30, 0.035), (35, 0.014), (39, 0.023), (45, 0.173), (50, 0.067), (52, 0.027), (60, 0.033), (77, 0.032), (90, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73500383 <a title="179-lda-1" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>Author: Atsushi Miyamae, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi</p><p>Abstract: In this paper, we propose an efﬁcient algorithm for estimating the natural policy gradient using parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods. 1</p><p>2 0.54263198 <a title="179-lda-2" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>3 0.54142773 <a title="179-lda-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.53898865 <a title="179-lda-4" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>Author: Matthew Hoffman, Francis R. Bach, David M. Blei</p><p>Abstract: We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by ﬁtting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA ﬁnds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</p><p>5 0.53818542 <a title="179-lda-5" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>6 0.53694898 <a title="179-lda-6" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>7 0.5364415 <a title="179-lda-7" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>8 0.53559029 <a title="179-lda-8" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>9 0.53508222 <a title="179-lda-9" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>10 0.53479552 <a title="179-lda-10" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<p>11 0.5345059 <a title="179-lda-11" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>12 0.5344094 <a title="179-lda-12" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>13 0.53433442 <a title="179-lda-13" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>14 0.53397381 <a title="179-lda-14" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>15 0.53382444 <a title="179-lda-15" href="./nips-2010-Multi-Stage_Dantzig_Selector.html">172 nips-2010-Multi-Stage Dantzig Selector</a></p>
<p>16 0.53279573 <a title="179-lda-16" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>17 0.53264314 <a title="179-lda-17" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>18 0.53201687 <a title="179-lda-18" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>19 0.53186274 <a title="179-lda-19" href="./nips-2010-Random_Conic_Pursuit_for_Semidefinite_Programming.html">219 nips-2010-Random Conic Pursuit for Semidefinite Programming</a></p>
<p>20 0.53184199 <a title="179-lda-20" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
