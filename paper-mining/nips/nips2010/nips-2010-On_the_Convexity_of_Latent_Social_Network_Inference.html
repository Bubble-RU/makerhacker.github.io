<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 nips-2010-On the Convexity of Latent Social Network Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-190" href="#">nips2010-190</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 nips-2010-On the Convexity of Latent Social Network Inference</h1>
<br/><p>Source: <a title="nips-2010-190-pdf" href="http://papers.nips.cc/paper/4113-on-the-convexity-of-latent-social-network-inference.pdf">pdf</a></p><p>Author: Seth Myers, Jure Leskovec</p><p>Abstract: In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1 -like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.</p><p>Reference: <a title="nips-2010-190-reference" href="../nips2010_reference/nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In many real-world scenarios, it is nearly impossible to collect explicit social network data. [sent-5, score-0.353]
</p><p>2 Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. [sent-7, score-0.699]
</p><p>3 We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. [sent-8, score-0.985]
</p><p>4 Given such node infection times, we then identify the optimal network that best explains the observed data. [sent-9, score-0.664]
</p><p>5 Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. [sent-11, score-0.385]
</p><p>6 As collecting such data is tedious and expensive, traditional social network studies typically involved a very limited number of people (usually less than 100). [sent-14, score-0.379]
</p><p>7 The emergence of large scale social computing applications has made massive social network data [16] available, but there are important settings where network data is hard to obtain and thus the whole network must thus be inferred from the data. [sent-15, score-0.934]
</p><p>8 Collecting social networks of such populations is near impossible, and thus whole networks have to be inferred from the observational data. [sent-17, score-0.409]
</p><p>9 Even though inferring social networks has been attempted in the past, it usually assumes that the pairwise interaction data is already available [5]. [sent-18, score-0.358]
</p><p>10 In this case, the problem of network inference reduces to deciding whether to include the interaction between a pair of nodes as an edge in the underlying network. [sent-19, score-0.369]
</p><p>11 For example, inferring networks from pairwise interactions of cell-phone call [5] or email [4, 13] records simply reduces down to selecting the right threshold τ such that an edge (u, v) is included in the network if u and v interacted more than τ times in the dataset. [sent-20, score-0.521]
</p><p>12 We address the problem of inferring the structure of unobserved social networks in a much more ambitious setting. [sent-22, score-0.39]
</p><p>13 , disease, information, product adoption) spreads over the edges of the network, and all that we observe are the infection times of nodes, but not who infected whom i. [sent-25, score-1.01]
</p><p>14 The goal then is to reconstruct the underlying social network along the edges of which the contagion diffused. [sent-28, score-0.565]
</p><p>15 1  We think of a diffusion on a network as a process where neighboring nodes switch states from inactive to active. [sent-29, score-0.341]
</p><p>16 Commonly, we only observe the times when particular nodes get “infected” but we do not observe who infected them. [sent-31, score-0.641]
</p><p>17 Thus, we only observe the time when a blog gets “infected” but not where it got infected from. [sent-33, score-0.535]
</p><p>18 Similarly, in disease spreading, we observe people getting sick without usually knowing who infected them [26]. [sent-34, score-0.568]
</p><p>19 Thus, the question is, if we assume that the network is static over time, is it possible to reconstruct the unobserved social network over which diffusions took place? [sent-36, score-0.618]
</p><p>20 We develop convex programming based approach for inferring the latent social networks from diffusion data. [sent-38, score-0.491]
</p><p>21 We then write down the likelihood of observed diffusion data under a given network and diffusion model parameters. [sent-40, score-0.39]
</p><p>22 Experiments reveal that we can near-perfectly recover the underlying network structure as well as the parameters of the propagation model. [sent-43, score-0.259]
</p><p>23 Our work here is similar in a sense that we “regress” the infection times of a target node on infection times of other nodes. [sent-50, score-0.888]
</p><p>24 The algorithm proposed (called NetInf) assumes that the weights of the edges in latent network are homogeneous, i. [sent-53, score-0.325]
</p><p>25 all connected nodes in the network infect/inﬂuence their neighbors with the same probability. [sent-55, score-0.26]
</p><p>26 2 Problem Formulation and the Proposed Method We now deﬁne the problem of inferring a latent social networks based on network diffusion data, where we only observe identities of infected nodes. [sent-58, score-1.153]
</p><p>27 Thus, for each node we know the interval during which the node was infected, whereas the source of each node’s infection is unknown. [sent-59, score-0.659]
</p><p>28 We assume only that an infected node was previously infected by some other previously infected node to which it is connected in the latent social network (which we are trying to infer). [sent-60, score-2.197]
</p><p>29 We show that calculating the maximum likelihood estimator (MLE) of the latent network (under any of the above diffusion models) is equivalent to a convex problem that can be efﬁciently solved. [sent-62, score-0.342]
</p><p>30 Each entry (i, j) of A models the conditional probability of infection transmission: Aij = P (node i infects node j | node i is infected). [sent-67, score-0.7]
</p><p>31 2  The temporal properties of most types of cascades, especially disease spread, are governed by a transmission (or incubation) period. [sent-68, score-0.264]
</p><p>32 The transmission time model w(t) speciﬁes how long it takes for the infection to transmit from one node to another, and the recovery model r(t) models the time of how long a node is infected before it recovers. [sent-69, score-1.385]
</p><p>33 Thus, whenever some node i, which was infected at time τi , infects another node j, the time separating two infection times is sampled from w(t), i. [sent-70, score-1.218]
</p><p>34 , infection time of node j is τj = τi + t, where t is distributed by w(t). [sent-72, score-0.497]
</p><p>35 Similarly, the duration of each node’s infection is sampled from r(t). [sent-73, score-0.335]
</p><p>36 A cascade c is initiated by randomly selecting a node to become infected at time t = 0. [sent-75, score-0.777]
</p><p>37 Speciﬁcally, if i becomes infected and j is susceptible, then j will become infected with probability Aij . [sent-78, score-0.98]
</p><p>38 Once it has been determined which of i’s neighbors will be infected, the infection time of each newly infected neighbor will be the sum of τi and an interval of time sampled from w(t). [sent-79, score-0.825]
</p><p>39 The transmission time for each new infection is sampled independently from w(t). [sent-80, score-0.571]
</p><p>40 In the SIS model, node i will become susceptible to infection again at time τi + ri . [sent-82, score-0.551]
</p><p>41 On the other hand, under the SIR model, node i will recover and can never be infected again. [sent-83, score-0.652]
</p><p>42 Our work here mainly considers the SI model, where nodes remain infected forever, i. [sent-84, score-0.565]
</p><p>43 For each cascade c, we then observe the node infection times τic as well as the duration of infection, but the source of each node’s infection remains hidden. [sent-88, score-1.009]
</p><p>44 The goal then is to, based on observed set of cascade infection times D, infer the weighted adjacency matrix A, where Aij models the edge transmission probability. [sent-89, score-0.867]
</p><p>45 For each cascade c, let τic be the time of infection for node i. [sent-92, score-0.622]
</p><p>46 Note that if node i did not get infected in cascade c, then τic = ∞. [sent-93, score-0.777]
</p><p>47 Also, let Xc (t) denote the set of all nodes that are in an infected state at time t in cascade c. [sent-94, score-0.69]
</p><p>48 We know the infection of each node was the result of an unknown, previously infected node to which it is connected, so the component of the likelihood function for each infection will be dependent on all previously infected nodes. [sent-95, score-1.999]
</p><p>49 Speciﬁcally, the likelihood function for a ﬁxed given A is     c∈D  = c∈D    P (i infected at τic |Xc (τic )) ·     L(A; D) =    c i;τi <∞    c i;τi <∞  1 −  c i;τi =∞  P (i never infected|Xc (t) ∀ t)     j;τj ≤τi  c (1 − w(τic − τj )Aji ) ·     (1 − Aji ) . [sent-96, score-0.515]
</p><p>50 First, for every node i that got infected at time τic we compute the probability that at least one other previously infected node could have infected it. [sent-99, score-1.815]
</p><p>51 For every non-infected node, we compute probability that no other node ever infected it. [sent-100, score-0.652]
</p><p>52 Moreover, in the case of the SIS model each node can be infected multiple times during a single cascade, so there will be multiple observed values for each τic and the likelihood function would have to include each infection time in the product sum. [sent-102, score-1.04]
</p><p>53 We can, however, break this problem into N independent subproblems, each with only N − 1 variables by observing that the incoming edges to a node can be inferred independently of the incoming edges of any other node. [sent-107, score-0.556]
</p><p>54 3  Let node i be the current node of interest for which we would like to infer its incoming connections. [sent-109, score-0.388]
</p><p>55 Lastly, the number of variables can further be reduced by observing that if node j is never infected in the same cascade as node i, then the MLE of Aji = 0, and Aji can thus be excluded from the set of variables. [sent-111, score-0.939]
</p><p>56 This dramatically reduces the number of variables as in practice the true A does not induce large cascades, causing the cascades to be sparse in the number of nodes they infect [14, 17]. [sent-112, score-0.245]
</p><p>57 In general, social networks are sparse in a sense that on average nodes are connected to a constant number rather than a constant fraction of other nodes in the network. [sent-129, score-0.435]
</p><p>58 We break the network inference down into a series of subproblems corresponding to the inference of the inbound edges of each node. [sent-136, score-0.356]
</p><p>59 The presence of the l1 penalty function makes the method extremely effective at predicting the presence of edges in the network, but it has the effect of distorting the estimated edge transmission probabilities. [sent-138, score-0.481]
</p><p>60 Of the resulting solution, the edge transmission probabilities that have been set zero are then restricted to remain at zero, and the problem is then relaxed with the sparsity parameter set to ρ = 0. [sent-140, score-0.36]
</p><p>61 This preserves the precision and recall of the edge location prediction of the algorithm while still generating accurate edge transmission probability predictions. [sent-141, score-0.545]
</p><p>62 Moreover, with the implementation described above, most 1000 node networks can be inferred inside of 10 minutes, running on a laptop. [sent-142, score-0.304]
</p><p>63 3 Experiments In this section, we evaluate our network inference method, which we will refer to as ConNIe (Convex Network Inference) on a range of datasets and network topologies. [sent-146, score-0.334]
</p><p>64 This includes both synthetically generated networks as well as real social networks, and both simulated and real diffusion data. [sent-147, score-0.423]
</p><p>65 In each o e case, the networks were constructed as unweighted graphs, and then each edge (i, j) was assigned a uniformly random transmission probability Aij between 0. [sent-153, score-0.423]
</p><p>66 In all of our experiments, we assume that the model w(t) of transmission times is known. [sent-156, score-0.264]
</p><p>67 We generate cascades by ﬁrst selecting a random starting node of the infection. [sent-162, score-0.305]
</p><p>68 From there, the infection is propagated to other nodes until no new infections occur: an infected node i transmits the infection to uninfected j with probability Aij , and if transmission occurs then the propagation time t is sampled according to the distribution w(t). [sent-163, score-1.692]
</p><p>69 (d)-(f): Mean square error of the edge transmission probability of the two algorithms. [sent-203, score-0.342]
</p><p>70 Not to make the problem too easy we generate enough cascades so that 99% of all edges of the network transmitted at least one infection. [sent-206, score-0.419]
</p><p>71 To assess the performance of ConNIe, we consider both the accuracy of the edge prediction, as well as the accuracy of edge transmission probability. [sent-210, score-0.448]
</p><p>72 For large values of ρ inferred networks have high precision but low recall, while for low values of ρ the precision will be poor but the recall will be high. [sent-213, score-0.294]
</p><p>73 To assess the accuracy of the estimated edge transmission probabilities Aij , we compute the meansquare error (MSE). [sent-214, score-0.36]
</p><p>74 The MSE is taken over the union of potential edge positions (node pairs) where there is an edge in the latent network, and the edge positions in which the algorithm has predicted the presence of an edge. [sent-215, score-0.35]
</p><p>75 NetInf ﬁrst reconstructs the most likely structure of each cascade, and then based on this reconstruction, it selects the next most likely edge of the social network. [sent-219, score-0.309]
</p><p>76 To apply this algorithm to the problem we are considering, we simply ﬁrst use the NetInf to infer the network structure and then estimate the edge transmission probabilities Aij by simply counting the fraction of times it was predicted that a cascade propagated along the edge (i, j). [sent-223, score-0.84]
</p><p>77 Figure 1 shows the precision-recall curves for the scale-free synthetic network with the three transmission models w(t). [sent-224, score-0.429]
</p><p>78 Also in Figure 1 we plot the Mean Squared Error of the estimates of the edge transmission 6  1  0. [sent-229, score-0.342]
</p><p>79 (e) PR Break-even point versus the perturbation size applied to the infection times. [sent-257, score-0.335]
</p><p>80 The green vertical line indicates the point where the inferred network contains the same number of edges as the real network. [sent-259, score-0.355]
</p><p>81 This, of course, is expected as NetInf assumes the network edge weights are homogeneous, which is not the case. [sent-262, score-0.29]
</p><p>82 Figure 2 shows the accuracy (Precision-Recall break-even point as well as edge MSE) as a function of the number of observed diffusions, as well as the effect of noise in the infection times. [sent-264, score-0.441]
</p><p>83 Noise was added to the cascades by adding independent normally distribution perturbations to each of the observed infection times, and the noise to signal ratio was calculated as the average perturbation over the average infection transmission time. [sent-265, score-1.066]
</p><p>84 Second, we experiment on a real email social network of 593 nodes and 2824 edges that is based on the email communication in a small European research institute. [sent-272, score-0.685]
</p><p>85 For the edges in the collaboration network we simply randomly assigned their edge transmission probabilities. [sent-273, score-0.655]
</p><p>86 For the email network we generated cascades using the power-law transmission time model, while for the collaboration network we used the Weibull distribution for sampling transmission times. [sent-281, score-1.051]
</p><p>87 Moreover, the edge transmission probability estimation error is less than 0. [sent-285, score-0.342]
</p><p>88 This is ideal: our method is capable of near perfect recovery of the underlying social network over which a relatively small number of contagions diffused. [sent-287, score-0.415]
</p><p>89 8  Recommendation network  Figure 3: The precision-recall curve of the network estimation and the mean-square error (left) of predicted transmission probabilities as a function of number edges being predicted (middle). [sent-323, score-0.718]
</p><p>90 (Right) Precision-recall curve on inferring a real recommendation network based on real product recommendation data. [sent-325, score-0.396]
</p><p>91 People generate cascades as follows: a node (person) v buys product p at time t, and then recommends it to nodes {w1 , . [sent-328, score-0.38]
</p><p>92 We consider a recommendation network of 275 users and 1522 edges and a set of 5,767 recommendations on 625 different products between a set of these users. [sent-334, score-0.342]
</p><p>93 Since the edge transmission model is unknown we model it with a power-law distribution with parameter α = 2. [sent-335, score-0.342]
</p><p>94 Our approach is able to recover the underlying social network surprisingly accurately. [sent-337, score-0.374]
</p><p>95 Since there are no ground truth edge transmission probabilities for us to compare against, we can not compute the error of edge weight estimation. [sent-342, score-0.466]
</p><p>96 4 Conclusion We have presented a general solution to the problem of inferring latent social networks from the network diffusion data. [sent-343, score-0.639]
</p><p>97 We evaluated our algorithm on a wide set of synthetic and real-world networks with several different cascade propagation models. [sent-346, score-0.264]
</p><p>98 Experiments reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the edge transmission model. [sent-348, score-0.569]
</p><p>99 By inferring and modeling the structure of such latent social networks, we can gain insight into positions and roles various nodes play in the diffusion process and assess the range of inﬂuence of nodes in the network. [sent-352, score-0.558]
</p><p>100 Recovering time-varying networks of dependencies in social and biological studies. [sent-358, score-0.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('infected', 0.49), ('netinf', 0.341), ('infection', 0.335), ('connie', 0.3), ('transmission', 0.236), ('social', 0.186), ('network', 0.167), ('node', 0.162), ('bji', 0.156), ('aji', 0.144), ('cascades', 0.143), ('cascade', 0.125), ('mse', 0.121), ('edges', 0.109), ('edge', 0.106), ('diffusion', 0.099), ('ic', 0.092), ('contagion', 0.082), ('networks', 0.081), ('aij', 0.078), ('nodes', 0.075), ('inferring', 0.074), ('weibull', 0.068), ('leskovec', 0.066), ('diffusions', 0.066), ('email', 0.065), ('inferred', 0.061), ('break', 0.061), ('tc', 0.059), ('mle', 0.058), ('pr', 0.055), ('precision', 0.055), ('susceptible', 0.054), ('pl', 0.052), ('xc', 0.051), ('recommendation', 0.049), ('recall', 0.042), ('contagions', 0.041), ('infects', 0.041), ('rumor', 0.041), ('sis', 0.041), ('viral', 0.041), ('wb', 0.041), ('collaboration', 0.037), ('infer', 0.037), ('marketing', 0.036), ('propagation', 0.032), ('latent', 0.032), ('unobserved', 0.032), ('pnas', 0.031), ('penalty', 0.03), ('person', 0.03), ('disease', 0.028), ('times', 0.028), ('incoming', 0.027), ('epidemic', 0.027), ('infect', 0.027), ('infections', 0.027), ('jure', 0.027), ('outbreak', 0.027), ('sars', 0.027), ('wj', 0.027), ('people', 0.026), ('synthetic', 0.026), ('likelihood', 0.025), ('observe', 0.024), ('spreads', 0.024), ('convexify', 0.024), ('sir', 0.024), ('reveal', 0.022), ('emails', 0.022), ('adoption', 0.022), ('mina', 0.022), ('drosophila', 0.022), ('si', 0.022), ('underlying', 0.021), ('curve', 0.021), ('www', 0.021), ('men', 0.021), ('synthetically', 0.021), ('got', 0.021), ('spread', 0.02), ('erd', 0.02), ('convex', 0.019), ('exp', 0.019), ('subproblems', 0.019), ('connected', 0.018), ('million', 0.018), ('probabilities', 0.018), ('link', 0.018), ('real', 0.018), ('matter', 0.018), ('perturbations', 0.017), ('mij', 0.017), ('nyi', 0.017), ('li', 0.017), ('assumes', 0.017), ('structure', 0.017), ('products', 0.017), ('moreover', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="190-tfidf-1" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>Author: Seth Myers, Jure Leskovec</p><p>Abstract: In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1 -like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.</p><p>2 0.11440789 <a title="190-tfidf-2" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>3 0.10471314 <a title="190-tfidf-3" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>4 0.099909388 <a title="190-tfidf-4" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>Author: David Weiss, Benjamin Sapp, Ben Taskar</p><p>Abstract: For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but ﬁlter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the ﬁltering loss of the ensemble as a theoretical justiﬁcation of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We ﬁnd that our approach signiﬁcantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem. 1</p><p>5 0.082320273 <a title="190-tfidf-5" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>Author: Zoubin Ghahramani, Michael I. Jordan, Ryan P. Adams</p><p>Abstract: Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a ﬂexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are inﬁnitely exchangeable. One can view our model as providing inﬁnite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data. 1</p><p>6 0.078916751 <a title="190-tfidf-6" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>7 0.076805636 <a title="190-tfidf-7" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>8 0.072867706 <a title="190-tfidf-8" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>9 0.072813116 <a title="190-tfidf-9" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>10 0.056654286 <a title="190-tfidf-10" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>11 0.054451264 <a title="190-tfidf-11" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>12 0.054207187 <a title="190-tfidf-12" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>13 0.052065343 <a title="190-tfidf-13" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>14 0.050364055 <a title="190-tfidf-14" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>15 0.050315093 <a title="190-tfidf-15" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>16 0.050104063 <a title="190-tfidf-16" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>17 0.048216157 <a title="190-tfidf-17" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>18 0.047081836 <a title="190-tfidf-18" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>19 0.045116875 <a title="190-tfidf-19" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>20 0.044481426 <a title="190-tfidf-20" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.047), (2, -0.025), (3, 0.057), (4, -0.083), (5, -0.055), (6, -0.026), (7, 0.003), (8, 0.013), (9, 0.033), (10, -0.131), (11, 0.018), (12, 0.008), (13, 0.07), (14, -0.035), (15, -0.053), (16, 0.067), (17, -0.012), (18, -0.182), (19, 0.133), (20, -0.014), (21, 0.022), (22, -0.063), (23, -0.0), (24, -0.005), (25, 0.074), (26, -0.006), (27, 0.03), (28, -0.015), (29, -0.052), (30, -0.027), (31, 0.051), (32, 0.09), (33, -0.03), (34, -0.032), (35, -0.102), (36, -0.025), (37, 0.01), (38, -0.031), (39, -0.003), (40, -0.097), (41, 0.011), (42, 0.032), (43, 0.005), (44, 0.029), (45, 0.021), (46, 0.038), (47, -0.005), (48, 0.029), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92482597 <a title="190-lsi-1" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>Author: Seth Myers, Jure Leskovec</p><p>Abstract: In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1 -like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.</p><p>2 0.69443679 <a title="190-lsi-2" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>3 0.59656233 <a title="190-lsi-3" href="./nips-2010-Penalized_Principal_Component_Regression_on_Graphs_for_Analysis_of_Subnetworks.html">204 nips-2010-Penalized Principal Component Regression on Graphs for Analysis of Subnetworks</a></p>
<p>Author: Ali Shojaie, George Michailidis</p><p>Abstract: Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a ﬂexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology. 1</p><p>4 0.55356836 <a title="190-lsi-4" href="./nips-2010-Subgraph_Detection_Using_Eigenvector_L1_Norms.html">259 nips-2010-Subgraph Detection Using Eigenvector L1 Norms</a></p>
<p>Author: Benjamin Miller, Nadya Bliss, Patrick J. Wolfe</p><p>Abstract: When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a “detection theory” for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets conﬁrms the efﬁcacy of this approach. 1</p><p>5 0.54549867 <a title="190-lsi-5" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>6 0.52969807 <a title="190-lsi-6" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>7 0.52529258 <a title="190-lsi-7" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>8 0.52323407 <a title="190-lsi-8" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>9 0.49178389 <a title="190-lsi-9" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>10 0.49090663 <a title="190-lsi-10" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>11 0.47588587 <a title="190-lsi-11" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>12 0.43126687 <a title="190-lsi-12" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>13 0.4274613 <a title="190-lsi-13" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>14 0.4223316 <a title="190-lsi-14" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>15 0.42093533 <a title="190-lsi-15" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>16 0.41325745 <a title="190-lsi-16" href="./nips-2010-A_Bayesian_Framework_for_Figure-Ground_Interpretation.html">3 nips-2010-A Bayesian Framework for Figure-Ground Interpretation</a></p>
<p>17 0.412956 <a title="190-lsi-17" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>18 0.40098435 <a title="190-lsi-18" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>19 0.4008849 <a title="190-lsi-19" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>20 0.39262009 <a title="190-lsi-20" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.067), (27, 0.058), (30, 0.04), (35, 0.018), (45, 0.168), (50, 0.097), (52, 0.028), (60, 0.029), (62, 0.285), (72, 0.01), (77, 0.059), (79, 0.013), (90, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74706978 <a title="190-lda-1" href="./nips-2010-On_the_Convexity_of_Latent_Social_Network_Inference.html">190 nips-2010-On the Convexity of Latent Social Network Inference</a></p>
<p>Author: Seth Myers, Jure Leskovec</p><p>Abstract: In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1 -like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks of thousands of nodes in a matter of minutes.</p><p>2 0.66307944 <a title="190-lda-2" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>Author: Li-jia Li, Hao Su, Li Fei-fei, Eric P. Xing</p><p>Abstract: Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classiﬁcation; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale-invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classiﬁers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efﬁcient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p><p>3 0.60535514 <a title="190-lda-3" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>4 0.59781152 <a title="190-lda-4" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>5 0.596398 <a title="190-lda-5" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>Author: James Sharpnack, Aarti Singh</p><p>Abstract: We consider the problem of identifying an activation pattern in a complex, largescale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of highdimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size. 1</p><p>6 0.59514606 <a title="190-lda-6" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>7 0.59310359 <a title="190-lda-7" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>8 0.59274697 <a title="190-lda-8" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>9 0.59234786 <a title="190-lda-9" href="./nips-2010-Learning_Multiple_Tasks_with_a_Sparse_Matrix-Normal_Penalty.html">147 nips-2010-Learning Multiple Tasks with a Sparse Matrix-Normal Penalty</a></p>
<p>10 0.59087515 <a title="190-lda-10" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>11 0.58928776 <a title="190-lda-11" href="./nips-2010-An_Inverse_Power_Method_for_Nonlinear_Eigenproblems_with_Applications_in_1-Spectral_Clustering_and_Sparse_PCA.html">30 nips-2010-An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA</a></p>
<p>12 0.58872294 <a title="190-lda-12" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>13 0.58863115 <a title="190-lda-13" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>14 0.58812249 <a title="190-lda-14" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>15 0.58668166 <a title="190-lda-15" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>16 0.5862093 <a title="190-lda-16" href="./nips-2010-Probabilistic_Multi-Task_Feature_Selection.html">217 nips-2010-Probabilistic Multi-Task Feature Selection</a></p>
<p>17 0.58520585 <a title="190-lda-17" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>18 0.5840975 <a title="190-lda-18" href="./nips-2010-Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models.html">242 nips-2010-Slice sampling covariance hyperparameters of latent Gaussian models</a></p>
<p>19 0.58403403 <a title="190-lda-19" href="./nips-2010-Boosting_Classifier_Cascades.html">42 nips-2010-Boosting Classifier Cascades</a></p>
<p>20 0.58375013 <a title="190-lda-20" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
