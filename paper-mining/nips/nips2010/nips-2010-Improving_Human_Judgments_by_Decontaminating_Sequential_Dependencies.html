<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-121" href="#">nips2010-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</h1>
<br/><p>Source: <a title="nips-2010-121-pdf" href="http://papers.nips.cc/paper/4162-improving-human-judgments-by-decontaminating-sequential-dependencies.pdf">pdf</a></p><p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>Reference: <a title="nips-2010-121-reference" href="../nips2010_reference/nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 of Psychological and Brain Sciences, Indiana University  Abstract For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. [sent-9, score-0.177]
</p><p>2 When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. [sent-10, score-0.296]
</p><p>3 This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. [sent-11, score-0.345]
</p><p>4 We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. [sent-13, score-0.216]
</p><p>5 In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. [sent-14, score-0.318]
</p><p>6 We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. [sent-15, score-0.383]
</p><p>7 Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. [sent-16, score-0.386]
</p><p>8 Our decontamination techniques yield an over 20% reduction in the error of human judgments. [sent-17, score-0.296]
</p><p>9 1  Introduction  Suppose you are asked to make a series of moral judgments by rating, on a 1–10 scale, various actions, with a rating of 1 indicating ’not particularly bad or wrong’ and a rating of 10 indicating ’extremely evil. [sent-18, score-0.377]
</p><p>10 Even though individuals are asked to make absolute judgments, the mean rating of statement (3) in the ﬁrst context is reliably higher than the mean rating of the identical statement (3 ) in the second context (Parducci, 1968). [sent-21, score-0.417]
</p><p>11 The classic explanation of this phenomenon is cast in terms of anchoring or primacy: information presented early in time serves as a basis for making judgments later in time (Tversky & Kahneman, 1974). [sent-22, score-0.149]
</p><p>12 In the Netﬂix contest, signiﬁcant attention was paid to anchoring effects by considering that an individual who gives high ratings early in a session is likely to be biased toward higher ratings later in a session (Koren, August 2009; Ellenberg, March 2008). [sent-23, score-0.301]
</p><p>13 The need for anchors comes from the fact that individuals are poor at or incapable of making absolute judgments and instead must rely on reference points to make relative judgments (e. [sent-24, score-0.446]
</p><p>14 There is a rich literature in experimental and theoretical psychology exploring sequential 1  dependencies suggesting that reference points change from one trial to the next in a systematic manner. [sent-28, score-0.302]
</p><p>15 (We use the psychological jargon ‘trial’ to refer to a single judgment or rating in a series. [sent-29, score-0.301]
</p><p>16 However, the most carefully controlled laboratory studies of sequential dependencies, dating back to the the 1950’s (discussed by Miller, 1956), involve the rating of unidimensional stimuli, such as the loudness of a tone or the length of a line. [sent-35, score-0.217]
</p><p>17 Human performance at rating stimuli is surprisingly poor compared to an individual’s ability to discriminate the same stimuli. [sent-36, score-0.21]
</p><p>18 Regardless of the domain, responses convey not much more than 2 bits of mutual information with the stimulus (Stewart et al. [sent-37, score-0.153]
</p><p>19 Different types of judgment tasks have been studied including absolute identiﬁcation, in which the individual’s task is to specify the distinct stimulus level (e. [sent-39, score-0.237]
</p><p>20 , 10 levels of loudness), magnitude estimation, in which the task is to estimate the magnitude of a stimulus which may vary continuously along a dimension, and categorization which is a hybrid task requiring individuals to label stimuli by range. [sent-41, score-0.31]
</p><p>21 Because the number of responses in absolute identiﬁcation and categorization tasks is often quite large, and because individuals are often not aware of the discreteness of stimuli in absolute identiﬁcation tasks, there isn’t a qualitative difference among tasks. [sent-42, score-0.379]
</p><p>22 Without feedback, there are no explicit anchors against which stimuli can be assessed. [sent-44, score-0.144]
</p><p>23 Typically, experimental trial t, trial t − 1 has a large inﬂuence on ratings, and trials t − 2, t − 3, etc. [sent-46, score-0.208]
</p><p>24 The inﬂuence of recent trials is exerted by both the stimuli and responses, a fact which makes sense in light of the assumption that individuals form their response on the current trial by analogy to recent trials (i. [sent-48, score-0.428]
</p><p>25 , they determine a response to the current stimulus that has the same relationship as the previous response had to the previous stimulus). [sent-50, score-0.261]
</p><p>26 Both assimilation and contrast effects occur: an assimilative response on trial t occurs when the response moves in the direction of the stimulus or response on trial t − k; a contrastive response is one that moves away. [sent-51, score-0.755]
</p><p>27 Interpreting recency effects in terms of assimilation and contrast is nontrivial and theory dependent (DeCarlo & Cross, 1990). [sent-52, score-0.157]
</p><p>28 Many mathematical models have been developed to explain the phenomena of sequential effects in judgment tasks. [sent-53, score-0.255]
</p><p>29 All adopt the assumption that the transduction of a stimulus to its internal representation is veridical. [sent-54, score-0.122]
</p><p>30 ) Sequential dependencies and other corruptions of the representation occur in the mapping of the sensation to a response. [sent-57, score-0.262]
</p><p>31 Other theories assume that multiple sensation-response anchors are required, one ﬁxed and unchanging and another varying from trial to trial (e. [sent-62, score-0.29]
</p><p>32 And in categorization and absolute identiﬁcation tasks, some theories posit anchors for each distinct response, which are adjusted trial-to-trial (e. [sent-65, score-0.183]
</p><p>33 Range-frequency theory (Parducci, 1965) claims that sequential effects arise because the sensation-response mapping is adjusted to utilize the full response range, and to produce roughly an equal number of responses of each type. [sent-68, score-0.282]
</p><p>34 Because recent history interacts with the current stimulus to determine an individual’s response, responses have a complex relationship with the underlying sensation, and do not provide as much information about the internal state of the individual as one would hope. [sent-70, score-0.188]
</p><p>35 In contrast, our approach to extracting more information from human judgments is to develop automatic techniques that recover the underlying sensation from a response that has been contaminated  2  by cognitive processes producing the response. [sent-72, score-0.437]
</p><p>36 2  Experiments  To collect ground-truth data for use in the design of decontamination techniques, we conducted two behavioral experiments using stimuli whose magnitudes could be objectively determined. [sent-77, score-0.339]
</p><p>37 In both experiments, participants were asked to judge the horizontal gap between two vertically aligned dots on a computer monitor. [sent-78, score-0.195]
</p><p>38 Participants were asked to respond to each dot pair using a 10-point rating scale, with 1 corresponding to the smallest gap they would see, and 10 corresponding to the largest. [sent-80, score-0.2]
</p><p>39 They were not told that only 10 unique stimuli were presented, and were likely unaware of this fact (memory of exact absolute gaps is too poor), and thus the task is indistinguishable from a magnitude estimation or categorization task in which the gap varied continuously. [sent-83, score-0.27]
</p><p>40 During the practice block, participants were shown every one of the ten gaps in random order, and simultaneous with the stimulus they were told—via text on the screen below the dots—the correct classiﬁcation. [sent-85, score-0.231]
</p><p>41 Although the psychology literature is replete with line-length judgment studies (two recent examples: Lacouture, 1997; Petrov & Anderson, 2005), the vast majority provide feedback to participants on at least some trials beyond the practice block. [sent-87, score-0.307]
</p><p>42 Within a block, the trial sequence was arranged such that each gap was preceded exactly once by each other gap, with the exception that no repetitions occurred. [sent-94, score-0.139]
</p><p>43 The main reason for conducting Experiment 2 was that we found the gaps used in Experiment 1 resulted in low error rates and few sequential effects for the smaller gaps. [sent-108, score-0.209]
</p><p>44 Two participants in Experiment 1 and one participant in Experiment 2 were excluded from data analysis because their accuracy was below 20%. [sent-112, score-0.127]
</p><p>45 3  error as a function of S(t−1) and S(t)  error as a function of stimulus difference 1  error as a function of lagged stimulus −0. [sent-122, score-0.246]
</p><p>46 The variation along the abscissa reﬂects sequential dependencies: assimilation is indicated by pairs of points with positive slopes (larger values of St−1 result in larger Rt ), and contrast is indicated by negative slopes. [sent-150, score-0.161]
</p><p>47 The middle column shows another depiction of sequential dependencies by characterizing the distribution of errors (Rt − St ∈ {> 1, 1, 0, −1, < −1}) as a function of St − St−1 . [sent-152, score-0.12]
</p><p>48 The predominance of assimilative responses is reﬂected in more Rt > St responses when St − St−1 < 0, and vice-versa. [sent-153, score-0.156]
</p><p>49 The rightmost column presents the lag proﬁle that characterizes how the stimulus on trial t − k for k = 1. [sent-154, score-0.202]
</p><p>50 For the purpose of the current work, most relevant is that sequential dependencies in this task may stretch back two or three trials. [sent-159, score-0.12]
</p><p>51 3  Approaches To Decontamination  From a machine learning perspective, decontamination can be formulated in at least three different ways. [sent-160, score-0.247]
</p><p>52 First, it could be considered an unsupervised infomax problem of determining a sensation associated with each distinct stimulus such that the sensation sequence has high mutual information with the response sequence. [sent-161, score-0.608]
</p><p>53 Third, decontamination models could be built based on ground-truth data for one group of individuals and then tested on another group. [sent-164, score-0.371]
</p><p>54 Formally, the decontamination problem involves inferring the sequence of (unobserved) sensations p given the complete response sequence. [sent-166, score-0.44]
</p><p>55 To introduce some notation, let Rt1 ,t2 denote the sequence of responses made by participant p on trials t1 through t2 when shown a sequence of stimuli that 4  p evoke the sensation sequence St1 ,t2 . [sent-167, score-0.443]
</p><p>56 Although psychological theories of human judgment address an altogether different problem—that p p p of predicting Rt , the response on trial t, given S1,t and R1,t−1 —they can inspire decontamination techniques. [sent-169, score-0.69]
</p><p>57 Two classes of psychological theories correspond to two distinct function approximation techniques. [sent-170, score-0.136]
</p><p>58 In contrast, other models favor highly ﬂexible, nonlinear approaches that allow for similarity-based assimilation and contrast, and independent representations for each response label (e. [sent-172, score-0.17]
</p><p>59 Given the discrete stimuli and responses, a lookup table seems the most general characterization of these models. [sent-175, score-0.226]
</p><p>60 The ﬁrst dimension of this space is the model class: regression, lookup table, or an additive hybrid. [sent-177, score-0.134]
</p><p>61 Similarly, we deﬁne our lookup table LUTt (m, n) to produce an estimate of St by indexing over the m responses Rt−m+1,t and the n sensations St−n,t−1 . [sent-179, score-0.306]
</p><p>62 Finally, we deﬁne an additive hybrid, REG⊕LUT(m, n) by ﬁrst constructing a regression model, and then building a lookup table on the residual error, St − REGt (m, n). [sent-180, score-0.164]
</p><p>63 The motivation for the hybrid is the complementarity of the two models, the regression model capturing linear regularities and the lookup table representing arbitrary nonlinear relationships. [sent-181, score-0.164]
</p><p>64 The second dimension in our space of decontamination techniques speciﬁes how inference is handled. [sent-182, score-0.247]
</p><p>65 To utilize any of the models above for n > 0, sensations St−n,t−1 must be estimated. [sent-184, score-0.127]
</p><p>66 As an alternative to the conditional random ﬁeld (hereafter, CRF), we also consider a simple approach in which we simply set n = 0 and discard the sensation terms in our regression and lookup tables. [sent-187, score-0.381]
</p><p>67 At the other extreme, we can assume an oracle that provides St−n,t−1 ; this oracle approach offers an upper bound on achievable performance. [sent-188, score-0.208]
</p><p>68 The difference is minor because the stimulus and sensation are in one-to-one correspondence. [sent-201, score-0.304]
</p><p>69 (Remember that lookup table values are indexed by St−1 , and therefore cannot be folded into the normalization constant. [sent-207, score-0.134]
</p><p>70 Having now described a 3 × 3 space of decontamination approaches, we turn to the details of our decontamination experiments. [sent-209, score-0.494]
</p><p>71 1  Debiasing and Decompressing  Although our focus is on decontaminating sequential dependencies, or desequencing, the quality of human judgments can be reduced by at least three other factors. [sent-211, score-0.232]
</p><p>72 Second, individuals may show compression, possibly nonlinear, of the response range. [sent-213, score-0.19]
</p><p>73 For example, compression will be a natural consequence of assimilation because the endpoints of the response scale will move toward the center. [sent-216, score-0.19]
</p><p>74 In the data from our two experiments, we found no evidence of drift, as determined by the fact that regression models with moving averages of the responses did not improve predictions. [sent-218, score-0.117]
</p><p>75 For example, in Experiment 1, the shortest stimuli reported as G1 and G2 with high accuracy, but the longest stimuli tended to be underestimated by all participants. [sent-222, score-0.184]
</p><p>76 The LUT(1, 0) compensates for this compression by associating responses G8 and G9 with higher sensation levels if the table entries are ﬁlled based on the training data according to: LUTt (1, 0) ≡ E[St |Rt ]. [sent-223, score-0.324]
</p><p>77 All of the higher order lookup tables, LUT (m, n), for m ≥ 1 and n ≥ 0, will also perform nonlinear decompression in the same manner. [sent-224, score-0.216]
</p><p>78 To debias the data, p ¯ we compute the mean response of a particular participant p, Rp ≡ 1/T Rt , and ensure the means p p ¯ p = St − S p . [sent-227, score-0.186]
</p><p>79 Assuming that the mean sensation is ¯ are homogeneous via the constraint Rt − R identical for all participants—as it should be in our experiments—debiasing can be incorporated p ¯ into the lookup tables by storing not E[St |Rt . [sent-228, score-0.374]
</p><p>80 ], and recovering the p ¯ sensation for a particular individual using LUT(m, n) − R . [sent-234, score-0.217]
</p><p>81 (This trick is necessary to index into the lookup table with discrete response levels. [sent-235, score-0.221]
</p><p>82 Note that this extra term—whether in the lookup table retrieval or the regression— ¯ results in additional features involving combinations of Rp and St , St−1 , and LUT(m, n) being added to the three CRF models. [sent-238, score-0.134]
</p><p>83 The SIMPLE - REG⊕LUT and ORACLE - REG⊕LUT models are trained ﬁrst by obtaining the regression coefﬁcients, and then ﬁlling lookup table entries p with the expected residual, E[St − REGp |Rt , Rt−1 , . [sent-251, score-0.185]
</p><p>84 99 1 sensation reconstruction error (RMSE)  Figure 2: Results from Experiment 1 (left column) and Experiment 2 (right column). [sent-278, score-0.241]
</p><p>85 The lookup tables used in the CRF - LUT and CRF - REG⊕LUT are the same as those in the ORACLE - LUT and ORACLE - REG⊕LUT models. [sent-282, score-0.157]
</p><p>86 We tested models in which the sensation and/or response values are log transformed, because sensory transduction introduces logarithmic compression. [sent-289, score-0.325]
</p><p>87 4  Results  Figure 2 shows the root mean squared error (RMSE) between the ground-truth sensation and the model-estimated sensation over the set of validation subjects for 100 different splits of the data. [sent-300, score-0.458]
</p><p>88 The difference between each pair of these results is highly reliable, indicating that bias, compression, and recency effects all contribute to the contamination of human judgments. [sent-303, score-0.12]
</p><p>89 7  The reduction of error due to debiasing is 14. [sent-304, score-0.13]
</p><p>90 Indeed models like CRF - REG⊕LUT perform nearly as well even without separate debiasing and decompression corrections. [sent-314, score-0.209]
</p><p>91 The joint model REG⊕LUT that exploits both the regularity of the regression model and the ﬂexibility of the lookup table clearly works better than either REG or LUT in isolation. [sent-316, score-0.164]
</p><p>92 We do not have a good explanation for the advantage of SIMPLE - LUT over CRF - LUT in Experiment 1, although there are some minor differences in how the lookup tables for the two models are constructed, and we are investigating whether those differences might be responsible. [sent-318, score-0.178]
</p><p>93 5  Discussion  Psychologists have long been struck by the relativity of human judgments and have noted that relativity limits how well individuals can communicate their internal sensations, impressions, and evaluations via rating scales. [sent-320, score-0.555]
</p><p>94 We’ve shown that decontamination techniques can improve the quality of judgments, reducing error by over 20% Is a 20% reduction signiﬁcant? [sent-321, score-0.271]
</p><p>95 Using the models we developed for this study, we can obtain a decontamination of the ratings and identify pairs of paintings where the participant’s ratings conﬂict with the decontaminated impressions. [sent-327, score-0.43]
</p><p>96 Via a later session in which we ask participants for pairwise preferences, we can determine whether the decontaminator or the raw ratings are more reliable. [sent-328, score-0.19]
</p><p>97 Indeed, it seems that if even responses to simple visual stimuli are contaminated, responses to more complex stimuli with a more complex judgment task will be even more vulnerable. [sent-330, score-0.421]
</p><p>98 One such hint is the ﬁnding that systematic effects of sequences have been observed on response latencies in judgment tasks (Lacouture, 1997); therefore, latencies may prove useful for decontamination. [sent-336, score-0.246]
</p><p>99 Bow, range, and sequential effects in absolute identiﬁcation: A response-time analysis. [sent-373, score-0.174]
</p><p>100 The dynamics of scaling: A memory-based anchor model of category rating and identiﬁcation. [sent-414, score-0.118]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lut', 0.554), ('reg', 0.363), ('crf', 0.258), ('decontamination', 0.247), ('st', 0.225), ('sensation', 0.217), ('lookup', 0.134), ('rating', 0.118), ('judgments', 0.108), ('rt', 0.107), ('debiasing', 0.106), ('sensations', 0.106), ('judgment', 0.105), ('oracle', 0.104), ('individuals', 0.103), ('stimuli', 0.092), ('trial', 0.09), ('stimulus', 0.087), ('participants', 0.087), ('response', 0.087), ('decompression', 0.082), ('lutt', 0.082), ('parducci', 0.082), ('ratings', 0.081), ('psychological', 0.078), ('sequential', 0.075), ('relativity', 0.071), ('responses', 0.066), ('ix', 0.063), ('psychology', 0.062), ('assimilation', 0.062), ('debias', 0.059), ('theories', 0.058), ('effects', 0.054), ('anchors', 0.052), ('gap', 0.049), ('experiment', 0.049), ('decarlo', 0.047), ('decompress', 0.047), ('stewart', 0.047), ('net', 0.047), ('dependencies', 0.045), ('absolute', 0.045), ('petrov', 0.041), ('anchoring', 0.041), ('recency', 0.041), ('compression', 0.041), ('participant', 0.04), ('mccallum', 0.037), ('decontaminate', 0.035), ('desequencing', 0.035), ('ellenberg', 0.035), ('lacouture', 0.035), ('mumma', 0.035), ('regt', 0.035), ('internal', 0.035), ('gaps', 0.034), ('rmse', 0.034), ('asked', 0.033), ('anderson', 0.031), ('regression', 0.03), ('reference', 0.03), ('brains', 0.029), ('categorization', 0.028), ('bar', 0.028), ('trials', 0.028), ('dots', 0.026), ('lag', 0.025), ('feedback', 0.025), ('human', 0.025), ('rp', 0.025), ('wilson', 0.024), ('error', 0.024), ('block', 0.024), ('abscissa', 0.024), ('assimilative', 0.024), ('barking', 0.024), ('chater', 0.024), ('decompressing', 0.024), ('decontaminating', 0.024), ('einhorn', 0.024), ('furnham', 0.024), ('hogarth', 0.024), ('impressions', 0.024), ('laming', 0.024), ('loudness', 0.024), ('outsmart', 0.024), ('poisoning', 0.024), ('portal', 0.024), ('psychologist', 0.024), ('struck', 0.024), ('wedell', 0.024), ('screen', 0.023), ('tables', 0.023), ('told', 0.022), ('conducting', 0.022), ('session', 0.022), ('models', 0.021), ('identi', 0.021), ('drift', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="121-tfidf-1" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>2 0.13967353 <a title="121-tfidf-2" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>3 0.097974822 <a title="121-tfidf-3" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>4 0.095662802 <a title="121-tfidf-4" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>Author: Han Liu, Kathryn Roeder, Larry Wasserman</p><p>Abstract: A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include K-fold cross-validation (K-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size diverges with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including K-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all these competing procedures.</p><p>5 0.072686948 <a title="121-tfidf-5" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>Author: Nadia Payet, Sinisa Todorovic</p><p>Abstract: We combine random forest (RF) and conditional random ﬁeld (CRF) into a new computational framework, called random forest random ﬁeld (RF)2 . Inference of (RF)2 uses the Swendsen-Wang cut algorithm, characterized by MetropolisHastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a nonparametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)2 . (RF)2 is applied to a challenging task of multiclass object recognition and segmentation over a random ﬁeld of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</p><p>6 0.063543409 <a title="121-tfidf-6" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>7 0.057633512 <a title="121-tfidf-7" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>8 0.055836115 <a title="121-tfidf-8" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>9 0.05084433 <a title="121-tfidf-9" href="./nips-2010-Online_Markov_Decision_Processes_under_Bandit_Feedback.html">196 nips-2010-Online Markov Decision Processes under Bandit Feedback</a></p>
<p>10 0.050605789 <a title="121-tfidf-10" href="./nips-2010-Interval_Estimation_for_Reinforcement-Learning_Algorithms_in_Continuous-State_Domains.html">130 nips-2010-Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains</a></p>
<p>11 0.050318923 <a title="121-tfidf-11" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>12 0.05005626 <a title="121-tfidf-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.047753971 <a title="121-tfidf-13" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>14 0.046719566 <a title="121-tfidf-14" href="./nips-2010-Natural_Policy_Gradient_Methods_with_Parameter-based_Exploration_for_Control_Tasks.html">179 nips-2010-Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks</a></p>
<p>15 0.04583424 <a title="121-tfidf-15" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>16 0.045233257 <a title="121-tfidf-16" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>17 0.044520933 <a title="121-tfidf-17" href="./nips-2010-Collaborative_Filtering_in_a_Non-Uniform_World%3A_Learning_with_the_Weighted_Trace_Norm.html">48 nips-2010-Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm</a></p>
<p>18 0.042575538 <a title="121-tfidf-18" href="./nips-2010-A_unified_model_of_short-range_and_long-range_motion_perception.html">20 nips-2010-A unified model of short-range and long-range motion perception</a></p>
<p>19 0.042420346 <a title="121-tfidf-19" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>20 0.041736186 <a title="121-tfidf-20" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, -0.008), (2, -0.05), (3, 0.032), (4, -0.017), (5, -0.003), (6, -0.045), (7, 0.006), (8, 0.008), (9, 0.033), (10, -0.04), (11, -0.034), (12, 0.017), (13, 0.004), (14, 0.058), (15, 0.012), (16, -0.044), (17, -0.038), (18, -0.039), (19, 0.101), (20, -0.092), (21, 0.026), (22, 0.1), (23, -0.012), (24, -0.007), (25, 0.036), (26, -0.033), (27, 0.04), (28, -0.047), (29, 0.052), (30, -0.06), (31, -0.11), (32, -0.162), (33, -0.028), (34, -0.078), (35, 0.117), (36, 0.016), (37, -0.07), (38, -0.007), (39, -0.04), (40, 0.119), (41, -0.003), (42, 0.005), (43, 0.066), (44, -0.014), (45, 0.047), (46, 0.041), (47, 0.043), (48, -0.001), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93945271 <a title="121-lsi-1" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>2 0.59427053 <a title="121-lsi-2" href="./nips-2010-Evidence-Specific_Structures_for_Rich_Tractable_CRFs.html">83 nips-2010-Evidence-Specific Structures for Rich Tractable CRFs</a></p>
<p>Author: Anton Chechetka, Carlos Guestrin</p><p>Abstract: We present a simple and effective approach to learning tractable conditional random ﬁelds with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efﬁcient exact inference and arbitrarily accurate parameter learning in polynomial time. At the same time, our algorithm does not suffer a large expressive power penalty inherent to ﬁxed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup. 1</p><p>3 0.52971172 <a title="121-lsi-3" href="./nips-2010-Efficient_Relational_Learning_with_Hidden_Variable_Detection.html">71 nips-2010-Efficient Relational Learning with Hidden Variable Detection</a></p>
<p>Author: Ni Lao, Jun Zhu, Liu Xinwang, Yandong Liu, William W. Cohen</p><p>Abstract: Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this ﬂexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efﬁcient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efﬁciency; and on the other hand, the CVI algorithm efﬁciently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efﬁcient yet does not sacriﬁce its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is signiﬁcantly more efﬁcient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.</p><p>4 0.51620233 <a title="121-lsi-4" href="./nips-2010-Double_Q-learning.html">66 nips-2010-Double Q-learning</a></p>
<p>Author: Hado V. Hasselt</p><p>Abstract: In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation. 1</p><p>5 0.48545155 <a title="121-lsi-5" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>Author: Pradeep Shenoy, Angela J. Yu, Rajesh P. Rao</p><p>Abstract: Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems. 1</p><p>6 0.44703826 <a title="121-lsi-6" href="./nips-2010-Improving_the_Asymptotic_Performance_of_Markov_Chain_Monte-Carlo_by_Inserting_Vortices.html">122 nips-2010-Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices</a></p>
<p>7 0.41324154 <a title="121-lsi-7" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>8 0.40792194 <a title="121-lsi-8" href="./nips-2010-Feature_Transitions_with_Saccadic_Search%3A_Size%2C_Color%2C_and_Orientation_Are_Not_Alike.html">95 nips-2010-Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike</a></p>
<p>9 0.40748093 <a title="121-lsi-9" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>10 0.39875859 <a title="121-lsi-10" href="./nips-2010-Stability_Approach_to_Regularization_Selection_%28StARS%29_for_High_Dimensional_Graphical_Models.html">254 nips-2010-Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models</a></p>
<p>11 0.394283 <a title="121-lsi-11" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>12 0.38429946 <a title="121-lsi-12" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>13 0.37460852 <a title="121-lsi-13" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>14 0.35589072 <a title="121-lsi-14" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>15 0.34758961 <a title="121-lsi-15" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>16 0.34435356 <a title="121-lsi-16" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>17 0.34131759 <a title="121-lsi-17" href="./nips-2010-Movement_extraction_by_detecting_dynamics_switches_and_repetitions.html">171 nips-2010-Movement extraction by detecting dynamics switches and repetitions</a></p>
<p>18 0.33690327 <a title="121-lsi-18" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>19 0.33240134 <a title="121-lsi-19" href="./nips-2010-Link_Discovery_using_Graph_Feature_Tracking.html">162 nips-2010-Link Discovery using Graph Feature Tracking</a></p>
<p>20 0.32853475 <a title="121-lsi-20" href="./nips-2010-Parametric_Bandits%3A_The_Generalized_Linear_Case.html">203 nips-2010-Parametric Bandits: The Generalized Linear Case</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.026), (27, 0.562), (30, 0.032), (35, 0.017), (44, 0.017), (45, 0.117), (50, 0.032), (52, 0.027), (60, 0.026), (77, 0.023), (90, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96025431 <a title="121-lda-1" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>2 0.93767655 <a title="121-lda-2" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>3 0.91753292 <a title="121-lda-3" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>same-paper 4 0.8885842 <a title="121-lda-4" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>5 0.85416579 <a title="121-lda-5" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>6 0.8329795 <a title="121-lda-6" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>7 0.76202422 <a title="121-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>8 0.72617078 <a title="121-lda-8" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>9 0.70263034 <a title="121-lda-9" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>10 0.69018716 <a title="121-lda-10" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>11 0.66654831 <a title="121-lda-11" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>12 0.66224831 <a title="121-lda-12" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>13 0.63708204 <a title="121-lda-13" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>14 0.62625301 <a title="121-lda-14" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>15 0.61917567 <a title="121-lda-15" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>16 0.61232114 <a title="121-lda-16" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>17 0.6109035 <a title="121-lda-17" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>18 0.60421848 <a title="121-lda-18" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>19 0.60208642 <a title="121-lda-19" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>20 0.59469539 <a title="121-lda-20" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
