<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>279 nips-2010-Universal Kernels on Non-Standard Input Spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-279" href="#">nips2010-279</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>279 nips-2010-Universal Kernels on Non-Standard Input Spaces</h1>
<br/><p>Source: <a title="nips-2010-279-pdf" href="http://papers.nips.cc/paper/4168-universal-kernels-on-non-standard-input-spaces.pdf">pdf</a></p><p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of Rd . Examples include SVMs for the analysis of histograms or colored images, SVMs for text classiÄ?Ĺš cation and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1</p><p>Reference: <a title="nips-2010-279-reference" href="../nips2010_reference/nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. [sent-11, score-1.214]
</p><p>2 So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . [sent-12, score-0.426]
</p><p>3 We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . [sent-13, score-1.706]
</p><p>4 We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. [sent-14, score-2.424]
</p><p>5 histograms, as input samples have been used to analyze histogram data such as colored images, see [5, 11, 14, 12, 27, 29], and also [17] for nonextensive information theoretic kernels on measures. [sent-23, score-0.534]
</p><p>6 Ĺš cation and web mining [15, 12, 16], Ă˘&euro;Ë&tilde; SVMs with kernels from computational biology, e. [sent-25, score-0.426]
</p><p>7 Interestingly, in this analysis, the kernel and its reproducing kernel Hilbert space (RKHS) make it possible to completely decouple the statistical analysis of SVMs from the input space X. [sent-32, score-0.516]
</p><p>8 For example, if one uses the hinge loss and a bounded measurable kernel whose RKHS H is separable and dense in L1 (Ă&sbquo;Äž) for all distributions Ă&sbquo;Äž on X, then [31, Theorem 7. [sent-33, score-0.409]
</p><p>9 In other words, independently of the input space X, the universal consistency of SVMs is well-understood modulo an approximation theoretical question, namely that of the denseness of H in all L1 (Ă&sbquo;Äž). [sent-37, score-0.537]
</p><p>10 For example, for compact X Ă˘&Scaron;&sbquo; Rd , [30] showed that, among a few others, the RKHSs of the Gaussian RBF kernels are universal, that is, they are dense in the space C(X) of continuous functions f : X Ă˘&dagger;&rsquo; R. [sent-39, score-0.879]
</p><p>11 14], it is then easy to conclude that these RKHS are also dense in all L1 (Ă&sbquo;Äž) for which Ă&sbquo;Äž has a compact support. [sent-43, score-0.31]
</p><p>12 This key result has been extended in a couple of different directions: For example, [18] establishes universality for more classes of kernels on compact X Ă˘&Scaron;&sbquo; Rd , whereas [32] shows the denseness of the Gaussian RKHSs in L1 (Ă&sbquo;Äž) for all distributions Ă&sbquo;Äž on Rd . [sent-44, score-0.959]
</p><p>13 Finally, [7, 8, 28, 29] show that universal kernels are closely related to so-called characteristic kernels that can be used to distinguish distributions. [sent-45, score-1.301]
</p><p>14 Ĺš cient or necessary conditions for universality of kernels on arbitrary compact metric spaces X, and [32] further shows that the compact metric spaces are exactly the compact topological spaces on which there exist universal spaces. [sent-47, score-2.301]
</p><p>15 Ĺš cient conditions for universality nor the proof of the existence of universal kernels can be used to construct universal kernels on compact metric spaces X Ă˘&Scaron;&sbquo; Rd . [sent-49, score-2.278]
</p><p>16 In fact, to the best of our knowledge, no explicit example of such kernels has so far been presented. [sent-50, score-0.465]
</p><p>17 Ĺš rst explicit and constructive examples of universal kernels that live on compact metric spaces X Ă˘&Scaron;&sbquo; Rd . [sent-53, score-1.375]
</p><p>18 Ĺš nition of the Gaussian RBF kernels, or more generally, kernels that can be expressed by a Taylor series, from the Euclidean Rd to its inÄ? [sent-56, score-0.426]
</p><p>19 Indeed, the closed balls of 2 are no longer (norm)-compact subsets of 2 and hence we cannot expect universality on these balls. [sent-60, score-0.32]
</p><p>20 To address this issue, one may be tempted to use the weakĂ˘&circ;&mdash; -topology on 2 , since in this topology the closed balls are both compact and metrizable, thus universal kernels do exist on them. [sent-61, score-1.158]
</p><p>21 However, the Taylor kernels do not belong to them, because Ă˘&euro;&ldquo;basicallyĂ˘&euro;&ldquo; the inner product Ă&sbquo;Ë&Dagger; , Ă&sbquo;Ë&Dagger; 2 fails to be continuous with respect to the weakĂ˘&circ;&mdash; -topology as the sequence of the standard orthonormal basis vectors show. [sent-62, score-0.563]
</p><p>22 Since the inner product of 2 is continuous with respect to the norm by virtue of the Cauchy-Schwarz inequality, it turns out that the Taylor kernels are continuous with respect to the norm topology. [sent-64, score-0.675]
</p><p>23 Moreover, we will see that in this situation the Stone-WeierstraÄ&sbquo;&Yuml;-argument of [30] yields a variety of universal kernels including the inÄ? [sent-65, score-0.833]
</p><p>24 Ĺš nite dimensional Euclidean spaces Rd and their compact subsets, the compact subsets of 2 can be hardly viewed as somewhat natural examples of input spaces X. [sent-68, score-0.816]
</p><p>25 Therefore, we go one step further by considering compact metric spaces X for which there exist a separable Hilbert space H and an injective and continuous map Ä&#x17D;  : X Ă˘&dagger;&rsquo; H. [sent-69, score-0.948]
</p><p>26 Ĺš nes a universal kernel on X and the same is true for the analogous deÄ? [sent-73, score-0.588]
</p><p>27 Indeed, we will use this general result to present examples of Gaussian kernels deÄ? [sent-78, score-0.457]
</p><p>28 Section 2 contains the main results and constructs examples for universal kernels based on our technique. [sent-81, score-0.83]
</p><p>29 In particular, we show how to construct universal kernels on sets of probability measures and on sets of functions, the latter being interesting for signal processing. [sent-82, score-0.864]
</p><p>30 Equivai,j=1 Ă&lsaquo;&oelig; Ă&lsaquo;&oelig; Ă&lsaquo;&oelig; lently, k is a kernel if and only there exists a Hilbert space H and a map Ă&#x17D;Ĺ&scaron; : X Ă˘&dagger;&rsquo; H such Ă&lsaquo;&oelig; Ă&lsaquo;&oelig; ) Ă&lsaquo;&oelig; for all x, x Ă˘&circ;&circ; X. [sent-89, score-0.312]
</p><p>31 Moreover, for a compact metric space (X, d), we write C(X) := {f : X Ă˘&dagger;&rsquo; R | f continuous} for the space of continuous functions on X and equip this space with the usual supremum norm Ă&sbquo;Ë&Dagger; Ă˘&circ;&#x17E; . [sent-95, score-0.765]
</p><p>32 A kernel k on X is called universal, if k is continuous and its RKHS H is dense in C(X). [sent-96, score-0.318]
</p><p>33 The kernels we consider in this paper are constructed by functions K : [Ă˘&circ;&rsquo;r, r] Ă˘&dagger;&rsquo; R that can be expressed by its Taylor series, that is Ă˘&circ;&#x17E;  an tn ,  K(t) =  t Ă˘&circ;&circ; [Ă˘&circ;&rsquo;r, r] . [sent-99, score-0.452]
</p><p>34 Ĺš nes a kernel on the closed ball rBRd := {x Ă˘&circ;&circ; Rd : x 2 Ă˘&permil;Â¤ r} with radius r, whenever all Taylor coefÄ? [sent-102, score-0.287]
</p><p>35 57], showed that Taylor kernels are universal, if an > 0 for all n Ă˘&permil;Ä˝ 0, while [21] notes that strict positivity on certain subsets of indices n sufÄ? [sent-106, score-0.466]
</p><p>36 Ĺš nite dimensional and 2 separable counterpart 2 := {(wj )jĂ˘&permil;Ä˝1 : (wj ) 22 := jĂ˘&permil;Ä˝1 wj < Ă˘&circ;&#x17E;}. [sent-110, score-0.215]
</p><p>37 Ĺš rst main result shows that this extension leads to a kernel, whose restrictions to compact subsets are universal, if an > 0 for all n Ă˘&circ;&circ; N0 := N Ă˘&circ;Ĺ&#x17E; {0}. [sent-113, score-0.278]
</p><p>38 (3)  n=0  ii) If an > 0 for all n Ă˘&circ;&circ; N0 , then the restriction k|W Ä&sbquo;&mdash;W : W Ä&sbquo;&mdash; W Ă˘&dagger;&rsquo; R of k to an arbitrary Ă˘&circ;&scaron; compact set W Ă˘&Scaron;&sbquo; rB 2 is universal. [sent-117, score-0.238]
</p><p>39 1 for all r > 0, and hence the resulting exponential kernel is universal on every compact subset W of 2 . [sent-122, score-0.783]
</p><p>40 Ĺš rst goal, namely explicit, constructive examples of universal kernels on X Ă˘&Scaron;&sbquo; Rd , the result is so far not really satisfying. [sent-127, score-0.83]
</p><p>41 2 Let X be a compact metric space and H be a separable Hilbert space such that there exists a continuous and injective map Ä&#x17D;  : X Ă˘&dagger;&rsquo; H. [sent-133, score-0.924]
</p><p>42 ,  (5)  n=0  ii) If an > 0 for all n Ă˘&circ;&circ; N0 , then k is a universal kernel. [sent-137, score-0.373]
</p><p>43 iii) For Ä&#x17D;&fnof; > 0, the Gaussian-type RBF-kernel kÄ&#x17D;&fnof; : X Ä&sbquo;&mdash; X Ă˘&dagger;&rsquo; R is a universal kernel, where kÄ&#x17D;&fnof; (x, x ) := exp Ă˘&circ;&rsquo;Ä&#x17D;&fnof; 2 Ä&#x17D; (x) Ă˘&circ;&rsquo; Ä&#x17D; (x )  2 H  x, x Ă˘&circ;&circ; X. [sent-138, score-0.402]
</p><p>44 H d Indeed, [25] uses the fact that on R such kernels have an integral representation in terms of the Gaussian RBF kernels to show, see [25, Corollary 4. [sent-140, score-0.852]
</p><p>45 9], that these kernels inherit approximation properties such as universality from the Gaussian RBF kernels. [sent-141, score-0.625]
</p><p>46 Ĺš ne explicit universal kernels, we point to a technical detail of Theorem 2. [sent-146, score-0.412]
</p><p>47 To this end, let (X, dX ) be an arbitrary metric space, H be a separable Hilbert space and Ä&#x17D;  : X Ă˘&dagger;&rsquo; H be an injective map. [sent-148, score-0.434]
</p><p>48 We write V := Ä&#x17D; (X) and equip this space with the metric deÄ? [sent-149, score-0.275]
</p><p>49 Moreover, since H is assumed to be separable, it is isometrically isomorphic to 2 , and hence there exists an isometric isomorphism I : H Ă˘&dagger;&rsquo; 2 . [sent-153, score-0.234]
</p><p>50 We write W := I(V ) and equip this set with the metric deÄ? [sent-154, score-0.232]
</p><p>51 Let us now assume that we have a kernel kW on W with RKHS HW and canonical feature map Ă&#x17D;Ĺ&scaron;W : W Ă˘&dagger;&rsquo; HW . [sent-160, score-0.233]
</p><p>52 Let us now assume that X is compact and that kW is one of the universal kernels considered in Theorem 2. [sent-166, score-1.037]
</p><p>53 2 shows that kX is one of the universal kernels considered in Theorem 2. [sent-169, score-0.799]
</p><p>54 Ĺš ned by kV (v, v ) := kW (I(v), I(v )), then an analogous argument shows that kV is a universal kernel. [sent-172, score-0.373]
</p><p>55 Ĺš ces to assume that Ä&#x17D;  is injective, continuous and has a compact image V . [sent-174, score-0.35]
</p><p>56 Then, in general, Ä&#x17D;  is not a homeomorphism and the sets of continuous functions on X and V are in general different, even if we consider the set of bounded continuous functions on X. [sent-181, score-0.365]
</p><p>57 Now this difference makes it impossible to conclude from the universality of kV (or kW ) to the universality of kX . [sent-186, score-0.436]
</p><p>58 Ĺš nes a metric that generates Ä&#x17D; Ă˘&circ;&rsquo;1 (Ä&#x17D;&bdquo;V ) and, since Ä&#x17D;  is isometric with respect to this new metric, we can conclude that (X, dÄ&#x17D;  ) is a compact metric space. [sent-192, score-0.671]
</p><p>59 2, and hence kX is universal with respect to the space C(X, dÄ&#x17D;  ) of functions X Ă˘&dagger;&rsquo; R that are continuous with respect to dÄ&#x17D;  . [sent-194, score-0.554]
</p><p>60 In other words, while HX may fail to approximate every function that is continuous with respect to dX , it does approximate every function that is continuous with respect to dÄ&#x17D;  . [sent-195, score-0.224]
</p><p>61 Let us now present some universal kernels of practical interest. [sent-198, score-0.799]
</p><p>62 Example 1: universal kernels on the set of probability measures. [sent-202, score-0.799]
</p><p>63 Let (Ă˘&bdquo;Ĺ&scaron;, dĂ˘&bdquo;Ĺ&scaron; ) be a compact metric space, B(Ă˘&bdquo;Ĺ&scaron;) be its Borel Ä&#x17D;&fnof;-algebra, and X := M1 (Ă˘&bdquo;Ĺ&scaron;) be the set of all Borel probability measures on Ă˘&bdquo;Ĺ&scaron;. [sent-203, score-0.416]
</p><p>64 Moreover, (X, dX ) is a compact metric space if and only if (Ă˘&bdquo;Ĺ&scaron;, dĂ˘&bdquo;Ĺ&scaron; ) is a compact metric space, see [19, Thm. [sent-211, score-0.799]
</p><p>65 In order to construct universal kernels on (X, dX ) with the help of Theorem 2. [sent-214, score-0.799]
</p><p>66 Ĺš nd separable Hilbert spaces H and injective, continuous embeddings Ä&#x17D;  : X Ă˘&dagger;&rsquo; H. [sent-216, score-0.403]
</p><p>67 Let kĂ˘&bdquo;Ĺ&scaron; be a continuous kernel on Ă˘&bdquo;Ĺ&scaron; with RKHS HĂ˘&bdquo;Ĺ&scaron; and canonical feature map Ă&#x17D;Ĺ&scaron;Ă˘&bdquo;Ĺ&scaron; (Ä&#x17D;&permil;) := kĂ˘&bdquo;Ĺ&scaron; (Ä&#x17D;&permil;, Ă&sbquo;Ë&Dagger;), Ä&#x17D;&permil; Ă˘&circ;&circ; Ă˘&bdquo;Ĺ&scaron;. [sent-217, score-0.345]
</p><p>68 3 Let (Ă˘&bdquo;Ĺ&scaron;, dĂ˘&bdquo;Ĺ&scaron; ) be a complete separable metric space, H be a separable Banach space and Ă&#x17D;Ĺ&scaron; : Ă˘&bdquo;Ĺ&scaron; Ă˘&dagger;&rsquo; H be a bounded, continuous function. [sent-234, score-0.579]
</p><p>69 Note that this kernel is conceptionally different to characteristic kernels on Ă˘&bdquo;Ĺ&scaron;. [sent-242, score-0.674]
</p><p>70 Indeed, characteristic kernels live on Ă˘&bdquo;Ĺ&scaron; and their RKHS consist of functions Ă˘&bdquo;Ĺ&scaron; Ă˘&dagger;&rsquo; R, while the new kernel kÄ&#x17D;&fnof; lives on M1 (Ă˘&bdquo;Ĺ&scaron;) and its RKHS consists of functions M1 (Ă˘&bdquo;Ĺ&scaron;) Ă˘&dagger;&rsquo; R. [sent-243, score-0.751]
</p><p>71 represented by histograms, densities or data, while characteristic kernels can only be used to check whether two of such distributions are equal or not. [sent-246, score-0.536]
</p><p>72 Example 2: universal kernels based on Fourier transforms of probability measures. [sent-247, score-0.799]
</p><p>73 Moreover, Ä&#x17D;  : P Ă˘&dagger;&rsquo; P is injective, and if a sequence (Pn ) converges weakly to some Ă&lsaquo;&dagger; P Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; P, then (Pn ) converges uniformly to P on every compact subset of Rd . [sent-254, score-0.238]
</p><p>74 2 ensures that the following Gaussian-type kernel is universal and bounded: Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; kÄ&#x17D;&fnof; (P, P ) := exp Ă˘&circ;&rsquo;Ä&#x17D;&fnof; 2 P Ă˘&circ;&rsquo; P  2 L2 (Ă&sbquo;Äž)  P, P Ă˘&circ;&circ; M1 (Ă˘&bdquo;Ĺ&scaron;). [sent-260, score-0.574]
</p><p>75 Our new kernels make it possible to directly plug the empirical distributions into the kernel kÄ&#x17D;&fnof; , even if these distributions do not have the same length. [sent-264, score-0.666]
</p><p>76 Moreover, other techniques to convert empirical distributions to absolutely continuous distributions such as kernel estimators derived via weighted averaging of rounded points (WAPRing) and (averaging) histograms with different origins, [20, 24] can be used in kÄ&#x17D;&fnof; , too. [sent-265, score-0.443]
</p><p>77 In addition, let us assume that our input values xi Ă˘&circ;&circ; X are functions taken from some compact set X Ă˘&Scaron;&sbquo; L2 (Ă&sbquo;Äž). [sent-274, score-0.295]
</p><p>78 This smoothing can often be described by a compact linear operator T : L2 ([0, 1]) Ă˘&dagger;&rsquo; L2 ([0, 1]), e. [sent-276, score-0.238]
</p><p>79 Hence, if we assume that the true signals are contained in the closed unit ball BL2 ([0,1]) , then the observed, smoothed signals T Ă˘&mdash;Ĺ&scaron; f are contained in a compact subset X of L2 ([0, 1]). [sent-279, score-0.31]
</p><p>80 2, and hence the Gaussian-type kernel kÄ&#x17D;&fnof; (g, g ) := exp Ă˘&circ;&rsquo;Ä&#x17D;&fnof; 2 g Ă˘&circ;&rsquo; g  2 L2 (Ă&sbquo;Äž)  ,  g, g Ă˘&circ;&circ; X,  (11)  deÄ? [sent-283, score-0.201]
</p><p>81 3  Discussion  The main goal of this paper was to provide an explicit construction of universal kernels that are deÄ? [sent-287, score-0.838]
</p><p>82 Ĺš ned on arbitrary compact metric spaces, which are not necessarily a subset of Rd . [sent-288, score-0.378]
</p><p>83 There is a still increasing interest in kernel methods including support vector machines on such input spaces, e. [sent-289, score-0.203]
</p><p>84 As examples, we gave explicit universal kernels on the set of probability distributions and for signal processing. [sent-293, score-0.899]
</p><p>85 One direction of further research may be to generalize our results to the case of non-compact metric spaces or to Ä? [sent-294, score-0.243]
</p><p>86 Then for all j Ă˘&circ;&circ; NN with |j| = n, there exists a constant 0 cj Ă˘&circ;&circ; (0, Ă˘&circ;&#x17E;) such that for all summable sequences (bi ) Ă˘&Scaron;&sbquo; [0, Ă˘&circ;&#x17E;) we have Ă˘&circ;&#x17E;  Ă˘&circ;&#x17E;  n  bi  =  i=1  bji . [sent-309, score-0.299]
</p><p>87 Moreover, 2 := 2 (N) is separable, and by using an orthonormal basis representation, it is further known that every separable Hilbert space is isometrically isomorphic to 2 . [sent-315, score-0.257]
</p><p>88 The following result provides a method to construct Taylor kernels on closed balls in  2. [sent-317, score-0.507]
</p><p>89 Ĺš ned by Ă˘&circ;&#x17E; Ă˘&circ;&scaron; j Ă&#x17D;Ĺ&scaron;(w) := cj wi i , w Ă˘&circ;&circ; rB 2 , i=1  jĂ˘&circ;&circ;J  Ă˘&circ;&scaron;  rB  2  Ă˘&dagger;&rsquo;  (12)  is a feature map of k, where we use the convention 00 := 1. [sent-329, score-0.248]
</p><p>90 2 then shows that, for all j Ă˘&circ;&circ; NN , there exists a constant cj Ă˘&circ;&circ; (0, Ă˘&circ;&#x17E;) such 0 that Ă˘&circ;&#x17E;  k(w, w ) =  i=1  jĂ˘&circ;&circ;NN 0  Setting cj := a kernel. [sent-334, score-0.312]
</p><p>91 (wi )ji  a|j| cj Ă&lsaquo;&oelig;  i=1  a|j| cj , we obtain that Ă&#x17D;Ĺ&scaron; deÄ? [sent-336, score-0.276]
</p><p>92 4 Let W be a compact metric space and k be a continuous kernel on W with k(w, w) > 0 for all w Ă˘&circ;&circ; W . [sent-341, score-0.705]
</p><p>93 Suppose that we have an injective feature map Ă&#x17D;Ĺ&scaron; : W Ă˘&dagger;&rsquo; 2 (J) of k, where J is some countable set. [sent-342, score-0.214]
</p><p>94 For the multi-index j Ă˘&circ;&circ; J that equals 1 at the i-component and vanishes everywhere else we then have Ă&#x17D;Ĺ&scaron;(w) = cj wi = cj wi = Ă&#x17D;Ĺ&scaron;(w ), and hence Ă&#x17D;Ĺ&scaron; is injective. [sent-366, score-0.374]
</p><p>95 2: Since H is separable Hilbert space there exists an isometric isomorphism I : H Ă˘&dagger;&rsquo; 2 . [sent-368, score-0.347]
</p><p>96 Since Ä&#x17D;  is continuous, V is the image of a compact set under a continuous map, and thus V is compact and the inverse of the bijective map I Ă˘&mdash;Ĺ&scaron; Ä&#x17D;  : X Ă˘&dagger;&rsquo; W is continuous. [sent-371, score-0.715]
</p><p>97 Consequently, there is a one-to-one relationship between the continuous functions fX on X and the continuous functions fW on W , namely C(X) = C(W ) Ă˘&mdash;Ĺ&scaron; I Ă˘&mdash;Ĺ&scaron; Ä&#x17D; , see also the discussion following (7). [sent-372, score-0.276]
</p><p>98 Moreover, the fact that I : H Ă˘&dagger;&rsquo; 2 is an isometric isomorphism yields I(Ä&#x17D; (x)), I(Ä&#x17D; (x )) 2 = Ä&#x17D; (x), Ä&#x17D; (x ) H for all x, x Ă˘&circ;&circ; X, and hence the kernel k considered in Theorem 2. [sent-373, score-0.298]
</p><p>99 Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. [sent-428, score-0.347]
</p><p>100 On the relation between universality, characteristic kernels and RKHS embeddings of measures. [sent-622, score-0.548]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernels', 0.426), ('universal', 0.373), ('hw', 0.249), ('compact', 0.238), ('universality', 0.199), ('kernel', 0.172), ('rkhs', 0.158), ('rb', 0.156), ('separable', 0.142), ('metric', 0.14), ('cj', 0.138), ('rd', 0.134), ('kw', 0.133), ('svms', 0.124), ('kx', 0.122), ('hilbert', 0.12), ('nn', 0.118), ('continuous', 0.112), ('rbf', 0.11), ('hx', 0.109), ('injective', 0.109), ('spaces', 0.103), ('fukumizu', 0.093), ('compactness', 0.083), ('taylor', 0.079), ('characteristic', 0.076), ('isometric', 0.072), ('ji', 0.069), ('bijective', 0.066), ('histograms', 0.066), ('theorem', 0.065), ('sriperumbudur', 0.062), ('denseness', 0.062), ('equip', 0.062), ('homeomorphism', 0.062), ('summable', 0.062), ('map', 0.061), ('kv', 0.059), ('hein', 0.056), ('steinwart', 0.056), ('reproducing', 0.055), ('isomorphism', 0.054), ('ep', 0.049), ('wi', 0.049), ('moreover', 0.049), ('consequently', 0.047), ('rkhss', 0.047), ('embeddings', 0.046), ('dx', 0.046), ('borel', 0.044), ('countable', 0.044), ('space', 0.043), ('nes', 0.043), ('sch', 0.042), ('bayreuth', 0.041), ('nonextensive', 0.041), ('rbrd', 0.041), ('stuttgart', 0.041), ('wj', 0.041), ('gretton', 0.041), ('balls', 0.041), ('subsets', 0.04), ('closed', 0.04), ('topology', 0.04), ('fourier', 0.039), ('explicit', 0.039), ('conclude', 0.038), ('measures', 0.038), ('lemma', 0.037), ('exists', 0.036), ('bji', 0.036), ('isometrically', 0.036), ('isomorphic', 0.036), ('colored', 0.036), ('pn', 0.035), ('distributions', 0.034), ('dense', 0.034), ('situation', 0.034), ('hush', 0.033), ('hilbertian', 0.033), ('dimensional', 0.032), ('ball', 0.032), ('examples', 0.031), ('input', 0.031), ('proposition', 0.031), ('write', 0.03), ('banach', 0.029), ('berlin', 0.029), ('exp', 0.029), ('modulo', 0.028), ('usual', 0.028), ('induction', 0.028), ('bi', 0.027), ('signal', 0.027), ('editors', 0.027), ('bounded', 0.027), ('functions', 0.026), ('inner', 0.025), ('live', 0.025), ('absolutely', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="279-tfidf-1" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of Rd . Examples include SVMs for the analysis of histograms or colored images, SVMs for text classiÄ?Ĺš cation and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1</p><p>2 0.29500949 <a title="279-tfidf-2" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>3 0.20818555 <a title="279-tfidf-3" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>Author: Prateek Jain, Brian Kulis, Inderjit S. Dhillon</p><p>Abstract: In this paper we consider the problem of semi-supervised kernel function learning. We ﬁrst propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. To this end, we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the k-NN classiﬁcation accuracy signiﬁcantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique signiﬁcantly reduces the dimensionality of the feature space while achieving competitive classiﬁcation accuracies. 1</p><p>4 0.20731407 <a title="279-tfidf-4" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>Author: Kun Gai, Guangyun Chen, Chang-shui Zhang</p><p>Abstract: In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of norm constraints on combination coefﬁcients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method signiﬁcantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches. 1</p><p>5 0.19297442 <a title="279-tfidf-5" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT [16] and HOG [3], are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a uniﬁed and principled framework to turn pixel attributes (gradient, color, local binary pattern, etc.) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA) [23]. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classiﬁcation benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p><p>6 0.16261967 <a title="279-tfidf-6" href="./nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">278 nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<p>7 0.16237916 <a title="279-tfidf-7" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>8 0.1368438 <a title="279-tfidf-8" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>9 0.11516701 <a title="279-tfidf-9" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>10 0.10306538 <a title="279-tfidf-10" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>11 0.10258681 <a title="279-tfidf-11" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>12 0.095176026 <a title="279-tfidf-12" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>13 0.091165416 <a title="279-tfidf-13" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>14 0.085142002 <a title="279-tfidf-14" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>15 0.08373642 <a title="279-tfidf-15" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>16 0.080051899 <a title="279-tfidf-16" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>17 0.076001197 <a title="279-tfidf-17" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>18 0.075201951 <a title="279-tfidf-18" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>19 0.073658846 <a title="279-tfidf-19" href="./nips-2010-Nonparametric_Density_Estimation_for_Stochastic_Optimization_with_an_Observable_State_Variable.html">185 nips-2010-Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable</a></p>
<p>20 0.072725341 <a title="279-tfidf-20" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.066), (2, 0.109), (3, -0.025), (4, 0.244), (5, 0.104), (6, 0.323), (7, -0.045), (8, 0.078), (9, 0.079), (10, -0.029), (11, -0.038), (12, -0.159), (13, -0.029), (14, -0.032), (15, 0.025), (16, 0.052), (17, 0.02), (18, -0.017), (19, -0.06), (20, -0.042), (21, -0.041), (22, -0.019), (23, 0.003), (24, 0.014), (25, 0.021), (26, 0.042), (27, -0.025), (28, -0.124), (29, -0.1), (30, 0.061), (31, -0.029), (32, -0.059), (33, 0.014), (34, 0.019), (35, -0.007), (36, -0.095), (37, 0.056), (38, 0.029), (39, 0.003), (40, -0.009), (41, 0.053), (42, -0.041), (43, 0.018), (44, 0.05), (45, 0.017), (46, 0.036), (47, -0.007), (48, -0.095), (49, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98143327 <a title="279-lsi-1" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of Rd . Examples include SVMs for the analysis of histograms or colored images, SVMs for text classiÄ?Ĺš cation and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1</p><p>2 0.87308133 <a title="279-lsi-2" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>Author: Ernesto D. Vito, Lorenzo Rosasco, Alessandro Toigo</p><p>Abstract: In this paper we consider the problem of learning from data the support of a probability distribution when the distribution does not have a density (with respect to some reference measure). We propose a new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space, which we call “completely regular”. Completely regular kernels allow to capture the relevant geometric and topological properties of an arbitrary probability space. In particular, they are the key ingredient to prove the universal consistency of the spectral estimators and in this respect they are the analogue of universal kernels for supervised problems. Numerical experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for density support estimation.</p><p>3 0.80449325 <a title="279-lsi-3" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>Author: Meihong Wang, Fei Sha, Michael I. Jordan</p><p>Abstract: We apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction. In this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses. We extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same. Our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data. Furthermore, when used in conjunction with supervised learners for classiﬁcation, our methods lead to lower classiﬁcation errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.</p><p>4 0.78150165 <a title="279-lsi-4" href="./nips-2010-Inductive_Regularized_Learning_of_Kernel_Functions.html">124 nips-2010-Inductive Regularized Learning of Kernel Functions</a></p>
<p>Author: Prateek Jain, Brian Kulis, Inderjit S. Dhillon</p><p>Abstract: In this paper we consider the problem of semi-supervised kernel function learning. We ﬁrst propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. To this end, we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the k-NN classiﬁcation accuracy signiﬁcantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique signiﬁcantly reduces the dimensionality of the feature space while achieving competitive classiﬁcation accuracies. 1</p><p>5 0.72179317 <a title="279-lsi-5" href="./nips-2010-Learning_Kernels_with_Radiuses_of_Minimum_Enclosing_Balls.html">145 nips-2010-Learning Kernels with Radiuses of Minimum Enclosing Balls</a></p>
<p>Author: Kun Gai, Guangyun Chen, Chang-shui Zhang</p><p>Abstract: In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classiﬁer. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2 ) of norm constraints on combination coefﬁcients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method signiﬁcantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches. 1</p><p>6 0.68277431 <a title="279-lsi-6" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>7 0.66031188 <a title="279-lsi-7" href="./nips-2010-Optimal_learning_rates_for_Kernel_Conjugate_Gradient_regression.html">199 nips-2010-Optimal learning rates for Kernel Conjugate Gradient regression</a></p>
<p>8 0.65694189 <a title="279-lsi-8" href="./nips-2010-Universal_Consistency_of_Multi-Class_Support_Vector_Classification.html">278 nips-2010-Universal Consistency of Multi-Class Support Vector Classification</a></p>
<p>9 0.60354728 <a title="279-lsi-9" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>10 0.55424356 <a title="279-lsi-10" href="./nips-2010-Multiple_Kernel_Learning_and_the_SMO_Algorithm.html">176 nips-2010-Multiple Kernel Learning and the SMO Algorithm</a></p>
<p>11 0.54797935 <a title="279-lsi-11" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>12 0.50201136 <a title="279-lsi-12" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>13 0.47093496 <a title="279-lsi-13" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>14 0.45663881 <a title="279-lsi-14" href="./nips-2010-Spatial_and_anatomical_regularization_of_SVM_for_brain_image_analysis.html">249 nips-2010-Spatial and anatomical regularization of SVM for brain image analysis</a></p>
<p>15 0.42975184 <a title="279-lsi-15" href="./nips-2010-Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">104 nips-2010-Generative Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>16 0.41405782 <a title="279-lsi-16" href="./nips-2010-Getting_lost_in_space%3A_Large_sample_analysis_of_the_resistance_distance.html">105 nips-2010-Getting lost in space: Large sample analysis of the resistance distance</a></p>
<p>17 0.38346395 <a title="279-lsi-17" href="./nips-2010-Heavy-Tailed_Process_Priors_for_Selective_Shrinkage.html">113 nips-2010-Heavy-Tailed Process Priors for Selective Shrinkage</a></p>
<p>18 0.3685948 <a title="279-lsi-18" href="./nips-2010-LSTD_with_Random_Projections.html">134 nips-2010-LSTD with Random Projections</a></p>
<p>19 0.36816701 <a title="279-lsi-19" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>20 0.35478371 <a title="279-lsi-20" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.028), (27, 0.031), (30, 0.06), (45, 0.139), (50, 0.039), (52, 0.455), (60, 0.064), (77, 0.036), (90, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8714326 <a title="279-lda-1" href="./nips-2010-Universal_Kernels_on_Non-Standard_Input_Spaces.html">279 nips-2010-Universal Kernels on Non-Standard Input Spaces</a></p>
<p>Author: Andreas Christmann, Ingo Steinwart</p><p>Abstract: During the last years support vector machines (SVMs) have been successfully applied in situations where the input space X is not necessarily a subset of Rd . Examples include SVMs for the analysis of histograms or colored images, SVMs for text classiÄ?Ĺš cation and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) H Ă˘&Scaron;&sbquo; Lp (PX ) is dense, or if the SVM uses a universal kernel k. So far, however, there are no kernels of practical interest known that satisfy these assumptions, if X Ă˘&Scaron;&sbquo; Rd . We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of Rd . We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing. 1</p><p>2 0.86994368 <a title="279-lda-2" href="./nips-2010-Robust_PCA_via_Outlier_Pursuit.html">231 nips-2010-Robust PCA via Outlier Pursuit</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Sujay Sanghavi</p><p>Abstract: Singular Value Decomposition (and Principal Component Analysis) is one of the most widely used techniques for dimensionality reduction: successful and efﬁciently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative ﬁltering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted. We present an efﬁcient convex optimization-based algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisﬁed, e.g., by the standard generative assumption in PCA problems) recovers the exact optimal low-dimensional subspace, and identiﬁes the corrupted points. Such identiﬁcation of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and ﬁnancial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct column space of the uncorrupted matrix, rather than the exact matrix itself.</p><p>3 0.85573941 <a title="279-lda-3" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>Author: Felipe Gerhard, Wulfram Gerstner</p><p>Abstract: Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-ﬁt using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural ﬁring rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-ﬁt tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models. 1</p><p>4 0.814915 <a title="279-lda-4" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: Divisive normalization (DN) has been advocated as an effective nonlinear efﬁcient coding transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture some important statistical properties of natural sensory signals. The multivariate t model justiﬁes DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate t model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm DN as an effective efﬁcient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small. 1</p><p>5 0.77823168 <a title="279-lda-5" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>6 0.54505163 <a title="279-lda-6" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>7 0.52458364 <a title="279-lda-7" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>8 0.5140208 <a title="279-lda-8" href="./nips-2010-Distributionally_Robust_Markov_Decision_Processes.html">64 nips-2010-Distributionally Robust Markov Decision Processes</a></p>
<p>9 0.51186639 <a title="279-lda-9" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>10 0.50896049 <a title="279-lda-10" href="./nips-2010-Fast_global_convergence_rates_of_gradient_methods_for_high-dimensional_statistical_recovery.html">92 nips-2010-Fast global convergence rates of gradient methods for high-dimensional statistical recovery</a></p>
<p>11 0.49913579 <a title="279-lda-11" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>12 0.49654314 <a title="279-lda-12" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>13 0.49389318 <a title="279-lda-13" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>14 0.49285331 <a title="279-lda-14" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>15 0.49215323 <a title="279-lda-15" href="./nips-2010-Spectral_Regularization_for_Support_Estimation.html">250 nips-2010-Spectral Regularization for Support Estimation</a></p>
<p>16 0.49178904 <a title="279-lda-16" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>17 0.49163336 <a title="279-lda-17" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>18 0.4883768 <a title="279-lda-18" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>19 0.48837146 <a title="279-lda-19" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>20 0.48438773 <a title="279-lda-20" href="./nips-2010-Relaxed_Clipping%3A_A_Global_Training_Method_for_Robust_Regression_and_Classification.html">225 nips-2010-Relaxed Clipping: A Global Training Method for Robust Regression and Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
