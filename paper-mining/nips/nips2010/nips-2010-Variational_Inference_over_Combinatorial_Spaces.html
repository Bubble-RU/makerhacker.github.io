<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>283 nips-2010-Variational Inference over Combinatorial Spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-283" href="#">nips2010-283</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>283 nips-2010-Variational Inference over Combinatorial Spaces</h1>
<br/><p>Source: <a title="nips-2010-283-pdf" href="http://papers.nips.cc/paper/4036-variational-inference-over-combinatorial-spaces.pdf">pdf</a></p><p>Author: Alexandre Bouchard-côté, Michael I. Jordan</p><p>Abstract: Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. 1</p><p>Reference: <a title="nips-2010-283-reference" href="../nips2010_reference/nips-2010-Variational_Inference_over_Combinatorial_Spaces_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Variational Inference over Combinatorial Spaces  ∗  Alexandre Bouchard-Cˆ t´ ∗ oe Michael I. [sent-1, score-0.074]
</p><p>2 Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. [sent-3, score-0.145]
</p><p>3 Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. [sent-4, score-0.43]
</p><p>4 We propose a new framework that extends variational inference to a wide range of combinatorial spaces. [sent-6, score-0.671]
</p><p>5 Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. [sent-7, score-0.144]
</p><p>6 Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. [sent-8, score-0.171]
</p><p>7 We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. [sent-9, score-0.524]
</p><p>8 This setup subsumes many probabilistic inference and classical combinatorics problems. [sent-11, score-0.176]
</p><p>9 It is often intractable to compute this sum, so approximations are used. [sent-12, score-0.08]
</p><p>10 Each Ci is larger than C, but paradoxically it is often possible to ﬁnd such a decomposition where for each i, 1 x∈Ci f (x) is tractable. [sent-14, score-0.041]
</p><p>11 This paper describes an effective way of using this type of decomposition to approximate the original sum. [sent-16, score-0.041]
</p><p>12 Another way of viewing this setup is in terms of exponential families. [sent-17, score-0.215]
</p><p>13 In this view, described in detail in Section 2, the decomposition becomes a factorization of the base measure. [sent-18, score-0.308]
</p><p>14 As we will show, the exponential family view gives a principled way of deﬁning variational approximations. [sent-19, score-0.423]
</p><p>15 In order to make variational approximations tractable in the combinatorial setup, we use what we call an implicit message representation. [sent-20, score-0.825]
</p><p>16 The canonical parameter space of the exponential family enables such representation. [sent-21, score-0.193]
</p><p>17 We also show how additional approximations can be introduced in cases where the factorization has a large number of factors. [sent-22, score-0.25]
</p><p>18 These further approximations rely on an outer bound of the partition function, and therefore preserve the guarantees of convex variational objective functions. [sent-23, score-0.392]
</p><p>19 While previous authors have proposed mean ﬁeld or loopy belief propagation algorithms to approximate the partition function of a few speciﬁc combinatorial models—for example [7, 8] for parsing, 1  The appendices can be found in the supplementary material. [sent-24, score-0.566]
</p><p>20 1  and [9, 10] for computing the permanent of a matrix—we are not aware of a general treatment of variational inference in combinatorial spaces. [sent-25, score-0.83]
</p><p>21 There has been work on applying variational algorithms to the problem of maximization over combinatorial spaces [11, 12, 13, 14], but maximization over combinatorial spaces is rather different than summation. [sent-26, score-1.107]
</p><p>22 For example, in the bipartite matching example considered in both [13] and this paper, there is a known polynomial algorithm for maximization, but not for summation. [sent-27, score-0.277]
</p><p>23 Our approach is also related to agreement-based learning [15, 16], although agreement-based learning is deﬁned within the context of unsupervised learning using EM, while our framework is agnostic with respect to parameter estimation. [sent-28, score-0.04]
</p><p>24 The paper is organized as follows: in Section 2 we present the measure factorization framework; in Section 3 we show examples of this framework applied to various combinatorial inference problems; and in Section 4 we present empirical results. [sent-29, score-0.633]
</p><p>25 2  Variational measure factorization  In this section, we present the variational measure factorization framework. [sent-30, score-0.731]
</p><p>26 At a high level, the ﬁrst step is to construct an equivalent but more convenient exponential family. [sent-31, score-0.099]
</p><p>27 This exponential family will allow us to transform variational algorithms over graphical models into approximation algorithms over combinatorial spaces. [sent-32, score-0.725]
</p><p>28 We ﬁrst describe the techniques needed to do this transformation in the case of a speciﬁc variational inference algorithm—loopy belief propagation—and then discuss mean-ﬁeld and tree-reweighted approximations. [sent-33, score-0.436]
</p><p>29 To make the exposition more concrete, we use the running example of approximating the value and gradient of the log-partition function of a Bipartite Matching model (BM) over KN,N , a well-known #P problem [17]. [sent-34, score-0.097]
</p><p>30 Unless we mention otherwise, we will consider bipartite perfect matchings; nonbipartite and non-perfect matchings are discussed in Section 3. [sent-35, score-0.374]
</p><p>31 The reader should keep in mind, however, that our framework is applicable to a much broader class of combinatorial objects. [sent-37, score-0.342]
</p><p>32 The link between this setup and the general problem of computing x∈C f (x) is the base measure ν, which is set to the indicator function over C: ν(x) = 1[x ∈ C], where 1[·] is equal to one if its argument holds true, and zero otherwise. [sent-42, score-0.274]
</p><p>33 The goal is to approximate A(θ) and A(θ) (recall that the j-th coordinate of the gradient, j A, is equal to the expectation of the sufﬁcient statistic φj under the exponential family with base measure ν [5]). [sent-43, score-0.423]
</p><p>34 We want to exploit situations where the base measure can be written as a product of I I measures ν(x) = i=1 νi (x) such that each factor νi : X → {0, 1} induces a super-partition function assumed to be tractable: Ai (θ) = log x∈X exp{ φ(x), θ }νi (x). [sent-44, score-0.158]
</p><p>35 In the case of BM, the space X is a product of N 2 binary alignment variables, x = x1,1 , x1,2 , . [sent-47, score-0.36]
</p><p>36 In the Standard Bipartite Matching formulation (which we denote by SBM), the sufﬁcient statistic takes the form φj (x) = xm,n . [sent-51, score-0.111]
</p><p>37 The measure factorization we use to enforce the matching property is ν = ν1 ν2 , where: N  ν1 (x) =  N  m=1  N  N  xm,n ≤ 1],  1[  ν2 (x) = n=1  n=1  2  xm,n ≤ 1]. [sent-52, score-0.304]
</p><p>38 1[ m=1  (2)  We start by constructing an equivalent but more convenient exponential family. [sent-53, score-0.099]
</p><p>39 < 25% identity short, 20% — 40% identity short, > 35% identity  0. [sent-54, score-0.174]
</p><p>40 89  Table 1: Average SP scores in the ref1/test1 directory of BAliBASE. [sent-74, score-0.043]
</p><p>41 BPMF-i denotes the average SP of the BPMF algorithm after i iterations of (parallel) message passing. [sent-75, score-0.091]
</p><p>42 The experimental setup is based on a generative model over noisy observations of bipartite perfect matchings described in Appendix C. [sent-78, score-0.49]
</p><p>43 We show in Figure 3(c) the results of a sequence of these experiments for different bipartite component sizes N/2. [sent-80, score-0.228]
</p><p>44 This experiments demonstrates the scalability of sophisticated factorizations, and their superiority over simpler ones. [sent-81, score-0.075]
</p><p>45 2  Multiple sequence alignment  To assess the practical signiﬁcance of this framework, we also apply it to BAliBASE [6], a standard protein multiple sequence alignment benchmark. [sent-83, score-0.976]
</p><p>46 12 [24], the most popular multiple alignment tool, and ProbCons 1. [sent-86, score-0.408]
</p><p>47 12, a state-of-the-art system [25] that also relies on enforcing transitivity constraints, but which is not derived via the optimization of an objective function. [sent-87, score-0.118]
</p><p>48 Our system uses a basic pair HMM [26] to score pairwise alignments. [sent-88, score-0.039]
</p><p>49 6 The advantage of our system over the other systems is the better optimization technique, based on the measure factorization described in Section 3. [sent-90, score-0.27]
</p><p>50 We used a standard technique to transform the pairwise alignment marginals into a single valid multiple sequence alignment (see Appendix C. [sent-92, score-0.834]
</p><p>51 Our system outperformed both baselines after three BPMF parallel message passing iterations. [sent-94, score-0.171]
</p><p>52 The algorithm converged in all protein groups, and performance was identical after more than three iterations. [sent-95, score-0.076]
</p><p>53 Although the overall performance gain is not statistically signiﬁcant according to a Wilcoxon signed-rank test, the larger gains were obtained in the small identity subset, the “twilight zone” where research on multiple sequence alignment has focused. [sent-96, score-0.532]
</p><p>54 One caveat of this multiple alignment approach is its running time, which is cubic in the length of the longest sequence, while most multiple sequence alignment approaches are quadratic. [sent-97, score-0.968]
</p><p>55 For example, the running time for one iteration of BPMF in this experiment was 364. [sent-98, score-0.046]
</p><p>56 98s for Clustal—this is why we have restricted the experiments to the short sequences section of BAliBASE. [sent-100, score-0.046]
</p><p>57 Fortunately, several techniques are available to decrease the computational complexity of this algorithm: the transitivity factors can be subsampled using a coarse pass, or along a phylogenetic tree; and computation of the factors can be entirely parallelized. [sent-101, score-0.158]
</p><p>58 5  Conclusion  Computing the moments of discrete exponential families can be difﬁcult for two reasons: the structure of the sufﬁcient statistic that can create junction trees of high tree-width, and the structure of the base measures that can induce an intractable combinatorial space. [sent-103, score-0.712]
</p><p>59 Most previous work on variational approximations has focused on the ﬁrst difﬁculty; however, the second challenge also arises frequently in machine learning. [sent-104, score-0.349]
</p><p>60 In this work, we have presented a framework that ﬁlls this gap. [sent-105, score-0.04]
</p><p>61 It is based on an intuitive notion of measure factorization, which, as we have shown, applies to a variety of combinatorial spaces. [sent-106, score-0.363]
</p><p>62 This notion enables variational algorithms to be adapted to the combinatorial setting. [sent-107, score-0.571]
</p><p>63 A polynomial-time approximation algorithm for the permanent of a matrix with non-negative entries. [sent-114, score-0.199]
</p><p>64 Mixing times of lozenge tiling and card shufﬂing Markov chains. [sent-117, score-0.043]
</p><p>65 BAliBASE: A benchmark alignments database for e e the evaluation of multiple sequence alignment programs. [sent-128, score-0.517]
</p><p>66 Belief propagation and loop calculus for the permanent of a non-negative matrix. [sent-141, score-0.265]
</p><p>67 Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudomoment matching. [sent-187, score-0.173]
</p><p>68 In oe Proceedings of Uncertainty in Artiﬁcal Intelligence, 2009. [sent-192, score-0.074]
</p><p>69 CLUSTAL: a package for performing multiple sequence alignment on a microcomputer. [sent-202, score-0.474]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignment', 0.36), ('combinatorial', 0.302), ('variational', 0.269), ('permanent', 0.199), ('clustal', 0.197), ('matchings', 0.173), ('factorization', 0.17), ('bipartite', 0.162), ('balibase', 0.148), ('bpmf', 0.148), ('dan', 0.132), ('setup', 0.116), ('statistic', 0.111), ('belief', 0.107), ('exponential', 0.099), ('probcons', 0.099), ('base', 0.097), ('emnlp', 0.097), ('parsing', 0.097), ('michael', 0.095), ('message', 0.091), ('ci', 0.087), ('mol', 0.086), ('ben', 0.083), ('tractable', 0.083), ('approximations', 0.08), ('transitivity', 0.079), ('phylogenetic', 0.079), ('percy', 0.079), ('protein', 0.076), ('sp', 0.074), ('oe', 0.074), ('matching', 0.073), ('alexandre', 0.071), ('david', 0.069), ('spaces', 0.068), ('appendix', 0.068), ('propagation', 0.066), ('sequence', 0.066), ('klein', 0.065), ('measure', 0.061), ('inference', 0.06), ('bm', 0.059), ('taskar', 0.058), ('identity', 0.058), ('north', 0.057), ('liang', 0.057), ('randomized', 0.056), ('family', 0.055), ('daphne', 0.054), ('moments', 0.054), ('approximating', 0.051), ('linguistics', 0.05), ('families', 0.049), ('maximization', 0.049), ('loopy', 0.048), ('multiple', 0.048), ('short', 0.046), ('running', 0.046), ('dp', 0.046), ('partition', 0.043), ('yusuke', 0.043), ('hindered', 0.043), ('brudno', 0.043), ('sbm', 0.043), ('hbm', 0.043), ('thompson', 0.043), ('card', 0.043), ('chuong', 0.043), ('sera', 0.043), ('ashish', 0.043), ('directory', 0.043), ('siepel', 0.043), ('alignments', 0.043), ('watanabe', 0.043), ('eld', 0.043), ('polynomial', 0.042), ('martin', 0.042), ('passing', 0.041), ('decomposition', 0.041), ('wainwright', 0.04), ('wilcoxon', 0.04), ('shuf', 0.04), ('mutation', 0.04), ('zone', 0.04), ('caveat', 0.04), ('carsten', 0.04), ('bert', 0.04), ('graham', 0.04), ('bart', 0.04), ('framework', 0.04), ('canonical', 0.039), ('perfect', 0.039), ('system', 0.039), ('sophisticated', 0.038), ('peterson', 0.037), ('enumeration', 0.037), ('yedidia', 0.037), ('conductance', 0.037), ('superiority', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="283-tfidf-1" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>Author: Alexandre Bouchard-côté, Michael I. Jordan</p><p>Abstract: Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. 1</p><p>2 0.10905297 <a title="283-tfidf-2" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>Author: Justin Domke</p><p>Abstract: This paper proposes a simple and eﬃcient ﬁnite diﬀerence method for implicit diﬀerentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, deﬁned on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to ﬁt graphical models with hidden variables, high treewidth and/or model misspeciﬁcation. 1</p><p>3 0.10652796 <a title="283-tfidf-3" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>Author: Georg Langs, Yanmei Tie, Laura Rigolo, Alexandra Golby, Polina Golland</p><p>Abstract: Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difﬁcult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We ﬁrst embed each brain into a functional map that reﬂects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions. 1</p><p>4 0.097070359 <a title="283-tfidf-4" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>5 0.092435859 <a title="283-tfidf-5" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>6 0.088191092 <a title="283-tfidf-6" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>7 0.082510673 <a title="283-tfidf-7" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>8 0.075496957 <a title="283-tfidf-8" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>9 0.069508284 <a title="283-tfidf-9" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>10 0.069253325 <a title="283-tfidf-10" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>11 0.06825427 <a title="283-tfidf-11" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>12 0.067963317 <a title="283-tfidf-12" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>13 0.066790283 <a title="283-tfidf-13" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>14 0.066755399 <a title="283-tfidf-14" href="./nips-2010-MAP_estimation_in_Binary_MRFs_via_Bipartite_Multi-cuts.html">165 nips-2010-MAP estimation in Binary MRFs via Bipartite Multi-cuts</a></p>
<p>15 0.066421106 <a title="283-tfidf-15" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>16 0.065987736 <a title="283-tfidf-16" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>17 0.06499932 <a title="283-tfidf-17" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>18 0.063750498 <a title="283-tfidf-18" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>19 0.060683101 <a title="283-tfidf-19" href="./nips-2010-Structured_Determinantal_Point_Processes.html">257 nips-2010-Structured Determinantal Point Processes</a></p>
<p>20 0.060284376 <a title="283-tfidf-20" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, 0.035), (2, 0.017), (3, 0.04), (4, -0.106), (5, 0.024), (6, 0.01), (7, 0.002), (8, 0.05), (9, 0.069), (10, -0.081), (11, -0.026), (12, 0.094), (13, 0.001), (14, -0.077), (15, 0.003), (16, -0.023), (17, 0.074), (18, 0.072), (19, -0.049), (20, -0.004), (21, 0.04), (22, 0.055), (23, -0.052), (24, 0.018), (25, -0.13), (26, -0.058), (27, -0.006), (28, -0.075), (29, -0.011), (30, 0.038), (31, -0.09), (32, -0.041), (33, -0.011), (34, 0.074), (35, -0.039), (36, -0.049), (37, -0.027), (38, 0.047), (39, 0.011), (40, 0.005), (41, -0.006), (42, 0.125), (43, -0.075), (44, 0.114), (45, -0.168), (46, -0.066), (47, -0.018), (48, 0.05), (49, -0.118)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95667362 <a title="283-lsi-1" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>Author: Alexandre Bouchard-côté, Michael I. Jordan</p><p>Abstract: Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. 1</p><p>2 0.62867367 <a title="283-lsi-2" href="./nips-2010-Fast_Large-scale_Mixture_Modeling_with_Component-specific_Data_Partitions.html">90 nips-2010-Fast Large-scale Mixture Modeling with Component-specific Data Partitions</a></p>
<p>Author: Bo Thiesson, Chong Wang</p><p>Abstract: Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-speciﬁc data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is signiﬁcantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data. 1</p><p>3 0.61219561 <a title="283-lsi-3" href="./nips-2010-Approximate_inference_in_continuous_time_Gaussian-Jump_processes.html">33 nips-2010-Approximate inference in continuous time Gaussian-Jump processes</a></p>
<p>Author: Manfred Opper, Andreas Ruttor, Guido Sanguinetti</p><p>Abstract: We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We ﬁrst consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efﬁcient mean ﬁeld approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p><p>4 0.60752016 <a title="283-lsi-4" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>Author: Justin Domke</p><p>Abstract: This paper proposes a simple and eﬃcient ﬁnite diﬀerence method for implicit diﬀerentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, deﬁned on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to ﬁt graphical models with hidden variables, high treewidth and/or model misspeciﬁcation. 1</p><p>5 0.5601244 <a title="283-lsi-5" href="./nips-2010-Approximate_Inference_by_Compilation_to_Arithmetic_Circuits.html">32 nips-2010-Approximate Inference by Compilation to Arithmetic Circuits</a></p>
<p>Author: Daniel Lowd, Pedro Domingos</p><p>Abstract: Arithmetic circuits (ACs) exploit context-speciﬁc independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the ﬁrst ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we ﬁnd that the methods based on sampling and learning work best: one such method (AC2 -F) is faster and usually more accurate than loopy belief propagation, mean ﬁeld, and Gibbs sampling; another (AC2 -G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines. 1</p><p>6 0.55189204 <a title="283-lsi-6" href="./nips-2010-Global_Analytic_Solution_for_Variational_Bayesian_Matrix_Factorization.html">106 nips-2010-Global Analytic Solution for Variational Bayesian Matrix Factorization</a></p>
<p>7 0.54386216 <a title="283-lsi-7" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>8 0.48908553 <a title="283-lsi-8" href="./nips-2010-Learning_Efficient_Markov_Networks.html">144 nips-2010-Learning Efficient Markov Networks</a></p>
<p>9 0.47611231 <a title="283-lsi-9" href="./nips-2010-Variational_bounds_for_mixed-data_factor_analysis.html">284 nips-2010-Variational bounds for mixed-data factor analysis</a></p>
<p>10 0.46679938 <a title="283-lsi-10" href="./nips-2010-MAP_Estimation_for_Graphical_Models_by_Likelihood_Maximization.html">164 nips-2010-MAP Estimation for Graphical Models by Likelihood Maximization</a></p>
<p>11 0.46581945 <a title="283-lsi-11" href="./nips-2010-Auto-Regressive_HMM_Inference_with_Incomplete_Data_for_Short-Horizon_Wind_Forecasting.html">35 nips-2010-Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting</a></p>
<p>12 0.46052498 <a title="283-lsi-12" href="./nips-2010-A_Primal-Dual_Message-Passing_Algorithm_for_Approximated_Large_Scale_Structured_Prediction.html">13 nips-2010-A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction</a></p>
<p>13 0.45667249 <a title="283-lsi-13" href="./nips-2010-Exact_inference_and_learning_for_cumulative_distribution_functions_on_loopy_graphs.html">84 nips-2010-Exact inference and learning for cumulative distribution functions on loopy graphs</a></p>
<p>14 0.45582214 <a title="283-lsi-14" href="./nips-2010-Inference_with_Multivariate_Heavy-Tails_in_Linear_Models.html">126 nips-2010-Inference with Multivariate Heavy-Tails in Linear Models</a></p>
<p>15 0.44902009 <a title="283-lsi-15" href="./nips-2010-Lifted_Inference_Seen_from_the_Other_Side_%3A_The_Tractable_Features.html">159 nips-2010-Lifted Inference Seen from the Other Side : The Tractable Features</a></p>
<p>16 0.43811581 <a title="283-lsi-16" href="./nips-2010-Repeated_Games_against_Budgeted_Adversaries.html">226 nips-2010-Repeated Games against Budgeted Adversaries</a></p>
<p>17 0.42551762 <a title="283-lsi-17" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>18 0.42479447 <a title="283-lsi-18" href="./nips-2010-A_POMDP_Extension_with_Belief-dependent_Rewards.html">11 nips-2010-A POMDP Extension with Belief-dependent Rewards</a></p>
<p>19 0.41891319 <a title="283-lsi-19" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>20 0.41871265 <a title="283-lsi-20" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.021), (17, 0.019), (27, 0.02), (30, 0.629), (45, 0.146), (50, 0.027), (52, 0.018), (60, 0.018), (77, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95586711 <a title="283-lda-1" href="./nips-2010-Sample_Complexity_of_Testing_the_Manifold_Hypothesis.html">232 nips-2010-Sample Complexity of Testing the Manifold Hypothesis</a></p>
<p>Author: Hariharan Narayanan, Sanjoy Mitter</p><p>Abstract: The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of ﬁtting a manifold with a nearly optimal least squared error. Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is independent of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume log 1 and curvature is unavoidable. Whether the known lower bound of O( k + 2 δ ) 2 for the sample complexity of Empirical Risk minimization on k−means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 [3]. Here is the desired bound on the error and δ is a bound on the probability of failure. We improve the best currently known upper bound [14] of 2 log 1 log4 k log 1 O( k2 + 2 δ ) to O k min k, 2 + 2 δ . Based on these results, we 2 devise a simple algorithm for k−means and another that uses a family of convex programs to ﬁt a piecewise linear curve of a speciﬁed length to high dimensional data, where the sample complexity is independent of the ambient dimension. 1</p><p>2 0.93214095 <a title="283-lda-2" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>same-paper 3 0.89001596 <a title="283-lda-3" href="./nips-2010-Variational_Inference_over_Combinatorial_Spaces.html">283 nips-2010-Variational Inference over Combinatorial Spaces</a></p>
<p>Author: Alexandre Bouchard-côté, Michael I. Jordan</p><p>Abstract: Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems [1, 2, 3], theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference [4]. Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models [5]; unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset [6]. 1</p><p>4 0.78557998 <a title="283-lda-4" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>Author: Ronny Luss, Saharon Rosset, Moni Shahar</p><p>Abstract: A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efﬁcient methods for each partitioning subproblem through an equivalent representation as a network ﬂow problem, and prove that this sequence of partitions converges to the global solution. These network ﬂow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm’s favorable computational properties are demonstrated through simulated examples as large as 2 × 105 variables and 107 constraints.</p><p>5 0.75314885 <a title="283-lda-5" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>Author: Tian Lan, Yang Wang, Weilong Yang, Greg Mori</p><p>Abstract: We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predeﬁned structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can signiﬁcantly improve activity recognition performance. 1</p><p>6 0.7133652 <a title="283-lda-6" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>7 0.6667341 <a title="283-lda-7" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>8 0.58774769 <a title="283-lda-8" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>9 0.58350337 <a title="283-lda-9" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>10 0.55589134 <a title="283-lda-10" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>11 0.55494326 <a title="283-lda-11" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>12 0.54347062 <a title="283-lda-12" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>13 0.54062235 <a title="283-lda-13" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>14 0.53091359 <a title="283-lda-14" href="./nips-2010-Random_Projections_for_%24k%24-means_Clustering.html">221 nips-2010-Random Projections for $k$-means Clustering</a></p>
<p>15 0.52406794 <a title="283-lda-15" href="./nips-2010-Trading_off_Mistakes_and_Don%27t-Know_Predictions.html">274 nips-2010-Trading off Mistakes and Don't-Know Predictions</a></p>
<p>16 0.52195388 <a title="283-lda-16" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>17 0.51700121 <a title="283-lda-17" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>18 0.51308012 <a title="283-lda-18" href="./nips-2010-Scrambled_Objects_for_Least-Squares_Regression.html">233 nips-2010-Scrambled Objects for Least-Squares Regression</a></p>
<p>19 0.50910652 <a title="283-lda-19" href="./nips-2010-Worst-case_bounds_on_the_quality_of_max-product_fixed-points.html">288 nips-2010-Worst-case bounds on the quality of max-product fixed-points</a></p>
<p>20 0.50804603 <a title="283-lda-20" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
