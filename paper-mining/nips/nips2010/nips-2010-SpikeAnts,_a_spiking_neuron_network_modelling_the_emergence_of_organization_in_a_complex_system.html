<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-252" href="#">nips2010-252</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</h1>
<br/><p>Source: <a title="nips-2010-252-pdf" href="http://papers.nips.cc/paper/3900-spikeants-a-spiking-neuron-network-modelling-the-emergence-of-organization-in-a-complex-system.pdf">pdf</a></p><p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>Reference: <a title="nips-2010-252-reference" href="../nips2010_reference/nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system  Sylvain Chevallier TAO, INRIA-Saclay Univ. [sent-1, score-0.361]
</p><p>2 How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. [sent-9, score-0.324]
</p><p>3 Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. [sent-10, score-0.586]
</p><p>4 Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. [sent-11, score-1.356]
</p><p>5 Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. [sent-12, score-0.549]
</p><p>6 Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. [sent-13, score-1.825]
</p><p>7 A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. [sent-14, score-0.84]
</p><p>8 For instance, the emergence of synchronized rhythmical activity has been observed in many social insect colonies [2, 4, 5, 7], where synchronized patterns of activity may indeed contribute to the collective efﬁciency in various ways. [sent-16, score-0.499]
</p><p>9 It thus comes naturally to investigate how spiking neuron networks (SNNs), also based on temporal dynamics, enable to model the emergence of collective phenomena, speciﬁcally synchronized activities, in complex systems. [sent-19, score-0.475]
</p><p>10 2  Target of the SpikeAnts model  The SpikeAnts model implements a distributed decision making process in a population of agents, say an ant colony. [sent-30, score-0.546]
</p><p>11 The model relies on the spatio-temporal interactions of spiking neurons, where each ant agent is accounted for by two neurons. [sent-32, score-0.661]
</p><p>12 Each agent a observes its environment and if it perceives none or too few working agents, a goes foraging for a given time and eventually goes to sleep. [sent-36, score-0.692]
</p><p>13 If the agent sees sufﬁciently many other foraging ants before the end of the observation period, it can switch at once to the self-grooming state. [sent-41, score-0.88]
</p><p>14 The agent decisions only depend on the information exchanged between them, through agent neurons sending spikes to (respectively, receiving spikes from) other agents in the population. [sent-45, score-0.519]
</p><p>15 It must be emphasized that the proposed decision process does not assume the agent ability to “count” (here the number of its foraging neighbors). [sent-46, score-0.625]
</p><p>16 2  The SpikeAnts spiking neuron network  This section describes the structure of the SpikeAnts model. [sent-48, score-0.3]
</p><p>17 Each ant agent is modelled by two spiking neurons. [sent-49, score-0.688]
</p><p>18 The ant colony thus deﬁnes a sparsely connected network of spiking neurons, referred to as SNN. [sent-51, score-0.619]
</p><p>19 1  Spiking neuron models  An agent is modelled by two coupled spiking neurons, respectively a Leaky Integrate-and-Fire (LIF) neuron [6, 14] and a Quadratic Integrate-and-Fire (QIF) neuron [8, 15]. [sent-53, score-0.806]
</p><p>20 A LIF neuron ﬁres a spike if its potential Vp exceeds a threshold ϑ. [sent-56, score-0.276]
</p><p>21 j The QIF neuron is described by the evolution of the potential Va , compared to the resting potential Vrest and an internal threshold Vthres . [sent-63, score-0.299]
</p><p>22 (3)  a Depending on whether the reset threshold is greater than the internal threshold (Vreset Vthres ), the a QIF neuron is bistable [12], which motivated the choice of this neuron model. [sent-65, score-0.448]
</p><p>23 If Vreset < Vthres , the membrane potential Va stabilizes on Vrest when there is no external perturbation, and the neuron a thus exhibits an integrator behavior. [sent-66, score-0.272]
</p><p>24 2  The ant agent model  Each SpikeAnts agent mimics an ant. [sent-69, score-0.656]
</p><p>25 During the observation round, the ant makes its decision (whether it goes foraging) based on the competition between its active and passive neurons (Fig. [sent-74, score-0.792]
</p><p>26 Both neurons are aware of the foraging neighbor ants. [sent-76, score-0.557]
</p><p>27 The active neuron additionally receives the excitatory signal Iclock (t) of the internal clock unit. [sent-79, score-0.322]
</p><p>28 In the case where the ant agent does not see too many foraging ants, the internal excitatory signal Iclock (t) dominates the inhibitory signal Iinh (t), the active neuron ﬁres ﬁrst and drives the ant to Active neuron Passive neuron Membrane potential (mV)  1. [sent-80, score-2.21]
</p><p>29 The ﬁrst observation state starts at 20ms: the active neuron ﬁres before the passive one, the agent thus goes foraging and the active neuron continues sending spikes during the whole foraging period (signalling its foraging behavior to other agents). [sent-84, score-2.352]
</p><p>30 This time the passive neuron ﬁres before the active one. [sent-86, score-0.346]
</p><p>31 During the last observation round, the active neuron wins again against the passive one, and the agent goes foraging. [sent-88, score-0.529]
</p><p>32 When foraging, the active neuron enters in a bursting phase and periodically sends a spike to the ant neighbors. [sent-91, score-0.813]
</p><p>33 Note that these spikes are only meaningful for the ants in observation state. [sent-92, score-0.335]
</p><p>34 After a foraging period (duration tF ), the ant goes to sleep (duration tS ). [sent-93, score-0.969]
</p><p>35 Quite the contrary, if the ant sees many other foraging ants, the excitatory signal Iexc (t) drives the passive neuron to ﬁre before the active one (second episode in Fig. [sent-95, score-1.304]
</p><p>36 2), and the ant accordingly sets in a self-grooming state (duration tG ). [sent-96, score-0.434]
</p><p>37 The decision making of the ant agent thus relies on the competition between its active and passive neurons. [sent-97, score-0.763]
</p><p>38 In particular, the number of spikes needed for an ant to go foraging or self-grooming depends on the temporal dynamics of the system; it varies from one observation episode to another. [sent-98, score-1.055]
</p><p>39 After some rest (self-grooming or sleeping states, with respective durations tG and tS , tG < tS ), the ant returns to the observation state. [sent-99, score-0.546]
</p><p>40 As above-mentioned, incoming spikes are only relevant to the active and passive neurons of an observing ant. [sent-100, score-0.285]
</p><p>41 During the foraging and resting states, presynaptic spikes have no inﬂuence, which can be thought of as an intrinsic plasticity mechanism [21] driven by the internal unit. [sent-101, score-0.657]
</p><p>42 The internal unit can indeed be seen as the ant biological clock. [sent-102, score-0.471]
</p><p>43 3  Model parameters  Overall, the SpikeAnts model is controlled by three types of parameters, respectively related to spiking neuron models, to ant agents (state durations) and to the whole population (size and connectivity of the SNN). [sent-107, score-0.885]
</p><p>44 Note that state duration timescale is not signiﬁcant at the ant colony level. [sent-110, score-0.525]
</p><p>45 0  mV  Spike ﬁring threshold  p Vreset  Passive neuron reset potential  Vthres  Active neuron bifurcation threshold  a Vreset  Active neuron reset potential  Iclock  Active neuron constant input current  -0. [sent-114, score-0.874]
</p><p>46 Goals of experiments A ﬁrst goal of experiments is to measure the global activity of the population, denoted F and deﬁned as the overall time spent foraging: F=  nF (t) t  4  (4)  where nF (t) is the number of foraging agents at time t. [sent-132, score-0.626]
</p><p>47 Each ant wakes up after some time uniformly drawn in ]0, 2tS ]. [sent-143, score-0.434]
</p><p>48 Spiking neurons are simulated using a discrete time scheme: numerical simulations of the spiking neuron network are based on a clock-driven simulator, using Runge-Kutta method for the approximation of differential equations, with a small time step of 0. [sent-144, score-0.368]
</p><p>49 1  Sensitivity analysis of the foraging effort  This section ﬁrst examines how the overall foraging effort F depends on the size M of the popua lation, the connection rate ρ and two neural parameters, the active neuron reset potential Vreset and ¯ the synaptic weight w. [sent-149, score-1.346]
</p><p>50 2  w  ¯ Figure 3: Sensitivity analysis of the average foraging effort F, versus population size M (top left), a connection probability ρ (top right), active neuron reset potential Vreset (bottom left) and synaptic weight w (bottom right). [sent-169, score-0.944]
</p><p>51 The overall foraging effort F was expected to linearly increase with the population size M . [sent-170, score-0.576]
</p><p>52 3, top right): the more neighbors, the more likely an ant will see other foraging ants, and will thus avoid go foraging itself. [sent-175, score-1.412]
</p><p>53 Along the same line, F was expected to decrease with the a a reset potential Vreset : the closer Vreset to ϑ, the more spikes a foraging ant will sent, exciting other ants’ passive neuron and thereby sending these ants to rest (Fig. [sent-176, score-1.605]
</p><p>54 It was expected that high w values would favor the triggering of passive neurons, and thus adversely affect the foraging effort. [sent-180, score-0.585]
</p><p>55 For low w values, an ant behaves as a “good statistician”, meaning that its decision is based on observing many other foraging agents. [sent-183, score-0.948]
</p><p>56 As w increases however, it makes it possible for an ant to take decisions based on few cues and the behavioral variability increases. [sent-185, score-0.465]
</p><p>57 More precisely, the F variance is low for small w values (an ant makes its decision based on about 80 spikes for w = 0. [sent-186, score-0.514]
</p><p>58 15; an ant makes its decision based on circa 6 spikes and small variations in the received spike trains might thus lead to different decisions, explaining the high variance of F. [sent-189, score-0.602]
</p><p>59 A close look at the experimental results reveals the existence of different temporal regimes with abrupt transitions among these, explaining the breaking down around M = 600 ants and the abrupt increase and decrease of F variance. [sent-191, score-0.38]
</p><p>60 2  600 400 200  50  100 150 200 250  nF (t)  200 400 600 800 1000  nF (t)  Figure 4: (Top row) Asynchronous, synchronous aperiodic and synchronous periodic patterns of activity (number of foraging ants versus time for t = 1 . [sent-193, score-1.495]
</p><p>61 4, left), depicts a situation where each ant (almost) independently makes its own decisions. [sent-203, score-0.434]
</p><p>62 4, middle) displays some coordination among the ants; speciﬁcally, the number of foraging ants is piecewise constant, though varying from a time interval to another. [sent-205, score-0.77]
</p><p>63 The third pattern, referred to as periodic synchronous (Fig. [sent-206, score-0.329]
</p><p>64 4, right) involves two stable subpopulations which forage alternatively; the population enters a bi-phase mode, as actually observed in some ants colonies [4, 5]. [sent-207, score-0.437]
</p><p>65 4, bottom row; transient states are removed in the synchronized periodic and aperiodic regime for the sake of clarity). [sent-209, score-0.403]
</p><p>66 The orbit of the synchronous aperiodic activity indicates the presence of at least one attractor whereas the synchronous periodic activity displays a ﬂip bifurcation. [sent-210, score-0.804]
</p><p>67 For synchronous aperiodic patterns, the mean value of the 3,500 Lyapunov exponents found with a 6 dimension analysis is also −0. [sent-215, score-0.366]
</p><p>68 Whereas the asynchronous and synchronous aperiodic activities lie at the edge of chaos, the periodic synchronous regime only displays large negative Lyapunov exponents, indicating a very stable behavior. [sent-218, score-0.873]
</p><p>69 Let I denote the set of values nF (t) (after pruning all transient time steps such that nF (t) = nF (t + 1) and nF (t) = nF (t − 1)); the foraging histogram is deﬁned by associating to each value k in I, the number nk of time steps such that nF (t) = k. [sent-220, score-0.534]
</p><p>70 The synchronization of the population is 6  ﬁnally measured from the histogram entropy H: nk log m nm  H=− k∈I  nk m nm  (5)  The entropy of the asynchronous regime is zero, since all states are transient. [sent-221, score-0.448]
</p><p>71 The synchronous periodic regime, where two subpopulations alternatively forage, gets a low entropy (< log 2). [sent-222, score-0.378]
</p><p>72 Finally, the synchronous aperiodic regime which involves a few dozens of subpopulations, gets a high entropy value. [sent-223, score-0.424]
</p><p>73 A high sociability enables the ants to base their foraging decision on reliable estimates of the current foraging activity, thus entailing a low variance of the global foraging effort. [sent-227, score-1.935]
</p><p>74 A high receptivity thus enables the ant to postpone its foraging decision based on few cues (i. [sent-230, score-1.104]
</p><p>75 visible foraging ants), thereby entailing a high variance of the global foraging effort. [sent-232, score-1.0]
</p><p>76 The sociability and receptivity factors, referred to as control parameters, support a clear picture of the asynchronous, synchronous aperiodic and periodic synchronous patterns. [sent-233, score-0.978]
</p><p>77 5, right) are displayed in the 2D plane deﬁned from the sociability and receptivity of the SpikeAnts system, deﬁning the phase diagram of the SpikeAnts system. [sent-236, score-0.39]
</p><p>78 For a low sociability and a high receptivity (region A in Fig. [sent-237, score-0.323]
</p><p>79 5), few interactions among ants take place and each ant makes its decisions based on few cues. [sent-238, score-0.719]
</p><p>80 In this region, the population is a collection of quasi independent individuals, and few ants (60 on average on Fig. [sent-239, score-0.341]
</p><p>81 For a higher sociability and a low receptivity (region B in Fig. [sent-241, score-0.323]
</p><p>82 5), ants see more of their peers and they base their decisions on reliable estimates of the foraging activity. [sent-242, score-0.796]
</p><p>83 A synchronization of the ant activities emerges, in the sense that many agents make their foraging decisions at the same time. [sent-243, score-1.18]
</p><p>84 the number of foraging ants varies from 50 to 240 (Fig. [sent-246, score-0.743]
</p><p>85 2  0 10  15  20  H (standard deviation)  For a high sociability and a high receptivity (region C in Fig. [sent-268, score-0.323]
</p><p>86 5), ants see many of their peers and they make their decisions based on few cues. [sent-269, score-0.307]
</p><p>87 In this case a periodic synchronized regime is observed, where two subpopulations alternatively go foraging (the ﬁrst one involves ∼ 950 ants in Fig. [sent-270, score-1.019]
</p><p>88 0  25  0  Sociability  5  10  15  20  25  Sociability  Figure 5: Emergence of synchronizations in the population activity: entropy H (left) and variance of H (right) versus the ant sociability and receptivity. [sent-272, score-0.716]
</p><p>89 The asynchronous pattern, with entropy H = 0 corresponds to a low sociability and high receptivity (region A). [sent-273, score-0.46]
</p><p>90 The synchronous aperiodic pattern, with high entropy, corresponds to a medium sociability and low receptivity (region B). [sent-274, score-0.649]
</p><p>91 The synchronous periodic pattern, H ∼ log 2, corresponds to both high sociability and receptivity (region C). [sent-275, score-0.628]
</p><p>92 7  250  nF (t)  200 150 100 50 0  0  1000  2000  3000  4000  5000 t  6000  7000  8000  9000  10000  Figure 6: A representative simulation: the global behavior switches from a synchronous aperiodic regime to an asynchronous one before stabilizing in a periodic synchronous regime. [sent-276, score-0.837]
</p><p>93 Speciﬁcally, an asynchronous aperiodic regime (region B) is prone to evolve into an asynchronous (region A) or periodic synchronous (region C) regimes (Figure 6). [sent-278, score-0.746]
</p><p>94 Quite the contrary, the periodic synchronous regime is stable, i. [sent-279, score-0.375]
</p><p>95 the population does not get back to any other regime after the periodic synchronous regime is installed. [sent-281, score-0.532]
</p><p>96 The aperiodic synchronous regime, though less stable than the periodic one, is far more stable than the asynchronous one. [sent-282, score-0.541]
</p><p>97 The synchronization patterns that emerge at the macroscopic scale can be fully controlled by several model parameters ruling the sociability of ants (whether an ant may observe many other ants) and their receptivity (whether an ant makes its foraging decision based on a few cues). [sent-284, score-2.133]
</p><p>98 in the transient regime, by making spiking neurons sensitive to the synchrony of spike trains. [sent-299, score-0.337]
</p><p>99 affecting the number of foraging ants) will be investigated. [sent-309, score-0.489]
</p><p>100 Towards energy optimisation: Emergent task allocation in a swarm of foraging robots. [sent-421, score-0.489]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('foraging', 0.489), ('ant', 0.434), ('ants', 0.254), ('spikeants', 0.234), ('synchronous', 0.199), ('neuron', 0.184), ('nf', 0.179), ('sociability', 0.167), ('receptivity', 0.156), ('vreset', 0.137), ('aperiodic', 0.127), ('synchronization', 0.126), ('spiking', 0.116), ('agent', 0.111), ('asynchronous', 0.109), ('periodic', 0.106), ('passive', 0.096), ('population', 0.087), ('vrest', 0.078), ('activity', 0.073), ('regime', 0.07), ('neurons', 0.068), ('iclock', 0.067), ('vthres', 0.067), ('active', 0.066), ('spike', 0.066), ('agents', 0.064), ('mv', 0.063), ('emergence', 0.061), ('sleeping', 0.059), ('iexc', 0.056), ('qif', 0.056), ('snns', 0.056), ('spikes', 0.055), ('synchronized', 0.055), ('emergent', 0.051), ('res', 0.049), ('tg', 0.049), ('insect', 0.049), ('synaptic', 0.049), ('patterns', 0.048), ('duration', 0.046), ('goes', 0.046), ('va', 0.046), ('transient', 0.045), ('colony', 0.045), ('subpopulations', 0.045), ('reset', 0.043), ('vp', 0.042), ('synchrony', 0.042), ('lyapunov', 0.042), ('exponents', 0.04), ('internal', 0.037), ('phase', 0.036), ('lif', 0.036), ('activities', 0.036), ('excitatory', 0.035), ('external', 0.035), ('iinh', 0.033), ('collective', 0.032), ('competition', 0.031), ('diagram', 0.031), ('decisions', 0.031), ('colonies', 0.029), ('assemblies', 0.029), ('entropy', 0.028), ('membrane', 0.027), ('modelled', 0.027), ('temporal', 0.027), ('bursting', 0.027), ('durations', 0.027), ('tf', 0.027), ('behavior', 0.027), ('displays', 0.027), ('inhibitory', 0.026), ('resting', 0.026), ('plasticity', 0.026), ('observation', 0.026), ('potential', 0.026), ('regimes', 0.026), ('region', 0.025), ('living', 0.025), ('transitions', 0.025), ('decision', 0.025), ('presynaptic', 0.024), ('sending', 0.024), ('abrupt', 0.024), ('referred', 0.024), ('division', 0.024), ('dynamics', 0.024), ('social', 0.024), ('ms', 0.023), ('uential', 0.023), ('bonabeau', 0.022), ('circa', 0.022), ('entailing', 0.022), ('forage', 0.022), ('grooming', 0.022), ('orsay', 0.022), ('peers', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="252-tfidf-1" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>2 0.13872112 <a title="252-tfidf-2" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>3 0.13176024 <a title="252-tfidf-3" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>4 0.10503395 <a title="252-tfidf-4" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>5 0.095977694 <a title="252-tfidf-5" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>Author: Adrien Wohrer, Ranulfo Romo, Christian K. Machens</p><p>Abstract: How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reﬂect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to ”ﬁll the gaps” in noise correlations matrices using an iterative application of the Wishart distribution over positive deﬁnitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternativeforced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations. 1</p><p>6 0.090645619 <a title="252-tfidf-6" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>7 0.08642412 <a title="252-tfidf-7" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>8 0.086311355 <a title="252-tfidf-8" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>9 0.083272688 <a title="252-tfidf-9" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>10 0.081787698 <a title="252-tfidf-10" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>11 0.078054704 <a title="252-tfidf-11" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>12 0.07102631 <a title="252-tfidf-12" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>13 0.070412725 <a title="252-tfidf-13" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>14 0.061526153 <a title="252-tfidf-14" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>15 0.058816619 <a title="252-tfidf-15" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>16 0.05813235 <a title="252-tfidf-16" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>17 0.052691005 <a title="252-tfidf-17" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>18 0.047491286 <a title="252-tfidf-18" href="./nips-2010-A_Computational_Decision_Theory_for_Interactive_Assistants.html">4 nips-2010-A Computational Decision Theory for Interactive Assistants</a></p>
<p>19 0.047456358 <a title="252-tfidf-19" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>20 0.045994755 <a title="252-tfidf-20" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.092), (1, -0.01), (2, -0.132), (3, 0.154), (4, 0.051), (5, 0.184), (6, -0.087), (7, 0.02), (8, 0.049), (9, -0.036), (10, 0.012), (11, 0.039), (12, 0.016), (13, 0.021), (14, 0.038), (15, -0.037), (16, -0.051), (17, -0.058), (18, -0.009), (19, -0.03), (20, -0.018), (21, -0.001), (22, -0.061), (23, 0.004), (24, 0.045), (25, -0.021), (26, 0.005), (27, -0.03), (28, 0.072), (29, -0.129), (30, -0.003), (31, -0.012), (32, 0.002), (33, 0.017), (34, -0.066), (35, 0.001), (36, 0.015), (37, 0.005), (38, -0.015), (39, 0.026), (40, -0.051), (41, -0.002), (42, 0.041), (43, -0.038), (44, -0.01), (45, 0.112), (46, -0.013), (47, 0.019), (48, -0.003), (49, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96257365 <a title="252-lsi-1" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>2 0.76800889 <a title="252-lsi-2" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>Author: Sebastian Millner, Andreas Grübl, Karlheinz Meier, Johannes Schemmel, Marc-olivier Schwartz</p><p>Abstract: We describe an accelerated hardware neuron being capable of emulating the adaptive exponential integrate-and-ﬁre neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulations and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientiﬁc experiments, the focus lays on parameterizability and reproduction of the analytical model. 1</p><p>3 0.722 <a title="252-lsi-3" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>Author: Joscha Schmiedt, Christian Albers, Klaus Pawelzik</p><p>Abstract: When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with modulated pre- and postsynaptic contributions to long-term synaptic modiﬁcations. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We ﬁnd that our model reproduces data from to recent experimental studies with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear ﬁlter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic ﬁring rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. Modiﬁcations are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also ﬁnd emphasis of speciﬁc baseline spike rates and suppression for high background rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models. 1</p><p>4 0.72114676 <a title="252-lsi-4" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>Author: Dan Goodman, Romain Brette</p><p>Abstract: To localise the source of a sound, we use location-speciﬁc properties of the signals received at the two ears caused by the asymmetric ﬁltering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism’s lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from ﬁltered sounds. We present a spiking neural network model of sound localisation based on extracting location-speciﬁc synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difﬁcult task of distinguishing sounds coming from the front and back. Keywords: Auditory Perception & Modeling (Primary); Computational Neural Models, Neuroscience, Supervised Learning (Secondary) 1</p><p>5 0.63170576 <a title="252-lsi-5" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>Author: Nicholas Fisher, Arunava Banerjee</p><p>Abstract: From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classiﬁcation based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach. 1</p><p>6 0.62056273 <a title="252-lsi-6" href="./nips-2010-Identifying_Dendritic_Processing.html">115 nips-2010-Identifying Dendritic Processing</a></p>
<p>7 0.60424584 <a title="252-lsi-7" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>8 0.59644324 <a title="252-lsi-8" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>9 0.53402358 <a title="252-lsi-9" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>10 0.52390641 <a title="252-lsi-10" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>11 0.502581 <a title="252-lsi-11" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>12 0.45059684 <a title="252-lsi-12" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>13 0.39115363 <a title="252-lsi-13" href="./nips-2010-Rescaling%2C_thinning_or_complementing%3F_On_goodness-of-fit_procedures_for_point_process_models_and_Generalized_Linear_Models.html">227 nips-2010-Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models</a></p>
<p>14 0.37558806 <a title="252-lsi-14" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>15 0.37115368 <a title="252-lsi-15" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>16 0.36908266 <a title="252-lsi-16" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>17 0.35518685 <a title="252-lsi-17" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>18 0.35092276 <a title="252-lsi-18" href="./nips-2010-Reward_Design_via_Online_Gradient_Ascent.html">229 nips-2010-Reward Design via Online Gradient Ascent</a></p>
<p>19 0.33140743 <a title="252-lsi-19" href="./nips-2010-A_Log-Domain_Implementation_of_the_Diffusion_Network_in_Very_Large_Scale_Integration.html">8 nips-2010-A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration</a></p>
<p>20 0.31193522 <a title="252-lsi-20" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.372), (13, 0.019), (17, 0.013), (27, 0.071), (30, 0.046), (45, 0.105), (50, 0.036), (52, 0.034), (60, 0.032), (77, 0.123), (90, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76868886 <a title="252-lda-1" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>Author: Sylvain Chevallier, Hél\`ene Paugam-moisy, Michele Sebag</p><p>Abstract: Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Speciﬁcally, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities − similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed. 1</p><p>2 0.57465053 <a title="252-lda-2" href="./nips-2010-Evaluation_of_Rarity_of_Fingerprints_in_Forensics.html">82 nips-2010-Evaluation of Rarity of Fingerprints in Forensics</a></p>
<p>Author: Chang Su, Sargur Srihari</p><p>Abstract: A method for computing the rarity of latent ﬁngerprints represented by minutiae is given. It allows determining the probability of ﬁnding a match for an evidence print in a database of n known prints. The probability of random correspondence between evidence and database is determined in three procedural steps. In the registration step the latent print is aligned by ﬁnding its core point; which is done using a procedure based on a machine learning approach based on Gaussian processes. In the evidence probability evaluation step a generative model based on Bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the conﬁdence of their presence in the evidence. In the speciﬁc probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a speciﬁc birthday. The generative model is validated using a goodness-of-ﬁt test evaluated with a standard database of ﬁngerprints. The probability of random correspondence for several latent ﬁngerprints are evaluated for varying numbers of minutiae. 1</p><p>3 0.43550232 <a title="252-lda-3" href="./nips-2010-Attractor_Dynamics_with_Synaptic_Depression.html">34 nips-2010-Attractor Dynamics with Synaptic Depression</a></p>
<p>Author: K. Wong, He Wang, Si Wu, Chi Fung</p><p>Abstract: Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We ﬁnd that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we ﬁnd that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p><p>4 0.43396124 <a title="252-lda-4" href="./nips-2010-A_Theory_of_Multiclass_Boosting.html">15 nips-2010-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. 1</p><p>5 0.43235126 <a title="252-lda-5" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>Author: Hairong Liu, Longin J. Latecki, Shuicheng Yan</p><p>Abstract: In this paper, we regard clustering as ensembles of k-ary afﬁnity relations and clusters correspond to subsets of objects with maximal average afﬁnity relations. The average afﬁnity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efﬁcient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a uniﬁed solution to clustering from k-ary afﬁnity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p><p>6 0.43115318 <a title="252-lda-6" href="./nips-2010-A_VLSI_Implementation_of_the_Adaptive_Exponential_Integrate-and-Fire_Neuron_Model.html">16 nips-2010-A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model</a></p>
<p>7 0.41992608 <a title="252-lda-7" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>8 0.41589704 <a title="252-lda-8" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>9 0.41521895 <a title="252-lda-9" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>10 0.41362363 <a title="252-lda-10" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>11 0.40374827 <a title="252-lda-11" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>12 0.40294307 <a title="252-lda-12" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>13 0.40217265 <a title="252-lda-13" href="./nips-2010-A_Novel_Kernel_for_Learning_a_Neuron_Model_from_Spike_Train_Data.html">10 nips-2010-A Novel Kernel for Learning a Neuron Model from Spike Train Data</a></p>
<p>14 0.39389953 <a title="252-lda-14" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>15 0.39194837 <a title="252-lda-15" href="./nips-2010-Spike_timing-dependent_plasticity_as_dynamic_filter.html">253 nips-2010-Spike timing-dependent plasticity as dynamic filter</a></p>
<p>16 0.38894051 <a title="252-lda-16" href="./nips-2010-Sodium_entry_efficiency_during_action_potentials%3A_A_novel_single-parameter_family_of_Hodgkin-Huxley_models.html">244 nips-2010-Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models</a></p>
<p>17 0.38867867 <a title="252-lda-17" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>18 0.38721675 <a title="252-lda-18" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>19 0.38532102 <a title="252-lda-19" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>20 0.38425297 <a title="252-lda-20" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
