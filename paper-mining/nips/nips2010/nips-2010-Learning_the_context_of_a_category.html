<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2010-Learning the context of a category</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-155" href="#">nips2010-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 nips-2010-Learning the context of a category</h1>
<br/><p>Source: <a title="nips-2010-155-pdf" href="http://papers.nips.cc/paper/4139-learning-the-context-of-a-category.pdf">pdf</a></p><p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>Reference: <a title="nips-2010-155-reference" href="../nips2010_reference/nips-2010-Learning_the_context_of_a_category_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. [sent-5, score-0.952]
</p><p>2 The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations. [sent-6, score-0.519]
</p><p>3 In part, this context speciﬁcity reﬂects the tendency for people to organize knowledge into independent “bundles” which may contain contradictory information, and which may be deemed appropriate to different contexts. [sent-10, score-0.677]
</p><p>4 This phenomenon is called knowledge partitioning [2–6], and is observed in artiﬁcial category learning experiments as well as real world situations. [sent-11, score-0.51]
</p><p>5 Context induced knowledge partitioning poses a challenge to models of human learning. [sent-13, score-0.374]
</p><p>6 This paper explores the possibility that Bayesian models of human category learning can provide the missing explanation. [sent-15, score-0.268]
</p><p>7 This model is then shown to provide a parsimonious and psychologically appealing account of the knowledge partitioning effect. [sent-17, score-0.375]
</p><p>8 Following this, a hierarchical extension is introduced to the model, which allows it to acquire abstract knowledge about the context speciﬁcity of the categories, in a manner that is consistent with the data on human learning. [sent-18, score-0.673]
</p><p>9 2 Learning categories in context This section outlines a Bayesian model that is sensitive to the learning context. [sent-19, score-0.592]
</p><p>10 It extends Anderson’s [7] rational model of categorization (RMC) by allowing the model to track the context in which observations are made, and draw inferences about the role that context plays. [sent-20, score-1.002]
</p><p>11 If zi denotes the cluster to which the ith observation is assigned, then the joint prior 1  distribution over zn = (z1 , . [sent-23, score-0.523]
</p><p>12 (1)  Each cluster of observations is mapped onto a distribution over features. [sent-27, score-0.283]
</p><p>13 While independence is reasonable when stimulus dimensions are separable [9], knowledge partitioning can occur regardless of whether dimensions are separable or integral (see [6] for details), so the more general formulation is useful. [sent-36, score-0.358]
</p><p>14 Each cluster is associated with a distribution over category labels. [sent-38, score-0.434]
</p><p>15 If ℓi denotes the label given to the ith observation, then ℓi θk  | zi = k, θk | β  ∼ Bernoulli(θk ) ∼ Beta(β, β)  (3)  The β parameter describes the extent to which items in the same cluster are allowed to have different labels. [sent-39, score-0.758]
</p><p>16 The extension to handle context dependence is straightforward: contextual information is treated as an auxiliary feature, and so each cluster is linked to a distribution over contexts. [sent-42, score-0.656]
</p><p>17 In the experiments considered later, each observation is assigned to a context individually, which allows us to apply the exact same model for contextual features as regular ones. [sent-43, score-0.468]
</p><p>18 Thus a very simple context model is sufﬁcient: ci φk  | zi = k, φk | γ  ∼ ∼  Bernoulli(φk ) Beta(γ, γ)  (4)  The context speciﬁcity parameter γ is analogous to β and controls the extent to which clusters can include observations made in different contexts. [sent-44, score-1.226]
</p><p>19 In more general contexts, a richer model would be required to capture the manner in which context can vary. [sent-45, score-0.418]
</p><p>20 Firstly, since the categories do not overlap in the experiments discussed here it makes sense to set β = 0, which has the effect of forcing each cluster to be associated only with one category. [sent-47, score-0.288]
</p><p>21 Secondly, human learners rarely have strong prior knowledge about the features used in artiﬁcial category learning experiments, expressed by setting κ0 = 1 and ν0 = 3 (ν0 is larger to ensure that the priors over features always has a well deﬁned covariance structure). [sent-48, score-0.458]
</p><p>22 Having made these choices, we may restrict our attention to α (the bias to introduce new clusters) and γ (the bias to treat clusters as context general). [sent-50, score-0.482]
</p><p>23 2 Inference in the model Inference is performed via a collapsed Gibbs sampler, integrating out φ, θ, µ and Σ and deﬁning a sampler only over the cluster assignments z. [sent-52, score-0.375]
</p><p>24 To do so, note that P (zi = k|x, ℓ, c, z−i )  ∝ P (xi , ℓi , ci |x−i , ℓ−i , c−i , z−i , zi = k)P (zi = k|z−i ) = P (xi |x−i , z−i , zi = k)P (ℓi |ℓ−i , z−i , zi = k) P (ci |c−i , z−i , zi = k)P (zi = k|z−i )  (5) (6)  where the dependence on the parameters that describe the prior (i. [sent-53, score-0.746]
</p><p>25 In this expression z−i denotes the set of all cluster assignments 2  except the ith, and the normalizing term is calculated by summing Equation 6 over all possible cluster assignments k, including the possibility that the ith item is assigned to an entirely new cluster. [sent-56, score-0.713]
</p><p>26 The conditional prior probability P (zi = k|z−i ) is P (zi = k|z−i ) =  nk n−1+α α n−1+α  if k is old if k is new  (7)  where nk counts the number of items (not including the ith) that have been assigned to the kth cluster. [sent-57, score-0.802]
</p><p>27 A similar result applies to the labelling scheme: (ℓ )  1  P (ℓi |ℓ−i , z−i , zi = k) =  0  P (ℓi |θk , zi = k)P (θk |ℓ−i , z−i ) dθk =  nk i + β nk + 2β  (9)  (ℓ )  where nk i counts the number of observations that have been assigned to cluster k and given the same label as observation i. [sent-59, score-1.506]
</p><p>28 Taken together, Equations 6, 8, 9 and 11 suggest a simple a Gibbs sampler over the cluster assignments z. [sent-65, score-0.345]
</p><p>29 Cluster assignments zi are initialized randomly, and are then sequentially redrawn from the conditional posterior distribution in Equation 6. [sent-66, score-0.286]
</p><p>30 3 Application to knowledge partitioning experiments To illustrate the behavior of the model, consider the most typical example of a knowledge partitioning experiment [3, 4, 6]. [sent-69, score-0.616]
</p><p>31 There are two categories organized into an “inside-outside” structure, with one category (black circles/squares) occupying a region along either side of the other one (white circles/squares). [sent-73, score-0.258]
</p><p>32 In Figure 1a, squares correspond to items presented in one context, and circles to items presented in the other context. [sent-75, score-0.366]
</p><p>33 Participants are trained on these items in a standard supervised categorization experiment: stimuli are presented one at a time (with the context variable), and participants are asked to predict the category label. [sent-76, score-1.052]
</p><p>34 Percentages refer to the probability of selecting category label A. [sent-79, score-0.264]
</p><p>35 At this point, participants are shown transfer items (the crosses in Figure 1a), and asked what category label these items should be given. [sent-81, score-0.857]
</p><p>36 Critically, each transfer item is presented in both contexts, to determine whether people generalize in a context speciﬁc way. [sent-83, score-0.61]
</p><p>37 Some participants are context insensitive (lower two panels) and their predictions about the transfer items do not change as a function of context. [sent-86, score-0.848]
</p><p>38 However, other participants are context sensitive (upper panels) and adopt a very different strategy depending on which context the transfer item is presented in. [sent-87, score-1.129]
</p><p>39 This is taken to imply [3, 4, 6] that the context sensitive participants have learned a conceptual representation in which knowledge is “partitioned” into different bundles, each associated with a different context. [sent-88, score-0.811]
</p><p>40 1 Learning the knowledge partition The initial investigation focused on what category representations the model learns, as a function of α and γ. [sent-90, score-0.368]
</p><p>41 In the four cluster solution (panel b, small γ), the clusters never aggregate across items observed in different contexts. [sent-92, score-0.509]
</p><p>42 In contrast, the three cluster solution (panel a, larger γ) is more context general, and collapses category B into a single cluster. [sent-93, score-0.822]
</p><p>43 As a result, for α > 1 the model tends not to produce the three cluster solution. [sent-95, score-0.262]
</p><p>44 The next aim was to quantify the extent to which γ inﬂuences the relative prevalence of the four cluster solution versus the three cluster solution. [sent-100, score-0.53]
</p><p>45 Since the adjusted Rand index measures the extent to which any given pair of items are classiﬁed in the same way by the two solutions, it is a natural measure of how close a model-generated solution is to one of the two idealized solutions. [sent-102, score-0.34]
</p><p>46 2  0  0  5  10  15  gamma  (a)  (b)  (c)  Figure 2: The two different clustering schemes produced by the context sensitive RMC, and the values of γ that produce them (for α ﬁxed at 0. [sent-107, score-0.474]
</p><p>47 7) the four cluster solution is extremely dominant whereas at larger values the three cluster solution is preferred. [sent-114, score-0.464]
</p><p>48 One of the most desirable characteristics is the fact that the partitioning of the learners knowledge is made explicit. [sent-118, score-0.366]
</p><p>49 That is, the model learns a much more differentiated and context bound representation when γ is small, and a more context general and less differentiated representation when γ is large. [sent-119, score-0.948]
</p><p>50 During training, the model learns to weight each of the rule modules differently depending on context, thereby producing context speciﬁc generalizations. [sent-122, score-0.536]
</p><p>51 As such, ATRIUM learns the context dependence, but not the knowledge partition itself. [sent-126, score-0.592]
</p><p>52 2 Generalizing in context-speciﬁc and context-general ways The discussion to this point shows how the value of γ shapes the conceptual knowledge that the model acquires, but has not looked at what generalizations the model makes. [sent-128, score-0.294]
</p><p>53 However, it is straightforward to show that varying γ does allow the context sensitive RMC to capture the two generalization patterns in Figure 1. [sent-129, score-0.474]
</p><p>54 With this in mind, Figure 3 plots the generalizations made by the model for two different levels of context speciﬁcity (γ = 0 and γ = 10) and for the two different clustering solutions. [sent-130, score-0.495]
</p><p>55 As is clear from inspection – and veriﬁed by the squared correlations listed in the Figure caption – when γ is small the model generalizes in a context speciﬁc manner, but when γ is large the generalizations are the same in all contexts. [sent-132, score-0.53]
</p><p>56 This happens for both clustering solutions, which implies that γ plays two distinct but related roles, insofar as it inﬂuences the context speciﬁcity of both the learned knowledge partition and the generalizations to new observations. [sent-133, score-0.601]
</p><p>57 4 Acquiring abstract knowledge about context speciﬁcity One thing missing from both ATRIUM and the RMC is an explanation for how the leaner decides whether context speciﬁc or context general representations are appropriate. [sent-134, score-1.257]
</p><p>58 In both cases, the model has free parameters that govern the switch between the two cases, and these parameters must be 5  γ=0 context 1  (c)  (b)  context 2  4 clusters  (a)  γ = 10 context 2  4 clusters  context 1  3 clusters  3 clusters  (d)  Figure 3: Generalizations made by the model. [sent-135, score-1.958]
</p><p>59 1% of the variance in the context sensitive data, but only 35. [sent-137, score-0.474]
</p><p>60 6% of the variance in the context sensitive data is explained, whereas 67. [sent-143, score-0.474]
</p><p>61 1% of the context insensitive data can be accounted for. [sent-144, score-0.438]
</p><p>62 To answer this, note that if the context varies in a systematic fashion, an intelligent learner might come to suspect that the context matters, and would be more likely to decide to generalize in a context speciﬁc way. [sent-152, score-1.433]
</p><p>63 On the other hand, if there are no systematic patterns to the way that observations are distributed across contexts, then the learner should deem the context to be irrelevant and hence decide to generalize broadly across contexts. [sent-153, score-0.678]
</p><p>64 One condition of this experiment was a standard knowledge partitioning experiment, identical in every meaningful respect to the data described earlier in this paper. [sent-156, score-0.358]
</p><p>65 As is typical for such experiments, knowledge partitioning was observed for at least some of the participants. [sent-157, score-0.308]
</p><p>66 In the other condition, however, the context variable was randomized: each of the training items was assigned to a randomly chosen context. [sent-158, score-0.621]
</p><p>67 What this implies is that human learners use the systematicity of the context as a cue to determine how broadly to generalize. [sent-160, score-0.512]
</p><p>68 As such, the model should learn that γ is small when the context varies systematically; and similarly should learn that γ is large if the context is random. [sent-161, score-0.836]
</p><p>69 1 A hierarchical context-sensitive RMC Extending the statistical model is straightforward: we place priors over γ, and allow the model to infer a joint posterior distribution over the cluster assignments z and the context speciﬁcity γ. [sent-164, score-0.873]
</p><p>70 This is closely related to other hierarchical Bayesian models of category learning [15–19]. [sent-165, score-0.275]
</p><p>71 6  1000 systematic context randomized context  frequency  800  600  400  200  0 −4  −3  −2  −1 log (γ)  0  1  2  10  Figure 4: Learned distributions over γ in the systematic (dark rectangles) and randomized (light rectangles) conditions, plotted on a logarithmic scale. [sent-170, score-1.102]
</p><p>72 The acceptance probabilities for the Metropolis sampler may be calculated by observing that P (γ|x, ℓ, c, z)  ∝ ∝ =  P (x, ℓ, c|z, γ)P (γ) P (c|z, γ)P (γ) P (c|z, φ)P (φ|γ) dφ P (γ) K  =  (14) (15) (16)  1  P (c(k) |φk )P (φk |γ) dφk  P (γ) k=1  (17)  0 K  =  ∝  exp(−λγ)  nk ! [sent-176, score-0.307]
</p><p>73 k=1 nk K (c=1) (c=2) B(nk + γ, nk  + γ)  B(γ, γ)  k=1  (c=2)  + γ, nk B(γ, γ)  (c=j)  where B(a, b) = Γ(a)Γ(b)/Γ(a + b) denotes the beta function, and nk items in cluster k that appeared in context j. [sent-179, score-1.838]
</p><p>74 2 Application of the extended model To explore the performance of the hierarchical extension of the context sensitive RMC, the model was trained on both the original, systematic version of the knowledge partitioning experiments, and on a version with the context variables randomly permuted. [sent-181, score-1.42]
</p><p>75 As expected, in the systematic condition the model notices the fact that the context varies systematically as a function of the feature values x, and learns to form context speciﬁc clusters. [sent-183, score-1.109]
</p><p>76 Indeed, 97% of the posterior distribution over z is absorbed by the four cluster solution (or other solutions that are sufﬁciently similar in the sense discussed earlier). [sent-184, score-0.344]
</p><p>77 In the process, the model infers that γ is small and generalizes in a context speciﬁc way (as per Figure 3). [sent-185, score-0.497]
</p><p>78 Nevertheless, without changing any parameter values, the same model in the randomized condition infers that there is no pattern to the context variable, which ends up being randomly scattered across the clusters. [sent-186, score-0.558]
</p><p>79 For this condition 57% of the posterior mass is approximately equivalent to the three cluster solution. [sent-187, score-0.347]
</p><p>80 As a result, the model infers that γ is large, and generalizes in the context general fashion. [sent-188, score-0.497]
</p><p>81 When considering the implications of Figure 4, it is clear that the model captures the critical feature of the experiment: the ability to learn when to make context speciﬁc generalizations and when not to. [sent-190, score-0.495]
</p><p>82 Inspection of Figure 4 reveals that in the 7  randomized context condition the posterior distribution over γ does not move all that far above the prior median of 3. [sent-193, score-0.588]
</p><p>83 If one were to suppose that people had no inherent prior biases to prefer to generalize one way or the other, it should follow that the less informative condition (i. [sent-196, score-0.3]
</p><p>84 Empirically, the reverse is true: in the less informative condition, all participants generalize in a context general fashion; whereas in the more informative condition (i. [sent-199, score-0.678]
</p><p>85 , systematic context) some but not all participants learn to generalize more narrowly. [sent-201, score-0.357]
</p><p>86 This does not pose any inherent difﬁculty for the model, but it does suggest that the “unbiased” prior chosen for this demonstration is not quite right: people do appear to have strong prior biases to prefer context general representations. [sent-202, score-0.617]
</p><p>87 5 Discussion The hierarchical Bayesian model outlined in this paper explains how human conceptual learning can be context general in some situations, and context sensitive in others. [sent-204, score-1.095]
</p><p>88 This success leads to an interesting question: why does ALCOVE [21] not account for knowledge partitioning (see [4])? [sent-206, score-0.308]
</p><p>89 On the basis of these similarities, one might expect similar behavior from ALCOVE and the context sensitive RMC. [sent-208, score-0.474]
</p><p>90 In ALCOVE, as in many connectionist models, the dimensional biases are chosen to optimize the ability to predict the category label. [sent-211, score-0.277]
</p><p>91 Since the context variable is not correlated with the label in these experiments (by construction), ALCOVE learns to ignore the context variable in all cases. [sent-212, score-0.906]
</p><p>92 The approach taken by the RMC is qualitatively different: it looks for clusters of items where the label, the context and the feature values are all similar to one another. [sent-213, score-0.665]
</p><p>93 Knowledge partitioning experiments more or less require that such clusters exist, so the RMC can learn that the context variable is not distributed randomly. [sent-214, score-0.697]
</p><p>94 In short, ALCOVE treats context as important only if it can predict the label; the RMC treats the context as important if it helps the learner infer the structure of the world. [sent-215, score-0.9]
</p><p>95 If ﬁre ﬁghters observe a very different distribution of ﬁres in the context of back-burns than in the context of to-be-controlled ﬁres, then it should be no surprise that they acquire two distinct theories of “ﬁres”, each bound to a different context. [sent-220, score-0.829]
</p><p>96 Although this particular example is a case in which the learned context speciﬁcity is incorrect, it takes only a minor shift to make the behavior correct. [sent-221, score-0.388]
</p><p>97 If the distinction were between ﬁres observed in a forest context and ﬁres observed in a tyre yard, context speciﬁc category representations suddenly seem very sensible. [sent-223, score-0.978]
</p><p>98 Similarly, social categories such as “polite behavior” are necessarily highly context dependent, so it makes sense that the learner would construct different rules for different contexts. [sent-224, score-0.506]
</p><p>99 If the world presents the learner with observations that vary systematically across contexts, partitioning knowledge by context would seem to be a rational learning strategy. [sent-225, score-0.916]
</p><p>100 Rational approximations to rational models: Alternative algorithms for category learning. [sent-377, score-0.271]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('context', 0.388), ('rmc', 0.367), ('nk', 0.249), ('cluster', 0.232), ('partitioning', 0.215), ('category', 0.202), ('items', 0.183), ('participants', 0.18), ('alcove', 0.171), ('zi', 0.166), ('atrium', 0.122), ('systematic', 0.117), ('city', 0.102), ('clusters', 0.094), ('knowledge', 0.093), ('res', 0.093), ('sensitive', 0.086), ('generalizations', 0.077), ('people', 0.075), ('psychological', 0.075), ('ghters', 0.073), ('lewandowsky', 0.073), ('sanborn', 0.073), ('hierarchical', 0.073), ('rational', 0.069), ('learns', 0.068), ('contexts', 0.067), ('human', 0.066), ('extent', 0.066), ('posterior', 0.065), ('conceptual', 0.064), ('learner', 0.062), ('label', 0.062), ('cognition', 0.062), ('panel', 0.06), ('generalize', 0.06), ('sampler', 0.058), ('learners', 0.058), ('categories', 0.056), ('cognitive', 0.055), ('assignments', 0.055), ('idealized', 0.055), ('psychology', 0.054), ('stimuli', 0.053), ('acquire', 0.053), ('observations', 0.051), ('stimulus', 0.05), ('exemplar', 0.05), ('modules', 0.05), ('rand', 0.05), ('assigned', 0.05), ('insensitive', 0.05), ('bayesian', 0.05), ('condition', 0.05), ('bundles', 0.049), ('unsatisfying', 0.049), ('ith', 0.049), ('solutions', 0.047), ('transfer', 0.047), ('xk', 0.046), ('randomized', 0.046), ('categorization', 0.046), ('panels', 0.045), ('deemed', 0.045), ('infers', 0.044), ('ci', 0.043), ('partition', 0.043), ('biases', 0.043), ('adelaide', 0.043), ('perfors', 0.043), ('sensible', 0.042), ('item', 0.04), ('contradictory', 0.039), ('prior', 0.039), ('beta', 0.039), ('systematically', 0.038), ('differentiated', 0.037), ('parsimonious', 0.037), ('organize', 0.037), ('zn', 0.037), ('memory', 0.037), ('adjusted', 0.036), ('linked', 0.036), ('generalizes', 0.035), ('altering', 0.035), ('prefer', 0.033), ('fashion', 0.033), ('re', 0.032), ('counts', 0.032), ('neutral', 0.032), ('outlines', 0.032), ('metropolis', 0.032), ('connectionist', 0.032), ('editors', 0.032), ('partitions', 0.032), ('treats', 0.031), ('rectangles', 0.031), ('varies', 0.03), ('model', 0.03), ('coded', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="155-tfidf-1" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>2 0.11033624 <a title="155-tfidf-2" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>Author: Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua B. Tenenbaum</p><p>Abstract: We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We extend the inﬁnite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p><p>3 0.10406904 <a title="155-tfidf-3" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>4 0.10300761 <a title="155-tfidf-4" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>Author: Joseph L. Austerweil, Thomas L. Griffiths</p><p>Abstract: Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to deﬁne a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features. 1</p><p>5 0.095149234 <a title="155-tfidf-5" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>Author: Tim Rogers, Chuck Kalish, Joseph Harrison, Xiaojin Zhu, Bryan R. Gibson</p><p>Abstract: When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classiﬁcation in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human’s ability to use a manifold in a semisupervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary. 1</p><p>6 0.092419505 <a title="155-tfidf-6" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>7 0.086750925 <a title="155-tfidf-7" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>8 0.086362042 <a title="155-tfidf-8" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<p>9 0.076882891 <a title="155-tfidf-9" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>10 0.07593789 <a title="155-tfidf-10" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>11 0.075127542 <a title="155-tfidf-11" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>12 0.071962833 <a title="155-tfidf-12" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>13 0.071374021 <a title="155-tfidf-13" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>14 0.069552332 <a title="155-tfidf-14" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>15 0.06760744 <a title="155-tfidf-15" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>16 0.06577006 <a title="155-tfidf-16" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>17 0.06572511 <a title="155-tfidf-17" href="./nips-2010-Efficient_Optimization_for_Discriminative_Latent_Class_Models.html">70 nips-2010-Efficient Optimization for Discriminative Latent Class Models</a></p>
<p>18 0.063543409 <a title="155-tfidf-18" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>19 0.06191396 <a title="155-tfidf-19" href="./nips-2010-Gaussian_Process_Preference_Elicitation.html">100 nips-2010-Gaussian Process Preference Elicitation</a></p>
<p>20 0.060608555 <a title="155-tfidf-20" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.189), (1, 0.051), (2, -0.015), (3, 0.012), (4, -0.082), (5, 0.033), (6, 0.01), (7, -0.013), (8, 0.057), (9, 0.003), (10, 0.049), (11, -0.032), (12, -0.024), (13, -0.072), (14, 0.156), (15, -0.065), (16, -0.011), (17, 0.104), (18, 0.018), (19, -0.005), (20, -0.014), (21, -0.017), (22, 0.117), (23, 0.016), (24, 0.038), (25, 0.132), (26, -0.078), (27, -0.038), (28, -0.052), (29, 0.058), (30, 0.031), (31, 0.04), (32, -0.005), (33, 0.046), (34, -0.086), (35, -0.06), (36, -0.039), (37, 0.079), (38, -0.036), (39, 0.073), (40, -0.073), (41, 0.123), (42, -0.006), (43, 0.003), (44, 0.084), (45, 0.045), (46, 0.014), (47, 0.045), (48, 0.113), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97068375 <a title="155-lsi-1" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>2 0.68447316 <a title="155-lsi-2" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>Author: Ziv Bar-joseph, Hai-son P. Le</p><p>Abstract: Recent studies compare gene expression data across species to identify core and species speciﬁc genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes. 1</p><p>3 0.66126168 <a title="155-lsi-3" href="./nips-2010-Dynamic_Infinite_Relational_Model_for_Time-varying_Relational_Data_Analysis.html">67 nips-2010-Dynamic Infinite Relational Model for Time-varying Relational Data Analysis</a></p>
<p>Author: Katsuhiko Ishiguro, Tomoharu Iwata, Naonori Ueda, Joshua B. Tenenbaum</p><p>Abstract: We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We extend the inﬁnite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p><p>4 0.56535894 <a title="155-lsi-4" href="./nips-2010-Discriminative_Clustering_by_Regularized_Information_Maximization.html">62 nips-2010-Discriminative Clustering by Regularized Information Maximization</a></p>
<p>Author: Andreas Krause, Pietro Perona, Ryan G. Gomes</p><p>Abstract: Is there a principled way to learn a probabilistic discriminative classiﬁer from an unlabeled data set? We present a framework that simultaneously clusters the data and trains a discriminative classiﬁer. We call it Regularized Information Maximization (RIM). RIM optimizes an intuitive information-theoretic objective function which balances class separation, class balance and classiﬁer complexity. The approach can ﬂexibly incorporate different likelihood functions, express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning. In particular, we instantiate the framework to unsupervised, multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets, and demonstrates that RIM is an effective model selection method. 1</p><p>5 0.54959333 <a title="155-lsi-5" href="./nips-2010-Improvements_to_the_Sequence_Memoizer.html">120 nips-2010-Improvements to the Sequence Memoizer</a></p>
<p>Author: Jan Gasthaus, Yee W. Teh</p><p>Abstract: The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efﬁcient representation, and inference algorithms operating on the new representation. Our derivations are based on precise deﬁnitions of the various processes that will also allow us to provide an elementary proof of the “mysterious” coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements. 1</p><p>6 0.52074647 <a title="155-lsi-6" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>7 0.51083171 <a title="155-lsi-7" href="./nips-2010-Switching_state_space_model_for_simultaneously_estimating_state_transitions_and_nonstationary_firing_rates.html">263 nips-2010-Switching state space model for simultaneously estimating state transitions and nonstationary firing rates</a></p>
<p>8 0.48530492 <a title="155-lsi-8" href="./nips-2010-Supervised_Clustering.html">261 nips-2010-Supervised Clustering</a></p>
<p>9 0.48321518 <a title="155-lsi-9" href="./nips-2010-Rates_of_convergence_for_the_cluster_tree.html">223 nips-2010-Rates of convergence for the cluster tree</a></p>
<p>10 0.46339896 <a title="155-lsi-10" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>11 0.45729923 <a title="155-lsi-11" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>12 0.4413408 <a title="155-lsi-12" href="./nips-2010-Inter-time_segment_information_sharing_for_non-homogeneous_dynamic_Bayesian_networks.html">129 nips-2010-Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks</a></p>
<p>13 0.4400087 <a title="155-lsi-13" href="./nips-2010-Towards_Property-Based_Classification_of_Clustering_Paradigms.html">273 nips-2010-Towards Property-Based Classification of Clustering Paradigms</a></p>
<p>14 0.43247697 <a title="155-lsi-14" href="./nips-2010-Robust_Clustering_as_Ensembles_of_Affinity_Relations.html">230 nips-2010-Robust Clustering as Ensembles of Affinity Relations</a></p>
<p>15 0.43154597 <a title="155-lsi-15" href="./nips-2010-Humans_Learn_Using_Manifolds%2C_Reluctantly.html">114 nips-2010-Humans Learn Using Manifolds, Reluctantly</a></p>
<p>16 0.42286178 <a title="155-lsi-16" href="./nips-2010-Probabilistic_Deterministic_Infinite_Automata.html">215 nips-2010-Probabilistic Deterministic Infinite Automata</a></p>
<p>17 0.40923724 <a title="155-lsi-17" href="./nips-2010-A_Bayesian_Approach_to_Concept_Drift.html">2 nips-2010-A Bayesian Approach to Concept Drift</a></p>
<p>18 0.40349862 <a title="155-lsi-18" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>19 0.40252891 <a title="155-lsi-19" href="./nips-2010-Effects_of_Synaptic_Weight_Diffusion_on_Learning_in_Decision_Making_Networks.html">68 nips-2010-Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks</a></p>
<p>20 0.38497829 <a title="155-lsi-20" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.043), (17, 0.029), (27, 0.11), (30, 0.117), (35, 0.027), (45, 0.197), (50, 0.078), (52, 0.034), (60, 0.05), (69, 0.145), (77, 0.056), (90, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89055645 <a title="155-lda-1" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>Author: Dan Navarro</p><p>Abstract: This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is ﬁt to multiple data sets, and provides a parsimonious method for describing how humans learn context speciﬁc conceptual representations.</p><p>2 0.85100806 <a title="155-lda-2" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>3 0.84420484 <a title="155-lda-3" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>Author: Samuel Gershman, Robert Wilson</p><p>Abstract: Optimal control entails combining probabilities and utilities. However, for most practical problems, probability densities can be represented only approximately. Choosing an approximation requires balancing the beneﬁts of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a neural population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive ﬁelds, GABAergic effects on saccadic movements, and risk aversion in decisions under uncertainty. 1</p><p>4 0.84408039 <a title="155-lda-4" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>Author: Hongbo Zhou, Qiang Cheng</p><p>Abstract: Regularization technique has become a principled tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further give mathematically exact deﬁnition for a novel representation called sparse grouping representation (SGR), and prove a set of sufﬁcient conditions for generating such group level sparsity. Under these sufﬁcient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also provide some generalization bounds in a classiﬁcation setting. 1</p><p>5 0.84401679 <a title="155-lda-5" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>Author: Surya Ganguli, Haim Sompolinsky</p><p>Abstract: Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of “orthogonal” recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance. 1</p><p>6 0.84264648 <a title="155-lda-6" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>7 0.83363569 <a title="155-lda-7" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>8 0.83240813 <a title="155-lda-8" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>9 0.83140451 <a title="155-lda-9" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>10 0.83118463 <a title="155-lda-10" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>11 0.83103609 <a title="155-lda-11" href="./nips-2010-Optimal_Bayesian_Recommendation_Sets_and_Myopically_Optimal_Choice_Query_Sets.html">197 nips-2010-Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets</a></p>
<p>12 0.82954097 <a title="155-lda-12" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>13 0.82749575 <a title="155-lda-13" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>14 0.82732832 <a title="155-lda-14" href="./nips-2010-Estimation_of_Renyi_Entropy_and_Mutual_Information_Based_on_Generalized_Nearest-Neighbor_Graphs.html">80 nips-2010-Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs</a></p>
<p>15 0.82664585 <a title="155-lda-15" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<p>16 0.82575089 <a title="155-lda-16" href="./nips-2010-Cross_Species_Expression_Analysis_using_a_Dirichlet_Process_Mixture_Model_with_Latent_Matchings.html">55 nips-2010-Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings</a></p>
<p>17 0.8251617 <a title="155-lda-17" href="./nips-2010-Learning_Networks_of_Stochastic_Differential_Equations.html">148 nips-2010-Learning Networks of Stochastic Differential Equations</a></p>
<p>18 0.82505888 <a title="155-lda-18" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>19 0.82385123 <a title="155-lda-19" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>20 0.82337946 <a title="155-lda-20" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
