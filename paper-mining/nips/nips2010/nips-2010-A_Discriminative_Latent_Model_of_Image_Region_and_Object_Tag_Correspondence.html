<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-6" href="#">nips2010-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</h1>
<br/><p>Source: <a title="nips-2010-6-pdf" href="http://papers.nips.cc/paper/3968-a-discriminative-latent-model-of-image-region-and-object-tag-correspondence.pdf">pdf</a></p><p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>Reference: <a title="nips-2010-6-reference" href="../nips2010_reference/nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. [sent-4, score-0.556]
</p><p>2 Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. [sent-5, score-0.701]
</p><p>3 In particular, we model the mapping that translates image regions to annotations. [sent-6, score-0.371]
</p><p>4 This mapping allows us to relate image regions to their corresponding annotation terms. [sent-7, score-0.927]
</p><p>5 We also model the overall scene label as latent information. [sent-8, score-0.373]
</p><p>6 Others go beyond single labels and assign a list of annotations to an image [1, 10, 21]. [sent-16, score-0.411]
</p><p>7 In this paper, we consider the problem of image understanding with unaligned textual annotations. [sent-20, score-0.555]
</p><p>8 The input to our learning algorithm is a set of images with unaligned textual annotations (object names). [sent-22, score-0.599]
</p><p>9 Our goal is to learn a model to predict the annotation (i. [sent-23, score-0.556]
</p><p>10 The main contribution of this paper is the development of a model that incorporates this object annotation to image region correspondence in a discriminative framework. [sent-28, score-0.924]
</p><p>11 In the computer vision literature, there has been a lot of work on exploiting images and their associated textual information. [sent-29, score-0.46]
</p><p>12 [1] predict words associated with whole images or regions by learning a joint distribution of image regions and words. [sent-31, score-0.541]
</p><p>13 scene classiﬁcation, object recognition, image annotation, and image segmentation. [sent-40, score-0.666]
</p><p>14 1  mallet athlete horse ground tree (a)  (b)  (c)  Figure 1: Our goal is to learn a model using images and their associated unaligned textual object annotations (a) as the training data. [sent-42, score-0.841]
</p><p>15 Given a new image (b), we can use the model to predict its textual annotations and roughly localize image regions corresponding to each of the annotation terms (c). [sent-43, score-1.499]
</p><p>16 Most of the previous work uses fairly crude “bag-of-words” models, treating image features (extracted from either segmented regions or local interest points) and textual annotations as unordered entities and looking at their co-occurrence statistics. [sent-44, score-0.74]
</p><p>17 Very little work explicitly models more detailed relationships between image regions and annotations that are obvious to humans. [sent-45, score-0.537]
</p><p>18 In this paper, we present a discriminative latent model that captures image regions, textual annotations, mappings between visual and textual information, and overall scene labels in a more explicit manner. [sent-48, score-1.194]
</p><p>19 However due to the limitation of the machine learning tools used in those work, they did not properly enforce the aforementioned constraint on how image regions are mapped to annotations. [sent-50, score-0.358]
</p><p>20 In that work, they learn to annotate and segment images by mapping image regions and textual words to a latent meaning space using context and adjective features. [sent-55, score-0.825]
</p><p>21 First of all, the input to [17] is a set of images (a handful of which are manually labeled) of a single sport category, and a collection of news articles for that sport. [sent-57, score-0.365]
</p><p>22 Although they have experimented on applying their model on image collections with mixed sport categories, their method seems to work better with single sport category training. [sent-59, score-0.68]
</p><p>23 In contrast, the input to our learning problem is a set of images from several sport categories, together with their associated textual annotations. [sent-60, score-0.594]
</p><p>24 We treat the sport category as a latent variable (we call it the scene label) and implicitly infer it during learning. [sent-61, score-0.552]
</p><p>25 2 Model We propose a discriminative latent model that jointly captures the relationships between image segments, textual annotations, region-text correspondence, and overall image visual scene labels. [sent-62, score-1.168]
</p><p>26 Of course, only the image segments and textual annotations are observed on training data. [sent-63, score-0.691]
</p><p>27 scene labels, the mapping between regions and annotations) are treated as latent variables in the model. [sent-66, score-0.468]
</p><p>28 The input to our learning module is a set of x, y pairs where x denotes an image, and y denotes the annotation associated with this image. [sent-69, score-0.583]
</p><p>29 The annotation y of an image is represented as a binary vector y = (y1 , y2 , . [sent-80, score-0.759]
</p><p>30 , yV ), where V is the total number of possible annotation terms. [sent-83, score-0.556]
</p><p>31 An annotation 2  image regions  dog  athlete 1  car  horse 1  . [sent-85, score-1.031]
</p><p>32 0  0  annotation  Figure 2: Graphical illustration of our model. [sent-91, score-0.556]
</p><p>33 The annotation of the image is represented as a 0-1 vector indicating the presence/absence of each possible annotation term. [sent-93, score-1.315]
</p><p>34 Our model captures the unobserved mapping that translate image regions to annotation terms associated with the image (e. [sent-94, score-1.216]
</p><p>35 For annotation terms not associated with the image (e. [sent-97, score-0.786]
</p><p>36 Our model also captures relationship between the unobserved scene label (e. [sent-100, score-0.348]
</p><p>37 We further assume the number of regions of an image is larger than or equal to the number of active V annotation terms for an image, i. [sent-104, score-0.89]
</p><p>38 In this work, we assume there are no visually irrelevant annotation terms (e. [sent-107, score-0.556]
</p><p>39 These can be achieved by pre-processing the annotation terms with Wordnet (see [17]). [sent-112, score-0.556]
</p><p>40 Given an image x and its annotation y, we assume there is an underlying unobserved many-to-one mapping which translates R image regions to each of the active annotation terms. [sent-113, score-1.741]
</p><p>41 We restrict the mapping to have the following conditions: (i) each image region is mapped to at most one annotation term. [sent-114, score-0.901]
</p><p>42 This condition will ensure that an image region is not used to explain two different annotations; (ii) an active annotation term has one or more image regions mapped to it. [sent-115, score-1.199]
</p><p>43 This condition will make sure that if an annotation term (say “building”) is assigned to an image, there is at least one image region supporting this annotation term; (iii) an inactive annotation term has no image regions mapped to it. [sent-116, score-2.369]
</p><p>44 This condition will guarantee there are no image regions supporting an inactive annotation term. [sent-117, score-0.924]
</p><p>45 For a ﬁxed annotation y, we use Z(y) to denote the set of all possible many-to-one mappings that satisfy the conditions (i,ii,iii). [sent-119, score-0.556]
</p><p>46 It is easy to verify that any z ∈ Z(y) can be represented using the following three sets of constraints: zij ≤ 1, ∀i; j  max zij = yj , ∀j; i  zij ∈ {0, 1}, ∀i, ∀j  (2)  For a given image, we also assume a discrete unobserved “scene” label s which takes its value between 1 and S. [sent-120, score-1.6]
</p><p>47 We introduce the scene label to capture the fact that the annotations of images are typically well clustered according to their underlying scenes. [sent-121, score-0.566]
</p><p>48 For example, an image of a “sailing” scene tends to have annotation terms like “athlete”, “sailboat”, “water”, etc. [sent-122, score-0.975]
</p><p>49 In our work, we treat the scene label as a latent variable (hence we do not need its ground-truth label or even a vocabulary for deﬁning it) and let the learning algorithm automatically ﬁgure out what constitutes a scene. [sent-124, score-0.446]
</p><p>50 Inspired by the latent SVM [7, 25], we measure the compatibility between an image x and an annotation y using the following scoring function: fθ (x, y) = max max θ⊤ · Φ(x, y, z, s) s∈S z∈Z(y)  (3)  where θ are the model parameters and Φ(x, y, z, s) is a feature vector deﬁned on x, y, z and s. [sent-128, score-1.024]
</p><p>51 Region-Annotation Matching Potential α⊤ φ(x, z): This potential function measures the compatibility of mapping image regions to their corresponding annotation terms. [sent-130, score-1.057]
</p><p>52 Each αc is a matrix of c=1 size Nc × V , where an entry αc can be interpreted as the compatibility between the codeword w w,j (1 ≤ w ≤ Nc ) of feature type c and the annotation term j (1 ≤ j ≤ V ). [sent-134, score-0.742]
</p><p>53 The potential function is written as: 4  R  4  V  R  Nc  V  αc · 1(xic = w) · zij w,j  αc ic ,j · zij = x  α⊤ φ(x, z) =  (5)  c=1 i=1 w=1 j=1  c=1 i=1 j=1  where 1(·) is the indicator function. [sent-135, score-0.815]
</p><p>54 Image-Scene Potential β ⊤ ψ(x, s): This potential function measures the compatibility between an image x and a scene label s. [sent-139, score-0.622]
</p><p>55 Similarly, the parameters β consist of four parts β = {β c }4 correc=1 c sponding to the four feature types, where an entry βw,s is the compatibility between the codeword w of type c and the scene label s. [sent-140, score-0.477]
</p><p>56 This potential function is written as: 4  4  R c βxic ,s  ⊤  β ψ(x, s) =  R  Nc  S c βw,t · 1(xic = w) · 1(s = t)  =  (6)  c=1 i=1 w=1 t=1  c=1 i=1  Annotation-Scene Potential γ ⊤ ϕ(y, s): This potential function measures the compatibility between an annotation y and a scene label s. [sent-141, score-1.03]
</p><p>57 Each component γ t is a V × 2 matrix, where γj,1 is the t compatibility of setting yj = 1 for the scene label t, and γj,0 is the compatibility of setting yj = 0 for the scene label t. [sent-143, score-1.33]
</p><p>58 3 Inference Given the model parameters θ = {α, β, γ}, the inference problem is to ﬁnd the best annotation y∗ for a new image x, i. [sent-145, score-0.759]
</p><p>59 The inference requires solving the following optimization problem: max fθ (x, y) = max max max θ⊤ Φ(x, y, z, s) y∈Y  s∈S y∈Y z∈Z(y)  (8)  Since we can enumerate all the possible values of the scene label s, the main difﬁculty of solving (8) is the inner maximization over y and z for a ﬁxed s, i. [sent-148, score-0.427]
</p><p>60 aij zij +  max  (11)  The optimization problem (11) is not convex. [sent-153, score-0.454]
</p><p>61 First we reformulate (11) as an integer linear program (ILP): y,z  zij , zij ∈ {0, 1}, yj ∈ {0, 1}, ∀i ∀j (12) i  j  j  i,j  zij ≤ 1, zij ≤ yj ≤  bj yj s. [sent-155, score-2.406]
</p><p>62 aij zij +  max  It is easy to verify that (11) and (12) are equivalent. [sent-157, score-0.501]
</p><p>63 Of course, (12) still has the integral constraint zij ∈ {0, 1}, which makes the optimization problem NP-hard. [sent-158, score-0.38]
</p><p>64 So we further relax the value of zij to a real value in the range of [0, 1]. [sent-159, score-0.401]
</p><p>65 Putting everything together, the LP relaxation of (11) can be written as: y,z  i,j  i  j  j  zij , 0 ≤ zij ≤ 1, 0 ≤ yj ≤ 1, ∀i ∀j (13)  zij ≤ 1, zij ≤ yj ≤  bj yj s. [sent-160, score-2.428]
</p><p>66 aij zij +  max  After solving (13) with any LP solver, we round zij to the closest integer and obtain yj as yj = maxi zij . [sent-162, score-1.772]
</p><p>67 We do not have the ground-truth scene label s or the mapping z for any of the training images, so we have to treat them as latent variables during learning. [sent-168, score-0.462]
</p><p>68 fθ (xn , yn ) − fθ (xn , y) ≥ ∆(y, yn ) − ξn , ∀n, ∀y  (14)  where ∆(y, yn ) is a loss function measuring the cost incurred by predicting y when the groundtruth annotation is yn . [sent-176, score-0.856]
</p><p>69 We use a simple Hamming loss which decomposes as ∆(y, yn ) = V n n n j=1 ℓ(yj , yj ), where ℓ(yj , yj ) is 1 if yj = yj and 0 otherwise. [sent-177, score-1.191]
</p><p>70 Note that our loss function only involves the annotation y, because this is the only ground-truth label we have access to. [sent-178, score-0.629]
</p><p>71 For a ﬁxed s, it is easy to show that the maximization over z can be solved by the following ILP: z  n zij = yj , ∀i; zij ∈ {0, 1}, ∀i ∀j  aij zij , s. [sent-193, score-1.489]
</p><p>72 max i,j  (19)  j  Similarly, we can solve (19) via LP relaxation by replacing the integral constraint zij ∈ {0, 1} with a linear constraint 0 ≤ zij ≤ 1. [sent-195, score-0.789]
</p><p>73 It contains images collected from eight sport classes: badminton, bocce, croquet, polo, rock climbing, rowing, sailing and snowboarding. [sent-197, score-0.407]
</p><p>74 We remove annotation terms occurring fewer than three times. [sent-199, score-0.556]
</p><p>75 We feed the training images and associated annotations (but not the ground-truth sport category labels) to our learning algorithm and set the number of latent scene labels to be eight (i. [sent-203, score-0.972]
</p><p>76 For each training image, we construct a feature vector from the visual information of the image itself and the textual information of its annotation. [sent-208, score-0.594]
</p><p>77 The visual information is simply the concatenation of visual word counts from all the regions in the image (normalized between 0 and 1), i. [sent-209, score-0.511]
</p><p>78 We then run k-means clustering based on the combined visual and textual features to cluster training images into eight clusters. [sent-215, score-0.613]
</p><p>79 We use the cluster membership of each training image as the initial guess of the scene label s (which we call pseudo-scene label). [sent-216, score-0.567]
</p><p>80 Similarly, we initialize the parameters γ by the co-occurrence counts of annotation terms and pseudo-scene labels. [sent-218, score-0.578]
</p><p>81 The parameters α are initialized by the co-occurrence counts of visual words and annotation terms with the mapping constraints ignored. [sent-219, score-0.727]
</p><p>82 We compare our model with a baseline method which is a set of linear SVMs separately trained for predicting the 0/1 output of each annotation term based on the feature vector from the visual information. [sent-220, score-0.746]
</p><p>83 Following [21], we use the F-measure to measure the annotation performance. [sent-221, score-0.556]
</p><p>84 The scene labels s produced by our model for the test images can be considered as a clustering of the scenes in those images. [sent-228, score-0.412]
</p><p>85 athlete  ceiling  ﬂoor  grass  rowboat sailboat  sky  sun  tree  water  Figure 4: Visualization of the “position” components of the α parameters for some annotation terms. [sent-233, score-0.742]
</p><p>86 For the third baseline algorithm (which we call pseudo-annotation+K-means), we ﬁrst train separate SVM classiﬁers to predict the annotation from the visual feature, using the ground-truth annotations of the validation set to choose the free parameters in SVM classiﬁers. [sent-249, score-0.899]
</p><p>87 Then we run k-means to cluster those test images based on both visual features and textual features. [sent-251, score-0.477]
</p><p>88 For a particular s scene label s, the parameter γj,1 measures the compatibility of setting the j-th annotation term s active for the scene label s. [sent-267, score-1.278]
</p><p>89 In Fig 3, we visualize the top ﬁve annotation terms for each of the eight possible values of s. [sent-269, score-0.662]
</p><p>90 Intuitively, these eight scene clusters obtained from our model seem to match well to the eight different sport categories of this dataset. [sent-270, score-0.591]
</p><p>91 For a particular annotation term j, we ﬁnd the most preferred “position” visual word w∗ for this annotation term by w∗ = arg maxw α4 . [sent-275, score-1.27]
</p><p>92 2227  Table 1: Comparison of image annotation (a) and scene clustering (b). [sent-288, score-1.021]
</p><p>93 Figure 5: (Best viewed in color) Results of annotation and segmentation on the UIUC sport dataset. [sent-291, score-0.809]
</p><p>94 Image regions mapped to an annotation term are overlayed with the color corresponding to that annotation term. [sent-293, score-1.291]
</p><p>95 6 Conclusion We have presented a discriminatively trained latent model for capturing the relationships among image regions, textual annotations, and overall scenes. [sent-294, score-0.578]
</p><p>96 the mapping between images regions and annotation terms) are hard to model, hence largely ignored in the generative approaches. [sent-302, score-0.842]
</p><p>97 There are many open issues to address in future research: (1) extending our model to handle a richer set of annotation terms (nouns, verbs, adjectives, etc) by modifying the many-to-one correspondence assumption. [sent-308, score-0.587]
</p><p>98 (2) exploring the use of this model with noisier annotation data (e. [sent-309, score-0.556]
</p><p>99 Towards total scene understanding: Classiﬁcation, annotation and segmentation in an automatic framework. [sent-403, score-0.8]
</p><p>100 Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. [sent-432, score-0.754]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('annotation', 0.556), ('zij', 0.38), ('yj', 0.279), ('textual', 0.247), ('sport', 0.225), ('scene', 0.216), ('image', 0.203), ('annotations', 0.182), ('regions', 0.108), ('athlete', 0.103), ('compatibility', 0.097), ('images', 0.095), ('visual', 0.089), ('xic', 0.085), ('latent', 0.084), ('nc', 0.079), ('nmi', 0.075), ('unaligned', 0.075), ('yn', 0.075), ('label', 0.073), ('svm', 0.065), ('eight', 0.061), ('mapping', 0.06), ('lp', 0.058), ('vision', 0.057), ('discriminative', 0.055), ('baseline', 0.051), ('names', 0.051), ('bj', 0.049), ('mapped', 0.047), ('clustering', 0.046), ('cluster', 0.046), ('visualize', 0.045), ('aij', 0.045), ('news', 0.045), ('object', 0.044), ('relationships', 0.044), ('codewords', 0.043), ('ilp', 0.041), ('wang', 0.04), ('barnard', 0.039), ('codeword', 0.039), ('horse', 0.039), ('inactive', 0.037), ('region', 0.035), ('recognition', 0.034), ('loeff', 0.034), ('lsvms', 0.034), ('uiuc', 0.034), ('youtube', 0.034), ('computer', 0.034), ('potential', 0.033), ('xn', 0.033), ('unobserved', 0.032), ('water', 0.031), ('correspondence', 0.031), ('socher', 0.03), ('understanding', 0.03), ('segments', 0.03), ('sailboat', 0.03), ('ln', 0.03), ('max', 0.029), ('berg', 0.029), ('training', 0.029), ('scenes', 0.029), ('categories', 0.028), ('segmentation', 0.028), ('annotate', 0.028), ('fraser', 0.028), ('polo', 0.028), ('society', 0.027), ('captures', 0.027), ('category', 0.027), ('associated', 0.027), ('labels', 0.026), ('consist', 0.026), ('sailing', 0.026), ('feature', 0.026), ('easy', 0.025), ('mori', 0.024), ('pattern', 0.024), ('term', 0.024), ('linguistic', 0.023), ('pictures', 0.023), ('active', 0.023), ('generative', 0.023), ('counts', 0.022), ('tags', 0.022), ('verify', 0.022), ('written', 0.022), ('dog', 0.022), ('sky', 0.022), ('enumerate', 0.022), ('etc', 0.022), ('validation', 0.021), ('preferred', 0.021), ('relax', 0.021), ('yang', 0.021), ('simon', 0.02), ('supporting', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="6-tfidf-1" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>2 0.26446295 <a title="6-tfidf-2" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>3 0.24662259 <a title="6-tfidf-3" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>4 0.18395422 <a title="6-tfidf-4" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>5 0.15027778 <a title="6-tfidf-5" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>Author: Congcong Li, Adarsh Kowdle, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classiﬁers already exist for many subtasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classiﬁer. We propose Feedback Enabled Cascaded Classiﬁcation Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classiﬁer for each sub-task. We use a two-layer cascade of classiﬁers, which are repeated instantiations of the original ones, with the output of the ﬁrst layer fed into the second layer as input. Our training method involves a feedback step that allows later classiﬁers to provide earlier classiﬁers information about what error modes to focus on. We show that our method signiﬁcantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classiﬁcation. 1</p><p>6 0.14431195 <a title="6-tfidf-6" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>7 0.13349776 <a title="6-tfidf-7" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>8 0.12856384 <a title="6-tfidf-8" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>9 0.12641785 <a title="6-tfidf-9" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>10 0.12554112 <a title="6-tfidf-10" href="./nips-2010-Transduction_with_Matrix_Completion%3A_Three_Birds_with_One_Stone.html">275 nips-2010-Transduction with Matrix Completion: Three Birds with One Stone</a></p>
<p>11 0.11811727 <a title="6-tfidf-11" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>12 0.11663887 <a title="6-tfidf-12" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>13 0.11521532 <a title="6-tfidf-13" href="./nips-2010-Label_Embedding_Trees_for_Large_Multi-Class_Tasks.html">135 nips-2010-Label Embedding Trees for Large Multi-Class Tasks</a></p>
<p>14 0.11210594 <a title="6-tfidf-14" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>15 0.10890717 <a title="6-tfidf-15" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>16 0.10785339 <a title="6-tfidf-16" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>17 0.090347268 <a title="6-tfidf-17" href="./nips-2010-Reverse_Multi-Label_Learning.html">228 nips-2010-Reverse Multi-Label Learning</a></p>
<p>18 0.082415387 <a title="6-tfidf-18" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>19 0.079694971 <a title="6-tfidf-19" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>20 0.077745073 <a title="6-tfidf-20" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, 0.122), (2, -0.143), (3, -0.296), (4, -0.061), (5, -0.055), (6, -0.078), (7, -0.005), (8, 0.076), (9, -0.002), (10, -0.024), (11, 0.084), (12, -0.033), (13, 0.077), (14, 0.079), (15, 0.013), (16, 0.089), (17, -0.039), (18, 0.209), (19, 0.044), (20, 0.078), (21, -0.003), (22, -0.03), (23, 0.022), (24, -0.039), (25, -0.006), (26, -0.007), (27, -0.046), (28, -0.011), (29, -0.043), (30, 0.095), (31, 0.017), (32, -0.023), (33, 0.087), (34, -0.025), (35, 0.032), (36, 0.038), (37, -0.046), (38, 0.066), (39, 0.124), (40, -0.007), (41, -0.028), (42, -0.088), (43, 0.013), (44, 0.019), (45, -0.001), (46, -0.079), (47, 0.06), (48, -0.026), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96662897 <a title="6-lsi-1" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth regionto-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p><p>2 0.81808221 <a title="6-lsi-2" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>Author: Matthew Blaschko, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images [17]. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs [14] and the PASCAL VOC dataset [17], and it is shown that for a signiﬁcant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results. 1</p><p>3 0.7267437 <a title="6-lsi-3" href="./nips-2010-Large_Margin_Learning_of_Upstream_Scene_Understanding_Models.html">137 nips-2010-Large Margin Learning of Upstream Scene Understanding Models</a></p>
<p>Author: Jun Zhu, Li-jia Li, Li Fei-fei, Eric P. Xing</p><p>Abstract: Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classiﬁcation. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efﬁciently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p><p>4 0.69673473 <a title="6-lsi-4" href="./nips-2010-Learning_To_Count_Objects_in_Images.html">149 nips-2010-Learning To Count Objects in Images</a></p>
<p>Author: Victor Lempitsky, Andrew Zisserman</p><p>Abstract: We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efﬁciently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very ﬂexible as it can accept any domain-speciﬁc visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data. 1</p><p>5 0.6903078 <a title="6-lsi-5" href="./nips-2010-The_Multidimensional_Wisdom_of_Crowds.html">267 nips-2010-The Multidimensional Wisdom of Crowds</a></p>
<p>Author: Peter Welinder, Steve Branson, Pietro Perona, Serge J. Belongie</p><p>Abstract: Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We ﬁnd that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different “schools of thought” amongst the annotators, and can group together images belonging to separate categories. 1</p><p>6 0.68089527 <a title="6-lsi-6" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>7 0.67483407 <a title="6-lsi-7" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>8 0.66396761 <a title="6-lsi-8" href="./nips-2010-Space-Variant_Single-Image_Blind_Deconvolution_for_Removing_Camera_Shake.html">245 nips-2010-Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake</a></p>
<p>9 0.66184783 <a title="6-lsi-9" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>10 0.66010809 <a title="6-lsi-10" href="./nips-2010-Structural_epitome%3A_a_way_to_summarize_one%E2%80%99s_visual_experience.html">256 nips-2010-Structural epitome: a way to summarize one’s visual experience</a></p>
<p>11 0.64691085 <a title="6-lsi-11" href="./nips-2010-Predictive_Subspace_Learning_for_Multi-view_Data%3A_a_Large_Margin_Approach.html">213 nips-2010-Predictive Subspace Learning for Multi-view Data: a Large Margin Approach</a></p>
<p>12 0.56209713 <a title="6-lsi-12" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>13 0.55632997 <a title="6-lsi-13" href="./nips-2010-Exploiting_weakly-labeled_Web_images_to_improve_object_classification%3A_a_domain_adaptation_approach.html">86 nips-2010-Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach</a></p>
<p>14 0.54717159 <a title="6-lsi-14" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>15 0.54353672 <a title="6-lsi-15" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>16 0.52770346 <a title="6-lsi-16" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>17 0.50194055 <a title="6-lsi-17" href="./nips-2010-Epitome_driven_3-D_Diffusion_Tensor_image_segmentation%3A_on_extracting_specific_structures.html">77 nips-2010-Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures</a></p>
<p>18 0.49469793 <a title="6-lsi-18" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>19 0.49233836 <a title="6-lsi-19" href="./nips-2010-Segmentation_as_Maximum-Weight_Independent_Set.html">234 nips-2010-Segmentation as Maximum-Weight Independent Set</a></p>
<p>20 0.49082208 <a title="6-lsi-20" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.029), (17, 0.013), (27, 0.4), (30, 0.092), (35, 0.013), (45, 0.185), (50, 0.024), (60, 0.03), (77, 0.02), (78, 0.028), (90, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98339194 <a title="6-lda-1" href="./nips-2010-Implicit_encoding_of_prior_probabilities_in_optimal_neural_populations.html">119 nips-2010-Implicit encoding of prior probabilities in optimal neural populations</a></p>
<p>Author: Deep Ganguli, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>2 0.97783196 <a title="6-lda-2" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>Author: Morten Mørup, Kristoffer Madsen, Anne-marie Dogonowski, Hartwig Siebner, Lars K. Hansen</p><p>Abstract: Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the deﬁnition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the inﬁnite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects. 1</p><p>3 0.96671379 <a title="6-lda-3" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>Author: Issei Sato, Kenichi Kurihara, Hiroshi Nakagawa</p><p>Abstract: We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p><p>4 0.95624667 <a title="6-lda-4" href="./nips-2010-Improving_Human_Judgments_by_Decontaminating_Sequential_Dependencies.html">121 nips-2010-Improving Human Judgments by Decontaminating Sequential Dependencies</a></p>
<p>Author: Harold Pashler, Matthew Wilder, Robert Lindsey, Matt Jones, Michael C. Mozer, Michael P. Holmes</p><p>Abstract: For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are inﬂuenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random ﬁeld with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments. 1</p><p>5 0.93979299 <a title="6-lda-5" href="./nips-2010-Bayesian_Action-Graph_Games.html">39 nips-2010-Bayesian Action-Graph Games</a></p>
<p>Author: Albert X. Jiang, Kevin Leyton-brown</p><p>Abstract: Games of incomplete information, or Bayesian games, are an important gametheoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-speciﬁc utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time. Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve signiﬁcantly on the state of the art. 1</p><p>6 0.91813636 <a title="6-lda-6" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>7 0.87302095 <a title="6-lda-7" href="./nips-2010-Evaluating_neuronal_codes_for_inference_using_Fisher_information.html">81 nips-2010-Evaluating neuronal codes for inference using Fisher information</a></p>
<p>same-paper 8 0.855528 <a title="6-lda-8" href="./nips-2010-A_Discriminative_Latent_Model_of_Image_Region_and_Object_Tag_Correspondence.html">6 nips-2010-A Discriminative Latent Model of Image Region and Object Tag Correspondence</a></p>
<p>9 0.84958506 <a title="6-lda-9" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>10 0.8298291 <a title="6-lda-10" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<p>11 0.80367482 <a title="6-lda-11" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>12 0.78899217 <a title="6-lda-12" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>13 0.78749001 <a title="6-lda-13" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>14 0.78263903 <a title="6-lda-14" href="./nips-2010-Functional_Geometry_Alignment_and_Localization_of_Brain_Areas.html">97 nips-2010-Functional Geometry Alignment and Localization of Brain Areas</a></p>
<p>15 0.78165072 <a title="6-lda-15" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>16 0.76553082 <a title="6-lda-16" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>17 0.7641294 <a title="6-lda-17" href="./nips-2010-A_rational_decision_making_framework_for_inhibitory_control.html">19 nips-2010-A rational decision making framework for inhibitory control</a></p>
<p>18 0.75456876 <a title="6-lda-18" href="./nips-2010-Individualized_ROI_Optimization_via_Maximization_of_Group-wise_Consistency_of_Structural_and_Functional_Profiles.html">123 nips-2010-Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles</a></p>
<p>19 0.74905777 <a title="6-lda-19" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>20 0.74401981 <a title="6-lda-20" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
