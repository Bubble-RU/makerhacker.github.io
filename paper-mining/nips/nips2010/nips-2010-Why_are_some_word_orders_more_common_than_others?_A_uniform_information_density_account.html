<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>285 nips-2010-Why are some word orders more common than others? A uniform information density account</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-285" href="#">nips2010-285</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>285 nips-2010-Why are some word orders more common than others? A uniform information density account</h1>
<br/><p>Source: <a title="nips-2010-285-pdf" href="http://papers.nips.cc/paper/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account.pdf">pdf</a></p><p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>Reference: <a title="nips-2010-285-reference" href="../nips2010_reference/nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Why are some word orders more common than others? [sent-1, score-0.605]
</p><p>2 au  Abstract Languages vary widely in many ways, including their canonical word order. [sent-7, score-0.461]
</p><p>3 A basic aspect of the observed variation is the fact that some word orders are much more common than others. [sent-8, score-0.637]
</p><p>4 1  Introduction  Many of the world’s languages are sensitive to word order. [sent-13, score-0.635]
</p><p>5 The so-called “basic” word order of a language is deﬁned according to the order of three of the principal components of basic transitive sentences: subject (S), verb (V) and object (O). [sent-16, score-0.754]
</p><p>6 This results in six logically distinct word orders: SOV, SVO, VSO, VOS, OVS and OSV (e. [sent-17, score-0.488]
</p><p>7 According to a survey of 402 languages [17], the majority of languages are either SOV (44. [sent-21, score-0.348]
</p><p>8 One of the most straightforward explanations is that the observed word order frequencies may be the consequence of genetically encoded biases toward particular orders, as part of the universal grammar hypothesis; this possibility is considered in [4]. [sent-32, score-0.519]
</p><p>9 A similar objection can be made against the proposal that all languages which are alive today descend from a single common ancestor, and that this proto–language used SOV word order [8], ex1  plaining the observation that SOV is the most common word order today. [sent-35, score-1.15]
</p><p>10 If there is nothing special about SOV, why has random drift (this time in language evolution, not human genetic evolution) not more signiﬁcantly changed the word order distribution from its ancient form? [sent-36, score-0.609]
</p><p>11 Furthermore, it is clear that ancient SOV languages must have changed into SVO languages much more frequently into than, say, VOS languages in order to arrive at the current state of affairs. [sent-37, score-0.549]
</p><p>12 Another explanation seeks to derive word order frequencies as a consequence of more fundamental or general linguistic principles. [sent-39, score-0.561]
</p><p>13 In this paper we propose a novel explanation for the observed distribution of word orders across languages, based on uniform information density (UID). [sent-45, score-0.685]
</p><p>14 A listener’s comprehension of an utterance is made more difﬁcult if a syllable, word or clause which carries a lot of information is lost due to ambient noise or problems with articulation or perception. [sent-49, score-0.539]
</p><p>15 It suggests that speakers should attempt to slow down the rate at which information is conveyed when unexpected, high entropy content is being discussed, and increase the rate when predictable, low entropy content is being discussed. [sent-55, score-0.262]
</p><p>16 In addition, analysis of corpus data suggests that the entropy of sentences taken out of context is higher for sentences further into a body of text [7, 12]. [sent-57, score-0.352]
</p><p>17 We propose that the basic word order of a language inﬂuences the average uniformity of information density for sentences in that language, and that a preference for languages that are closer to the UID ideal can explain some of the structure in the observed distribution over basic word orders. [sent-67, score-1.493]
</p><p>18 We assume that languages are grounded in a world, consisting of objects (elements of a set O) and actions (which are binary relations between objects, and elements of a set R, such that if r ∈ R then r ⊂ O × O). [sent-71, score-0.283]
</p><p>19 An event in the world is a triple consisting of a relation r and two objects o1 , o2 and is written (o1 , r, o2 ). [sent-72, score-0.359]
</p><p>20 Events in the world are generated probabilistically in a sequential fashion, as independent identically 2  distributed draws from a probability distribution P over the set of events O × R × O. [sent-73, score-0.221]
</p><p>21 We assume that a language consists of nouns (each of which corresponds to a unique object) and verbs (each of which corresponds to a unique action). [sent-74, score-0.274]
</p><p>22 Utterances are generated from events by combining the three relevant words in one of the six possible orders. [sent-75, score-0.203]
</p><p>23 To make this idea more concrete, we construct a simple toy world consisting of thirteen objects and two relations. [sent-78, score-0.233]
</p><p>24 The two relations are E AT and D RINK, so that the events in this world represent particular people eating or drinking particular items (e. [sent-80, score-0.28]
</p><p>25 What is the link between word order and information density in this toy world? [sent-87, score-0.589]
</p><p>26 Consider a listener who learns about events in this toy world by hearing three-word utterances (such as “Alice eats apples” or “Bob drinks coffee”), one word at a time. [sent-88, score-0.955]
</p><p>27 Until they have heard all three words in the utterance, there will generally remain some degree of uncertainty about what the event is, with the uncertainty decreasing as each word is heard. [sent-89, score-0.701]
</p><p>28 Formally, the event underlying an utterance is a random variable, and the listener’s uncertainty is represented by the entropy of that random variable. [sent-90, score-0.348]
</p><p>29 Before any words are spoken, the observer’s uncertainty is given by the entropy of the event distribution (which we refer to as the base entropy and denote H0 ): −P (o1 , r, o2 ) log(P (o1 , r, o2 )),  H0 = H(P ) =  (1)  (o1 ,r,o2 )  where the sum is taken over all possible events in the world. [sent-91, score-0.522]
</p><p>30 Similarly, after the second word, the uncertainty is the entropy of one of the conditional distributions P (o2 |o1 , r), P (o1 |r, o2 ) or P (r|o1 , o2 ), depending again on word order. [sent-93, score-0.537]
</p><p>31 After the third word the event is uniquely determined and the entropy is zero. [sent-94, score-0.731]
</p><p>32 This means that for any particular event, the six different choices of word order each deﬁne a different monotonically decreasing sequence of intermediate entropies, with the ﬁrst point in the sequence always being H0 and the ﬁnal point always being zero. [sent-95, score-0.515]
</p><p>33 Equivalently, the different choices of word order result in different distributions of the total information content of a sentence amongst its constituent words. [sent-96, score-0.556]
</p><p>34 Figure 1 shows the entropy trajectories and corresponding information proﬁles for the event (A LICE , E AT A PPLE) in our toy world, for three different word orders. [sent-98, score-0.832]
</p><p>35 The ﬁgure demonstrates the correspondence between trajectories and proﬁles, as well as the dependency of both on word order. [sent-99, score-0.494]
</p><p>36 For example, in our toy world the entropy trajectories for the word orders SOV, OSV and OVS (two of which are pictured in Figure 1) are perfectly horizontal at various points (equivalently, some words carry zero information) because 1 Obviously this is not true. [sent-103, score-0.944]
</p><p>37 However, in order for this simplifying assumption to skew our results, the length of nouns would need to vary systematically depending on the relative frequency with which the nouns were the subject and orbject of sentences, which is highly unlikely to be the case. [sent-104, score-0.243]
</p><p>38 3  Figure 1: The entropy trajectories and corresponding information proﬁles for the event (A LICE , E AT, A PPLE) in our toy world, for three different word orders. [sent-105, score-0.832]
</p><p>39 Observe that word orders in which the object preceeds the verb have signiﬁcant “troughs” in their information proﬁles, making them far from ideal. [sent-107, score-0.693]
</p><p>40 This pattern arises because of the event structure in our toy world; our question is what word orders are optimal given real-world event structure. [sent-108, score-1.061]
</p><p>41 knowledge of the object in this world uniquely determines the verb (since foods are strictly eaten and drinks are strictly drunk). [sent-109, score-0.281]
</p><p>42 Thus, any word order that places O before V renders the verb entirely uninformative, in signiﬁcant conﬂict with the UID hypothesis. [sent-110, score-0.549]
</p><p>43 The UID deviation score allow us, for each event in the model world, to produce both an ordering of the word orders from “most UID-like” to “least UID-like”, as well as a quantitative measure of the extent to which each word order approaches uniform information density. [sent-114, score-1.355]
</p><p>44 We can straightforwardly calculate a mean deviation score for the entire model world, by summing the scores for each individual event and weighting by that event’s probability according to the event distribution P . [sent-115, score-0.463]
</p><p>45 This lets us assess the extent to which each word order is UID-suited to a given world. [sent-116, score-0.488]
</p><p>46 For our toy world, the ordering of word orders from lowest to highest mean deviation score is: VSO, VOS, SVO, OVS, SOV, OSV. [sent-117, score-0.741]
</p><p>47 Of course, our toy world is a highly contrived example, and so there is no reason to expect it to produce the observed cross-linguistic distribution of word orders. [sent-118, score-0.62]
</p><p>48 The toy example is intended only as a demonstration of the core idea underlying our hypothesis: that different choices of word order map the same probabilistic structure of the world (P ) onto different information proﬁles. [sent-120, score-0.647]
</p><p>49 Since these proﬁles have differing levels of information density uniformity, the UID hypothesis implies a preference ranking of word orders. [sent-121, score-0.597]
</p><p>50 What are the mean deviation scores when the event distribution P more accurately approximates reality? [sent-122, score-0.269]
</p><p>51 Does the preferred ranking of word orders implied by the UID hypothesis reﬂect the observed cross-linguistic distribution of word orders? [sent-123, score-1.169]
</p><p>52 3  Corpus analysis  Our work above implies that a particular word ordering in a language is good to the extent that it produces minimal UID deviation scores for events in the world. [sent-125, score-0.785]
</p><p>53 Accordingly, it would be ideal to assess the optimality of a particular word ordering with respect to the true distribution over “psychologically meaningful” events in the everyday environment. [sent-126, score-0.663]
</p><p>54 One option is to assume that spontaneous speech is informative about event probabilities – that the probability with which speakers discuss an event is roughly proportional to the actual frequency or psychological importance of that event. [sent-128, score-0.511]
</p><p>55 From this data we extract all of the child-directed utterances involving a random subset of the singly transitive verbs in the corpus (a total of 544 utterances). [sent-132, score-0.299]
</p><p>56 The subjects and objects of these utterances deﬁne the set O and the verbs deﬁne the set R. [sent-133, score-0.271]
</p><p>57 In our analysis, we treat each utterance as a distinct event, setting the probability of an event in P to be proportional to the number of times the corresponding utterance occurs in the corpus. [sent-134, score-0.35]
</p><p>58 Thus the event distribution P is a measure of the probability that speakers of the language choose to discuss events (rather than their frequency in the real world). [sent-135, score-0.494]
</p><p>59 Utterances involving pronouns which were considered likely to refer to a wide range of objects across the corpus (such as “it”, “this”, etc. [sent-137, score-0.203]
</p><p>60 Figure 2 shows the distribution of information amongst words (summarizing all of the model world’s information proﬁles) for all six word orders according to the event distribution P derived from the “Adam” transcripts. [sent-139, score-0.872]
</p><p>61 The mean deviation scores for the six word orders are (from lowest to highest) VSO (0. [sent-140, score-0.707]
</p><p>62 The distribution of information amongst words for the event distribution derived from the Japanese transcripts are shown in Figure 3. [sent-149, score-0.285]
</p><p>63 4  Experiment  In the previous analyses, the event distribution P was estimated on the basis of linguistic input. [sent-158, score-0.22]
</p><p>64 In one version of the UID hypothesis, we would expect that word order would be optimal with respect to the latter, “speaker-weighted” frequencies. [sent-162, score-0.488]
</p><p>65 We refer to this as the “weak” hypothesis since it only requires that a language be “internally” consistent, insofar as the word order is expected to be optimal with respect to the topics spoken about. [sent-163, score-0.671]
</p><p>66 However, there is also a “strong” version of the hypothesis, which states that the language must also be optimal with respect to the perceived frequencies of events in the external world. [sent-164, score-0.222]
</p><p>67 To test the strong version of the UID word order hypothesis, it is not valid to rely on corpus analysis. [sent-165, score-0.564]
</p><p>68 In the ﬁrst part we identify the objects O and relations R for the model world based on the ﬁrst words learned by English-speaking children, on the assumption that those words would reﬂect the objects and relations that are highly salient. [sent-168, score-0.401]
</p><p>69 We identiﬁed all of the words that were either singly-transitive verbs or nouns that were potential subjects or objects for these verbs, yielding 324 nouns and 81 verbs. [sent-170, score-0.427]
</p><p>70 In order to limit the total number of possible events to a number tractable for parts two and three of the experiment, we then identiﬁed the 40 objects and 10 relations2 uttered by the highest percentage of children below the age of 16 months; these comprise the sets O and R. [sent-174, score-0.231]
</p><p>71 The 40 objects and 10 relations in our world deﬁne a total of 16,000 events, but the overwhelming majority of the events in the world are physically impossible (e. [sent-176, score-0.459]
</p><p>72 When both judges agreed that an event was impossible, its probability was set to zero; if they disagreed, we solicited a third judgement and set the event probability to zero if the majority agreed that it was impossible. [sent-183, score-0.421]
</p><p>73 Subsequent analysis revealed that many participants had interpreted the noun O UTSIDE as an adverb in events such as (B EAR , E AT, O UTSIDE), leading to events which should properly 2  The ratio of 4 objects for every 1 relation was chosen to reﬂect the proportion of each reported in [6]. [sent-185, score-0.4]
</p><p>74 3  6  Figure 4: Distribution of information across words for the world instantiated from the experimentally produced event distribution. [sent-191, score-0.331]
</p><p>75 have been considered impossible being classed as possible; we therefore set all events involving the noun O UTSIDE which did not involve the verb S EE to also be impossible. [sent-192, score-0.284]
</p><p>76 In each task, participants were presented with a pair of events and asked to indicate which of the two events they considered most probable. [sent-195, score-0.299]
</p><p>77 Table 2 shows the most and least probable completions of several event frames according to the distribution P produced by our experiment. [sent-197, score-0.28]
</p><p>78 From these different distributions we derive three different preferential rankings of word orders according to the UID hypothesis. [sent-210, score-0.702]
</p><p>79 The strongest empirical regularity regarding word order frequency - that object-ﬁrst word orders are extremely rare - coincides with our most robust ﬁnding: object-ﬁrst word orders lead to the least uniform information density in all three of our estimated event distributions. [sent-215, score-1.949]
</p><p>80 These orders together account for less than 2% of the world’s word order-sensitive languages, and in all our models have deviation scores that are notably greater than the deviation scores of the other word orders. [sent-216, score-1.216]
</p><p>81 As the proﬁles in Figures 2, 3 and 4 indicate, objectﬁrst word orders deviate from uniformity because the ﬁrst word (the object) carries disproportionate amount of information. [sent-218, score-1.102]
</p><p>82 For instance, hearing the object word “water” implies only a few possibilities for verbs (e. [sent-220, score-0.6]
</p><p>83 All three of our estimated event distributions lead to word order rankings in which VSO is ranked more highly than VOS, which is in agreement with the data. [sent-230, score-0.779]
</p><p>84 The greatest apparent discrepancy between the rankings produced by our analyses and the empirical data is the fact that SOV word order, which occurs frequently in real languages, appears to be only moderately compatible with the UID hypothesis. [sent-232, score-0.585]
</p><p>85 One possible explanation for this is that some other factor besides UID-compatibility has inﬂuenced the distribution of word orders, and this factor may favour SOV sufﬁciently to lift it to the top or equal-top place in a combined ranking. [sent-233, score-0.508]
</p><p>86 Another possibility is to combine the idea we saw earlier of common descent from SOV with the idea that word order change away from SOV is inﬂuenced by the UID hypothesis. [sent-234, score-0.488]
</p><p>87 This explanation could also lift SOV word order to a higher position in the word order ranking. [sent-235, score-1.023]
</p><p>88 To what extent are our rankings consistent with the the theme-ﬁrst principle (TFP), verb-object bonding (VOB) and animate-ﬁrst principle (AFP) principles of [17], which perfectly explain the empirical ranking? [sent-236, score-0.205]
</p><p>89 This suggests that perhaps the UID word order hypothesis is unable to provide a complete explanation of all of the word order rankings, but is able provide a sensible justiﬁcation for the TFP and/or AFP. [sent-240, score-1.079]
</p><p>90 A full consideration of the effects of word order on information density should not limit itself only to the considerations made in this paper, and so our results here must be considered only preliminary. [sent-241, score-0.521]
</p><p>91 For instance, we have given no consideration to sentences involving intransitive verbs (SV sentences), sentences without an explicit subject (VO sentences), or sentences involving ditransitive verbs (SVO1 O2 sentences). [sent-242, score-0.528]
</p><p>92 A word order optimal for one of these sentence classes may not be optimal for others, so that the question of how to meaningfully combine the results of separate analyses becomes a central challenge in such an extended study. [sent-243, score-0.555]
</p><p>93 Furthermore, a number of other word order parameters beyond basic word order may have a signiﬁcant effect on information density, such as whether a language uses prepositions or postpositions, or the relative position of nouns and adjectives or nouns and relative clauses. [sent-244, score-1.333]
</p><p>94 Several correlations and rules are known to exist between various word order parameters, and it is possible that these effects may be able to be explained in terms of information density. [sent-255, score-0.488]
</p><p>95 On the whole, while the word order rankings recovered from our analyses do not perfectly match the empirically observed ranking, they are in much better agreement with observation than one would expect if a preference for UID had played no role whatsoever. [sent-256, score-0.637]
</p><p>96 Furthermore, the particular pattern of what our rankings do and do not explain, and the ways our two rankings differ, are consistent with a weaker hypothesis that UID may be able to provide a principled cognitive explanation for the theme-ﬁrst and/or animate-ﬁrst principles of earlier work. [sent-257, score-0.384]
</p><p>97 It is possible that the discrepancies which do exist between our results and the empirical distribution could be explained by a combination of more and richer data and consideration of additional word order parameters. [sent-258, score-0.488]
</p><p>98 Regardless, we have shown that informationtheoretic principles can explain several aspects of the empirical distribution of word orders, and most robustly explains the most pronounced of these aspects: the nearly complete lack of objectﬁrst languages. [sent-260, score-0.519]
</p><p>99 Moreover, they do so on independently justiﬁed, general cognitive principles, and as such represent a signiﬁcant advance in our understanding of word order. [sent-261, score-0.49]
</p><p>100 Effects of disﬂuencies, predictability, and utterance position on word form variation in English conversation. [sent-265, score-0.539]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('word', 0.461), ('sov', 0.368), ('svo', 0.254), ('uid', 0.25), ('ovs', 0.216), ('vso', 0.216), ('vos', 0.203), ('event', 0.194), ('languages', 0.174), ('orders', 0.144), ('events', 0.13), ('osv', 0.127), ('sentences', 0.1), ('rankings', 0.097), ('nouns', 0.096), ('language', 0.092), ('world', 0.091), ('rink', 0.089), ('verbs', 0.086), ('utterances', 0.082), ('utterance', 0.078), ('corpus', 0.076), ('entropy', 0.076), ('objects', 0.074), ('pro', 0.068), ('toy', 0.068), ('english', 0.066), ('drinks', 0.064), ('lice', 0.064), ('pple', 0.064), ('verb', 0.061), ('principles', 0.058), ('hypothesis', 0.056), ('japanese', 0.055), ('speakers', 0.054), ('completions', 0.051), ('drink', 0.051), ('utside', 0.051), ('explanation', 0.047), ('ranking', 0.047), ('les', 0.046), ('words', 0.046), ('speech', 0.045), ('ideal', 0.045), ('transcripts', 0.045), ('adjectives', 0.041), ('deviation', 0.041), ('sentence', 0.04), ('participants', 0.039), ('childes', 0.038), ('foods', 0.038), ('offee', 0.038), ('tfp', 0.038), ('impossible', 0.038), ('uniformity', 0.036), ('relations', 0.035), ('probable', 0.035), ('spoken', 0.035), ('scores', 0.034), ('trajectories', 0.033), ('erson', 0.033), ('judgement', 0.033), ('listener', 0.033), ('syntactic', 0.033), ('density', 0.033), ('basic', 0.032), ('water', 0.031), ('explanations', 0.031), ('subjects', 0.029), ('genetic', 0.029), ('cognitive', 0.029), ('content', 0.028), ('involving', 0.028), ('redundancy', 0.028), ('order', 0.027), ('transitive', 0.027), ('noun', 0.027), ('analyses', 0.027), ('six', 0.027), ('object', 0.027), ('ordering', 0.027), ('linguistic', 0.026), ('entropies', 0.026), ('hearing', 0.026), ('unexpected', 0.026), ('afp', 0.025), ('ate', 0.025), ('bites', 0.025), ('bonding', 0.025), ('edible', 0.025), ('elp', 0.025), ('florian', 0.025), ('ilk', 0.025), ('maurits', 0.025), ('ooth', 0.025), ('optional', 0.025), ('pronouns', 0.025), ('perfectly', 0.025), ('people', 0.024), ('frequency', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="285-tfidf-1" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>2 0.2353427 <a title="285-tfidf-2" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>3 0.1669261 <a title="285-tfidf-3" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>4 0.15032478 <a title="285-tfidf-4" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>Author: Yariv Maron, Elie Bienenstock, Michael James</p><p>Abstract: Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a hig hdimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks. 1 In trod u cti on The embedding of objects in a low-dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10, 13]. Most methods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3], embed objects of heterogeneous types, based on their co-occurrence statistics. In this paper we demonstrate that the latter can be successfully applied to unsupervised part-of-speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1, 4, 5, 6, 7]. The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features). The induction task is fully unsupervised, i.e., it uses no annotations. This task has been addressed in the past using a variety of methods. Some approaches, such as [1], combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [7, 2, 5]. Using a latent-descriptor clustering approach, [15] obtain the best results to date for distributional-only unsupervised POS tagging of the widely-used WSJ corpus. Using a heterogeneous-data embedding approach for this task, we define separate embedding functions for the objects</p><p>5 0.1126477 <a title="285-tfidf-5" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>Author: Nimar Arora, Stuart Russell, Paul Kidwell, Erik B. Sudderth</p><p>Abstract: The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the ﬁrst stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits signiﬁcantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output. 1</p><p>6 0.07553979 <a title="285-tfidf-6" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>7 0.074077323 <a title="285-tfidf-7" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>8 0.069095522 <a title="285-tfidf-8" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>9 0.052119724 <a title="285-tfidf-9" href="./nips-2010-Tree-Structured_Stick_Breaking_for_Hierarchical_Data.html">276 nips-2010-Tree-Structured Stick Breaking for Hierarchical Data</a></p>
<p>10 0.050899852 <a title="285-tfidf-10" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>11 0.048970595 <a title="285-tfidf-11" href="./nips-2010-Object_Bank%3A_A_High-Level_Image_Representation_for_Scene_Classification_%26_Semantic_Feature_Sparsification.html">186 nips-2010-Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification</a></p>
<p>12 0.047585055 <a title="285-tfidf-12" href="./nips-2010-Learning_from_Logged_Implicit_Exploration_Data.html">152 nips-2010-Learning from Logged Implicit Exploration Data</a></p>
<p>13 0.047272906 <a title="285-tfidf-13" href="./nips-2010-Online_Learning_for_Latent_Dirichlet_Allocation.html">194 nips-2010-Online Learning for Latent Dirichlet Allocation</a></p>
<p>14 0.045124773 <a title="285-tfidf-14" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<p>15 0.043890759 <a title="285-tfidf-15" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>16 0.040871024 <a title="285-tfidf-16" href="./nips-2010-Size_Matters%3A_Metric_Visual_Search_Constraints_from_Monocular_Metadata.html">241 nips-2010-Size Matters: Metric Visual Search Constraints from Monocular Metadata</a></p>
<p>17 0.040096179 <a title="285-tfidf-17" href="./nips-2010-Learning_invariant_features_using_the_Transformed_Indian_Buffet_Process.html">153 nips-2010-Learning invariant features using the Transformed Indian Buffet Process</a></p>
<p>18 0.039484199 <a title="285-tfidf-18" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>19 0.038385265 <a title="285-tfidf-19" href="./nips-2010-Simultaneous_Object_Detection_and_Ranking_with_Weak_Supervision.html">240 nips-2010-Simultaneous Object Detection and Ranking with Weak Supervision</a></p>
<p>20 0.037561107 <a title="285-tfidf-20" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.026), (2, -0.047), (3, -0.023), (4, -0.159), (5, 0.055), (6, 0.05), (7, -0.032), (8, -0.015), (9, 0.017), (10, 0.1), (11, 0.045), (12, -0.06), (13, 0.026), (14, 0.01), (15, 0.042), (16, 0.086), (17, 0.022), (18, -0.1), (19, -0.081), (20, -0.017), (21, 0.048), (22, 0.144), (23, 0.092), (24, 0.309), (25, -0.234), (26, 0.129), (27, -0.101), (28, 0.04), (29, 0.062), (30, 0.032), (31, -0.033), (32, -0.092), (33, 0.013), (34, -0.061), (35, -0.179), (36, 0.109), (37, -0.035), (38, -0.083), (39, -0.033), (40, 0.046), (41, 0.079), (42, 0.082), (43, 0.025), (44, 0.074), (45, 0.107), (46, 0.03), (47, 0.013), (48, 0.019), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9829815 <a title="285-lsi-1" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>2 0.90077758 <a title="285-lsi-2" href="./nips-2010-Synergies_in_learning_words_and_their_referents.html">264 nips-2010-Synergies in learning words and their referents</a></p>
<p>Author: Mark Johnson, Katherine Demuth, Bevan Jones, Michael J. Black</p><p>Abstract: This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these. 1</p><p>3 0.68824041 <a title="285-lsi-3" href="./nips-2010-Sphere_Embedding%3A_An_Application_to_Part-of-Speech_Induction.html">251 nips-2010-Sphere Embedding: An Application to Part-of-Speech Induction</a></p>
<p>Author: Yariv Maron, Elie Bienenstock, Michael James</p><p>Abstract: Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a hig hdimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks. 1 In trod u cti on The embedding of objects in a low-dimensional Euclidean space is a form of dimensionality reduction that has been used in the past mostly to create 2D representations of data for the purpose of visualization and exploratory data analysis [10, 13]. Most methods work on objects of a single type, endowed with a measure of similarity. Other methods, such as [ 3], embed objects of heterogeneous types, based on their co-occurrence statistics. In this paper we demonstrate that the latter can be successfully applied to unsupervised part-of-speech (POS) induction, an extensively studied, challenging, problem in natural language processing [1, 4, 5, 6, 7]. The problem we address is distributional POS tagging, in which words are to be tagged based on the statistics of their immediate left and right context in a corpus (ignoring morphology and other features). The induction task is fully unsupervised, i.e., it uses no annotations. This task has been addressed in the past using a variety of methods. Some approaches, such as [1], combine a Markovian assumption with clustering. Many recent works use HMMs, perhaps due to their excellent performance on the supervised version of the task [7, 2, 5]. Using a latent-descriptor clustering approach, [15] obtain the best results to date for distributional-only unsupervised POS tagging of the widely-used WSJ corpus. Using a heterogeneous-data embedding approach for this task, we define separate embedding functions for the objects</p><p>4 0.61354381 <a title="285-lsi-4" href="./nips-2010-Word_Features_for_Latent_Dirichlet_Allocation.html">286 nips-2010-Word Features for Latent Dirichlet Allocation</a></p>
<p>Author: James Petterson, Wray Buntine, Shravan M. Narayanamurthy, Tibério S. Caetano, Alex J. Smola</p><p>Abstract: We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model. 1</p><p>5 0.59773386 <a title="285-lsi-5" href="./nips-2010-Inference_and_communication_in_the_game_of_Password.html">125 nips-2010-Inference and communication in the game of Password</a></p>
<p>Author: Yang Xu, Charles Kemp</p><p>Abstract: Communication between a speaker and hearer will be most efﬁcient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efﬁcient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We ﬁnd evidence in support of all three predictions, and demonstrate in addition that efﬁcient communication tends to break down when speakers and hearers are placed under time pressure.</p><p>6 0.56746835 <a title="285-lsi-6" href="./nips-2010-Global_seismic_monitoring_as_probabilistic_inference.html">107 nips-2010-Global seismic monitoring as probabilistic inference</a></p>
<p>7 0.3559047 <a title="285-lsi-7" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>8 0.33449414 <a title="285-lsi-8" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>9 0.31846595 <a title="285-lsi-9" href="./nips-2010-Probabilistic_Belief_Revision_with_Structural_Constraints.html">214 nips-2010-Probabilistic Belief Revision with Structural Constraints</a></p>
<p>10 0.31099004 <a title="285-lsi-10" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>11 0.30900782 <a title="285-lsi-11" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>12 0.2952137 <a title="285-lsi-12" href="./nips-2010-Latent_Variable_Models_for_Predicting_File_Dependencies_in_Large-Scale_Software_Development.html">139 nips-2010-Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development</a></p>
<p>13 0.29503289 <a title="285-lsi-13" href="./nips-2010-Learning_concept_graphs_from_text_with_stick-breaking_priors.html">150 nips-2010-Learning concept graphs from text with stick-breaking priors</a></p>
<p>14 0.28983414 <a title="285-lsi-14" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>15 0.27644214 <a title="285-lsi-15" href="./nips-2010-Direct_Loss_Minimization_for_Structured_Prediction.html">61 nips-2010-Direct Loss Minimization for Structured Prediction</a></p>
<p>16 0.24505633 <a title="285-lsi-16" href="./nips-2010-Static_Analysis_of_Binary_Executables_Using_Structural_SVMs.html">255 nips-2010-Static Analysis of Binary Executables Using Structural SVMs</a></p>
<p>17 0.22738117 <a title="285-lsi-17" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>18 0.22126128 <a title="285-lsi-18" href="./nips-2010-Learning_to_localise_sounds_with_spiking_neural_networks.html">157 nips-2010-Learning to localise sounds with spiking neural networks</a></p>
<p>19 0.20898584 <a title="285-lsi-19" href="./nips-2010-Using_body-anchored_priors_for_identifying_actions_in_single_images.html">281 nips-2010-Using body-anchored priors for identifying actions in single images</a></p>
<p>20 0.20126779 <a title="285-lsi-20" href="./nips-2010-Deterministic_Single-Pass_Algorithm_for_LDA.html">60 nips-2010-Deterministic Single-Pass Algorithm for LDA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.025), (17, 0.023), (25, 0.324), (27, 0.068), (30, 0.114), (35, 0.017), (45, 0.137), (50, 0.036), (52, 0.03), (60, 0.018), (77, 0.046), (78, 0.023), (90, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75926071 <a title="285-lda-1" href="./nips-2010-Why_are_some_word_orders_more_common_than_others%3F_A_uniform_information_density_account.html">285 nips-2010-Why are some word orders more common than others? A uniform information density account</a></p>
<p>Author: Luke Maurits, Dan Navarro, Amy Perfors</p><p>Abstract: Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an informationtheoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-ﬁrst languages are particularly disfavored because they are highly nonoptimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work. 1</p><p>2 0.67425746 <a title="285-lda-2" href="./nips-2010-Estimating_Spatial_Layout_of_Rooms_using_Volumetric_Reasoning_about_Objects_and_Surfaces.html">79 nips-2010-Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces</a></p>
<p>Author: Abhinav Gupta, Martial Hebert, Takeo Kanade, David M. Blei</p><p>Abstract: There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning signiﬁcantly improves the performance of the state-of-the-art. 1</p><p>3 0.59275275 <a title="285-lda-3" href="./nips-2010-Exact_learning_curves_for_Gaussian_process_regression_on_large_random_graphs.html">85 nips-2010-Exact learning curves for Gaussian process regression on large random graphs</a></p>
<p>Author: Matthew Urry, Peter Sollich</p><p>Abstract: We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difﬁcult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a ﬁnite number of others. Our approach is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations of the learning curve fail. 1</p><p>4 0.5786894 <a title="285-lda-4" href="./nips-2010-Network_Flow_Algorithms_for_Structured_Sparsity.html">181 nips-2010-Network Flow Algorithms for Structured Sparsity</a></p>
<p>Author: Julien Mairal, Rodolphe Jenatton, Francis R. Bach, Guillaume R. Obozinski</p><p>Abstract: We consider a class of learning problems that involve a structured sparsityinducing norm deﬁned as the sum of ℓ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a speciﬁc hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network ﬂow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost ﬂow problem. We propose an efﬁcient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. 1</p><p>5 0.51323944 <a title="285-lda-5" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>Author: Sivan Sabato, Nathan Srebro, Naftali Tishby</p><p>Abstract: We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin classiﬁcation with L2 regularization: We introduce the γ-adapted-dimension, which is a simple function of the spectrum of a distribution’s covariance matrix, and show distribution-speciﬁc upper and lower bounds on the sample complexity, both governed by the γ-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classiﬁcation. The bounds hold for a rich family of sub-Gaussian distributions. 1</p><p>6 0.51007062 <a title="285-lda-6" href="./nips-2010-Decomposing_Isotonic_Regression_for_Efficiently_Solving_Large_Problems.html">58 nips-2010-Decomposing Isotonic Regression for Efficiently Solving Large Problems</a></p>
<p>7 0.505337 <a title="285-lda-7" href="./nips-2010-Beyond_Actions%3A_Discriminative_Models_for_Contextual_Group_Activities.html">40 nips-2010-Beyond Actions: Discriminative Models for Contextual Group Activities</a></p>
<p>8 0.50421667 <a title="285-lda-8" href="./nips-2010-Random_Projection_Trees_Revisited.html">220 nips-2010-Random Projection Trees Revisited</a></p>
<p>9 0.50413203 <a title="285-lda-9" href="./nips-2010-Random_Walk_Approach_to_Regret_Minimization.html">222 nips-2010-Random Walk Approach to Regret Minimization</a></p>
<p>10 0.50274974 <a title="285-lda-10" href="./nips-2010-Learning_the_context_of_a_category.html">155 nips-2010-Learning the context of a category</a></p>
<p>11 0.49925801 <a title="285-lda-11" href="./nips-2010-Sufficient_Conditions_for_Generating_Group_Level_Sparsity_in_a_Robust_Minimax_Framework.html">260 nips-2010-Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework</a></p>
<p>12 0.49559873 <a title="285-lda-12" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>13 0.49531999 <a title="285-lda-13" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>14 0.49497625 <a title="285-lda-14" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>15 0.49423289 <a title="285-lda-15" href="./nips-2010-Online_Learning%3A_Random_Averages%2C_Combinatorial_Parameters%2C_and_Learnability.html">193 nips-2010-Online Learning: Random Averages, Combinatorial Parameters, and Learnability</a></p>
<p>16 0.49140149 <a title="285-lda-16" href="./nips-2010-Extensions_of_Generalized_Binary_Search_to_Group_Identification_and_Exponential_Costs.html">88 nips-2010-Extensions of Generalized Binary Search to Group Identification and Exponential Costs</a></p>
<p>17 0.49048245 <a title="285-lda-17" href="./nips-2010-Empirical_Risk_Minimization_with_Approximations_of_Probabilistic_Grammars.html">75 nips-2010-Empirical Risk Minimization with Approximations of Probabilistic Grammars</a></p>
<p>18 0.48860884 <a title="285-lda-18" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>19 0.48837289 <a title="285-lda-19" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>20 0.48675558 <a title="285-lda-20" href="./nips-2010-Unsupervised_Kernel_Dimension_Reduction.html">280 nips-2010-Unsupervised Kernel Dimension Reduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
