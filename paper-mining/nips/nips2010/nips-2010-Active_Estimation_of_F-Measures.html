<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2010-Active Estimation of F-Measures</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-22" href="#">nips2010-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2010-Active Estimation of F-Measures</h1>
<br/><p>Source: <a title="nips-2010-22-pdf" href="http://papers.nips.cc/paper/3999-active-estimation-of-f-measures.pdf">pdf</a></p><p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of estimating the Fα -measure of a given model as accurately as possible on a ﬁxed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reﬂect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. 1</p><p>Reference: <a title="nips-2010-22-reference" href="../nips2010_reference/nips-2010-Active_Estimation_of_F-Measures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this case, new test instances have to be drawn and labeled at a cost. [sent-5, score-0.287]
</p><p>2 An active estimation procedure selects instances according to an instrumental sampling distribution. [sent-6, score-0.78]
</p><p>3 An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. [sent-7, score-0.478]
</p><p>4 We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. [sent-8, score-0.526]
</p><p>5 Finally, when a model has been trained actively, the labeled data is biased towards small-margin instances which would incur a pessimistic bias on any cross-validation estimate. [sent-13, score-0.219]
</p><p>6 For a given binary classiﬁer and sample of size n, let ntp and nf p denote the number of true and false positives, respectively, and nf n the number of false negatives. [sent-19, score-0.447]
</p><p>7 (1) α(ntp + nf p ) + (1 − α)(ntp + nf n ) Precision and recall are special cases for α = 1 and α = 0, respectively. [sent-21, score-0.284]
</p><p>8 We will now introduce the class of generalized risk functionals that we study in this paper. [sent-24, score-0.349]
</p><p>9 Like any risk functional, the generalized risk is parameterized with a function : Y × Y → R determining either the loss or—alternatively—the gain that is incurred for a pair of predicted and 1  true label. [sent-29, score-0.511]
</p><p>10 In addition, the generalized risk is parameterized with a function w that assigns a weight w(x, y, fθ ) to each instance. [sent-30, score-0.297]
</p><p>11 For instance, precision sums over instances with fθ (x) = 1 with weight 1 and gives no consideration to other instances. [sent-31, score-0.234]
</p><p>12 Note that the generalized risk (Equation 2) reduces to the regular risk for w(x, y, fθ ) = 1. [sent-34, score-0.487]
</p><p>13 On a sample of size n, a consistent estimator can be obtained by replacing the cumulative distribution function with the empirical distribution function. [sent-35, score-0.279]
</p><p>14 The quantity ˆ Gn =  n i=1  (fθ (xi ), yi )w(xi , yi , fθ ) n i=1 w(xi , yi , fθ )  (3)  is a consistent estimate of the generalized risk G deﬁned by Equation 2. [sent-41, score-0.642]
</p><p>15 ˆ Consistency means asymptotical unbiasedness; that is, the expected value of the estimate Gn converges in distribution to the true risk G for n → ∞. [sent-44, score-0.284]
</p><p>16 We now observe that Fα -measures—including precision and recall—are consistent empirical estimates of generalized risks for appropriately chosen functions w. [sent-45, score-0.399]
</p><p>17 Fα is a consistent estimate of the generalized risk with Y = {0, 1}, w(x, y, fθ ) = αfθ (x) + (1 − α)y and = 1 − 0/1 , where 0/1 denotes the zero-one loss. [sent-47, score-0.387]
</p><p>18 The claim follows from Proposition 1 since n i=1 (1  ˆ Gn = =  α  −  0/1 (fθ (xi ), yi )) (αfθ (xi ) + (1 − α)yi ) n i=1 (αfθ (xi ) + (1 − α)yi ) n i=1 fθ (xi )yi = n n α (ntp + nf p ) fθ (xi ) + (1 − α) i=1 yi i=1  ntp . [sent-49, score-0.478]
</p><p>19 + (1 − α) (ntp + nf n )  Having established and motivated the generalized risk functional, we now turn towards the problem of acquiring a consistent estimate with minimal estimation error on a ﬁxed labeling budget n. [sent-50, score-0.798]
</p><p>20 Instead, we study an active estimation process that selects test instances according to an instrumental distribution q. [sent-55, score-0.776]
</p><p>21 When instances are sampled from q, an estimator of the generalized risk can be deﬁned as ˆ Gn,q =  n p(xi ) i=1 q(xi ) (fθ (xi ), yi )w(xi , yi , fθ ) n p(xi ) i=1 q(xi ) w(xi , yi , fθ )  (4)  i) where (xi , yi ) are drawn from q(x)p(y|x). [sent-56, score-0.947]
</p><p>22 Weighting factors p(xi ) compensate for the discrepancy q(x between test and instrumental distributions. [sent-57, score-0.316]
</p><p>23 Because of the weighting factors, Slutsky’s Theorem again implies that Equation 4 deﬁnes a consistent estimator for G, under the precondition that for all x ∈ X with p(x) > 0 it holds that q(x) > 0. [sent-58, score-0.21]
</p><p>24 Note that Equation 3 is a special case of Equation 4, using the instrumental distribution q = p. [sent-59, score-0.225]
</p><p>25 ˆ The estimate Gn,q given by Equation 4 depends on the selected instances (xi , yi ), which are drawn ˆ according to the distribution q(x)p(y|x). [sent-60, score-0.399]
</p><p>26 Our overall goal is to determine the instrumental distribution q such that the expected deviation from the generalized risk is minimal for ﬁxed labeling costs n: ˆ Gn,q − G  q ∗ = arg min E q  2  2  . [sent-62, score-0.705]
</p><p>27 2  Active Estimation through Variance Minimization  The bias-variance decomposition expresses the estimation error as a sum of a squared bias and a variance term [5]: ˆ ˆ E (Gn,q − G)2 = E Gn,q − G  2  +E  ˆ ˆ Gn,q − E Gn,q  2  ˆ ˆ = Bias2 [Gn,q ] + Var[Gn,q ]. [sent-63, score-0.231]
</p><p>28 Lemma 2 states that the active risk estimator Gn,q is asymptotically normally distributed, and characterizes its variance in the limit. [sent-70, score-0.659]
</p><p>29 In the following, we will consequently derive a 2 ˆ sampling distribution q ∗ that minimizes the asymptotic variance σq of the estimator Gn,q . [sent-78, score-0.466]
</p><p>30 1  Optimal Sampling Distribution  2 The following theorem derives the sampling distribution that minimizes the asymptotic variance σq :  Theorem 1 (Optimal Sampling Distribution). [sent-80, score-0.41]
</p><p>31 The instrumental distribution that minimizes the 2 ˆ asymptotic variance σq of the generalized risk estimator Gn,q is given by q ∗ (x) ∝ p(x)  2  w(x, y, fθ )2 ( (fθ (x), y) − G) p(y|x)dy. [sent-81, score-0.816]
</p><p>32 Since F -measures are estimators of generalized risks according to Corollary 1, we can now derive their variance-minimizing sampling distributions. [sent-83, score-0.357]
</p><p>33 1: Compute optimal sampling distribution q ∗ according to Corollary 2, 3, or 4, respectively. [sent-87, score-0.203]
</p><p>34 According to Corollary 1, Fα estimates a generalized risk with Y = {0, 1}, w(x, y, fθ ) = αfθ (x) + (1 − α)y and = 1 − 0/1 . [sent-94, score-0.343]
</p><p>35 The sampling distribution that minimizes σq for recall resolves to  q ∗ (x) ∝  p(x) p(fθ (x)|x)(1 − G)2 p(x) (1 − p(fθ (x)|x))G2  : f (x) = 1 : f (x) = 0. [sent-97, score-0.328]
</p><p>36 The sampling distribution that minimizes σq for precision resolves to  q ∗ (x) ∝ p(x)fθ (x) (1 − 2G)p(fθ (x)|x) + G2 . [sent-99, score-0.367]
</p><p>37 Note that for standard risks (that is, w = 1) Theorem 1 coincides with the optimal sampling distribution derived in [7]. [sent-101, score-0.297]
</p><p>38 We now turn towards a setting in which a large pool D of unlabeled test instances is available. [sent-104, score-0.28]
</p><p>39 Drawing instances from the pool replaces generating 1 them under the test distribution; that is, p(x) = m for all x ∈ D. [sent-106, score-0.28]
</p><p>40 This approximation constitutes an analogy to active learning: In active learning, the model-based output probability p(y|x; θ) serves as the basis on which the least conﬁdent instances are selected. [sent-109, score-0.609]
</p><p>41 Note that as long as p(x) > 0 implies q(x) > 0, the weighting factors ensure that such approximations do not introduce an asymptotic bias in our estimator (Equation 4). [sent-110, score-0.273]
</p><p>42 Finally, Theorem 1 and its corollaries depend on the true generalized risk G. [sent-111, score-0.365]
</p><p>43 G is replaced by an intrinsic generalized risk calculated from Equation 2, 1 where the integral over X is replaced by a sum over the pool, p(x) = m , and p(y|x) ≈ p(y|x; θ). [sent-112, score-0.297]
</p><p>44 Algorithm 1 summarizes the procedure for active estimation of F -measures. [sent-113, score-0.321]
</p><p>45 3  Conﬁdence Intervals  ˆ Lemma 2 shows that the estimator Gn,q is asymptotically normally distributed and character2 izes its asymptotic variance. [sent-121, score-0.252]
</p><p>46 , (xn , yn ) drawn from the distribution q(x)p(y|x) by computing empirical variance 2 Sn,q =  1  n  n p(xi ) i=1 q(xi ) i=1  p(xi ) q(xi )  2  w(xi , yi , fθ )2  ˆ (fθ (xi ), yi ) − Gn,q  2  . [sent-125, score-0.351]
</p><p>47 As in the standard case of drawing test instances xi from the original distribution p, such conﬁdence intervals are approximate for ﬁnite n, but become exact for n → ∞. [sent-127, score-0.333]
</p><p>48 3  Empirical Studies  We compare active estimation of Fα -measures according to Algorithm 1 (denoted activeF ) to estimation based on a sample of instances drawn uniformly from the pool (denoted passive). [sent-128, score-0.708]
</p><p>49 We also consider the active estimator for risks presented in [7]. [sent-129, score-0.457]
</p><p>50 Instances are drawn according to the opti∗ mal sampling distribution q0/1 for zero-one risk (Derivation 1 in [7]); the Fα -measure is computed ∗ according to Equation 4 using q = q0/1 (denoted activeerr ). [sent-130, score-0.828]
</p><p>51 We collect 169,612 emails from an email service provider between June 2007 and April 2010; of these, 42,165 emails received by February 2008 are used for training. [sent-142, score-0.29]
</p><p>52 The Reuters-21578 text classiﬁcation task [4] allows us to study the effect of class skew, and serves as a prototypical domain for active learning. [sent-146, score-0.358]
</p><p>53 We employ an active learner that always queries the example with minimal functional margin p(fθ (x)|x; θ) − maxy=fθ (x) p(y|x; θ) [9]. [sent-148, score-0.261]
</p><p>54 2  Empirical Results  We study the performance of active and passive estimates as a function of (a) the precision-recall trade-off parameter α, (b) the discrepancy between training and test distribution, and (c) class skew in the test distribution. [sent-154, score-0.867]
</p><p>55 Point (b) is of interest because active estimates require the approximation p(y|x) ≈ p(y|x; θ); this assumption is violated when training and test distributions differ. [sent-155, score-0.371]
</p><p>56 For the spam ﬁltering domain, Figure 1 shows the average absolute estimation error for F0 (recall), F0. [sent-157, score-0.311]
</p><p>57 5 , and F1 (precision) estimates on a test set of 33,296 emails received between February 2008 and October 2008. [sent-158, score-0.263]
</p><p>58 The active generalized risk estimate activeF signiﬁcantly outperforms the passive estimate passive for all three measures. [sent-159, score-1.239]
</p><p>59 In order to reach the estimation accuracy of passive with a labeling budget of n = 800, activeF requires fewer than 150 (recall), 200 (F0. [sent-160, score-0.546]
</p><p>60 2  estimation error (absolute)  passive activeF  estimation error (absolute)  estimation error (absolute)  0. [sent-172, score-0.707]
</p><p>61 05  800  0  200  400 600 labeling costs n  800  Figure 1: Spam ﬁltering: Estimation error over labeling costs. [sent-182, score-0.346]
</p><p>62 5  estimation error (absolute)  Optimal Sampling Distribution (class ratio: 5/95) 25  passive activeF  0. [sent-193, score-0.435]
</p><p>63 Ratio of passive and active estimation error, error bars indicate standard deviation (center). [sent-199, score-0.707]
</p><p>64 activeF are at least as accurate as those of activeerr , and more accurate for high α values. [sent-201, score-0.354]
</p><p>65 Figure 2 (left) shows the sampling distribution q ∗ (x) for recall, precision and F0. [sent-203, score-0.267]
</p><p>66 5 -measure in the spam ﬁltering domain as a function of the classiﬁer’s conﬁdence, characterized by the log-odds ratio log p(y=1|x;θ) . [sent-204, score-0.207]
</p><p>67 The ﬁgure also shows the optimal sampling distribution for zero-one risk as used p(y=0|x;θ) in activeerr (denoted “0/1-Risk”). [sent-205, score-0.716]
</p><p>68 We observe that the precision estimator dismisses all examples with fθ (x) = 0; this is intuitive because precision is a function of true-positive and false-positive examples only. [sent-206, score-0.311]
</p><p>69 By contrast, the recall estimator selects examples on both sides of the decision boundary, as it has to estimate both the true positive and the false negative rate. [sent-207, score-0.244]
</p><p>70 The optimal sampling distribution for zero-one risk is symmetric, it prefers instances close to the decision boundary. [sent-208, score-0.501]
</p><p>71 We keep the training set of emails ﬁxed and move the time interval from which test instances are drawn increasingly further away into the future, thereby creating a growing gap between training and test distribution. [sent-210, score-0.502]
</p><p>72 Speciﬁcally, we divide 127,447 emails received between February 2008 and April 2010 into ten different test sets spanning approximately 2. [sent-211, score-0.262]
</p><p>73 Figure 2 (center, red curve) shows the discrepancy between training and test distribution measured in terms of the exponentiated average log-likelihood of the test labels given the model parameters θ. [sent-213, score-0.257]
</p><p>74 A ˆ n,q∗ −G| value above one indicates that the active estimate is more accurate than a passive estimate. [sent-217, score-0.574]
</p><p>75 The active estimate consistently outperforms the passive estimate; its advantage diminishes when training and test distributions diverge and the assumption of p(y|x) ≈ p(y|x; θ) becomes less accurate. [sent-218, score-0.747]
</p><p>76 In the spam ﬁltering domain we artiﬁcially sub-sampled data to different ratios of spam and non-spam emails. [sent-220, score-0.287]
</p><p>77 Figure 2 (right) shows the performance of activeF , passive, and activeerr for F0. [sent-221, score-0.354]
</p><p>78 Furthermore, activeF outperforms activeerr for imbalanced classes, while the approaches perform comparably when classes are balanced. [sent-224, score-0.413]
</p><p>79 02  200  400 600 labeling costs n  800  passive activeF activeerr  0. [sent-233, score-0.836]
</p><p>80 005  0  200  400 600 labeling costs n  800  estimation error (absolute)  0. [sent-236, score-0.319]
</p><p>81 02 passive activeF  estimation error (absolute)  estimation error (absolute)  0. [sent-239, score-0.571]
</p><p>82 Estimation error over class ratio for all ten classes, logarithmic scale (right). [sent-247, score-0.191]
</p><p>83 Figure 3 shows the estimation error of activeF , passive, and activeerr for an infrequent class (“crude”, 4. [sent-251, score-0.59]
</p><p>84 Figure 3 (right) shows the estimation error of activeF , passive, and activeerr on all ten one-versus-rest problems as a function of the problem’s class skew. [sent-255, score-0.587]
</p><p>85 We again observe that activeF outperforms passive consistently, and activeF outperforms activeerr for strongly skewed class distributions. [sent-256, score-0.814]
</p><p>86 Our experimental ﬁndings show that for estimating F -measures their varianceminimizing sampling distribution performs worse than the sampling distributions characterized by Theorem 1, especially for skewed class distributions. [sent-260, score-0.393]
</p><p>87 We use the model itself to approximate this distribution and decide on instances whose class labels are queried. [sent-264, score-0.245]
</p><p>88 Speciﬁcally, Bach derives a sampling distribution for active learning under the assumption that the current model gives a good approximation to the conditional probability p(y|x) [1]. [sent-266, score-0.466]
</p><p>89 To compensate for the bias incurred by the instrumental distribution, several active learning algorithms use importance weighting: for regression [8], exponential family models [1], or SVMs [2]. [sent-267, score-0.504]
</p><p>90 Finally, the proposed active estimation approach can be considered an instance of the general principle of importance sampling [6], which we employ in the context of generalized risk estimation. [sent-268, score-0.736]
</p><p>91 5  Conclusions  Fα -measures are deﬁned as empirical estimates; we have shown that they are consistent estimates of a generalized risk functional which Proposition 1 identiﬁes. [sent-269, score-0.419]
</p><p>92 Generalized risks can be estimated actively by sampling test instances from an instrumental distribution q. [sent-270, score-0.669]
</p><p>93 An analysis of the sources of estimation error leads to an instrumental distribution q ∗ that minimizes estimator variance. [sent-271, score-0.531]
</p><p>94 The optimal sampling distribution depends on the unknown conditional p(y|x); the active generalized risk estimator approximates this conditional by the model to be evaluated. [sent-272, score-0.879]
</p><p>95 Our empirical study supports the conclusion that the advantage of active over passive evaluation is particularly strong for skewed classes. [sent-273, score-0.61]
</p><p>96 The advantage of active evaluation is also correlated to the quality of the model as measured by the model-based likelihood of the test labels. [sent-274, score-0.32]
</p><p>97 In our experiments, active evaluation consistently outperformed passive evaluation, even for the greatest divergence between training and test distribution that we could observe. [sent-275, score-0.728]
</p><p>98 Let G0 = n i=1  vi wi with vi =  p(xi ) q(xi ) ,  wi = w(xi , yi , fθ ) and  i  vi i wi and Wn = ˆ n,q = = (fθ (xi ), yi ). [sent-280, score-1.22]
</p><p>99 Furthermore, T  f (G E [wi ] , E [wi ]) Σ f (G E [wi ] , E [wi ]) = Var [wi i vi ] − 2G Cov [wi vi , wi i vi ] + G2 Var [wi vi ] 2 2 2 2 2 2 = E wi 2 vi − 2G E wi i vi + G2 E wi vi i  p(x) q(x)  =  2 2  w(x, y, fθ )2 ( (fθ (x), y) − G) p(y|x)q(x)dydx. [sent-290, score-1.814]
</p><p>100 Support vector machine active learning with applications to text classiﬁcation. [sent-341, score-0.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('activef', 0.424), ('activeerr', 0.354), ('passive', 0.299), ('active', 0.235), ('wi', 0.212), ('risk', 0.19), ('instrumental', 0.171), ('ntp', 0.165), ('var', 0.145), ('instances', 0.139), ('vi', 0.138), ('emails', 0.133), ('spam', 0.124), ('estimator', 0.121), ('sampling', 0.118), ('nf', 0.114), ('labeling', 0.113), ('generalized', 0.107), ('risks', 0.101), ('wn', 0.098), ('precision', 0.095), ('sawade', 0.094), ('estimation', 0.086), ('yi', 0.085), ('pool', 0.081), ('xi', 0.08), ('asymptotic', 0.071), ('landwehr', 0.071), ('costs', 0.07), ('corollaries', 0.068), ('corollary', 0.067), ('test', 0.06), ('equation', 0.06), ('recall', 0.056), ('distribution', 0.054), ('unde', 0.053), ('variance', 0.053), ('discrepancy', 0.053), ('dx', 0.053), ('class', 0.052), ('absolute', 0.051), ('resolves', 0.051), ('skewed', 0.051), ('error', 0.05), ('ltering', 0.05), ('consistent', 0.05), ('drawn', 0.05), ('minimizes', 0.049), ('infrequent', 0.048), ('budget', 0.048), ('dydx', 0.047), ('estimates', 0.046), ('ten', 0.045), ('ratio', 0.044), ('bias', 0.042), ('february', 0.042), ('cov', 0.042), ('scheffer', 0.041), ('slutsky', 0.041), ('gn', 0.041), ('estimate', 0.04), ('weighting', 0.039), ('domain', 0.039), ('potsdam', 0.038), ('labeled', 0.038), ('bars', 0.037), ('yamada', 0.036), ('digit', 0.034), ('lemma', 0.033), ('normally', 0.033), ('theorem', 0.033), ('text', 0.032), ('compensate', 0.032), ('derives', 0.032), ('skew', 0.032), ('frequent', 0.032), ('dence', 0.032), ('according', 0.031), ('center', 0.03), ('imbalanced', 0.03), ('training', 0.03), ('fraction', 0.029), ('outperforms', 0.029), ('diverge', 0.029), ('claim', 0.029), ('proposition', 0.028), ('asymptotically', 0.027), ('conditional', 0.027), ('false', 0.027), ('actively', 0.026), ('functional', 0.026), ('maxy', 0.026), ('vn', 0.026), ('april', 0.025), ('consistently', 0.025), ('evaluation', 0.025), ('received', 0.024), ('yn', 0.024), ('incurred', 0.024), ('coincides', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="22-tfidf-1" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of estimating the Fα -measure of a given model as accurately as possible on a ﬁxed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reﬂect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. 1</p><p>2 0.21915999 <a title="22-tfidf-2" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>Author: Alina Beygelzimer, John Langford, Zhang Tong, Daniel J. Hsu</p><p>Abstract: We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classiﬁcation. 1</p><p>3 0.1442169 <a title="22-tfidf-3" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>Author: Sheng-jun Huang, Rong Jin, Zhi-hua Zhou</p><p>Abstract: Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in ﬁnding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed Q UIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed Q UIRE approach outperforms several state-of -the-art active learning approaches. 1</p><p>4 0.14146295 <a title="22-tfidf-4" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classiﬁcation models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods. 1</p><p>5 0.11307698 <a title="22-tfidf-5" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>Author: Nathan Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: √ ˜ We establish an excess risk bound of O HR2 + HL∗ Rn for ERM with an H-smooth loss n function and a hypothesis class with Rademacher complexity Rn , where L∗ is the best risk achievable by the hypothesis class. For typical hypothesis classes where Rn = R/n, this translates to ˜ ˜ a learning rate of O (RH/n) in the separable (L∗ = 0) case and O RH/n + L∗ RH/n more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective. 1</p><p>6 0.1099285 <a title="22-tfidf-6" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>7 0.095036253 <a title="22-tfidf-7" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>8 0.087350138 <a title="22-tfidf-8" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>9 0.08423762 <a title="22-tfidf-9" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>10 0.083766833 <a title="22-tfidf-10" href="./nips-2010-The_LASSO_risk%3A_asymptotic_results_and_real_world_examples.html">265 nips-2010-The LASSO risk: asymptotic results and real world examples</a></p>
<p>11 0.08094722 <a title="22-tfidf-11" href="./nips-2010-Throttling_Poisson_Processes.html">269 nips-2010-Throttling Poisson Processes</a></p>
<p>12 0.078405716 <a title="22-tfidf-12" href="./nips-2010-More_data_means_less_inference%3A_A_pseudo-max_approach_to_structured_learning.html">169 nips-2010-More data means less inference: A pseudo-max approach to structured learning</a></p>
<p>13 0.077366717 <a title="22-tfidf-13" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>14 0.076001577 <a title="22-tfidf-14" href="./nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">191 nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>15 0.074568212 <a title="22-tfidf-15" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<p>16 0.074490786 <a title="22-tfidf-16" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>17 0.072474532 <a title="22-tfidf-17" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>18 0.071723111 <a title="22-tfidf-18" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>19 0.07102631 <a title="22-tfidf-19" href="./nips-2010-SpikeAnts%2C_a_spiking_neuron_network_modelling_the_emergence_of_organization_in_a_complex_system.html">252 nips-2010-SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system</a></p>
<p>20 0.07015489 <a title="22-tfidf-20" href="./nips-2010-Graph-Valued_Regression.html">108 nips-2010-Graph-Valued Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, 0.026), (2, 0.116), (3, -0.019), (4, 0.006), (5, 0.112), (6, -0.142), (7, -0.106), (8, -0.018), (9, -0.228), (10, 0.013), (11, -0.063), (12, -0.095), (13, -0.046), (14, -0.07), (15, -0.002), (16, -0.083), (17, 0.03), (18, -0.012), (19, 0.034), (20, -0.008), (21, 0.028), (22, -0.041), (23, 0.066), (24, 0.025), (25, 0.021), (26, -0.001), (27, 0.012), (28, 0.028), (29, -0.079), (30, -0.038), (31, 0.001), (32, -0.037), (33, 0.038), (34, -0.126), (35, -0.008), (36, -0.009), (37, -0.18), (38, 0.07), (39, 0.049), (40, 0.011), (41, -0.065), (42, 0.073), (43, 0.017), (44, 0.032), (45, 0.03), (46, -0.082), (47, 0.014), (48, -0.073), (49, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95078665 <a title="22-lsi-1" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of estimating the Fα -measure of a given model as accurately as possible on a ﬁxed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reﬂect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. 1</p><p>2 0.80899179 <a title="22-lsi-2" href="./nips-2010-Agnostic_Active_Learning_Without_Constraints.html">27 nips-2010-Agnostic Active Learning Without Constraints</a></p>
<p>Author: Alina Beygelzimer, John Langford, Zhang Tong, Daniel J. Hsu</p><p>Abstract: We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classiﬁcation. 1</p><p>3 0.69981694 <a title="22-lsi-3" href="./nips-2010-Multi-View_Active_Learning_in_the_Non-Realizable_Case.html">173 nips-2010-Multi-View Active Learning in the Non-Realizable Case</a></p>
<p>Author: Wei Wang, Zhi-hua Zhou</p><p>Abstract: The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be O(log 1 ), contrasting to single-view setting where the polynomial improveǫ ment is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is O( 1 ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise, ǫ contrasting to previous polynomial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. 1</p><p>4 0.68052882 <a title="22-lsi-4" href="./nips-2010-Active_Learning_by_Querying_Informative_and_Representative_Examples.html">25 nips-2010-Active Learning by Querying Informative and Representative Examples</a></p>
<p>Author: Sheng-jun Huang, Rong Jin, Zhi-hua Zhou</p><p>Abstract: Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criteria for query selection, they are usually ad hoc in ﬁnding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed Q UIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed Q UIRE approach outperforms several state-of -the-art active learning approaches. 1</p><p>5 0.63781732 <a title="22-lsi-5" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed continuous optimization problem. The proposed active learning approach is independent of employed classiﬁcation models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods. 1</p><p>6 0.62483245 <a title="22-lsi-6" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>7 0.57333523 <a title="22-lsi-7" href="./nips-2010-Learning_Bounds_for_Importance_Weighting.html">142 nips-2010-Learning Bounds for Importance Weighting</a></p>
<p>8 0.56185734 <a title="22-lsi-8" href="./nips-2010-On_the_Theory_of_Learnining_with_Privileged_Information.html">191 nips-2010-On the Theory of Learnining with Privileged Information</a></p>
<p>9 0.4940879 <a title="22-lsi-9" href="./nips-2010-b-Bit_Minwise_Hashing_for_Estimating_Three-Way_Similarities.html">289 nips-2010-b-Bit Minwise Hashing for Estimating Three-Way Similarities</a></p>
<p>10 0.49055994 <a title="22-lsi-10" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>11 0.47724456 <a title="22-lsi-11" href="./nips-2010-Parallelized_Stochastic_Gradient_Descent.html">202 nips-2010-Parallelized Stochastic Gradient Descent</a></p>
<p>12 0.46330857 <a title="22-lsi-12" href="./nips-2010-Lower_Bounds_on_Rate_of_Convergence_of_Cutting_Plane_Methods.html">163 nips-2010-Lower Bounds on Rate of Convergence of Cutting Plane Methods</a></p>
<p>13 0.46160856 <a title="22-lsi-13" href="./nips-2010-Active_Learning_Applied_to_Patient-Adaptive_Heartbeat_Classification.html">24 nips-2010-Active Learning Applied to Patient-Adaptive Heartbeat Classification</a></p>
<p>14 0.46056089 <a title="22-lsi-14" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>15 0.45483655 <a title="22-lsi-15" href="./nips-2010-Learning_from_Candidate_Labeling_Sets.html">151 nips-2010-Learning from Candidate Labeling Sets</a></p>
<p>16 0.43770242 <a title="22-lsi-16" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>17 0.43229628 <a title="22-lsi-17" href="./nips-2010-Tight_Sample_Complexity_of_Large-Margin_Learning.html">270 nips-2010-Tight Sample Complexity of Large-Margin Learning</a></p>
<p>18 0.43177384 <a title="22-lsi-18" href="./nips-2010-Self-Paced_Learning_for_Latent_Variable_Models.html">235 nips-2010-Self-Paced Learning for Latent Variable Models</a></p>
<p>19 0.42865816 <a title="22-lsi-19" href="./nips-2010-Near-Optimal_Bayesian_Active_Learning_with_Noisy_Observations.html">180 nips-2010-Near-Optimal Bayesian Active Learning with Noisy Observations</a></p>
<p>20 0.41604987 <a title="22-lsi-20" href="./nips-2010-Semi-Supervised_Learning_with_Adversarially_Missing_Label_Information.html">236 nips-2010-Semi-Supervised Learning with Adversarially Missing Label Information</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.04), (17, 0.015), (27, 0.046), (30, 0.036), (35, 0.014), (45, 0.242), (50, 0.064), (52, 0.03), (60, 0.047), (77, 0.047), (78, 0.051), (90, 0.04), (93, 0.247)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87404573 <a title="22-lda-1" href="./nips-2010-Efficient_algorithms_for_learning_kernels_from_multiple_similarity_matrices_with_general_convex_loss_functions.html">72 nips-2010-Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</a></p>
<p>Author: Achintya Kundu, Vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-tal</p><p>Abstract: In this paper we consider the problem of learning an n × n kernel matrix from m(≥ 1) similarity matrices under general convex loss. Past research have extensively studied the m = 1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m > 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in 2 O( m ǫlog n ) iterations; in each iteration one solves an MKL involving m kernels 2 and m eigen-decomposition of n × n matrices. By suitably deﬁning a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efﬁcacy of the proposed algorithms.</p><p>same-paper 2 0.8277595 <a title="22-lda-2" href="./nips-2010-Active_Estimation_of_F-Measures.html">22 nips-2010-Active Estimation of F-Measures</a></p>
<p>Author: Christoph Sawade, Niels Landwehr, Tobias Scheffer</p><p>Abstract: We address the problem of estimating the Fα -measure of a given model as accurately as possible on a ﬁxed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reﬂect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of Fα -measures are more accurate than estimates based on instances sampled from the test distribution. 1</p><p>3 0.78919321 <a title="22-lda-3" href="./nips-2010-Construction_of_Dependent_Dirichlet_Processes_based_on_Poisson_Processes.html">51 nips-2010-Construction of Dependent Dirichlet Processes based on Poisson Processes</a></p>
<p>Author: Dahua Lin, Eric Grimson, John W. Fisher</p><p>Abstract: We present a novel method for constructing dependent Dirichlet processes. The approach exploits the intrinsic relationship between Dirichlet and Poisson processes in order to create a Markov chain of Dirichlet processes suitable for use as a prior over evolving mixture models. The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for model inference and test it on both synthetic and real data. Empirical results demonstrate that the approach is effective in estimating dynamically varying mixture models. 1</p><p>4 0.73867702 <a title="22-lda-4" href="./nips-2010-Joint_Cascade_Optimization_Using_A_Product_Of_Boosted_Classifiers.html">132 nips-2010-Joint Cascade Optimization Using A Product Of Boosted Classifiers</a></p>
<p>Author: Leonidas Lefakis, Francois Fleuret</p><p>Abstract: The standard strategy for efﬁcient object detection consists of building a cascade composed of several binary classiﬁers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classiﬁers, and concentrates the computation on difﬁcult parts of the image which cannot be trivially rejected. We introduce a novel algorithm to construct jointly the classiﬁers of such a cascade, which interprets the response of a classiﬁer as the probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set. Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efﬁciency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines. 1</p><p>5 0.73195761 <a title="22-lda-5" href="./nips-2010-Distributed_Dual_Averaging_In_Networks.html">63 nips-2010-Distributed Dual Averaging In Networks</a></p>
<p>Author: Alekh Agarwal, Martin J. Wainwright, John C. Duchi</p><p>Abstract: The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is conﬁrmed both by theoretical lower bounds and simulations for various networks. 1</p><p>6 0.73180318 <a title="22-lda-6" href="./nips-2010-Variable_margin_losses_for_classifier_design.html">282 nips-2010-Variable margin losses for classifier design</a></p>
<p>7 0.73007423 <a title="22-lda-7" href="./nips-2010-Learning_sparse_dynamic_linear_systems_using_stable_spline_kernels_and_exponential_hyperpriors.html">154 nips-2010-Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</a></p>
<p>8 0.72970223 <a title="22-lda-8" href="./nips-2010-Active_Instance_Sampling_via_Matrix_Partition.html">23 nips-2010-Active Instance Sampling via Matrix Partition</a></p>
<p>9 0.72956431 <a title="22-lda-9" href="./nips-2010-Multi-label_Multiple_Kernel_Learning_by_Stochastic_Approximation%3A_Application_to_Visual_Object_Recognition.html">174 nips-2010-Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition</a></p>
<p>10 0.72876883 <a title="22-lda-10" href="./nips-2010-Extended_Bayesian_Information_Criteria_for_Gaussian_Graphical_Models.html">87 nips-2010-Extended Bayesian Information Criteria for Gaussian Graphical Models</a></p>
<p>11 0.72862363 <a title="22-lda-11" href="./nips-2010-Learning_via_Gaussian_Herding.html">158 nips-2010-Learning via Gaussian Herding</a></p>
<p>12 0.7283361 <a title="22-lda-12" href="./nips-2010-Convex_Multiple-Instance_Learning_by_Estimating_Likelihood_Ratio.html">52 nips-2010-Convex Multiple-Instance Learning by Estimating Likelihood Ratio</a></p>
<p>13 0.72813857 <a title="22-lda-13" href="./nips-2010-Computing_Marginal_Distributions_over_Continuous_Markov_Networks_for_Statistical_Relational_Learning.html">49 nips-2010-Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning</a></p>
<p>14 0.72763389 <a title="22-lda-14" href="./nips-2010-Sidestepping_Intractable_Inference_with_Structured_Ensemble_Cascades.html">239 nips-2010-Sidestepping Intractable Inference with Structured Ensemble Cascades</a></p>
<p>15 0.72739422 <a title="22-lda-15" href="./nips-2010-Two-Layer_Generalization_Analysis_for_Ranking_Using_Rademacher_Average.html">277 nips-2010-Two-Layer Generalization Analysis for Ranking Using Rademacher Average</a></p>
<p>16 0.72720307 <a title="22-lda-16" href="./nips-2010-Smoothness%2C_Low_Noise_and_Fast_Rates.html">243 nips-2010-Smoothness, Low Noise and Fast Rates</a></p>
<p>17 0.72651672 <a title="22-lda-17" href="./nips-2010-Hashing_Hyperplane_Queries_to_Near_Points_with_Applications_to_Large-Scale_Active_Learning.html">112 nips-2010-Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning</a></p>
<p>18 0.72566217 <a title="22-lda-18" href="./nips-2010-%28RF%29%5E2_--_Random_Forest_Random_Field.html">1 nips-2010-(RF)^2 -- Random Forest Random Field</a></p>
<p>19 0.72523016 <a title="22-lda-19" href="./nips-2010-A_Primal-Dual_Algorithm_for_Group_Sparse_Regularization_with_Overlapping_Groups.html">12 nips-2010-A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups</a></p>
<p>20 0.72521335 <a title="22-lda-20" href="./nips-2010-Worst-Case_Linear_Discriminant_Analysis.html">287 nips-2010-Worst-Case Linear Discriminant Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
