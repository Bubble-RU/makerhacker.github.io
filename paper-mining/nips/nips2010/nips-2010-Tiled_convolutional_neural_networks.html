<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>271 nips-2010-Tiled convolutional neural networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2010" href="../home/nips2010_home.html">nips2010</a> <a title="nips-2010-271" href="#">nips2010-271</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>271 nips-2010-Tiled convolutional neural networks</h1>
<br/><p>Source: <a title="nips-2010-271-pdf" href="http://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks.pdf">pdf</a></p><p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>Reference: <a title="nips-2010-271-reference" href="../nips2010_reference/nips-2010-Tiled_convolutional_neural_networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. [sent-6, score-0.291]
</p><p>2 We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. [sent-8, score-1.061]
</p><p>3 By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. [sent-9, score-0.388]
</p><p>4 However, one disadvantage of this hard-coding approach is that the pooling architecture captures only translational invariance; the network does not, for example, pool across units that are rotations of each other or capture more complex invariances, such as out-of-plane rotations. [sent-17, score-0.521]
</p><p>5 Is it better to hard-code translational invariance – since this is a useful form of prior knowledge – or let the network learn its own invariances from unlabeled data? [sent-18, score-0.313]
</p><p>6 In particular, we present tiled convolutional networks (Tiled CNNs), which use a novel weight-tying scheme (“tiling”) that simultaneously enjoys the beneﬁt of signiﬁcantly reducing the number of learnable parameters while giving the algorithm ﬂexibility to learn other invariances. [sent-20, score-0.757]
</p><p>7 In order to learn these invariances from unlabeled data, we employ unsupervised pretraining, which has been shown to help performance [5, 6, 7]. [sent-22, score-0.224]
</p><p>8 In particular, we use a modiﬁcation of Topographic ICA (TICA) [8], which learns to organize features in a topographical map by pooling together groups 1  Figure 1: Left: Convolutional Neural Networks with local receptive ﬁelds and tied weights. [sent-23, score-0.415]
</p><p>9 Right: Partially untied local receptive ﬁeld networks – Tiled CNNs. [sent-24, score-0.32]
</p><p>10 Units with the same color belong to the same map; within each map, units with the same ﬁll texture have tied weights. [sent-25, score-0.224]
</p><p>11 By pooling together local groups of features, it produces representations that are robust to local transformations [9]. [sent-28, score-0.26]
</p><p>12 The resulting Tiled CNNs pretrained with TICA are indeed able to learn invariant representations, with pooling units that are robust to both scaling and rotation. [sent-30, score-0.418]
</p><p>13 2  Tiled CNNs  CNNs [1, 11] are based on two key concepts: local receptive ﬁelds, and weight-tying. [sent-32, score-0.172]
</p><p>14 Using local receptive ﬁelds means that each unit in the network only “looks” at a small, localized region of the input image. [sent-33, score-0.248]
</p><p>15 This is more computationally efﬁcient than having full receptive ﬁelds, and allows CNNs to scale up well. [sent-34, score-0.127]
</p><p>16 This reduces the number of learnable parameters, and (by pooling over neighboring units) further hard-codes translational invariance into the model. [sent-36, score-0.338]
</p><p>17 Even though weight-tying allows one to hard-code translational invariance, it also prevents the pooling units from capturing more complex invariances, such as scale and rotation invariance. [sent-37, score-0.417]
</p><p>18 This is because the second layer units are constrained to pool over translations of identical bases. [sent-38, score-0.295]
</p><p>19 This lets second-layer units pool over simple units that have different basis functions, and hence learn a more complex range of invariances. [sent-40, score-0.429]
</p><p>20 ” Tiled CNNs are parametrized by a tile size k: we constrain only units that are k steps away from each other to be tied. [sent-42, score-0.293]
</p><p>21 By varying k, we obtain a spectrum of models which trade off between being able to learn complex invariances, and having few learnable parameters. [sent-43, score-0.133]
</p><p>22 At one end of the spectrum we have traditional CNNs (k = 1), and at the other, we have fully untied simple units. [sent-44, score-0.13]
</p><p>23 A map is a set of pooling units and simple units that collectively cover the entire image (see Figure 1-Right). [sent-46, score-0.527]
</p><p>24 When varying the tiling size, we change the degree of weight tying within each map; for example, if k = 1, the simple units within each map will have the same weights. [sent-47, score-0.256]
</p><p>25 In our model, simple units in different maps are never tied. [sent-48, score-0.257]
</p><p>26 By having units in different maps learn different features, our model can learn a rich and diverse set of features. [sent-49, score-0.317]
</p><p>27 Tiled CNNs with multiple maps enjoy the twin beneﬁts of (i) being able to represent complex invariances, by pooling over (partially) untied weights, and (ii) having a relatively small number of learnable parameters. [sent-50, score-0.43]
</p><p>28 Unfortunately, existing methods for pretraining CNNs [11, 12] are not suitable for untied weights; for example, the CDBN algorithm [11] breaks down without the weight-tying constraints. [sent-53, score-0.25]
</p><p>29 In the following sections, we discuss a pretraining method for Tiled CNNs based on the TICA algorithm. [sent-54, score-0.137]
</p><p>30 3  Unsupervised feature learning via TICA  TICA is an unsupervised learning algorithm that learns features from unlabeled image patches. [sent-55, score-0.13]
</p><p>31 The weights W in the ﬁrst layer are learned, while the weights V in the second layer are ﬁxed and hard-coded to represent the neighborhood/topographical structure of the neurons in the ﬁrst layer. [sent-57, score-0.308]
</p><p>32 Speciﬁcally, each second layer hidden unit pi pools over a small neighborhood of adjacent ﬁrst layer units hi . [sent-58, score-0.494]
</p><p>33 We call the hi and pi simple and pooling units, respectively. [sent-59, score-0.233]
</p><p>34 More precisely, given an input pattern x(t) , the activation of each second layer unit is m  (t)  n  2 pi (x(t) ; W, V ) = k=1 Vik ( j=1 Wkj xj ) . [sent-60, score-0.158]
</p><p>35 1 Here, W ∈ Rm×n and V ∈ Rm×m , where n t=1 is the size of the input and m is the number of hidden units in a layer. [sent-62, score-0.194]
</p><p>36 V is a ﬁxed matrix (Vij = 1 or 0) that encodes the 2D topography of the hidden units hi . [sent-63, score-0.252]
</p><p>37 Speciﬁcally, the hi units lie on a 2D grid, with each pi connected to a contiguous 3x3 (or other size) block of hi units. [sent-64, score-0.306]
</p><p>38 One important property of TICA is that it can learn invariances even when trained only on unlabeled data, as demonstrated in [8, 9]. [sent-67, score-0.187]
</p><p>39 This is due both to the pooling architecture, which gives rise to pooling units that are robust to local transformations of their inputs, and the learning algorithm, which promotes selectivity by optimizing for sparsity. [sent-68, score-0.524]
</p><p>40 If we choose square and square-root activations for the simple and pooling units in the Tiled CNN, we can view the Tiled CNN as a special case of a TICA network, with the topography of the pooling units specifying the matrix V . [sent-70, score-0.684]
</p><p>41 3 Crucially, Tiled CNNs incorporate local receptive ﬁelds, which play an important role in speeding up TICA. [sent-71, score-0.172]
</p><p>42 When learning overcomplete representations [14], the orthogonality constraint cannot be satisﬁed exactly, and we instead try to satisfy an approximate orthogonality constraint [15]. [sent-77, score-0.154]
</p><p>43 We can avoid approximate orthogonalization by using local receptive ﬁelds, which are inherently built into Tiled CNNs. [sent-80, score-0.249]
</p><p>44 This locality constraint automatically ensures that the weights of any two simple units with non-overlapping receptive ﬁelds are orthogonal, without the need for an explicit orthogonality constraint. [sent-82, score-0.428]
</p><p>45 Empirically, we ﬁnd that orthogonalizing partially overlapping receptive ﬁelds is not necessary for learning distinct, informative features either. [sent-83, score-0.15]
</p><p>46 However, orthogonalization is still needed to decorrelate units that occupy the same position in their respective maps, for they look at the same region on the image. [sent-84, score-0.254]
</p><p>47 Speciﬁcally, so long as l ≤ s, we can demand that these l units that share an input patch be orthogonal. [sent-86, score-0.196]
</p><p>48 We note that setting k to its maximum value of n − s + 1 gives exactly the untied local TICA model outlined in the previous section. [sent-93, score-0.158]
</p><p>49 5α end while W ← W new until convergence  Our pretraining algorithm, which is based on gradient descent on the TICA objective function (1), is shown in Algorithm 1. [sent-95, score-0.137]
</p><p>50 For example, when trained on natural images, TICA’s ﬁrst layer weights usually resemble localized Gabor ﬁlters (Figure 2-Right). [sent-98, score-0.195]
</p><p>51 4  In orthogonalize local RF (W new ), we only orthogonalize the weights that have completely overlapping receptive ﬁelds. [sent-100, score-0.359]
</p><p>52 1  Speed-up  We ﬁrst establish that the local receptive ﬁelds intrinsic to Tiled CNNs allows us to implement TICA learning for overcomplete representations in a much more efﬁcient manner. [sent-106, score-0.246]
</p><p>53 Figure 3 shows the relative speed-up of pretraining Tiled CNNs over standard TICA using approximate ﬁxed-point orthogonalization 1 3 (W = 2 W − 2 W W T W )[15]. [sent-107, score-0.214]
</p><p>54 Hence, the speed-up observed here is not from an efﬁcient convolutional implementation, but purely due to the local receptive ﬁelds. [sent-112, score-0.298]
</p><p>55 2  Classiﬁcation on NORB  Next, we show that TICA pretraining for Tiled CNNs performs well on object recognition. [sent-116, score-0.166]
</p><p>56 6 In our classiﬁcation experiments, we ﬁx the size of each local receptive ﬁeld to 8x8, and set V such that each pooling unit pi in the second layer pools over a block of 3x3 simple units in the ﬁrst layer, without wraparound at the borders. [sent-119, score-0.694]
</p><p>57 The number of pooling units in each map is exactly the same as the number of simple units. [sent-120, score-0.35]
</p><p>58 We densely tile the input images with overlapping 8x8 local receptive ﬁelds, with a step size (or “stride”) of 1. [sent-121, score-0.314]
</p><p>59 This gives us 25 × 25 = 625 simple units and 625 pooling units per map in our experiments on 32x32 images. [sent-122, score-0.527]
</p><p>60 1  Unsupervised pretraining  We ﬁrst consider the case in which the features are learned purely from unsupervised data. [sent-126, score-0.258]
</p><p>61 We call this initial phase the unsupervised pretraining phase. [sent-147, score-0.198]
</p><p>62 , the activations of the pooling units) on the labeled training set. [sent-150, score-0.172]
</p><p>63 During this supervised training phase, only the weights of the linear classiﬁer were learned, while the lower weights of the Tiled CNN model remained ﬁxed. [sent-151, score-0.172]
</p><p>64 We train a range of models to investigate the role of the tile size k and the number of maps l. [sent-152, score-0.196]
</p><p>65 Using a randomly sampled hold-out validation set of 2430 examples (10%) taken from the training set, we selected a convolutional model with 48 maps that achieved an accuracy of 94. [sent-154, score-0.263]
</p><p>66 2  Supervised ﬁnetuning of W  Next, we study the effects of supervised ﬁnetuning [23] on the models produced by the unsupervised pretraining phase. [sent-158, score-0.227]
</p><p>67 Using softmax regression to calculate the gradients, we backpropagated the error signal from the output back to the learned features in order to update W , the weights of the simple units in the Tiled CNN model. [sent-160, score-0.281]
</p><p>68 The best performing ﬁne-tuned model on the validation set was the model with 16 maps and k = 2, which achieved a test-set accuracy of 96. [sent-165, score-0.133]
</p><p>69 3  Limited training data  To test the ability of our pretrained features to generalize across rotations and lighting conditions given only a weak supervised signal, we limited the labeled training set to comprise only examples with a particular set of viewing angles and lighting conditions. [sent-170, score-0.226]
</p><p>70 Models were trained with various untied map sizes k ∈ {1, 2, 9, 16, 25} and number of maps l ∈ {4, 6, 10, 16}. [sent-175, score-0.239]
</p><p>71 When k = 1, we were able to use an efﬁcient convolutional implementation to scale up the number of maps in the models, allowing us to train additional models with l ∈ {22, 36, 48}. [sent-176, score-0.189]
</p><p>72 6  Figure 4: Left: NORB test set accuracy across various tile sizes and numbers of maps, without ﬁnetuning. [sent-177, score-0.144]
</p><p>73 5x overcomplete model with k = 2 and 4 maps obtained an accuracy of 64. [sent-187, score-0.163]
</p><p>74 1  Unsupervised pretraining and supervised ﬁnetuning  As before, models were trained with tile size k ∈ {1, 2, 25}, and number of maps l ∈ {4, 10, 16, 22, 32}. [sent-213, score-0.386]
</p><p>75 The convolutional model (k = 1) was also trained with l = 48 maps. [sent-214, score-0.133]
</p><p>76 This 48-map convolutional model performed the best on our 10% hold-out validation set, and achieved a test set accuracy of 66. [sent-215, score-0.179]
</p><p>77 We ﬁnd that supervised ﬁnetuning of these models on CIFAR-10 causes overﬁtting, and generally reduces test-set accuracy; the top model on the validation set, with 32 maps and k = 1, only achieves 65. [sent-217, score-0.134]
</p><p>78 2  Deep Tiled CNNs  We additionally investigate the possibility of training a deep Tiled CNN in a greedy layer-wise fashion, similar to models such as DBNs [6] and stacked autoencoders [26, 18]. [sent-223, score-0.139]
</p><p>79 The resulting four-layer network has the structure W1 → V1 → W2 → V2 , where the weights W1 are local receptive ﬁelds of size 4x4, and W2 is of size 3x3, i. [sent-225, score-0.296]
</p><p>80 , each unit in the third layer “looks” at a 3x3 window of each of the 10 maps in the ﬁrst layer. [sent-227, score-0.203]
</p><p>81 The number of maps in the third and fourth layer is also 10. [sent-229, score-0.173]
</p><p>82 After ﬁnetuning, we found that the deep model outperformed all previous models on the validation set, and achieved a test set accuracy of 73. [sent-230, score-0.166]
</p><p>83 This demonstrates the potential of deep Tiled CNNs to learn more complex representations. [sent-232, score-0.146]
</p><p>84 4  Effects of optimizing the pooling units  When the tile size is 1 (i. [sent-234, score-0.444]
</p><p>85 , a fully tied model), a na¨ve approach to learn the ﬁlter weights is to ı directly train the ﬁrst layer ﬁlters using small patches (e. [sent-236, score-0.231]
</p><p>86 We use ICA to learn the ﬁrst layer weights on CIFAR-10 with 16 ﬁlters. [sent-241, score-0.184]
</p><p>87 These weights are then used in a Tiled CNN with a tile size of 1 and 16 maps. [sent-242, score-0.177]
</p><p>88 This method is compared to pretraining the model of the same architecture with TICA. [sent-243, score-0.168]
</p><p>89 54% on the test set, while pretraining with TICA achieves ı 58. [sent-246, score-0.154]
</p><p>90 These results conﬁrm that optimizing for sparsity of the pooling units results in better features than just na¨vely approximating the ﬁrst layer weights. [sent-248, score-0.444]
</p><p>91 Speciﬁcally, we ﬁnd that selecting a tile size of k = 2 achieves the best results for both the NORB and CIFAR-10 datasets, even with deep networks. [sent-250, score-0.212]
</p><p>92 More importantly, untying weights allow the networks to learn more complex invariances from unlabeled data. [sent-251, score-0.329]
</p><p>93 By visualizing [28, 29] the range of optimal stimulus that activate each pooling unit in a Tiled CNN, we found units that were scale and rotationally invariant. [sent-252, score-0.376]
</p><p>94 A natural choice of the tile size k would be to set it to the size of the pooling region p, which in this case is 3. [sent-254, score-0.284]
</p><p>95 In this case, each pooling unit always combines simple units which are not tied. [sent-255, score-0.358]
</p><p>96 Our preliminary results on networks pretrained using 250000 unlabeled images from the Tiny images dataset [30] show that performance increases as k goes from 1 to 3, ﬂattening out at k = 4. [sent-258, score-0.173]
</p><p>97 In this paper, we introduced Tiled CNNs as an extension of CNNs that support both unsupervised pretraining and weight tiling. [sent-260, score-0.198]
</p><p>98 Furthermore, the use of local receptive ﬁelds enable our models to scale up well, producing massively overcomplete representations that perform well on classiﬁcation tasks. [sent-262, score-0.246]
</p><p>99 Best practices for convolutional neural networks applied to visual document analysis. [sent-281, score-0.144]
</p><p>100 Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. [sent-336, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cnns', 0.526), ('tiled', 0.517), ('tica', 0.33), ('netuning', 0.188), ('units', 0.177), ('norb', 0.175), ('pooling', 0.151), ('pretraining', 0.137), ('receptive', 0.127), ('untied', 0.113), ('convolutional', 0.109), ('tile', 0.099), ('deep', 0.096), ('cnn', 0.095), ('layer', 0.093), ('invariances', 0.087), ('maps', 0.08), ('orthogonalization', 0.077), ('translational', 0.069), ('learnable', 0.066), ('orthogonalize', 0.063), ('weights', 0.061), ('unsupervised', 0.061), ('overcomplete', 0.055), ('invariance', 0.052), ('untying', 0.05), ('wkj', 0.05), ('hi', 0.047), ('tied', 0.047), ('elds', 0.047), ('unlabeled', 0.046), ('local', 0.045), ('pretrained', 0.04), ('orthogonality', 0.04), ('tiling', 0.038), ('vik', 0.038), ('pi', 0.035), ('networks', 0.035), ('topographic', 0.034), ('saxe', 0.033), ('architecture', 0.031), ('tiny', 0.031), ('hyvarinen', 0.03), ('unit', 0.03), ('learn', 0.03), ('network', 0.029), ('object', 0.029), ('supervised', 0.029), ('topography', 0.028), ('accuracy', 0.028), ('lighting', 0.028), ('images', 0.026), ('layers', 0.026), ('validation', 0.025), ('qpm', 0.025), ('pool', 0.025), ('trained', 0.024), ('na', 0.024), ('rf', 0.023), ('locality', 0.023), ('features', 0.023), ('classi', 0.022), ('sees', 0.022), ('ica', 0.022), ('map', 0.022), ('elevations', 0.022), ('autoencoders', 0.022), ('rbm', 0.022), ('bengio', 0.021), ('training', 0.021), ('recognition', 0.021), ('learned', 0.02), ('complex', 0.02), ('lcc', 0.02), ('azimuths', 0.02), ('koh', 0.02), ('invariant', 0.02), ('ranzato', 0.02), ('representations', 0.019), ('binocular', 0.019), ('pools', 0.019), ('rgb', 0.019), ('tying', 0.019), ('courville', 0.019), ('erhan', 0.019), ('rotations', 0.019), ('patch', 0.019), ('tie', 0.018), ('visualizing', 0.018), ('boltzmann', 0.018), ('purely', 0.017), ('localized', 0.017), ('size', 0.017), ('pixels', 0.017), ('spectrum', 0.017), ('coding', 0.017), ('test', 0.017), ('kavukcuoglu', 0.016), ('whitened', 0.016), ('avoided', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="271-tfidf-1" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>2 0.17771298 <a title="271-tfidf-2" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>3 0.14179838 <a title="271-tfidf-3" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>Author: Koray Kavukcuoglu, Pierre Sermanet, Y-lan Boureau, Karol Gregor, Michael Mathieu, Yann L. Cun</p><p>Abstract: We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting ﬁlters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efﬁciency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efﬁcient feed-forward encoder that predicts quasisparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse ﬁlters, including center-surround ﬁlters, corner detectors, cross detectors, and oriented grating detectors. We show that using these ﬁlters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks. 1</p><p>4 0.11484646 <a title="271-tfidf-4" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>5 0.08888337 <a title="271-tfidf-5" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>Author: Yuanqing Lin, Zhang Tong, Shenghuo Zhu, Kai Yu</p><p>Abstract: This paper proposes a principled extension of the traditional single-layer ﬂat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p><p>6 0.08406055 <a title="271-tfidf-6" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>7 0.079395019 <a title="271-tfidf-7" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>8 0.078420728 <a title="271-tfidf-8" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>9 0.064574555 <a title="271-tfidf-9" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>10 0.054200105 <a title="271-tfidf-10" href="./nips-2010-Infinite_Relational_Modeling_of_Functional_Connectivity_in_Resting_State_fMRI.html">128 nips-2010-Infinite Relational Modeling of Functional Connectivity in Resting State fMRI</a></p>
<p>11 0.053554326 <a title="271-tfidf-11" href="./nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">141 nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>12 0.049009785 <a title="271-tfidf-12" href="./nips-2010-Kernel_Descriptors_for_Visual_Recognition.html">133 nips-2010-Kernel Descriptors for Visual Recognition</a></p>
<p>13 0.047017235 <a title="271-tfidf-13" href="./nips-2010-Gaussian_sampling_by_local_perturbations.html">101 nips-2010-Gaussian sampling by local perturbations</a></p>
<p>14 0.046544883 <a title="271-tfidf-14" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>15 0.046166461 <a title="271-tfidf-15" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>16 0.045937464 <a title="271-tfidf-16" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>17 0.045188177 <a title="271-tfidf-17" href="./nips-2010-Regularized_estimation_of_image_statistics_by_Score_Matching.html">224 nips-2010-Regularized estimation of image statistics by Score Matching</a></p>
<p>18 0.041025124 <a title="271-tfidf-18" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>19 0.040584657 <a title="271-tfidf-19" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>20 0.039384197 <a title="271-tfidf-20" href="./nips-2010-Inferring_Stimulus_Selectivity_from_the_Spatial_Structure_of_Neural_Network_Dynamics.html">127 nips-2010-Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, 0.058), (2, -0.122), (3, -0.072), (4, 0.028), (5, -0.01), (6, 0.0), (7, 0.043), (8, -0.065), (9, 0.025), (10, 0.027), (11, -0.045), (12, 0.054), (13, -0.156), (14, -0.107), (15, -0.06), (16, -0.042), (17, -0.051), (18, -0.101), (19, -0.127), (20, 0.05), (21, 0.065), (22, -0.062), (23, 0.041), (24, 0.021), (25, 0.111), (26, 0.029), (27, 0.03), (28, 0.056), (29, 0.057), (30, -0.059), (31, -0.107), (32, -0.004), (33, 0.103), (34, 0.077), (35, 0.026), (36, 0.032), (37, 0.026), (38, 0.008), (39, 0.02), (40, 0.01), (41, -0.015), (42, -0.033), (43, -0.007), (44, 0.023), (45, -0.038), (46, 0.025), (47, -0.007), (48, -0.022), (49, -0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92576438 <a title="271-lsi-1" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>2 0.82755792 <a title="271-lsi-2" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>Author: Grégoire Montavon, Klaus-Robert Müller, Mikio L. Braun</p><p>Abstract: Deep networks can potentially express a learning problem more efﬁciently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers. 1</p><p>3 0.74577129 <a title="271-lsi-3" href="./nips-2010-Phone_Recognition_with_the_Mean-Covariance_Restricted_Boltzmann_Machine.html">206 nips-2010-Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine</a></p>
<p>Author: George Dahl, Marc'aurelio Ranzato, Abdel-rahman Mohamed, Geoffrey E. Hinton</p><p>Abstract: Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the ﬁrst-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonalcovariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), ﬁrst introduced for modeling natural images, is a much more representationally efﬁcient and powerful way of modeling the covariance structure of speech data. Every conﬁguration of the precision units of the mcRBM speciﬁes a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date. 1</p><p>4 0.69564748 <a title="271-lsi-4" href="./nips-2010-Hallucinations_in_Charles_Bonnet_Syndrome_Induced_by_Homeostasis%3A_a_Deep_Boltzmann_Machine_Model.html">111 nips-2010-Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model</a></p>
<p>Author: Peggy Series, David P. Reichert, Amos J. Storkey</p><p>Abstract: The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative ﬁndings in CBS. We also introduce a modiﬁcation to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception. 1</p><p>5 0.65156341 <a title="271-lsi-5" href="./nips-2010-Learning_Convolutional_Feature_Hierarchies_for_Visual_Recognition.html">143 nips-2010-Learning Convolutional Feature Hierarchies for Visual Recognition</a></p>
<p>Author: Koray Kavukcuoglu, Pierre Sermanet, Y-lan Boureau, Karol Gregor, Michael Mathieu, Yann L. Cun</p><p>Abstract: We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting ﬁlters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efﬁciency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efﬁcient feed-forward encoder that predicts quasisparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse ﬁlters, including center-surround ﬁlters, corner detectors, cross detectors, and oriented grating detectors. We show that using these ﬁlters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks. 1</p><p>6 0.60763472 <a title="271-lsi-6" href="./nips-2010-Gated_Softmax_Classification.html">99 nips-2010-Gated Softmax Classification</a></p>
<p>7 0.56349558 <a title="271-lsi-7" href="./nips-2010-Deep_Coding_Network.html">59 nips-2010-Deep Coding Network</a></p>
<p>8 0.54291987 <a title="271-lsi-8" href="./nips-2010-Learning_to_combine_foveal_glimpses_with_a_third-order_Boltzmann_machine.html">156 nips-2010-Learning to combine foveal glimpses with a third-order Boltzmann machine</a></p>
<p>9 0.52311844 <a title="271-lsi-9" href="./nips-2010-Phoneme_Recognition_with_Large_Hierarchical_Reservoirs.html">207 nips-2010-Phoneme Recognition with Large Hierarchical Reservoirs</a></p>
<p>10 0.47683284 <a title="271-lsi-10" href="./nips-2010-Pose-Sensitive_Embedding_by_Nonlinear_NCA_Regression.html">209 nips-2010-Pose-Sensitive Embedding by Nonlinear NCA Regression</a></p>
<p>11 0.44892406 <a title="271-lsi-11" href="./nips-2010-An_analysis_on_negative_curvature_induced_by_singularity_in_multi-layer_neural-network_learning.html">31 nips-2010-An analysis on negative curvature induced by singularity in multi-layer neural-network learning</a></p>
<p>12 0.40556392 <a title="271-lsi-12" href="./nips-2010-Generating_more_realistic_images_using_gated_MRF%27s.html">103 nips-2010-Generating more realistic images using gated MRF's</a></p>
<p>13 0.3932831 <a title="271-lsi-13" href="./nips-2010-An_Alternative_to_Low-level-Sychrony-Based_Methods_for_Speech_Detection.html">28 nips-2010-An Alternative to Low-level-Sychrony-Based Methods for Speech Detection</a></p>
<p>14 0.37908107 <a title="271-lsi-14" href="./nips-2010-On_Herding_and_the_Perceptron_Cycling_Theorem.html">188 nips-2010-On Herding and the Perceptron Cycling Theorem</a></p>
<p>15 0.35405609 <a title="271-lsi-15" href="./nips-2010-Layered_image_motion_with_explicit_occlusions%2C_temporal_consistency%2C_and_depth_ordering.html">141 nips-2010-Layered image motion with explicit occlusions, temporal consistency, and depth ordering</a></p>
<p>16 0.34711111 <a title="271-lsi-16" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>17 0.33566248 <a title="271-lsi-17" href="./nips-2010-Towards_Holistic_Scene_Understanding%3A_Feedback_Enabled_Cascaded_Classification_Models.html">272 nips-2010-Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models</a></p>
<p>18 0.3286784 <a title="271-lsi-18" href="./nips-2010-Energy_Disaggregation_via_Discriminative_Sparse_Coding.html">76 nips-2010-Energy Disaggregation via Discriminative Sparse Coding</a></p>
<p>19 0.32634583 <a title="271-lsi-19" href="./nips-2010-The_Maximal_Causes_of_Natural_Scenes_are_Edge_Filters.html">266 nips-2010-The Maximal Causes of Natural Scenes are Edge Filters</a></p>
<p>20 0.31868184 <a title="271-lsi-20" href="./nips-2010-Feature_Set_Embedding_for_Incomplete_Data.html">94 nips-2010-Feature Set Embedding for Incomplete Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.034), (17, 0.058), (27, 0.089), (30, 0.022), (35, 0.041), (45, 0.156), (46, 0.283), (50, 0.035), (52, 0.077), (59, 0.013), (60, 0.029), (77, 0.035), (78, 0.013), (90, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71593255 <a title="271-lda-1" href="./nips-2010-Tiled_convolutional_neural_networks.html">271 nips-2010-Tiled convolutional neural networks</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang W. Koh, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efﬁcient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets. 1</p><p>2 0.60856384 <a title="271-lda-2" href="./nips-2010-Implicit_Differentiation_by_Perturbation.html">118 nips-2010-Implicit Differentiation by Perturbation</a></p>
<p>Author: Justin Domke</p><p>Abstract: This paper proposes a simple and eﬃcient ﬁnite diﬀerence method for implicit diﬀerentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, deﬁned on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to ﬁt graphical models with hidden variables, high treewidth and/or model misspeciﬁcation. 1</p><p>3 0.57229191 <a title="271-lda-3" href="./nips-2010-Fractionally_Predictive_Spiking_Neurons.html">96 nips-2010-Fractionally Predictive Spiking Neurons</a></p>
<p>Author: Jaldert Rombouts, Sander M. Bohte</p><p>Abstract: Recent experimental work has suggested that the neural ﬁring rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron sufﬁces to carry out such an approximation, given a suitable refractory response. Empirically, we ﬁnd that the online approximation of signals with a sum of powerlaw kernels is beneﬁcial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spiketrains by a receiving neuron allows for natural and transparent temporal signal ﬁltering by tuning the weights of the decoding kernel. 1</p><p>4 0.57205784 <a title="271-lda-4" href="./nips-2010-Accounting_for_network_effects_in_neuronal_responses_using_L1_regularized_point_process_models.html">21 nips-2010-Accounting for network effects in neuronal responses using L1 regularized point process models</a></p>
<p>Author: Ryan Kelly, Matthew Smith, Robert Kass, Tai S. Lee</p><p>Abstract: Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive ﬁeld or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable inﬂuence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the ﬁring rate of many individual units recorded simultaneously from V1 with a 96-electrode “Utah” array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron’s response, in addition to the neuron’s receptive ﬁeld transfer function. We also found that the same spikes could be accounted for with the local ﬁeld potentials, a surrogate measure of global network states. This work shows that accounting for network ﬂuctuations can improve estimates of single trial ﬁring rate and stimulus-response transfer functions. 1</p><p>5 0.56986809 <a title="271-lda-5" href="./nips-2010-Group_Sparse_Coding_with_a_Laplacian_Scale_Mixture_Prior.html">109 nips-2010-Group Sparse Coding with a Laplacian Scale Mixture Prior</a></p>
<p>Author: Pierre Garrigues, Bruno A. Olshausen</p><p>Abstract: We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefﬁcients. Each coefﬁcient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efﬁcient inference procedures for both the coefﬁcients and the scale parameter. When the scale parameters of a group of coefﬁcients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude ﬂuctuations among coefﬁcients, which have been shown to constitute a large fraction of the redundancy in natural images [1]. We show that, as a consequence of this group sparse coding, the resulting inference of the coefﬁcients follows a divisive normalization rule, and that this may be efﬁciently implemented in a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model. 1</p><p>6 0.56483638 <a title="271-lda-6" href="./nips-2010-Deciphering_subsampled_data%3A_adaptive_compressive_sampling_as_a_principle_of_brain_communication.html">56 nips-2010-Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</a></p>
<p>7 0.56422621 <a title="271-lda-7" href="./nips-2010-Fast_detection_of_multiple_change-points_shared_by_many_signals_using_group_LARS.html">91 nips-2010-Fast detection of multiple change-points shared by many signals using group LARS</a></p>
<p>8 0.56245762 <a title="271-lda-8" href="./nips-2010-Linear_readout_from_a_neural_population_with_partial_correlation_data.html">161 nips-2010-Linear readout from a neural population with partial correlation data</a></p>
<p>9 0.5618552 <a title="271-lda-9" href="./nips-2010-Short-term_memory_in_neuronal_networks_through_dynamical_compressed_sensing.html">238 nips-2010-Short-term memory in neuronal networks through dynamical compressed sensing</a></p>
<p>10 0.56164849 <a title="271-lda-10" href="./nips-2010-A_biologically_plausible_network_for_the_computation_of_orientation_dominance.html">17 nips-2010-A biologically plausible network for the computation of orientation dominance</a></p>
<p>11 0.55988663 <a title="271-lda-11" href="./nips-2010-Brain_covariance_selection%3A_better_individual_functional_connectivity_models_using_population_prior.html">44 nips-2010-Brain covariance selection: better individual functional connectivity models using population prior</a></p>
<p>12 0.55983347 <a title="271-lda-12" href="./nips-2010-Over-complete_representations_on_recurrent_neural_networks_can_support_persistent_percepts.html">200 nips-2010-Over-complete representations on recurrent neural networks can support persistent percepts</a></p>
<p>13 0.55949402 <a title="271-lda-13" href="./nips-2010-Layer-wise_analysis_of_deep_networks_with_Gaussian_kernels.html">140 nips-2010-Layer-wise analysis of deep networks with Gaussian kernels</a></p>
<p>14 0.55685949 <a title="271-lda-14" href="./nips-2010-Divisive_Normalization%3A_Justification_and_Effectiveness_as_Efficient_Coding_Transform.html">65 nips-2010-Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform</a></p>
<p>15 0.55540782 <a title="271-lda-15" href="./nips-2010-Joint_Analysis_of_Time-Evolving_Binary_Matrices_and_Associated_Documents.html">131 nips-2010-Joint Analysis of Time-Evolving Binary Matrices and Associated Documents</a></p>
<p>16 0.55302984 <a title="271-lda-16" href="./nips-2010-The_Neural_Costs_of_Optimal_Control.html">268 nips-2010-The Neural Costs of Optimal Control</a></p>
<p>17 0.55263102 <a title="271-lda-17" href="./nips-2010-Functional_form_of_motion_priors_in_human_motion_perception.html">98 nips-2010-Functional form of motion priors in human motion perception</a></p>
<p>18 0.55254489 <a title="271-lda-18" href="./nips-2010-A_novel_family_of_non-parametric_cumulative_based_divergences_for_point_processes.html">18 nips-2010-A novel family of non-parametric cumulative based divergences for point processes</a></p>
<p>19 0.55074418 <a title="271-lda-19" href="./nips-2010-Identifying_graph-structured_activation_patterns_in_networks.html">117 nips-2010-Identifying graph-structured activation patterns in networks</a></p>
<p>20 0.5500952 <a title="271-lda-20" href="./nips-2010-A_Family_of_Penalty_Functions_for_Structured_Sparsity.html">7 nips-2010-A Family of Penalty Functions for Structured Sparsity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
