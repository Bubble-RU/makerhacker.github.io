<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-147" href="#">nips2003-147</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</h1>
<br/><p>Source: <a title="nips-2003-147-pdf" href="http://papers.nips.cc/paper/2528-online-learning-via-global-feedback-for-phrase-recognition.pdf">pdf</a></p><p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>Reference: <a title="nips-2003-147-reference" href="../nips2003_reference/nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 es  Abstract This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. [sent-3, score-0.919]
</p><p>2 The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. [sent-4, score-1.453]
</p><p>3 We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. [sent-5, score-0.242]
</p><p>4 Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. [sent-6, score-0.364]
</p><p>5 1  Introduction  Over the past few years, many machine learning methods have been successfully applied to Natural Language tasks in which phrases of some type have to be recognized. [sent-7, score-0.621]
</p><p>6 Generally, given an input sentence —as a sequence of words— the task is to predict a bracketing for the sentence representing a structure of phrases, either sequential or hierarchical. [sent-8, score-0.334]
</p><p>7 For instance, syntactic analysis of Natural Language provides several problems of this type, such as partial parsing tasks [1, 2], or even full parsing [3]. [sent-9, score-0.161]
</p><p>8 The general approach consists of decomposing the global phrase recognition problem into a number of local learnable subproblems, and infer the global solution from the outcomes of the local subproblems. [sent-10, score-0.923]
</p><p>9 For chunking problems —in which phrases are sequentially structured— the approach is typically to perform a tagging. [sent-11, score-0.619]
</p><p>10 In this case, local subproblems include learning whether a word opens, closes, or is inside a phrase of some type (noun phrase, verb phrase, . [sent-12, score-0.898]
</p><p>11 When hierarchical structure has to be recognized, additional local decisions are required to determine the embedding of phrases, resulting in a more complex inference process which recursively builds the global solution [3, 2, 6, 7]. [sent-16, score-0.157]
</p><p>12 However, when performing the phrase recognition task, the classiﬁers are used together and dependently, in the sense that one classiﬁer predictions’ may affect the prediction of another. [sent-20, score-0.739]
</p><p>13 Indeed, the global performance of a system is measured in terms of precision and recall of the recognized phrases, which, although related, is not the local classiﬁcation accuracy measure for which the local classiﬁers are usually trained. [sent-21, score-0.223]
</p><p>14 The general idea consists of moving the learning strategy from the binary classiﬁcation setting to a general ranking context into which the global problem can be casted. [sent-23, score-0.204]
</p><p>15 Crammer and Singer [8] present a label-ranking algorithm, in which several perceptrons receive feedback from the ranking they produce over a training instance. [sent-24, score-0.182]
</p><p>16 For structured outputs, and motivating this work, Collins [10] introduces a variant of the perceptron for tagging tasks, in which the learning feedback is globally given from the output of the Viterbi decoding algorithm. [sent-27, score-0.153]
</p><p>17 In this paper we present a global learning strategy for the general task of recognizing phrases in a sentence. [sent-28, score-0.734]
</p><p>18 We adopt the general phrase recognition strategy of our previous work [6]. [sent-29, score-0.741]
</p><p>19 Given a sentence, learning is ﬁrst applied at word level to identify phrase candidates of the solution. [sent-30, score-0.895]
</p><p>20 Then, learning is applied at a higher-order level in which phrase candidates are scored to discriminate among competing ones. [sent-31, score-0.778]
</p><p>21 The overall strategy infers the global solution by exploring with learning components a number of plausible solutions. [sent-32, score-0.152]
</p><p>22 When visiting a sentence, the perceptrons are ﬁrst used to recognize the set of phrases, and then updated according to the correctness of the global solution. [sent-35, score-0.169]
</p><p>23 Furthermore, following [11] the ﬁnal model incorporates voted prediction methods for the perceptrons and the use of kernel functions. [sent-37, score-0.153]
</p><p>24 1  Phrase Recognition Formalization  Let x be a sentence formed by n words xi , with i ranging from 0 to n − 1, belonging to the sentence space X . [sent-40, score-0.409]
</p><p>25 For instance, in syntactic parsing K may include noun phrases, verb phrases, prepositional phrases and clauses, among others. [sent-42, score-0.693]
</p><p>26 A phrase, denoted as (s, e)k , is the sequence of consecutive words spanning from word xs to word xe , having s ≤ e, with category k ∈ K. [sent-43, score-0.46]
</p><p>27 A solution for a phrase recognition problem is a set y of phrases which is coherent with respect to some constraints. [sent-48, score-1.326]
</p><p>28 For the problem of recognizing sequentially organized phrases, often referred to as chunking, phrases are not allowed to overlap or embed. [sent-50, score-0.604]
</p><p>29 More generally, for the problem of recognizing phrases organized hierarchically, a solution is a set of phrases which do not overlap but may be embedded. [sent-52, score-1.176]
</p><p>30 In order to evaluate a phrase recognition system we use the standard measures for recognition tasks: precision (p) —the ratio of recognized phrases that are correct—, recall (r) 2pr —the ratio of correct phrases that are recognized— and their harmonic mean F1 = p+r . [sent-54, score-1.988]
</p><p>31 2  Recognizing Phrases  The mechanism to recognize phrases is described here as a function which, given a sentence x, identiﬁes the set of phrases y of x: R : X → Y. [sent-56, score-1.293]
</p><p>32 First, we assume a function P which, given a sentence x, identiﬁes a set of candidate phrases, not necessarily coherent, for the sentence, P(x) ⊆ P. [sent-58, score-0.193]
</p><p>33 Second, we assume a score function which, given a phrase, produces a real-valued prediction indicating the plausability of the phrase for the sentence. [sent-59, score-0.78]
</p><p>34 Note that the R function constructs the optimal phrase set by evaluating scores of phrase candidates, and, regarding the length of the sentence, there is a quadratic number of possible phrases, that is, the set P. [sent-62, score-1.274]
</p><p>35 Thus, considering straightforwardly all phrases in P would result in a very expensive exploration. [sent-63, score-0.55]
</p><p>36 The function P is intended to ﬁlter out phrase candidates from P by applying decisions at word level. [sent-64, score-0.916]
</p><p>37 A simple setting for this function is a start-end classiﬁcation for each phrase type: each word of the sentence is tested as k-start —if it is likely to start phrases of type k— and as k-end —if it is likely to end phrases type k. [sent-65, score-2.14]
</p><p>38 Each k-start word xs with each k-end word xe , having s ≤ e, forms the phrase candidates (s, e)k . [sent-66, score-1.136]
</p><p>39 In general, each classiﬁer will be applied to each word in the sentence, and deciding the best strategy for identifying phrase candidates will depend on the sparseness of phrases in a sentence, the length of phrases and the number of categories. [sent-68, score-2.037]
</p><p>40 Once the phrase candidates are identiﬁed, the optimal coherent phrase set is selected according to (1). [sent-69, score-1.449]
</p><p>41 Due to its nature, there is no need to explicitly enumerate each possible coherent phrase set, which would result in an exponential exploration. [sent-70, score-0.693]
</p><p>42 Instead, by guiding the exploration through the problem constraints and using dynamic programming the optimal coherent phrase set can be found in polynomial time over the sentence length. [sent-71, score-0.879]
</p><p>43 When embedding of phrases is allowed, a cubic-time bottom-up exploration is required [6]. [sent-73, score-0.569]
</p><p>44 Summarizing, the phrase recognition system is performed in two layers: the identiﬁcation layer, which ﬁlters out phrase candidates in linear time, and the scoring layer, which selects the optimal phrase chunking in quadratic or cubic time. [sent-75, score-2.206]
</p><p>45 3  Additive Online Learning via Recognition Feedback  In this section we describe an online learning strategy for training the learning components of the Phrase Recognizer, namely the start-end classiﬁers in P and the score function. [sent-76, score-0.242]
</p><p>46 The learning challenge consists in approximating the functions so as to maximize the global F1 measure on the problem, taking into account that the functions interact. [sent-77, score-0.162]
</p><p>47 The function P consists of two classiﬁers per phrase type: the start classiﬁer (hk ) S and the end classiﬁer (hk ). [sent-80, score-0.715]
</p><p>48 Thus, the P function is formed by a prediction vector for each E classiﬁer, noted as wk or wk , and a unique shared representation function φw which maps a S E word in context into a feature vector. [sent-81, score-0.567]
</p><p>49 A prediction is computed as hk (x) = wk · φw (x), and S S similarly for the hk , and the sign is taken as the binary classiﬁcation. [sent-82, score-0.402]
</p><p>50 The score function E computes a real-valued score for a phrase candidate (s, e)k . [sent-83, score-0.867]
</p><p>51 We implement this function with a prediction vector wk for each type k ∈ K, and also a shared representation function φp which maps a phrase into a feature vector. [sent-84, score-0.925]
</p><p>52 The score prediction is then given by the expression: score((s, e)k , x, y) = wk · φp ((s, e)k , x, y). [sent-85, score-0.335]
</p><p>53 We give the algorithm the name FR-Perceptron since it is a Perceptron-based learning algorithm that approximates the prediction vectors in P as Filters of words, and the score vectors as Rankers of phrases. [sent-88, score-0.205]
</p><p>54 Given a sentence, it predicts its optimal phrase solution as speciﬁed in (1) using the current vectors. [sent-90, score-0.659]
</p><p>55 As in the traditional Perceptron algorithm, if the predicted phrase set is not perfect the vectors responsible of the incorrect prediction are updated additively. [sent-91, score-0.717]
</p><p>56 , (xm , y m )}, xi are sentences, y i are solutions in Y • Deﬁne: W = {wk , wk , wk |k ∈ K}. [sent-95, score-0.405]
</p><p>57 By analyzing the dependencies between each function and a solution, we derive a feedback rule which naturally ﬁts the phrase recognition setting. [sent-106, score-0.77]
</p><p>58 Let y ∗ be the gold set of phrases for a sentence x, and y the set ˆ predicted by the R function. [sent-107, score-0.738]
</p><p>59 Let goldS(xi , k) and goldE(xi , k) be, respectively, the perfect indicator functions for start and end boundaries of phrases of type k. [sent-108, score-0.667]
</p><p>60 That is, they return 1 if word xi starts/ends some k-phrase in y ∗ and -1 otherwise. [sent-109, score-0.138]
</p><p>61 We differentiate three kinds of phrases in order to give feedback to the functions being learned:  • Phrases correctly identiﬁed: ∀(s, e)k ∈ y ∗ ∩ y : ˆ – Do nothing, since they are correct. [sent-110, score-0.65]
</p><p>62 Update misclassiﬁed boundary words: if (wk · φw (xs ) ≤ 0) then wk = wk + φw (xs ) S S S if (wk · φw (xe ) ≤ 0) then wk = wk + φw (xe ) E E E 2. [sent-112, score-0.787]
</p><p>63 Update score function, if applied: if (wk · φw (xs ) > 0 ∧ wk · φw (xe ) > 0) then wk = wk + φp ((s, e)k , x, y) S E • Over-predicted phrases: ∀(s, e)k ∈ y \y ∗ : ˆ 1. [sent-113, score-0.678]
</p><p>64 Update score function: wk = wk − φp ((s, e)k , x, y) 2. [sent-114, score-0.486]
</p><p>65 Update words misclassiﬁed as S or E: if (goldS(xs , k) = −1) then wk = wk − φw (xs ) S S if (goldE(xe , k) = −1) then wk = wk − φw (xe ) E E This feedback models the interaction between the two layers of the recognition process. [sent-115, score-0.975]
</p><p>66 The start-end layer ﬁlters out phrase candidates for the scoring layer. [sent-116, score-0.856]
</p><p>67 Thus, misclassifying the boundary words of a correct phrase blocks the generation of the candidate and produces a missed phrase. [sent-117, score-0.796]
</p><p>68 Therefore, we move the start or end prediction vectors toward the misclassiﬁed boundary words of a missed phrase. [sent-118, score-0.228]
</p><p>69 When an incorrect phrase is predicted, we move away the prediction vectors from the start or end words, provided that they are not boundary words of a phrase in the gold solution. [sent-119, score-1.507]
</p><p>70 Regarding the scoring layer, each category prediction vector is moved toward missed phrases and moved away from over-predicted phrases. [sent-121, score-0.698]
</p><p>71 It is important to note that this feedback operates only on the basis of the predicted solution y , avoiding to make updates ˆ for every prediction the function has made. [sent-122, score-0.135]
</p><p>72 Thus, the learning strategy is taking advantage of the recognition process, and concentrates on (i) assigning high scores for the correct phrases and (ii) making the incorrect competing phrases to score lower than the correct ones. [sent-123, score-1.418]
</p><p>73 As a consequence, this feedback rule tends to approximate the desired behavior of the global R function, that is, to make the summation of the scores of the correct phrase set maximal with respect to other phrase set candidates. [sent-124, score-1.456]
</p><p>74 This learning strategy is closely related to other recent works on learning ranking functions [10, 8, 9]. [sent-125, score-0.147]
</p><p>75 4  Experiments on Clause Identiﬁcation  Clause Identiﬁcation is the problem of recognizing the clauses of a sentence. [sent-127, score-0.173]
</p><p>76 A clause can be roughly deﬁned as a phrase with a subject, possibly implicit, and a predicate. [sent-128, score-0.782]
</p><p>77 Clauses in a sentence form a hierarchical structure which constitutes the skeleton of the full syntactic tree. [sent-129, score-0.211]
</p><p>78 The problem consists of recognizing the set of clauses on the basis of words, part-of-speech tags (PoS), and syntactic base phrases (or chunks). [sent-132, score-0.807]
</p><p>79 There is only one category of phrases to be considered, namely the clauses. [sent-133, score-0.576]
</p><p>80 Representation Functions We now describe the representation functions φw and φp , which respectively map a word or a phrase and their local context into a feature vector in {0, 1}n . [sent-135, score-0.809]
</p><p>81 For the function φw (xi ) we capture the form, PoS and chunk tags of words in a window around xi , that is, words xi+l with l ∈ [−Lw , +Lw ]. [sent-137, score-0.169]
</p><p>82 Each attribute type, together with each relative position l and each returned value forms a ﬁnal binary indicator feature (for instance, “the word at position -2 is that” is a binary feature). [sent-138, score-0.163]
</p><p>83 Also, we consider the word decisions of the words to the left of xi , that is, binary ﬂags indicating whether the [−Lw , −1] words in the window are starts and/or ends of a phrase. [sent-139, score-0.331]
</p><p>84 For the function φp (s, e) we represent the context of the phrase by capturing a [−Lp , 0] window of forms, PoS and chunks at the s word, and a separate [0, +Lp ] window at the e word. [sent-140, score-0.703]
</p><p>85 Furthermore, we represent the (s, e) phrase by evaluating a pattern from s to e which captures the relevant elements in the sentence fragment from word s to word e 2 . [sent-141, score-1.056]
</p><p>86 The system to train was composed by the start and end functions which identify clause candidates, and a score function for clauses. [sent-144, score-0.359]
</p><p>87 To train the score classiﬁer, we generated only the phrase candidates formed with all pairs of correct phrase boundaries. [sent-148, score-1.545]
</p><p>88 The alternative of generating all possible phrases as examples would be more realistic, but infeasible for the learning algorithm since it would produce 1,377,843 examples, with a 98. [sent-150, score-0.572]
</p><p>89 That is, the training sentences are visited online as in the FR-Perceptron: ﬁrst, the start-end functions are applied to each word, and according to their positive decisions, phrase examples are generated to train the score function. [sent-153, score-0.895]
</p><p>90 In this way, the input of the score function is dynamically adapted to the start-end behavior, but a classiﬁcation feedback is given to each function for each decision taken. [sent-154, score-0.174]
</p><p>91 4 We trained the perceptron models for up to 20 epochs via the FR-Perceptron algorithm and via classiﬁcation feedback, either online (CO-VP) or batch (CB-VP). [sent-159, score-0.159]
</p><p>92 Bottom: given the start-end ﬁlters, upper bound on the global F1 (left) and number of proposed phrase candidates (right). [sent-172, score-0.821]
</p><p>93 The FR-Perceptron model exhibits the desirable ﬁltering behavior for this local decision, which consists in maintaining a very high recall (so that no correct candidates are blocked) while increasing the precision during epochs. [sent-177, score-0.27]
</p><p>94 The left plot shows the maximum achievable global F1 , assuming a perfect scorer, given the phrases proposed by the start-end functions. [sent-181, score-0.615]
</p><p>95 Additionally, the right plot depicts the ﬁltering capabilities in terms of the number of phrase candidates produced, out of a total number of 300,511 possible phrases. [sent-182, score-0.756]
</p><p>96 The FR-Perceptron behavior in the ﬁltering layer is clear: while it maintains a high recall on identifying correct phrases (above 95%), it substantially reduces the number of phrase candidates to explore in the scoring layer, and thus, it progressively simpliﬁes the input to the score function. [sent-183, score-1.605]
</p><p>97 Far from this behavior, the classiﬁcation-based models are not sensitive to the global performance in the ﬁltering layer and, although they aggressively reduce the search space, provide only a moderate upper bound on the global F1 . [sent-184, score-0.184]
</p><p>98 5  Conclusion  We have presented a global learning strategy for the general problem of recognizing structures of phrases, in which, typically, several different learning functions interact to explore and recognize the structure. [sent-225, score-0.26]
</p><p>99 The effectiveness of our method has been empirically proved in the problem of clause identiﬁcation, where we have shown that a considerable improvement can be obtained by exploiting high-order global dependencies in learning, in contrast to concentrating only on the local subproblems. [sent-226, score-0.237]
</p><p>100 These results suggest to scale up global learning strategies to more complex problems found in the natural language area (such as full parsing or machine translation), or other structured domains. [sent-227, score-0.175]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phrase', 0.637), ('phrases', 0.55), ('wk', 0.192), ('sentence', 0.167), ('clause', 0.145), ('clauses', 0.119), ('candidates', 0.119), ('word', 0.117), ('score', 0.102), ('xe', 0.084), ('perceptrons', 0.078), ('hk', 0.073), ('feedback', 0.072), ('classi', 0.07), ('chunking', 0.069), ('global', 0.065), ('xs', 0.062), ('recognition', 0.061), ('coherent', 0.056), ('layer', 0.054), ('words', 0.054), ('recognizing', 0.054), ('online', 0.053), ('sentences', 0.05), ('parsing', 0.049), ('scoring', 0.046), ('syntactic', 0.044), ('recognized', 0.044), ('strategy', 0.043), ('identi', 0.043), ('decisions', 0.043), ('prediction', 0.041), ('perceptron', 0.04), ('carreras', 0.04), ('lw', 0.039), ('epochs', 0.038), ('missed', 0.035), ('subproblems', 0.034), ('voted', 0.034), ('ltering', 0.032), ('ranking', 0.032), ('ers', 0.032), ('verb', 0.031), ('pos', 0.031), ('recall', 0.031), ('start', 0.03), ('type', 0.03), ('end', 0.029), ('precision', 0.029), ('lters', 0.029), ('cation', 0.028), ('batch', 0.028), ('functions', 0.028), ('chunks', 0.028), ('local', 0.027), ('development', 0.027), ('recognize', 0.026), ('golde', 0.026), ('golds', 0.026), ('kudo', 0.026), ('punyakanok', 0.026), ('rquez', 0.026), ('sang', 0.026), ('tjong', 0.026), ('upc', 0.026), ('xavier', 0.026), ('candidate', 0.026), ('category', 0.026), ('correct', 0.025), ('shared', 0.025), ('train', 0.025), ('er', 0.025), ('binary', 0.023), ('solution', 0.022), ('learning', 0.022), ('xi', 0.021), ('identifying', 0.021), ('concentrates', 0.021), ('gold', 0.021), ('tags', 0.021), ('svm', 0.021), ('language', 0.021), ('behavior', 0.02), ('layers', 0.02), ('vectors', 0.02), ('consists', 0.019), ('recognizer', 0.019), ('noun', 0.019), ('tagging', 0.019), ('incorrect', 0.019), ('exploration', 0.019), ('lp', 0.019), ('window', 0.019), ('boundary', 0.019), ('tasks', 0.019), ('misclassi', 0.019), ('fragment', 0.018), ('strategies', 0.018), ('collins', 0.017), ('crammer', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="147-tfidf-1" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>2 0.085145742 <a title="147-tfidf-2" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>Author: Haifeng Li, Tao Jiang, Keshu Zhang</p><p>Abstract: A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix Sw . Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efﬁcient and stable. 1</p><p>3 0.077579632 <a title="147-tfidf-3" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>Author: Xuerui Wang, Rebecca Hutchinson, Tom M. Mitchell</p><p>Abstract: We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classiﬁers constitute “virtual sensors” of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classiﬁers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classiﬁers that can be applied across multiple human subjects, including subjects who were not involved in training the classiﬁer. We describe the design of several machine learning approaches to training multiple-subject classiﬁers, and report experimental results demonstrating the success of these methods in learning cross-subject classiﬁers for two different fMRI data sets. 1</p><p>4 0.066924244 <a title="147-tfidf-4" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>5 0.057699062 <a title="147-tfidf-5" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>Author: Stuart Andrews, Thomas Hofmann</p><p>Abstract: Learning from ambiguous training data is highly relevant in many applications. We present a new learning algorithm for classiﬁcation problems where labels are associated with sets of pattern instead of individual patterns. This encompasses multiple instance learning as a special case. Our approach is based on a generalization of linear programming boosting and uses results from disjunctive programming to generate successively stronger linear relaxations of a discrete non-convex problem. 1</p><p>6 0.054233618 <a title="147-tfidf-6" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>7 0.053534437 <a title="147-tfidf-7" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>8 0.050638679 <a title="147-tfidf-8" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>9 0.048653554 <a title="147-tfidf-9" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>10 0.047990054 <a title="147-tfidf-10" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>11 0.046890102 <a title="147-tfidf-11" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>12 0.0455507 <a title="147-tfidf-12" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>13 0.04365531 <a title="147-tfidf-13" href="./nips-2003-Ranking_on_Data_Manifolds.html">164 nips-2003-Ranking on Data Manifolds</a></p>
<p>14 0.041434586 <a title="147-tfidf-14" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>15 0.040579978 <a title="147-tfidf-15" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>16 0.040556174 <a title="147-tfidf-16" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>17 0.039403219 <a title="147-tfidf-17" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>18 0.039215457 <a title="147-tfidf-18" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>19 0.039007083 <a title="147-tfidf-19" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>20 0.037260834 <a title="147-tfidf-20" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.124), (1, -0.026), (2, -0.005), (3, -0.081), (4, -0.001), (5, -0.041), (6, -0.038), (7, -0.001), (8, 0.002), (9, 0.034), (10, 0.092), (11, -0.044), (12, -0.039), (13, -0.037), (14, 0.099), (15, -0.012), (16, -0.07), (17, -0.021), (18, 0.016), (19, -0.041), (20, 0.083), (21, -0.049), (22, -0.127), (23, -0.013), (24, -0.046), (25, 0.058), (26, -0.006), (27, -0.086), (28, -0.107), (29, -0.01), (30, 0.051), (31, 0.024), (32, 0.028), (33, 0.059), (34, -0.104), (35, -0.153), (36, -0.051), (37, 0.028), (38, 0.075), (39, -0.139), (40, 0.033), (41, -0.129), (42, -0.107), (43, 0.145), (44, -0.049), (45, -0.135), (46, -0.008), (47, -0.02), (48, 0.041), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91102743 <a title="147-lsi-1" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>2 0.69649708 <a title="147-lsi-2" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>3 0.62580943 <a title="147-lsi-3" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>4 0.46911073 <a title="147-lsi-4" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>Author: Haifeng Li, Tao Jiang, Keshu Zhang</p><p>Abstract: A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix Sw . Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efﬁcient and stable. 1</p><p>5 0.42793718 <a title="147-lsi-5" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>Author: Jun Suzuki, Yutaka Sasaki, Eisaku Maeda</p><p>Abstract: This paper devises a novel kernel function for structured natural language data. In the ﬁeld of Natural Language Processing, feature extraction consists of the following two steps: (1) syntactically and semantically analyzing raw data, i.e., character strings, then representing the results as discrete structures, such as parse trees and dependency graphs with part-of-speech tags; (2) creating (possibly high-dimensional) numerical feature vectors from the discrete structures. The new kernels, called Hierarchical Directed Acyclic Graph (HDAG) kernels, directly accept DAGs whose nodes can contain DAGs. HDAG data structures are needed to fully reﬂect the syntactic and semantic structures that natural language data inherently have. In this paper, we deﬁne the kernel function and show how it permits efﬁcient calculation. Experiments demonstrate that the proposed kernels are superior to existing kernel functions, e.g., sequence kernels, tree kernels, and bag-of-words kernels. 1</p><p>6 0.42616874 <a title="147-lsi-6" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>7 0.35794893 <a title="147-lsi-7" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>8 0.35471141 <a title="147-lsi-8" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>9 0.34903604 <a title="147-lsi-9" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>10 0.34186253 <a title="147-lsi-10" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>11 0.33090541 <a title="147-lsi-11" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>12 0.30123895 <a title="147-lsi-12" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>13 0.30016804 <a title="147-lsi-13" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>14 0.28612339 <a title="147-lsi-14" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>15 0.28326377 <a title="147-lsi-15" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>16 0.26284188 <a title="147-lsi-16" href="./nips-2003-Phonetic_Speaker_Recognition_with_Support_Vector_Machines.html">156 nips-2003-Phonetic Speaker Recognition with Support Vector Machines</a></p>
<p>17 0.25987968 <a title="147-lsi-17" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>18 0.25954416 <a title="147-lsi-18" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>19 0.25806072 <a title="147-lsi-19" href="./nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</a></p>
<p>20 0.25538325 <a title="147-lsi-20" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.062), (11, 0.025), (26, 0.024), (30, 0.019), (33, 0.015), (35, 0.033), (49, 0.01), (53, 0.098), (71, 0.092), (76, 0.057), (85, 0.071), (88, 0.25), (91, 0.109), (99, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79893422 <a title="147-lda-1" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>2 0.7847507 <a title="147-lda-2" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>Author: Michael J. Quinlan, Stephan K. Chalup, Richard H. Middleton</p><p>Abstract: This article addresses the issues of colour classiﬁcation and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classiﬁcation with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse ﬁtting for colour classiﬁcation and the statistical approach used for collision detection.</p><p>3 0.77385902 <a title="147-lda-3" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>4 0.6193707 <a title="147-lda-4" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>Author: Sanjiv Kumar, Martial Hebert</p><p>Abstract: In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classiﬁcation of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classiﬁcation problems using the graph min-cut algorithms. The performance of the model was veriﬁed on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments. 1</p><p>5 0.61114377 <a title="147-lda-5" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>6 0.61001313 <a title="147-lda-6" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>7 0.60955238 <a title="147-lda-7" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>8 0.60877401 <a title="147-lda-8" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>9 0.60841519 <a title="147-lda-9" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>10 0.60675943 <a title="147-lda-10" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>11 0.606736 <a title="147-lda-11" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>12 0.6053099 <a title="147-lda-12" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>13 0.60480845 <a title="147-lda-13" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>14 0.60441673 <a title="147-lda-14" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>15 0.60241514 <a title="147-lda-15" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>16 0.60118079 <a title="147-lda-16" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>17 0.60113961 <a title="147-lda-17" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>18 0.60083073 <a title="147-lda-18" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>19 0.59933335 <a title="147-lda-19" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>20 0.59799737 <a title="147-lda-20" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
