<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-147" href="#">nips2003-147</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</h1>
<br/><p>Source: <a title="nips-2003-147-pdf" href="http://papers.nips.cc/paper/2528-online-learning-via-global-feedback-for-phrase-recognition.pdf">pdf</a></p><p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>Reference: <a title="nips-2003-147-reference" href="../nips2003_reference/nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phrase', 0.637), ('phrases', 0.549), ('claus', 0.264), ('wk', 0.192), ('sent', 0.163), ('scor', 0.133), ('word', 0.106), ('candid', 0.1), ('recogn', 0.094), ('chunk', 0.087), ('perceptron', 0.085), ('xe', 0.083), ('hk', 0.073), ('feedback', 0.072), ('glob', 0.065), ('xs', 0.062), ('recognit', 0.061), ('lay', 0.061), ('gold', 0.055), ('onlin', 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="147-tfidf-1" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>2 0.089806318 <a title="147-tfidf-2" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>3 0.08920823 <a title="147-tfidf-3" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>4 0.085966758 <a title="147-tfidf-4" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>Author: Haifeng Li, Tao Jiang, Keshu Zhang</p><p>Abstract: A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix Sw . Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efﬁcient and stable. 1</p><p>5 0.058146421 <a title="147-tfidf-5" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>Author: Xuerui Wang, Rebecca Hutchinson, Tom M. Mitchell</p><p>Abstract: We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classiﬁers constitute “virtual sensors” of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classiﬁers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classiﬁers that can be applied across multiple human subjects, including subjects who were not involved in training the classiﬁer. We describe the design of several machine learning approaches to training multiple-subject classiﬁers, and report experimental results demonstrating the success of these methods in learning cross-subject classiﬁers for two different fMRI data sets. 1</p><p>6 0.055886216 <a title="147-tfidf-6" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>7 0.055884819 <a title="147-tfidf-7" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>8 0.055478334 <a title="147-tfidf-8" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>9 0.049708162 <a title="147-tfidf-9" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>10 0.048280641 <a title="147-tfidf-10" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>11 0.04712794 <a title="147-tfidf-11" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>12 0.046619408 <a title="147-tfidf-12" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>13 0.045016907 <a title="147-tfidf-13" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>14 0.044760659 <a title="147-tfidf-14" href="./nips-2003-Ranking_on_Data_Manifolds.html">164 nips-2003-Ranking on Data Manifolds</a></p>
<p>15 0.043289278 <a title="147-tfidf-15" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>16 0.042211112 <a title="147-tfidf-16" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>17 0.040642031 <a title="147-tfidf-17" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>18 0.040196463 <a title="147-tfidf-18" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>19 0.036821306 <a title="147-tfidf-19" href="./nips-2003-Phonetic_Speaker_Recognition_with_Support_Vector_Machines.html">156 nips-2003-Phonetic Speaker Recognition with Support Vector Machines</a></p>
<p>20 0.036704894 <a title="147-tfidf-20" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.118), (1, 0.007), (2, -0.011), (3, 0.049), (4, 0.013), (5, -0.004), (6, 0.024), (7, -0.012), (8, -0.019), (9, 0.023), (10, 0.082), (11, -0.009), (12, 0.085), (13, -0.088), (14, -0.045), (15, 0.059), (16, 0.081), (17, 0.078), (18, -0.014), (19, -0.049), (20, -0.002), (21, -0.011), (22, -0.052), (23, -0.062), (24, -0.089), (25, -0.02), (26, -0.071), (27, 0.033), (28, 0.072), (29, 0.074), (30, 0.107), (31, -0.065), (32, -0.021), (33, 0.198), (34, 0.059), (35, -0.062), (36, 0.095), (37, 0.004), (38, 0.107), (39, -0.029), (40, 0.042), (41, -0.121), (42, -0.04), (43, -0.014), (44, 0.088), (45, 0.055), (46, 0.007), (47, 0.061), (48, 0.121), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8926034 <a title="147-lsi-1" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>2 0.73056364 <a title="147-lsi-2" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>3 0.72140759 <a title="147-lsi-3" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>4 0.57552838 <a title="147-lsi-4" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>Author: Xuerui Wang, Rebecca Hutchinson, Tom M. Mitchell</p><p>Abstract: We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classiﬁers constitute “virtual sensors” of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classiﬁers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classiﬁers that can be applied across multiple human subjects, including subjects who were not involved in training the classiﬁer. We describe the design of several machine learning approaches to training multiple-subject classiﬁers, and report experimental results demonstrating the success of these methods in learning cross-subject classiﬁers for two different fMRI data sets. 1</p><p>5 0.39336622 <a title="147-lsi-5" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>Author: Jun Suzuki, Yutaka Sasaki, Eisaku Maeda</p><p>Abstract: This paper devises a novel kernel function for structured natural language data. In the ﬁeld of Natural Language Processing, feature extraction consists of the following two steps: (1) syntactically and semantically analyzing raw data, i.e., character strings, then representing the results as discrete structures, such as parse trees and dependency graphs with part-of-speech tags; (2) creating (possibly high-dimensional) numerical feature vectors from the discrete structures. The new kernels, called Hierarchical Directed Acyclic Graph (HDAG) kernels, directly accept DAGs whose nodes can contain DAGs. HDAG data structures are needed to fully reﬂect the syntactic and semantic structures that natural language data inherently have. In this paper, we deﬁne the kernel function and show how it permits efﬁcient calculation. Experiments demonstrate that the proposed kernels are superior to existing kernel functions, e.g., sequence kernels, tree kernels, and bag-of-words kernels. 1</p><p>6 0.39154363 <a title="147-lsi-6" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>7 0.35704744 <a title="147-lsi-7" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>8 0.32677287 <a title="147-lsi-8" href="./nips-2003-Phonetic_Speaker_Recognition_with_Support_Vector_Machines.html">156 nips-2003-Phonetic Speaker Recognition with Support Vector Machines</a></p>
<p>9 0.30592021 <a title="147-lsi-9" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>10 0.30574197 <a title="147-lsi-10" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>11 0.29839453 <a title="147-lsi-11" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>12 0.29400751 <a title="147-lsi-12" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>13 0.29222813 <a title="147-lsi-13" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>14 0.28278878 <a title="147-lsi-14" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>15 0.27889878 <a title="147-lsi-15" href="./nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</a></p>
<p>16 0.27863166 <a title="147-lsi-16" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>17 0.27717087 <a title="147-lsi-17" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>18 0.25747675 <a title="147-lsi-18" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>19 0.25551748 <a title="147-lsi-19" href="./nips-2003-Reasoning_about_Time_and_Knowledge_in_Neural_Symbolic_Learning_Systems.html">165 nips-2003-Reasoning about Time and Knowledge in Neural Symbolic Learning Systems</a></p>
<p>20 0.25468165 <a title="147-lsi-20" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.063), (14, 0.013), (26, 0.024), (31, 0.483), (38, 0.041), (53, 0.036), (56, 0.022), (58, 0.052), (62, 0.051), (76, 0.055), (85, 0.013), (95, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95679474 <a title="147-lda-1" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><p>same-paper 2 0.86277235 <a title="147-lda-2" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>3 0.81209153 <a title="147-lda-3" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>Author: Ben Taskar, Carlos Guestrin, Daphne Koller</p><p>Abstract: In typical classiﬁcation tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of conﬁdence of the classiﬁer, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efﬁciently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efﬁcient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classiﬁcation demonstrate very signiﬁcant gains over previous approaches. 1</p><p>4 0.75490642 <a title="147-lda-4" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>5 0.65256613 <a title="147-lda-5" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>Author: Charles Kemp, Thomas L. Griffiths, Sean Stromsten, Joshua B. Tenenbaum</p><p>Abstract: We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efﬁcient computation of the optimal Bayesian classiﬁcation function from the labeled examples. We test our approach on eight real-world datasets. 1</p><p>6 0.58248717 <a title="147-lda-6" href="./nips-2003-Link_Prediction_in_Relational_Data.html">118 nips-2003-Link Prediction in Relational Data</a></p>
<p>7 0.56088459 <a title="147-lda-7" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>8 0.55748671 <a title="147-lda-8" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>9 0.54214156 <a title="147-lda-9" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>10 0.53781551 <a title="147-lda-10" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>11 0.53430843 <a title="147-lda-11" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>12 0.52793056 <a title="147-lda-12" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>13 0.52643013 <a title="147-lda-13" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>14 0.51939601 <a title="147-lda-14" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>15 0.50852811 <a title="147-lda-15" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>16 0.50665182 <a title="147-lda-16" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>17 0.50663686 <a title="147-lda-17" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>18 0.50480473 <a title="147-lda-18" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>19 0.50395787 <a title="147-lda-19" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>20 0.50271255 <a title="147-lda-20" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
