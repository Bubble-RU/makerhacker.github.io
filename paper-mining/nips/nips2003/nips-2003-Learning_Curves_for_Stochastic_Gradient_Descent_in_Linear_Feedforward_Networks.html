<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-104" href="#">nips2003-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</h1>
<br/><p>Source: <a title="nips-2003-104-pdf" href="http://papers.nips.cc/paper/2511-learning-curves-for-stochastic-gradient-descent-in-linear-feedforward-networks.pdf">pdf</a></p><p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>Reference: <a title="nips-2003-104-reference" href="../nips2003_reference/nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning curves for stochastic gradient descent in linear feedforward networks Justin Werfel Dept. [sent-1, score-1.017]
</p><p>2 edu  Abstract Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. [sent-10, score-0.34]
</p><p>3 We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. [sent-11, score-1.184]
</p><p>4 The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. [sent-12, score-0.901]
</p><p>5 These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. [sent-13, score-0.179]
</p><p>6 1  Introduction  Learning in artiﬁcial systems can be formulated as optimization of an objective function which quantiﬁes the system’s performance. [sent-14, score-0.046]
</p><p>7 A typical approach to this optimization is to follow the gradient of the objective function with respect to the tunable parameters of the system. [sent-15, score-0.375]
</p><p>8 Frequently this is accomplished directly, by calculating the gradient explicitly and updating the parameters by a small step in the direction of locally greatest improvement. [sent-16, score-0.323]
</p><p>9 In many circumstances, however, attempts at direct gradient-following can encounter problems. [sent-17, score-0.146]
</p><p>10 In VLSI and other hardware implementations, computation of the gradient may be excessively unwieldy, if not impossible due to unavoidable imperfections in manufacturing [1]-[5]. [sent-18, score-0.286]
</p><p>11 In some cases, as with many where the reinforcement learning framework is used, there may be no explicit form for the objective function and hence no way of calculating its gradient [6]. [sent-19, score-0.477]
</p><p>12 And in biological systems, any argument that direct gradient calculation might be what the system is actually doing typically encounters severe obstacles. [sent-20, score-0.378]
</p><p>13 For reasons such as these, there has been broad interest in stochastic methods which approximate the gradient on average. [sent-22, score-0.465]
</p><p>14 Compared to a method that follows the true gradient directly, we would intuitively expect a stochastic gradient-following approach to learn more slowly. [sent-23, score-0.465]
</p><p>15 Moreover, if the network is made larger and the number of parameters thereby increased, this credit assignment problem becomes still more difﬁcult; thus we expect the performance of stochastic gradient methods to scale up with network size more poorly than deterministic methods. [sent-25, score-0.791]
</p><p>16 However, under some circumstances stochastic methods can be equally as effective as direct ones in training even large networks, generating near-identical learning curves (see, e. [sent-26, score-0.611]
</p><p>17 Under what circumstances, then, will stochastic gradient descent have performance comparable to that of the deterministic variety? [sent-30, score-0.606]
</p><p>18 In this paper, we investigate these issues quantitatively by calculating the learning curves for a linear perceptron using a direct gradient method and two stochastic methods, node perturbation and weight perturbation. [sent-32, score-1.712]
</p><p>19 Additionally, when learning rates are chosen to be very low, and such that the weight updates prescribed by each method are equal on average, we ﬁnd that all three methods give identical learning curves. [sent-34, score-0.58]
</p><p>20 2  Perceptron comparison  Direct and stochastic gradient approaches are general classes of training methods. [sent-35, score-0.498]
</p><p>21 We study the operation of exemplars of both on a feedforward linear perceptron, which has the advantage over the nonlinear case that the learning curves can be calculated exactly [8]. [sent-36, score-0.411]
</p><p>22 We have N input units and M output units, connected by a weight matrix w of M N elements; outputs in response to an input x are given by y = wx. [sent-37, score-0.516]
</p><p>23 For the ensemble of possible inputs, we want to train the network to produce desired corresponding outputs y = d; in order to ensure that this task is realizable by the network, we assume the existence of a teacher network w ∗ such that d = w ∗ x. [sent-38, score-0.413]
</p><p>24 We use the squared error function 1 1 1 E = |y − d|2 = |(w − w∗ )x|2 = |W x|2 (1) 2 2 2 where we have deﬁned the matrix W ≡ w − w ∗ . [sent-39, score-0.178]
</p><p>25 We train the network with an online approach, choosing at each time step an input vector x with components drawn from a Gaussian distribution with mean 0 and unit variance, and using it to construct a weight update according to one of the three prescriptions below. [sent-40, score-0.71]
</p><p>26 The online gradient-following approach explicitly uses the gradient of the error function for a given input to determine the weight update: ∆WOL = −η E  where η > 0 is the learning rate. [sent-41, score-0.862]
</p><p>27 In the stochastic algorithms, the gradient is not calculated directly; instead, some noise is introduced into the system, affecting its error for a given input, and the difference between the error with and without noise is used to estimate the gradient. [sent-45, score-1.049]
</p><p>28 The simplest case is when noise is added directly to the weight matrix: 1 EWP = |(W + ψ)x|2 2  Such an approach is sometimes termed ‘weight perturbation’ [2, 4]. [sent-46, score-0.391]
</p><p>29 We choose each element of the noise matrix ψ from a Gaussian distribution with mean 0 and variance σ 2 . [sent-47, score-0.163]
</p><p>30 Intuitively, if the addition of the noise lowers the error, that perturbation to the weight matrix is retained, which will mean lower error for that input in future. [sent-48, score-1.013]
</p><p>31 Conversely, if the noise leads to an increase in error, the opposite change is made to the weights; the effect of small noise on error can be approximated as linear, and the opposite change in weights will lead to the opposite change in error, again decreasing error for that input in future. [sent-49, score-0.81]
</p><p>32 Here if the noise leads to a decrease in error, the weights are adjusted in such a way as to move the outputs in the direction of that noise. [sent-51, score-0.236]
</p><p>33 The degree of freedom for each output unit corresponds to the adjustment of its threshold, making the unit more or less responsive to a given pattern of input activity. [sent-52, score-0.22]
</p><p>34 1  Online gradient method  Taking the gradient of the error function of Eq. [sent-57, score-0.701]
</p><p>35 1 gives ∆WOL = −ηW xxT  (2)  as the individual weight update for particular values of W and x. [sent-58, score-0.324]
</p><p>36 We therefore use this averaged error E (t) as the learning curve measuring the performance of the system. [sent-61, score-0.287]
</p><p>37 3 as (1)2  Wij  (0)2  =  ij  ij  Wij (1 − 2η + 3η 2 ) + η 2 (N − 1)  (0)  Wij ij  where the ﬁrst term is due entirely to the gradient signal and the second to the gradient noise; choosing η 1/N allows the signal to be revealed via averaging over N samples (see also the Discussion). [sent-64, score-1.084]
</p><p>38 This gradient noise is common to all three algorithms considered here. [sent-65, score-0.516]
</p><p>39 2  Node perturbation  Here averages are taken at each step not only over the inputs x but also over the noise ξ. [sent-67, score-0.672]
</p><p>40 The latter is a result of the noise ξ; when W is far from the minimum of the objective function, ξ will typically be small in comparison to W x and the additive term will be negligible, but close to the minimum the noise will prevent the system from attaining arbitrarily low error. [sent-69, score-0.407]
</p><p>41 The limit on η is stricter by a factor of M , the dimensionality of the noise, as discussed below. [sent-71, score-0.155]
</p><p>42 The limit on η is a further factor of N smaller, corresponding to the greater dimensionality of ψ compared to ξ. [sent-74, score-0.155]
</p><p>43 The magnitude of a, which depends on the parameter η but not on σ, determines whether the average error will converge and the rate at which it will do so. [sent-76, score-0.21]
</p><p>44 For the online gradient method, b = 0; a network trained this way, if it converges, will approach zero error as t → ∞. [sent-77, score-0.672]
</p><p>45 The stochastic algorithms have positive residual noise b, which depends on both η and σ; in the limit σ → 0, this residual error vanishes. [sent-78, score-0.796]
</p><p>46 Of course, σ cannot be set directly to 0 or the stochastic algorithms will cease to function. [sent-79, score-0.211]
</p><p>47 1  Maximal learning rates  The analysis of the previous section suggests at least two reasonable ways to compare these different algorithms with respect to performance. [sent-81, score-0.148]
</p><p>48 One is to choose the optimal learning rate for each, that value of η for which the average error converges most quickly. [sent-82, score-0.325]
</p><p>49 The noise takes different forms in the two stochastic variants. [sent-84, score-0.342]
</p><p>50 For node perturbation, ξ i is added directly to the ith output unit; for weight perturbation, the quantity added to the same output unit is ij ψij xj . [sent-85, score-0.627]
</p><p>51 By the central limit theorem, the latter approaches a Gaussian with mean 0 and variance N σ 2 for large N . [sent-86, score-0.047]
</p><p>52 For most direct comparison of the two stochas√ tic variants, therefore, σ for ξ should be chosen a factor N larger than for ψ. [sent-87, score-0.129]
</p><p>53 With this choice, the residual error for the two stochastic variants becomes identical, and the learning curves differ only in their rates of convergence. [sent-88, score-0.756]
</p><p>54 2  Equal average updates  A second way to compare the algorithms is to choose learning rates such that all three have the same average weight update. [sent-90, score-0.546]
</p><p>55 As noted above, choosing the same value of η in all three cases will ensure this condition. [sent-91, score-0.035]
</p><p>56 However, for equal small η, the average error for all three algorithms converges at the same rate. [sent-93, score-0.308]
</p><p>57 Weight perturbation approaches a larger value of residual error than does node perturbation; however, in the σ → 0 limit, the residual error vanishes for both. [sent-94, score-1.078]
</p><p>58 The difference in the rate of convergence is the dimensionality of the noise. [sent-96, score-0.161]
</p><p>59 Weight perturbation operates by explicit exploration of the entire M N -dimensional weight space; only one component of a particular update will be in the direction of the true gradient for a given input, while the other components can be viewed as noise masking that signal. [sent-97, score-1.219]
</p><p>60 That is, an update can be written as ∆W = ∆W (the ‘learning signal’, the actual gradient) + (∆W − ∆W ) (the ‘learning noise’), where the average is √ taken over all values of ψ. [sent-98, score-0.131]
</p><p>61 This learning noise will typically have magnitude M N larger than the learning signal, and so M N samples are required in order to average it away. [sent-99, score-0.334]
</p><p>62 Direct gradient descent gives weight updates that are purely signal in this sense; while still occurring in an M N -dimensional space, they are by deﬁnition exactly in the direction of the gradient for a given input. [sent-100, score-1.066]
</p><p>63 Thus no exploration of the weight space nor averaging over multiple samples is necessary, and the maximum learning speed is correspondingly greater. [sent-101, score-0.474]
</p><p>64 Node perturbation is a stochastic algorithm like weight perturbation, but it explores the M dimensional output space rather than the larger weight space; the learning noise is of lower dimension, and correspondingly fewer samples need to be averaged to reveal a learning signal of a given size. [sent-102, score-1.633]
</p><p>65 It has previously been argued that the maximum learning rate should scale, not with the dimensionality of the update as shown here, but with the square root of that dimensionality [4]. [sent-103, score-0.387]
</p><p>66 That claim is based on the fact that the squared magnitude of the update goes as the number of dimensions, and for a given error landscape and position in parameter space, there will be a maximum update size, greater than which instability will result. [sent-104, score-0.37]
</p><p>67 However, a more quantitative approach is to examine the conditions under which error will decrease, as we have done above. [sent-105, score-0.169]
</p><p>68 Rather than stopping with the statement that the size of the weight update scales as the square root of the number of dimensions, we have shown that this fact implies that the restriction on convergence scales with the ﬁrst power of the dimensionality. [sent-106, score-0.497]
</p><p>69 Numerical simulations of error curves, averaged over many individual trials with online updating, support these conclusions with respect to both the quantitative shapes of the learning curves and the scaling behavior of the conditions on convergence (Fig. [sent-107, score-0.649]
</p><p>70 In each case, a network of linear units with N = 20, M = 25, σ = 10−3 , and optimal η was trained on successive input examples for the number of iterations shown. [sent-110, score-0.377]
</p><p>71 100 such runs were averaged together in each case; the three gray lines show the mean (solid) and standard deviation (dashed) of squared error among those runs. [sent-111, score-0.319]
</p><p>72 This scaling result means that, for these stochastic methods, there is no net advantage in speed of training when all degrees of freedom are varied at the same time, compared to when they are varied sequentially, in terms of scaling with M and N . [sent-112, score-0.471]
</p><p>73 ) The analysis here describes the behavior in a worst case of sorts, where the objective function and distribution of inputs are isotropic. [sent-116, score-0.109]
</p><p>74 In the anisotropic case, where the problem is effectively lower-dimensional, the scaling behavior of all three methods can be correspondingly more favorable than that derived here, and the relative performance of the stochastic methods can be better. [sent-117, score-0.344]
</p><p>75 The results described in this paper extend at least qualitatively to more complicated networks and architectures. [sent-118, score-0.068]
</p><p>76 2 shows learning curves that result from applying the three algorithms to a two-layer feedforward network of nonlinear units. [sent-120, score-0.6]
</p><p>77 In the above, we have shown that stochastic gradient descent techniques can be expected to scale with increasing network size more poorly than direct ones, in terms of maximum learning rate. [sent-122, score-0.921]
</p><p>78 This may serve as a caution regarding the size of networks they may usefully be applied to. [sent-123, score-0.068]
</p><p>79 However, with learning rates in the regime where error converges, equal learning curves in each of the three will follow from equal learning rates, although individual weight updates will typically be considerably different. [sent-124, score-0.939]
</p><p>80 This is because for correspondingly small adjustments to the weights, only the component parallel to the gradient will have a signiﬁcant effect on error; orthogonal components will not affect the error to ﬁrst order. [sent-125, score-0.502]
</p><p>81 Moreover, node perturbation can have performance comparable to that of direct gradient descent even in training very large networks, so long as the number of output units is small [6]. [sent-126, score-1.288]
</p><p>82 Thus these stochastic methods may be of considerable utility for training networks in some situations, particularly in reinforcement learning frameworks and those where the gradient of the objective function is difﬁcult or impossible to calculate, for mathematical or practical reasons. [sent-127, score-0.758]
</p><p>83 The input, hidden, and output layers each had 10 units, whose output was equal to the hyperbolic tangent of their weighted input. [sent-129, score-0.14]
</p><p>84 Inputs and noises were drawn from the same distributions as in the linear case; σ = 10−3 , η had the value shown for all three algorithms in each panel. [sent-130, score-0.067]
</p><p>85 In each case, the network was trained on successive input examples for the number of iterations shown; curves show single representative runs. [sent-131, score-0.44]
</p><p>86 Error was evaluated based on the total squared difference between the output of the network and that of a teacher network with randomly chosen weights; the test error shown was the mean of that for 100 random inputs not used in training. [sent-132, score-0.586]
</p><p>87 Weight perturbation: an optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayered networks. [sent-147, score-0.275]
</p><p>88 Summed weight neuron perturbation: an O(n) improvement over weight perturbation. [sent-151, score-0.456]
</p><p>89 A fast stochastic error-descent algorithm for supervised learning and optimization. [sent-154, score-0.247]
</p><p>90 An analog VLSI recurrent neural network learning a continuous-time trajectory. [sent-157, score-0.224]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('perturbation', 0.446), ('gradient', 0.286), ('weight', 0.228), ('stochastic', 0.179), ('feedforward', 0.173), ('curves', 0.17), ('wij', 0.166), ('noise', 0.163), ('enp', 0.156), ('ewp', 0.156), ('descent', 0.141), ('error', 0.129), ('node', 0.128), ('residual', 0.123), ('network', 0.122), ('ij', 0.118), ('units', 0.107), ('online', 0.104), ('update', 0.096), ('eol', 0.094), ('wnp', 0.094), ('wwp', 0.094), ('direct', 0.092), ('seung', 0.092), ('recursion', 0.091), ('correspondingly', 0.087), ('perceptron', 0.078), ('cauwenberghs', 0.074), ('dimensionality', 0.071), ('circumstances', 0.069), ('learning', 0.068), ('networks', 0.068), ('injected', 0.065), ('updates', 0.065), ('inputs', 0.063), ('fiete', 0.062), ('flower', 0.062), ('jabri', 0.062), ('wol', 0.062), ('signal', 0.06), ('vlsi', 0.057), ('ensemble', 0.056), ('relation', 0.055), ('output', 0.055), ('gray', 0.055), ('encounter', 0.054), ('inversely', 0.053), ('speed', 0.053), ('averaged', 0.051), ('credit', 0.049), ('kaufman', 0.049), ('squared', 0.049), ('rates', 0.048), ('limit', 0.047), ('input', 0.047), ('converges', 0.047), ('scales', 0.047), ('teacher', 0.046), ('objective', 0.046), ('rate', 0.046), ('opposite', 0.046), ('varied', 0.044), ('convergence', 0.044), ('tunable', 0.043), ('mateo', 0.043), ('unit', 0.043), ('scaling', 0.043), ('multiplicative', 0.042), ('weights', 0.041), ('reinforcement', 0.04), ('quantitative', 0.04), ('brackets', 0.039), ('curve', 0.039), ('examples', 0.039), ('variants', 0.039), ('identical', 0.038), ('averaging', 0.038), ('reward', 0.038), ('frameworks', 0.038), ('calculating', 0.037), ('factor', 0.037), ('princeton', 0.035), ('root', 0.035), ('train', 0.035), ('additive', 0.035), ('three', 0.035), ('average', 0.035), ('faster', 0.035), ('xt', 0.035), ('recurrent', 0.034), ('training', 0.033), ('publishers', 0.033), ('poorly', 0.033), ('outputs', 0.032), ('freedom', 0.032), ('algorithms', 0.032), ('trained', 0.031), ('successive', 0.031), ('increased', 0.031), ('equal', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="104-tfidf-1" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>2 0.14865099 <a title="104-tfidf-2" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>Author: Léon Bottou, Yann L. Cun</p><p>Abstract: We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented. 1</p><p>3 0.12229078 <a title="104-tfidf-3" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>Author: Yoichi Miyawaki, Masato Okada</p><p>Abstract: This paper proposes neural mechanisms of transcranial magnetic stimulation (TMS). TMS can stimulate the brain non-invasively through a brief magnetic pulse delivered by a coil placed on the scalp, interfering with speciﬁc cortical functions with a high temporal resolution. Due to these advantages, TMS has been a popular experimental tool in various neuroscience ﬁelds. However, the neural mechanisms underlying TMSinduced interference are still unknown; a theoretical basis for TMS has not been developed. This paper provides computational evidence that inhibitory interactions in a neural population, not an isolated single neuron, play a critical role in yielding the neural interference induced by TMS.</p><p>4 0.11496966 <a title="104-tfidf-4" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>Author: Ciamac C. Moallemi, Benjamin V. Roy</p><p>Abstract: We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overﬂow in a sensor network. Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective. 1</p><p>5 0.095753558 <a title="104-tfidf-5" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>6 0.087950639 <a title="104-tfidf-6" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>7 0.083708592 <a title="104-tfidf-7" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>8 0.081066161 <a title="104-tfidf-8" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>9 0.08027304 <a title="104-tfidf-9" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>10 0.079345725 <a title="104-tfidf-10" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>11 0.076409571 <a title="104-tfidf-11" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>12 0.075758323 <a title="104-tfidf-12" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>13 0.075230703 <a title="104-tfidf-13" href="./nips-2003-Minimax_Embeddings.html">128 nips-2003-Minimax Embeddings</a></p>
<p>14 0.07253772 <a title="104-tfidf-14" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>15 0.071794875 <a title="104-tfidf-15" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>16 0.071641341 <a title="104-tfidf-16" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>17 0.070537828 <a title="104-tfidf-17" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>18 0.069531038 <a title="104-tfidf-18" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>19 0.069082543 <a title="104-tfidf-19" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>20 0.065428153 <a title="104-tfidf-20" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.247), (1, 0.048), (2, 0.069), (3, 0.003), (4, 0.065), (5, 0.02), (6, 0.014), (7, 0.048), (8, 0.018), (9, -0.017), (10, -0.005), (11, 0.032), (12, 0.091), (13, -0.052), (14, -0.079), (15, -0.093), (16, -0.045), (17, -0.032), (18, 0.151), (19, 0.104), (20, 0.061), (21, -0.003), (22, -0.083), (23, 0.161), (24, -0.213), (25, 0.102), (26, 0.022), (27, 0.014), (28, -0.153), (29, -0.014), (30, -0.174), (31, 0.124), (32, -0.015), (33, 0.088), (34, -0.013), (35, 0.043), (36, -0.016), (37, 0.105), (38, 0.004), (39, 0.194), (40, -0.032), (41, 0.024), (42, -0.146), (43, -0.046), (44, -0.007), (45, 0.095), (46, -0.107), (47, 0.03), (48, -0.031), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98299628 <a title="104-lsi-1" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>2 0.76635236 <a title="104-lsi-2" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>Author: Bob Ricks, Dan Ventura</p><p>Abstract: Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. 1</p><p>3 0.67481935 <a title="104-lsi-3" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>Author: Léon Bottou, Yann L. Cun</p><p>Abstract: We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented. 1</p><p>4 0.60914171 <a title="104-lsi-4" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>Author: Ciamac C. Moallemi, Benjamin V. Roy</p><p>Abstract: We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overﬂow in a sensor network. Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective. 1</p><p>5 0.55352861 <a title="104-lsi-5" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>6 0.53703249 <a title="104-lsi-6" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>7 0.4759275 <a title="104-lsi-7" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>8 0.45399219 <a title="104-lsi-8" href="./nips-2003-Circuit_Optimization_Predicts_Dynamic_Networks_for_Chemosensory_Orientation_in_Nematode_C._elegans.html">45 nips-2003-Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans</a></p>
<p>9 0.43789619 <a title="104-lsi-9" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>10 0.43181646 <a title="104-lsi-10" href="./nips-2003-Reasoning_about_Time_and_Knowledge_in_Neural_Symbolic_Learning_Systems.html">165 nips-2003-Reasoning about Time and Knowledge in Neural Symbolic Learning Systems</a></p>
<p>11 0.39662117 <a title="104-lsi-11" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>12 0.38927421 <a title="104-lsi-12" href="./nips-2003-Salient_Boundary_Detection_using_Ratio_Contour.html">168 nips-2003-Salient Boundary Detection using Ratio Contour</a></p>
<p>13 0.38752666 <a title="104-lsi-13" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>14 0.38100618 <a title="104-lsi-14" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>15 0.37782007 <a title="104-lsi-15" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>16 0.37141714 <a title="104-lsi-16" href="./nips-2003-Dopamine_Modulation_in_a_Basal_Ganglio-Cortical_Network_of_Working_Memory.html">56 nips-2003-Dopamine Modulation in a Basal Ganglio-Cortical Network of Working Memory</a></p>
<p>17 0.36492664 <a title="104-lsi-17" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>18 0.36420846 <a title="104-lsi-18" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>19 0.36062032 <a title="104-lsi-19" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>20 0.36001554 <a title="104-lsi-20" href="./nips-2003-Can_We_Learn_to_Beat_the_Best_Stock.html">44 nips-2003-Can We Learn to Beat the Best Stock</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.055), (11, 0.03), (29, 0.02), (30, 0.017), (35, 0.058), (53, 0.128), (71, 0.096), (76, 0.033), (85, 0.09), (91, 0.129), (94, 0.243), (99, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93798631 <a title="104-lda-1" href="./nips-2003-Algorithms_for_Interdependent_Security_Games.html">19 nips-2003-Algorithms for Interdependent Security Games</a></p>
<p>Author: Michael Kearns, Luis E. Ortiz</p><p>Abstract: unkown-abstract</p><p>2 0.91503298 <a title="104-lda-2" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>same-paper 3 0.85950261 <a title="104-lda-3" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>4 0.68480664 <a title="104-lda-4" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>5 0.68447435 <a title="104-lda-5" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>6 0.68380237 <a title="104-lda-6" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>7 0.68331498 <a title="104-lda-7" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>8 0.68329161 <a title="104-lda-8" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>9 0.68217266 <a title="104-lda-9" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>10 0.6807164 <a title="104-lda-10" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>11 0.67939293 <a title="104-lda-11" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>12 0.6793775 <a title="104-lda-12" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>13 0.67874849 <a title="104-lda-13" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>14 0.6770255 <a title="104-lda-14" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>15 0.67655957 <a title="104-lda-15" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>16 0.67462921 <a title="104-lda-16" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>17 0.67401117 <a title="104-lda-17" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>18 0.67308217 <a title="104-lda-18" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>19 0.67300189 <a title="104-lda-19" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>20 0.67285717 <a title="104-lda-20" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
