<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2003-PAC-Bayesian Generic Chaining</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-151" href="#">nips2003-151</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>151 nips-2003-PAC-Bayesian Generic Chaining</h1>
<br/><p>Source: <a title="nips-2003-151-pdf" href="http://papers.nips.cc/paper/2387-pac-bayesian-generic-chaining.pdf">pdf</a></p><p>Author: Jean-yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds for classiﬁcation. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classiﬁers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classiﬁers, and such priors also arise in the PAC-bayesian setting. 1</p><p>Reference: <a title="nips-2003-151-reference" href="../nips2003_reference/nips-2003-PAC-Bayesian_Generic_Chaining_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract There exist many different generalization error bounds for classiﬁcation. [sent-6, score-0.111]
</p><p>2 Each of these bounds contains an improvement over the others for certain situations. [sent-7, score-0.078]
</p><p>3 Our goal is to combine these different improvements into a single bound. [sent-8, score-0.053]
</p><p>4 In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classiﬁers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. [sent-9, score-0.491]
</p><p>5 This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classiﬁers, and such priors also arise in the PAC-bayesian setting. [sent-10, score-0.274]
</p><p>6 On the one hand, people developing empirical processes theory like Dudley and Talagrand (among others) obtained very interesting results concerning the behaviour of the suprema of empirical processes. [sent-13, score-0.122]
</p><p>7 One crucial aspect of all the generalization error bounds is that they aim at controlling the behaviour of the function that is returned by the algorithm. [sent-15, score-0.139]
</p><p>8 the difference between its empirical error and true error), one has to be able to predict which function is likely to be chosen by the algorithm. [sent-19, score-0.066]
</p><p>9 The way to perform this union bound optimally is now well mastered in the empirical processes community. [sent-22, score-0.318]
</p><p>10 In the learning theory setting, one is interested in bounds that are as algorithm and data dependent as possible. [sent-23, score-0.057]
</p><p>11 This particular focus has made concentration inequalities (see e. [sent-24, score-0.07]
</p><p>12 Another aspect that is of interest for learning is the case where the classiﬁers are randomized or averaged. [sent-27, score-0.064]
</p><p>13 McAllester [1, 4] has proposed a new type of bound that takes the randomization into account in a clever way. [sent-28, score-0.14]
</p><p>14 Next section introduces the notation and reviews the previous improved bounds that have been proposed. [sent-31, score-0.077]
</p><p>15 Finally we give the proof of the presented results. [sent-33, score-0.049]
</p><p>16 2  Previous results  We ﬁrst introduce the notation and then give an overview of existing generalization error bounds. [sent-34, score-0.075]
</p><p>17 We consider an input space X , an output space Y and a probability distribution P on the product space Z X × Y. [sent-35, score-0.063]
</p><p>18 Let Z (X, Y ) denote a pair of random variables distributed according to P and for a given integer n, let Z1 , . [sent-36, score-0.076]
</p><p>19 We denote by Pn , Pn and P2n the empirical measures associated respectively to the ﬁrst, the second and the union of both samples. [sent-43, score-0.276]
</p><p>20 For such functions, we denote their expectation under P by P f and their n empirical expectation by Pn f (i. [sent-47, score-0.154]
</p><p>21 En , En and E2n denote the expectation with respect to the ﬁrst, second and union of both training samples. [sent-50, score-0.225]
</p><p>22 We consider the pseudo-distances d2 (f1 , f2 ) = P (f1 − f2 )2 and similarly dn , dn and d2n . [sent-51, score-0.102]
</p><p>23 We denote by ρ and π two probability measures on the space F, so that ρP f will actually mean the expectation of P f when f is sampled according to the probability measure ρ. [sent-53, score-0.201]
</p><p>24 For two such measures, K(ρ, π) will denote their Kullback-Leibler divergence (K(ρ, π) = dρ ρ log dπ when ρ is absolutely continuous with respect to π and K(ρ, π) = +∞ otherwise). [sent-54, score-0.15]
</p><p>25 Also, β denotes some positive real number while C is some positive constant (whose value may differ from line to line) and M1 (F) is the set of probability measures on F. [sent-55, score-0.093]
</p><p>26 Generalization error bounds give an upper bound on the difference between the true and empirical error of functions in a given class, which holds with high probability with respect to the sampling of the training set. [sent-57, score-0.368]
</p><p>27 By Hoeffding’s inequality one easily gets that for each ﬁxed f ∈ F, with probability at least 1 − β, log 1/β P f − Pn f ≤ C . [sent-59, score-0.244]
</p><p>28 The simplest form of the union bound gives that with probability at least 1 − β, log |F| + log 1/β . [sent-62, score-0.618]
</p><p>29 When the functions have values in {0, 1}, this is a ﬁnite set and the above union bound applies. [sent-68, score-0.298]
</p><p>30 This idea was ﬁrst used by Vapnik and Chervonenkis [5] to obtain that with probability at least 1 − β, ∀f ∈ F, P f − Pn f ≤ C  log E2n N (F, 1/n, d2n ) + log 1/β . [sent-69, score-0.367]
</p><p>31 The ﬁnite union bound can be directly extended to the countable case by introducing a probability distribution π over F which weights each function and gives that with probability at least 1 − β, ∀f ∈ F, P f − Pn f ≤ C  log 1/π(f ) + log 1/β . [sent-71, score-0.729]
</p><p>32 (4) n It is interesting to notice that now the bound depends on the actual function f being considered and not just on the set F. [sent-72, score-0.127]
</p><p>33 Consider a probability distribution ρ deﬁned on a countable F, take the expectation of (4) with respect to ρ and use Jensen’s inequality. [sent-78, score-0.138]
</p><p>34 This gives with probability at least 1 − β, K(ρ, π) + H(ρ) + log 1/β ∀ρ, ρ(P f − Pn f ) ≤ C , n where H(ρ) is the Shannon entropy. [sent-79, score-0.216]
</p><p>35 is the difference between true and empirical error of a randomized classiﬁer which uses ρ as weights for choosing the decision function (independently of the data). [sent-83, score-0.147]
</p><p>36 The PAC-Bayes bound [1] is a reﬁned version of the above bound since it has the form (for possibly uncountable F) K(ρ, π) + log n + log 1/β . [sent-84, score-0.48]
</p><p>37 (6) n To some extent, one can consider that the PAC-Bayes bound is a reﬁned union bound where the gain happens when ρ is not concentrated on a single function (or more precisely ρ has entropy larger than log n). [sent-85, score-0.551]
</p><p>38 The quantity En Eσ supf ∈F n σi f (Zi ), where the σi are independent random signs (+1, −1 with probability 1/2), called the Rademacher average for F, is, up to a constant equal to En supf ∈F P f − Pn f which means that it best captures the complexity of F. [sent-87, score-0.213]
</p><p>39 One has with probability 1 − β, ∀ρ, ρ(P f − Pn f ) ≤ C  ∀f ∈ F, P f − Pn f ≤ C  1 En Eσ sup n f ∈F  σi f (Zi ) +  log 1/β n  . [sent-88, score-0.231]
</p><p>40 Another direction in which the union bound can be reﬁned is by considering ﬁnite covers of the set of function at different scales. [sent-90, score-0.311]
</p><p>41 This is called the chaining technique, pioneered by Dudley (see e. [sent-91, score-0.101]
</p><p>42 The results involve the Koltchinskii-Pollard entropy integral as, for example in [7], with probability 1 − β, ∀f ∈ F, P f − Pn f ≤ C  1 √ En n  ∞  log N (F, , dn )d + 0  log 1/β n  . [sent-94, score-0.358]
</p><p>43 It has been noticed by Fernique and Talagrand that it is possible to capture the complexity in a better way than using minimal covers by considering majorizing measures (essentially optimal for Gaussian processes). [sent-96, score-0.205]
</p><p>44 Let r > 0 and (Aj )j≥1 be partitions of F of diameter r −j w. [sent-97, score-0.064]
</p><p>45 Using (7) and techniques from [2] we obtain that with probability 1 − β, ∀f ∈ F   ∞ 1 log 1/β  P f − P n f ≤ C  √ En . [sent-101, score-0.196]
</p><p>46 (9) inf sup r−j log 1/πAj (f ) + n n π∈M1 (F ) f ∈F j=1 + If one takes partitions induced by minimal covers of F at radii r −j , one recovers (8) up to a constant. [sent-102, score-0.296]
</p><p>47 Using concentration inequalities as in [3] for example, one can get rid of the expectation appearing in the r. [sent-104, score-0.114]
</p><p>48 of (3), (8), (7) or (9) and thus obtain a bound that can be computed from the data. [sent-107, score-0.129]
</p><p>49 Reﬁning the bound (7) is possible as one can localize it (see e. [sent-108, score-0.109]
</p><p>50 Instead of using + partitions as in (9) we use approximating sets (which also induce partitions but are easier to handle here). [sent-115, score-0.076]
</p><p>51 Let pj : F → Sj be maps (which can be thought of as projections) satisfying pj (f ) = f for f ∈ Sj and pj−1 ◦ pj = pj−1 . [sent-117, score-1.767]
</p><p>52 2n The quantities π, Sj and pj are allowed to depend on X1 in an exchangeable way (i. [sent-118, score-0.651]
</p><p>53 For a probability distribution ρ on F, deﬁne its j-th projection as ρj = f ∈Sj ρ{f : pj (f ) = f }δf , where δf denotes the Dirac measure on f . [sent-121, score-0.652]
</p><p>54 To shorten notations, we denote the average distance between two successive “projections” by ρd2 ρd2 [pj (f ), pj−1 (f )]. [sent-122, score-0.082]
</p><p>55 Finally, let ∆n,j (f ) 2n j Pn [f − pj (f )] − Pn [f − pj (f )]. [sent-123, score-1.21]
</p><p>56 Theorem 1 If the following condition holds lim sup ∆n,j (f ) = 0,  j→+∞ f ∈F  a. [sent-124, score-0.098]
</p><p>57 (10)  then for any 0 < β < 1/2, with probability at least 1 − β, for any distribution ρ, we have +∞  ρPn f − Pn f0 ≤ ρPn f − Pn f0 + 5  j=1  ρd2 K(ρj , πj ) 1 j +√ n n  +∞  χj (ρd2 ), j j=1  where χj (x) = 4  x log 4j 2 β −1 log(e2 /x) . [sent-126, score-0.234]
</p><p>58 For instance, it is satisﬁed when F is ﬁnite, or when limj→+∞ supf ∈F |f −pj (f )| = 0, almost surely or also when the empirical process f → P f − Pn f is uniformly continuous (which happens for classes with ﬁnite V C dimension in particular) and limj→+∞ supf ∈F d2n (f, pj (f )) = 0. [sent-128, score-0.85]
</p><p>59 The previous theorem compares the risk on the second g g sample of any (randomized) estimator with the risk on the second sample of the reference function g . [sent-135, score-0.095]
</p><p>60 ˜ Now let us give a version of the previous theorem in which the second sample does not appear. [sent-136, score-0.112]
</p><p>61 Theorem 2 If the following condition holds lim sup En ∆n,j (f ) = 0,  j→+∞ f ∈F  a. [sent-137, score-0.098]
</p><p>62 (11)  then for any 0 < β < 1/2, with probability at least 1 − β, for any distribution ρ, we have +∞  ρP f − P f0 ≤ ρPn f − Pn f0 + 5  4  j=1  En [ρd2 ]En [K(ρj , πj )] 1 j +√ n n  +∞  χj En [ρd2 ] . [sent-139, score-0.103]
</p><p>63 Notice that our bound is localized in the sense that it depends on the function of interest (or rather on the averaging distribution ρ) and does not involve a supremum over the class. [sent-141, score-0.222]
</p><p>64 Also, the union bound is performed in an optimal way since, if one plugs in a distribution ρ concentrated on a single function, takes a supremum over F in the r. [sent-142, score-0.337]
</p><p>65 , and upper bounds the squared distance by the diameter of the partition, one recovers a result similar to (9) up to logarithmic factors but which is localized. [sent-145, score-0.133]
</p><p>66 Also, when two successive projections are identical, they do not enter in the bound (which comes from the fact that the variance weights the complexity terms). [sent-146, score-0.201]
</p><p>67 Moreover Theorem 1 also includes the PAC-Bayesian improvement for averaging classiﬁers since if one considers the set S1 = F one recovers a result similar to McAllester’s (6) which in addition contains the variance improvement such as in [9]. [sent-147, score-0.106]
</p><p>68 Finally due to the power of the generic chaining, it is possible to upper bound our result by Rademacher averages, up to logarithmic factors (using the results of [10] and [11]). [sent-148, score-0.183]
</p><p>69 As a remark, the choice of the sequence of sets Sj can generally be done by taking successive covers of the hypothesis space with geometrically decreasing radii. [sent-149, score-0.069]
</p><p>70 However, the obtained bound is not completely empirical since it involves the expectation with respect to an extra sample. [sent-150, score-0.2]
</p><p>71 Future work will focus on using concentration inequalities to give a fully empirical bound. [sent-153, score-0.138]
</p><p>72 5  Proofs  Proof of Theorem 1: The proof is inspired by previous works on PAC-bayesian bounds [12, 13] and on the generic chaining [2]. [sent-154, score-0.262]
</p><p>73 Lemma 1 For any β > 0, λ > 0, j ∈ N∗ and any exchangeable function π : X 2n → M1 (F), with probability at least 1 − β, for any probability distribution ρ ∈ M1 (F), we + + have ρ Pn [pj (f ) − pj−1 (f )] − Pn [pj (f ) − pj−1 (f )] ≤  2λ 2 n ρd2n [pj (f ), pj−1 (f )]  +  K(ρ,π)+log(β −1 ) . [sent-156, score-0.21]
</p><p>74 λ  Proof Let λ > 0 and let π : X 2n → M1 (F) be an exchangeable function. [sent-157, score-0.094]
</p><p>75 Introduce the + quantity ∆i pj (f )(Zn+i ) − pj−1 (f )(Zn+i ) + pj−1 (f )(Zi ) − pj (f )(Zi ) and h  λPn pj (f ) − pj−1 (f ) − λPn pj (f ) − pj−1 (f ) −  2λ2 d2n pj (f ), pj−1 (f ) . [sent-158, score-2.945]
</p><p>76 Now take the expectation wrt σ, where σ is a n-dimensional vector of Rademacher variables. [sent-160, score-0.061]
</p><p>77 We obtain E2n πeh  = E2n πe− ≤ E2n πe  2λ2 n  2 − 2λ n  d2n [pj (f ),pj−1 (f )] d2n [pj (f ),pj−1 (f )]  where at the last step we use that cosh s ≤ e  s2 2  e  n i=1 Pn  cosh  λ2 i=1 2n2  ∆2 i  λ n ∆i  . [sent-161, score-0.098]
</p><p>78 Since  ∆2 ≤ 2 pj (f )(Zn+i ) − pj−1 (f )(Zn+i ) i  2  2  + 2 pj (f )(Zi ) − pj−1 (f )(Zi ) ,  we obtain that for any λ > 0, E2n πeh ≤ 1. [sent-162, score-1.198]
</p><p>79 Now let us apply this result to the projected measures πj and ρj . [sent-164, score-0.08]
</p><p>80 Since, by deﬁnition, π, Sj and pj are exchangeable, πj is also exchangeable. [sent-165, score-0.589]
</p><p>81 Since pj (f ) = f for any f ∈ Sj , with probability at least 1 − β, uniformly in ρ, we have ρj Pn [f − pj−1 (f )] − Pn [pj (f ) − pj−1 (f )] ≤ where Kj  Kj 2λ ρj d2 [f, pj−1 (f )] + , 2n n λ  K(ρj , πj ) + log(β −1 ). [sent-166, score-0.699]
</p><p>82 Therefore, we need to get a version of this inequality which holds uniformly in λ. [sent-170, score-0.077]
</p><p>83 log 2 2n  First let us note that when ρd2 = 0, we have ρ∆j = 0. [sent-171, score-0.163]
</p><p>84 When ρd2 > 0, let m j j k/2  and  ∗  λk = me and let b be a function from R to (0, 1] such that k≥1 b(λk ) ≤ 1. [sent-172, score-0.064]
</p><p>85 From the previous lemma and a union bound, we obtain that for any β > 0 and any integer j with probability at least 1 − β, for any k ∈ N∗ and any distribution ρ, we have ρ∆j ≤  2λk 2 K(ρj , πj ) + log [b(λk )]−1 β −1 ρdj + . [sent-173, score-0.461]
</p><p>86 n λk  Let us take the function b such that λ →  log [b(λ)]−1 λ  is continuous and decreasing. [sent-174, score-0.131]
</p><p>87 λ∗  ∗  For Then there exists a parameter λ∗ > 0 such that 2λ ρd2 = j n any β < 1/2, we have (λ∗ )2 ρd2 ≥ log 2 n, hence λ∗ ≥ m. [sent-176, score-0.131]
</p><p>88 Then we have ∗√ K(ρj ,πj )+log([b(λ∗ )]−1 β −1 ) ρ∆j ≤ 2λ eρd2 + j n λ∗ (16) √ 2 2 K(ρ , π ) + log ([b(λ )]−1 β −1 ) . [sent-178, score-0.131]
</p><p>89 By simply using an union bound with weights taken proportional to 1/j 2 , we have that the previous inequation holds uniformly in j ∈ N∗ provided that β −1 is replaced with π 2 2 −1 since j∈N∗ 1/j 2 = π 2 /6 ≈ 1. [sent-185, score-0.357]
</p><p>90 Notice that 6 j β J  ρ Pn f − Pn f0 + Pn f0 − Pn f = ρ∆n,J (f ) +  j=1  ρj (Pn − Pn )f − (Pn − Pn )pj−1 (f )  because pj−1 = pj−1 ◦ pj . [sent-187, score-0.589]
</p><p>91 So, with probability at least 1 − β, for any distribution ρ, we have ρ Pn f − Pn f0 + Pn f0 − Pn f ≤ supF ∆n,J + 5 +3. [sent-188, score-0.103]
</p><p>92 75  J j=1  J j=1 ρd2 j n  ρd2 K(ρj ,πj ) j n  log 3. [sent-189, score-0.131]
</p><p>93 Proof of Theorem 2: It sufﬁces to modify slightly the proof of theorem 1. [sent-192, score-0.067]
</p><p>94 Introduce U supρ ρh + log β − K(ρ, π) , where h is still deﬁned as in equation (12). [sent-193, score-0.131]
</p><p>95 So with probability at least 1 − β, we have supρ En ρh + log β − K(ρ, π) ≤  En U ≤ 0. [sent-196, score-0.216]
</p><p>96 6  Conclusion  We have obtained a generalization error bound for randomized classiﬁers which combines several previous improvements. [sent-197, score-0.247]
</p><p>97 It contains an optimal union bound, both in the sense of optimally taking into account the metric structure of the set of functions (via the majorizing measure approach) and in the sense of taking into account the averaging distribution. [sent-198, score-0.338]
</p><p>98 We believe that this is a very natural way of combining these two aspects as the result relies on the comparison of a majorizing measure which can be thought of as a prior probability distribution and a randomization distribution which can be considered as a posterior distribution. [sent-199, score-0.229]
</p><p>99 Future work will focus on giving a totally empirical bound (in the induction setting) and investigating possible constructions for the approximating sets Sj . [sent-200, score-0.176]
</p><p>100 Data-dependent generalization error bounds for (noisy) classiﬁcation: a PACbayesian approach. [sent-269, score-0.111]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pn', 0.601), ('pj', 0.589), ('en', 0.181), ('union', 0.162), ('log', 0.131), ('majorizing', 0.117), ('rademacher', 0.117), ('eh', 0.115), ('bound', 0.109), ('sj', 0.107), ('chaining', 0.101), ('supf', 0.084), ('zn', 0.077), ('zi', 0.065), ('randomized', 0.064), ('exchangeable', 0.062), ('talagrand', 0.058), ('bounds', 0.057), ('generic', 0.056), ('sup', 0.055), ('dn', 0.051), ('chervonenkis', 0.051), ('measures', 0.048), ('empirical', 0.047), ('aj', 0.045), ('probability', 0.045), ('expectation', 0.044), ('mcallester', 0.043), ('vapnik', 0.042), ('covers', 0.04), ('concentration', 0.04), ('least', 0.04), ('theorem', 0.039), ('cosh', 0.039), ('dudley', 0.039), ('fernique', 0.039), ('limj', 0.039), ('kj', 0.038), ('partitions', 0.038), ('generalization', 0.035), ('localized', 0.034), ('pacbayesian', 0.034), ('shorten', 0.034), ('remark', 0.033), ('let', 0.032), ('recovers', 0.032), ('averaging', 0.032), ('nements', 0.031), ('randomization', 0.031), ('laboratoire', 0.031), ('countable', 0.031), ('combine', 0.031), ('inequalities', 0.03), ('successive', 0.029), ('supremum', 0.029), ('behaviour', 0.028), ('proof', 0.028), ('inequality', 0.028), ('functions', 0.027), ('france', 0.027), ('dj', 0.027), ('paris', 0.027), ('re', 0.026), ('ers', 0.026), ('classi', 0.026), ('bousquet', 0.026), ('diameter', 0.026), ('uniformly', 0.025), ('integer', 0.025), ('comes', 0.025), ('holds', 0.024), ('notations', 0.024), ('verlag', 0.023), ('improvements', 0.022), ('projections', 0.021), ('jensen', 0.021), ('improvement', 0.021), ('give', 0.021), ('happens', 0.021), ('nite', 0.02), ('previous', 0.02), ('induction', 0.02), ('berlin', 0.02), ('obtain', 0.02), ('concentrated', 0.019), ('denote', 0.019), ('lim', 0.019), ('error', 0.019), ('logarithmic', 0.018), ('notice', 0.018), ('distribution', 0.018), ('risk', 0.018), ('weights', 0.017), ('legendre', 0.017), ('probabilit', 0.017), ('een', 0.017), ('bringing', 0.017), ('upperbound', 0.017), ('wrt', 0.017), ('preprint', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="151-tfidf-1" href="./nips-2003-PAC-Bayesian_Generic_Chaining.html">151 nips-2003-PAC-Bayesian Generic Chaining</a></p>
<p>Author: Jean-yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds for classiﬁcation. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classiﬁers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classiﬁers, and such priors also arise in the PAC-bayesian setting. 1</p><p>2 0.29320011 <a title="151-tfidf-2" href="./nips-2003-Geometric_Clustering_Using_the_Information_Bottleneck_Method.html">82 nips-2003-Geometric Clustering Using the Information Bottleneck Method</a></p>
<p>Author: Susanne Still, William Bialek, Léon Bottou</p><p>Abstract: We argue that K–means and deterministic annealing algorithms for geometric clustering can be derived from the more general Information Bottleneck approach. If we cluster the identities of data points to preserve information about their location, the set of optimal solutions is massively degenerate. But if we treat the equations that deﬁne the optimal solution as an iterative algorithm, then a set of “smooth” initial conditions selects solutions with the desired geometrical properties. In addition to conceptual uniﬁcation, we argue that this approach can be more efﬁcient and robust than classic algorithms. 1</p><p>3 0.29025754 <a title="151-tfidf-3" href="./nips-2003-Self-calibrating_Probability_Forecasting.html">170 nips-2003-Self-calibrating Probability Forecasting</a></p>
<p>Author: Vladimir Vovk, Glenn Shafer, Ilia Nouretdinov</p><p>Abstract: In the problem of probability forecasting the learner’s goal is to output, given a training set and a new object, a suitable probability measure on the possible values of the new object’s label. An on-line algorithm for probability forecasting is said to be well-calibrated if the probabilities it outputs agree with the observed frequencies. We give a natural nonasymptotic formalization of the notion of well-calibratedness, which we then study under the assumption of randomness (the object/label pairs are independent and identically distributed). It turns out that, although no probability forecasting algorithm is automatically well-calibrated in our sense, there exists a wide class of algorithms for “multiprobability forecasting” (such algorithms are allowed to output a set, ideally very narrow, of probability measures) which satisfy this property; we call the algorithms in this class “Venn probability machines”. Our experimental results demonstrate that a 1-Nearest Neighbor Venn probability machine performs reasonably well on a standard benchmark data set, and one of our theoretical results asserts that a simple Venn probability machine asymptotically approaches the true conditional probabilities regardless, and without knowledge, of the true probability measure generating the examples.</p><p>4 0.16354325 <a title="151-tfidf-4" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We discuss an idea for collecting data in a relatively efﬁcient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantiﬁes the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design. For example, we are able to explicitly calculate the asymptotic relative efﬁciency of the “staircase method” widely employed in psychophysics research, and to demonstrate the dependence of this efﬁciency on the form of the “psychometric function” underlying the output responses. 1</p><p>5 0.14342649 <a title="151-tfidf-5" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>Author: Ting-fan Wu, Chih-jen Lin, Ruby C. Weng</p><p>Abstract: Pairwise coupling is a popular multi-class classiﬁcation method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3]. 1</p><p>6 0.071277834 <a title="151-tfidf-6" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>7 0.067569345 <a title="151-tfidf-7" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>8 0.060048889 <a title="151-tfidf-8" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>9 0.05765247 <a title="151-tfidf-9" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>10 0.05752394 <a title="151-tfidf-10" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>11 0.057318382 <a title="151-tfidf-11" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>12 0.056563687 <a title="151-tfidf-12" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>13 0.054256555 <a title="151-tfidf-13" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>14 0.052732505 <a title="151-tfidf-14" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>15 0.052287839 <a title="151-tfidf-15" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>16 0.051363092 <a title="151-tfidf-16" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>17 0.050911348 <a title="151-tfidf-17" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>18 0.048978418 <a title="151-tfidf-18" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>19 0.047074202 <a title="151-tfidf-19" href="./nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</a></p>
<p>20 0.045812506 <a title="151-tfidf-20" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, -0.072), (2, -0.051), (3, 0.02), (4, 0.126), (5, 0.085), (6, -0.044), (7, 0.038), (8, -0.265), (9, 0.105), (10, -0.241), (11, -0.082), (12, 0.143), (13, 0.242), (14, 0.289), (15, -0.11), (16, 0.105), (17, -0.007), (18, -0.207), (19, -0.013), (20, 0.169), (21, 0.124), (22, -0.085), (23, 0.06), (24, 0.065), (25, 0.054), (26, -0.083), (27, 0.005), (28, -0.027), (29, 0.022), (30, -0.022), (31, 0.081), (32, -0.062), (33, -0.058), (34, -0.05), (35, -0.076), (36, -0.03), (37, 0.058), (38, -0.1), (39, 0.0), (40, 0.057), (41, 0.06), (42, 0.065), (43, -0.024), (44, -0.031), (45, 0.026), (46, 0.052), (47, 0.011), (48, -0.059), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96632248 <a title="151-lsi-1" href="./nips-2003-PAC-Bayesian_Generic_Chaining.html">151 nips-2003-PAC-Bayesian Generic Chaining</a></p>
<p>Author: Jean-yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds for classiﬁcation. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classiﬁers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classiﬁers, and such priors also arise in the PAC-bayesian setting. 1</p><p>2 0.88806176 <a title="151-lsi-2" href="./nips-2003-Self-calibrating_Probability_Forecasting.html">170 nips-2003-Self-calibrating Probability Forecasting</a></p>
<p>Author: Vladimir Vovk, Glenn Shafer, Ilia Nouretdinov</p><p>Abstract: In the problem of probability forecasting the learner’s goal is to output, given a training set and a new object, a suitable probability measure on the possible values of the new object’s label. An on-line algorithm for probability forecasting is said to be well-calibrated if the probabilities it outputs agree with the observed frequencies. We give a natural nonasymptotic formalization of the notion of well-calibratedness, which we then study under the assumption of randomness (the object/label pairs are independent and identically distributed). It turns out that, although no probability forecasting algorithm is automatically well-calibrated in our sense, there exists a wide class of algorithms for “multiprobability forecasting” (such algorithms are allowed to output a set, ideally very narrow, of probability measures) which satisfy this property; we call the algorithms in this class “Venn probability machines”. Our experimental results demonstrate that a 1-Nearest Neighbor Venn probability machine performs reasonably well on a standard benchmark data set, and one of our theoretical results asserts that a simple Venn probability machine asymptotically approaches the true conditional probabilities regardless, and without knowledge, of the true probability measure generating the examples.</p><p>3 0.61316621 <a title="151-lsi-3" href="./nips-2003-Geometric_Clustering_Using_the_Information_Bottleneck_Method.html">82 nips-2003-Geometric Clustering Using the Information Bottleneck Method</a></p>
<p>Author: Susanne Still, William Bialek, Léon Bottou</p><p>Abstract: We argue that K–means and deterministic annealing algorithms for geometric clustering can be derived from the more general Information Bottleneck approach. If we cluster the identities of data points to preserve information about their location, the set of optimal solutions is massively degenerate. But if we treat the equations that deﬁne the optimal solution as an iterative algorithm, then a set of “smooth” initial conditions selects solutions with the desired geometrical properties. In addition to conceptual uniﬁcation, we argue that this approach can be more efﬁcient and robust than classic algorithms. 1</p><p>4 0.47584024 <a title="151-lsi-4" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We discuss an idea for collecting data in a relatively efﬁcient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantiﬁes the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design. For example, we are able to explicitly calculate the asymptotic relative efﬁciency of the “staircase method” widely employed in psychophysics research, and to demonstrate the dependence of this efﬁciency on the form of the “psychometric function” underlying the output responses. 1</p><p>5 0.39988667 <a title="151-lsi-5" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>Author: Ting-fan Wu, Chih-jen Lin, Ruby C. Weng</p><p>Abstract: Pairwise coupling is a popular multi-class classiﬁcation method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3]. 1</p><p>6 0.24137734 <a title="151-lsi-6" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>7 0.24107175 <a title="151-lsi-7" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>8 0.23536463 <a title="151-lsi-8" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>9 0.21018641 <a title="151-lsi-9" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>10 0.20849869 <a title="151-lsi-10" href="./nips-2003-Sparseness_of_Support_Vector_Machines---Some_Asymptotically_Sharp_Bounds.html">180 nips-2003-Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds</a></p>
<p>11 0.20217845 <a title="151-lsi-11" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>12 0.20205869 <a title="151-lsi-12" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>13 0.20162016 <a title="151-lsi-13" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>14 0.19997467 <a title="151-lsi-14" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>15 0.19552425 <a title="151-lsi-15" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>16 0.1921061 <a title="151-lsi-16" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>17 0.19000274 <a title="151-lsi-17" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>18 0.18671919 <a title="151-lsi-18" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>19 0.17792036 <a title="151-lsi-19" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>20 0.17587876 <a title="151-lsi-20" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.036), (11, 0.017), (26, 0.278), (29, 0.01), (35, 0.054), (53, 0.088), (58, 0.013), (69, 0.016), (71, 0.06), (76, 0.041), (85, 0.101), (91, 0.118), (99, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90913117 <a title="151-lda-1" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>Author: Jun Suzuki, Yutaka Sasaki, Eisaku Maeda</p><p>Abstract: This paper devises a novel kernel function for structured natural language data. In the ﬁeld of Natural Language Processing, feature extraction consists of the following two steps: (1) syntactically and semantically analyzing raw data, i.e., character strings, then representing the results as discrete structures, such as parse trees and dependency graphs with part-of-speech tags; (2) creating (possibly high-dimensional) numerical feature vectors from the discrete structures. The new kernels, called Hierarchical Directed Acyclic Graph (HDAG) kernels, directly accept DAGs whose nodes can contain DAGs. HDAG data structures are needed to fully reﬂect the syntactic and semantic structures that natural language data inherently have. In this paper, we deﬁne the kernel function and show how it permits efﬁcient calculation. Experiments demonstrate that the proposed kernels are superior to existing kernel functions, e.g., sequence kernels, tree kernels, and bag-of-words kernels. 1</p><p>same-paper 2 0.83404064 <a title="151-lda-2" href="./nips-2003-PAC-Bayesian_Generic_Chaining.html">151 nips-2003-PAC-Bayesian Generic Chaining</a></p>
<p>Author: Jean-yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds for classiﬁcation. Each of these bounds contains an improvement over the others for certain situations. Our goal is to combine these different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester [1], which is interesting for averaging classiﬁers, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand [2]. This combination is quite natural since the generic chaining is based on the notion of majorizing measures, which can be considered as priors on the set of classiﬁers, and such priors also arise in the PAC-bayesian setting. 1</p><p>3 0.74354237 <a title="151-lda-3" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>Author: Sanjiv Kumar, Martial Hebert</p><p>Abstract: In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classiﬁcation of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classiﬁcation problems using the graph min-cut algorithms. The performance of the model was veriﬁed on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments. 1</p><p>4 0.58022082 <a title="151-lda-4" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>5 0.58018702 <a title="151-lda-5" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>6 0.58001393 <a title="151-lda-6" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>7 0.57595146 <a title="151-lda-7" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>8 0.570889 <a title="151-lda-8" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>9 0.56647646 <a title="151-lda-9" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>10 0.56536973 <a title="151-lda-10" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>11 0.56420475 <a title="151-lda-11" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>12 0.56319118 <a title="151-lda-12" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>13 0.56214273 <a title="151-lda-13" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>14 0.56204945 <a title="151-lda-14" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>15 0.56180114 <a title="151-lda-15" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>16 0.56169122 <a title="151-lda-16" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>17 0.56167465 <a title="151-lda-17" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>18 0.56111276 <a title="151-lda-18" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>19 0.56055623 <a title="151-lda-19" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>20 0.56042647 <a title="151-lda-20" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
