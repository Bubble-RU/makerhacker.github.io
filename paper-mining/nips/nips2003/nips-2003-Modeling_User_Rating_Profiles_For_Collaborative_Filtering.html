<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-131" href="#">nips2003-131</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</h1>
<br/><p>Source: <a title="nips-2003-131-pdf" href="http://papers.nips.cc/paper/2377-modeling-user-rating-profiles-for-collaborative-filtering.pdf">pdf</a></p><p>Author: Benjamin M. Marlin</p><p>Abstract: In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. 1</p><p>Reference: <a title="nips-2003-131-reference" href="../nips2003_reference/nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). [sent-3, score-0.281]
</p><p>2 The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. [sent-4, score-1.426]
</p><p>3 Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. [sent-5, score-0.682]
</p><p>4 The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. [sent-6, score-1.434]
</p><p>5 URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. [sent-7, score-0.471]
</p><p>6 1  Introduction  In rating-based collaborative ﬁltering, users express their preferences by explicitly assigning ratings to items that they have accessed, viewed, or purchased. [sent-8, score-0.623]
</p><p>7 , M }, and a set of V discrete rating u values {1, . [sent-15, score-0.48]
</p><p>8 In the natural case where each user has at most one rating r y for each item y, the ratings for each user form a vector with one component per item. [sent-19, score-1.515]
</p><p>9 We refer to user u’s rating vector as their rating proﬁle denoted ru . [sent-21, score-1.392]
</p><p>10 Given a particular item and user, the goal is to predict the user’s true rating for the item in question. [sent-23, score-0.756]
</p><p>11 Early work on rating prediction focused on neighborhood-based methods such as the GroupLens algorithm [9]. [sent-24, score-0.511]
</p><p>12 Personalized recommendations can be generated for any user by ﬁrst predicting ratings for all items the user has not rated, and recommending items with the highest predicted ratings. [sent-25, score-1.072]
</p><p>13 The capability to predict ratings has other interesting applications. [sent-26, score-0.402]
</p><p>14 Rating prediction also facilitates an active approach to collaborative ﬁltering using expected value of information. [sent-28, score-0.15]
</p><p>15 In such a framework the predicted rating of each item is interpreted as its expected utility to the user [2]. [sent-29, score-0.917]
</p><p>16 A handful of such models exist including the multinomial mixture model shown in ﬁgure 3, and the aspect model shown in ﬁgure 1 [7]. [sent-31, score-0.502]
</p><p>17 As latent variable models, both the aspect model and the multinomial mixture model have an intuitive appeal. [sent-32, score-0.544]
</p><p>18 They can be interpreted as decomposing user preferences proﬁles into a set of typical preference patterns, and the degree to which each user participates in each preference pattern. [sent-33, score-0.701]
</p><p>19 The settings of the latent variable are casually referred to as user attitudes. [sent-34, score-0.339]
</p><p>20 The multinomial mixture model constrains all users to have the same prior distribution over user attitudes, while the aspect model allows each user to have a diﬀerent prior distribution over user attitudes. [sent-35, score-1.427]
</p><p>21 The added ﬂexibility of the aspect model is quite attractive, but the interpretation of the distribution over user attitudes as parameters instead of random variables induces several problems. [sent-36, score-0.537]
</p><p>22 1 First, the aspect model lacks a principled, maximum likelihood inference procedure for novel user proﬁles. [sent-37, score-0.475]
</p><p>23 Second the number of parameters in the model grows linearly with the number of users in the data set. [sent-38, score-0.109]
</p><p>24 Recent research has seen the proposal of several generative latent variable models for discrete data, including Latent Dirichlet Allocation [1] shown in ﬁgure 2, and multinomial PCA (a generalization of LDA to priors other than Dirichlet) [3]. [sent-39, score-0.383]
</p><p>25 They can only be applied to rating data if the data is ﬁrst processed into user-item pairs using some type of thresholding operation on the rating values. [sent-41, score-0.96]
</p><p>26 These models can then be used to generate recommendations; however, they can not be used to infer a distribution over ratings of items, or to predict the ratings of items. [sent-42, score-0.763]
</p><p>27 The contribution of this paper is a new generative, latent variable model that views rating-based data at the level of user rating proﬁles. [sent-43, score-0.863]
</p><p>28 The URP model incorporates proper generative semantics at the user level that are similar to those used in LDA and mPCA, while the inner workings of the model are designed speciﬁcally for rating proﬁles. [sent-44, score-0.939]
</p><p>29 Like the aspect model and the multinomial mixture model, the URP model can be interpreted in terms of decomposing rating proﬁles into typical preference patterns, and the degree to which each user participates in each pattern. [sent-45, score-1.326]
</p><p>30 In this paper we describe the URP model, give model ﬁtting and initialization procedures, and present empirical results for two data sets. [sent-46, score-0.066]
</p><p>31 2  The User Rating Proﬁle Model  The graphical representation of the aspect, LDA, multinomial mixture, and URP models are shown in ﬁgures 1 through 4. [sent-47, score-0.225]
</p><p>32 In all models U is a user index, Y is an item index, Z is a user attitude, Zy is the user attitude responsible for item y, R is a rating value, Ry is a rating value for item Y , and βvyz is a multinomial parameter giving P (Ry = v|Zy = z). [sent-48, score-2.431]
</p><p>33 In the aspect model θ is a set of multinomial parameters where u θz represents P (Z = z|U = u). [sent-49, score-0.36]
</p><p>34 The number of these parameters obviously grows as the number of training users is increased. [sent-50, score-0.078]
</p><p>35 In the mixture of multinomials model θ is a single distribution over user attitudes where θz represents P (Z = z). [sent-51, score-0.505]
</p><p>36 This gives the multinomial mixture model correct, yet simplistic, generative semantics at the user level. [sent-52, score-0.692]
</p><p>37 A unique θ is sampled for each user where θz gives 1 Girolami and Kab´n have recently shown that a co-occurrence version of the aspect a model can be interpreted as a MAP/ML estimated LDA model under a uniform Dirichlet prior [5]. [sent-54, score-0.48]
</p><p>38 Essentially the same relationship holds between the aspect model for ratings shown in ﬁgure 1, and the URP model. [sent-55, score-0.498]
</p><p>39 This gives URP much more powerful generative semantics at the user level than the multinomial mixture model. [sent-57, score-0.674]
</p><p>40 Note that the bottom level of the LDA model consists of an item variable Y , and ratings do not come into LDA at any point. [sent-59, score-0.529]
</p><p>41 The probability of observing a given user rating proﬁle ru under the URP model is shown in equation 1 where we deﬁne δ(ru , v) to be equal to 1 if user u assigned y rating v to item y, and 0 otherwise. [sent-60, score-1.825]
</p><p>42 Note that we assume unspeciﬁed ratings are missing at random. [sent-61, score-0.35]
</p><p>43 As in LDA, the Dirichlet prior renders the computation of the posterior distribution p(θ, z|ru , α, β) = P (θ, z, ru |α, β)/P (ru |α, β) intractable. [sent-62, score-0.161]
</p><p>44 M u  P (r |α, β) =  V  P (θ|α) θ  u δ(ry ,v)  K  P (Zy = z|θ)P (Ry = v|Zy = z, β) y=1 v=1  dθ  z=1  (1)  3  Parameter Estimation  The procedure we use for parameter estimation is a variational expectation maximization algorithm based on free energy maximization. [sent-63, score-0.144]
</p><p>45 We choose to apply a fully factored variational q-distribution as shown in equation 2. [sent-65, score-0.075]
</p><p>46 We deﬁne q(θ|γ u ) u to be a Dirichlet distribution with Dirichlet parameters γz , and q(Zy |φu ) to be a y multinomial distribution with parameters φu . [sent-66, score-0.236]
</p><p>47 zy M  P (θ, z|α, β, ru )  ≈ q(θ, z|γ u , φu ) = q(θ|γ u )  q(Zy = zy |φu ) y y=1  (2)  A per-user free energy function F [γ u , φu , α, β] provides a variational lower bound on the log likelihood log p(ru |α, β) of a single user rating proﬁle. [sent-67, score-1.457]
</p><p>48 The sum of the per-user free energy functions F [γ u , φu , α, β] yields the total free energy function F [γ, φ, α, β], which is a lower bound on the log likelihood of a complete data set of user rating proﬁles. [sent-68, score-0.883]
</p><p>49 The variational and model parameter updates are obtained by expanding F [γ, φ, α, β] using the previously described distributions, and maximizing the result with respect to γ u , φu , α and β. [sent-69, score-0.118]
</p><p>50 The variational parameter updates are shown in equations 3, and 4. [sent-70, score-0.087]
</p><p>51 V  φu zy  u  u βvyz δ(ry ,v) exp(Ψ(γz ) − Ψ(  ∝  k u j=1 γj ))  (3)  v=1 M u γz  φu zy  = αz +  (4)  y=1  By iterating the the variational updates with ﬁxed α and β for a particular user, we are guaranteed to reach a local maximum of the per-user free energy F [γ u , φu , α, β]. [sent-72, score-0.545]
</p><p>52 The model multinomial update has a closed form solution as shown in equation 5. [sent-74, score-0.243]
</p><p>53 1  Model Fitting  The variational inference procedure should be run to convergence to insure a maximum likelihood solution. [sent-82, score-0.119]
</p><p>54 However, if we are satisﬁed with simply increasing the free energy at each step, other ﬁtting procedures are possible. [sent-83, score-0.075]
</p><p>55 In general, the number of steps of variational inference can be determined by a user dependant heuristic function H(u). [sent-84, score-0.375]
</p><p>56 Buntine uses a single step of variational inference for each user to ﬁt the mPCA model. [sent-85, score-0.375]
</p><p>57 Empirically, we have found that simple linear functions, of the number of ratings in each user proﬁle provide a good heuristic. [sent-88, score-0.633]
</p><p>58 u  u βryz δ(ry ,v) exp(Ψ(γz ) − Ψ(  u γz = αz +  M y=1  k u j=1 γj ))  φu zy  M-Step: 1. [sent-94, score-0.202]
</p><p>59 The initialization method we have adopted is to partially ﬁt a multinomial mixture model with the same number of user attitudes as the URP model. [sent-101, score-0.74]
</p><p>60 Fitting the multinomial mixture model for a small number of EM iterations yields a set of multinomial distributions encoded by β , as well as a single multinomial distribution over user attitudes encoded by θ . [sent-102, score-1.141]
</p><p>61 Normally EM is run until the bound on log likelihood converges, but this tends to lead to over ﬁtting in some models including the aspect model. [sent-105, score-0.155]
</p><p>62 We implemented early stopping for all models using a separate validation set to allow for a fair comparison. [sent-107, score-0.067]
</p><p>63 5  Prediction  The primary task for any model applied to the rating-based collaborative ﬁltering problem is to predict ratings for the items a user has not rated, based on the ratings the user has speciﬁed. [sent-108, score-1.514]
</p><p>64 Assume we have a user u with rating proﬁle ru , and we wish u to predict the user’s rating ry for an unrated item y. [sent-109, score-1.688]
</p><p>65 K u  p(Ry = v|r ) =  βvyz z=1  u γz K j=1  u γj  (9)  To compute P (Ry = v|ru ) according to equation 9 given the model parameters α and β, it is necessary to apply our variational inference procedure to compute γ u . [sent-111, score-0.138]
</p><p>66 However, this only needs to be done once for each user in order to predict all unknown ratings in the user’s proﬁle. [sent-112, score-0.671]
</p><p>67 One could predict the rating with maximal probability, predict the expected rating, or predict the median rating. [sent-114, score-0.608]
</p><p>68 Of course, each of these prediction rules minimizes a diﬀerent prediction error measure. [sent-115, score-0.062]
</p><p>69 In particular, median prediction minimizes the mean absolute error and is the prediction rule we use in our experiments. [sent-116, score-0.106]
</p><p>70 6  Experimentation  We consider two diﬀerent experimental procedures that test the predictive ability of a rating-based collaborative ﬁltering method. [sent-117, score-0.14]
</p><p>71 The ﬁrst is a weak generalization all-but-1 experiment where one of each user’s ratings is held out. [sent-118, score-0.475]
</p><p>72 The model is then trained on the remaining observed ratings and tested on the held out ratings. [sent-119, score-0.42]
</p><p>73 This experiment is designed to test the ability of a method to generalize to other items rated by the users it was trained on. [sent-120, score-0.233]
</p><p>74 The model is ﬁrst trained using all ratings from a set of training users. [sent-122, score-0.398]
</p><p>75 This experiment is designed to test the ability of the model to generalize to novel user proﬁles. [sent-124, score-0.352]
</p><p>76 The well known EachMovie data set, and the recently released million rating MovieLens data set. [sent-126, score-0.48]
</p><p>77 Both data sets were ﬁltered to contain users with at least 20 ratings. [sent-127, score-0.078]
</p><p>78 EachMovie was ﬁltered to remove movies with less than 2 ratings leaving 1621 movies. [sent-128, score-0.378]
</p><p>79 The EachMovie training sets contained 30000 users while the test sets contained 5000 users. [sent-130, score-0.118]
</p><p>80 The MovieLens training sets contained 5000 users while the test sets contained 1000 users. [sent-131, score-0.118]
</p><p>81 The EachMovie rating scale is from 0 to 5, while the MovieLens rating scale is from 1 to 5. [sent-132, score-0.96]
</p><p>82 Both types of experiment were performed for a range of numbers of user attitudes. [sent-133, score-0.301]
</p><p>83 For each model and number of user attitudes, each experiment was repeated on three diﬀerent random partitions of each base data set into known ratings, held out ratings, validation ratings, training users and testing users. [sent-134, score-0.432]
</p><p>84 In the weak generalization experiments the aspect, multinomial mixture, and URP models were tested. [sent-135, score-0.31]
</p><p>85 In the strong generalization experiments only the multinomial mixture and URP models were tested since a trained aspect model can not be applied to new user proﬁles. [sent-136, score-0.83]
</p><p>86 Also recall that LDA and mPCA can not be used for rating prediction so they are not be tested in these experiments. [sent-137, score-0.511]
</p><p>87 We deﬁne our NMAE to be the standard MAE normalized by the the expected value of the MAE assuming uniformly distributed rating values and rating predictions. [sent-188, score-0.979]
</p><p>88 In both the weak and strong generalization experiments using the EachMovie data set, the URP model performs signiﬁcantly better than the other methods, and obtains the lowest prediction error. [sent-196, score-0.174]
</p><p>89 The results obtained from the MovieLens data set do not show the same clean trends as the EachMovie data set for the weak generalization experiment. [sent-197, score-0.085]
</p><p>90 Nevertheless, the lowest error attained by URP is not signiﬁcantly diﬀerent than that obtained by the aspect model. [sent-199, score-0.117]
</p><p>91 In the strong generalization experiment the URP model again out performs the other methods. [sent-200, score-0.121]
</p><p>92 8  Conclusions  In this paper we have presented the URP model for rating-based collaborative ﬁltering. [sent-201, score-0.15]
</p><p>93 Our model combines the intuitive appeal of the multinomial mixture and aspect models, with the strong high level generative semantics of LDA and mPCA. [sent-202, score-0.578]
</p><p>94 As a result of being specially designed for collaborative ﬁltering, our model also contains unique rating proﬁle generative semantics not found in LDA or mPCA. [sent-203, score-0.731]
</p><p>95 This gives URP the capability to operate directly on ratings data, and to eﬃciently predict all missing ratings in a user proﬁle. [sent-204, score-1.035]
</p><p>96 This means URP can be applied to recommendation, as well as many other tasks based on rating prediction. [sent-205, score-0.48]
</p><p>97 We have empirically demonstrated on two diﬀerent data sets that the weak generalization performance of URP is at least as good as that of the aspect and multinomial mixture models. [sent-206, score-0.499]
</p><p>98 For online applications where it is impractical to reﬁt the model each time a rating is supplied by a user, the result of interest is strong generalization performance. [sent-207, score-0.583]
</p><p>99 The aspect model can not be applied in a principled manner in such a scenario, and we see that URP outperforms the other methods by a signiﬁcant margin. [sent-208, score-0.148]
</p><p>100 Combining content-based and collaborative ﬁlters in an online newspaper. [sent-236, score-0.119]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rating', 0.48), ('urp', 0.445), ('ratings', 0.35), ('user', 0.283), ('multinomial', 0.212), ('zy', 0.202), ('eachmovie', 0.162), ('movielens', 0.162), ('ru', 0.149), ('ry', 0.139), ('lda', 0.139), ('collaborative', 0.119), ('item', 0.119), ('aspect', 0.117), ('pro', 0.105), ('dirichlet', 0.102), ('attitudes', 0.094), ('mixture', 0.085), ('users', 0.078), ('variational', 0.075), ('vyz', 0.067), ('items', 0.06), ('grouplens', 0.054), ('mpca', 0.054), ('di', 0.046), ('generalization', 0.045), ('generative', 0.044), ('attitude', 0.04), ('nmae', 0.04), ('rated', 0.04), ('weak', 0.04), ('latent', 0.04), ('le', 0.039), ('ltering', 0.038), ('predict', 0.038), ('erent', 0.038), ('semantics', 0.037), ('initialization', 0.035), ('stopping', 0.034), ('preference', 0.032), ('fitting', 0.031), ('prediction', 0.031), ('model', 0.031), ('absolute', 0.03), ('energy', 0.03), ('tting', 0.029), ('mae', 0.027), ('marlin', 0.027), ('strong', 0.027), ('goldberg', 0.027), ('em', 0.025), ('les', 0.025), ('neighborhood', 0.024), ('free', 0.024), ('digamma', 0.023), ('kab', 0.023), ('ltered', 0.022), ('held', 0.022), ('participates', 0.021), ('zemel', 0.021), ('procedures', 0.021), ('early', 0.02), ('blei', 0.02), ('designed', 0.02), ('contained', 0.02), ('acm', 0.02), ('recommendations', 0.019), ('normalized', 0.019), ('experiment', 0.018), ('girolami', 0.018), ('interpreted', 0.018), ('trained', 0.017), ('inference', 0.017), ('predicted', 0.017), ('minka', 0.016), ('decomposing', 0.016), ('preferences', 0.016), ('variable', 0.016), ('leaving', 0.016), ('procedure', 0.015), ('gures', 0.014), ('median', 0.014), ('retrieval', 0.014), ('capability', 0.014), ('filtering', 0.013), ('toronto', 0.013), ('including', 0.013), ('level', 0.013), ('models', 0.013), ('european', 0.012), ('intuitive', 0.012), ('distribution', 0.012), ('gure', 0.012), ('likelihood', 0.012), ('updates', 0.012), ('rm', 0.012), ('minnesota', 0.012), ('movies', 0.012), ('carolina', 0.012), ('ceedings', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="131-tfidf-1" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>Author: Benjamin M. Marlin</p><p>Abstract: In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. 1</p><p>2 0.12164918 <a title="131-tfidf-2" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>Author: Mark Girolami, Ata Kabán</p><p>Abstract: To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the deﬁnition of individual speciﬁc and global models. This paper proposes a linear-time distributed model for ﬁnite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained’ by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-speciﬁc proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efﬁcient representation scheme, reﬂected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.</p><p>3 0.082273483 <a title="131-tfidf-3" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>4 0.050566178 <a title="131-tfidf-4" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>Author: Felix A. Wichmann, Arnulf B. Graf</p><p>Abstract: We attempt to understand visual classiﬁcation in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classiﬁcation task. Human subjects classiﬁed the faces and their gender judgment, reaction time and conﬁdence rating were recorded. Several hyperplane learning algorithms were used on the same classiﬁcation task using the Principal Components of the texture and shape representation of the faces. The classiﬁcation performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classiﬁcation can be modeled by some hyperplane algorithms in the feature space we used. For classiﬁcation, the brain needs more processing for stimuli close to that hyperplane than for those further away. 1</p><p>5 0.050096571 <a title="131-tfidf-5" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>Author: Haifeng Li, Tao Jiang, Keshu Zhang</p><p>Abstract: A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix Sw . Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efﬁcient and stable. 1</p><p>6 0.045786571 <a title="131-tfidf-6" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>7 0.044797845 <a title="131-tfidf-7" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>8 0.043468256 <a title="131-tfidf-8" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>9 0.04149133 <a title="131-tfidf-9" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>10 0.040434834 <a title="131-tfidf-10" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>11 0.040283028 <a title="131-tfidf-11" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>12 0.038878355 <a title="131-tfidf-12" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>13 0.038751669 <a title="131-tfidf-13" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>14 0.037415594 <a title="131-tfidf-14" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>15 0.033606049 <a title="131-tfidf-15" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>16 0.031418968 <a title="131-tfidf-16" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>17 0.031285528 <a title="131-tfidf-17" href="./nips-2003-GPPS%3A_A_Gaussian_Process_Positioning_System_for_Cellular_Networks.html">76 nips-2003-GPPS: A Gaussian Process Positioning System for Cellular Networks</a></p>
<p>18 0.028512083 <a title="131-tfidf-18" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>19 0.027912745 <a title="131-tfidf-19" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>20 0.025582086 <a title="131-tfidf-20" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.092), (1, -0.014), (2, -0.003), (3, 0.018), (4, -0.018), (5, -0.011), (6, 0.08), (7, -0.014), (8, -0.013), (9, -0.017), (10, -0.033), (11, -0.072), (12, -0.072), (13, -0.014), (14, 0.053), (15, 0.027), (16, -0.086), (17, 0.019), (18, 0.052), (19, -0.073), (20, -0.106), (21, 0.066), (22, -0.073), (23, -0.029), (24, 0.007), (25, 0.032), (26, -0.081), (27, -0.079), (28, 0.075), (29, 0.086), (30, 0.054), (31, -0.037), (32, -0.13), (33, 0.116), (34, -0.107), (35, 0.064), (36, 0.061), (37, -0.07), (38, -0.103), (39, 0.033), (40, -0.203), (41, -0.111), (42, -0.04), (43, -0.044), (44, -0.002), (45, 0.049), (46, -0.014), (47, -0.025), (48, 0.064), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95828331 <a title="131-lsi-1" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>Author: Benjamin M. Marlin</p><p>Abstract: In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. 1</p><p>2 0.74513751 <a title="131-lsi-2" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>Author: Mark Girolami, Ata Kabán</p><p>Abstract: To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the deﬁnition of individual speciﬁc and global models. This paper proposes a linear-time distributed model for ﬁnite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained’ by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-speciﬁc proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efﬁcient representation scheme, reﬂected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.</p><p>3 0.4453665 <a title="131-lsi-3" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>4 0.43825638 <a title="131-lsi-4" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>5 0.34314653 <a title="131-lsi-5" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>Author: Jason Palmer, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a speciﬁc approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice. 1</p><p>6 0.33871585 <a title="131-lsi-6" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>7 0.33512712 <a title="131-lsi-7" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>8 0.33380133 <a title="131-lsi-8" href="./nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F.html">195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</a></p>
<p>9 0.32468185 <a title="131-lsi-9" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>10 0.30091575 <a title="131-lsi-10" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>11 0.29920921 <a title="131-lsi-11" href="./nips-2003-Link_Prediction_in_Relational_Data.html">118 nips-2003-Link Prediction in Relational Data</a></p>
<p>12 0.27757129 <a title="131-lsi-12" href="./nips-2003-GPPS%3A_A_Gaussian_Process_Positioning_System_for_Cellular_Networks.html">76 nips-2003-GPPS: A Gaussian Process Positioning System for Cellular Networks</a></p>
<p>13 0.27594486 <a title="131-lsi-13" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>14 0.27073753 <a title="131-lsi-14" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>15 0.26641068 <a title="131-lsi-15" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>16 0.26543099 <a title="131-lsi-16" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>17 0.25580114 <a title="131-lsi-17" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>18 0.25172096 <a title="131-lsi-18" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>19 0.24671955 <a title="131-lsi-19" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>20 0.2410294 <a title="131-lsi-20" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (5, 0.374), (11, 0.017), (29, 0.012), (35, 0.106), (53, 0.07), (66, 0.014), (71, 0.051), (76, 0.027), (85, 0.075), (91, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80293334 <a title="131-lda-1" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>Author: Benjamin M. Marlin</p><p>Abstract: In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. 1</p><p>2 0.63028222 <a title="131-lda-2" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>Author: Ting-fan Wu, Chih-jen Lin, Ruby C. Weng</p><p>Abstract: Pairwise coupling is a popular multi-class classiﬁcation method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3]. 1</p><p>3 0.57226056 <a title="131-lda-3" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>Author: Volker Roth, Tilman Lange</p><p>Abstract: A novel approach to combining clustering and feature selection is presented. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. On the technical side, we present an efﬁcient optimization algorithm with guaranteed local convergence property. The only free parameter of this method is selected by a resampling-based stability analysis. Experiments with real-world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features. 1</p><p>4 0.42179906 <a title="131-lda-4" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>5 0.41389763 <a title="131-lda-5" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>6 0.40905795 <a title="131-lda-6" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>7 0.40860242 <a title="131-lda-7" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>8 0.40721309 <a title="131-lda-8" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>9 0.40471753 <a title="131-lda-9" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>10 0.39994025 <a title="131-lda-10" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>11 0.39947784 <a title="131-lda-11" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>12 0.39942485 <a title="131-lda-12" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>13 0.39868474 <a title="131-lda-13" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>14 0.39667946 <a title="131-lda-14" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>15 0.39569777 <a title="131-lda-15" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>16 0.39566723 <a title="131-lda-16" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>17 0.39502496 <a title="131-lda-17" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>18 0.39419192 <a title="131-lda-18" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>19 0.39410421 <a title="131-lda-19" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>20 0.39356998 <a title="131-lda-20" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
