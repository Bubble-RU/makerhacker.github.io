<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-95" href="#">nips2003-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</h1>
<br/><p>Source: <a title="nips-2003-95-pdf" href="http://papers.nips.cc/paper/2484-insights-from-machine-learning-applied-to-human-visual-classification.pdf">pdf</a></p><p>Author: Felix A. Wichmann, Arnulf B. Graf</p><p>Abstract: We attempt to understand visual classiﬁcation in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classiﬁcation task. Human subjects classiﬁed the faces and their gender judgment, reaction time and conﬁdence rating were recorded. Several hyperplane learning algorithms were used on the same classiﬁcation task using the Principal Components of the texture and shape representation of the faces. The classiﬁcation performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classiﬁcation can be modeled by some hyperplane algorithms in the feature space we used. For classiﬁcation, the brain needs more processing for stimuli close to that hyperplane than for those further away. 1</p><p>Reference: <a title="nips-2003-95-reference" href="../nips2003_reference/nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We attempt to understand visual classiﬁcation in humans using both psychophysical and machine learning techniques. [sent-8, score-0.227]
</p><p>2 Frontal views of human faces were used for a gender classiﬁcation task. [sent-9, score-0.604]
</p><p>3 Human subjects classiﬁed the faces and their gender judgment, reaction time and conﬁdence rating were recorded. [sent-10, score-0.869]
</p><p>4 Several hyperplane learning algorithms were used on the same classiﬁcation task using the Principal Components of the texture and shape representation of the faces. [sent-11, score-0.175]
</p><p>5 The classiﬁcation performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. [sent-12, score-0.946]
</p><p>6 We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. [sent-13, score-0.547]
</p><p>7 Our results suggest that human classiﬁcation can be modeled by some hyperplane algorithms in the feature space we used. [sent-14, score-0.266]
</p><p>8 For classiﬁcation, the brain needs more processing for stimuli close to that hyperplane than for those further away. [sent-15, score-0.341]
</p><p>9 1  Introduction  The last decade has seen tremendous technological advances in neuroscience from the microscopic to the macroscopic scale (e. [sent-16, score-0.107]
</p><p>10 On an algorithmic level, however, methods and understanding of brain processes are still limited. [sent-19, score-0.072]
</p><p>11 Here we report on a study combining psychophysical and machine learning techniques in order to improve our understanding of human classiﬁcation of visual stimuli. [sent-20, score-0.274]
</p><p>12 What algorithms best describe the way the human brain classiﬁes? [sent-21, score-0.219]
</p><p>13 Might humans use something akin to hyperplanes for classiﬁcation? [sent-22, score-0.203]
</p><p>14 If so, is the learning rule as simple as in mean-of-class prototype learners or are more sophisticated algorithms better candidates? [sent-23, score-0.191]
</p><p>15 In our experiments, subjects and machines classiﬁed human faces according to gender. [sent-24, score-0.694]
</p><p>16 The stimuli were presented and we collected the subjects’ responses, which are the estimated gender, reaction time and conﬁdence rating (sec. [sent-25, score-0.345]
</p><p>17 For every subject two personal new datasets were created: the original faces either with the true or with the subject’s labels (true or estimated gender response). [sent-27, score-0.565]
</p><p>18 We then applied a Principal Component Analysis to a texture and shape representation of the faces. [sent-28, score-0.056]
</p><p>19 Various algorithms such as Support Vec-  tor Machines, Relevance Vector Machines, Prototype and K-means Learners (sec. [sent-29, score-0.03]
</p><p>20 3) were applied on this low-dimensional dataset with either the true or the subjects’ labels. [sent-30, score-0.06]
</p><p>21 The resulting classiﬁcation performances were compared, the corresponding decision hyperplanes were computed and the distances of the faces to the hyperplanes were correlated with the subjects’ responses, the data being pooled among all subjects and stimuli or on a stimulus-by-stimulus basis (sec. [sent-31, score-0.94]
</p><p>22 2  Human Classiﬁcation Behaviour  We used grey-scale frontal views of human faces taken from the MPI face database [1]. [sent-33, score-0.688]
</p><p>23 Because of technical inhomogeneities of the faces in the database we post-processed each face such that all faces have same mean intensity, same pixel-surface area and are centred [2]. [sent-34, score-0.72]
</p><p>24 This processing stage is followed by a slight low-pass ﬁltering of each face in the database in order to eliminate, as much as possible, scanning artifacts. [sent-35, score-0.24]
</p><p>25 The database is gender-balanced and contains 200 Caucasian faces (see Fig. [sent-36, score-0.345]
</p><p>26 Twenty-seven human 15  13  i  eigenvalue log(λ )  14  12 11 10 9 8 7 0  20  40  60  80  100  120  140  160  180  200  index of component i  Figure 1: Female and male faces from the processed database (left). [sent-38, score-0.628]
</p><p>27 01 · 103 (the last eigenvalue being 0 is not plotted) and λmax = 2. [sent-41, score-0.051]
</p><p>28 subjects were asked to classify the faces according to their gender and we recorded three responses: estimated class (i. [sent-43, score-0.845]
</p><p>29 female/male), reaction time (RT) and, after each estimatedclass-response, a conﬁdence rating (CR) on a scale from 1 (unsure) to 3 (sure). [sent-45, score-0.164]
</p><p>30 The stimuli were presented sequentially to the subjects on a carefully calibrated display using a modiﬁed Hanning window (a raised cosine function with a raising time of ttransient = 500ms and a plateau time of tsteady = 1000ms, for a total presentation time t = 2000ms per face). [sent-46, score-0.522]
</p><p>31 Subjects were asked to answer as fast as possible to obtain perceptual, rather than cognitive, judgements. [sent-47, score-0.041]
</p><p>32 Most of the time they responded well before the presentation of the stimulus had ended (mean RT over all stimuli and subjects was approximately 900ms). [sent-48, score-0.492]
</p><p>33 All subjects had normal or corrected-to-normal vision and were paid for their participation. [sent-49, score-0.315]
</p><p>34 Analysis of the classiﬁcation performance of humans is based on signal detection theory [3] and we assume that, on the decision axis, the internal signal and noise distributions are Gaussian with same unit variance but different means. [sent-51, score-0.135]
</p><p>35 We deﬁne correct response probabilities for males (+) and females (−) as P+ = P (ˆ = 1|y = 1) and y P− = P (ˆ = −1|y = −1) where y is the estimated class and y the true class of the stimuy ˆ lus. [sent-52, score-0.212]
</p><p>36 The discriminability of both stimuli can then be computed as: d = Z(P+ ) + Z(P− ) where Z = Φ−1 , and Φ is the cumulative normal distribution with zero mean and unit variance. [sent-53, score-0.21]
</p><p>37 This value indicates that the classiﬁcation task is comparatively easy for the subjects, although without being trivial (no ceiling effect). [sent-57, score-0.06]
</p><p>38 We observe a strong male bias (a large number of females  classiﬁed as males but very few males classiﬁed as females) and express this bias as: η = Z 2 (P+ ) − Z 2 (P− ) = 3. [sent-58, score-0.28]
</p><p>39 2 show the correlations of (a) RT and classiﬁcation error, (b) classiﬁcation error and CR, and (c) RT and CR. [sent-62, score-0.04]
</p><p>40 8  0 no error  error  1  2 CR  3  1  2 CR  3  Figure 2: Human classiﬁcation behaviour: mutual dependencies of the subject’s responses. [sent-72, score-0.08]
</p><p>41 RT’s are longer for incorrect answers than for correct ones (a). [sent-73, score-0.044]
</p><p>42 Second, a high CR is correlated with a low classiﬁcation error (b) and thus subjects have veridical knowledge about the difﬁculty of individual responses—this is certainly not the case in many low-level psychophysical settings. [sent-74, score-0.428]
</p><p>43 It may thus be concluded that a high error (or equivalently a low CR) implies higher RT’s. [sent-78, score-0.101]
</p><p>44 This may suggest that patterns difﬁcult to classify need more computation, i. [sent-79, score-0.068]
</p><p>45 longer processing, by the brain than patterns easy to classify. [sent-81, score-0.116]
</p><p>46 3  Machine Learning Classiﬁers  In the following, various hyperplane classiﬁcation algorithms are expressed as weighted dual space learners with different learning rules. [sent-82, score-0.188]
</p><p>47 Given a dataset {xi , yi }p , we assume i=1 classiﬁcation is done in the input space, i. [sent-83, score-0.076]
</p><p>48 The hyperplanes can be written using a weight (or normal) vector w and an offset b in order to yield a classiﬁcation rule as y(x) = sign( w|x + b) in the ﬁrst three cases whereas in the last one, the decision rule is a collection of hyperplanes. [sent-87, score-0.168]
</p><p>49 These classiﬁers are compared on a two-dimensional toy dataset in Fig. [sent-88, score-0.033]
</p><p>50 The weight vector is given as: w = i αi yi x i where α is obtained by maximising i αi − 1 ij yi yj αi αj xi |xj subject to i αi yi = 2 0 and 0 ≤ αi ≤ C where C is a regularisation parameter, determined using for instance cross-validation. [sent-91, score-0.213]
</p><p>51 The offset is computed as: b = yi − w|xi i|0<αi  < 5 · 10 −4 which allows us to reject the null hypothesis with a high degree of conﬁdence. [sent-92, score-0.073]
</p><p>52 02  1  50 100 150 200 |δ| to SH  1  1  50 100 150 200 |δ| to SH  1  1  50 100 150 200 |δ| to SH  Figure 5: Scatter plots relating the subjects’ responses (classiﬁcation error, RT and CR) to the distance |δ| to the SH for each face in the database, the pooling being done across subjects. [sent-117, score-0.24]
</p><p>53 From these results it can be seen that RVMs correlate best all the subject’s responses with the distances of the stimuli to the SH. [sent-118, score-0.272]
</p><p>54 The RT seems to be the performance measure where most correlation between man and machine can be asserted although all performance measures are related as shown in sec. [sent-119, score-0.145]
</p><p>55 The prototype algorithm again behaves in the least human-like manner of the four classiﬁers. [sent-121, score-0.122]
</p><p>56 The correlation between the classiﬁcation behaviour of man and machine indicates for RVMs, and to some extent SVMs, that heads far from the SH are more easily processed by humans. [sent-122, score-0.249]
</p><p>57 It may be concluded that the brain needs to do more processing (higher RT) to classify stimuli close to the decision hyperplane, while stimuli far from it are classiﬁed more accurately (low error) and with higher conﬁdence (high CR). [sent-123, score-0.536]
</p><p>58 Human classiﬁcation behaviour can thus be modeled by hyperplane algorithms; a piecewise linear decision function as found in Kmean seems however to be not biologically-plausible. [sent-124, score-0.222]
</p><p>59 5  Conclusions  Our study compared classiﬁcation of faces by man and machine. [sent-125, score-0.31]
</p><p>60 Psychophysically we noted that a high classiﬁcation error and a low CR for humans is accompanied by a longer processing of information by the brain (a longer RT). [sent-126, score-0.328]
</p><p>61 First, SVMs and RVMs can learn to classify faces  using the subjects’ labels but perform much better when using the true labels. [sent-129, score-0.353]
</p><p>62 Second, correlating the average response of humans (classiﬁcation error, RT or CR) with the distance to the SH on a face-by-face basis using Spearman’s rank correlation coefﬁcients shows that RVMs recreate human performance most closely in every respect. [sent-130, score-0.303]
</p><p>63 Third, the mean-of-class prototype, its popularity in neuroscience notwithstanding, is the least human-like classiﬁer in all cases examined. [sent-131, score-0.051]
</p><p>64 Third, when rejecting the prototype learner as a plausible candidate for human classiﬁcation we assume the representativeness of our face space: we assume that the mean face of our human subjects’ is close to the sample mean of our database. [sent-133, score-0.688]
</p><p>65 Clearly, a larger face database would be welcome, but is not trivial as we need texture maps and the corresponding shapes. [sent-134, score-0.328]
</p><p>66 Machines were trained on the dataset proper, whereas humans were assumed to have extracted the relevant information during their lifetime, and they were tested on faces with some cues removed. [sent-136, score-0.36]
</p><p>67 However, the representation we used does allow the genders to be separated well, as shown by the SVM classiﬁcation performance on the true labels. [sent-137, score-0.057]
</p><p>68 As a ﬁrst attempt to extend the neuroscience community’s toolbox with machine learning methods we believe to have shown the fruitfulness of this approach. [sent-138, score-0.083]
</p><p>69 Acknowledgements The authors would like to thank Volker Blanz for providing the face database and the ﬂow¨ ﬁeld algorithms. [sent-139, score-0.24]
</p><p>70 In addition we are grateful to Gokhan Bakır, Heinrich B¨ lthoff, Jez Hill, u Carl Rasmussen, Gunnar R¨ tsch, Bernhard Sch¨ lkopf and Vladimir Vapnik for helpful a o comments and suggestions. [sent-140, score-0.026]
</p><p>71 Nonlinear Component Analysis as a Kero u nel Eigenvalue Problem. [sent-211, score-0.028]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sh', 0.444), ('subjects', 0.283), ('rt', 0.278), ('cr', 0.278), ('classi', 0.232), ('faces', 0.227), ('gender', 0.195), ('stimuli', 0.15), ('human', 0.147), ('kmean', 0.14), ('rvms', 0.14), ('face', 0.122), ('prototype', 0.122), ('hyperplane', 0.119), ('database', 0.118), ('cation', 0.106), ('prot', 0.105), ('hyperplanes', 0.103), ('humans', 0.1), ('responses', 0.092), ('graf', 0.091), ('rating', 0.091), ('man', 0.083), ('rvm', 0.083), ('females', 0.077), ('males', 0.077), ('reaction', 0.073), ('brain', 0.072), ('dence', 0.071), ('learners', 0.069), ('classify', 0.068), ('behaviour', 0.068), ('psychophysical', 0.066), ('concluded', 0.061), ('texture', 0.056), ('blanz', 0.055), ('svm', 0.055), ('subject', 0.054), ('neuroscience', 0.051), ('eigenvalue', 0.051), ('male', 0.049), ('longer', 0.044), ('yi', 0.043), ('cognitive', 0.042), ('con', 0.042), ('asked', 0.041), ('error', 0.04), ('frontal', 0.039), ('correlated', 0.039), ('machines', 0.037), ('processed', 0.036), ('decision', 0.035), ('views', 0.035), ('relevance', 0.034), ('dataset', 0.033), ('trivial', 0.032), ('machine', 0.032), ('normal', 0.032), ('presentation', 0.031), ('labels', 0.031), ('estimated', 0.031), ('tor', 0.03), ('genders', 0.03), ('angeles', 0.03), ('lncs', 0.03), ('lthoff', 0.03), ('mpi', 0.03), ('correlate', 0.03), ('kero', 0.03), ('plateau', 0.03), ('psychophysically', 0.03), ('regularisation', 0.03), ('resonance', 0.03), ('turk', 0.03), ('offset', 0.03), ('svms', 0.03), ('correlation', 0.03), ('visual', 0.029), ('ers', 0.028), ('ceiling', 0.028), ('accompanied', 0.028), ('raised', 0.028), ('ended', 0.028), ('ag', 0.028), ('felix', 0.028), ('discriminability', 0.028), ('eigenfaces', 0.028), ('spemannstra', 0.028), ('bak', 0.028), ('nel', 0.028), ('rejecting', 0.028), ('technological', 0.028), ('tremendous', 0.028), ('true', 0.027), ('smola', 0.027), ('pca', 0.026), ('lkopf', 0.026), ('pooling', 0.026), ('correlating', 0.026), ('centred', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="95-tfidf-1" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>Author: Felix A. Wichmann, Arnulf B. Graf</p><p>Abstract: We attempt to understand visual classiﬁcation in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classiﬁcation task. Human subjects classiﬁed the faces and their gender judgment, reaction time and conﬁdence rating were recorded. Several hyperplane learning algorithms were used on the same classiﬁcation task using the Principal Components of the texture and shape representation of the faces. The classiﬁcation performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classiﬁcation can be modeled by some hyperplane algorithms in the feature space we used. For classiﬁcation, the brain needs more processing for stimuli close to that hyperplane than for those further away. 1</p><p>2 0.17053135 <a title="95-tfidf-2" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>Author: Xuerui Wang, Rebecca Hutchinson, Tom M. Mitchell</p><p>Abstract: We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classiﬁers constitute “virtual sensors” of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classiﬁers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classiﬁers that can be applied across multiple human subjects, including subjects who were not involved in training the classiﬁer. We describe the design of several machine learning approaches to training multiple-subject classiﬁers, and report experimental results demonstrating the success of these methods in learning cross-subject classiﬁers for two different fMRI data sets. 1</p><p>3 0.15547188 <a title="95-tfidf-3" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>Author: G.C. Littlewort, M.S. Bartlett, I.R. Fasel, J. Chenu, T. Kanda, H. Ishiguro, J.R. Movellan</p><p>Abstract: Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a time scale of less than a second. In this paper we present progress on a perceptual primitive to automatically detect frontal faces in the video stream and code them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face ﬁnder employs a cascade of feature detectors trained with boosting techniques [13, 2]. The expression recognizer employs a novel combination of Adaboost and SVM’s. The generalization performance to new subjects for a 7-way forced choice was 93.3% and 97% correct on two publicly available datasets. The outputs of the classiﬁer change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system was deployed and evaluated for measuring spontaneous facial expressions in the ﬁeld in an application for automatic assessment of human-robot interaction.</p><p>4 0.14169259 <a title="95-tfidf-4" href="./nips-2003-One_Microphone_Blind_Dereverberation_Based_on_Quasi-periodicity_of_Speech_Signals.html">144 nips-2003-One Microphone Blind Dereverberation Based on Quasi-periodicity of Speech Signals</a></p>
<p>Author: Tomohiro Nakatani, Masato Miyoshi, Keisuke Kinoshita</p><p>Abstract: Speech dereverberation is desirable with a view to achieving, for example, robust speech recognition in the real world. However, it is still a challenging problem, especially when using a single microphone. Although blind equalization techniques have been exploited, they cannot deal with speech signals appropriately because their assumptions are not satisﬁed by speech signals. We propose a new dereverberation principle based on an inherent property of speech signals, namely quasi-periodicity. The present methods learn the dereverberation ﬁlter from a lot of speech data with no prior knowledge of the data, and can achieve high quality speech dereverberation especially when the reverberation time is long. 1</p><p>5 0.12423442 <a title="95-tfidf-5" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>Author: Konrad P. Körding, Daniel M. Wolpert</p><p>Abstract: When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.</p><p>6 0.11479385 <a title="95-tfidf-6" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>7 0.099001445 <a title="95-tfidf-7" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>8 0.096675463 <a title="95-tfidf-8" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>9 0.089818329 <a title="95-tfidf-9" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>10 0.089058116 <a title="95-tfidf-10" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>11 0.08549393 <a title="95-tfidf-11" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>12 0.084425285 <a title="95-tfidf-12" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>13 0.081582077 <a title="95-tfidf-13" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>14 0.079687521 <a title="95-tfidf-14" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>15 0.079183593 <a title="95-tfidf-15" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>16 0.072222702 <a title="95-tfidf-16" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>17 0.070448138 <a title="95-tfidf-17" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>18 0.070094042 <a title="95-tfidf-18" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>19 0.067452848 <a title="95-tfidf-19" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>20 0.067351274 <a title="95-tfidf-20" href="./nips-2003-Locality_Preserving_Projections.html">120 nips-2003-Locality Preserving Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.198), (1, -0.043), (2, 0.076), (3, -0.229), (4, -0.082), (5, -0.071), (6, 0.064), (7, -0.032), (8, -0.028), (9, 0.095), (10, 0.093), (11, 0.0), (12, 0.103), (13, -0.077), (14, 0.065), (15, 0.117), (16, 0.049), (17, 0.062), (18, 0.081), (19, -0.173), (20, 0.168), (21, 0.114), (22, 0.073), (23, -0.218), (24, 0.018), (25, 0.166), (26, 0.059), (27, -0.145), (28, 0.121), (29, 0.019), (30, -0.042), (31, 0.034), (32, -0.039), (33, 0.031), (34, 0.076), (35, -0.066), (36, 0.113), (37, 0.019), (38, -0.085), (39, 0.096), (40, 0.014), (41, 0.134), (42, -0.086), (43, -0.08), (44, -0.022), (45, 0.099), (46, 0.104), (47, -0.01), (48, 0.028), (49, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94960386 <a title="95-lsi-1" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>Author: Felix A. Wichmann, Arnulf B. Graf</p><p>Abstract: We attempt to understand visual classiﬁcation in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classiﬁcation task. Human subjects classiﬁed the faces and their gender judgment, reaction time and conﬁdence rating were recorded. Several hyperplane learning algorithms were used on the same classiﬁcation task using the Principal Components of the texture and shape representation of the faces. The classiﬁcation performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classiﬁcation can be modeled by some hyperplane algorithms in the feature space we used. For classiﬁcation, the brain needs more processing for stimuli close to that hyperplane than for those further away. 1</p><p>2 0.73777699 <a title="95-lsi-2" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>Author: Xuerui Wang, Rebecca Hutchinson, Tom M. Mitchell</p><p>Abstract: We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classiﬁers constitute “virtual sensors” of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classiﬁers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classiﬁers that can be applied across multiple human subjects, including subjects who were not involved in training the classiﬁer. We describe the design of several machine learning approaches to training multiple-subject classiﬁers, and report experimental results demonstrating the success of these methods in learning cross-subject classiﬁers for two different fMRI data sets. 1</p><p>3 0.60046244 <a title="95-lsi-3" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>Author: Guido Dornhege, Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Brain-Computer Interfaces (BCI) are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intentions into a control signal for devices like computers or neuroprostheses. If this can be done bypassing the usual human output pathways like peripheral nerves and muscles it can ultimately become a valuable tool for paralyzed patients. Most activity in BCI research is devoted to ﬁnding suitable features and algorithms to increase information transfer rates (ITRs). The present paper studies the implications of using more classes, e.g., left vs. right hand vs. foot, for operating a BCI. We contribute by (1) a theoretical study showing under some mild assumptions that it is practically not useful to employ more than three or four classes, (2) two extensions of the common spatial pattern (CSP) algorithm, one interestingly based on simultaneous diagonalization, and (3) controlled EEG experiments that underline our theoretical ﬁndings and show excellent improved ITRs. 1</p><p>4 0.57504499 <a title="95-lsi-4" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>Author: G.C. Littlewort, M.S. Bartlett, I.R. Fasel, J. Chenu, T. Kanda, H. Ishiguro, J.R. Movellan</p><p>Abstract: Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a time scale of less than a second. In this paper we present progress on a perceptual primitive to automatically detect frontal faces in the video stream and code them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face ﬁnder employs a cascade of feature detectors trained with boosting techniques [13, 2]. The expression recognizer employs a novel combination of Adaboost and SVM’s. The generalization performance to new subjects for a 7-way forced choice was 93.3% and 97% correct on two publicly available datasets. The outputs of the classiﬁer change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system was deployed and evaluated for measuring spontaneous facial expressions in the ﬁeld in an application for automatic assessment of human-robot interaction.</p><p>5 0.54187846 <a title="95-lsi-5" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>Author: Matthew R. Rudary, Satinder P. Singh</p><p>Abstract: Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems. One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models. Empirical work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs. We introduce a new notion of tests which allows us to deﬁne a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems. These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation—in particular, its potential to be exponentially larger than the equivalent POMDP. 1</p><p>6 0.49608776 <a title="95-lsi-6" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>7 0.47315082 <a title="95-lsi-7" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>8 0.45745072 <a title="95-lsi-8" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>9 0.45298451 <a title="95-lsi-9" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>10 0.45156717 <a title="95-lsi-10" href="./nips-2003-One_Microphone_Blind_Dereverberation_Based_on_Quasi-periodicity_of_Speech_Signals.html">144 nips-2003-One Microphone Blind Dereverberation Based on Quasi-periodicity of Speech Signals</a></p>
<p>11 0.42504489 <a title="95-lsi-11" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>12 0.38770705 <a title="95-lsi-12" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>13 0.38464677 <a title="95-lsi-13" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>14 0.3676984 <a title="95-lsi-14" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>15 0.35772377 <a title="95-lsi-15" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>16 0.32964784 <a title="95-lsi-16" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<p>17 0.32804352 <a title="95-lsi-17" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>18 0.3251003 <a title="95-lsi-18" href="./nips-2003-Link_Prediction_in_Relational_Data.html">118 nips-2003-Link Prediction in Relational Data</a></p>
<p>19 0.3219485 <a title="95-lsi-19" href="./nips-2003-Impact_of_an_Energy_Normalization_Transform_on_the_Performance_of_the_LF-ASD_Brain_Computer_Interface.html">89 nips-2003-Impact of an Energy Normalization Transform on the Performance of the LF-ASD Brain Computer Interface</a></p>
<p>20 0.31546515 <a title="95-lsi-20" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.021), (5, 0.01), (11, 0.024), (35, 0.015), (53, 0.068), (71, 0.034), (76, 0.033), (85, 0.638), (91, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98377055 <a title="95-lda-1" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>Author: Felix A. Wichmann, Arnulf B. Graf</p><p>Abstract: We attempt to understand visual classiﬁcation in humans using both psychophysical and machine learning techniques. Frontal views of human faces were used for a gender classiﬁcation task. Human subjects classiﬁed the faces and their gender judgment, reaction time and conﬁdence rating were recorded. Several hyperplane learning algorithms were used on the same classiﬁcation task using the Principal Components of the texture and shape representation of the faces. The classiﬁcation performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels, and also with the gender estimated by the subjects. We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms. Our results suggest that human classiﬁcation can be modeled by some hyperplane algorithms in the feature space we used. For classiﬁcation, the brain needs more processing for stimuli close to that hyperplane than for those further away. 1</p><p>2 0.96968997 <a title="95-lda-2" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean ﬁeld approximations.</p><p>3 0.96390671 <a title="95-lda-3" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><p>4 0.92052144 <a title="95-lda-4" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>Author: Maxim Likhachev, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: In real world planning problems, time for deliberation is often limited. Anytime planners are well suited for these problems: they ﬁnd a feasible solution quickly and then continually work on improving it until time runs out. In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time. It starts by ﬁnding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows. Given enough time it ﬁnds a provably optimal solution. While improving its bound, ARA* reuses previous search efforts and, as a result, is signiﬁcantly more efﬁcient than other anytime search methods. In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover. 1</p><p>5 0.87885606 <a title="95-lda-5" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><p>6 0.82410991 <a title="95-lda-6" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>7 0.77305812 <a title="95-lda-7" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>8 0.72682005 <a title="95-lda-8" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>9 0.68621957 <a title="95-lda-9" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>10 0.68113631 <a title="95-lda-10" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>11 0.64662576 <a title="95-lda-11" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>12 0.620906 <a title="95-lda-12" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>13 0.61273038 <a title="95-lda-13" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>14 0.60673332 <a title="95-lda-14" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>15 0.59258461 <a title="95-lda-15" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<p>16 0.58750838 <a title="95-lda-16" href="./nips-2003-Online_Passive-Aggressive_Algorithms.html">148 nips-2003-Online Passive-Aggressive Algorithms</a></p>
<p>17 0.58503181 <a title="95-lda-17" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>18 0.58469146 <a title="95-lda-18" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>19 0.57923716 <a title="95-lda-19" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>20 0.57178265 <a title="95-lda-20" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
