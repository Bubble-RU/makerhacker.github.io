<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-91" href="#">nips2003-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</h1>
<br/><p>Source: <a title="nips-2003-91-pdf" href="http://papers.nips.cc/paper/2391-inferring-state-sequences-for-non-linear-systems-with-embedded-hidden-markov-models.pdf">pdf</a></p><p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>Reference: <a title="nips-2003-91-reference" href="../nips2003_reference/nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. [sent-6, score-0.661]
</p><p>2 This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). [sent-7, score-0.526]
</p><p>3 An update begins with the creation of “pools” of candidate states at each time. [sent-8, score-0.221]
</p><p>4 We then deﬁne an embedded HMM whose states are indexes within these pools. [sent-9, score-0.414]
</p><p>5 Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. [sent-10, score-0.68]
</p><p>6 We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. [sent-11, score-0.508]
</p><p>7 We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. [sent-12, score-0.455]
</p><p>8 1  Introduction  Consider a dynamical model in which a sequence of hidden states, x = (x0 , . [sent-13, score-0.175]
</p><p>9 , yn−1 ), with each yt being generated from the corresponding xt according to some stochastic observation process. [sent-20, score-0.573]
</p><p>10 Both the xt and the yt could be multidimensional. [sent-21, score-0.529]
</p><p>11 We wish to randomly sample hidden state sequences from the conditional distribution for the state sequence given the observations, which we can then use to make Monte Carlo inferences about this posterior distribution for the state sequence. [sent-22, score-0.825]
</p><p>12 If the state space is ﬁnite, of size K, so that this is a Hidden Markov Model (HMM), a hidden state sequence can be sampled by a forward-backwards dynamic programming algorithm in time proportional to nK 2 (see [5] for a review of this and related algorithms). [sent-24, score-0.588]
</p><p>13 If the state space is p and the dynamics and observation process are linear, with Gaussian noise, an analogous adaptation of the Kalman ﬁlter can be used. [sent-25, score-0.26]
</p><p>14 For more general models,  or for ﬁnite state space models in which K is large, one might use Markov chain sampling (see [3] for a review). [sent-26, score-0.38]
</p><p>15 For instance, one could perform Gibbs sampling or Metropolis updates for each xt in turn. [sent-27, score-0.567]
</p><p>16 Such simple Markov chain updates may be very slow to converge, however, if the states at nearby times are highly dependent. [sent-28, score-0.329]
</p><p>17 A popular recent approach is to use a particle smoother, such as the one described by Doucet, Godsill, and West [2], but this approach can fail when the set of particles doesn’t adequately cover the space, or when particles are eliminated prematurely. [sent-29, score-0.153]
</p><p>18 We also show how it can be used to in effect discretize the state space without producing any discretization error. [sent-32, score-0.295]
</p><p>19 Finally, we demonstrate the embedded HMM on a problem of tracking human motion in 3D based on the 2D projections of marker positions, and compare it with a particle smoother. [sent-33, score-0.432]
</p><p>20 2  The Embedded HMM Algorithm  In our description of the algorithm, model probabilities will be denoted by P , which will denote probabilities or probability densities without distinction, as appropriate for the state space, X , and observation space, Y. [sent-34, score-0.302]
</p><p>21 The model’s initial state distribution is given by P (x0 ), transition probabilities are given by P (xt | xt−1 ), and observation probabilities are given by P (yt | xt ). [sent-35, score-0.851]
</p><p>22 To accomplish this, we will simulate a Markov chain whose state space is X n — i. [sent-46, score-0.349]
</p><p>23 , a state of this chain is an entire sequence of hidden states. [sent-48, score-0.461]
</p><p>24 We will arrange for the equilibrium distribution of this Markov chain to be π(x0 , . [sent-49, score-0.192]
</p><p>25 , xn−1 ), so that simulating the chain for a suitably long time will produce a state sequence from the desired distribution. [sent-52, score-0.445]
</p><p>26 The (i) (i) state at iteration i of this chain will be written as x(i) = (x0 , . [sent-53, score-0.306]
</p><p>27 The transition probabilities for this Markov chain will be denoted using Q. [sent-57, score-0.241]
</p><p>28 In particular, we will use some initial distribution for the state of the chain, Q(x(0) ), and will simulate the chain according to the transition probabilities Q(x(i) | x(i−1) ). [sent-58, score-0.46]
</p><p>29 ) This is implied by the detailed balance condition: π(x)Q(x | x)  = π(x )Q(x | x ), for all x and x in X n  (2)  The transition Q(x(i) | x(i−1) ) is deﬁned in terms of “pools” of states for each time. [sent-60, score-0.226]
</p><p>30 The current state at time t is always part of the pool for time t. [sent-61, score-0.556]
</p><p>31 Other states in the pool are produced using a pool distribution, ρt , which is designed so that points drawn from ρt are plausible alternatives to the current state at time t. [sent-62, score-1.039]
</p><p>32 The simplest way to generate these additional pool states is to draw points independently from ρt . [sent-63, score-0.472]
</p><p>33 This may not be feasible, however, or may not be desirable, in which case we can instead simulate an “inner” Markov chain deﬁned by transition probabilities written as Rt (· | ·), which leave the pool distribution, ρt , invariant. [sent-64, score-0.615]
</p><p>34 To generate pool states by drawing from ρt independently, we can let Rt (x |x) = ˜ Rt (x |x) = ρt (x ). [sent-66, score-0.472]
</p><p>35 To perform a transition Q to a new state sequence, we begin by at each time, t, producing (i−1) a pool of K states, Ct . [sent-68, score-0.579]
</p><p>36 One of the states in Ct is the current state, xt ; the others are ˜ produced using Rt and Rt . [sent-69, score-0.658]
</p><p>37 The new state sequence, x(i) , is then randomly selected from among all sequences whose states at each time t are in Ct , using a form of the forwardbackward procedure. [sent-70, score-0.41]
</p><p>38 In detail, the pool of candidate states for time t is found as follows: 1) Pick an integer Jt uniformly from {0, . [sent-71, score-0.542]
</p><p>39 ) [j]  3) For j from 1 to Jt , randomly pick xt according to the transition probabilities [j] [j−1] Rt (xt | xt ). [sent-77, score-1.107]
</p><p>40 [j]  4) For j from −1 down to −K + Jt + 1, randomly pick xt according to the reversed ˜ [j] [j+1] ). [sent-78, score-0.54]
</p><p>41 transition probabilities, Rt (xt | xt [j]  5) Let Ct be the pool consisting of xt , for j ∈ {−K+Jt+1, . [sent-79, score-1.315]
</p><p>42 If some [j] of the xt are the same, they will be present in the pool more than once. [sent-86, score-0.793]
</p><p>43 Once the pools of candidate states have been found, a new state sequence, x (i) , is picked from among all sequences, x, for which every xt is in Ct . [sent-87, score-1.039]
</p><p>44 If duplicate states occur in some of the pools, they are treated as if they were distinct when picking a sequence in this way. [sent-89, score-0.303]
</p><p>45 In effect, we pick indexes of states in these pools, with probabilities as above, rather than states themselves. [sent-90, score-0.419]
</p><p>46 Crucially, using the forward-backward technique, it is possible to randomly pick a new state sequence from this distribution in time growing linearly with n, even though the number of possible sequences [j] grows as K n . [sent-92, score-0.446]
</p><p>47 After the above procedure has been used to produce the pool states, xt for t = 0 to n−1 and j = −K +Jt + 1 to Jt , this algorithm operates as follows (see [5]): [j]  [j]  1) For t = 0 to n−1 and for j = −K +Jt +1 to Jt , let ut,j = P (yt | xt )/ρt (xt ). [sent-93, score-1.257]
</p><p>48 Finally, the embedded HMM transition is completed by letting the new state sequence, x (i) , [sn−1 ] [s ] [s ] be equal to (x0 0 , x1 1 , . [sent-104, score-0.463]
</p><p>49 , xn−1 )  3  Proof of Correctness  To show that a Markov chain with these transitions will converge to π, we need to show that it leaves π invariant, and that the chain is ergodic. [sent-107, score-0.344]
</p><p>50 However, it is easy to see that the chain will be ergodic if all possible state sequences have non-zero probability density under π, the pool distributions, ρt , have non-zero density everywhere, and the transitions Rt are ergodic. [sent-109, score-0.787]
</p><p>51 4  A simple demonstration  The following simple example illustrates the operation of the embedded HMM. [sent-117, score-0.237]
</p><p>52 The state space X and the observation space, Y, are both , and each observation is simply the state plus Gaussian noise of standard deviation σ — i. [sent-118, score-0.443]
</p><p>53 The state transitions are deﬁned by P (xt | xt−1 ) = N (xt | tanh(ηxt−1 ), τ 2 ), for some constant expansion factor η and transition noise standard deviation τ . [sent-121, score-0.294]
</p><p>54 The state sequence stays in the vicinity of +1 or −1 for long periods, with rare switches between these regions. [sent-132, score-0.328]
</p><p>55 We sampled from this distribution over state sequences using an embedded HMM in which the pool distributions, ρt , were normal with mean zero and standard deviation one, and the pool transitions simply sampled independently from this distribution (ignoring the current pool state). [sent-134, score-1.635]
</p><p>56 Figure 2 shows that after only two updates using pools of ten states, embedded HMM sampling produces a state sequence with roughly the correct characteristics. [sent-135, score-0.79]
</p><p>57 Figure 3 demonstrates how a single embedded HMM update can make a large change to the state sequence. [sent-136, score-0.432]
</p><p>58 It shows a portion of the state sequence after 99 updates, the pools of states produced for the next update, and the state sequence found by the embedded HMM using these pools. [sent-137, score-1.126]
</p><p>59 A large change is made to the state sequence in the region from time 840 to 870, with states in this region switching from the vicinity of −1 to the vicinity of +1. [sent-138, score-0.49]
</p><p>60 This example is explored in more detail in [4], where it is shown that the embedded HMM is superior to simple Metropolis methods that update one hidden state at a time. [sent-139, score-0.489]
</p><p>61 5  Discretization without discretization error  A simple way to handle a model with a continuous state space is to discretize the space by laying down a regular grid, after transforming to make the space bounded if necessary. [sent-140, score-0.337]
</p><p>62 An HMM with grid points as states can then be built that approximates the original model. [sent-141, score-0.227]
</p><p>63 Inference using this HMM is only approximate, however, due to the discretization error involved in replacing the continuous space by a grid of points. [sent-142, score-0.156]
</p><p>64 The embedded HMM can use a similar grid as a deterministic method of creating pools of states, aligning the grid so that the current state lies on a grid point. [sent-143, score-0.889]
</p><p>65 This is a special case of the general procedure for creating pools, in which ρt is uniform, Rt moves to the next grid ˜ point and Rt moves to the previous grid point, with both wrapping around when the ﬁrst or last grid point is reached. [sent-144, score-0.279]
</p><p>66 If the number of pool states is set equal to the number of points in a grid, every pool will consist of a complete grid aligned to include the current state. [sent-145, score-0.926]
</p><p>67 On their own, such embedded HMM updates will never change the alignments of the grids. [sent-146, score-0.285]
</p><p>68 However, we can alternately apply such an embedded HMM update and some other MCMC update (eg, Metropolis) which is capable of making small changes to the state. [sent-147, score-0.291]
</p><p>69 We have tried this method on the example described above, laying the grid over the transformed state tanh(xt ), with suitably transformed transition densities. [sent-151, score-0.36]
</p><p>70 With K = 10, the grid method samples more efﬁciently than when using N (0, 1) pool distributions, as above. [sent-152, score-0.413]
</p><p>71 5 0 −5  0  200  400  600  800  1000  −5  0  5  Figure 1: A state sequence (black dots) and observation sequence (gray dots) of length 1000 produced by the model with σ = 2. [sent-153, score-0.438]
</p><p>72 0  200  400  600  800  1000  −6  −4  −2  0  2  4  6  Figure 2: The state sequence (black dots) produced after two embedded HMM updates, starting with the states set equal to the data points (gray dots), as in the ﬁgure above. [sent-157, score-0.676]
</p><p>73 820  840  860  880  900  920  940  Figure 3: Closeup of an embedded HMM update. [sent-158, score-0.237]
</p><p>74 The true state sequence is shown by black dots and the observation sequence by gray dots. [sent-159, score-0.492]
</p><p>75 The current state sequence is shown by the dark line. [sent-160, score-0.287]
</p><p>76 The pools of ten states at each time used for the update are shown as small dots, and the new state sequence picked by the embedded HMM by the light line. [sent-161, score-0.905]
</p><p>77 Figure 4: The four-second motion sequence used for the experiment, shown in three snapshots with streamers showing earlier motion. [sent-162, score-0.149]
</p><p>78 The left plot shows frames 1-59, the middle plot frames 5991, and the right plot frames 91-121. [sent-163, score-0.162]
</p><p>79 )  6  Tracking human motion  We have applied the embedded HMM to the more challenging problem of tracking 3D human motion from 2D observations of markers attached to certain body points. [sent-167, score-0.516]
</p><p>80 1 Our goal was to recover the 3D motion of the six markers, by using the embedded HMM to generate samples from the posterior distribution over 3D positions at each time (the hidden states of the model), given the 2D observations. [sent-172, score-0.584]
</p><p>81 The energy function we used contains terms pertaining to the pairwise distances between the six markers and to the heights of the markers above the plane of the ﬂoor, as well as a term that penalizes bending the torso far backwards while the legs are vertical. [sent-177, score-0.295]
</p><p>82 The embedded HMM was initialized by setting the state at all times to a single frame of the subject in a typical stance, taken from a different trial. [sent-179, score-0.443]
</p><p>83 As the pool distribution at time t, we used the posterior distribution when using the Boltzmann distribution for the energy as the prior and the single observation at time t. [sent-180, score-0.554]
</p><p>84 The pool transitions used were Langevin updates with respect to this pool distribution. [sent-181, score-0.774]
</p><p>85 For comparison, we also tried solving this problem with the particle smoother of [2], in which a particle ﬁlter is applied to the data in time order, after which a state sequence is selected at random in a backwards pass. [sent-182, score-0.543]
</p><p>86 The initial particle set was created by drawing frames randomly from sequences other than the sequence being tested, and translating the markers in each frame so that their centre of mass was at the same point as the centre of mass in the test sequence. [sent-184, score-0.483]
</p><p>87 We ran the embedded HMM using ﬁve pool states for 300 iterations, taking 1. [sent-188, score-0.709]
</p><p>88 We chose markers 167, 72, 62, 63, 31, 38, downsampled to 30 frames per second. [sent-194, score-0.176]
</p><p>89 than those produced by the particle smoother, and were quantitatively better with respect to likelihood and dynamical transition probabilities. [sent-197, score-0.197]
</p><p>90 The embedded HMM updates appeared to be sampling from the correct posterior distribution, but moving rather slowly among those trajectories that are plausible given the observations. [sent-199, score-0.421]
</p><p>91 7  Conclusions  We have shown that the embedded HMM can work very well for a non-linear model with a low-dimensional state. [sent-200, score-0.237]
</p><p>92 For the higher-dimensional motion tracking example, the embedded HMM has some difﬁculties exploring the full posterior distribution, due, we think, to the difﬁculty of creating pool distributions with a dense enough sampling of states to allow linking of new states at adjacent times. [sent-201, score-1.053]
</p><p>93 The embedded HMM therefore appears to be a promising alternative to particle smoothers in such contexts. [sent-203, score-0.326]
</p><p>94 The idea behind the embedded HMM should also be applicable to more general treestructured graphical models. [sent-204, score-0.237]
</p><p>95 A pool of values would be created for each variable in the tree (which would include the current value for the variable). [sent-205, score-0.35]
</p><p>96 The fast sampling algorithm possible for such an “embedded tree” (a generalization of the sampling algorithm used for the embedded HMM) would then be used to sample a new set of values for all variables, choosing from all combinations of values from the pools. [sent-206, score-0.347]
</p><p>97 Finally, while much of the elaboration in this paper is designed to create a Markov chain whose equilibrium distribution is exactly the correct posterior, π(x), the embedded HMM idea can be also used as a simple search technique, to ﬁnd a state sequence, x, which maximizes π(x). [sent-207, score-0.597]
</p><p>98 If the current state at each time is always included in the pool, this Viterbi procedure will always either ﬁnd a new x that increases π(x), or return the current x again. [sent-209, score-0.229]
</p><p>99 This embedded HMM optimizer has been successfully used to infer segment boundaries in a segmental model for voicing detection and pitch tracking in speech signals [1], as well as in other applications such as robot localization from sensor logs. [sent-210, score-0.326]
</p><p>100 (2003) “Markov chain sampling for non-linear state space models using embedded hidden Markov models”, Technical Report No. [sent-236, score-0.674]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xt', 0.464), ('jt', 0.397), ('hmm', 0.366), ('pool', 0.329), ('embedded', 0.237), ('rt', 0.225), ('pools', 0.184), ('state', 0.168), ('states', 0.143), ('chain', 0.138), ('markers', 0.103), ('sequence', 0.098), ('particle', 0.089), ('grid', 0.084), ('ct', 0.069), ('transitions', 0.068), ('yt', 0.065), ('langevin', 0.065), ('picking', 0.062), ('smoother', 0.059), ('transition', 0.058), ('sequences', 0.058), ('hidden', 0.057), ('markov', 0.056), ('sampling', 0.055), ('pick', 0.054), ('frames', 0.054), ('discretization', 0.053), ('candidate', 0.051), ('motion', 0.051), ('updates', 0.048), ('probabilities', 0.045), ('observation', 0.044), ('dots', 0.044), ('metropolis', 0.043), ('xn', 0.039), ('toronto', 0.039), ('proportional', 0.038), ('tracking', 0.036), ('indexes', 0.034), ('feet', 0.033), ('segmental', 0.033), ('posterior', 0.032), ('particles', 0.032), ('trajectories', 0.032), ('vicinity', 0.031), ('switches', 0.031), ('discretize', 0.031), ('energy', 0.03), ('produced', 0.03), ('neal', 0.03), ('dynamics', 0.029), ('picked', 0.029), ('laying', 0.028), ('jn', 0.028), ('godsill', 0.028), ('update', 0.027), ('creating', 0.027), ('equilibrium', 0.027), ('distribution', 0.027), ('ergodic', 0.026), ('ontario', 0.026), ('monte', 0.025), ('balance', 0.025), ('producing', 0.024), ('simulate', 0.024), ('randomly', 0.022), ('yn', 0.022), ('gray', 0.022), ('west', 0.022), ('suitably', 0.022), ('boltzmann', 0.022), ('doucet', 0.022), ('current', 0.021), ('frame', 0.021), ('leave', 0.021), ('tanh', 0.021), ('backwards', 0.021), ('unrealistic', 0.021), ('sampled', 0.021), ('dynamical', 0.02), ('speech', 0.02), ('aligned', 0.02), ('penalizes', 0.02), ('chose', 0.019), ('human', 0.019), ('time', 0.019), ('centre', 0.019), ('carlo', 0.019), ('space', 0.019), ('correctness', 0.018), ('ht', 0.018), ('six', 0.018), ('wt', 0.018), ('viewing', 0.018), ('black', 0.018), ('invariant', 0.018), ('roweis', 0.017), ('moving', 0.017), ('subject', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="91-tfidf-1" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>2 0.23456056 <a title="91-tfidf-2" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><p>3 0.17280203 <a title="91-tfidf-3" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>4 0.16074051 <a title="91-tfidf-4" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>5 0.14619647 <a title="91-tfidf-5" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>Author: Cynthia Rudin, Ingrid Daubechies, Robert E. Schapire</p><p>Abstract: In order to understand AdaBoost’s dynamics, especially its ability to maximize margins, we derive an associated simpliﬁed nonlinear iterated map and analyze its behavior in low-dimensional cases. We ﬁnd stable cycles for these cases, which can explicitly be used to solve for AdaBoost’s output. By considering AdaBoost as a dynamical system, we are able to prove R¨ tsch and Warmuth’s conjecture that AdaBoost may fail a to converge to a maximal-margin combined classiﬁer when given a ‘nonoptimal’ weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost∗ and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv. 1</p><p>6 0.14228311 <a title="91-tfidf-6" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>7 0.13658565 <a title="91-tfidf-7" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>8 0.13077235 <a title="91-tfidf-8" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>9 0.11983229 <a title="91-tfidf-9" href="./nips-2003-Online_Passive-Aggressive_Algorithms.html">148 nips-2003-Online Passive-Aggressive Algorithms</a></p>
<p>10 0.10712256 <a title="91-tfidf-10" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>11 0.089421839 <a title="91-tfidf-11" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>12 0.086698219 <a title="91-tfidf-12" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>13 0.081508994 <a title="91-tfidf-13" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>14 0.073776819 <a title="91-tfidf-14" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>15 0.073218696 <a title="91-tfidf-15" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>16 0.072636694 <a title="91-tfidf-16" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>17 0.072222702 <a title="91-tfidf-17" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>18 0.071911268 <a title="91-tfidf-18" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>19 0.071588002 <a title="91-tfidf-19" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>20 0.068615548 <a title="91-tfidf-20" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.189), (1, 0.149), (2, -0.037), (3, -0.133), (4, 0.07), (5, -0.036), (6, 0.215), (7, 0.281), (8, 0.191), (9, 0.003), (10, -0.046), (11, -0.105), (12, 0.077), (13, -0.012), (14, 0.048), (15, -0.039), (16, 0.058), (17, -0.092), (18, -0.02), (19, -0.124), (20, -0.09), (21, 0.137), (22, 0.001), (23, -0.12), (24, -0.075), (25, -0.165), (26, -0.151), (27, -0.014), (28, 0.061), (29, 0.038), (30, -0.083), (31, 0.096), (32, 0.132), (33, -0.006), (34, 0.024), (35, -0.041), (36, -0.11), (37, -0.064), (38, 0.037), (39, -0.041), (40, -0.071), (41, 0.038), (42, 0.09), (43, 0.032), (44, 0.093), (45, 0.046), (46, 0.02), (47, -0.073), (48, -0.041), (49, -0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9745155 <a title="91-lsi-1" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>2 0.60290366 <a title="91-lsi-2" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><p>3 0.57818359 <a title="91-lsi-3" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>4 0.5481801 <a title="91-lsi-4" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>5 0.51787293 <a title="91-lsi-5" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>Author: Nicholas P. Hughes, Lionel Tarassenko, Stephen J. Roberts</p><p>Abstract: We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling. 1</p><p>6 0.51581079 <a title="91-lsi-6" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>7 0.51072061 <a title="91-lsi-7" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>8 0.49597448 <a title="91-lsi-8" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>9 0.39931744 <a title="91-lsi-9" href="./nips-2003-Can_We_Learn_to_Beat_the_Best_Stock.html">44 nips-2003-Can We Learn to Beat the Best Stock</a></p>
<p>10 0.3807933 <a title="91-lsi-10" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>11 0.37464795 <a title="91-lsi-11" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>12 0.36549678 <a title="91-lsi-12" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>13 0.36473706 <a title="91-lsi-13" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>14 0.35342526 <a title="91-lsi-14" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>15 0.31635395 <a title="91-lsi-15" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>16 0.30403945 <a title="91-lsi-16" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>17 0.3006506 <a title="91-lsi-17" href="./nips-2003-Online_Passive-Aggressive_Algorithms.html">148 nips-2003-Online Passive-Aggressive Algorithms</a></p>
<p>18 0.29555991 <a title="91-lsi-18" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>19 0.26824635 <a title="91-lsi-19" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>20 0.26406655 <a title="91-lsi-20" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (11, 0.041), (29, 0.028), (30, 0.018), (33, 0.023), (35, 0.11), (53, 0.052), (69, 0.043), (71, 0.049), (76, 0.034), (80, 0.197), (85, 0.125), (91, 0.104), (99, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86042196 <a title="91-lda-1" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>2 0.69144762 <a title="91-lda-2" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>3 0.68413955 <a title="91-lda-3" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>4 0.68050313 <a title="91-lda-4" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>Author: Quaid D. Morris, Brendan J. Frey</p><p>Abstract: This paper addresses the problem of untangling hidden graphs from a set of noisy detections of undirected edges. We present a model of the generation of the observed graph that includes degree-based structure priors on the hidden graphs. Exact inference in the model is intractable; we present an eﬃcient approximate inference algorithm to compute edge appearance posteriors. We evaluate our model and algorithm on a biological graph inference problem. 1 Introduction and motivation The inference of hidden graphs from noisy edge appearance data is an important problem with obvious practical application. For example, biologists are currently building networks of all the physical protein-protein interactions (PPI) that occur in particular organisms. The importance of this enterprise is commensurate with its scale: a completed network would be as valuable as a completed genome sequence, and because each organism contains thousands of diﬀerent types of proteins, there are millions of possible types of interactions. However, scalable experimental methods for detecting interactions are noisy, generating many false detections. Motivated by this application, we formulate the general problem of inferring hidden graphs as probabilistic inference in a graphical model, and we introduce an eﬃcient algorithm that approximates the posterior probability that an edge is present. In our model, a set of hidden, constituent graphs are combined to generate the observed graph. Each hidden graph is independently sampled from a prior on graph structure. The combination mechanism acts independently on each edge but can be either stochastic or deterministic. Figure 1 shows an example of our generative model. Typically one of the hidden graphs represents the graph of interest (the true graph), the others represent diﬀerent types of observation noise. Independent edge noise may also be added by the combination mechanism. We use probabilistic inference to compute a likely decomposition of the observed graph into its constituent parts. This process is deemed “untangling”. We use the term “denoising” to refer to the special case where the edge noise is independent. In denoising there is a single hidden graph, the true graph, and all edge noise in the observed graph is due E1 1 eij E2 i 2 eij j xij i j i j X Figure 1: Illustrative generative model example. Figure shows an example where an observed graph, X, is a noisy composition of two constituent graphs, E 1 and E 2 . All graphs share the same vertex set, so each can be represented by a symmetric matrix of random binary variables (i.e., an adjacency matrix). This generative model is designed to solve a toy counter-espionage problem. The vertices represent suspects and each edge in X represents an observed call between two suspects. The graph X reﬂects zero or more spy rings (represented by E 1 ), telemarketing calls (represented by E 2 ), social calls (independent edge noise), and lost call records (more independent edge noise). The task is to locate any spy rings hidden in X. We model the distribution of spy ring graphs using a prior, P (E 1 ), that has support only on graphs where all vertices have degree of either 2 (i.e., are in the ring) or 0 (i.e., are not). Graphs of telemarketing call patterns are represented using a prior, P (E 2 ), under which all nodes have degrees of > 3 (i.e., are telemarketers), 1 (i.e., are telemarketees), or 0 (i.e., are neither). The displayed hidden graphs are one likely untangling of X. to the combination mechanism. Prior distributions over graphs can be speciﬁed in various ways, but our choice is motivated by problems we want to solve, and by a view to deriving an eﬃcient inference algorithm. One compact representation of a distribution over graphs consists of specifying a distribution over vertex degrees, and assuming that graphs that have the same vertex degrees are equiprobable. Such a prior can model quite rich distributions over graphs. These degree-based structure priors are natural representions of graph structure; many classes of real-world networks have a characteristic functional form associated with their degree distributions [1], and sometimes this form can be predicted using knowledge about the domain (see, e.g., [2]) or detected empirically (see, e.g., [3, 4]). As such, our model incorporates degree-based structure priors. Though exact inference in our model is intractable in general, we present an eﬃcient algorithm for approximate inference for arbitrary degree distributions. We evaluate our model and algorithm using the real-world example of untangling yeast proteinprotein interaction networks. 2 A model of noisy and tangled graphs For degree-based structure priors, inference consists of searching over vertex degrees and edge instantiations, while comparing each edge with its noisy observation and enforcing the constraint that the number of edges connected to every vertex must equal the degree of the vertex. Our formulation of the problem in this way is inspired by the success of the sum-product algorithm (loopy belief propagation) for solving similar formulations of problems in error-correcting decoding [6, 7], phase unwrapping [8], and random satisﬁability [9]. For example, in error-correcting decoding, inference consists of searching over conﬁgurations of codeword bits, while comparing each bit with its noisy observation and enforcing parity-check constraints on subsets of bits [10]. For a graph on a set of N vertices, eij is a variable that indicates the presence of an edge connecting vertices i and j: eij = 1 if there is an edge, and eij = 0 otherwise. We assume the vertex set is ﬁxed, so each graph is speciﬁed by an adjacency matrix, E = {eij }N . The degree of vertex i is denoted by di and the i,j=1 degree set by D = {di }N . The observations are given by a noisy adjacency matrix, i=1 X = {xij }N . Generally, edges can be directed, but in this paper we focus on i,j=1 undirected graphs, so eij = eji and xij = xji . Assuming the observation noise is independent for diﬀerent edges, the joint distribution is P (X, E, D) = P (X|E)P (E, D) = P (xij |eij ) P (E, D). j≥i P (xij |eij ) models the edge observation noise. We use an undirected model for the joint distribution over edges and degrees, P (E, D), where the prior distribution over di is determined by a non-negative potential fi (di ). Assuming graphs that have the same vertex degrees are equiprobable, we have N eij ) , fi (di )I(di , P (E, D) ∝ i j=1 where I(a, b) = 1 if a = b, and I(a, b) = 0 if a = b. The term I(di , j eij ) ensures that the number of edges connected to vertex i is equal to di . It is straightforward to show that the marginal distribution over di is P (di ) ∝ fi (di ) D\di nD j=i fj (dj ) , where nD is the number of graphs with degrees D and the sum is over all degree variables except di . The potentials, fi , can be estimated from a given degree prior using Markov chain Monte Carlo; or, as an approximation, they can be set to an empirical degree distribution obtained from noise-free graphs. Fig 2a shows the factor graph [11] for the above model. Each ﬁlled square corresponds to a term in the factorization of the joint distribution and the square is connected to all variables on which the term depends. Factor graphs are graphical models that unify the properties of Bayesian networks and Markov random ﬁelds [12]. Many inference algorithms, including the sum-product algorithm (a.k.a. loopy belief propagation), are more easily derived using factor graphs than Bayesian networks or Markov random ﬁelds. We describe the sum-product algorithm for our model in section 3. (a) I(d ,e + e +e +e 4 14 24 34 44 d1 e11 e12 e14 4 d3 d2 e13 f 4(d ) e22 e23 e24 (b) ) d1 d4 e33 e34 e1 e44 11 x11 x11 x12 x13 x14 x22 x23 x24 x33 d1 1 x34 x44 e2 11 e1 12 x12 e2 12 d1 2 e1 13 e1 e2 13 e1 14 x13 e1 22 x14 e2 14 d1 3 23 x22 e2 22 x23 e2 23 4 e1 e1 24 e2 24 e1 33 x24 34 x33 e2 33 x34 e2 34 e1 44 x44 e2 44 P( x44 | e44 ) (c) d4 s41 e14 e24 d2 1 d4 e34 e44 e14 s42 e24 s43 e34 d2 2 d2 3 d2 4 s44 P( x e44 44 1 ,e 2 ) 44 44 |e Figure 2: (a) A factor graph that describes a distribution over graphs with vertex degrees di , binary edge indicator variables eij , and noisy edge observations xij . The indicator function I(di , j eij ) enforces the constraint that the sum of the binary edge indicator variables for vertex i must equal the degree of vertex i. (b) A factor graph that explains noisy observed edges as a combination of two constituent graphs, with edge indicator variables e 1 and e2 . ij ij (c) The constraint I(di , j eij ) can be implemented using a chain with state variables, which leads to an exponentially faster message-passing algorithm. 2.1 Combining multiple graphs The above model is suitable when we want to infer a graph that matches a degree prior, assuming the edge observation noise is independent. A more challenging goal, with practical application, is to infer multiple hidden graphs that combine to explain the observed edge data. In section 4, we show how priors over multiple hidden graphs can be be used to infer protein-protein interactions. When there are H hidden graphs, each constituent graph is speciﬁed by a set of edges on the same set of N common vertices. For the degree variables and edge variables, we use a superscript to indicate which hidden graph the variable is used to describe. Assuming the graphs are independent, the joint distribution over the observed edge data X, and the edge variables and degree variables for the hidden graphs, E 1 , D1 , . . . , E H , DH , is H P (X, E 1 , D1 , . . . , E H , DH ) = P (xij |e1 , . . . , eH ) ij ij j≥i P (E h , Dh ), (1) h=1 where for each hidden graph, P (E h , Dh ) is modeled as described above. Here, the likelihood P (xij |e1 , . . . , eH ) describes how the edges in the hidden graphs combine ij ij to model the observed edge. Figure 2b shows the factor graph for this model. 3 Probabilistic inference of constituent graphs Exact probabilistic inference in the above models is intractable, here we introduce an approximate inference algorithm that consists of applying the sum-product algorithm, while ignoring cycles in the factor graph. Although the sum-product algorithm has been used to obtain excellent results on several problems [6, 7, 13, 14, 8, 9], we have found that the algorithm works best when the model consists of uncertain observations of variables that are subject to a large number of hard constraints. Thus the formulation of the model described above. Conceptually, our inference algorithm is a straight-forward application of the sumproduct algorithm, c.f. [15], where messages are passed along edges in the factor graph iteratively, and then combined at variables to obtain estimates of posterior probabilities. However, direct implementation of the message-passing updates will lead to an intractable algorithm. In particular, direct implementation of the update for the message sent from function I(di , j eij ) to edge variable eik takes a number of scalar operations that is exponential in the number of vertices. Fortunately there exists a more eﬃcient way to compute these messages. 3.1 Eﬃciently summing over edge conﬁgurations The function I(di , j eij ) ensures that the number of edges connected to vertex i is equal to di . Passing messages through this function requires summing over all edge conﬁgurations that correspond to each possible degree, di , and summing over di . Speciﬁcally, the message, µIi →eik (eik ), sent from function I(di , j eij ) to edge variable eik is given by I(di , di {eij | j=1,...,N, j=k} eij ) j µeij →Ii (eij ) , j=k where µeij →Ii (eij ) is the message sent from eij to function I(di , j eij ). The sum over {eij | j = 1, . . . , N, j = k} contains 2N −1 terms, so direct computation is intractable. However, for a maximum degree of dmax , all messages departing from the function I(di , j eij ) can be computed using order dmax N binary scalar operations, by introducing integer state variables sij . We deﬁne sij = n≤j ein and note that, by recursion, sij = sij−1 + eij , where si0 = 0 and 0 ≤ sij ≤ dmax . This recursive expression enables us to write the high-complexity constraint as the sum of a product of low-complexity constraints, N I(di , eij ) = j I(si1 , ei1 ) {sij | j=1,...,N } I(sij , sij−1 + eij ) I(di , siN ). j=2 This summation can be performed using the forward-backward algorithm. In the factor graph, the summation can be implemented by replacing the function I(di , j eij ) with a chain of lower-complexity functions, connected as shown in Fig. 2c. The function vertex (ﬁlled square) on the far left corresponds to I(si1 , ei1 ) and the function vertex in the upper right corresponds to I(di , siN ). So, messages can be passed through each constraint function I(di , j eij ) in an eﬃcient manner, by performing a single forward-backward pass in the corresponding chain. 4 Results We evaluate our model using yeast protein-protein interaction (PPI) data compiled by [16]. These data include eight sets of putative, but noisy, interactions derived from various sources, and one gold-standard set of interactions detected by reliable experiments. Using the ∼ 6300 yeast proteins as vertices, we represent the eight sets of putative m interactions using adjacency matrices {Y m }8 m=1 where yij = 1 if and only if putative interaction dataset m contains an interaction between proteins i and j. We similarly use Y gold to represent the gold-standard interactions. m We construct an observed graph, X, by setting xij = maxm yij for all i and j, thus the observed edge set is the union of all the putative edge sets. We test our model (a) (b) 40 0 untangling baseline random empirical potential posterior −2 30 log Pr true positives (%) 50 20 10 −4 −6 −8 0 0 5 10 −10 0 false positives (%) 10 20 30 degree (# of nodes) Figure 3: Protein-protein interaction network untangling results. (a) ROC curves measuring performance of predicting e1 when xij = 1. (b) Degree distributions. Compares the empirical ij degree distribution of the test set subgraph of E 1 to the degree potential f 1 estimated on the ˆ ij training set subgraph of E 1 and to the distribution of di = j pij where pij = P (e1 = 1|X) is estimated by untangling. on the task of discerning which of the edges in X are also in Y gold . We formalize this problem as that of decomposing X into two constituent graphs E 1 and E 2 , the gold true and the noise graphs respectively, such that e1 = xij yij and e2 = xij − e1 . ij ij ij We use a training set to ﬁt our model parameters and then measure task performance on a test set. The training set contains a randomly selected half of the ∼ 6300 yeast proteins, and the subgraphs of E 1 , E 2 , and X restricted to those vertices. The test contains the other half of the proteins and the corresponding subgraphs. Note that interactions connecting test set proteins to training set proteins (and vice versa) are ignored. We ﬁt three sets of parameters: a set of Naive Bayes parameters that deﬁne a set of edge-speciﬁc likelihood functions, Pij (xij |e1 , e2 ), one degree potential, f 1 , which ij ij is the same for every vertex in E1 and deﬁnes the prior P (E 1 ), and a second, f 2 , that similarly deﬁnes the prior P (E 2 ). The likelihood functions, Pij , are used to both assign likelihoods and enforce problem constraints. Given our problem deﬁnition, if xij = 0 then e1 = e2 = 0, ij ij otherwise xij = 1 and e1 = 1 − e2 . We enforce the former constraint by setij ij ting Pij (xij = 0|e1 , e2 ) = (1 − e1 )(1 − e2 ), and the latter by setting Pij (xij = ij ij ij ij 1|e1 , e2 ) = 0 whenever e1 = e2 . This construction of Pij simpliﬁes the calculation ij ij ij ij of the µPij →eh messages and improves the computational eﬃciency of inference beij cause when xij = 0, we need never update messages to and from variables e1 and ij e2 . We complete the speciﬁcation of Pij (xij = 1|e1 , e2 ) as follows: ij ij ij ym Pij (xij = 1|e1 , e2 ) ij ij = m ij θm (1 − θm )1−yij , if e1 = 1 and e2 = 0, ij ij ym m ij ψm (1 − ψm )1−yij , if e1 = 0 and e2 = 1. ij ij where {θm } and {ψm } are naive Bayes parameters, θm = i,j m yij e1 / ij i,j e1 and ij ψm = i,j m yij e2 / ij i,j e2 , respectively. ij The degree potentials f 1 (d) and f 2 (d) are kernel density estimates ﬁt to the degree distribution of the training set subgraphs of E 1 and E 2 , respectively. We use Gaussian kernels and set the width parameter (standard deviation) σ using leaveone-out cross-validation to maximize the total log density of the held-out datapoints. Each datapoint is the degree of a single vertex. Both degree potentials closely followed the training set empirical degree distributions. Untangling was done on the test set subgraph of X. We initially set the µ Pij →e1 ij messages equal to the likelihood function Pij and we randomly initialized the 1 µIj →e1 messages with samples from a normal distribution with mean 0 and variij ance 0.01. We then performed 40 iterations of the following message update order: 2 2 1 1 µe1 →Ij , µIj →e1 , µe1 →Pij , µPij →e2 , µe2 →Ij , µIj →e2 , µe2 →Pij , µPij →e1 . ij ij ij ij ij ij ij ij We evaluated our untangling algorithm using an ROC curve by comparing the actual ˆ test set subgraph of E 1 to posterior marginal probabilities,P (e1 = 1|X), estimated ij by our sum-product algorithm. Note that because the true interaction network is sparse (less than 0.2% of the 1.8 × 107 possible interactions are likely present [16]) and, in this case, true positive predictions are of greater biological interest than true negative predictions, we focus on low false positive rate portions of the ROC curve. Figure 3a compares the performance of a classiﬁer for e1 based on thresholding ij ˆ P (eij = 1|X) to a baseline method based on thresholding the likelihood functions, Pij (xij = 1|e1 = 1, e2 = 0). Note because e1 = 0 whenever xij = 0, we exclude ij ij ij the xij = 0 cases from our performance evaluation. The ROC curve shows that for the same low false positive rate, untangling produces 50% − 100% more true positives than the baseline method. Figure 3b shows that the degree potential, the true degree distribution, and the predicted degree distribution are all comparable. The slight overprediction of the true degree distribution may result because the degree potential f 1 that deﬁnes P (E 1 ) is not equal to the expected degree distribution of graphs sampled from the distribution P (E 1 ). 5 Summary and Related Work Related work includes other algorithms for structure-based graph denoising [17, 18]. These algorithms use structural properties of the observed graph to score edges and rely on the true graph having a surprisingly large number of three (or four) edge cycles compared to the noise graph. In contrast, we place graph generation in a probabilistic framework; our algorithm computes structural ﬁt in the hidden graph, where this computation is not aﬀected by the noise graph(s); and we allow for multiple sources of observation noise, each with its own structural properties. After submitting this paper to the NIPS conference, we discovered [19], in which a degree-based graph structure prior is used to denoise (but not untangle) observed graphs. This paper addresses denoising in directed graphs as well as undirected graphs, however, the prior that they use is not amenable to deriving an eﬃcient sumproduct algorithm. Instead, they use Markov Chain Monte Carlo to do approximate inference in a hidden graph containing 40 vertices. It is not clear how well this approach scales to the ∼ 3000 vertex graphs that we are using. In summary, the contributions of the work described in this paper include: a general formulation of the problem of graph untangling as inference in a factor graph; an eﬃcient approximate inference algorithm for a rich class of degree-based structure priors; and a set of reliability scores (i.e., edge posteriors) for interactions from a current version of the yeast protein-protein interaction network. References [1] A L Barabasi and R Albert. Emergence of scaling in random networks. Science, 286(5439), October 1999. [2] A Rzhetsky and S M Gomez. Birth of scale-free molecular networks and the number of distinct dna and protein domains per genome. Bioinformatics, pages 988–96, 2001. [3] M Faloutsos, P Faloutsos, and C Faloutsos. On power-law relationships of the Internet topology. Computer Communications Review, 29, 1999. [4] Hawoong Jeong, B Tombor, R´ka Albert, Z N Oltvai, and Albert-L´szl´ Barab´si. e a o a The large-scale organization of metabolic networks. Nature, 407, October 2000. [5] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San Mateo CA., 1988. [6] D. J. C. MacKay and R. M. Neal. Near Shannon limit performance of low density parity check codes. Electronics Letters, 32(18):1645–1646, August 1996. Reprinted in Electronics Letters, vol. 33, March 1997, 457–458. [7] B. J. Frey and F. R. Kschischang. Probability propagation and iterative decoding. In Proceedings of the 1996 Allerton Conference on Communication, Control and Computing, 1996. [8] B. J. Frey, R. Koetter, and N. Petrovic. Very loopy belief propagation for unwrapping phase images. In 2001 Conference on Advances in Neural Information Processing Systems, Volume 14. MIT Press, 2002. [9] M. M´zard, G. Parisi, and R. Zecchina. Analytic and algorithmic solution of random e satisﬁability problems. Science, 297:812–815, 2002. [10] B. J. Frey and D. J. C. MacKay. Trellis-constrained codes. In Proceedings of the 35 th Allerton Conference on Communication, Control and Computing 1997, 1998. [11] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, Special Issue on Codes on Graphs and Iterative Algorithms, 47(2):498–519, February 2001. [12] B. J. Frey. Factor graphs: A uniﬁcation of directed and undirected graphical models. University of Toronto Technical Report PSI-2003-02, 2003. [13] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate inference: An empirical study. In Uncertainty in Artiﬁcial Intelligence 1999. Stockholm, Sweden, 1999. [14] W. Freeman and E. Pasztor. Learning low-level vision. In Proceedings of the International Conference on Computer Vision, pages 1182–1189, 1999. [15] M. I. Jordan. An Inroduction to Learning in Graphical Models. 2004. In preparation. [16] C von Mering et al. Comparative assessment of large-scale data sets of protein-protein interactions. Nature, 2002. [17] R Saito, H Suzuki, and Y Hayashizaki. Construction of reliable protein-protein interaction networks with a new interaction generality measure. Bioinformatics, pages 756–63, 2003. [18] D S Goldberg and F P Roth. Assessing experimentally derived interactions in a small world. Proceedings of the National Academy of Science, 2003. [19] S M Gomez and A Rzhetsky. Towards the prediction of complete protein–protein interaction networks. In Paciﬁc Symposium on Biocomputing, pages 413–24, 2002.</p><p>5 0.67828935 <a title="91-lda-5" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>Author: Ben Taskar, Carlos Guestrin, Daphne Koller</p><p>Abstract: In typical classiﬁcation tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of conﬁdence of the classiﬁer, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efﬁciently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efﬁcient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classiﬁcation demonstrate very signiﬁcant gains over previous approaches. 1</p><p>6 0.66794997 <a title="91-lda-6" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>7 0.66629452 <a title="91-lda-7" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>8 0.66173738 <a title="91-lda-8" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>9 0.6575352 <a title="91-lda-9" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>10 0.6562441 <a title="91-lda-10" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>11 0.65616846 <a title="91-lda-11" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>12 0.65605736 <a title="91-lda-12" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>13 0.65367818 <a title="91-lda-13" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>14 0.65031558 <a title="91-lda-14" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>15 0.64935249 <a title="91-lda-15" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>16 0.64690709 <a title="91-lda-16" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>17 0.64653593 <a title="91-lda-17" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>18 0.64600408 <a title="91-lda-18" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>19 0.64545012 <a title="91-lda-19" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>20 0.64482617 <a title="91-lda-20" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
