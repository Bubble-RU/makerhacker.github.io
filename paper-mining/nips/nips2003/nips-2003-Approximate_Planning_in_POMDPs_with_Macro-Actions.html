<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-33" href="#">nips2003-33</a> knowledge-graph by maker-knowledge-mining</p><h1>33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</h1>
<br/><p>Source: <a title="nips-2003-33-pdf" href="http://papers.nips.cc/paper/2485-approximate-planning-in-pomdps-with-macro-actions.pdf">pdf</a></p><p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efÔ¨Åciently.</p><p>Reference: <a title="nips-2003-33-reference" href="../nips2003_reference/nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('macro', 0.472), ('pomdp', 0.318), ('believ', 0.29), ('grid', 0.278), ('gi', 0.255), ('primit', 0.254), ('resolv', 0.226), ('robot', 0.167), ('policy', 0.155), ('interpol', 0.142), ('baryc', 0.137), ('qmdp', 0.131), ('ag', 0.123), ('corrid', 0.116), ('reward', 0.115), ('mdp', 0.111), ('navig', 0.094), ('episod', 0.091), ('dicret', 0.079), ('smdp', 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="33-tfidf-1" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>2 0.4054594 <a title="33-tfidf-2" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>3 0.22393538 <a title="33-tfidf-3" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>4 0.19810961 <a title="33-tfidf-4" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>5 0.19280902 <a title="33-tfidf-5" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>6 0.17497849 <a title="33-tfidf-6" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>7 0.16676533 <a title="33-tfidf-7" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>8 0.15332618 <a title="33-tfidf-8" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>9 0.14750358 <a title="33-tfidf-9" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>10 0.12956375 <a title="33-tfidf-10" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>11 0.12560478 <a title="33-tfidf-11" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>12 0.12128078 <a title="33-tfidf-12" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>13 0.11628336 <a title="33-tfidf-13" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>14 0.11061168 <a title="33-tfidf-14" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>15 0.10712244 <a title="33-tfidf-15" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>16 0.10677824 <a title="33-tfidf-16" href="./nips-2003-Learning_Near-Pareto-Optimal_Conventions_in_Polynomial_Time.html">105 nips-2003-Learning Near-Pareto-Optimal Conventions in Polynomial Time</a></p>
<p>17 0.10592175 <a title="33-tfidf-17" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>18 0.10368575 <a title="33-tfidf-18" href="./nips-2003-An_MDP-Based_Approach_to_Online_Mechanism_Design.html">26 nips-2003-An MDP-Based Approach to Online Mechanism Design</a></p>
<p>19 0.096875921 <a title="33-tfidf-19" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>20 0.096407309 <a title="33-tfidf-20" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.23), (1, -0.296), (2, -0.221), (3, -0.108), (4, 0.063), (5, 0.178), (6, 0.035), (7, -0.039), (8, -0.122), (9, 0.118), (10, 0.087), (11, -0.142), (12, -0.05), (13, -0.091), (14, 0.026), (15, -0.018), (16, -0.026), (17, -0.136), (18, -0.016), (19, -0.05), (20, 0.053), (21, -0.149), (22, -0.015), (23, 0.088), (24, 0.095), (25, 0.091), (26, 0.029), (27, 0.017), (28, -0.167), (29, -0.004), (30, 0.008), (31, 0.054), (32, -0.054), (33, -0.009), (34, 0.071), (35, 0.147), (36, -0.184), (37, -0.055), (38, 0.081), (39, -0.025), (40, -0.104), (41, -0.05), (42, 0.129), (43, 0.034), (44, -0.097), (45, 0.008), (46, -0.012), (47, -0.001), (48, 0.013), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96005625 <a title="33-lsi-1" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>2 0.80236924 <a title="33-lsi-2" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>3 0.62999165 <a title="33-lsi-3" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>4 0.60765731 <a title="33-lsi-4" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>5 0.55618453 <a title="33-lsi-5" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>6 0.48739967 <a title="33-lsi-6" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>7 0.47011149 <a title="33-lsi-7" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>8 0.46943554 <a title="33-lsi-8" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>9 0.46455306 <a title="33-lsi-9" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>10 0.44657171 <a title="33-lsi-10" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>11 0.43337423 <a title="33-lsi-11" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>12 0.42659712 <a title="33-lsi-12" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>13 0.38049576 <a title="33-lsi-13" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>14 0.37444186 <a title="33-lsi-14" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>15 0.37250176 <a title="33-lsi-15" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>16 0.36988688 <a title="33-lsi-16" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>17 0.36259955 <a title="33-lsi-17" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>18 0.35638505 <a title="33-lsi-18" href="./nips-2003-An_MDP-Based_Approach_to_Online_Mechanism_Design.html">26 nips-2003-An MDP-Based Approach to Online Mechanism Design</a></p>
<p>19 0.35349068 <a title="33-lsi-19" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>20 0.34650651 <a title="33-lsi-20" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.065), (9, 0.013), (11, 0.051), (24, 0.013), (26, 0.112), (31, 0.05), (38, 0.014), (53, 0.041), (54, 0.276), (58, 0.067), (62, 0.051), (76, 0.1), (85, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70179403 <a title="33-lda-1" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>2 0.55656809 <a title="33-lda-2" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>3 0.55439413 <a title="33-lda-3" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>4 0.54695415 <a title="33-lda-4" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>5 0.54515821 <a title="33-lda-5" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>6 0.54135364 <a title="33-lda-6" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>7 0.5407421 <a title="33-lda-7" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>8 0.54040939 <a title="33-lda-8" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>9 0.53333831 <a title="33-lda-9" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>10 0.52964753 <a title="33-lda-10" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<p>11 0.52105236 <a title="33-lda-11" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>12 0.52100724 <a title="33-lda-12" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>13 0.52094078 <a title="33-lda-13" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>14 0.51643425 <a title="33-lda-14" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>15 0.51516426 <a title="33-lda-15" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>16 0.51308674 <a title="33-lda-16" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>17 0.50947851 <a title="33-lda-17" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>18 0.50574851 <a title="33-lda-18" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>19 0.50568253 <a title="33-lda-19" href="./nips-2003-An_MDP-Based_Approach_to_Online_Mechanism_Design.html">26 nips-2003-An MDP-Based Approach to Online Mechanism Design</a></p>
<p>20 0.5021987 <a title="33-lda-20" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
