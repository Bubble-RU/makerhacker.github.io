<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2003-Markov Models for Automated ECG Interval Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-123" href="#">nips2003-123</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 nips-2003-Markov Models for Automated ECG Interval Analysis</h1>
<br/><p>Source: <a title="nips-2003-123-pdf" href="http://papers.nips.cc/paper/2347-markov-models-for-automated-ecg-interval-analysis.pdf">pdf</a></p><p>Author: Nicholas P. Hughes, Lionel Tarassenko, Stephen J. Roberts</p><p>Abstract: We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling. 1</p><p>Reference: <a title="nips-2003-123-reference" href="../nips2003_reference/nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. [sent-6, score-0.563]
</p><p>2 An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. [sent-7, score-0.32]
</p><p>3 We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling. [sent-8, score-0.568]
</p><p>4 Of particular interest is the electrocardiogram (ECG1 ) of the patient, which provides detailed information about the state of the patient’s heart. [sent-12, score-0.155]
</p><p>5 By examining the ECG signal in detail it is possible to derive a number of informative measurements from the characteristic ECG waveform. [sent-13, score-0.098]
</p><p>6 In particular, drug-induced prolongation of the QT interval (so called Long QT Syndrome) can result in a very fast, abnormal heart rhythm known as torsade de pointes, which is often followed by sudden cardiac death 2 . [sent-16, score-0.219]
</p><p>7 In practice, QT interval measurements are carried out manually by specially trained ECG analysts. [sent-17, score-0.171]
</p><p>8 terfenadine, which had the side-effect of signiﬁcantly prolonging the QT interval in a number of patients. [sent-29, score-0.105]
</p><p>9 In this paper we consider the problem of automated ECG interval analysis from a machine learning perspective. [sent-31, score-0.161]
</p><p>10 In particular, we examine the use of hidden Markov models for automatically segmenting an ECG signal into its constituent waveform features. [sent-32, score-0.336]
</p><p>11 A redundant wavelet transform is used to provide an informative representation which is both robust to noise and tuned to the morphological characteristics of the waveform features. [sent-33, score-0.451]
</p><p>12 Finally we investigate the use of hidden semi-Markov models for explicit state duration modelling. [sent-34, score-0.302]
</p><p>13 Figure 1 shows a human ECG waveform and the associated features. [sent-38, score-0.162]
</p><p>14 The standard features of the ECG waveform are the P wave, the QRS complex and the T wave. [sent-39, score-0.214]
</p><p>15 Additionally a small U wave (following the T wave) is occasionally present. [sent-40, score-0.305]
</p><p>16 The cardiac cycle begins with the P wave (the start and end points of which are referred to as Pon and Poﬀ ), which corresponds to the period of atrial depolarization in the heart. [sent-41, score-0.47]
</p><p>17 The T wave follows the QRS complex and corresponds to the period of ventricular repolarization. [sent-44, score-0.373]
</p><p>18 The end point of the T wave is referred to as Toﬀ and represents the end of the cardiac cycle (presuming the absence of a U wave). [sent-45, score-0.407]
</p><p>19 2  ECG Interval Analysis  The timing between the onset and offset of particular features of the ECG (referred to as an interval) is of great importance since it provides a measure of the state of the heart and can indicate the presence of certain cardiological conditions. [sent-47, score-0.214]
</p><p>20 The two most important intervals in the ECG waveform are the QT interval and the PR interval. [sent-48, score-0.267]
</p><p>21 The QT interval is deﬁned as the time from the start of the QRS complex to the end of the T wave, i. [sent-49, score-0.176]
</p><p>22 Toﬀ − Q, and corresponds to the total duration of electrical activity (both depolarization and repolarization) in the ventricles. [sent-51, score-0.163]
</p><p>23 Similarly, the PR interval is deﬁned as the time from the start of the P wave to the start of the QRS complex, i. [sent-52, score-0.446]
</p><p>24 Q − Pon , and corresponds to the time from the onset of atrial depolarization to the onset of ventricular depolarization. [sent-54, score-0.144]
</p><p>25 The measurement of the QT interval is complicated by the fact that a precise mathematical deﬁnition of the end of the T wave does not exist. [sent-55, score-0.431]
</p><p>26 Thus T wave end measurements are inherently subjective and the resulting QT interval measurements often suffer from a high degree of inter- and intra-analyst variability. [sent-56, score-0.509]
</p><p>27 An automated ECG interval analysis system, which could provide robust and consistent measurements (together with an associated degree of conﬁdence in each measurement), would therefore be of great beneﬁt to the medical community. [sent-57, score-0.2]
</p><p>28 3  Previous Work on Automated ECG Interval Analysis  The vast majority of algorithms for automated QT analysis are based on threshold methods which attempt to predict the end of the T wave as the point where the T wave crosses a predetermined threshold [3]. [sent-59, score-0.687]
</p><p>29 An exception to this is the work of Koski [4] who trained a hidden Markov model on raw ECG data using the Baum-Welch algorithm. [sent-60, score-0.125]
</p><p>30 More recently, Graja and Boucher have investigated the use of hidden Markov tree models for segmenting ECG signals encoded with the discrete wavelet transform [2]. [sent-62, score-0.369]
</p><p>31 3  Data Collection  In order to develop an automated system for ECG interval analysis, we collected a data set of over 100 ECG waveforms (sampled at 500 Hz), together with the corresponding waveform feature boundaries3 as determined by a group of expert ECG analysts. [sent-63, score-0.367]
</p><p>32 Due to time constraints it was not possible for each expert analyst to label every ECG waveform in the data set. [sent-64, score-0.203]
</p><p>33 Therefore we chose to distribute the waveforms at random amongst the different experts (such that each waveform was measured by one expert only). [sent-65, score-0.206]
</p><p>34 For each ECG waveform, the following points were labelled: Pon , Poﬀ , Q, J and Toﬀ (if a U wave was present the Uoﬀ point was also labelled). [sent-66, score-0.305]
</p><p>35 In addition, the point corresponding to the start of the next P wave (i. [sent-67, score-0.323]
</p><p>36 the P wave of the following heart beat), NPon , was also labelled. [sent-69, score-0.335]
</p><p>37 4  A Hidden Markov Model for ECG Interval Analysis  It is natural to view the ECG signal as the result of a generative process, in which each waveform feature is generated by the corresponding cardiological state of the heart. [sent-71, score-0.347]
</p><p>38 In addition, the ECG state sequence obeys the Markov property, since each state is solely 3 We developed a novel software application which enabled an ECG analyst to label the boundaries of each of the features of an ECG waveform, using a pair of “onscreen calipers”. [sent-72, score-0.258]
</p><p>39 P wave Baseline 1 QRS complex T wave Baseline 2 U wave  5. [sent-73, score-0.947]
</p><p>40 Thus, hidden Markov models (HMMs) would seem ideally suited to the task of segmenting an ECG signal into its constituent waveform features. [sent-111, score-0.321]
</p><p>41 Using the labelled data set of ECG waveforms we trained a hidden Markov model in a supervised manner. [sent-112, score-0.157]
</p><p>42 The parameters of the transition matrix aij were computed using the maximum likelihood estimates, given by: aij = nij / ˆ  nik  (1)  k  where nij is the total number of transitions from state i to state j over all of the label sequences. [sent-114, score-0.286]
</p><p>43 We estimated the observation (or emission) probability densities bi for each state i by ﬁtting a Gaussian mixture model (GMM) to the set of signal samples corresponding to that particular state4 . [sent-115, score-0.17]
</p><p>44 In our initial experiments, we found that the use of a single state to represent all the regions of baseline in the ECG waveform resulted in poor performance when the model was used to infer the underlying state sequence of new unseen waveforms. [sent-117, score-0.447]
</p><p>45 In particular, a single baseline state allowed for the possibility of the model returning to the P wave state, following a P wave - Baseline sequence. [sent-118, score-0.788]
</p><p>46 Therefore we decided to partition the Baseline state into two separate states; one corresponding to the region of baseline between the P oﬀ and Q points (which we termed “Baseline 1”), and a second corresponding to the region between the Toﬀ and NPon points5 (termed “Baseline 2”). [sent-119, score-0.178]
</p><p>47 This was done in order to normalise the dynamic range of the signals and stabilise the baseline sections. [sent-122, score-0.091]
</p><p>48 Once the model had been trained, the Viterbi algorithm [9] was used to infer the optimal state sequence for each of the signals in the test set. [sent-123, score-0.127]
</p><p>49 Table 1 shows the resulting confusion matrix (computed from the state assignments on a sample-point basis). [sent-124, score-0.131]
</p><p>50 Although reasonable classiﬁcation accuracies are obtained for the QRS complex and T wave states, the P wave state is almost entirely misclassiﬁed as Baseline 1, Baseline 2 or U wave. [sent-125, score-0.749]
</p><p>51 In order to improve the performance of the model, we require an encoding of the ECG that captures the key temporal and spectral characteristics of the waveform features in a more informative representation than that of the raw time series data alone. [sent-126, score-0.301]
</p><p>52 Thus we now examine the use of wavelet methods for this purpose. [sent-127, score-0.206]
</p><p>53 5 If a U wave was present the Uoﬀ point was used instead of Toﬀ . [sent-129, score-0.305]
</p><p>54 P wave Baseline 1 QRS complex T wave Baseline 2 U wave  74. [sent-130, score-0.947]
</p><p>55 4  Table 2: Percentage confusion matrix for an HMM trained on the wavelet encoded ECG. [sent-157, score-0.268]
</p><p>56 They are able to capture the non-stationary spectral characteristics of a signal by decomposing it over a set of atoms which are localised in both time and frequency. [sent-160, score-0.118]
</p><p>57 The most popular wavelet transform algorithm is the discrete wavelet transform (DWT), which uses the set of dyadic scales (i. [sent-162, score-0.479]
</p><p>58 those based on powers of two) and translates of the mother wavelet to form an orthonormal basis for signal analysis. [sent-164, score-0.264]
</p><p>59 An alternative transform is derived by allowing the translation parameter to vary continuously, whilst restricting the scale parameter to a dyadic scale (thus, the set of time-frequency atoms now forms a frame). [sent-166, score-0.109]
</p><p>60 This leads to the undecimated wavelet transform6 (UWT), which for a signal s ∈ L2 (R), is given by: 1 wυ (τ ) = √ υ  +∞  s(t) ψ ∗ −∞  t−τ υ  dt  υ = 2k , k ∈ Z, τ ∈ R  (2)  where wυ (τ ) are the UWT coefﬁcients at scale υ and shift τ , and ψ ∗ is the complex conjugate of the mother wavelet. [sent-167, score-0.344]
</p><p>61 The UWT is particularly well-suited to ECG interval analysis as it provides a timefrequency description of the ECG signal on a sample-by-sample basis. [sent-169, score-0.147]
</p><p>62 In order to ﬁnd the most effective wavelet basis for our application, we examined the performance of HMMs trained on ECG data encoded with wavelets from the Daubechies, Symlet, Coiﬂet and Biorthogonal wavelet families. [sent-171, score-0.454]
</p><p>63 In the frequency domain, a wavelet at a given scale is associated with a bandpass ﬁlter7 of a particular centre frequency. [sent-172, score-0.207]
</p><p>64 Thus the optimal wavelet basis will correspond to the set of bandpass ﬁlters that are tuned to the unique spectral characteristics of the ECG. [sent-173, score-0.266]
</p><p>65 In our experiments we found that the Coiﬂet wavelet with two vanishing moments resulted in the highest overall classiﬁcation accuracy. [sent-174, score-0.191]
</p><p>66 It is evident that the UWT encoding results in a signiﬁcant improvement in classiﬁcation accuracy (for all but the U wave state), when compared with the results obtained on the raw ECG data. [sent-176, score-0.367]
</p><p>67 6 The undecimated wavelet transform is also known as the stationary wavelet transform and the translation-invariant wavelet transform. [sent-177, score-0.699]
</p><p>68 005 0  0 0  50  100 150 State duration (ms)  200  0 0  50 100 State duration (ms)  150  0  100 200 300 State duration (ms)  400  Figure 2: Histograms of the true state durations and those decoded by the HMM. [sent-197, score-0.669]
</p><p>69 2  HMM State Durations  A signiﬁcant limitation of the standard hidden Markov model is the manner in which it models state durations. [sent-199, score-0.166]
</p><p>70 For a given state i with self-transition coefﬁcient aii , the probability density of the state duration d is a geometric distribution, given by: pi (d) = (aii )d−1 (1 − aii )  (3)  For the waveform features of the ECG signal, this geometric distribution is inappropriate. [sent-200, score-0.601]
</p><p>71 Figure 2 shows histograms of the true state durations and the durations of the states decoded by the HMM, for each of the P wave, QRS complex and T wave states. [sent-201, score-0.81]
</p><p>72 In each case it is clear that a signiﬁcant number of decoded states have a duration that is much shorter than the minimum state duration observed with real ECG signals. [sent-202, score-0.455]
</p><p>73 Thus for a given ECG waveform the decoded state sequence may contain many more state transitions than are actually present in the signal. [sent-203, score-0.46]
</p><p>74 The resulting HMM state segmentation is then likely to be poor and the resulting QT and PR interval measurements unreliable. [sent-204, score-0.275]
</p><p>75 One solution to this problem is to post-process the decoded state sequences using a median ﬁlter designed to smooth out sequences whose duration is known to be physiologically implausible. [sent-205, score-0.312]
</p><p>76 A more principled and more effective approach, however, is to model the probability density of the individual state durations explicitly, using a hidden semi-Markov model. [sent-206, score-0.281]
</p><p>77 5  A Hidden Semi-Markov Model for ECG Interval Analysis  A hidden semi-Markov model (HSMM) differs from a standard HMM in that each of the self-transition coefﬁcients aii are set to zero, and an explicit probability density is speciﬁed for the duration of each state [5]. [sent-207, score-0.344]
</p><p>78 In this way, the individual state duration densities govern the amount of time the model spends in a given state, and the transition matrix governs the probability of the next state once this time has elapsed. [sent-208, score-0.356]
</p><p>79 To model the durations pi (d) of the various waveform features of the ECG, we used a Gamma density since this is a positive distribution which is able to capture the inherent skewness of the ECG state durations. [sent-210, score-0.404]
</p><p>80 For each state i, maximum likelihood estimates of the shape and scale parameters were computed directly from the set of labelled ECG signals (as part of the cross-validation procedure). [sent-211, score-0.171]
</p><p>81 In order to infer the most probable state sequence Q = {q1 q2 · · · qT } for a given observation sequence O = {O1 O2 · · · OT }, the standard Viterbi algorithm must be modiﬁed to  P wave  QRS complex  0. [sent-212, score-0.465]
</p><p>82 005 0  0 0  50  100 150 State duration (ms)  200  0 0  50 100 State duration (ms)  150  0  100 200 300 State duration (ms)  400  Figure 3: Histograms of the true state durations and those decoded by the HSMM. [sent-230, score-0.669]
</p><p>83 handle the explicit state duration densities of the HSMM. [sent-231, score-0.264]
</p><p>84 We start by deﬁning the likelihood of the most probable state sequence that accounts for the ﬁrst t observations and ends in state i: δt (i) = max p(q1 q2 · · · qt = i, O1 O2 · · · Ot |λ) (4) q1 q2 ···qt−1  where λ is the set of parameters governing the HSMM. [sent-232, score-0.392]
</p><p>85 The recurrence relation for computing δt (i) is then given by: δt (i) = max max δt−di (j)aji pi (di ) Πt =t−di +1 bi (Ot ) t di  j  (5)  where the outer maximisation is performed over all possible values of the state duration d i for state i, and the inner maximisation is over all states j. [sent-233, score-0.418]
</p><p>86 At each time t and for each state i, the two arguments that maximise equation (5) are recorded, and a simple backtracking procedure can then be used to ﬁnd the most probable state sequence. [sent-234, score-0.235]
</p><p>87 The time complexity of the Viterbi decoding procedure for an HSMM is given by O(K 2 T Dmax ), where K is the total number of states, and Dmax is the maximum range of state durations over all K states, i. [sent-235, score-0.222]
</p><p>88 Figure 3 shows histograms of the resulting state durations for an HSMM trained on a wavelet encoding of the ECG (using 5-fold cross-validation). [sent-240, score-0.493]
</p><p>89 Clearly, the durations of the decoded state sequences are very well matched to the true durations of each of the ECG features. [sent-241, score-0.421]
</p><p>90 This improvement in duration modelling is reﬂected in the accuracy and robustness of the segmentations produced by the HSMM. [sent-242, score-0.153]
</p><p>91 Model HMM on raw ECG HMM on wavelet encoded ECG HSMM on wavelet encoded ECG  Pon 157 12 13  Q 31 11 3  J 27 20 7  Toﬀ 139 46 12  Table 3: Mean absolute segmentation errors (in milliseconds) for each of the models. [sent-243, score-0.497]
</p><p>92 On the important task of accurately determining the Q and T oﬀ points for QT interval measurements, the HSMM signiﬁcantly outperforms the HMM. [sent-245, score-0.105]
</p><p>93 6  Discussion  In this work we have focused on the two core issues in developing an automated system for ECG interval analysis: the choice of representation for the ECG signal and the choice of model for the segmentation. [sent-247, score-0.203]
</p><p>94 We have demonstrated that wavelet methods, and in particular the undecimated wavelet transform, can be used to generate an encoding of the ECG which is tuned to the unique spectral characteristics of the ECG waveform features. [sent-248, score-0.674]
</p><p>95 With this representation the performance of the models on new unseen ECG waveforms is signiﬁcantly better than similar models trained on the raw time series data. [sent-249, score-0.093]
</p><p>96 We have also shown that the robustness of the segmentation process can be improved through the use of explicit state duration modelling with hidden semi-Markov models. [sent-250, score-0.358]
</p><p>97 The robustness with which we can detect such unreliable QT interval measurements based on this log likelihood score is one of the main focuses of our current research. [sent-257, score-0.16]
</p><p>98 Evaluation of an automatic threshold based detector e ia, of waveform limits in Holter ECG with QT database. [sent-279, score-0.162]
</p><p>99 Continuously variable duration hidden Markov models for automatic speech recognition. [sent-289, score-0.195]
</p><p>100 The dose-response relationship between Terfenadine (Seldane) and the QTc interval on the scalar electrocardiogram in normals and patients with cardiovascular disease and the QTc interval variability. [sent-304, score-0.258]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ecg', 0.796), ('wave', 0.305), ('wavelet', 0.191), ('qrs', 0.169), ('waveform', 0.162), ('qt', 0.139), ('duration', 0.121), ('durations', 0.115), ('state', 0.107), ('interval', 0.105), ('decoded', 0.084), ('uwt', 0.072), ('baseline', 0.071), ('hsmm', 0.06), ('hidden', 0.059), ('hmm', 0.058), ('automated', 0.056), ('pon', 0.052), ('electrocardiogram', 0.048), ('undecimated', 0.048), ('labelled', 0.044), ('depolarization', 0.042), ('aii', 0.042), ('cardiac', 0.042), ('signal', 0.042), ('transform', 0.039), ('measurements', 0.039), ('raw', 0.039), ('atoms', 0.036), ('cardiological', 0.036), ('dwt', 0.036), ('ventricular', 0.036), ('segmenting', 0.034), ('markov', 0.033), ('complex', 0.032), ('mother', 0.031), ('ms', 0.031), ('histograms', 0.03), ('heart', 0.03), ('trained', 0.027), ('waveforms', 0.027), ('dmax', 0.027), ('ot', 0.027), ('drug', 0.027), ('encoded', 0.026), ('patient', 0.025), ('segmentation', 0.024), ('analyst', 0.024), ('atrial', 0.024), ('coi', 0.024), ('graja', 0.024), ('npon', 0.024), ('qtc', 0.024), ('repolarization', 0.024), ('terfenadine', 0.024), ('uo', 0.024), ('confusion', 0.024), ('constituent', 0.024), ('pr', 0.024), ('characteristics', 0.023), ('encoding', 0.023), ('di', 0.023), ('hmms', 0.022), ('viterbi', 0.022), ('states', 0.022), ('end', 0.021), ('densities', 0.021), ('onset', 0.021), ('probable', 0.021), ('sudden', 0.021), ('death', 0.021), ('nij', 0.021), ('features', 0.02), ('signals', 0.02), ('dence', 0.02), ('dyadic', 0.019), ('syndrome', 0.019), ('maximisation', 0.019), ('wavelets', 0.019), ('tuned', 0.019), ('coef', 0.018), ('oxford', 0.018), ('gmm', 0.018), ('ltd', 0.018), ('start', 0.018), ('referred', 0.018), ('informative', 0.017), ('expert', 0.017), ('spectral', 0.017), ('comprised', 0.017), ('robustness', 0.016), ('modelling', 0.016), ('po', 0.016), ('bandpass', 0.016), ('cients', 0.015), ('explicit', 0.015), ('whilst', 0.015), ('examine', 0.015), ('speech', 0.015), ('aij', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="123-tfidf-1" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>Author: Nicholas P. Hughes, Lionel Tarassenko, Stephen J. Roberts</p><p>Abstract: We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling. 1</p><p>2 0.098295674 <a title="123-tfidf-2" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>3 0.070807546 <a title="123-tfidf-3" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>Author: Zhou Wang, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>4 0.056739435 <a title="123-tfidf-4" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>5 0.049794707 <a title="123-tfidf-5" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>Author: Yuanqing Li, Shun-ichi Amari, Sergei Shishkin, Jianting Cao, Fanji Gu, Andrzej S. Cichocki</p><p>Abstract: In this paper, sparse representation (factorization) of a data matrix is ﬁrst discussed. An overcomplete basis matrix is estimated by using the K−means method. We have proved that for the estimated overcomplete basis matrix, the sparse solution (coefﬁcient matrix) with minimum l1 −norm is unique with probability of one, which can be obtained using a linear programming algorithm. The comparisons of the l1 −norm solution and the l0 −norm solution are also presented, which can be used in recoverability analysis of blind source separation (BSS). Next, we apply the sparse matrix factorization approach to BSS in the overcomplete case. Generally, if the sources are not sufﬁciently sparse, we perform blind separation in the time-frequency domain after preprocessing the observed data using the wavelet packets transformation. Third, an EEG experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate its performance. Two almost independent components obtained by the sparse representation method are selected for phase synchronization analysis, and their periods of signiﬁcant phase synchronization are found which are related to tasks. Finally, concluding remarks review the approach and state areas that require further study. 1</p><p>6 0.04323652 <a title="123-tfidf-6" href="./nips-2003-Automatic_Annotation_of_Everyday_Movements.html">37 nips-2003-Automatic Annotation of Everyday Movements</a></p>
<p>7 0.043131836 <a title="123-tfidf-7" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>8 0.040971603 <a title="123-tfidf-8" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>9 0.036163867 <a title="123-tfidf-9" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>10 0.033996195 <a title="123-tfidf-10" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>11 0.030678982 <a title="123-tfidf-11" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>12 0.029589314 <a title="123-tfidf-12" href="./nips-2003-Human_and_Ideal_Observers_for_Detecting_Image_Curves.html">85 nips-2003-Human and Ideal Observers for Detecting Image Curves</a></p>
<p>13 0.029284947 <a title="123-tfidf-13" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>14 0.027941009 <a title="123-tfidf-14" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>15 0.027173216 <a title="123-tfidf-15" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>16 0.026845457 <a title="123-tfidf-16" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>17 0.025595488 <a title="123-tfidf-17" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>18 0.025463197 <a title="123-tfidf-18" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>19 0.025055913 <a title="123-tfidf-19" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>20 0.02402835 <a title="123-tfidf-20" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.093), (1, 0.046), (2, 0.026), (3, -0.003), (4, -0.037), (5, -0.0), (6, 0.059), (7, 0.007), (8, -0.001), (9, 0.027), (10, 0.021), (11, -0.047), (12, -0.022), (13, 0.026), (14, 0.017), (15, -0.019), (16, -0.001), (17, -0.015), (18, 0.001), (19, -0.028), (20, -0.008), (21, 0.032), (22, 0.022), (23, -0.057), (24, -0.087), (25, -0.062), (26, -0.089), (27, 0.008), (28, -0.017), (29, 0.029), (30, -0.064), (31, 0.033), (32, -0.007), (33, -0.063), (34, -0.068), (35, 0.008), (36, -0.01), (37, -0.224), (38, -0.076), (39, -0.034), (40, 0.094), (41, 0.083), (42, 0.161), (43, -0.007), (44, 0.045), (45, -0.006), (46, -0.122), (47, -0.012), (48, -0.057), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93025446 <a title="123-lsi-1" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>Author: Nicholas P. Hughes, Lionel Tarassenko, Stephen J. Roberts</p><p>Abstract: We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling. 1</p><p>2 0.64028221 <a title="123-lsi-2" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>3 0.5261088 <a title="123-lsi-3" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: We explore the phenomena of subjective randomness as a case study in understanding how people discover structure embedded in noise. We present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we characterize regularity in terms of a hierarchy of automata that augment a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format, and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats. 1</p><p>4 0.49152493 <a title="123-lsi-4" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>Author: Zhou Wang, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>5 0.46631277 <a title="123-lsi-5" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>6 0.46413884 <a title="123-lsi-6" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>7 0.43756825 <a title="123-lsi-7" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>8 0.42770055 <a title="123-lsi-8" href="./nips-2003-Impact_of_an_Energy_Normalization_Transform_on_the_Performance_of_the_LF-ASD_Brain_Computer_Interface.html">89 nips-2003-Impact of an Energy Normalization Transform on the Performance of the LF-ASD Brain Computer Interface</a></p>
<p>9 0.37254402 <a title="123-lsi-9" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>10 0.34504417 <a title="123-lsi-10" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>11 0.33978686 <a title="123-lsi-11" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>12 0.33175719 <a title="123-lsi-12" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>13 0.28142104 <a title="123-lsi-13" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>14 0.27724937 <a title="123-lsi-14" href="./nips-2003-The_Diffusion-Limited_Biochemical_Signal-Relay_Channel.html">184 nips-2003-The Diffusion-Limited Biochemical Signal-Relay Channel</a></p>
<p>15 0.2760095 <a title="123-lsi-15" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>16 0.26575047 <a title="123-lsi-16" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>17 0.25911134 <a title="123-lsi-17" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>18 0.25726995 <a title="123-lsi-18" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>19 0.25069758 <a title="123-lsi-19" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>20 0.24525125 <a title="123-lsi-20" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.052), (11, 0.018), (29, 0.01), (30, 0.03), (35, 0.103), (53, 0.06), (68, 0.01), (71, 0.056), (76, 0.043), (77, 0.337), (82, 0.011), (85, 0.049), (91, 0.076), (93, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7980122 <a title="123-lda-1" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>Author: Nicholas P. Hughes, Lionel Tarassenko, Stephen J. Roberts</p><p>Abstract: We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling. 1</p><p>2 0.68970376 <a title="123-lda-2" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>Author: Ofer Dekel, Yoram Singer, Christopher D. Manning</p><p>Abstract: Label ranking is the task of inferring a total order over a predeﬁned set of labels for each given instance. We present a general framework for batch learning of label ranking functions from supervised data. We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent. This enables us to accommodate a variety of ranking problems. In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels. Special cases of our setting are multilabel categorization and hierarchical classiﬁcation. We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration. The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus. 1</p><p>3 0.4315443 <a title="123-lda-3" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>4 0.42626223 <a title="123-lda-4" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classiﬁcation of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with ﬁve known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 1</p><p>5 0.42509833 <a title="123-lda-5" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Trevor J. Hastie</p><p>Abstract: Margin maximizing properties play an important role in the analysis of classi£cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a suf£cient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classi£cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines. 1</p><p>6 0.42076418 <a title="123-lda-6" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>7 0.42019838 <a title="123-lda-7" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>8 0.41842097 <a title="123-lda-8" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>9 0.41661313 <a title="123-lda-9" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>10 0.41592002 <a title="123-lda-10" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>11 0.41576678 <a title="123-lda-11" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>12 0.4153325 <a title="123-lda-12" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>13 0.41457739 <a title="123-lda-13" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>14 0.41345665 <a title="123-lda-14" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>15 0.41322884 <a title="123-lda-15" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>16 0.41138217 <a title="123-lda-16" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>17 0.40949473 <a title="123-lda-17" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>18 0.40710706 <a title="123-lda-18" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>19 0.40640497 <a title="123-lda-19" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>20 0.40618193 <a title="123-lda-20" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
