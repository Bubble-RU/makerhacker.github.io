<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-80" href="#">nips2003-80</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</h1>
<br/><p>Source: <a title="nips-2003-80-pdf" href="http://papers.nips.cc/paper/2505-generalised-propagation-for-fast-fourier-transforms-with-partial-or-missing-data.pdf">pdf</a></p><p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>Reference: <a title="nips-2003-80-reference" href="../nips2003_reference/nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). [sent-4, score-0.191]
</p><p>2 However there are many situations where doing fast Fourier transforms without complete data would be desirable. [sent-5, score-0.184]
</p><p>3 In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. [sent-6, score-0.55]
</p><p>4 Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. [sent-7, score-1.275]
</p><p>5 This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. [sent-8, score-0.193]
</p><p>6 Setting missing values to zeros or using interpolation will introduce various biases which will also aﬀect the results; these approaches can not help in using Fourier information to help restore the missing data. [sent-26, score-0.472]
</p><p>7 Prior information is needed to infer the missing data or the corresponding Fourier components. [sent-27, score-0.23]
</p><p>8 The FFT algorithm can be described as a belief network with deterministic connections where each intermediate node has two parents and two children (a form commonly called the butterﬂy net). [sent-30, score-0.708]
</p><p>9 By choosing a suitable cluster set for the network nodes and doing generalised propagation using these clusters, reasonable inference can be achieved. [sent-34, score-0.888]
</p><p>10 There have been other uses of belief networks and Bayesian methods to improve standard transforms. [sent-36, score-0.231]
</p><p>11 Other authors have recognised the problem of missing data in hierarchical systems. [sent-38, score-0.234]
</p><p>12 Inference in their model is propagation within a tree structured belief network. [sent-40, score-0.502]
</p><p>13 FFT related Toeplitz methods combined with partition inverse equations are applicable for inference in grid based Gaussian process systems with missing data [9]. [sent-41, score-0.263]
</p><p>14 For W = exp(−2πi/n), the kth Fourier coeﬃcient Fk is given by def  n/2−1  n−1  W kj xj =  Fk =  j=0  n/2−1  W 2kj x2j + j=0  e o W (2j+1)k x2j+1 = Fk + W k Fk  (1)  j=0  e where Fk denotes the kth component of the length n/2 Fourier transform of the o even components of xj . [sent-45, score-0.147]
</p><p>15 The two new shorter Fourier transforms can be split in the same way, recursively down to the transforms of length 1 which are just the data points themselves. [sent-47, score-0.179]
</p><p>16 This recursive algorithm can be drawn as a network of dependencies, using the inverse FFT as a generative model; it takes a set of Fourier components and creates some data. [sent-50, score-0.315]
</p><p>17 Doing this, we get the belief network of Figure 1a as a representation of the dependencies. [sent-53, score-0.445]
</p><p>18 The top row of this ﬁgure gives the Fourier components in order, and the bottom row gives the bit reversed data. [sent-54, score-0.128]
</p><p>19 The intermediate nodes are the even and odd Fourier coeﬃcients at diﬀerent levels of recursion. [sent-55, score-0.184]
</p><p>20 2 A Prior on Fourier Coeﬃcients The network of Figure 1a, combined with (1), speciﬁes the (deterministic) conditional distributions for all the nodes below the top layer. [sent-57, score-0.359]
</p><p>21 In general little prior phase information is known, but often there might be some  (a)  (b)  Figure 1: (a) The belief network corresponding to the fast Fourier transform. [sent-59, score-0.524]
</p><p>22 The intermediate layers denote the partial odd and even transforms that the algorithm uses. [sent-62, score-0.166]
</p><p>23 (b) The moralised undirected network with three clusters indicated by the boxes. [sent-63, score-0.357]
</p><p>24 All nodes surrounded by the same box type form part of the same cluster. [sent-64, score-0.119]
</p><p>25 For example we might expect a 1/f power spectra, or in some circumstances empirical priors may be appropriate. [sent-66, score-0.179]
</p><p>26 Then the variance of each prior will represent the magnitude of the expected power of that particular coeﬃcient. [sent-68, score-0.121]
</p><p>27 The belief network of Figure 1a is not singly connected and so exact propagation methods are not appropriate. [sent-71, score-0.716]
</p><p>28 Forming the full Gaussian distribution of the whole network and calculating using that is too expensive except in the smallest situations. [sent-72, score-0.214]
</p><p>29 Using exact optimisation (eg conjugate gradient) in the conditional Gaussian system is O(n2 ), although a smaller number of iterations of conjugate gradient can provide a good approximation. [sent-73, score-0.143]
</p><p>30 1  Loopy Propagation  Loopy propagation [7, 10, 3] in the FFT network suﬀers from some serious deﬁcits. [sent-76, score-0.485]
</p><p>31 Experiments with loopy propagation suggest that often there are convergence problems in the network, especially for systems of any signiﬁcant size. [sent-77, score-0.408]
</p><p>32 Intuitively the approximation given by loopy propagation fails to capture the moralisation of the parents, which, given the deterministic network, provides strong couplings. [sent-81, score-0.485]
</p><p>33 1  4  Generalised Belief Propagation for the FFT  In [14] the authors show that stationary points of loopy propagation between nodes of a Markov network are minimisers of the Bethe free energy of the probabilistic system. [sent-86, score-0.767]
</p><p>34 They also show that more general propagation procedures, such as propagation of information between clusters of a network correspond to the minimisation of a more general Kikuchi free energy of which The Bethe free energy is a special case. [sent-87, score-0.973]
</p><p>35 To overcome the shortfalls of belief propagation methods, a generalised belief propagation scheme is used here. [sent-88, score-1.163]
</p><p>36 The basic problem is that there is strong dependence between parents of a given node, and the fact that the values for those two nodes are fully determined by the two children but undetermined by only one. [sent-89, score-0.265]
</p><p>37 This can be done for all nodes at all levels, and we ﬁnd that the cluster separator between any two clusters consists of at most one node. [sent-91, score-0.349]
</p><p>38 At each stage of propagation between clusters only the messages (in each direction) at single nodes need to be maintained. [sent-92, score-0.679]
</p><p>39 The procedure can be summarised as follows: Start with the belief network of Figure 1a and convert it to an undirected network by moralisation (Figure 1b). [sent-93, score-0.764]
</p><p>40 Then we identify the clusters of the graph, which each consist of four nodes as illustrated by the boxes in Figure 1b. [sent-94, score-0.253]
</p><p>41 Each cluster consists of two common parents and their common children. [sent-95, score-0.182]
</p><p>42 Building a network of clusters involves creating an edge for each separator. [sent-97, score-0.323]
</p><p>43 From Figure 1 it can be seen that this network will have undirected loops. [sent-98, score-0.248]
</p><p>44 Hence belief propagation in this system will not be exact. [sent-99, score-0.502]
</p><p>45 However it will be possible to iteratively propagate messages in this system. [sent-100, score-0.18]
</p><p>46 Hopefully the iteration will result in an equilibrium being reached which we can use as an approximate inference for the marginals of the network nodes, although such convergence is not guaranteed. [sent-101, score-0.306]
</p><p>47 1  Propagation Equations  This section provides the propagation messages for the approach described above. [sent-103, score-0.451]
</p><p>48 For simplicity, and to maintain symmetry we use an update scheme where messages are ﬁrst passed down from what were the root nodes (before moralisation) to the leaf nodes, and then messages are passed up from the leaf to the root nodes. [sent-104, score-0.733]
</p><p>49 The ﬁrst pass down the network is data independent and can be precomputed. [sent-106, score-0.239]
</p><p>50 1  Messages Propagating Down  The Markov network derived from a belief network has the potentials of each cluster deﬁned by the conditional probability of all the child nodes in that cluster given their parents. [sent-109, score-0.95]
</p><p>51 Two adjoining clusters of the network are illustrated in Figure 2a. [sent-110, score-0.348]
</p><p>52 All the cluster interactions in the network have this form, and so the message passing described below applies to all the nodes. [sent-111, score-0.461]
</p><p>53 + The message ρ4 ≡ N (µ+ , σ4 ) is deﬁned to be that passed down from some cluster C1 4 containing nodes y1 , y2 (originally the parents) and y3 , y4 (originally the children) to the cluster below: C2 = (y4 , y5 , y6 , y7 ), with y6 and y7 the children. [sent-112, score-0.462]
</p><p>54 The message is given by the marginal of the cluster potential multiplied by the incoming messages from the other nodes. [sent-114, score-0.406]
</p><p>55 The standard message passing scheme can be followed to get the usual form of results for Gaussian networks [7, 11]. [sent-115, score-0.189]
</p><p>56 − Suppose λ3 (y3 ) = N (y3 ; µ− , σ3 ) is the message passing up the network at node 3 + + + 3, whereas ρ1 (y1 ) = N (y1 ; µ1 , σ1 ) and ρ2 (y2 ) = N (y2 ; µ+ , σ2 ) are the messages 2 passing down the network at nodes 1 and 2 respectively. [sent-116, score-0.988]
</p><p>57 2  (4)  Messages Propagating Up  In the same way we can calculate the messages which are propagated up the network. [sent-121, score-0.211]
</p><p>58 2 Initialisation The network is initialised by setting the λ messages at the leaf nodes to be N (x, 0) for a node known to take value x and N (0, ∞) for the missing data. [sent-128, score-0.825]
</p><p>59 All the other λ messages are initialised to N (0, ∞). [sent-129, score-0.232]
</p><p>60 The ρ message at a given root node is set to the prior at that root node. [sent-130, score-0.282]
</p><p>61 No other ρ messages need to be initialised as they are not needed before they are computed during the ﬁrst pass. [sent-131, score-0.269]
</p><p>62 Computationally, we usually have to add a small jitter term network noise, and represent the inﬁnite variances by large numbers to avoid numeric problems. [sent-132, score-0.279]
</p><p>63 5  Demonstrations and Results  In all the tests in this section the generalised propagation converged in a small number of iterations without the need to resort to damping. [sent-133, score-0.496]
</p><p>64 All the clusters in the network contain four nodes. [sent-151, score-0.323]
</p><p>65 Hence the interaction between any two connected clusters is of the form illustrated in this ﬁgure. [sent-153, score-0.134]
</p><p>66 (b) How the weighted mean square error varies for spectra with diﬀerent power laws (f Power ). [sent-154, score-0.219]
</p><p>67 The ﬁlled line is the belief network approach, the dashed line is linear interpolation, the dotted line uses mean-valued data. [sent-155, score-0.445]
</p><p>68 ‘Zero ﬁll’ replaces missing values by zero and then does an FFT. [sent-165, score-0.168]
</p><p>69 The resulting means are compared with the components obtained by replacing the missing data with zeros or with interpolated values and taking an FFT . [sent-173, score-0.26]
</p><p>70 Weighted mean squared errors (WMSE) are also calculated, where each frequency component is divided by its prior variance before averaging. [sent-175, score-0.14]
</p><p>71 The generalised belief propagation produces better results than any of the other approaches. [sent-177, score-0.661]
</p><p>72 The beneﬁts of interpolation are seen for situations where there are only low frequency components, and the zeroing approach becomes more reasonable in white noise like situations, but across a broad spread of spectral priors, the belief network approach tends to perform better. [sent-179, score-0.597]
</p><p>73 Note that the approach is particularly good at the 1/f power spectra point, which corresponds to the form of spectra in many real life problems. [sent-182, score-0.316]
</p><p>74 52 × 10−5  Table 2: Testing the MSE predictive ability of the belief network approach. [sent-194, score-0.445]
</p><p>75 00031 Table 3: Testing the ability of the belief network approach on real life audio data. [sent-207, score-0.53]
</p><p>76 The BN approach performs better than all others for both prediction of the correct spectrum and prediction of the missing data. [sent-208, score-0.168]
</p><p>77 MSE: mean squared error, WMSE: weighted mean squared error, MSEPRED: Mean squared error of the data predictor. [sent-209, score-0.208]
</p><p>78 Next we compare approaches for ﬁlling in missing data. [sent-210, score-0.168]
</p><p>79 Note that ignoring periodic boundary constraints, a 1/f 2 power spectra produces a Brownian curve for which the linear predictor is the optimal mean predictor. [sent-212, score-0.219]
</p><p>80 In this case the mean square error for the belief network propagation approach (Table 2) is close to the linear error. [sent-213, score-0.752]
</p><p>81 The local smoothness information is not easily used in the belief network propagation, because neighbouring points in data space are only connected at the highest level in the belief network. [sent-215, score-0.701]
</p><p>82 The approximations of loopy propagation methods do not preserve enough information when propagated over these distances. [sent-216, score-0.411]
</p><p>83 However for data such as that produced by the common 1/f power spectra, interpolation methods are less eﬀective, and the belief network propagation performs well. [sent-217, score-0.878]
</p><p>84 In this situation the belief network approach outperforms interpolation. [sent-218, score-0.445]
</p><p>85 Fourier power spectra of the mean of 15 other diﬀerent sections of laughter are used to estimate the prior power spectral characteristics. [sent-222, score-0.365]
</p><p>86 A belief network FFT is then calculated in the usual way, and compared with the true FFT calculated on the whole data. [sent-224, score-0.525]
</p><p>87 The belief network approach performs better than all other methods including linear and cubic spline interpolation. [sent-226, score-0.554]
</p><p>88 6  Discussion  This paper provides a clear practical example of a situation where generalised propagation overcomes deﬁcits in simpler propagation methods. [sent-227, score-0.701]
</p><p>89 It demonstrates how a belief network representation of the fast Fourier transform allows Fourier approaches to be used in situations where data is missing. [sent-228, score-0.603]
</p><p>90 Kikuchi inference in the FFT belief network proves superior to many naive approaches for dealing with missing data in the calculation of Fourier transforms. [sent-229, score-0.701]
</p><p>91 It does this while maintaining O(n log2 n) nature of the FFT algorithm, if we assume that the number of iterations needed for convergence does not increase with data size. [sent-231, score-0.127]
</p><p>92 Further investigation is needed  to show exactly what the scaling is, and further documentation of the beneﬁts of generalised propagation over loopy propagation and conjugate gradient methods is needed beyond the space available here. [sent-233, score-0.937]
</p><p>93 It might be possible that variational approximation using clusters [12] could provide another approach to inference in this system. [sent-234, score-0.148]
</p><p>94 This paper has also not considered the possibility of dependent or sparse priors over Fourier coeﬃcients, or priors over phase information, all of which would be interesting. [sent-235, score-0.128]
</p><p>95 In conclusion the tests done indicate that this is a valuable approach for dealing with missing data in Fourier analysis. [sent-237, score-0.256]
</p><p>96 In fact loopy propagation methods in FFT networks are also valuable in many scenarios. [sent-239, score-0.414]
</p><p>97 Very recent work of Yedidia [13], shows that discrete generalised belief propagation in FFT constructions may enable the beneﬁts of sparse decoders to be used for Reed-Solomon codes. [sent-240, score-0.661]
</p><p>98 Stable ﬁxed points of loopy propagation are minima of the Bethe Free Energy. [sent-265, score-0.38]
</p><p>99 Correctness of local probability propagation in graphical models with loops. [sent-307, score-0.271]
</p><p>100 Correctness of belief propagation in Gaussian models of arbitrary topology. [sent-313, score-0.502]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fourier', 0.459), ('fft', 0.417), ('propagation', 0.271), ('belief', 0.231), ('network', 0.214), ('messages', 0.18), ('coe', 0.179), ('missing', 0.168), ('generalised', 0.159), ('fk', 0.151), ('nodes', 0.119), ('message', 0.116), ('loopy', 0.109), ('clusters', 0.109), ('spectra', 0.103), ('parents', 0.096), ('wmse', 0.094), ('mse', 0.093), ('cluster', 0.086), ('power', 0.08), ('bn', 0.078), ('spline', 0.078), ('transforms', 0.077), ('moralisation', 0.071), ('priors', 0.064), ('cients', 0.06), ('interpolation', 0.057), ('node', 0.055), ('audio', 0.055), ('passed', 0.055), ('conjugate', 0.053), ('initialised', 0.052), ('transform', 0.051), ('children', 0.05), ('multiscale', 0.049), ('msepred', 0.047), ('toeplitz', 0.047), ('erent', 0.047), ('passing', 0.045), ('situations', 0.044), ('di', 0.044), ('bethe', 0.043), ('prior', 0.041), ('recognised', 0.041), ('ll', 0.04), ('inference', 0.039), ('fast', 0.038), ('components', 0.038), ('odd', 0.037), ('leaf', 0.037), ('yedidia', 0.037), ('february', 0.037), ('squared', 0.037), ('iterations', 0.037), ('needed', 0.037), ('mean', 0.036), ('root', 0.035), ('circumstances', 0.035), ('separator', 0.035), ('valuable', 0.034), ('variances', 0.034), ('undirected', 0.034), ('deterministic', 0.034), ('bit', 0.033), ('diag', 0.033), ('recursive', 0.032), ('propagated', 0.031), ('jitter', 0.031), ('kikuchi', 0.031), ('reversed', 0.031), ('cubic', 0.031), ('inverse', 0.031), ('life', 0.03), ('edinburgh', 0.03), ('tests', 0.029), ('complex', 0.029), ('kth', 0.029), ('zeros', 0.029), ('usual', 0.028), ('convergence', 0.028), ('free', 0.028), ('intermediate', 0.028), ('propagating', 0.028), ('correctness', 0.027), ('frequency', 0.026), ('calculated', 0.026), ('energy', 0.026), ('top', 0.026), ('marginals', 0.025), ('practically', 0.025), ('data', 0.025), ('help', 0.025), ('illustrated', 0.025), ('spectral', 0.025), ('table', 0.024), ('calculation', 0.024), ('ts', 0.024), ('marginal', 0.024), ('originally', 0.024), ('layers', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="80-tfidf-1" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>2 0.20738663 <a title="80-tfidf-2" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>Author: Tom Heskes, Onno Zoeter, Wim Wiegerinck</p><p>Abstract: We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach. 1</p><p>3 0.17444634 <a title="80-tfidf-3" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>Author: Mark A. Paskin</p><p>Abstract: Rao–Blackwellization is an approximation technique for probabilistic inference that ﬂexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efﬁcient implementation of Rao–Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster’s variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems. 1</p><p>4 0.16824514 <a title="80-tfidf-4" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>5 0.14322023 <a title="80-tfidf-5" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>Author: Yuan Qi, Tom Minka</p><p>Abstract: Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a “message” to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms. 1</p><p>6 0.12040906 <a title="80-tfidf-6" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>7 0.11653785 <a title="80-tfidf-7" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<p>8 0.11493079 <a title="80-tfidf-8" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>9 0.11287381 <a title="80-tfidf-9" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>10 0.10913177 <a title="80-tfidf-10" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>11 0.096147612 <a title="80-tfidf-11" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>12 0.093086518 <a title="80-tfidf-12" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>13 0.090753078 <a title="80-tfidf-13" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>14 0.0899956 <a title="80-tfidf-14" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>15 0.08688587 <a title="80-tfidf-15" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>16 0.083811834 <a title="80-tfidf-16" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>17 0.082555622 <a title="80-tfidf-17" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>18 0.080054082 <a title="80-tfidf-18" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>19 0.07856971 <a title="80-tfidf-19" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>20 0.077720508 <a title="80-tfidf-20" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.234), (1, -0.028), (2, -0.07), (3, 0.277), (4, -0.01), (5, -0.2), (6, 0.076), (7, 0.088), (8, 0.028), (9, 0.083), (10, -0.014), (11, 0.12), (12, 0.006), (13, -0.034), (14, -0.108), (15, -0.036), (16, 0.071), (17, 0.071), (18, 0.082), (19, 0.059), (20, -0.026), (21, 0.069), (22, 0.025), (23, 0.053), (24, -0.031), (25, 0.124), (26, -0.094), (27, 0.059), (28, -0.163), (29, 0.004), (30, 0.061), (31, 0.149), (32, -0.091), (33, 0.01), (34, -0.033), (35, 0.125), (36, 0.1), (37, -0.107), (38, -0.056), (39, -0.078), (40, 0.04), (41, -0.031), (42, -0.039), (43, 0.08), (44, -0.024), (45, 0.019), (46, 0.092), (47, 0.025), (48, 0.035), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97981983 <a title="80-lsi-1" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>2 0.78761417 <a title="80-lsi-2" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>Author: Tom Heskes, Onno Zoeter, Wim Wiegerinck</p><p>Abstract: We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach. 1</p><p>3 0.5760017 <a title="80-lsi-3" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>4 0.56668609 <a title="80-lsi-4" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>Author: Mark A. Paskin</p><p>Abstract: Rao–Blackwellization is an approximation technique for probabilistic inference that ﬂexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efﬁcient implementation of Rao–Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster’s variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems. 1</p><p>5 0.5447306 <a title="80-lsi-5" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>Author: Chen Yanover, Yair Weiss</p><p>Abstract: Loopy belief propagation (BP) has been successfully used in a number of diﬃcult graphical models to ﬁnd the most probable conﬁguration of the hidden variables. In applications ranging from protein folding to image analysis one would like to ﬁnd not just the best conﬁguration but rather the top M . While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large. In this work we address the problem of ﬁnding the M best conﬁgurations when exact inference is impossible. We start by developing a new exact inference algorithm for calculating the best conﬁgurations that uses only max-marginals. For approximate inference, we replace the max-marginals with the beliefs calculated using max-product BP and generalized BP. We show empirically that the algorithm can accurately and rapidly approximate the M best conﬁgurations in graphs with hundreds of variables. 1</p><p>6 0.53640544 <a title="80-lsi-6" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>7 0.48071346 <a title="80-lsi-7" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>8 0.46309942 <a title="80-lsi-8" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>9 0.45926243 <a title="80-lsi-9" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>10 0.45256239 <a title="80-lsi-10" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>11 0.44932291 <a title="80-lsi-11" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>12 0.40589878 <a title="80-lsi-12" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<p>13 0.3942993 <a title="80-lsi-13" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>14 0.36196697 <a title="80-lsi-14" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>15 0.36141172 <a title="80-lsi-15" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>16 0.36037022 <a title="80-lsi-16" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>17 0.34599864 <a title="80-lsi-17" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>18 0.34229809 <a title="80-lsi-18" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>19 0.34000218 <a title="80-lsi-19" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>20 0.3364563 <a title="80-lsi-20" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.076), (11, 0.024), (29, 0.012), (30, 0.03), (35, 0.097), (53, 0.128), (69, 0.013), (70, 0.228), (71, 0.068), (76, 0.049), (85, 0.06), (91, 0.104), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82351911 <a title="80-lda-1" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>2 0.80894911 <a title="80-lda-2" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>Author: Noam Shental, Aharon Bar-hillel, Tomer Hertz, Daphna Weinshall</p><p>Abstract: Density estimation with Gaussian Mixture Models is a popular generative technique used also for clustering. We develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure. Equivalence constraints are deﬁned on pairs of data points, indicating whether the points arise from the same source (positive constraints) or from different sources (negative constraints). Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others. For the estimation of model parameters we present a closed form EM procedure which handles positive constraints, and a Generalized EM procedure using a Markov net which handles negative constraints. Using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks, and that our algorithm is preferable to two other suggested methods using the same type of side information.</p><p>3 0.68811393 <a title="80-lda-3" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>4 0.68739527 <a title="80-lda-4" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>5 0.68465972 <a title="80-lda-5" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classiﬁcation of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with ﬁve known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 1</p><p>6 0.68050593 <a title="80-lda-6" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>7 0.67998677 <a title="80-lda-7" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>8 0.67566395 <a title="80-lda-8" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>9 0.67414248 <a title="80-lda-9" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>10 0.67266232 <a title="80-lda-10" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>11 0.67160946 <a title="80-lda-11" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>12 0.66980761 <a title="80-lda-12" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>13 0.6689114 <a title="80-lda-13" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>14 0.66838819 <a title="80-lda-14" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>15 0.66815835 <a title="80-lda-15" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>16 0.66776419 <a title="80-lda-16" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>17 0.66769075 <a title="80-lda-17" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>18 0.66626698 <a title="80-lda-18" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>19 0.66412985 <a title="80-lda-19" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>20 0.66379398 <a title="80-lda-20" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
