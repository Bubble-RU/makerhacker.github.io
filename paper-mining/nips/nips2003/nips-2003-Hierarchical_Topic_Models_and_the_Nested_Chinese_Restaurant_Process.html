<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-83" href="#">nips2003-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</h1>
<br/><p>Source: <a title="nips-2003-83-pdf" href="http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf">pdf</a></p><p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>Reference: <a title="nips-2003-83-reference" href="../nips2003_reference/nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  University of California, Berkeley Berkeley, CA 94720  Massachusetts Institute of Technology Cambridge, MA 02139  Abstract We address the problem of learning topic hierarchies from data. [sent-11, score-0.368]
</p><p>2 We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. [sent-13, score-0.757]
</p><p>3 We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. [sent-15, score-0.472]
</p><p>4 An important instance of such modeling challenges is provided by the problem of learning a topic hierarchy from data. [sent-20, score-0.409]
</p><p>5 Given a collection of “documents,” each of which contains a set of “words,” we wish to discover common usage patterns or “topics” in the documents, and to organize these topics into a hierarchy. [sent-21, score-0.231]
</p><p>6 (Note that while we use the terminology of document modeling throughout this paper, the methods that we describe are general. [sent-22, score-0.249]
</p><p>7 We approach this model selection problem by specifying a generative probabilistic model for hierarchical structures and taking a Bayesian perspective on the problem of learning these structures from data. [sent-24, score-0.224]
</p><p>8 Thus our hierarchies are random variables; moreover, these random variables are speciﬁed procedurally, according to an algorithm that constructs the hierarchy as data are made available. [sent-25, score-0.28]
</p><p>9 The probabilistic object that underlies this approach  is a distribution on partitions of integers known as the Chinese restaurant process [1]. [sent-26, score-0.6]
</p><p>10 We show how to extend the Chinese restaurant process to a hierarchy of partitions, and show how to use this new process as a representation of prior and posterior distributions for topic hierarchies. [sent-27, score-1.017]
</p><p>11 There are several possible approaches to the modeling of topic hierarchies. [sent-28, score-0.267]
</p><p>12 In our approach, each node in the hierarchy is associated with a topic, where a topic is a distribution across words. [sent-29, score-0.442]
</p><p>13 A document is generated by choosing a path from the root to a leaf, repeatedly sampling topics along that path, and sampling the words from the selected topics. [sent-30, score-0.68]
</p><p>14 Thus the organization of topics into a hierarchy aims to capture the breadth of usage of topics across the corpus, reﬂecting underlying syntactic and semantic notions of generality and speciﬁcity. [sent-31, score-0.524]
</p><p>15 This approach differs from models of topic hierarchies which are built on the premise that the distributions associated with parents and children are similar [2]. [sent-32, score-0.41]
</p><p>16 Our model more closely resembles the hierarchical topic model considered in [3], though that work does not address the model selection problem which is our primary focus. [sent-34, score-0.404]
</p><p>17 2  Chinese restaurant processes  We begin with a brief description of the Chinese restaurant process and subsequently show how this process can be extended to hierarchies. [sent-35, score-1.06]
</p><p>18 1  The Chinese restaurant process  The Chinese restaurant process (CRP) is a distribution on partitions of integers obtained by imagining a process by which M customers sit down in a Chinese restaurant with an inﬁnite number of tables. [sent-37, score-1.789]
</p><p>19 The mth subsequent customer sits at a table drawn from the following distribution: p(occupied table i | previous customers) = p(next unoccupied table | previous customers) =  mi γ+m−1 γ γ+m−1  (1)  where mi is the number of previous customers at table i, and γ is a parameter. [sent-40, score-0.306]
</p><p>20 In a species sampling mixture [6], each table in the Chinese restaurant is associated with a draw from p(β | η) where β is a mixture component parameter. [sent-47, score-0.868]
</p><p>21 First, it is a distribution over seating plans; the number of mixture components is determined by the number of tables which the data occupy. [sent-51, score-0.24]
</p><p>22 Applications to various kinds of mixture models have begun to appear in recent years; examples include Gaussian mixture models [8], hidden Markov models [9] and mixtures of experts [10]. [sent-54, score-0.184]
</p><p>23 1 The terminology was inspired by the Chinese restaurants in San Francisco which seem to have an inﬁnite seating capacity. [sent-55, score-0.257]
</p><p>24 2  Extending the CRP to hierarchies  The CRP is amenable to mixture modeling because we can establish a one-to-one relationship between tables and mixture components and a one-to-many relationship between mixture components and data. [sent-58, score-0.532]
</p><p>25 In the models that we will consider, however, each data point is associated with multiple mixture components which lie along a path in a hierarchy. [sent-59, score-0.213]
</p><p>26 A nested Chinese restaurant process can be deﬁned by imagining the following scenario. [sent-61, score-0.733]
</p><p>27 Suppose that there are an inﬁnite number of inﬁnite-table Chinese restaurants in a city. [sent-62, score-0.169]
</p><p>28 One restaurant is determined to be the root restaurant and on each of its inﬁnite tables is a card with the name of another restaurant. [sent-63, score-1.094]
</p><p>29 On each of the tables in those restaurants are cards that refer to other restaurants, and this structure repeats inﬁnitely. [sent-64, score-0.236]
</p><p>30 Each restaurant is referred to exactly once; thus, the restaurants in the city are organized into an inﬁnitely-branched tree. [sent-65, score-0.66]
</p><p>31 Note that each restaurant is associated with a level in this tree (e. [sent-66, score-0.629]
</p><p>32 , the root restaurant is at level 1 and the restaurants it refers to are at level 2). [sent-68, score-0.822]
</p><p>33 On the ﬁrst evening, he enters the root Chinese restaurant and selects a table using Eq. [sent-70, score-0.594]
</p><p>34 On the second evening, he goes to the restaurant identiﬁed on the ﬁrst night’s table and chooses another table, again from Eq. [sent-72, score-0.525]
</p><p>35 At the end of the trip, the tourist has sat at L restaurants which constitute a path from the root to a restaurant at the Lth level in the inﬁnite tree described above. [sent-75, score-0.941]
</p><p>36 After M tourists take L-day vacations, the collection of paths describe a particular L-level subtree of the inﬁnite tree (see Figure 1a for an example of such a tree). [sent-76, score-0.224]
</p><p>37 This prior can be used to model topic hierarchies. [sent-77, score-0.302]
</p><p>38 Just as a standard CRP can be used to express uncertainty about a possible number of components, the nested CRP can be used to express uncertainty about possible L-level trees. [sent-78, score-0.21]
</p><p>39 3  A hierarchical topic model  Let us consider a data set composed of a corpus of documents. [sent-79, score-0.442]
</p><p>40 Each document is a collection of words, where a word is an item in a vocabulary. [sent-80, score-0.262]
</p><p>41 Our basic assumption is that the words in a document are generated according to a mixture model where the mixing proportions are random and document-speciﬁc. [sent-81, score-0.461]
</p><p>42 These topics (one distribution for each possible value of z) are the basic mixture components in our model. [sent-83, score-0.317]
</p><p>43 Temporarily assuming K possible topics in the corpus, an assumption that we will soon relax, z thus ranges over K possible values and θ is a K-dimensional K vector. [sent-85, score-0.181]
</p><p>44 LDA is thus a twolevel generative process in which documents are associated with topic proportions, and the corpus is modeled as a Dirichlet distribution on these topic proportions. [sent-89, score-0.847]
</p><p>45 We now describe an extension of this model in which the topics lie in a hierarchy. [sent-90, score-0.204]
</p><p>46 A document is generated as follows: (1) choose a path from the root of the tree to a leaf; (2) draw a vector of topic proportions θ from an L-dimensional Dirichlet; (3) generate the words in the document from a mixture of the topics along the path from root to leaf, with  mixing proportions θ. [sent-92, score-1.449]
</p><p>47 Finally, we use the nested CRP to relax the assumption of a ﬁxed tree structure. [sent-94, score-0.234]
</p><p>48 As we have seen, the nested CRP can be used to place a prior on possible trees. [sent-95, score-0.216]
</p><p>49 We also place a prior on the topics βi , each of which is associated with a restaurant in the inﬁnite tree (in particular, we assume a symmetric Dirichlet with hyperparameter η). [sent-96, score-0.852]
</p><p>50 A document is drawn by ﬁrst choosing an L-level path through the restaurants and then drawing the words from the L topics which are associated with the restaurants along that path. [sent-97, score-0.865]
</p><p>51 Note that all documents share the topic associated with the root restaurant. [sent-98, score-0.488]
</p><p>52 , L}: (a) Draw a table from restaurant c −1 using Eq. [sent-105, score-0.525]
</p><p>53 Set c to be the restaurant referred to by that table. [sent-107, score-0.491]
</p><p>54 Draw an L-dimensional topic proportion vector θ from Dir(α). [sent-109, score-0.235]
</p><p>55 (b) Draw wn from the topic associated with restaurant cz . [sent-118, score-0.768]
</p><p>56 The node labeled T refers to a collection of an inﬁnite number of L-level paths drawn from a nested CRP. [sent-120, score-0.314]
</p><p>57 Given T , the cm, variables are deterministic—simply look up the th level of the mth path in the inﬁnite collection of paths. [sent-121, score-0.189]
</p><p>58 However, not having observed T , the distribution of c m, will be deﬁned by the nested Chinese restaurant process, conditioned on all the c q, for q < m. [sent-122, score-0.683]
</p><p>59 Its posterior path will depend, through the unobserved T , on the posterior paths of all the documents in the original corpus. [sent-129, score-0.353]
</p><p>60 Subsequent new documents will also depend on the original corpus and any new documents which were observed before them. [sent-130, score-0.396]
</p><p>61 (1), any new document can choose a previously unvisited restaurant at any level of the tree. [sent-132, score-0.746]
</p><p>62 , even if we have a peaked posterior on T which has essentially selected a particular tree, a new document can change that hierarchy if its words provide justiﬁcation for such a change. [sent-135, score-0.418]
</p><p>63 In another variation of this model, we can consider a process that ﬂattens the nested CRP into a standard CRP, but retains the idea that a tourist eats L meals. [sent-136, score-0.303]
</p><p>64 That is, the tourist eats L times in a single restaurant under the constraint that he does not choose the same table twice. [sent-137, score-0.617]
</p><p>65 In particular, it can be used as a prior for a ﬂat LDA model in which each document can use at most L topics from the potentially inﬁnite total set of topics. [sent-139, score-0.438]
</p><p>66 4  Approximate inference by Gibbs sampling  In this section, we describe a Gibbs sampling algorithm for sampling from the posterior nested CRP and corresponding topics in the hLDA model. [sent-141, score-0.555]
</p><p>67 The Gibbs sampler provides a method for simultaneously exploring the parameter space (the particular topics of the corpus) and the model space (L-level trees). [sent-142, score-0.289]
</p><p>68 The solid lines connect each restaurant to the restaurants referred to by its tables. [sent-144, score-0.66]
</p><p>69 This illustrates a sample from the state space of the posterior nested CRP of Figure 1b for four documents. [sent-146, score-0.218]
</p><p>70 (b) The graphical model representation of hierarchical LDA with a nested CRP prior. [sent-147, score-0.267]
</p><p>71 We have separated the nested Chinese restaurant process from the topics. [sent-148, score-0.702]
</p><p>72 The conditional distribution for cm , the L topics associated with document m, is: p(cm | w, c−m , z) ∝ p(wm | c, w−m , z)p(cm | c−m ), where w−m and c−m denote the w and c variables for all documents other than m. [sent-156, score-0.715]
</p><p>73 This expression is an instance of Bayes’ rule with p(wm | c, w−m , z) as the likelihood of the data given a particular choice of cm and p(cm | c−m ) as the prior on cm implied by the nested CRP. [sent-157, score-0.448]
</p><p>74 The set of possible values for cm corresponds to the union of the set of existing paths through the tree, equal to the number of leaves, with the set of possible novel paths, equal to the number of internal nodes. [sent-161, score-0.18]
</p><p>75 (1) and the deﬁnition of a nested CRP in Section 2. [sent-163, score-0.172]
</p><p>76 Each document has 1000 words from a 25 term vocabulary. [sent-166, score-0.249]
</p><p>77 (b) The correct hierarchy found by the Gibbs sampler on this corpus. [sent-167, score-0.208]
</p><p>78 We illustrate that the nested CRP process is feasible for learning text hierarchies in hLDA by using a contrived corpus on a small vocabulary. [sent-176, score-0.474]
</p><p>79 We generated a corpus of 100 1000word documents from a three-level hierarchy with a vocabulary of 25 terms. [sent-177, score-0.474]
</p><p>80 In this corpus, topics on the vocabulary can be viewed as bars on a 5 × 5 grid. [sent-178, score-0.256]
</p><p>81 The root topic places its probability mass on the bottom bar. [sent-179, score-0.324]
</p><p>82 On the second level, one topic is identiﬁed with the leftmost bar, while the rightmost bar represents a second topic. [sent-180, score-0.292]
</p><p>83 The leftmost topic has two subtopics while the rightmost topic has one subtopic. [sent-181, score-0.539]
</p><p>84 Figure 2b illustrates the recovered hierarchy using the Gibbs sampling algorithm described in Section 4. [sent-183, score-0.175]
</p><p>85 In estimating hierarchy structures, hypothesis testing approaches to model selection are impractical since they do not provide a viable method of searching over the large space of trees. [sent-184, score-0.174]
</p><p>86 We generated 210 corpora of 100 1000-word documents each from an LDA model with K ∈ {5, . [sent-186, score-0.219]
</p><p>87 , 25}, L = 5, a vocabulary size of 100, and randomly generated mixture components from a symmetric Dirichlet (η = 0. [sent-189, score-0.213]
</p><p>88 Each corpus has 100 documents of 1000 words from a vocabulary of 100 terms. [sent-204, score-0.388]
</p><p>89 Figure 3 reports the results of sampling from the resulting posterior on trees with the Gibbs sampler from Section 4. [sent-205, score-0.208]
</p><p>90 Using 1717 NIPS abstracts from 1987–1999 [14] with 208,896 words and a vocabulary of 1600 terms, we estimated a three level hierarchy as illustrated in Figure 5. [sent-209, score-0.344]
</p><p>91 6  Summary  We have presented the nested Chinese restaurant process, a distribution on hierarchical partitions. [sent-214, score-0.755]
</p><p>92 We have shown that this process can be used as a nonparametric prior for a hierarchical extension to the latent Dirichlet allocation model. [sent-215, score-0.219]
</p><p>93 The result is a ﬂexible, general model for topic hierarchies that naturally accommodates growing data collections. [sent-216, score-0.422]
</p><p>94 First, we have restricted ourselves to hierarchies of ﬁxed depth L for simplicity, but it is straightforward to consider a model in which L can vary from document to document. [sent-219, score-0.346]
</p><p>95 Each document is still a mixture of topics along a path in a hierarchy, but different documents can express paths of different lengths as they represent varying levels of specialization. [sent-220, score-0.743]
</p><p>96 Second, although in our current model a document is associated with a single path, it is also natural to consider models in which documents are allowed to mix over paths. [sent-221, score-0.397]
</p><p>97 Each node contains the top eight words from its corresponding topic distribution. [sent-225, score-0.316]
</p><p>98 The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data. [sent-239, score-0.386]
</p><p>99 Generalized weighted Chinese restaurant processes for species sampling mixture models. [sent-254, score-0.659]
</p><p>100 Markov chain sampling methods for Dirichlet process mixture models. [sent-258, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('restaurant', 0.491), ('crp', 0.445), ('chinese', 0.292), ('topic', 0.235), ('document', 0.19), ('topics', 0.181), ('nested', 0.172), ('restaurants', 0.169), ('documents', 0.142), ('hierarchies', 0.133), ('hierarchy', 0.123), ('ncm', 0.123), ('cm', 0.116), ('corpus', 0.112), ('dirichlet', 0.107), ('hlda', 0.107), ('lda', 0.093), ('mixture', 0.092), ('sampler', 0.085), ('gibbs', 0.081), ('vocabulary', 0.075), ('hierarchical', 0.072), ('root', 0.069), ('customers', 0.067), ('paths', 0.064), ('tree', 0.062), ('seating', 0.061), ('tourist', 0.061), ('words', 0.059), ('path', 0.055), ('proportions', 0.054), ('wm', 0.053), ('abstracts', 0.053), ('sampling', 0.052), ('bayes', 0.05), ('leaf', 0.049), ('tourists', 0.046), ('posterior', 0.046), ('mth', 0.045), ('prior', 0.044), ('tables', 0.043), ('associated', 0.042), ('draw', 0.041), ('word', 0.041), ('process', 0.039), ('factors', 0.039), ('level', 0.034), ('blei', 0.034), ('table', 0.034), ('modeling', 0.032), ('corpora', 0.032), ('hyperparameter', 0.032), ('accommodates', 0.031), ('eats', 0.031), ('evening', 0.031), ('imagining', 0.031), ('sit', 0.031), ('sits', 0.031), ('subtopics', 0.031), ('unvisited', 0.031), ('collection', 0.031), ('nite', 0.03), ('partitions', 0.03), ('selection', 0.028), ('structures', 0.028), ('customer', 0.027), ('terminology', 0.027), ('ths', 0.027), ('latent', 0.026), ('refers', 0.025), ('trees', 0.025), ('components', 0.024), ('repeats', 0.024), ('species', 0.024), ('dimension', 0.024), ('variables', 0.024), ('grif', 0.023), ('model', 0.023), ('node', 0.022), ('generative', 0.022), ('generated', 0.022), ('simulated', 0.021), ('subtree', 0.021), ('nth', 0.021), ('nonparametric', 0.021), ('mixing', 0.021), ('mass', 0.02), ('syntactic', 0.02), ('integers', 0.02), ('distribution', 0.02), ('grow', 0.02), ('express', 0.019), ('leftmost', 0.019), ('challenges', 0.019), ('rightmost', 0.019), ('abstraction', 0.019), ('bar', 0.019), ('usage', 0.019), ('text', 0.018), ('allocation', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="83-tfidf-1" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>2 0.10163052 <a title="83-tfidf-2" href="./nips-2003-Learning_a_Distance_Metric_from_Relative_Comparisons.html">108 nips-2003-Learning a Distance Metric from Relative Comparisons</a></p>
<p>Author: Matthew Schultz, Thorsten Joachims</p><p>Abstract: This paper presents a method for learning a distance metric from relative comparison such as “A is closer to B than A is to C”. Taking a Support Vector Machine (SVM) approach, we develop an algorithm that provides a ﬂexible way of describing qualitative training data as a set of constraints. We show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for SVM training. We empirically evaluate the performance and the modelling ﬂexibility of the algorithm on a collection of text documents. 1</p><p>3 0.099638365 <a title="83-tfidf-3" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>4 0.080086395 <a title="83-tfidf-4" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>Author: Mark Girolami, Ata Kabán</p><p>Abstract: To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the deﬁnition of individual speciﬁc and global models. This paper proposes a linear-time distributed model for ﬁnite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained’ by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-speciﬁc proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efﬁcient representation scheme, reﬂected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.</p><p>5 0.070017792 <a title="83-tfidf-5" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>Author: Konrad P. Körding, Daniel M. Wolpert</p><p>Abstract: When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.</p><p>6 0.058155585 <a title="83-tfidf-6" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>7 0.05741943 <a title="83-tfidf-7" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>8 0.056931704 <a title="83-tfidf-8" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>9 0.052270286 <a title="83-tfidf-9" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>10 0.0504703 <a title="83-tfidf-10" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>11 0.047680713 <a title="83-tfidf-11" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>12 0.047188804 <a title="83-tfidf-12" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>13 0.045216471 <a title="83-tfidf-13" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>14 0.044797845 <a title="83-tfidf-14" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>15 0.042063151 <a title="83-tfidf-15" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>16 0.040969733 <a title="83-tfidf-16" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>17 0.040525332 <a title="83-tfidf-17" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>18 0.039311662 <a title="83-tfidf-18" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>19 0.03874423 <a title="83-tfidf-19" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>20 0.038572878 <a title="83-tfidf-20" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, -0.018), (2, -0.005), (3, 0.056), (4, -0.037), (5, -0.014), (6, 0.026), (7, 0.021), (8, -0.0), (9, 0.033), (10, 0.031), (11, -0.05), (12, -0.036), (13, -0.008), (14, 0.138), (15, -0.002), (16, -0.138), (17, 0.05), (18, 0.122), (19, -0.057), (20, -0.03), (21, 0.029), (22, -0.117), (23, 0.118), (24, 0.011), (25, -0.033), (26, 0.06), (27, -0.102), (28, 0.199), (29, 0.028), (30, 0.038), (31, -0.054), (32, -0.035), (33, 0.08), (34, -0.054), (35, 0.024), (36, -0.135), (37, -0.041), (38, -0.076), (39, 0.029), (40, 0.058), (41, -0.119), (42, -0.011), (43, -0.01), (44, 0.048), (45, -0.042), (46, 0.181), (47, 0.069), (48, -0.028), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94294918 <a title="83-lsi-1" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>2 0.47846401 <a title="83-lsi-2" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>Author: Benjamin M. Marlin</p><p>Abstract: In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. 1</p><p>3 0.46420601 <a title="83-lsi-3" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>Author: Konrad P. Körding, Daniel M. Wolpert</p><p>Abstract: When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.</p><p>4 0.45151162 <a title="83-lsi-4" href="./nips-2003-Learning_a_Distance_Metric_from_Relative_Comparisons.html">108 nips-2003-Learning a Distance Metric from Relative Comparisons</a></p>
<p>Author: Matthew Schultz, Thorsten Joachims</p><p>Abstract: This paper presents a method for learning a distance metric from relative comparison such as “A is closer to B than A is to C”. Taking a Support Vector Machine (SVM) approach, we develop an algorithm that provides a ﬂexible way of describing qualitative training data as a set of constraints. We show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard methods for SVM training. We empirically evaluate the performance and the modelling ﬂexibility of the algorithm on a collection of text documents. 1</p><p>5 0.44959012 <a title="83-lsi-5" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>6 0.4366188 <a title="83-lsi-6" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>7 0.43396026 <a title="83-lsi-7" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>8 0.42948821 <a title="83-lsi-8" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>9 0.38723657 <a title="83-lsi-9" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>10 0.38498977 <a title="83-lsi-10" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>11 0.35236916 <a title="83-lsi-11" href="./nips-2003-Link_Prediction_in_Relational_Data.html">118 nips-2003-Link Prediction in Relational Data</a></p>
<p>12 0.34760553 <a title="83-lsi-12" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>13 0.32246819 <a title="83-lsi-13" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>14 0.31536907 <a title="83-lsi-14" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>15 0.31406185 <a title="83-lsi-15" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>16 0.29356778 <a title="83-lsi-16" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>17 0.28444499 <a title="83-lsi-17" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>18 0.28382263 <a title="83-lsi-18" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>19 0.27972838 <a title="83-lsi-19" href="./nips-2003-Fast_Embedding_of_Sparse_Similarity_Graphs.html">71 nips-2003-Fast Embedding of Sparse Similarity Graphs</a></p>
<p>20 0.27540776 <a title="83-lsi-20" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.041), (11, 0.025), (29, 0.024), (30, 0.025), (34, 0.01), (35, 0.037), (53, 0.076), (63, 0.01), (71, 0.099), (76, 0.046), (85, 0.065), (91, 0.092), (94, 0.314), (99, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82721639 <a title="83-lda-1" href="./nips-2003-Algorithms_for_Interdependent_Security_Games.html">19 nips-2003-Algorithms for Interdependent Security Games</a></p>
<p>Author: Michael Kearns, Luis E. Ortiz</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.78665411 <a title="83-lda-2" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>3 0.69418955 <a title="83-lda-3" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>4 0.48912969 <a title="83-lda-4" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>Author: Tong Zhang</p><p>Abstract: In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simpliﬁes and enhances previous results. Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution. This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist “bad” prior structures even at places far away from the true distribution. 1</p><p>5 0.48852438 <a title="83-lda-5" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>Author: Yuan Qi, Tom Minka</p><p>Abstract: Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a “message” to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms. 1</p><p>6 0.48493436 <a title="83-lda-6" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>7 0.48276764 <a title="83-lda-7" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>8 0.48260787 <a title="83-lda-8" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>9 0.48191071 <a title="83-lda-9" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>10 0.48137701 <a title="83-lda-10" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>11 0.48107955 <a title="83-lda-11" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>12 0.48072863 <a title="83-lda-12" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>13 0.48071983 <a title="83-lda-13" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>14 0.48034108 <a title="83-lda-14" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>15 0.48019093 <a title="83-lda-15" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>16 0.47978693 <a title="83-lda-16" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>17 0.47952235 <a title="83-lda-17" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>18 0.47938669 <a title="83-lda-18" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>19 0.47891331 <a title="83-lda-19" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>20 0.47827977 <a title="83-lda-20" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
