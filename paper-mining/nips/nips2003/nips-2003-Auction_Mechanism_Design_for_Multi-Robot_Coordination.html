<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-36" href="#">nips2003-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</h1>
<br/><p>Source: <a title="nips-2003-36-pdf" href="http://papers.nips.cc/paper/2537-auction-mechanism-design-for-multi-robot-coordination.pdf">pdf</a></p><p>Author: Curt Bererton, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 1</p><p>Reference: <a title="nips-2003-36-reference" href="../nips2003_reference/nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. [sent-4, score-0.198]
</p><p>2 For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. [sent-6, score-0.226]
</p><p>3 We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. [sent-7, score-0.407]
</p><p>4 We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. [sent-8, score-0.61]
</p><p>5 1  Introduction  In recent years, the design of cooperative multi-robot systems has become a highly active research area within robotics [1, 2, 3, 4, 5, 6]. [sent-9, score-0.189]
</p><p>6 Many planning problems in robotics are best phrased as MDPs, deﬁned over world states or—in case of partial observability—belief states [7]. [sent-10, score-0.403]
</p><p>7 This enormous complexity has conﬁned MDP planning techniques largely to single-robot systems. [sent-12, score-0.303]
</p><p>8 In many cases, robots in a multi-robot system interact only in limited ways. [sent-13, score-0.388]
</p><p>9 Robots might seek not to collide with each other [1], coordinate their locations to carry out a joint task [4, 6], or consume a joint resource with limited availability [8, 9, 10]. [sent-14, score-0.515]
</p><p>10 Marketbased algorithms are particularly attractive for multi-robot planning because many common types of interactions can be phrased as constraints on resources such as space (two robots can’t occupy the same location at once) and time (a robot can only work on a limited number of tasks at once). [sent-18, score-1.201]
</p><p>11 The resource usage and state depend on the robots’ plans between now and time t, which in turn depend on the price. [sent-20, score-0.504]
</p><p>12 Worse yet, future resource usage depends on random events which can’t be predicted exactly. [sent-21, score-0.435]
</p><p>13 In particular, we propose a general technique for decomposing multi-robot MDP problems into “loosely coupled” MDPs which interact only through resource production and consumption constraints. [sent-23, score-0.577]
</p><p>14 The decomposition works by turning all interactions into streams of payments between robots, thereby allowing each robot to learn its own local value function. [sent-24, score-0.443]
</p><p>15 The actual prices for these resources are set by a “master” agent; the master agent takes into account the possibility of re-allocating resources at each step, but it approximates the effect of interactions between robots. [sent-26, score-0.683]
</p><p>16 Our algorithm can be distributed so that each robot reasons only about its own local interactions, and it always produces the same answer as a particular centralized planning algorithm. [sent-28, score-0.73]
</p><p>17 ca ∈ N is the immediate reward for taking action a and Ta ∈ N ×N is the matrix representation of the transition probabilities for action a. [sent-41, score-0.218]
</p><p>18 minV α · V (1) ∀a : V ≥ ca + γTa V  maxfa a fa  −γ  a ca · f a T a Ta fa = α  (2)  ∀a : fa ≥ 0  The dual of the Bellman LP gives us an interesting alternative from which to view the problem of ﬁnding an optimal policy. [sent-44, score-0.289]
</p><p>19 The vector fa represents the expected number of times we perform action a from each state. [sent-46, score-0.182]
</p><p>20 , fi or Ai ) will distinguish the planning problems for different robots. [sent-50, score-0.47]
</p><p>21 1  Algorithm Loosely coupled MDPs  Our algorithm is designed for multi-robot problems that can be decomposed into separate single-robot MDPs which interact through the production or consumption of ﬁctitious resources. [sent-52, score-0.243]
</p><p>22 These resources may be physical goods such as fuel; or they may be logical resources such as the right to pass over a bridge at a particular time, the right to explore an area of the environment, or the right to collect reward for achieving a particular subgoal. [sent-53, score-0.287]
</p><p>23 Time may be part of the individual robot states, in which case a resource could be the right to consume a unit of fuel at a particular time (a futures contract). [sent-54, score-0.949]
</p><p>24 In more detail, each robot has a vector of state-action visitation frequencies f i which must satisfy its own local dynamics Ai fi = bi . [sent-55, score-0.564]
</p><p>25 Its production or consumption of resources is deﬁned by a matrix Ci : element (j, k) of Ci is the amount of resource j which is produced or consumed by robot i in state-action pair k. [sent-56, score-0.961]
</p><p>26 (So, Ci fi is the vector of expected resource usages for robot i. [sent-57, score-0.956]
</p><p>27 ) The robots interact through resource constraints: the instantaneous production and consumption of each resource must balance exactly. [sent-59, score-1.264]
</p><p>28 This representation is in many ways related to an undirected dynamic Bayes network: each node of the network corresponds to the state and action of a single MDP, and a resource constraint involving a subset of the MDPs plays the role of a clique potential on the corresponding nodes. [sent-60, score-0.512]
</p><p>29 In the same (trivial) sense as Bayes nets, our representation is completely general: by collapsing all robots into a single giant agent we can represent an arbitrary MDP. [sent-62, score-0.39]
</p><p>30 More importantly, in the more-typical case that some pieces of our model can be written as resource constraints, we can achieve an exponential savings in representation size compared to the monolithic planning problem. [sent-63, score-0.693]
</p><p>31 2  Approximation  The resource constraints are what make loosely-coupled MDPs difﬁcult to solve. [sent-65, score-0.436]
</p><p>32 However, by making a simple approximation we can remove the nonlinearity and so factor our planning problem: we relax the resource constraints so that they must only be satisﬁed in expectation over all time steps, rather than deterministically on each time step. [sent-67, score-0.739]
</p><p>33 Under this assumption, knowing the expected resources available to a robot allows that robot to plan independently: since Ci fi is the vector of expected resource usages for robot i, adding the constraint Ci fi = k to equation (2) gives us the single-robot resourceconstrained planning problem. [sent-68, score-2.284]
</p><p>34 The (approximate) global planning problem then becomes to determine an optimal resource allocation among robots and corresponding single-robot plans, or equivalently to determine the optimal resource prices and corresponding single-robot value functions. [sent-69, score-1.682]
</p><p>35 More formally, the planning problem is to solve (3): maxfi i ci · fi ∀i : Ai fi = bi (3) (∗) i C i fi = d ∀i : fi ≥ 0 Without the constraints marked (∗), this LP would represent a set of completely uncoupled robot planning problems. [sent-70, score-1.847]
</p><p>36 The constraints (∗) are the approximated resource constraints:  they say that expected production must equal expected consumption for each resource. [sent-71, score-0.62]
</p><p>37 The resource prices are the dual variables for (∗), and the local value functions are the dual variables for the remaining equality constraints. [sent-72, score-0.707]
</p><p>38 The quality of our prices and value functions will depend on whether it is valid to assume a single price for each resource: if the prices stay constant then our approximate plan will translate perfectly to the physical world. [sent-73, score-0.477]
</p><p>39 4, two robots might each plan to break down at the same time and be towed by the other. [sent-76, score-0.423]
</p><p>40 The only way to ﬁx this problem is to make a more accurate model; in the worst case we will have to combine several robots into one large MDP so that we can track their joint allocation of resources at all times. [sent-77, score-0.449]
</p><p>41 3  Action selection  Because the value functions incorporate information about future actions and random events, the robots only need to look ahead a short time to choose good actions. [sent-79, score-0.297]
</p><p>42 So, the robots can run a simple auction to determine their best joint action: each individual robot estimates its future cost for each action by a single-step backup from its value function. [sent-80, score-0.911]
</p><p>43 The difference between these future costs then tells the robot how much it is willing to bid for the right to execute each action. [sent-81, score-0.375]
</p><p>44 The optimal joint action is then the feasible action with the highest sum of bids. [sent-82, score-0.234]
</p><p>45 4  Example R3 R2 G Re R1  Figure 1: A simple example (left panel): the objective is to have all robots (R1,R2,R3) reach the goal (G) where they receive a reward. [sent-84, score-0.297]
</p><p>46 Any action may result in a robot becoming disabled, in which case it must be towed to the repair area (Re) to continue with the task. [sent-85, score-0.661]
</p><p>47 Each robot receives a large reward upon reaching the goal but incurs a small cost for each step it takes. [sent-88, score-0.377]
</p><p>48 Robots can break whenever they take a step, but a functioning robot may tow a failed robot to the repair area and the repaired robot may then proceed to the goal. [sent-89, score-1.268]
</p><p>49 Each robot has the action set A = { 8connected move, pickup for towing, request tow}. [sent-90, score-0.459]
</p><p>50 The state of each robot is its x position, its y position and its status {towing, going to goal, being towed, doing nothing}. [sent-91, score-0.371]
</p><p>51 The joint state space of all three robots is |Sjoint | = |S|3 and the joint action space is |A| = 103 . [sent-94, score-0.477]
</p><p>52 However, this problem lends itself to resource-based decomposition because the robots only interact through towing. [sent-96, score-0.411]
</p><p>53 Speciﬁcally, we design our Ci matrices to represent the constraint that the expected number of times a robot executes a pickup action at a position should be equal to the expected number of times some other robot executes a request-tow action. [sent-97, score-0.932]
</p><p>54 Thus, we have a weakly coupled MDP with robot interactions that can be modeled by linear constraints. [sent-98, score-0.467]
</p><p>55 5  Dantzig-Wolfe decomposition  We have reduced the multi-robot planning problem to the problem of solving the LP (3). [sent-104, score-0.384]
</p><p>56 So, one possible planning algorithm is just to pass this LP to a pre-packaged linear-program solver. [sent-105, score-0.303]
</p><p>57 This planning algorithm can be fairly efﬁcient, but it is completely centralized: each agent must communicate its entire dynamics to a central location and wait to receive its value function in return. [sent-106, score-0.374]
</p><p>58 This decomposition splits our original LP (3) into a master LP (4) and one slave LP (5) for each robot i. [sent-109, score-0.724]
</p><p>59 It then solves each slave program repeatedly, generating a new value for fi each time, and combines these solutions by inserting them into the master LP (Figure 2). [sent-110, score-0.538]
</p><p>60 Each slave LP is the same as the corresponding robot’s MDP except that it has different state-action costs; so, the robots can run standard MDP planners (which are often much faster than general LP solvers) to produce their plans. [sent-112, score-0.443]
</p><p>61 And, instead of sending whole MDPs and value functions back and forth, the Dantzig-Wolfe decomposition only needs to send resource prices and expected usages. [sent-113, score-0.692]
</p><p>62 The master program can be located on a separate agent, or on an arbitrary robot. [sent-114, score-0.247]
</p><p>63 In more detail, the master and slave LPs are: maxqi i cT Fi qi i (∗) Ci (Fi qi ) = d i ∀i : qi ≥ 0 ∀i : j qij = 1  (4)  maxfi (cT − pT Ci )fi i A i fi = b i fi ≥ 0  (5)  The master LP is the same as the original problem (3) except that fi has been replaced by Fi qi . [sent-115, score-1.236]
</p><p>64 Each column of Fi is one of the solutions fi which we have computed for the ith slave LP. [sent-116, score-0.313]
</p><p>65 ) So, i solving the master LP means ﬁnding a convex combination qi of the known solutions for each slave LP. [sent-118, score-0.399]
</p><p>66 The slave LP is the same as a single-robot planning problem (2) except that its costs have been altered by subtracting pT Ci . [sent-119, score-0.485]
</p><p>67 The vector p is the dual variable for the constraints (∗) from the last time we solved the master LP. [sent-120, score-0.297]
</p><p>68 6  An economic interpretation  We have described how to use the Dantzig-Wolfe decomposition to derive an efﬁcient distributed planning algorithm for loosely-coupled MDPs. [sent-122, score-0.442]
</p><p>69 Associated with each row of the constraint matrices Ci in the master program (4) is a dual variable; that is, there is one dual variable pj for each resource j. [sent-125, score-0.753]
</p><p>70 We can interpret this dual variable as a price for resource j. [sent-126, score-0.527]
</p><p>71 To see why, notice that the slave program charges robot i a cost of pj [Ci ]j,k each time it visits state-action pair k, and that visiting state-action pair k consumes an amount [Ci ]j,k of resource j. [sent-127, score-0.948]
</p><p>72 The Dantzig-Wolfe algorithm can be interpreted as a search for optimal resource prices. [sent-128, score-0.415]
</p><p>73 The master agent repeatedly asks the robots what they would do if the prices were p, then tries to combine their answers to produce a good plan for all the robots together. [sent-129, score-1.077]
</p><p>74 5  1 1  2  3  4  5  6  7  8  9  10  Iterations  Figure 3: Auctions for multi-robot path planning with limited fuel usage. [sent-134, score-0.561]
</p><p>75 Left to right: in an auction based on the assumption of cheap fuel, all robots go to the globally most tempting goal. [sent-135, score-0.427]
</p><p>76 If we assume very expensive fuel, each robot crashes through obstacles and goes to its closest goal. [sent-136, score-0.378]
</p><p>77 With the optimal fuel price, the auction trades goal quality against distance to achieve the best possible total cost. [sent-137, score-0.342]
</p><p>78 We then place 15 robots in random starting locations and ask them to plan paths to 10 random goals. [sent-143, score-0.348]
</p><p>79 Each robot can choose whichever goal it wants, but must pay a random goal-speciﬁc price. [sent-144, score-0.339]
</p><p>80 The robots are coupled through a constraint on fuel usage: there is a quadratic penalty on total path length. [sent-145, score-0.577]
</p><p>81 In this problem, our algorithm starts from an arbitrary initial guess at the value of a unit of fuel (which causes the individual robots to make poor policy decisions) and rapidly improves the estimated value by examining the individual robot plans. [sent-146, score-0.894]
</p><p>82 To demonstrate scaling, we used our learning algorithm to coordinate the robot towing problem in the simulation shown in ﬁgure 4, with a grid size of 300 × 300 and 9 robots. [sent-148, score-0.47]
</p><p>83 Many more robots could be handled, but because we only coordinated towing and not path  Figure 4: Left: an example of the output of the algorithm on a towing problem on a map generated using the robots on the right. [sent-149, score-0.945]
</p><p>84 Note that the nearest live robot (R1) tows the damaged robot to the repair area before heading to the goal. [sent-150, score-0.835]
</p><p>85 planning in this example, there was a bottleneck at the repair area due to the unmodeled coordination. [sent-153, score-0.46]
</p><p>86 Because our algorithm uses an arbitrary MDP planner as a subroutine, very large problems can be solved by combining our approach with fast planning algorithms. [sent-155, score-0.408]
</p><p>87 The rules of the game are that the last team standing wins and that it takes 4 hits to cause a robot to fail. [sent-157, score-0.376]
</p><p>88 There is a repair area to which a tagged teammate may be towed in order to repair it so that it may continue to play. [sent-158, score-0.475]
</p><p>89 Policies used are: do nothing, attack target i, coordinated attack (with a teammate) target i, tow teammate i, and be repaired. [sent-161, score-0.365]
</p><p>90 The objective of our multi-robot planner is to determine at a given time which ﬁxed policy each robot on the team should be executing so that the team will perform better. [sent-163, score-0.571]
</p><p>91 Coordination constraints are that any coordinated attacks or towing/repairing must be consistent: if teammate 1 requests a tow from teammate 2, then teammate 2 must perform a tow of teammate 1. [sent-164, score-0.81]
</p><p>92 Enemy positions are sampled from the particle ﬁlters at the beginning of each rollout and each policy is evaluated over several possible enemy position combinations to determine the performance of a policy. [sent-167, score-0.181]
</p><p>93 The robots replan at ﬁxed intervals; the simulation is halted while planning occurs. [sent-168, score-0.6]
</p><p>94 We compared our coordination planner to a similar planner without coordination. [sent-169, score-0.219]
</p><p>95 The coordinated planner won 48 of 50 games against the default behavior. [sent-172, score-0.226]
</p><p>96 Thus, the addition of coordination (via our factored planning algorithm) signiﬁcantly improved the performance. [sent-173, score-0.356]
</p><p>97 5  Conclusions  We have developed a decentralized method for solving large loosely-coupled multi-robot planning problems. [sent-174, score-0.408]
</p><p>98 Our algorithm works by ﬁnding an optimal solution to an approximate planning problem in which resource constraints hold only in expectation. [sent-175, score-0.764]
</p><p>99 We have applied our algorithm to multirobot towing, optimal use of fuel in a multi-robot path planning problem, and planning for multi-robot paintball. [sent-178, score-0.911]
</p><p>100 Optimizing schedules for prioritized path planning of multi-robot systems. [sent-184, score-0.34]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('resource', 0.39), ('robot', 0.339), ('planning', 0.303), ('robots', 0.297), ('fuel', 0.187), ('master', 0.182), ('prices', 0.179), ('mdp', 0.168), ('fi', 0.167), ('ci', 0.151), ('slave', 0.146), ('lp', 0.137), ('teammate', 0.131), ('towing', 0.131), ('auction', 0.13), ('repair', 0.112), ('mdps', 0.106), ('resources', 0.102), ('tow', 0.094), ('action', 0.09), ('market', 0.089), ('planner', 0.083), ('decentralized', 0.081), ('enemy', 0.075), ('towed', 0.075), ('agent', 0.071), ('dual', 0.069), ('price', 0.068), ('robotics', 0.067), ('fa', 0.065), ('consumption', 0.065), ('production', 0.065), ('decomposition', 0.057), ('interact', 0.057), ('coupled', 0.056), ('multirobot', 0.056), ('centralized', 0.055), ('gi', 0.055), ('coordination', 0.053), ('coordinated', 0.052), ('plan', 0.051), ('economic', 0.049), ('cooperative', 0.049), ('paint', 0.049), ('policy', 0.049), ('qi', 0.047), ('interactions', 0.047), ('constraints', 0.046), ('usage', 0.045), ('area', 0.045), ('attack', 0.044), ('ta', 0.044), ('program', 0.043), ('obstacles', 0.039), ('send', 0.039), ('reward', 0.038), ('dias', 0.037), ('icra', 0.037), ('maxfi', 0.037), ('visitation', 0.037), ('plans', 0.037), ('team', 0.037), ('path', 0.037), ('costs', 0.036), ('bellman', 0.034), ('ct', 0.034), ('limited', 0.034), ('distributed', 0.033), ('default', 0.033), ('usages', 0.033), ('consume', 0.033), ('burgard', 0.033), ('phrased', 0.033), ('state', 0.032), ('gordon', 0.032), ('particle', 0.031), ('games', 0.03), ('consumes', 0.03), ('pickup', 0.03), ('loosely', 0.03), ('automation', 0.03), ('joint', 0.029), ('design', 0.028), ('won', 0.028), ('expected', 0.027), ('executes', 0.026), ('observability', 0.026), ('guestrin', 0.026), ('determine', 0.026), ('optimal', 0.025), ('simulator', 0.025), ('weakly', 0.025), ('solving', 0.024), ('robotic', 0.024), ('geoffrey', 0.024), ('sebastian', 0.023), ('nothing', 0.022), ('arbitrary', 0.022), ('frequencies', 0.021), ('allocation', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="36-tfidf-1" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>Author: Curt Bererton, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 1</p><p>2 0.17970926 <a title="36-tfidf-2" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>Author: Natalia H. Gardiol, Leslie P. Kaelbling</p><p>Abstract: A mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes. Indeed, almost all interesting sequential decision-making domains involve large state spaces and large, stochastic action sets. We investigate a way to act intelligently as quickly as possible in domains where ﬁnding a complete policy would take a hopelessly long time. This approach, Relational Envelopebased Planning (REBP) tackles large, noisy problems along two axes. First, describing a domain as a relational MDP (instead of as an atomic or propositionally-factored MDP) allows problem structure and dynamics to be captured compactly with a small set of probabilistic, relational rules. Second, an envelope-based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit. 1</p><p>3 0.1764545 <a title="36-tfidf-3" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efﬁciently.</p><p>4 0.15523018 <a title="36-tfidf-4" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>5 0.14047423 <a title="36-tfidf-5" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>6 0.13299403 <a title="36-tfidf-6" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>7 0.12272006 <a title="36-tfidf-7" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>8 0.10703656 <a title="36-tfidf-8" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>9 0.10691212 <a title="36-tfidf-9" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>10 0.10181134 <a title="36-tfidf-10" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>11 0.10042307 <a title="36-tfidf-11" href="./nips-2003-An_MDP-Based_Approach_to_Online_Mechanism_Design.html">26 nips-2003-An MDP-Based Approach to Online Mechanism Design</a></p>
<p>12 0.099581569 <a title="36-tfidf-12" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>13 0.093246028 <a title="36-tfidf-13" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>14 0.083363347 <a title="36-tfidf-14" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>15 0.0815209 <a title="36-tfidf-15" href="./nips-2003-Learning_Near-Pareto-Optimal_Conventions_in_Polynomial_Time.html">105 nips-2003-Learning Near-Pareto-Optimal Conventions in Polynomial Time</a></p>
<p>16 0.080833763 <a title="36-tfidf-16" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>17 0.080467209 <a title="36-tfidf-17" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>18 0.078815259 <a title="36-tfidf-18" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>19 0.072994716 <a title="36-tfidf-19" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>20 0.071469508 <a title="36-tfidf-20" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.18), (1, 0.25), (2, -0.107), (3, 0.034), (4, -0.046), (5, -0.057), (6, -0.144), (7, -0.094), (8, 0.058), (9, 0.017), (10, 0.053), (11, -0.123), (12, -0.02), (13, 0.13), (14, -0.019), (15, 0.085), (16, 0.095), (17, -0.051), (18, -0.024), (19, 0.085), (20, 0.008), (21, -0.041), (22, -0.098), (23, -0.001), (24, 0.126), (25, 0.045), (26, -0.118), (27, 0.015), (28, -0.064), (29, 0.057), (30, 0.075), (31, -0.165), (32, -0.056), (33, -0.004), (34, 0.233), (35, -0.133), (36, -0.152), (37, 0.012), (38, -0.017), (39, 0.126), (40, -0.111), (41, -0.053), (42, 0.119), (43, -0.019), (44, 0.046), (45, 0.059), (46, -0.09), (47, -0.007), (48, 0.054), (49, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97225714 <a title="36-lsi-1" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>Author: Curt Bererton, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 1</p><p>2 0.82134169 <a title="36-lsi-2" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>Author: Maxim Likhachev, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: In real world planning problems, time for deliberation is often limited. Anytime planners are well suited for these problems: they ﬁnd a feasible solution quickly and then continually work on improving it until time runs out. In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time. It starts by ﬁnding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows. Given enough time it ﬁnds a provably optimal solution. While improving its bound, ARA* reuses previous search efforts and, as a result, is signiﬁcantly more efﬁcient than other anytime search methods. In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover. 1</p><p>3 0.73981571 <a title="36-lsi-3" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>Author: David Ferguson, Aaron Morris, Dirk Hähnel, Christopher Baker, Zachary Omohundro, Carlos Reverte, Scott Thayer, Charles Whittaker, William Whittaker, Wolfram Burgard, Sebastian Thrun</p><p>Abstract: We present the software architecture of a robotic system for mapping abandoned mines. The software is capable of acquiring consistent 2D maps of large mines with many cycles, represented as Markov random £elds. 3D C-space maps are acquired from local 3D range scans, which are used to identify navigable paths using A* search. Our system has been deployed in three abandoned mines, two of which inaccessible to people, where it has acquired maps of unprecedented detail and accuracy. 1</p><p>4 0.59806609 <a title="36-lsi-4" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>Author: Natalia H. Gardiol, Leslie P. Kaelbling</p><p>Abstract: A mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes. Indeed, almost all interesting sequential decision-making domains involve large state spaces and large, stochastic action sets. We investigate a way to act intelligently as quickly as possible in domains where ﬁnding a complete policy would take a hopelessly long time. This approach, Relational Envelopebased Planning (REBP) tackles large, noisy problems along two axes. First, describing a domain as a relational MDP (instead of as an atomic or propositionally-factored MDP) allows problem structure and dynamics to be captured compactly with a small set of probabilistic, relational rules. Second, an envelope-based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit. 1</p><p>5 0.47567087 <a title="36-lsi-5" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efﬁciently.</p><p>6 0.4708176 <a title="36-lsi-6" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>7 0.42089847 <a title="36-lsi-7" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>8 0.39214298 <a title="36-lsi-8" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>9 0.36091989 <a title="36-lsi-9" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>10 0.34755218 <a title="36-lsi-10" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>11 0.33375874 <a title="36-lsi-11" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>12 0.32004723 <a title="36-lsi-12" href="./nips-2003-Learning_Near-Pareto-Optimal_Conventions_in_Polynomial_Time.html">105 nips-2003-Learning Near-Pareto-Optimal Conventions in Polynomial Time</a></p>
<p>13 0.30853978 <a title="36-lsi-13" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>14 0.30381969 <a title="36-lsi-14" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>15 0.30373904 <a title="36-lsi-15" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>16 0.29615527 <a title="36-lsi-16" href="./nips-2003-Can_We_Learn_to_Beat_the_Best_Stock.html">44 nips-2003-Can We Learn to Beat the Best Stock</a></p>
<p>17 0.28934878 <a title="36-lsi-17" href="./nips-2003-Bounded_Invariance_and_the_Formation_of_Place_Fields.html">43 nips-2003-Bounded Invariance and the Formation of Place Fields</a></p>
<p>18 0.28750587 <a title="36-lsi-18" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>19 0.2720978 <a title="36-lsi-19" href="./nips-2003-An_MDP-Based_Approach_to_Online_Mechanism_Design.html">26 nips-2003-An MDP-Based Approach to Online Mechanism Design</a></p>
<p>20 0.26658154 <a title="36-lsi-20" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.07), (11, 0.026), (21, 0.319), (29, 0.02), (30, 0.03), (35, 0.074), (53, 0.066), (69, 0.018), (71, 0.034), (76, 0.025), (82, 0.016), (85, 0.092), (91, 0.106), (99, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81315839 <a title="36-lda-1" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>Author: Curt Bererton, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 1</p><p>2 0.72522593 <a title="36-lda-2" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>3 0.50024569 <a title="36-lda-3" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>4 0.49481162 <a title="36-lda-4" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>5 0.49362972 <a title="36-lda-5" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>6 0.48677948 <a title="36-lda-6" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>7 0.48177296 <a title="36-lda-7" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>8 0.48170939 <a title="36-lda-8" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>9 0.48142469 <a title="36-lda-9" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>10 0.48051703 <a title="36-lda-10" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>11 0.48030791 <a title="36-lda-11" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>12 0.47991845 <a title="36-lda-12" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>13 0.47979027 <a title="36-lda-13" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>14 0.47904032 <a title="36-lda-14" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>15 0.4713583 <a title="36-lda-15" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>16 0.46984458 <a title="36-lda-16" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>17 0.46868852 <a title="36-lda-17" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>18 0.4686684 <a title="36-lda-18" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>19 0.46826756 <a title="36-lda-19" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>20 0.46826422 <a title="36-lda-20" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
