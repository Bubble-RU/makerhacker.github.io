<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-39" href="#">nips2003-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</h1>
<br/><p>Source: <a title="nips-2003-39-pdf" href="http://papers.nips.cc/paper/2426-bayesian-color-constancy-with-non-gaussian-models.pdf">pdf</a></p><p>Author: Charles Rosenberg, Alok Ladsariya, Tom Minka</p><p>Abstract: We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.</p><p>Reference: <a title="nips-2003-39-reference" href="../nips2003_reference/nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. [sent-7, score-0.769]
</p><p>2 The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. [sent-8, score-0.131]
</p><p>3 The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. [sent-9, score-1.016]
</p><p>4 This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base. [sent-10, score-0.191]
</p><p>5 Because the illuminants in the world have varying colors, the measured color of an object will change under different light sources. [sent-12, score-0.656]
</p><p>6 We propose an algorithm for color constancy which, given an image, will automatically estimate the color of the illuminant (assumed constant over the image), allowing the image to be color corrected. [sent-13, score-1.95]
</p><p>7 This color constancy problem is ill-posed, because object color and illuminant color are not uniquely separable. [sent-14, score-1.864]
</p><p>8 Historically, algorithms for color constancy have fallen into two groups. [sent-15, score-0.683]
</p><p>9 The second group uses a statistical model to quantify the probability of each illuminant and then makes an estimate from these probabilities. [sent-17, score-0.469]
</p><p>10 But as shown by [3, 1], currently the best performance on real images is achieved by gamut mapping, a constraint-based algorithm. [sent-19, score-0.479]
</p><p>11 And, in the words of some leading researchers, even gamut mapping is not “good enough” for object recognition [8]. [sent-20, score-0.487]
</p><p>12 In this paper, we show that it is possible to outperform gamut mapping with a statistical approach, by using appropriate probability models with the appropriate statistical framework. [sent-21, score-0.538]
</p><p>13 We use the principled Bayesian color constancy framework of [4], but combine it with rich, nonparametric image models, such as used by Color by Correlation [1]. [sent-22, score-0.769]
</p><p>14 Even though our algorithm outperforms gamut mapping on average, there are cases in which gamut mapping provides better estimates, and, in fact, the errors of the two methods are surprisingly uncorrelated. [sent-25, score-0.97]
</p><p>15 This is an interesting result, because it suggests that gamut mapping exploits image properties which are different from what is learned by our algorithm, and probably other statistical algorithms. [sent-26, score-0.575]
</p><p>16 2 The imaging model Our approach is to model the observed image pixels with a probabilistic generative model, decomposing them as the product of unknown surface reﬂectances with an unknown illuminant. [sent-28, score-0.253]
</p><p>17 Let y be an image pixel with three color channels: (yr , yg , yb ). [sent-32, score-0.64]
</p><p>18 The pixel is assumed to be the result of light reﬂecting off of a surface under the Lambertian reﬂectance model. [sent-33, score-0.139]
</p><p>19 Denote the power of the light in each channel by = ( r , g , b ), with each channel ranging from zero to inﬁnity. [sent-34, score-0.188]
</p><p>20 Denote this reﬂectance by x = (xr , xg , xb ), with each channel ranging from zero to one. [sent-36, score-0.128]
</p><p>21 The model for the pixel is the well-known diagonal lighting model: yr = r xr yg = g xg yb = b xb (1) To simplify the equations below, we write this in matrix form as L y  = diag( ) = Lx  (2) (3)  This speciﬁes the conditional distribution p(y| , x). [sent-37, score-0.342]
</p><p>22 The prior distribution for the illuminant (p( )) will be uniform over a constraint set, described later in section 5. [sent-40, score-0.463]
</p><p>23 The most difﬁcult step is to construct a model for the surface reﬂectances in an image containing many pixels: Y = X =  (y(1), . [sent-42, score-0.141]
</p><p>24 If mk is the probability of a surface having a reﬂectance value in bin k, so that k mk = 1,  then independence says f (n1 , . [sent-63, score-0.279]
</p><p>25 , nK ) ≈ (smk Γ(nk )) (9) sΓ(n) k  0 if nk = 0 1 if nk > 0  clip(nk ) =  (10)  This resembles a multinomial distribution on clipped counts. [sent-75, score-0.443]
</p><p>26 Unfortunately, this distribution strongly prefers that the image contains a small number of different reﬂectances, which biases the light source estimate. [sent-76, score-0.157]
</p><p>27 , nK ) =  (11)  clip(nk ) k clip(nk )  (12)  k  = n  νk  The modiﬁed counts νk sum to n just like the original counts nk , but are distributed equally over all reﬂectances present in the image. [sent-80, score-0.254]
</p><p>28 3 The color constancy algorithm The algorithm for estimating the illuminant has two parts: (1) discretize the set of all illuminants on a ﬁne grid and compute their likelihood and (2) pick the illuminant which minimizes the risk. [sent-81, score-1.865]
</p><p>29 The likelihood of the observed image data Y for a given illuminant is p(Y| )  p(y(i)| , x(i)) p(X)dX  = X  i −1 n  | p(X = L−1 Y)  = |L  (13) (14)  −1  The quantity L Y can be understood as the color-corrected image. [sent-82, score-0.555]
</p><p>30 The determinant term, 1/( r g b )n , makes this a valid distribution over Y and has the effect of introducing a preference for dimmer illuminants independently of the prior on reﬂectances. [sent-83, score-0.224]
</p><p>31 An answer that the illuminant is ∗ , when it is really , incurs some cost, denoted R( ∗ | ). [sent-85, score-0.444]
</p><p>32 Let this function be quadratic in some transformation g of the illuminant vector : R( ∗ | ) =  ||g( ∗ ) − g( )||2  (18)  This occurs, for example, when the cost function is squared error in chromaticity. [sent-86, score-0.469]
</p><p>33 4 Relation to other algorithms In this section we describe related color constancy algorithms using the framework of the imaging model introduced in section 2. [sent-88, score-0.744]
</p><p>34 Scale by max The scale by max algorithm (as tested e. [sent-95, score-0.126]
</p><p>35 in [3]) estimates the illuminant by the simple formula r  = max yr (i) i  g  = max yg (i)  b  i  = max yb (i) i  (20)  which is the dimmest illuminant in the valid set (15). [sent-97, score-1.224]
</p><p>36 Then p(X) is constant and the maximum-likelihood illuminant is (20). [sent-99, score-0.444]
</p><p>37 Gray-world The gray-world algorithm [5] chooses the illuminant such that the average value in each channel of the corrected image is a constant, e. [sent-101, score-0.624]
</p><p>38 Let the reﬂectances be independent for each pixel and each channel, with distribution p(xc ) ∝ exp(−2xc ) in each channel c. [sent-106, score-0.109]
</p><p>39 (21)  whose maximum is (as desired) c  =  2 n  yc (i) i  (22)  Figure 1: Plots of slices of the three dimensional color surface reﬂectance distribution along a single dimension. [sent-108, score-0.507]
</p><p>40 Row one plots green versus blue with 0,0 at the upper left of each subplot and slices in red whose magnitude increases from left to right. [sent-109, score-0.149]
</p><p>41 Row two plots red versus blue with slices in green. [sent-110, score-0.131]
</p><p>42 Row three plots red versus green with slices in blue. [sent-111, score-0.131]
</p><p>43 Instead, observed pixels are quantized into color bins, and the frequency of each bin is counted for each illuminant, in a ﬁnite set of illuminants. [sent-113, score-0.472]
</p><p>44 ) Let mk ( ) be the frequency of color bin k for illuminant , and let n1 · · · nK be the color histogram of the image, then the likelihood of is computed as mk ( )clip(nk )  p(Y| ) =  (23)  k  While theoretically this is very general, there are practical limitations. [sent-115, score-1.432]
</p><p>45 One must learn the color frequencies for every possible illuminant. [sent-117, score-0.357]
</p><p>46 Since collecting real-world data whose illuminant is known is difﬁcult, mk ( ) is typically trained synthetically with random surfaces, which may not represent the statistics of natural scenes. [sent-118, score-0.539]
</p><p>47 The second issue is that colors and illuminants live in an unbounded 3D space [1], unlike reﬂectances which are bounded. [sent-119, score-0.227]
</p><p>48 In order to store a color distribution for each illuminant, brightness variation needs to be artiﬁcially bounded. [sent-120, score-0.403]
</p><p>49 To reduce the storage of the mk ( )’s, Barnard et al [1] store the color distribution only for illuminants of a ﬁxed brightness. [sent-122, score-0.676]
</p><p>50 The other part of the bias is due to using clipped counts in the likelihood. [sent-124, score-0.107]
</p><p>51 As explained in section 2, a multinomial likelihood with clipped counts is a special case of the Dirichlet-multinomial, and prefers images with a small number of different colors. [sent-125, score-0.258]
</p><p>52 1 Reﬂectance Distribution To implement the Bayesian algorithm, we need to learn the real-world frequencies mk of quantized reﬂectance vectors. [sent-128, score-0.125]
</p><p>53 The direct approach to this would require a set of images with ground truth information regarding the associated illumination parameters or, alternately, a set of images captured under a canonical illuminant and camera. [sent-129, score-0.759]
</p><p>54 Unfortunately, it is quite difﬁcult to collect a large number of images under controlled conditions. [sent-130, score-0.103]
</p><p>55 The estimates from some “base” color constancy algorithm are used as a proxy for the ground truth. [sent-132, score-0.737]
</p><p>56 We used approximately 2300 randomly selected JPEG images from news sites on the web for bootstrapping, consisting mostly of outdoor scenes, indoor news conferences, and sporting event scenes. [sent-135, score-0.151]
</p><p>57 A new image is formed in which each pixel is the mean of an m by m block of the original image. [sent-143, score-0.124]
</p><p>58 The second pre-processing step removes dark pixels from the computation, which, because of noise and quantization effects do not contain reliable color information. [sent-144, score-0.469]
</p><p>59 Pixels whose yr + yg + yb channel sum is less than a given threshold are excluded from the computation. [sent-145, score-0.293]
</p><p>60 In addition to the reﬂectance prior, the parameters of our algorithm are: the number of reﬂectance histogram bins, the scale down factor, and the dark pixel threshold value. [sent-146, score-0.131]
</p><p>61 (The “ball2” object was removed so that there was no overlap between the tuning and test sets. [sent-150, score-0.134]
</p><p>62 ) For the purpose of speed, only images captured with the Philips Ultralume and the Macbeth Judge II ﬂuorescent illuminants were included. [sent-151, score-0.329]
</p><p>63 The best set of parameters was found to be: 32 × 32 × 32 reﬂectance bins, scale down by m = 3, and omit pixels with a channel sum less than 8/(3 × 255). [sent-152, score-0.149]
</p><p>64 3 Illuminant prior To facilitate a direct comparison, we adopt the two illuminant priors from [3]. [sent-154, score-0.463]
</p><p>65 The ﬁrst prior, full set, discretizes the illuminants uniformly in polar coordinates. [sent-156, score-0.205]
</p><p>66 The second prior, hull set, is a subset of full set restricted to be within the convex hull of the test set illuminants and other real world illuminants. [sent-157, score-0.385]
</p><p>67 1 Evaluation Speciﬁcs To test the algorithms we use the publicly available real world image data set [2] used by Barnard, Martin, Coath and Funt in a comprehensive evaluation of color constancy algorithms in [3]. [sent-161, score-0.857]
</p><p>68 The data set consists of images of 30 scenes captured under 11 light sources, for a total of 321 images (after the authors removed images which had collection problems) with ground truth illuminant information provided in the form of an RGB value. [sent-162, score-0.894]
</p><p>69 As in the “rg error” measure of [3], illuminant error is measured in chromaticity space: (24) 1 = r /( r + g + b ) 2 = g /( r + g + b ) R( ∗ | ) = ( ∗ − 1 )2 + ( ∗ − 2 )2 (25) 1 2 The Bayesian algorithm is adapted to minimize this risk by computing the posterior mean in chromaticity space. [sent-163, score-0.615]
</p><p>70 Table 1: The average error of several color constancy algorithms on the test set. [sent-165, score-0.733]
</p><p>71 We compare two versions of our Bayesian method to the gamut mapping and scale by max algorithms. [sent-200, score-0.549]
</p><p>72 The appropriate preprocessing for each algorithm was applied to the images to achieve the best possible performance. [sent-201, score-0.15]
</p><p>73 (Note that we do not include results for color by correlation since the gamut mapping results were found to be signiﬁcantly better in [3]. [sent-202, score-0.87]
</p><p>74 ) In all conﬁgurations, our algorithm exhibits the lowest RMS error except in a single case where it is not statistically different than that of gamut mapping. [sent-203, score-0.424]
</p><p>75 The tuning set, while composed of separate images than the test set, is very similar and has known illuminants, and, accordingly, gives the best results. [sent-207, score-0.169]
</p><p>76 1, is not that different, particularly when the illuminant search is constrained. [sent-209, score-0.444]
</p><p>77 The gamut mapping algorithm (called CRULE and ECRULE in [3]) is also presented in two versions: with and without segmenting the images as a preprocessing step as described in [3]. [sent-210, score-0.614]
</p><p>78 In the evaluation of color constancy algorithms in [3] gamut mapping was found on average to outperform all other algorithms when evaluated on real world images. [sent-212, score-1.196]
</p><p>79 It is interesting to note that the gamut mapping algorithm is sensitive to segmentation. [sent-213, score-0.487]
</p><p>80 Since fundamentally it should not be sensitive to the number of pixels of a particular color in the image we must assume that this is because the segmentation is implementing some form of noise ﬁltering. [sent-214, score-0.537]
</p><p>81 Scale by max is also included as a reference point and still performs quite well given its simplicity, often beating out much more complex constancy algorithms [8, 3]. [sent-216, score-0.364]
</p><p>82 Its performance is the same for both illuminant sets since it does not involve a search over illuminants. [sent-217, score-0.444]
</p><p>83 edu/˜chuck/nips-2003/  Surprisingly, when the error of the Bayesian method is compared with the gamut mapping method on individual test images, the correlation coefﬁcient is -0. [sent-221, score-0.563]
</p><p>84 Thus the images which confuse the Bayesian method are quite different from the images which confuse gamut mapping. [sent-223, score-0.632]
</p><p>85 This suggests that an algorithm which could jointly model the image properties exploited by both algorithms might give dramatic improvements. [sent-224, score-0.109]
</p><p>86 As an example of the potential improvement, the RMS error of an ideal algorithm whose error is the minimum of Bayes and gamut on each image in the test set is only 0. [sent-225, score-0.56]
</p><p>87 7 Conclusions and Future Work We have demonstrated empirically that Bayesian color constancy with the appropriate nonGaussian models can outperform gamut mapping on a standard test set. [sent-227, score-1.196]
</p><p>88 This is true regardless of whether a calibrated or uncalibrated training set is used, or whether the full set or a restricted set of illuminants is searched. [sent-228, score-0.25]
</p><p>89 This should give new hope to the pursuit of statistical methods as a unifying framework for color constancy. [sent-229, score-0.401]
</p><p>90 This is simply an image modeling problem which can be attacked using standard statistical methods. [sent-232, score-0.111]
</p><p>91 A particularly promising direction is to pursue models which can enforce constraints like that in the gamut mapping algorithm, since the images where Bayes has the largest errors appear to be relatively easy for gamut mapping. [sent-233, score-0.943]
</p><p>92 Acknowledgments We would like to thank Kobus Barnard for making his test images and code publicly available. [sent-234, score-0.166]
</p><p>93 Funt, “Colour by correlation in a three dimensional colour space,” Proceedings of the 6th European Conference on Computer Vision, pp. [sent-240, score-0.144]
</p><p>94 Funt, “A comparison of color constancy algorithms; Part Two. [sent-254, score-0.683]
</p><p>95 Freeman, “Bayesian color constancy,” Journal of the Optical Society of America A, vol. [sent-265, score-0.357]
</p><p>96 Buchsbaum, “A spatial processor model for object colour perception,” Journal of the Franklin Institute, vol. [sent-270, score-0.118]
</p><p>97 Hubel, “Colour by correlation: a simple, unifying approach to colour constancy,” The Proceedings of the Seventh IEEE International Conference on Computer Vision, vol. [sent-279, score-0.114]
</p><p>98 Barnard, “Learning color constancy,” Proceedings of Imaging Science and Technology / Society for Information Display Fourth Color Imaging Conference. [sent-285, score-0.357]
</p><p>99 “Bootstrapping color constancy,” Proceedings of SPIE: Electronic Imaging IV, 3644, 1999. [sent-297, score-0.357]
</p><p>100 Vrhel, “Estimation of illumination for color correction,” Proc ICASSP, pp. [sent-302, score-0.395]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('illuminant', 0.444), ('gamut', 0.376), ('color', 0.357), ('constancy', 0.326), ('ectances', 0.256), ('ectance', 0.223), ('illuminants', 0.205), ('nk', 0.186), ('funt', 0.12), ('barnard', 0.113), ('images', 0.103), ('re', 0.1), ('colour', 0.095), ('mk', 0.095), ('mapping', 0.088), ('image', 0.086), ('clip', 0.085), ('yg', 0.085), ('yb', 0.074), ('channel', 0.071), ('hull', 0.065), ('rms', 0.063), ('bayesian', 0.063), ('yr', 0.063), ('imaging', 0.061), ('surface', 0.055), ('pixels', 0.051), ('chromaticity', 0.051), ('clipped', 0.051), ('coath', 0.051), ('smk', 0.051), ('slices', 0.05), ('correlation', 0.049), ('light', 0.046), ('uncalibrated', 0.045), ('yc', 0.045), ('bins', 0.043), ('martin', 0.043), ('segmentation', 0.043), ('tuning', 0.041), ('bayes', 0.039), ('pixel', 0.038), ('illumination', 0.038), ('publicly', 0.038), ('max', 0.038), ('bootstrapping', 0.036), ('counts', 0.034), ('lambertian', 0.034), ('xg', 0.034), ('bin', 0.034), ('bootstrap', 0.032), ('ground', 0.031), ('quantized', 0.03), ('brightness', 0.027), ('scale', 0.027), ('base', 0.026), ('quantization', 0.025), ('confuse', 0.025), ('prefers', 0.025), ('xr', 0.025), ('statistical', 0.025), ('histogram', 0.025), ('error', 0.025), ('test', 0.025), ('world', 0.025), ('likelihood', 0.025), ('outperform', 0.024), ('news', 0.024), ('nongaussian', 0.024), ('carnegie', 0.024), ('mellon', 0.024), ('pittsburgh', 0.024), ('removed', 0.024), ('red', 0.024), ('preprocessing', 0.024), ('object', 0.023), ('algorithm', 0.023), ('xb', 0.023), ('bias', 0.022), ('colors', 0.022), ('posterior', 0.021), ('captured', 0.021), ('overlap', 0.021), ('minka', 0.021), ('plots', 0.02), ('versions', 0.02), ('multinomial', 0.02), ('unifying', 0.019), ('truth', 0.019), ('store', 0.019), ('prior', 0.019), ('pa', 0.019), ('versus', 0.019), ('surprisingly', 0.019), ('vision', 0.018), ('removes', 0.018), ('green', 0.018), ('blue', 0.018), ('dark', 0.018), ('grid', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="39-tfidf-1" href="./nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models.html">39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</a></p>
<p>Author: Charles Rosenberg, Alok Ladsariya, Tom Minka</p><p>Abstract: We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.</p><p>2 0.11763185 <a title="39-tfidf-2" href="./nips-2003-Unsupervised_Color_Decomposition_Of_Histologically_Stained_Tissue_Samples.html">190 nips-2003-Unsupervised Color Decomposition Of Histologically Stained Tissue Samples</a></p>
<p>Author: Andrew Rabinovich, Sameer Agarwal, Casey Laris, Jeffrey H. Price, Serge J. Belongie</p><p>Abstract: Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the ﬁrst automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results. 1</p><p>3 0.10854274 <a title="39-tfidf-3" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<p>Author: Lyndsey C. Pickup, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: Super-resolution aims to produce a high-resolution image from a set of one or more low-resolution images by recovering or inventing plausible high-frequency image content. Typical approaches try to reconstruct a high-resolution image using the sub-pixel displacements of several lowresolution images, usually regularized by a generic smoothness prior over the high-resolution image space. Other methods use training data to learn low-to-high-resolution matches, and have been highly successful even in the single-input-image case. Here we present a domain-speciﬁc image prior in the form of a p.d.f. based upon sampled images, and show that for certain types of super-resolution problems, this sample-based prior gives a signiﬁcant improvement over other common multiple-image super-resolution techniques. 1</p><p>4 0.087939821 <a title="39-tfidf-4" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>Author: Victor Lavrenko, R. Manmatha, Jiwoon Jeon</p><p>Abstract: We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model signiﬁcantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval. 1</p><p>5 0.079203203 <a title="39-tfidf-5" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>Author: Michael J. Quinlan, Stephan K. Chalup, Richard H. Middleton</p><p>Abstract: This article addresses the issues of colour classiﬁcation and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classiﬁcation with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse ﬁtting for colour classiﬁcation and the statistical approach used for collision detection.</p><p>6 0.062548704 <a title="39-tfidf-6" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>7 0.053157602 <a title="39-tfidf-7" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<p>8 0.048233669 <a title="39-tfidf-8" href="./nips-2003-Bias-Corrected_Bootstrap_and_Model_Uncertainty.html">40 nips-2003-Bias-Corrected Bootstrap and Model Uncertainty</a></p>
<p>9 0.044154696 <a title="39-tfidf-9" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>10 0.042377286 <a title="39-tfidf-10" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>11 0.04143266 <a title="39-tfidf-11" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>12 0.041303705 <a title="39-tfidf-12" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>13 0.038283244 <a title="39-tfidf-13" href="./nips-2003-Reconstructing_MEG_Sources_with_Unknown_Correlations.html">166 nips-2003-Reconstructing MEG Sources with Unknown Correlations</a></p>
<p>14 0.037820317 <a title="39-tfidf-14" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>15 0.037648018 <a title="39-tfidf-15" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>16 0.037608869 <a title="39-tfidf-16" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<p>17 0.036126133 <a title="39-tfidf-17" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>18 0.035211489 <a title="39-tfidf-18" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>19 0.034992281 <a title="39-tfidf-19" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>20 0.032996919 <a title="39-tfidf-20" href="./nips-2003-The_Diffusion-Limited_Biochemical_Signal-Relay_Channel.html">184 nips-2003-The Diffusion-Limited Biochemical Signal-Relay Channel</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.121), (1, -0.027), (2, 0.027), (3, -0.018), (4, -0.114), (5, -0.028), (6, 0.047), (7, -0.035), (8, -0.002), (9, -0.019), (10, -0.043), (11, 0.004), (12, -0.128), (13, 0.093), (14, -0.032), (15, -0.049), (16, -0.06), (17, -0.003), (18, -0.04), (19, 0.127), (20, 0.035), (21, 0.087), (22, -0.024), (23, -0.063), (24, -0.053), (25, 0.063), (26, -0.02), (27, 0.079), (28, 0.05), (29, -0.033), (30, 0.047), (31, -0.12), (32, 0.182), (33, -0.082), (34, 0.1), (35, 0.055), (36, -0.023), (37, 0.041), (38, 0.086), (39, -0.041), (40, 0.009), (41, 0.133), (42, -0.03), (43, -0.035), (44, 0.058), (45, 0.112), (46, 0.051), (47, -0.038), (48, -0.09), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93601221 <a title="39-lsi-1" href="./nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models.html">39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</a></p>
<p>Author: Charles Rosenberg, Alok Ladsariya, Tom Minka</p><p>Abstract: We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.</p><p>2 0.7166155 <a title="39-lsi-2" href="./nips-2003-Unsupervised_Color_Decomposition_Of_Histologically_Stained_Tissue_Samples.html">190 nips-2003-Unsupervised Color Decomposition Of Histologically Stained Tissue Samples</a></p>
<p>Author: Andrew Rabinovich, Sameer Agarwal, Casey Laris, Jeffrey H. Price, Serge J. Belongie</p><p>Abstract: Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the ﬁrst automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results. 1</p><p>3 0.68428802 <a title="39-lsi-3" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<p>Author: Lyndsey C. Pickup, Stephen J. Roberts, Andrew Zisserman</p><p>Abstract: Super-resolution aims to produce a high-resolution image from a set of one or more low-resolution images by recovering or inventing plausible high-frequency image content. Typical approaches try to reconstruct a high-resolution image using the sub-pixel displacements of several lowresolution images, usually regularized by a generic smoothness prior over the high-resolution image space. Other methods use training data to learn low-to-high-resolution matches, and have been highly successful even in the single-input-image case. Here we present a domain-speciﬁc image prior in the form of a p.d.f. based upon sampled images, and show that for certain types of super-resolution problems, this sample-based prior gives a signiﬁcant improvement over other common multiple-image super-resolution techniques. 1</p><p>4 0.53072155 <a title="39-lsi-4" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>Author: Victor Lavrenko, R. Manmatha, Jiwoon Jeon</p><p>Abstract: We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model signiﬁcantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval. 1</p><p>5 0.49932688 <a title="39-lsi-5" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>Author: Koji Tsuda, Gunnar Rätsch</p><p>Abstract: A common way of image denoising is to project a noisy image to the subspace of admissible images made for instance by PCA. However, a major drawback of this method is that all pixels are updated by the projection, even when only a few pixels are corrupted by noise or occlusion. We propose a new method to identify the noisy pixels by 1 -norm penalization and update the identiﬁed pixels only. The identiﬁcation and updating of noisy pixels are formulated as one linear program which can be solved efﬁciently. Especially, one can apply the ν-trick to directly specify the fraction of pixels to be reconstructed. Moreover, we extend the linear program to be able to exploit prior knowledge that occlusions often appear in contiguous blocks (e.g. sunglasses on faces). The basic idea is to penalize boundary points and interior points of the occluded area differently. We are able to show the ν-property also for this extended LP leading a method which is easy to use. Experimental results impressively demonstrate the power of our approach.</p><p>6 0.43854862 <a title="39-lsi-6" href="./nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F.html">195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</a></p>
<p>7 0.38604867 <a title="39-lsi-7" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>8 0.38379329 <a title="39-lsi-8" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>9 0.3781637 <a title="39-lsi-9" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>10 0.36206582 <a title="39-lsi-10" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>11 0.31112862 <a title="39-lsi-11" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>12 0.30581009 <a title="39-lsi-12" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<p>13 0.30461895 <a title="39-lsi-13" href="./nips-2003-A_Mixed-Signal_VLSI_for_Real-Time_Generation_of_Edge-Based_Image_Vectors.html">11 nips-2003-A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors</a></p>
<p>14 0.2868692 <a title="39-lsi-14" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>15 0.28217101 <a title="39-lsi-15" href="./nips-2003-Bias-Corrected_Bootstrap_and_Model_Uncertainty.html">40 nips-2003-Bias-Corrected Bootstrap and Model Uncertainty</a></p>
<p>16 0.281335 <a title="39-lsi-16" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>17 0.26566419 <a title="39-lsi-17" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>18 0.26399285 <a title="39-lsi-18" href="./nips-2003-Reconstructing_MEG_Sources_with_Unknown_Correlations.html">166 nips-2003-Reconstructing MEG Sources with Unknown Correlations</a></p>
<p>19 0.25282142 <a title="39-lsi-19" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>20 0.25093466 <a title="39-lsi-20" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.036), (11, 0.045), (29, 0.016), (30, 0.014), (35, 0.045), (53, 0.101), (71, 0.058), (73, 0.348), (76, 0.043), (85, 0.083), (91, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77300537 <a title="39-lda-1" href="./nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models.html">39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</a></p>
<p>Author: Charles Rosenberg, Alok Ladsariya, Tom Minka</p><p>Abstract: We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.</p><p>2 0.7550351 <a title="39-lda-2" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We compute approximate analytical bootstrap averages for support vector classiﬁcation using a combination of the replica method of statistical physics and the TAP approach for approximate inference. We test our method on a few datasets and compare it with exact averages obtained by extensive Monte-Carlo sampling. 1</p><p>3 0.49055186 <a title="39-lda-3" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>4 0.46508652 <a title="39-lda-4" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>Author: Sanjiv Kumar, Martial Hebert</p><p>Abstract: In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classiﬁcation of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classiﬁcation problems using the graph min-cut algorithms. The performance of the model was veriﬁed on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments. 1</p><p>5 0.46087503 <a title="39-lda-5" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>6 0.45991009 <a title="39-lda-6" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>7 0.45909321 <a title="39-lda-7" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>8 0.45868763 <a title="39-lda-8" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>9 0.45786977 <a title="39-lda-9" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>10 0.45665109 <a title="39-lda-10" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>11 0.45615923 <a title="39-lda-11" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>12 0.45608512 <a title="39-lda-12" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>13 0.45580804 <a title="39-lda-13" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>14 0.45505199 <a title="39-lda-14" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>15 0.45464155 <a title="39-lda-15" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>16 0.45426077 <a title="39-lda-16" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>17 0.45330009 <a title="39-lda-17" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>18 0.45253733 <a title="39-lda-18" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>19 0.45221755 <a title="39-lda-19" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>20 0.4521341 <a title="39-lda-20" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
