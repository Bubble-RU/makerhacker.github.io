<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 nips-2003-Training a Quantum Neural Network</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-187" href="#">nips2003-187</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>187 nips-2003-Training a Quantum Neural Network</h1>
<br/><p>Source: <a title="nips-2003-187-pdf" href="http://papers.nips.cc/paper/2363-training-a-quantum-neural-network.pdf">pdf</a></p><p>Author: Bob Ricks, Dan Ventura</p><p>Abstract: Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. 1</p><p>Reference: <a title="nips-2003-187-reference" href="../nips2003_reference/nips-2003-Training_a_Quantum_Neural_Network_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Most proposals for quantum neural networks have skipped over the problem of how to train the networks. [sent-5, score-0.96]
</p><p>2 The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. [sent-6, score-1.079]
</p><p>3 We propose a simple quantum neural network and a training method for it. [sent-7, score-1.057]
</p><p>4 It can be shown that this algorithm works in quantum systems. [sent-8, score-0.904]
</p><p>5 Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. [sent-9, score-0.982]
</p><p>6 1  Introduction  Many quantum neural networks have been proposed [1], but very few of these proposals have attempted to provide an in-depth method of training them. [sent-10, score-1.032]
</p><p>7 Most either do not mention how the network will be trained or simply state that they use a standard gradient descent algorithm. [sent-11, score-0.15]
</p><p>8 This assumes that training a quantum neural network will be straightforward and analogous to classical methods. [sent-12, score-1.133]
</p><p>9 While some quantum neural networks seem quite similar to classical networks [2], others have proposed quantum networks that are vastly different [3, 4, 5]. [sent-13, score-1.976]
</p><p>10 Several of these networks also employ methods which are speculative or difﬁcult to do in quantum systems [7, 8]. [sent-15, score-0.922]
</p><p>11 These signiﬁcant differences between classical networks and quantum neural networks, as well as the problems associated with quantum computation itself, require us to look more deeply at the issue of training quantum neural networks. [sent-16, score-2.882]
</p><p>12 It is an open question what advantages a quantum neural network (QNN) would have over a classical network. [sent-18, score-1.061]
</p><p>13 Other results have shown that QNNs may work best with some classical components as well as quantum components [2]. [sent-20, score-0.962]
</p><p>14 This paper details such a network and how training could be done on it. [sent-23, score-0.151]
</p><p>15 2  Quantum Computation  Several necessary ideas that form the basis for the study of quantum computation are brieﬂy reviewed here. [sent-25, score-0.903]
</p><p>16 The Hilbert space has a set of states, |φi , that form a basis, and the system is described by a quantum state |ψ = i ci |φi . [sent-30, score-0.938]
</p><p>17 |ψ is said to be coherent or to be in a linear superposition of the basis states |φi , and in general the coefﬁcients ci are complex. [sent-31, score-0.194]
</p><p>18 A postulate of quantum mechanics is that if a coherent system interacts in any way with its environment (by being measured, for example), the superposition is destroyed. [sent-32, score-1.059]
</p><p>19 A two-state quantum system is used as the basic unit of quantum computation. [sent-37, score-1.772]
</p><p>20 Such a system is referred to as a quantum bit or qubit and naming the two states |0 and |1 , it is easy to see why this is so. [sent-38, score-0.959]
</p><p>21 2  Operators  Operators on a Hilbert space describe how one wave function is changed into another and they may be represented as matrices acting on vectors (the notation |· indicates a column vector and the ·| a [complex conjugate] row vector). [sent-40, score-0.153]
</p><p>22 In the quantum formalism, all properties are represented as operators whose eigenstates are the basis for the Hilbert space associated with that property and whose eigenvalues are the quantum allowed values for that property. [sent-44, score-1.89]
</p><p>23 It is important to note that operators in quantum mechanics must be linear operators and further that they must be unitary. [sent-45, score-1.039]
</p><p>24 This is a phenomenon common to all kinds of wave mechanics from water waves to optics. [sent-49, score-0.164]
</p><p>25 The well known double slit experiment demonstrates empirically that at the quantum level interference also applies to the probability waves of quantum mechanics. [sent-50, score-1.839]
</p><p>26 The wave function interferes with itself through the action of an operator – the different parts of the wave function interfere constructively or destructively according to their relative phases just like any other kind of wave. [sent-51, score-0.238]
</p><p>27 4  Entanglement  Entanglement is the potential for quantum systems to exhibit correlations that cannot be accounted for classically. [sent-53, score-0.902]
</p><p>28 From a computational standpoint, entanglement seems intuitive enough – it is simply the fact that correlations can exist between different qubits – for example if one qubit is in the |1 state, another will be in the |1 state. [sent-54, score-0.228]
</p><p>29 However, from a physical standpoint, entanglement is little understood. [sent-55, score-0.154]
</p><p>30 What makes it so powerful (and so little understood) is the fact that since quantum states exist as superpositions, these correlations exist in superposition as well. [sent-57, score-1.032]
</p><p>31 There are different degrees of entanglement and much work has been done on better understanding and quantifying it [10, 11]. [sent-67, score-0.148]
</p><p>32 Finally, it should be mentioned that while interference is a quantum property that has a classical cousin, entanglement is a completely quantum phenomenon for which there is no classical analog. [sent-68, score-2.113]
</p><p>33 5  An Example – Quantum Search  One of the best known quantum algorithms searches an unordered database quadratically faster than any classical method [12, 13]. [sent-72, score-1.005]
</p><p>34 The algorithm begins with a superposition of all N data items and depends upon an oracle that can recognize the target of the search. [sent-73, score-0.247]
</p><p>35 Classically, searching such a database requires O(N ) oracle calls; however, on a quan√ tum computer, the task requires only O( N ) oracle calls. [sent-74, score-0.252]
</p><p>36 Each oracle call consists of a quantum operator that inverts the phase of the search target. [sent-75, score-1.099]
</p><p>37 3  A Simple Quantum Neural Network  We would like a QNN with features that make it easy for us to model, yet powerful enough to leverage quantum physics. [sent-78, score-0.907]
</p><p>38 The output layer does the same thing as the hidden layer(s), except that it also checks its accuracy against the target output of the network. [sent-83, score-0.241]
</p><p>39 The network as a whole computes a function by checking which output bit is high. [sent-84, score-0.182]
</p><p>40 The output layer works similarly, taking a weighted sum of the hidden nodes and checking against a threshold. [sent-92, score-0.224]
</p><p>41 At the quantum gate level, the network will require O(blm + m2 ) gates for each node of the network. [sent-95, score-1.059]
</p><p>42 Here b is the number of bits used for ﬂoating point arithmetic in |β , l is the number of bits for each weight and m is the number of inputs to the node [14]-[15]. [sent-96, score-0.307]
</p><p>43 The overall network works as follows on a training set. [sent-97, score-0.169]
</p><p>44 In our example, the network has two input parameters, so all n training examples will have two input registers. [sent-98, score-0.173]
</p><p>45 Each hidden or output node has a weight vector, represented by |ψ i , each vector containing weights for each of its inputs. [sent-101, score-0.35]
</p><p>46 After classifying a training example, the registers |ϕ 1 and |ϕ 2 reﬂect the networks ability to classify that the training example. [sent-102, score-0.269]
</p><p>47 When all training examples have  Figure 2: QNN Training  been classiﬁed, |ρ will be the sum of the output nodes that have the correct answer throughout the training set and will range between zero and the number of training examples times the number of output nodes. [sent-104, score-0.417]
</p><p>48 4  Using Quantum Search to Learn Network Weights  One possibility for training this kind of a network is to search through the possible weight vectors for one which is consistent with the training data. [sent-105, score-0.464]
</p><p>49 Quantum searches have been used already in quantum learning [16] and many of the problems associated with them have already been explored [17]. [sent-106, score-0.913]
</p><p>50 We would like to ﬁnd a solution which classiﬁes all training examples correctly; in other words we would like |ρ = n ∗ m where n is the number of training examples and m is the number of output nodes. [sent-107, score-0.238]
</p><p>51 Since we generally do not know how many weight vectors will do this, we use a generalization of the original search algorithm [18], intended for problems where the number of solutions t is unknown. [sent-108, score-0.241]
</p><p>52 The basic idea is that we will put |ψ into a superposition of all possible weight vectors and search for one which classiﬁes all training examples correctly. [sent-109, score-0.446]
</p><p>53 We start out with |ψ as a superposition of all possible weight vectors. [sent-110, score-0.234]
</p><p>54 By using a superposition we classify the training examples with respect to every possible weight vector simultaneously. [sent-113, score-0.38]
</p><p>55 Each weight vector is now entangled with |ρ in such a way that |ρ corresponds with how well every weight vector classiﬁes all the training data. [sent-114, score-0.413]
</p><p>56 In this case, the oracle for the quantum search is |ρ = n ∗ m, which corresponds to searching for a weight vector which correctly classiﬁes the entire set. [sent-115, score-1.284]
</p><p>57 Unfortunately, searching the weight vectors while entangled with |ρ would cause unwanted weight vectors to grow that would be entangled with the performance metric we are looking for. [sent-116, score-0.47]
</p><p>58 The solution is to disentangle |ψ from the other registers after inverting the phase of those weights which match the search criteria, based on |ρ . [sent-117, score-0.18]
</p><p>59 This means that the network will need to be recomputed  each time we make an oracle call and after each measurement. [sent-119, score-0.177]
</p><p>60 First, not all training data will have any solution networks that correctly classify all training instances. [sent-121, score-0.248]
</p><p>61 This means that nothing will be marked by the search oracle, so every weight vector will have an equal chance of being measured. [sent-122, score-0.225]
</p><p>62 Second, the amount of time needed to ﬁnd a vector which correctly classiﬁes the training set is O( 2b /t), which has exponential complexity with respect to the number of bits in the weight vector. [sent-124, score-0.306]
</p><p>63 One way to deal with the ﬁrst problem is to search until we ﬁnd a solution which covers an acceptable percentage, p, of the training data. [sent-125, score-0.155]
</p><p>64 In other words, the search oracle is modiﬁed to be |ρ ≥ n ∗ m ∗ p. [sent-126, score-0.181]
</p><p>65 5  Piecewise Weight Learning  Our quantum search algorithm gives us a good polynomial speed-up to the exponential task of ﬁnding a solution to the QNN. [sent-128, score-0.991]
</p><p>66 This algorithm does not scale well, in fact it is exponential in the total number of weights in the network and the bits per weight. [sent-129, score-0.161]
</p><p>67 Therefore, we propose a randomized training algorithm which searches each node’s weight vector independently. [sent-130, score-0.295]
</p><p>68 The network starts off, once again, with training examples in |α , the corresponding answers in |Ω , and zeros in all the other registers. [sent-131, score-0.173]
</p><p>69 A node is randomly selected and its weight vector, |ψ i , is put into superposition. [sent-132, score-0.197]
</p><p>70 All other weight vectors start with random classical initial weights. [sent-133, score-0.234]
</p><p>71 We then search for a weight vector for this node that causes the entire network to classify a certain percentage, p, of the training examples correctly. [sent-134, score-0.505]
</p><p>72 That weight is ﬁxed classically and the process is repeated randomly for the other nodes. [sent-136, score-0.148]
</p><p>73 Searching each node’s weight vector separately is, in effect, a random search through the weight space where we select weight vectors which give a good level of performance for each node. [sent-137, score-0.506]
</p><p>74 Each node takes on weight vectors that tend to increase performance with some amount of randomness that helps keep it out of local minima. [sent-138, score-0.253]
</p><p>75 First, to insure that hidden nodes ﬁnd weight vectors that compute something useful, a small performance penalty is added to weight vectors which cause a hidden node to output the same value for all training examples. [sent-141, score-0.63]
</p><p>76 This helps select weight vectors which contain useful information for the output nodes. [sent-142, score-0.229]
</p><p>77 Since each output node’s performance is independent of the performance or all output nodes, the algorithm only considers the accuracy of the output node being trained when training an output node. [sent-143, score-0.346]
</p><p>78 Each of the hidden and the output nodes are thresholded nodes with three weights, one for each input and one for the threshold. [sent-145, score-0.166]
</p><p>79 After an average of 58 weight updates, the algorithm was able to correctly classify the training data. [sent-150, score-0.263]
</p><p>80 Since this is a randomized algorithm both in the number of iterations of the search algorithm before measuring and in the order which nodes update their weight vectors, the standard deviation for this method was much higher, but still reasonable. [sent-151, score-0.298]
</p><p>81 In the randomized search algorithm,  an epoch refers to ﬁnding and ﬁxing the weight of a single node. [sent-152, score-0.26]
</p><p>82 We also tried the randomized search algorithm for a few real-world machine learning problems: lenses, Hayes-Roth and the iris datasets [19]. [sent-153, score-0.176]
</p><p>83 The lenses data set is a data set that tries to predict whether people will need soft contact lenses, hard contact lenses or no contacts. [sent-154, score-0.214]
</p><p>84 98%  Backprop 96% 92% 83%  Table 1: Training Results The lenses data set can be solved with a network that has three hidden nodes. [sent-163, score-0.204]
</p><p>85 The randomized search algorithm found the correct target for 97. [sent-168, score-0.175]
</p><p>86 We used four hidden nodes with two bit weights for the hidden nodes. [sent-171, score-0.169]
</p><p>87 We had to normalize the inputs to range from zero to one once again so the larger inputs would not dominate the weight vectors. [sent-172, score-0.159]
</p><p>88 86% of the training examples correctly since we are checking each output node for accuracy on each training example. [sent-176, score-0.352]
</p><p>89 7  Conclusions and Future Work  This paper proposes a simple quantum neural network and a method of training it which works well in quantum systems. [sent-179, score-1.961]
</p><p>90 By using a quantum search we are able to use a wellknown algorithm for quantum systems which has already been used for quantum learning. [sent-180, score-2.741]
</p><p>91 The algorithm is able to search for solutions that cover an arbitrary percentage of the training set. [sent-181, score-0.172]
</p><p>92 This algorithm is exponential in the number of qubits of each node’s weight vector instead of in the composite weight vector of the entire network. [sent-185, score-0.38]
</p><p>93 There may be quantum methods which could be used to improve current gradient descent and other learning algorithms. [sent-189, score-0.935]
</p><p>94 It may also be possible to combine some of these with a quantum search. [sent-190, score-0.886]
</p><p>95 An example would be to use gradient descent to try and reﬁne a composite weight vector found by quantum search. [sent-191, score-1.094]
</p><p>96 Conversely, a quantum search could start with the weight vector of a gradient descent search. [sent-192, score-1.16]
</p><p>97 This would allow the search to start with an  accurate weight vector and search locally for weight vectors which improve overall performance. [sent-193, score-0.466]
</p><p>98 Other types of QNNs may be able to use a quantum search as well since the algorithm only requires a weight space which can be searched in superposition. [sent-195, score-1.092]
</p><p>99 In addition, more traditional gradient descent techniques might beneﬁt from a quantum speed-up themselves. [sent-196, score-0.935]
</p><p>100 Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. [sent-299, score-0.886]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('quantum', 0.886), ('entanglement', 0.127), ('weight', 0.123), ('qnn', 0.113), ('superposition', 0.111), ('oracle', 0.098), ('lenses', 0.085), ('search', 0.083), ('wave', 0.08), ('network', 0.079), ('classical', 0.076), ('node', 0.074), ('training', 0.072), ('entangled', 0.057), ('qubits', 0.057), ('registers', 0.056), ('randomized', 0.054), ('operators', 0.054), ('output', 0.05), ('xor', 0.049), ('interference', 0.045), ('mechanics', 0.045), ('brigham', 0.042), ('qnns', 0.042), ('searching', 0.04), ('hidden', 0.04), ('dan', 0.039), ('iris', 0.039), ('target', 0.038), ('nodes', 0.038), ('ventura', 0.037), ('hilbert', 0.036), ('networks', 0.036), ('bits', 0.035), ('vectors', 0.035), ('correctly', 0.035), ('interfere', 0.034), ('classify', 0.033), ('descent', 0.032), ('layer', 0.032), ('checks', 0.031), ('ci', 0.03), ('alexandr', 0.028), ('behrman', 0.028), ('constructively', 0.028), ('eigenstates', 0.028), ('ezhov', 0.028), ('provo', 0.028), ('qubit', 0.028), ('vedral', 0.028), ('young', 0.028), ('volume', 0.028), ('physical', 0.027), ('searches', 0.027), ('checking', 0.027), ('factorized', 0.027), ('bit', 0.026), ('weights', 0.025), ('classically', 0.025), ('steck', 0.025), ('pages', 0.023), ('arithmetic', 0.022), ('contact', 0.022), ('waves', 0.022), ('lov', 0.022), ('state', 0.022), ('examples', 0.022), ('sciences', 0.022), ('exponential', 0.022), ('helps', 0.021), ('leverage', 0.021), ('quantifying', 0.021), ('associative', 0.02), ('standpoint', 0.02), ('coherence', 0.02), ('gates', 0.02), ('neural', 0.02), ('vector', 0.019), ('sum', 0.019), ('states', 0.019), ('classi', 0.019), ('ut', 0.019), ('represented', 0.019), ('amplitudes', 0.018), ('register', 0.018), ('proposals', 0.018), ('inputs', 0.018), ('works', 0.018), ('percentage', 0.017), ('coherent', 0.017), ('gradient', 0.017), ('basis', 0.017), ('phenomenon', 0.017), ('composite', 0.017), ('governed', 0.017), ('http', 0.016), ('operator', 0.016), ('phase', 0.016), ('database', 0.016), ('correlations', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="187-tfidf-1" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>Author: Bob Ricks, Dan Ventura</p><p>Abstract: Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. 1</p><p>2 0.079345725 <a title="187-tfidf-2" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>3 0.045047753 <a title="187-tfidf-3" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><p>4 0.044900138 <a title="187-tfidf-4" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>Author: Xuanlong Nguyen, Michael I. Jordan</p><p>Abstract: We present an analysis of concentration-of-expectation phenomena in layered Bayesian networks that use generalized linear models as the local conditional probabilities. This framework encompasses a wide variety of probability distributions, including both discrete and continuous random variables. We utilize ideas from large deviation analysis and the delta method to devise and evaluate a class of approximate inference algorithms for layered Bayesian networks that have superior asymptotic error bounds and very fast computation time. 1</p><p>5 0.039210483 <a title="187-tfidf-5" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>6 0.036780123 <a title="187-tfidf-6" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>7 0.035444394 <a title="187-tfidf-7" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>8 0.034150485 <a title="187-tfidf-8" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>9 0.033996195 <a title="187-tfidf-9" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>10 0.030453114 <a title="187-tfidf-10" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>11 0.029704597 <a title="187-tfidf-11" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>12 0.029455887 <a title="187-tfidf-12" href="./nips-2003-Reconstructing_MEG_Sources_with_Unknown_Correlations.html">166 nips-2003-Reconstructing MEG Sources with Unknown Correlations</a></p>
<p>13 0.028873267 <a title="187-tfidf-13" href="./nips-2003-Subject-Independent_Magnetoencephalographic_Source_Localization_by_a_Multilayer_Perceptron.html">182 nips-2003-Subject-Independent Magnetoencephalographic Source Localization by a Multilayer Perceptron</a></p>
<p>14 0.028762799 <a title="187-tfidf-14" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>15 0.027247012 <a title="187-tfidf-15" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>16 0.026356108 <a title="187-tfidf-16" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>17 0.026351249 <a title="187-tfidf-17" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>18 0.026163951 <a title="187-tfidf-18" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>19 0.025351971 <a title="187-tfidf-19" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>20 0.025226211 <a title="187-tfidf-20" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.101), (1, 0.017), (2, 0.021), (3, 0.017), (4, 0.004), (5, -0.028), (6, -0.012), (7, -0.002), (8, 0.016), (9, 0.042), (10, 0.028), (11, -0.014), (12, 0.026), (13, -0.009), (14, -0.015), (15, -0.038), (16, -0.043), (17, 0.01), (18, 0.093), (19, 0.03), (20, 0.038), (21, -0.017), (22, -0.075), (23, 0.076), (24, -0.065), (25, 0.026), (26, -0.032), (27, 0.089), (28, -0.066), (29, -0.029), (30, -0.068), (31, 0.064), (32, 0.027), (33, -0.018), (34, 0.001), (35, 0.005), (36, 0.023), (37, -0.02), (38, 0.005), (39, 0.052), (40, -0.016), (41, -0.008), (42, -0.041), (43, 0.007), (44, 0.062), (45, 0.105), (46, -0.093), (47, -0.057), (48, -0.098), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93459004 <a title="187-lsi-1" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>Author: Bob Ricks, Dan Ventura</p><p>Abstract: Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. 1</p><p>2 0.70482135 <a title="187-lsi-2" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>3 0.5311299 <a title="187-lsi-3" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>4 0.51871306 <a title="187-lsi-4" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><p>5 0.482923 <a title="187-lsi-5" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>Author: Virginia Sa</p><p>Abstract: Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the “visual input”. We explain this ﬁnding in terms of the statistical structure in sensory inputs. 1</p><p>6 0.46953684 <a title="187-lsi-6" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>7 0.44861105 <a title="187-lsi-7" href="./nips-2003-Reasoning_about_Time_and_Knowledge_in_Neural_Symbolic_Learning_Systems.html">165 nips-2003-Reasoning about Time and Knowledge in Neural Symbolic Learning Systems</a></p>
<p>8 0.4437336 <a title="187-lsi-8" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>9 0.4386422 <a title="187-lsi-9" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>10 0.43254372 <a title="187-lsi-10" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>11 0.42056286 <a title="187-lsi-11" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>12 0.41866511 <a title="187-lsi-12" href="./nips-2003-The_Diffusion-Limited_Biochemical_Signal-Relay_Channel.html">184 nips-2003-The Diffusion-Limited Biochemical Signal-Relay Channel</a></p>
<p>13 0.40799198 <a title="187-lsi-13" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>14 0.39962775 <a title="187-lsi-14" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>15 0.39893264 <a title="187-lsi-15" href="./nips-2003-Can_We_Learn_to_Beat_the_Best_Stock.html">44 nips-2003-Can We Learn to Beat the Best Stock</a></p>
<p>16 0.39023763 <a title="187-lsi-16" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>17 0.38374263 <a title="187-lsi-17" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>18 0.38137007 <a title="187-lsi-18" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>19 0.36972043 <a title="187-lsi-19" href="./nips-2003-Circuit_Optimization_Predicts_Dynamic_Networks_for_Chemosensory_Orientation_in_Nematode_C._elegans.html">45 nips-2003-Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans</a></p>
<p>20 0.36185572 <a title="187-lsi-20" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (9, 0.3), (11, 0.028), (29, 0.016), (30, 0.021), (35, 0.053), (48, 0.014), (53, 0.098), (71, 0.079), (76, 0.033), (85, 0.084), (91, 0.103), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86854041 <a title="187-lda-1" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>Author: Francesco Tenore, Ralph Etienne-Cummings, M. A. Lewis</p><p>Abstract: We have constructed a second generation CPG chip capable of generating the necessary timing to control the leg of a walking machine. We demonstrate improvements over a previous chip by moving toward a significantly more versatile device. This includes a larger number of silicon neurons, more sophisticated neurons including voltage dependent charging and relative and absolute refractory periods, and enhanced programmability of neural networks. This chip builds on the basic results achieved on a previous chip and expands its versatility to get closer to a self-contained locomotion controller for walking robots. 1</p><p>2 0.79605734 <a title="187-lda-2" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>Author: Lorenzo Torresani, Aaron Hertzmann, Christoph Bregler</p><p>Abstract: This paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data. We model shape motion as a rigid component (rotation and translation) combined with a non-rigid deformation. Reconstruction is ill-posed if arbitrary deformations are allowed. We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution. Based on this assumption, the algorithm simultaneously estimates 3D shape and motion for each time frame, learns the parameters of the Gaussian, and robustly ﬁlls-in missing data points. We then extend the algorithm to model temporal smoothness in object shape, thus allowing it to handle severe cases of missing data. 1</p><p>same-paper 3 0.76819569 <a title="187-lda-3" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>Author: Bob Ricks, Dan Ventura</p><p>Abstract: Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. 1</p><p>4 0.54105979 <a title="187-lda-4" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>5 0.53569436 <a title="187-lda-5" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>Author: Sanjiv Kumar, Martial Hebert</p><p>Abstract: In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classiﬁcation of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classiﬁcation problems using the graph min-cut algorithms. The performance of the model was veriﬁed on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments. 1</p><p>6 0.53500307 <a title="187-lda-6" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>7 0.53403497 <a title="187-lda-7" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>8 0.5333482 <a title="187-lda-8" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>9 0.53275716 <a title="187-lda-9" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>10 0.53273231 <a title="187-lda-10" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>11 0.53188574 <a title="187-lda-11" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>12 0.53166163 <a title="187-lda-12" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>13 0.53109598 <a title="187-lda-13" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>14 0.53061521 <a title="187-lda-14" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>15 0.52992707 <a title="187-lda-15" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>16 0.52971697 <a title="187-lda-16" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>17 0.52956867 <a title="187-lda-17" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>18 0.52931595 <a title="187-lda-18" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>19 0.5290038 <a title="187-lda-19" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>20 0.52852136 <a title="187-lda-20" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
