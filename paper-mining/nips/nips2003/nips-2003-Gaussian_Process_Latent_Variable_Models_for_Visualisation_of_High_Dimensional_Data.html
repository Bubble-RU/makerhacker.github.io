<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-77" href="#">nips2003-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</h1>
<br/><p>Source: <a title="nips-2003-77-pdf" href="http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf">pdf</a></p><p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>Reference: <a title="nips-2003-77-reference" href="../nips2003_reference/nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). [sent-7, score-0.178]
</p><p>2 Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. [sent-8, score-0.629]
</p><p>3 We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. [sent-9, score-0.116]
</p><p>4 This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. [sent-10, score-0.884]
</p><p>5 Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs. [sent-11, score-0.164]
</p><p>6 1 Introduction Visualisation of high dimensional data can be achieved through projecting a data-set onto a lower dimensional manifold. [sent-12, score-0.088]
</p><p>7 Linear projections have traditionally been preferred due to the ease with which they can be computed. [sent-13, score-0.031]
</p><p>8 One approach to visualising a data-set in two dimensions is to project the data along two of its principal components. [sent-14, score-0.17]
</p><p>9 If we were forced to choose a priori which components to project along, we might sensibly choose those associated with the largest eigenvalues. [sent-15, score-0.061]
</p><p>10 The probabilistic reformulation of principal component analysis (PCA) also informs us that choosing the ﬁrst two components is also the choice that maximises the likelihood of the data [11]. [sent-16, score-0.226]
</p><p>11 and maximising the likelihood of the data-set,   ¨  ¤ ! [sent-27, score-0.087]
</p><p>12  £¤2¢ ¤    where  Probabilistic principal component analysis and other latent variable models, such as factor analysis (FA) or independent component analysis (ICA), require a marginalisation of the latent variables and optimisation of the parameters. [sent-34, score-1.262]
</p><p>13 In this paper we consider the dual approach of marginalising and optimising each . [sent-35, score-0.088]
</p><p>14 This probabilistic model also turns out to be equivalent to PCA. [sent-36, score-0.048]
</p><p>15 The corresponding log-likelihood is  T  §  ¥ D    45  4  D  D  %  fd gp5  & % $! [sent-41, score-0.047]
</p><p>16 The gradients of (2) with respect to may be found as, 3  ! [sent-43, score-0.144]
</p><p>17 Note that the eigenvalue problem we have developed can easily be shown to be equivalent to that solved in PCA (see e. [sent-46, score-0.043]
</p><p>18 [10]), indeed the formulation of T PCA in this manner is a key step in the development of kernel PCA [9] where is replaced with a kernel. [sent-48, score-0.096]
</p><p>19 Our probabilistic PCA model shares an underlying structure with [11] but differs in that where they optimise we marginalise and where they marginalise we optimise. [sent-49, score-0.346]
</p><p>20 The marginalised likelihood we are optimising in (1) is recognised as the product of independent Gaussian processes where the (linear) covariance function is given by T . [sent-50, score-0.323]
</p><p>21 Therefore a natural extension is the non-linearisation of the mapping from latent space to the data space through the introduction of a non-linear covariance function. [sent-51, score-0.642]
</p><p>22 £  1  we will disregard  2 For independent component analysis the correct rotation matrix must also be found, here we have placed no constraints on the orientation of the axes so this matrix cannot be recovered. [sent-53, score-0.153]
</p><p>23 h  2 Gaussian Process Latent Variable Models We saw in the previous section how PCA can be interpreted as a Gaussian process ‘mapping3 ’ from a latent space to a data space where the locale of the points in latent space is determined by maximising the Gaussian process likelihood with respect to . [sent-54, score-1.262]
</p><p>24 We will refer to models of this class as Gaussian process latent variable models (GPLVM). [sent-55, score-0.556]
</p><p>25 Gradients of (2) with respect to the latent points can be found through combining    D  ! [sent-57, score-0.529]
</p><p>26 These gradients may be used in combination with (2) in a non-linear optimiser such as scaled conjugate gradients (SCG) [7] to obtain a latent variable representation of the data. [sent-59, score-0.79]
</p><p>27 Furthermore gradients with respect to the parameters of the kernel matrix may be computed and used to jointly optimise , , and . [sent-60, score-0.488]
</p><p>28 1 Illustration of GPLVM via SCG To illustrate a simple Gaussian process latent variable model we turn to the ‘multi-phase oil ﬂow’ data [2]. [sent-63, score-0.704]
</p><p>29 This is a twelve dimensional data-set containing data of three known classes corresponding to the phase of ﬂow in an oil pipeline: stratiﬁed, annular and homogeneous. [sent-64, score-0.244]
</p><p>30 Figure 1 shows visualisations of the data using both PCA and our GPLVM algorithm which required 766 iterations of SCG. [sent-66, score-0.12]
</p><p>31 The positions for the GPLVM model were initialised using PCA (see http://www. [sent-67, score-0.03]
</p><p>32 3  The gradient based optimisation of the RBF based GPLVM’s latent space shows results which are clearly superior (in terms of greater separation between the different ﬂow domains) to those achieved by the linear PCA model. [sent-72, score-0.551]
</p><p>33 Additionally the use of a Gaussian process to perform our ‘mapping’ means that there is uncertainty in the positions of the points in the data space. [sent-73, score-0.15]
</p><p>34 For our formulation the level of uncertainty is shared across all 4 dimensions and thus may be visualised in the latent space. [sent-74, score-0.605]
</p><p>35 Unfortunately, a quick analysis of the complexity of the algorithm shows that each gradient step requires an inverse of the kernel matrix, an operation, rendering the algorithm impractical for many data-sets of interest. [sent-76, score-0.096]
</p><p>36 ¡ "h¥  3 Strictly speaking the model does not represent a mapping as a Gaussian process ‘maps’ to a distribution in data space rather than a point. [sent-78, score-0.149]
</p><p>37 4 This apparent weakness in the model may be easily rectiﬁed to allow different levels of uncertainty for each output dimension, our more constrained model allows us to visualise this uncertainty in the latent space and is therefore preferred for this work. [sent-79, score-0.642]
</p><p>38 Crosses, circles and plus signs represent stratiﬁed, annular and homogeneous ﬂows respectively. [sent-119, score-0.052]
</p><p>39 The greyscales in plot (b) indicate the precision with which the manifold was , expressed in data-space for that latent point. [sent-120, score-0.452]
</p><p>40 The optimised parameters of the kernel were and . [sent-121, score-0.191]
</p><p>41 2 A Practical Algorithm for GPLVMs There are three main components to our revised, computationally efﬁcient, optimisation process: Sparsiﬁcation. [sent-123, score-0.067]
</p><p>42 representing the data-set by a subset, , of points known as the active set. [sent-126, score-0.147]
</p><p>43 We make use of the informative vector machine [6] which selects points sequentially according to the reduction in the posterior process’s entropy that they induce. [sent-128, score-0.097]
</p><p>44 A point from the inactive set, , can be shown to project into the data space as a Gaussian distribution   (3)  $" %#! [sent-130, score-0.107]
</p><p>45     ¢  #   ' & ¨& ¢ §&  T whose mean is where denotes the kernel matrix developed from is a column vector consisting of the elements from the th column of the active set and T that correspond to the active set. [sent-135, score-0.416]
</p><p>46 The variance is Note that since does not appear in the inverse, gradients with respect to do not depend on other data in . [sent-136, score-0.144]
</p><p>47 We can therefore independently optimise the likelihood of each with respect to each . [sent-137, score-0.268]
</p><p>48 Thus the full set can be optimised with one pass through the data. [sent-138, score-0.066]
</p><p>49 The likelihood of the active set is given by  &  & ¢ § & D &    R Q 45  ¥  &¢ &    I G PE C  and  U  #  ,  ! [sent-142, score-0.158]
</p><p>50 )  D  4 C  9 %  7 5 8(  & %  &   )  which can be optimised5 with respect to  T  (4)  with gradient evaluations costing   ©  ! [sent-143, score-0.04]
</p><p>51 Note that whilst we never optimise points in the active set, we repeatedly reselect the active set so it is 5 In practice we looked for MAP solutions for all our optimisations, specifying a unit covariance Gaussian prior for the matrix and using , and for , and respectively. [sent-145, score-0.588]
</p><p>52 for iterations do Select a new active set using the IVM algorithm. [sent-150, score-0.141]
</p><p>53 Optimise (4) with respect to the parameters of using scaled conjugate gradients. [sent-151, score-0.144]
</p><p>54 Optimise (3) with respect to end for end for     3  3     D      unlikely that many points remain in their original location. [sent-155, score-0.077]
</p><p>55 For all the experiments that follow we used iterations and an active set of size . [sent-156, score-0.141]
</p><p>56 The experiments were run on a ‘one-shot’ basis6 so we cannot make statements as to the effects that signiﬁcant modiﬁcation of these parameters would have. [sent-157, score-0.029]
</p><p>57 We present results on three data-sets: for the oil ﬂow data (Figure 2) from the previous section we now make use of all 1000 available points and we include a comparison with the generative topographic mapping (GTM) [4]. [sent-158, score-0.389]
</p><p>58 5 −3  1  −2  −1  0  1  2  3  Figure 2: The full oil ﬂow data-set visualised with (a) GTM with 225 latent points laid out on a grid and with 16 RBF nodes and (b) an RBF based GPLVM. [sent-185, score-0.756]
</p><p>59 The parameters of the latent variable model were found to be , and . [sent-186, score-0.536]
</p><p>60 Notice how the GTM artiﬁcially ‘discretises’ the latent space around the locations of the 225 latent points. [sent-187, score-0.936]
</p><p>61  ©  ¡   ©  §¤ ¨¦£ ¡  f  ¥ ¢ ¥ 2£ £2£  ¥¤ § ¨¥¤ ¡ ©  We follow [5] in our 2-D visualisation of a sub-set of 3000 of the digits 0-4 (600 of each digit) from a greyscale version of the USPS digit data-set (Figure 3). [sent-188, score-0.442]
</p><p>62     4  4  Finally we modelled a face data-set [8] consisting of 1965 images from a video sequence digitised at . [sent-189, score-0.093]
</p><p>63 Since the images are originally from a video sequence we might expect the underlying dimensionality of the data to be one — the images are produced in a smooth way over time which can be thought of as a piece of string embedded in a high (560) dimensional pixel space. [sent-190, score-0.192]
</p><p>64 We therefore present ordered results from a 1-D visualisation in Figure 4 . [sent-191, score-0.284]
</p><p>65 ¢  5   5  All the code used for performing the experiments is available from http://www. [sent-192, score-0.035]
</p><p>66 If we were producing a visualisation for only one dataset this would leave us open to the criticism that our one-shot result was ‘lucky’. [sent-195, score-0.31]
</p><p>67 4  −4  −2  0  2  4  Figure 3: The digit images visualised in the 2-D latent space. [sent-206, score-0.68]
</p><p>68 We followed [5] in plotting images in a random order but not plotting any image which would overlap an existing image. [sent-207, score-0.143]
</p><p>69 Note how little space is taken by the ‘ones’ (the thin line running from (-4, -1. [sent-209, score-0.032]
</p><p>70 5) to (-1, 0)) in our visualisation, this may be contrasted with the visualisation of a similar data-set in [5]. [sent-210, score-0.284]
</p><p>71 We suggest this is because ‘ones’ are easier to model and therefore do not require a large region in latent space. [sent-211, score-0.452]
</p><p>72 uk/~neil/gplvm/ along with avi video ﬁles of the 1-D visualisation and results from two further experiments on the same data (a 1-D GPLVM model of the digits and a 2-D GPLVM model of the faces). [sent-214, score-0.426]
</p><p>73 3 Discussion Empirically the RBF based GPLVM model gives useful visualisations of a range of datasets. [sent-215, score-0.089]
</p><p>74 Strengths of the method include the ability to optimise the kernel parameters and to generate fantasy data from any point in latent space. [sent-216, score-0.846]
</p><p>75 Through the use of a probabilistic process we can obtain error bars on the position of the manifolds which can be visualised by imposing a greyscale image upon the latent space. [sent-217, score-0.727]
</p><p>76 When Kernels Collide: Twin Kernel PCA The eigenvalue problem which provides the maxima of (2) with respect to for the linear kernel is exploited in kernel PCA. [sent-218, score-0.275]
</p><p>77 One could T T consider a ‘twin kernel’ PCA where both and are replaced by kernel functions. [sent-219, score-0.096]
</p><p>78 Twin kernel PCA could no longer be undertaken with an eigenvalue decomposition but Algorithm 1 would still be a suitable mechanism with which to determine the values of and the parameters of ’s kernel. [sent-220, score-0.168]
</p><p>79  T  £    § ¨#  `  3  3 3 Te)  3  3  Figure 4: Top: Fantasy faces from the 1-D model for the face data. [sent-221, score-0.038]
</p><p>80 These faces were created by taking 64 uniformly spaced and ordered points from the latent space and visualising the mean of their distribution in data space. [sent-222, score-0.648]
</p><p>81 Ideally the transition between the images should be smooth. [sent-224, score-0.055]
</p><p>82 Bottom: Examples from the data-set which are closest to the corresponding fantasy images in latent space. [sent-225, score-0.596]
</p><p>83 Full sequences of 2000 fantasies and the entire dataset are available on the web as avi ﬁles. [sent-226, score-0.059]
</p><p>84 The entropy of is constant7 in , we therefore may add it to to obtain & ¡  %  ¡  £  D ! [sent-229, score-0.032]
</p><p>85 ¢  $  ( & '%  ¡      ¡ ¦¡   & ¡ D ¤£ ¢¡   ¡ 3  KL  (5)  which is recognised Kullback-Leibler (KL) divergence between the two distributions. [sent-236, score-0.052]
</p><p>86 Stochastic neighbor embedding (SNE) [5] also minimises this KL divergence to visualise data. [sent-237, score-0.101]
</p><p>87 The Generative topographic mapping [3] makes use of a radial basis function network to perform the mapping from latent space to observed space. [sent-240, score-0.732]
</p><p>88 Marginalisation of the latent space is achieved with an expectation-maximisation 7 Computing the entropy requires by adding ‘jitter’ to , e. [sent-241, score-0.516]
</p><p>89 to be of full rank, this is not true in general but can be forced . [sent-243, score-0.033]
</p><p>90 A radial basis function network is a special case of a generalised linear model and can be interpreted as a Gaussian process. [sent-245, score-0.03]
</p><p>91 Under this interpretation the GTM becomes GPLVM with a particular covariance function. [sent-246, score-0.058]
</p><p>92 The special feature of the GTM is the manner in which the latent space is represented, as a set of uniformly spaced delta functions. [sent-247, score-0.57]
</p><p>93 One could view the GPLVM as having a delta function associated with each data-point: in the GPLVM the positions of the delta functions are optimised, in the GTM each data point is associated with several different ﬁxed delta functions. [sent-248, score-0.198]
</p><p>94 4 Conclusions We have presented a new class of models for probabilistic modelling and visualisation of high dimensional data. [sent-249, score-0.402]
</p><p>95 We provided strong theoretical grounding for the approach by proving that principal component analysis is a special case. [sent-250, score-0.13]
</p><p>96 On three real world data-sets we showed that visualisations provided by the model cluster the data in a reasonable way. [sent-251, score-0.089]
</p><p>97 Our model has an advantage over the various spectral clustering algorithms that have been presented in recent years in that, in common with the GTM, it is truly generative with an underlying probabilistic interpretation. [sent-252, score-0.102]
</p><p>98 Our theoretical analysis also suggested a novel non-linearisation of PCA involving two kernel functions. [sent-254, score-0.096]
</p><p>99 Advances in Neural Information Processing Systems, volume 15, Cambridge, MA, 2003. [sent-260, score-0.029]
</p><p>100 Fast sparse Gaussian process methods: The informative vector machine. [sent-299, score-0.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gplvm', 0.475), ('latent', 0.452), ('visualisation', 0.284), ('pca', 0.212), ('gtm', 0.197), ('optimise', 0.18), ('oil', 0.148), ('twin', 0.119), ('visualised', 0.119), ('active', 0.11), ('gradients', 0.104), ('kernel', 0.096), ('fantasy', 0.089), ('visualisations', 0.089), ('optimising', 0.088), ('principal', 0.083), ('topographic', 0.082), ('marginalised', 0.077), ('rbf', 0.074), ('mapping', 0.068), ('ow', 0.067), ('optimisation', 0.067), ('optimised', 0.066), ('th', 0.061), ('avi', 0.059), ('greyscale', 0.059), ('marginalisation', 0.059), ('marginalise', 0.059), ('sne', 0.059), ('sparsi', 0.059), ('visualise', 0.059), ('visualising', 0.059), ('covariance', 0.058), ('delta', 0.056), ('variable', 0.055), ('images', 0.055), ('digit', 0.054), ('generative', 0.054), ('annular', 0.052), ('scg', 0.052), ('ppca', 0.052), ('gd', 0.052), ('recognised', 0.052), ('svens', 0.052), ('gaussian', 0.05), ('process', 0.049), ('becker', 0.049), ('bishop', 0.049), ('probabilistic', 0.048), ('likelihood', 0.048), ('component', 0.047), ('fd', 0.047), ('te', 0.047), ('inactive', 0.047), ('shef', 0.047), ('digits', 0.045), ('dimensional', 0.044), ('neil', 0.044), ('plotting', 0.044), ('strati', 0.044), ('eigenvalue', 0.043), ('neighbor', 0.042), ('kl', 0.042), ('ows', 0.041), ('lawrence', 0.041), ('respect', 0.04), ('conjugate', 0.04), ('integrating', 0.039), ('maximising', 0.039), ('matrix', 0.039), ('video', 0.038), ('faces', 0.038), ('points', 0.037), ('code', 0.035), ('scaled', 0.035), ('uncertainty', 0.034), ('forced', 0.033), ('entropy', 0.032), ('space', 0.032), ('preferred', 0.031), ('iterations', 0.031), ('positions', 0.03), ('radial', 0.03), ('spaced', 0.03), ('volume', 0.029), ('parameters', 0.029), ('prior', 0.028), ('informative', 0.028), ('rotation', 0.028), ('dietterich', 0.028), ('project', 0.028), ('tr', 0.027), ('modelling', 0.026), ('specifying', 0.026), ('illustration', 0.026), ('portobello', 0.026), ('criticism', 0.026), ('optimisations', 0.026), ('temptation', 0.026), ('sq', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="77-tfidf-1" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>2 0.20283252 <a title="77-tfidf-2" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>Author: Jakob J. Verbeek, Sam T. Roweis, Nikos A. Vlassis</p><p>Abstract: We propose a non-linear Canonical Correlation Analysis (CCA) method which works by coordinating or aligning mixtures of linear models. In the same way that CCA extends the idea of PCA, our work extends recent methods for non-linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinates are observed, each lying on a different high dimensional manifold. We also show that a special case of our method, when applied to only a single manifold, reduces to the Laplacian Eigenmaps algorithm. As with previous alignment schemes, once the mixture models have been estimated, all of the parameters of our model can be estimated in closed form without local optima in the learning. Experimental results illustrate the viability of the approach as a non-linear extension of CCA. 1</p><p>3 0.13448876 <a title="77-tfidf-3" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><p>4 0.11394349 <a title="77-tfidf-4" href="./nips-2003-Optimal_Manifold_Representation_of_Data%3A_An_Information_Theoretic_Approach.html">149 nips-2003-Optimal Manifold Representation of Data: An Information Theoretic Approach</a></p>
<p>Author: Denis V. Chigirev, William Bialek</p><p>Abstract: We introduce an information theoretic method for nonparametric, nonlinear dimensionality reduction, based on the inﬁnite cluster limit of rate distortion theory. By constraining the information available to manifold coordinates, a natural probabilistic map emerges that assigns original data to corresponding points on a lower dimensional manifold. With only the information-distortion trade off as a parameter, our method determines the shape of the manifold, its dimensionality, the probabilistic map and the prior that provide optimal description of the data. 1 A simple example Some data sets may not be as complicated as they appear. Consider the set of points on a plane in Figure 1. As a two dimensional set, it requires a two dimensional density ρ(x, y) for its description. Since the data are sparse the density will be almost singular. We may use a smoothing kernel, but then the data set will be described by a complicated combination of troughs and peaks with no obvious pattern and hence no ability to generalize. We intuitively, however, see a strong one dimensional structure (a curve) underlying the data. In this paper we attempt to capture this intuition formally, through the use of the inﬁnite cluster limit of rate distortion theory. Any set of points can be embedded in a hypersurface of any intrinsic dimensionality if we allow that hypersurface to be highly “folded.” For example, in Figure 1, any curve that goes through all the points gives a one dimensional representation. We would like to avoid such solutions, since they do not help us discover structure in the data. Looking for a simpler description one may choose to penalize the curvature term [1]. The problem with this approach is that it is not easily generalized to multiple dimensions, and requires the dimensionality of the solution as an input. An alternative approach is to allow curves of all shapes and sizes, but to send the reduced coordinates through an information bottleneck. With a ﬁxed number of bits, position along a highly convoluted curve becomes uncertain. This will penalize curves that follow the data too closely (see Figure 1). There are several advantages to this approach. First, it removes the artiﬁciality introduced by Hastie [2] of adding to the cost function only orthogonal errors. If we believe that data points fall out of the manifold due to noise, there is no reason to treat the projection onto the manifold as exact. Second, it does not require the dimension- 9 8 Figure 1: Rate distortion curve for a data set of 25 points (red). We used 1000 points to represent the curve which where initialized by scattering them uniformly on the plane. Note that the produced curve is well deﬁned, one dimensional and smooth. 7 6 5 4 3 2 1 0 2 4 6 8 10 12 ality of the solution manifold as an input. By adding extra dimensions, one quickly looses the precision with which manifold points are speciﬁed (due to the ﬁxed information bottleneck). Hence, the optimal dimension emerges naturally. This also means that the method works well in many dimensions with no adjustments. Third, the method handles sparse data well. This is important since in high dimensional spaces all data sets are sparse, i.e. they look like points in Figure 1, and the density estimation becomes impossible. Luckily, if the data are truly generated by a lower dimensional process, then density estimation in the data space is not important (from the viewpoint of prediction or any other). What is critical is the density of the data along the manifold (known in latent variable modeling as a prior), and our algorithm ﬁnds it naturally. 2 Latent variable models and dimensionality reduction Recently, the problem of reducing the dimensionality of a data set has received renewed attention [3,4]. The underlying idea, due to Hotelling [5], is that most of the variation in many high dimensional data sets can often be explained by a few latent variables. Alternatively, we say that rather than ﬁlling the whole space, the data lie on a lower dimensional manifold. The dimensionality of this manifold is the dimensionality of the latent space and the coordinate system on this manifold provides the latent variables. Traditional tools of principal component analysis (PCA) and factor analysis (FA) are still the most widely used methods in data analysis. They project the data onto a hyperplane, so the reduced coordinates are easy to interpret. However, these methods are unable to deal with nonlinear correlations in a data set. To accommodate nonlinearity in a data set, one has to relax the assumption that the data is modeled by a hyperplane, and allow a general low dimensional manifold of unknown shape and dimensionality. The same questions that we asked in the previous section apply here. What do we mean by requiring that “the manifold models the data well”? In the next section, we formalize this notion by deﬁning the manifold description of data as a doublet (the shape of the manifold and the projection map). Note that we do not require the probability distribution over the manifold (known for generative models [6,7] as a prior distribution over the latent variables and postulated a priori). It is completely determined by the doublet. Nonlinear correlations in data can also be accommodated implicitly, without constructing an actual low dimensional manifold. By mapping the data from the original space to an even higher dimensional feature space, we may hope that the correlations will become linearized and PCA will apply. Kernel methods [8] allow us to do this without actually constructing an explicit map to feature space. They introduce nonlinearity through an a priori nonlinear kernel. Alternatively, autoassociative neural networks [9] force the data through a bottleneck (with an internal layer of desired dimensionality) to produce a reduced description. One of the disadvantages of these methods is that the results are not easy to interpret. Recent attempts to describe a data set with a low dimensional representation generally follow into two categories: spectral methods and density modeling methods. Spectral methods (LLE [3], ISOMAP [4], Laplacian eigenmaps [10]) give reduced coordinates of an a priori dimensionality by introducing a quadratic cost function in reduced coordinates (hence eigenvectors are solutions) that mimics the relationships between points in the original data space (geodesic distance for ISOMAP, linear reconstruction for LLE). Density modeling methods (GTM [6], GMM [7]) are generative models that try to reproduce the data with fewer variables. They require a prior and a parametric generative model to be introduced a priori and then ﬁnd optimal parameters via maximum likelihood. The approach that we will take is inspired by the work of Kramer [9] and others who tried to formulate dimensionality reduction as a compression problem. They tried to solve the problem by building an explicit neural network encoder-decoder system which restricted the information implicitly by limiting the number of nodes in the bottleneck layer. Extending their intuition with the tools of information theory, we recast dimensionality reduction as a compression problem where the bottleneck is the information available to manifold coordinates. This allows us to deﬁne the optimal manifold description as that which produces the best reconstruction of the original data set, given that the coordinates can only be transmitted through a channel of ﬁxed capacity. 3 Dimensionality reduction as compression Suppose that we have a data set X in a high dimensional state space RD described by a density function ρ(x). We would like to ﬁnd a “simpliﬁed” description of this data set. One may do so by visualizing a lower dimensional manifold M that “almost” describes the data. If we have a manifold M and a stochastic map PM : x → PM (µ|x) to points µ on the manifold, we will say that they provide a manifold description of the data set X. Note that the stochastic map here is well justiﬁed: if a data point does not lie exactly on the manifold then we should expect some uncertainty in the estimation of the value of its latent variables. Also note that we do not need to specify the inverse (generative) map: M → RD ; it can be obtained by Bayes’ rule. The manifold description (M, PM ) is a less than faithful representation of the data. To formalize this notion we will introduce the distortion measure D(M, PM , ρ): ρ(x)PM (µ|x) x − µ 2 dD xDµ. D(M, PM , ρ) = x∈RD (1) µ∈M Here we have assumed the Euclidean distance function for simplicity. The stochastic map, PM (µ|x), together with the density, ρ(x), deﬁne a joint probability function P (M, X) that allows us to calculate the mutual information between the data and its manifold representation: I(X, M) = P (x, µ) log x∈X µ∈M P (x, µ) dD xDµ. ρ(x)PM (µ) (2) This quantity tells us how many bits (on average) are required to encode x into µ. If we view the manifold representation of X as a compression scheme, then I(X, M) tells us the necessary capacity of the channel needed to transmit the compressed data. Ideally, we would like to obtain a manifold description {M, PM (M|X)} of the data set X that provides both a low distortion D(M, PM , ρ) and a good compression (i.e. small I(X, M)). The more bits we are willing to provide for the description of the data, the more detailed a manifold that can be constructed. So there is a trade off between how faithful a manifold representation can be and how much information is required for its description. To formalize this notion we introduce the concept of an optimal manifold. DEFINITION. Given a data set X and a channel capacity I, a manifold description (M, PM (M|X)) that minimizes the distortion D(M, PM , X), and requires only information I for representing an element of X, will be called an optimal manifold M(I, X). Note that another way to deﬁne an optimal manifold is to require that the information I(M, X) is minimized while the average distortion is ﬁxed at value D. The shape and the dimensionality of optimal manifold depends on our information resolution (or the description length that we are willing to allow). This dependence captures our intuition that for real world, multi-scale data, a proper manifold representation must reﬂect the compression level we are trying to achieve. To ﬁnd the optimal manifold (M(I), PM(I) ) for a given data set X, we must solve a constrained optimization problem. Let us introduce a Lagrange multiplier λ that represents the trade off between information and distortion. Then optimal manifold M(I) minimizes the functional: F(M, PM ) = D + λI. (3) Let us parametrize the manifold M by t (presumably t ∈ Rd for some d ≤ D). The function γ(t) : t → M maps the points from the parameter space onto the manifold and therefore describes the manifold. Our equations become: D = dD x dd t ρ(x)P (t|x) x − γ(t) 2 , I = dD x dd t ρ(x)P (t|x) log P (t|x) , P (t) F(γ(t), P (t|x)) = D + λI. (4) (5) (6) Note that both information and distortion measures are properties of the manifold description doublet {M, PM (M|X)} and are invariant under reparametrization. We require the variations of the functional to vanish for optimal manifolds δF/δγ(t) = 0 and δF/δP (t|x) = 0, to obtain the following set of self consistent equations: P (t) = γ(t) = P (t|x) = Π(x) = dD x ρ(x)P (t|x), 1 dD x xρ(x)P (t|x), P (t) P (t) − 1 x−γ (t) 2 e λ , Π(x) 2 1 dd t P (t)e− λ x−γ (t) . (7) (8) (9) (10) In practice we do not have the full density ρ(x), but only a discrete number of samples. 1 So we have to approximate ρ(x) = N δ(x − xi ), where N is the number of samples, i is the sample label, and xi is the multidimensional vector describing the ith sample. Similarly, instead of using a continuous variable t we use a discrete set t ∈ {t1 , t2 , ..., tK } of K points to model the manifold. Note that in (7 − 10) the variable t appears only as an argument for other functions, so we can replace the integral over t by a sum over k = 1..K. Then P (t|x) becomes Pk (xi ),γ(t) is now γ k , and P (t) is Pk . The solution to the resulting set of equations in discrete variables (11 − 14) can be found by an iterative Blahut-Arimoto procedure [11] with an additional EM-like step. Here (n) denotes the iteration step, and α is a coordinate index in RD . The iteration scheme becomes: (n) Pk (n) γk,α = = N 1 N (n) Pk (xi ) = Π(n) (xi ) N 1 1 (n) N P k where α (11) i=1 = (n) xi,α Pk (xi ), (12) i=1 1, . . . , D, K (n) 1 (n) Pk e− λ xi −γ k 2 (13) k=1 (n) (n+1) Pk (xi ) = (n) 2 Pk 1 . e− λ xi −γ k (n) (x ) Π i (14) 0 0 One can initialize γk and Pk (xi ) by choosing K points at random from the data set and 0 letting γk = xi(k) and Pk = 1/K, then use equations (13) and (14) to initialize the 0 association map Pk (xi ). The iteration procedure (11 − 14) is terminated once n−1 n max |γk − γk | < , (15) k where determines the precision with which the manifold points are located. The above algorithm requires the information distortion cost λ = −δD/δI as a parameter. If we want to ﬁnd the manifold description (M, P (M|X)) for a particular value of information I, we can plot the curve I(λ) and, because it’s monotonic, we can easily ﬁnd the solution iteratively, arbitrarily close to a given value of I. 4 Evaluating the solution The result of our algorithm is a collection of K manifold points, γk ∈ M ⊂ RD , and a stochastic projection map, Pk (xi ), which maps the points from the data space onto the manifold. Presumably, the manifold M has a well deﬁned intrinsic dimensionality d. If we imagine a little ball of radius r centered at some point on the manifold of intrinsic dimensionality d, and then we begin to grow the ball, the number of points on the manifold that fall inside will scale as rd . On the other hand, this will not be necessarily true for the original data set, since it is more spread out and resembles locally the whole embedding space RD . The Grassberger-Procaccia algorithm [12] captures this intuition by calculating the correlation dimension. First, calculate the correlation integral: 2 C(r) = N (N − 1) N N H(r − |xi − xj |), (16) i=1 j>i where H(x) is a step function with H(x) = 1 for x > 0 and H(x) = 0 for x < 0. This measures the probability that any two points fall within the ball of radius r. Then deﬁne 0 original data manifold representation -2 ln C(r) -4 -6 -8 -10 -12 -14 -5 -4 -3 -2 -1 0 1 2 3 4 ln r Figure 2: The semicircle. (a) N = 3150 points randomly scattered around a semicircle of radius R = 20 by a normal process with σ = 1 and the ﬁnal positions of 100 manifold points. (b) Log log plot of C(r) vs r for both the manifold points (squares) and the original data set (circles). the correlation dimension at length scale r as the slope on the log log plot. dcorr (r) = d log C(r) . d log r (17) For points lying on a manifold the slope remains constant and the dimensionality is ﬁxed, while the correlation dimension of the original data set quickly approaches that of the embedding space as we decrease the length scale. Note that the slope at large length scales always tends to decrease due to ﬁnite span of the data and curvature effects and therefore does not provide a reliable estimator of intrinsic dimensionality. 5 5.1 Examples Semi-Circle We have randomly generated N = 3150 data points scattered by a normal distribution with σ = 1 around a semi-circle of radius R = 20 (Figure 2a). Then we ran the algorithm with K = 100 and λ = 8, and terminated the iterative algorithm once the precision = 0.1 had been reached. The resulting manifold is depicted in red. To test the quality of our solution, we calculated the correlation dimension as a function of spatial scale for both the manifold points and the original data set (Figure 2b). As one can see, the manifold solution is of ﬁxed dimensionality (the slope remains constant), while the original data set exhibits varying dimensionality. One should also note that the manifold points have dcorr (r) = 1 well into the territory where the original data set becomes two dimensional. This is what we should expect: at a given information level (in this case, I = 2.8 bits), the information about the second (local) degree of freedom is lost, and the resulting structure is one dimensional. A note about the parameters. Letting K → ∞ does not alter the solution. The information I and distortion D remain the same, and the additional points γk also fall on the semi-circle and are simple interpolations between the original manifold points. This allows us to claim that what we have found is a manifold, and not an agglomeration of clustering centers. Second, varying λ changes the information resolution I(λ): for small λ (high information rate) the local structure becomes important. At high information rate the solution undergoes 3.5 3 3 3 2.5 2.5 2 2.5 2 2 1.5 1.5 1.5 1 1 1 0.5 0.5 0 0.5 -0.5 0 0 -1 5 -0.5 -0.5 4 1 3 0.5 2 -1 -1 0 1 -0.5 0 -1 -1.5 -1.5 -1 -0.5 0 0.5 1 1.5 -1.5 -1.5 -1 -0.5 0 0.5 1 1.5 Figure 3: S-shaped sheet in 3D. (a) N = 2000 random points on a surface of an S-shaped sheet in 3D. (b) Normal noise added. XY-plane projection of the data. (c) Optimal manifold points in 3D, projected onto an XY plane for easy visualization. a phase transition, and the resulting manifold becomes two dimensional to take into account the local structure. Alternatively, if we take λ → ∞, the cost of information rate becomes very high and the whole manifold collapses to a single point (becomes zero dimensional). 5.2 S-surface Here we took N = 2000 points covering an S-shaped sheet in three dimensions (Figure 3a), and then scattered the position of each point by adding Gaussian noise. The resulting manifold is difﬁcult to visualize in three dimensions, so we provided its projection onto an XY plane for an illustrative purpose (Figure 3b). After running our algorithm we have recovered the original structure of the manifold (Figure 3c). 6 Discussion The problem of ﬁnding low dimensional manifolds in high dimensional data requires regularization to avoid hgihly folded, Peano curve like solutions which are low dimensional in the mathematical sense but fail to capture our geometric intuition. Rather than constraining geometrical features of the manifold (e.g., the curvature) we have constrained the mutual information between positions on the manifold and positions in the original data space, and this is invariant to all invertible coordinate transformations in either space. This approach enforces “smoothness” of the manifold only implicitly, but nonetheless seems to work. Our information theoretic approach has considerable generality relative to methods based on speciﬁc smoothing criteria, but requires a separate algorithm, such as LLE, to give the manifold points curvilinear coordinates. For data points not in the original data set, equations (9-10) and (13-14) provide the mapping onto the manifold. Eqn. (7) gives the probability distribution over the latent variable, known in the density modeling literature as “the prior.” The running time of the algorithm is linear in N . This compares favorably with other methods and makes it particularly attractive for very large data sets. The number of manifold points K usually is chosen as large as possible, given the computational constraints, to have a dense sampling of the manifold. However, a value of K << N is often sufﬁcient, since D(λ, K) → D(λ) and I(λ, K) → I(λ) approach their limits rather quickly (the convergence improves for large λ and deteriorates for small λ). In the example of a semi-circle, the value of K = 30 was sufﬁcient at the compression level of I = 2.8 bits. In general, the threshold value for K scales exponentially with the latent dimensionality (rather than with the dimensionality of the embedding space). The choice of λ depends on the desired information resolution, since I depends on λ. Ideally, one should plot the function I(λ) and then choose the region of interest. I(λ) is a monotonically decreasing function, with the kinks corresponding to phase transitions where the optimal manifold abruptly changes its dimensionality. In practice, we may want to run the algorithm only for a few choices of λ, and we would like to start with values that are most likely to correspond to a low dimensional latent variable representation. In this case, as a rule of thumb, we choose λ smaller, but on the order of the largest linear dimension (i.e. λ/2 ∼ Lmax ). The dependence of the optimal manifold M(I) on information resolution reﬂects the multi-scale nature of the data and should not be taken as a shortcoming. References [1] Bregler, C. & Omohundro, S. (1995) Nonlinear image interpolation using manifold learning. Advances in Neural Information Processing Systems 7. MIT Press. [2] Hastie, T. & Stuetzle, W. (1989) Principal curves. Journal of the American Statistical Association, 84(406), 502-516. [3] Roweis, S. & Saul, L. (2000) Nonlinear dimensionality reduction by locally linear embedding. Science, 290, 2323–2326. [4] Tenenbaum, J., de Silva, V., & Langford, J. (2000) A global geometric framework for nonlinear dimensionality reduction. Science, 290 , 2319–2323. [5] Hotelling, H. (1933) Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24:417-441,498-520. [6] Bishop, C., Svensen, M. & Williams, C. (1998) GTM: The generative topographic mapping. Neural Computation,10, 215–234. [7] Brand, M. (2003) Charting a manifold. Advances in Neural Information Processing Systems 15. MIT Press. [8] Scholkopf, B., Smola, A. & Muller K-R. (1998) Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10, 1299-1319. [9] Kramer, M. (1991) Nonlinear principal component analysis using autoassociative neural networks. AIChE Journal, 37, 233-243. [10] Belkin M. & Niyogi P. (2003) Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6), 1373-1396. [11] Blahut, R. (1972) Computation of channel capacity and rate distortion function. IEEE Trans. Inform. Theory, IT-18, 460-473. [12] Grassberger, P., & Procaccia, I. (1983) Characterization of strange attractors. Physical Review Letters, 50, 346-349.</p><p>5 0.11059473 <a title="77-tfidf-5" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>6 0.10314795 <a title="77-tfidf-6" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>7 0.080855966 <a title="77-tfidf-7" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>8 0.078820556 <a title="77-tfidf-8" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>9 0.076394193 <a title="77-tfidf-9" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>10 0.065843448 <a title="77-tfidf-10" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>11 0.065211222 <a title="77-tfidf-11" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<p>12 0.064550981 <a title="77-tfidf-12" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>13 0.063875109 <a title="77-tfidf-13" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>14 0.063514441 <a title="77-tfidf-14" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>15 0.062475339 <a title="77-tfidf-15" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>16 0.062390056 <a title="77-tfidf-16" href="./nips-2003-Locality_Preserving_Projections.html">120 nips-2003-Locality Preserving Projections</a></p>
<p>17 0.060816042 <a title="77-tfidf-17" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>18 0.060666207 <a title="77-tfidf-18" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>19 0.059108943 <a title="77-tfidf-19" href="./nips-2003-Out-of-Sample_Extensions_for_LLE%2C_Isomap%2C_MDS%2C_Eigenmaps%2C_and_Spectral_Clustering.html">150 nips-2003-Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering</a></p>
<p>20 0.058734208 <a title="77-tfidf-20" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, -0.069), (2, 0.009), (3, 0.004), (4, -0.114), (5, 0.16), (6, 0.089), (7, -0.08), (8, 0.1), (9, -0.168), (10, -0.055), (11, -0.042), (12, -0.016), (13, -0.035), (14, 0.06), (15, -0.042), (16, 0.012), (17, 0.153), (18, 0.122), (19, 0.027), (20, -0.027), (21, 0.119), (22, -0.06), (23, 0.006), (24, 0.126), (25, 0.056), (26, -0.018), (27, -0.1), (28, -0.036), (29, -0.203), (30, 0.054), (31, 0.052), (32, 0.002), (33, 0.055), (34, 0.086), (35, 0.061), (36, -0.015), (37, 0.103), (38, -0.04), (39, -0.056), (40, -0.07), (41, -0.078), (42, 0.024), (43, -0.047), (44, 0.083), (45, -0.035), (46, 0.067), (47, 0.208), (48, 0.017), (49, -0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94945639 <a title="77-lsi-1" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>2 0.81887329 <a title="77-lsi-2" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>Author: Jakob J. Verbeek, Sam T. Roweis, Nikos A. Vlassis</p><p>Abstract: We propose a non-linear Canonical Correlation Analysis (CCA) method which works by coordinating or aligning mixtures of linear models. In the same way that CCA extends the idea of PCA, our work extends recent methods for non-linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinates are observed, each lying on a different high dimensional manifold. We also show that a special case of our method, when applied to only a single manifold, reduces to the Laplacian Eigenmaps algorithm. As with previous alignment schemes, once the mixture models have been estimated, all of the parameters of our model can be estimated in closed form without local optima in the learning. Experimental results illustrate the viability of the approach as a non-linear extension of CCA. 1</p><p>3 0.60467285 <a title="77-lsi-3" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><p>4 0.55532318 <a title="77-lsi-4" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>Author: Max Welling, Christopher Williams, Felix V. Agakov</p><p>Abstract: Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for “extreme components analysis” (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efﬁcient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.</p><p>5 0.54933697 <a title="77-lsi-5" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, Geoffrey J. Gordon, David S. Touretzky, Nathaniel D. Daw</p><p>Abstract: We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments. Traditional accounts of conditioning ﬁt parameters within a ﬁxed generative model of reinforcer delivery; uncertainty over the model structure is not considered. We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes. According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justiﬁed by additional experience. 1</p><p>6 0.46833089 <a title="77-lsi-6" href="./nips-2003-Optimal_Manifold_Representation_of_Data%3A_An_Information_Theoretic_Approach.html">149 nips-2003-Optimal Manifold Representation of Data: An Information Theoretic Approach</a></p>
<p>7 0.41594815 <a title="77-lsi-7" href="./nips-2003-Locality_Preserving_Projections.html">120 nips-2003-Locality Preserving Projections</a></p>
<p>8 0.4156284 <a title="77-lsi-8" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>9 0.41127053 <a title="77-lsi-9" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>10 0.4089914 <a title="77-lsi-10" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>11 0.39199722 <a title="77-lsi-11" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>12 0.37583244 <a title="77-lsi-12" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>13 0.36233306 <a title="77-lsi-13" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>14 0.35640311 <a title="77-lsi-14" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>15 0.33374649 <a title="77-lsi-15" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>16 0.3131313 <a title="77-lsi-16" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>17 0.29999879 <a title="77-lsi-17" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.29835024 <a title="77-lsi-18" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>19 0.29325268 <a title="77-lsi-19" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>20 0.29281247 <a title="77-lsi-20" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (11, 0.025), (29, 0.013), (30, 0.015), (35, 0.045), (49, 0.013), (53, 0.136), (57, 0.339), (69, 0.022), (71, 0.069), (76, 0.049), (85, 0.054), (91, 0.088), (99, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76292419 <a title="77-lda-1" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>2 0.49946612 <a title="77-lda-2" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>3 0.4984732 <a title="77-lda-3" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors. 1</p><p>4 0.49711356 <a title="77-lda-4" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>5 0.49697027 <a title="77-lda-5" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 1</p><p>6 0.49635288 <a title="77-lda-6" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>7 0.49495316 <a title="77-lda-7" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>8 0.49480531 <a title="77-lda-8" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>9 0.49309504 <a title="77-lda-9" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>10 0.49046671 <a title="77-lda-10" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>11 0.49028805 <a title="77-lda-11" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>12 0.48958254 <a title="77-lda-12" href="./nips-2003-Optimal_Manifold_Representation_of_Data%3A_An_Information_Theoretic_Approach.html">149 nips-2003-Optimal Manifold Representation of Data: An Information Theoretic Approach</a></p>
<p>13 0.48946384 <a title="77-lda-13" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>14 0.48804682 <a title="77-lda-14" href="./nips-2003-Geometric_Clustering_Using_the_Information_Bottleneck_Method.html">82 nips-2003-Geometric Clustering Using the Information Bottleneck Method</a></p>
<p>15 0.48802724 <a title="77-lda-15" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>16 0.48798761 <a title="77-lda-16" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>17 0.48702857 <a title="77-lda-17" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>18 0.48647177 <a title="77-lda-18" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>19 0.48643219 <a title="77-lda-19" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>20 0.4861728 <a title="77-lda-20" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
