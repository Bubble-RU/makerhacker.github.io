<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2003-1-norm Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-1" href="#">nips2003-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2003-1-norm Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2003-1-pdf" href="http://papers.nips.cc/paper/2450-1-norm-support-vector-machines.pdf">pdf</a></p><p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><p>Reference: <a title="nips-2003-1-reference" href="../nips2003_reference/nips-2003-1-norm_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. [sent-5, score-0.22]
</p><p>2 We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. [sent-6, score-0.861]
</p><p>3 (xn , yn ), where the input xi ∈ Rp , and the output yi ∈ {1, −1} is binary. [sent-10, score-0.224]
</p><p>4 To handle this problem, we consider the 1-norm support vector machine (SVM):    n  min β0 ,β  s. [sent-12, score-0.097]
</p><p>5 1 − yi β0 +  i=1  β  q  βj hj (xi )  j=1 1  = |β1 | + · · · + |βq | ≤ s,  (1)  +  (2)  where D = {h1 (x), . [sent-14, score-0.425]
</p><p>6 hq (x)} is a dictionary of basis functions, and s is a tuning parameˆ ˆ ter. [sent-17, score-0.281]
</p><p>7 The solution is denoted as β0 (s) and β(s); the £tted model is q  ˆ βj hj (x). [sent-18, score-0.35]
</p><p>8 We argue in this paper that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. [sent-21, score-0.22]
</p><p>9 ˆ To get a good £tted model f (x) that performs well on future data, we also need to select an appropriate tuning parameter s. [sent-22, score-0.231]
</p><p>10 Under some mild ˆ assumptions, we show that the computational cost to compute the whole solution path β(s) 2 is O(nq min(n, q) ) in the worst case and O(nq) in the best case. [sent-25, score-0.583]
</p><p>11 8  Before delving into the technical details, we illustrate the concept of piece-wise linearity ˆ of the solution path β(s) with a simple example. [sent-31, score-0.425]
</p><p>12 The £rst class has two standard normal independent inputs x 1 , x2 . [sent-33, score-0.134]
</p><p>13 The dictionary of basis functions is D = { 2x1 , 2x2 , 2x1 x2 , x2 , x2 }. [sent-36, score-0.155]
</p><p>14 The solution 1 2 ˆ path β(s) as a function of s is shown in Figure 1. [sent-37, score-0.356]
</p><p>15 Hence the right derivative of β(s) with respect to s is piece-wise constant (in Rq ). [sent-39, score-0.154]
</p><p>16 5  ˆ Figure 1: The solution path β(s) as a function of s. [sent-45, score-0.356]
</p><p>17 In section 3, we ˆ describe the algorithm that computes the whole solution path β(s). [sent-47, score-0.522]
</p><p>18 2  Regularized support vector machines  The standard 2-norm SVM is equivalent to £t a model that    n  min  β0 ,βj  i=1  1 − yi β0 +  q  j=1  βj hj (xi ) + λ β  2 2,  (4)  +  where λ is a tuning parameter. [sent-49, score-0.648]
</p><p>19 In practice, people usually choose hj (x)’s to be the basis functions of a reproducing kernel Hilbert space. [sent-50, score-0.386]
</p><p>20 In this paper, however, we will concentrate on the basis representation (3) rather than a kernel representation. [sent-54, score-0.104]
</p><p>21 Notice that (4) has the form loss + penalty, and λ is the tuning parameter that controls the tradeoff between loss and penalty. [sent-55, score-0.366]
</p><p>22 The loss (1 − yf )+ is called the hinge loss, and  the penalty is called the ridge penalty. [sent-56, score-0.569]
</p><p>23 The ridge ˆ penalty shrinks the £tted coef£cients β towards zero. [sent-58, score-0.453]
</p><p>24 It is well known that this shrinkage ˆ has the effect of controlling the variances of β, hence possibly improves the £tted model’s prediction accuracy, especially when there are many highly correlated features [6]. [sent-59, score-0.142]
</p><p>25 So from a statistical function estimation point of view, the ridge penalty could possibly explain the success of the SVM ([6] and [12]). [sent-60, score-0.411]
</p><p>26 On the other hand, computational learning theory has associated the good performance of the SVM to its margin maximizing property [11], a property of the hinge loss. [sent-61, score-0.122]
</p><p>27 In this paper, we replace the ridge penalty in (4) with the L1 -norm of β, i. [sent-63, score-0.411]
</p><p>28 the lasso penalty [10], and consider the 1-norm SVM problem:    n  min β0 ,β  i=1  1 − yi β0 +  q  j=1  βj hj (xi ) + λ β  1,  (5)  +  which is an equivalent Lagrange version of the optimization problem (1)-(2). [sent-65, score-0.954]
</p><p>29 The lasso penalty was £rst proposed in [10] for regression problems, where the response y is continuous rather than categorical. [sent-66, score-0.482]
</p><p>30 Similar to the ridge penalty, the lasso penalty also ˆ shrinks the £tted coef£cients β’s towards zero, hence (5) also bene£ts from the reduction in £tted coef£cients’ variances. [sent-68, score-0.768]
</p><p>31 Another property of the lasso penalty is that because of the L1 nature of the penalty, making λ suf£ciently large, or equivalently s suf£ciently small, ˆ will cause some of the coef£cients βj ’s to be exactly zero. [sent-69, score-0.511]
</p><p>32 Thus the lasso penalty does a kind of continuous feature selection, while this is not the case for the ridge penalty. [sent-71, score-0.689]
</p><p>33 It is interesting to note that the ridge penalty corresponds to a Gaussian prior for the βj ’s, while the lasso penalty corresponds to a double-exponential prior. [sent-73, score-0.893]
</p><p>34 This re¤ects the greater tendency of the lasso to produce some large £tted coef£cients and leave others at 0, especially in high dimensional problems. [sent-75, score-0.291]
</p><p>35 only a small number of true coef£cients β j ’s are nonzero, the lasso penalty works better than the ridge penalty; while in the non-sparse scenario, e. [sent-83, score-0.662]
</p><p>36 the true coef£cients β j ’s have a Gaussian distribution, neither the lasso penalty nor the ridge penalty will £t the coef£cients well, since there is too little data from which to estimate these non-zero coef£cients. [sent-85, score-0.893]
</p><p>37 Based on these observations, [3] further propose the bet on sparsity principle for highdimensional problems, which encourages using lasso penalty. [sent-87, score-0.251]
</p><p>38 To solve the 1-norm SVM for a £xed value of s, we can transform (1)-(2) into a linear programming ˆ problem and use standard software packages; but to get a good £tted model f (x) that performs well on future data, we need to select an appropriate value for the tuning paramter s. [sent-89, score-0.231]
</p><p>39 In this section, we propose an ef£cient algorithm that computes the whole solution path ˆ β(s), hence facilitates adaptive selection of s. [sent-90, score-0.735]
</p><p>40 This implies that the derivative of β(s) with respect to s is piece-wise constant, because when the Karush-Kuhn-Tucker conditions do ˆ not change, the derivative of β(s) will not change either. [sent-93, score-0.308]
</p><p>41 solution path β(s) ˆ Thus to compute the whole solution path β(s), all we need to do is to £nd the joints, i. [sent-96, score-0.841]
</p><p>42 the asterisk points in Figure 1, on this piece-wise linear path, then use straight lines to ˆ ˆ interpolate them, or equivalently, to start at β(0) = 0, £nd the right derivative of β(s), let ˆ gets to a joint. [sent-98, score-0.206]
</p><p>43 s increase and only change the derivative when β(s) 3. [sent-99, score-0.154]
</p><p>44 Let V = {j : βj (s) = 0}, E = {i : 1 − yi fi = 0}, ˆ ˆ ˆ > 0} and u for the right derivative of βV (s): u 1 = 1 and βV (s) L = {i : 1 − yi fi ˆ denotes the components of β(s) with indices in V. [sent-103, score-0.836]
</p><p>45 Without loss of generality, we assume ˆ ˆ ˆ #{yi = 1} ≥ #{yi = −1}; then β0 (0) = 1, βj (0) = 0. [sent-104, score-0.12]
</p><p>46 We consider a modi£ed problem: follows, we need to compute the derivative of β(s) (1 − yi fi )+ +  min  β0 ,βj  yi =1  (1 − yi fi )  (6)  yi =−1 q  β  s. [sent-106, score-1.271]
</p><p>47 (7)  j=1  Notice that if yi = 1, the loss is still (1 − yi fi )+ ; but if yi = −1, the loss becomes ˆ (1 − yi fi ). [sent-109, score-1.278]
</p><p>48 In this setup, the derivative of β(∆s) with respect to ∆s is the same no matter ˆ what value ∆s is, and one can show that it coincides with the right derivative of β(s) ˆ when s is suf£ciently small. [sent-110, score-0.308]
</p><p>49 Hence this setup helps us £nd the initial derivative u of β(s). [sent-111, score-0.154]
</p><p>50 3  Main algorithm  ˆ The main algorithm that computes the whole solution path β(s) proceeds as following: 1. [sent-117, score-0.522]
</p><p>51 ˆold ˆ ˆ ˆ Let the current β0 , β and s be denoted by β0 , β old and sold . [sent-124, score-0.159]
</p><p>52 For each j ∗ ∈ V, we solve: / u0 + V uj hj (xi ) + uj ∗ hj ∗ (xi ) ˆ sign(β old )uj + |uj ∗ |  = 0 for i ∈ E = 1 j V where u0 , uj and uj ∗ are the unknowns. [sent-126, score-1.566]
</p><p>53 We then compute: ∆lossj ∗ = ∆s  yi  u0 +  L  uj hj (xi ) + uj ∗ hj ∗ (xi ) . [sent-127, score-1.166]
</p><p>54 For each i ∈ E, we solve: u0 + V uj hj (xi ) = 0 for i ∈ E\{i } ˆold = 1 V sign(βj )uj where u0 and uj are the unknowns. [sent-129, score-0.741]
</p><p>55 We then compute: ∆lossi = ∆s  (9)  yi  u0 +  L  (11)  uj hj (xi ) . [sent-130, score-0.672]
</p><p>56 Hence, ∆s • If the smallest ∆loss is non-negative, the algorithm terminates; else ∆s • If the smallest negative ∆loss corresponds to a j ∗ in step 2, we update ∆s V ← V ∪ {j ∗ }, u ←  u uj ∗  . [sent-135, score-0.396]
</p><p>57 =  ˆold β0 ˆold βV  + ∆s ·  u0 u  ,  (15)  ˆ In the end, we get a path β(s), which is piece-wise linear. [sent-138, score-0.28]
</p><p>58 4  Remarks  Due to the page limit, we omit the proof that this algorithm does indeed give the exact ˆ whole solution path β(s) of (1)-(2) (see [13] for detailed proof). [sent-140, score-0.453]
</p><p>59 ˆ Step 2 computes the possible right derivative of β(s) if adding each basis function hj ∗ (x) ˆ into V. [sent-144, score-0.545]
</p><p>60 Step 3 computes the possible right derivative of β(s) if removing each point i ˆ (determined by either (9) or (11)) is such that from E. [sent-145, score-0.223]
</p><p>61 The possible right derivative of β(s) the training points in E are kept in E when s increases, until the next joint (step 1) occurs. [sent-146, score-0.191]
</p><p>62 ˆ ∆loss/∆s indicates how fast the loss will decrease if β(s) changes according to u. [sent-147, score-0.165]
</p><p>63 When the loss can not be decreased, the algorithm terminates. [sent-149, score-0.12]
</p><p>64 06) 65  # Joints 94 (13) 149 (20) 225 (30) 374 (52) 499 (67)  Computational cost  ˆ We have proposed an algorithm that computes the whole solution path β(s). [sent-181, score-0.553]
</p><p>65 Suppose |E| = m at a joint on the piece-wise linear solution path, then it takes O(qm2 ) to compute step 2 and step 3 of the algorithm through Sherman-Morrison updating formula. [sent-183, score-0.213]
</p><p>66 If we assume the training data are separable by the dictionary D, then all the training data are eventually ˆ going to have loss (1 − yi fi )+ equal to zero. [sent-184, score-0.615]
</p><p>67 Hence it is reasonable to assume the number of joints on the piece-wise linear solution path is O(n). [sent-185, score-0.547]
</p><p>68 Since the maximum value of m is min(n, q) and the minimum value of m is 1, we get the worst computational cost is O(nq min(n, q)2 ) and the best computational cost is O(nq). [sent-186, score-0.116]
</p><p>69 Simulation results (section 4) actually indicate that the number of joints tends to be O(min(n, q)). [sent-188, score-0.191]
</p><p>70 1  Simulation results  The data generation mechanism is the same as the one described in section 1, except that we generate 50 training data in each of two classes, and to make harder problems, we sequentially augment the inputs with additional two, four, six and eight standard normal noise inputs. [sent-191, score-0.21]
</p><p>71 In the original input space, a hyperplane cannot separate the classes; we use an enlarged feature space corresponding to √ 2nd degree polythe √ nomial kernel, hence the dictionary of basis functions is D = { 2xj , 2xj xj , x2 , j, j = j 1, . [sent-195, score-0.271]
</p><p>72 The average test errors over 50 simulations, with different numbers of noise inputs, are shown in Table 1. [sent-200, score-0.105]
</p><p>73 For both the 1-norm SVM and the 2-norm SVM, we choose the tuning parameters to minimize the test error, to be as fair as possible to each method. [sent-201, score-0.156]
</p><p>74 From Table 1 we can see that the non-penalized SVM performs signi£cantly worse than the penalized ones; the 1-norm SVM and the 2-norm SVM perform similarly when there is no noise input (line 1), but the 2-norm SVM is adversely affected by noise inputs (line 2 - line 5). [sent-203, score-0.288]
</p><p>75 Since the 1-norm SVM has the ability to select relevant features and ignore redundant features, it does not suffer from the noise inputs as much as the 2-norm SVM. [sent-204, score-0.296]
</p><p>76 Table 1 also shows the number of basis functions q and the number of joints on the piece-wise linear solution path. [sent-205, score-0.369]
</p><p>77 The left panel is the ˆ piece-wise linear solution path β(s). [sent-217, score-0.393]
</p><p>78 The middle panel is the test error along the solution path. [sent-219, score-0.17]
</p><p>79 The right panel illustrates the linear relationship between the number of basis functions and the number of joints on the solution path when q < n. [sent-221, score-0.659]
</p><p>80 2  Real data results  In this section, we apply the 1-norm SVM to classi£cation of gene microarrays. [sent-223, score-0.13]
</p><p>81 Classi£cation of patient samples is an important aspect of cancer diagnosis and treatment. [sent-224, score-0.182]
</p><p>82 The 2-norm SVM has been successfully applied to microarray cancer diagnosis problems ([5] and [7]). [sent-225, score-0.282]
</p><p>83 However, one weakness of the 2-norm SVM is that it only predicts a cancer class label but does not automatically select relevant genes for the classi£cation. [sent-226, score-0.327]
</p><p>84 Often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classi£cation, rather than class prediction. [sent-227, score-0.409]
</p><p>85 [4] and [5] have proposed gene selection methods, which we call univariate ranking (UR) and recursive feature elimination (RFE) (see [14]), that can be combined with the 2-norm SVM. [sent-228, score-0.237]
</p><p>86 However, these procedures are two-step procedures that depend on external gene selection methods. [sent-229, score-0.21]
</p><p>87 On the other hand, the 1-norm SVM has an inherent gene (feature) selection property due to the lasso penalty. [sent-230, score-0.49]
</p><p>88 Hence the 1-norm SVM achieves the goals of classi£cation of patients and selection of genes simultaneously. [sent-231, score-0.171]
</p><p>89 This data set consists of 38 training data and 34 test data of two types of acute leukemia, acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). [sent-233, score-0.462]
</p><p>90 The tuning parameter is chosen according to 10-fold cross-validation, then the £nal model is £tted on all the training data and evaluated on the test data. [sent-240, score-0.193]
</p><p>91 The number of joints on the solution path is 104, which appears to be O(n) O(q). [sent-241, score-0.547]
</p><p>92 We should notice that the maximum number of genes that the 1-norm SVM can select is upper bounded by n, which is usually much less than q in microarray problems. [sent-244, score-0.294]
</p><p>93 We illustrate that the 1-norm SVM may have some advantage over the 2-norm SVM, especially when there are redundant features. [sent-246, score-0.14]
</p><p>94 ˆ The solution path β(s) of the 1-norm SVM is a piece-wise linear function in the tuning  Table 2: Results on Microarray Classi£cation Method 2-norm SVM UR 2-norm SVM RFE 1-norm SVM  CV Error 2/38 2/38 2/38  Test Error 3/34 1/34 2/34  # of Genes 22 31 17  parameter s. [sent-247, score-0.482]
</p><p>95 We have proposed an ef£cient algorithm to compute the whole solution path ˆ β(s) of the 1-norm SVM, and facilitate adaptive selection of the tuning parameter s. [sent-248, score-0.753]
</p><p>96 (1998) Feature selection via concave minimization and support vector machines. [sent-253, score-0.13]
</p><p>97 (1999) Molecular classi£cation of cancer: class discovery and class prediction by gene expression monitoring. [sent-284, score-0.202]
</p><p>98 (2002) Gene selection for cancer classi£cation using support vector machines. [sent-290, score-0.244]
</p><p>99 (2003) Boosting as a regularized path to a maximum margin classi£er. [sent-309, score-0.304]
</p><p>100 (2003) Classi£cation of gene microarrays by penalized logistic regression. [sent-342, score-0.168]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svm', 0.363), ('path', 0.253), ('lasso', 0.251), ('hj', 0.247), ('uj', 0.247), ('penalty', 0.231), ('joints', 0.191), ('ridge', 0.18), ('yi', 0.178), ('fi', 0.163), ('tted', 0.163), ('derivative', 0.154), ('gene', 0.13), ('tuning', 0.126), ('loss', 0.12), ('nq', 0.115), ('cancer', 0.114), ('coef', 0.104), ('solution', 0.103), ('microarray', 0.1), ('whole', 0.097), ('tibshirani', 0.095), ('hastie', 0.094), ('genes', 0.091), ('leukemia', 0.085), ('old', 0.084), ('zhu', 0.084), ('cients', 0.082), ('dictionary', 0.08), ('selection', 0.08), ('basis', 0.075), ('noise', 0.075), ('acute', 0.075), ('sold', 0.075), ('inputs', 0.072), ('classi', 0.072), ('computes', 0.069), ('diagnosis', 0.068), ('rosset', 0.068), ('hence', 0.064), ('redundant', 0.063), ('smallest', 0.055), ('simulation', 0.054), ('notice', 0.053), ('stanford', 0.052), ('rfe', 0.05), ('rq', 0.05), ('select', 0.05), ('support', 0.05), ('min', 0.047), ('xi', 0.046), ('changes', 0.045), ('shrinks', 0.042), ('ur', 0.042), ('argue', 0.042), ('cation', 0.041), ('especially', 0.04), ('mild', 0.04), ('step', 0.039), ('hinge', 0.038), ('penalized', 0.038), ('shrinkage', 0.038), ('training', 0.037), ('panel', 0.037), ('illustrate', 0.037), ('grant', 0.036), ('class', 0.036), ('relevant', 0.036), ('reproducing', 0.035), ('adaptive', 0.035), ('facilitates', 0.034), ('nih', 0.034), ('linearity', 0.032), ('residual', 0.032), ('compute', 0.032), ('cost', 0.031), ('sign', 0.03), ('hilbert', 0.03), ('scenario', 0.03), ('test', 0.03), ('kernel', 0.029), ('table', 0.029), ('property', 0.029), ('performs', 0.028), ('boosting', 0.027), ('feature', 0.027), ('facilitate', 0.027), ('friedman', 0.027), ('gets', 0.027), ('paths', 0.027), ('worst', 0.027), ('get', 0.027), ('margin', 0.026), ('normal', 0.026), ('regularized', 0.025), ('lines', 0.025), ('enlarged', 0.025), ('packages', 0.025), ('retention', 0.025), ('rob', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="1-tfidf-1" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><p>2 0.2729837 <a title="1-tfidf-2" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Trevor J. Hastie</p><p>Abstract: Margin maximizing properties play an important role in the analysis of classi£cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a suf£cient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classi£cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines. 1</p><p>3 0.13452928 <a title="1-tfidf-3" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>Author: Pedro J. Moreno, Purdy P. Ho, Nuno Vasconcelos</p><p>Abstract: Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identiﬁcation/veriﬁcation and image classiﬁcation tasks and show that these new kernels have the best performance in speaker veriﬁcation and mostly outperform the Fisher kernel based SVM’s and the generative classiﬁers in speaker identiﬁcation and image classiﬁcation. 1</p><p>4 0.1280264 <a title="1-tfidf-4" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>5 0.12614666 <a title="1-tfidf-5" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classiﬁcation of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with ﬁve known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 1</p><p>6 0.11792072 <a title="1-tfidf-6" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>7 0.11757049 <a title="1-tfidf-7" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>8 0.11720975 <a title="1-tfidf-8" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>9 0.11123611 <a title="1-tfidf-9" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>10 0.10928929 <a title="1-tfidf-10" href="./nips-2003-ICA-based_Clustering_of_Genes_from_Microarray_Expression_Data.html">86 nips-2003-ICA-based Clustering of Genes from Microarray Expression Data</a></p>
<p>11 0.10549048 <a title="1-tfidf-11" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>12 0.10127951 <a title="1-tfidf-12" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>13 0.098366171 <a title="1-tfidf-13" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>14 0.097732931 <a title="1-tfidf-14" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>15 0.088522792 <a title="1-tfidf-15" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>16 0.085511275 <a title="1-tfidf-16" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>17 0.079384036 <a title="1-tfidf-17" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>18 0.077860087 <a title="1-tfidf-18" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>19 0.07774093 <a title="1-tfidf-19" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>20 0.076490305 <a title="1-tfidf-20" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.249), (1, -0.139), (2, -0.044), (3, -0.203), (4, 0.088), (5, -0.003), (6, -0.122), (7, -0.095), (8, 0.009), (9, 0.117), (10, 0.091), (11, 0.086), (12, -0.119), (13, 0.033), (14, -0.065), (15, 0.23), (16, 0.018), (17, -0.013), (18, 0.064), (19, 0.001), (20, -0.048), (21, -0.01), (22, 0.069), (23, 0.174), (24, 0.085), (25, -0.01), (26, -0.057), (27, 0.008), (28, -0.087), (29, 0.095), (30, -0.094), (31, 0.073), (32, -0.052), (33, -0.047), (34, 0.09), (35, 0.021), (36, -0.084), (37, 0.037), (38, -0.055), (39, -0.031), (40, 0.002), (41, 0.052), (42, 0.137), (43, -0.064), (44, -0.024), (45, 0.011), (46, 0.028), (47, -0.038), (48, -0.089), (49, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96517634 <a title="1-lsi-1" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><p>2 0.72138339 <a title="1-lsi-2" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Trevor J. Hastie</p><p>Abstract: Margin maximizing properties play an important role in the analysis of classi£cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a suf£cient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classi£cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines. 1</p><p>3 0.67002702 <a title="1-lsi-3" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classiﬁcation of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with ﬁve known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 1</p><p>4 0.61628842 <a title="1-lsi-4" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>Author: Ben Taskar, Carlos Guestrin, Daphne Koller</p><p>Abstract: In typical classiﬁcation tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of conﬁdence of the classiﬁer, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efﬁciently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efﬁcient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classiﬁcation demonstrate very signiﬁcant gains over previous approaches. 1</p><p>5 0.57893002 <a title="1-lsi-5" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>Author: Thomas R. Strohmann, Andrei Belitski, Gregory Z. Grudic, Dennis DeCoste</p><p>Abstract: The Minimax Probability Machine Classiﬁcation (MPMC) framework [Lanckriet et al., 2002] builds classiﬁers by minimizing the maximum probability of misclassiﬁcation, and gives direct estimates of the probabilistic accuracy bound Ω. The only assumptions that MPMC makes is that good estimates of means and covariance matrices of the classes exist. However, as with Support Vector Machines, MPMC is computationally expensive and requires extensive cross validation experiments to choose kernels and kernel parameters that give good performance. In this paper we address the computational cost of MPMC by proposing an algorithm that constructs nonlinear sparse MPMC (SMPMC) models by incrementally adding basis functions (i.e. kernels) one at a time – greedily selecting the next one that maximizes the accuracy bound Ω. SMPMC automatically chooses both kernel parameters and feature weights without using computationally expensive cross validation. Therefore the SMPMC algorithm simultaneously addresses the problem of kernel selection and feature selection (i.e. feature weighting), based solely on maximizing the accuracy bound Ω. Experimental results indicate that we can obtain reliable bounds Ω, as well as test set accuracies that are comparable to state of the art classiﬁcation algorithms.</p><p>6 0.5260734 <a title="1-lsi-6" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>7 0.49558944 <a title="1-lsi-7" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>8 0.49313781 <a title="1-lsi-8" href="./nips-2003-ICA-based_Clustering_of_Genes_from_Microarray_Expression_Data.html">86 nips-2003-ICA-based Clustering of Genes from Microarray Expression Data</a></p>
<p>9 0.49145916 <a title="1-lsi-9" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>10 0.48758221 <a title="1-lsi-10" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>11 0.48361549 <a title="1-lsi-11" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>12 0.47544762 <a title="1-lsi-12" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>13 0.47407293 <a title="1-lsi-13" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>14 0.46526116 <a title="1-lsi-14" href="./nips-2003-Sparseness_of_Support_Vector_Machines---Some_Asymptotically_Sharp_Bounds.html">180 nips-2003-Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds</a></p>
<p>15 0.43418947 <a title="1-lsi-15" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>16 0.41966867 <a title="1-lsi-16" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>17 0.4160358 <a title="1-lsi-17" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>18 0.38276774 <a title="1-lsi-18" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>19 0.37744436 <a title="1-lsi-19" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>20 0.3748154 <a title="1-lsi-20" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (11, 0.012), (35, 0.583), (53, 0.066), (71, 0.041), (76, 0.042), (85, 0.063), (91, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97572654 <a title="1-lda-1" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>Author: Tom Heskes, Onno Zoeter, Wim Wiegerinck</p><p>Abstract: We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach. 1</p><p>2 0.95796746 <a title="1-lda-2" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>Author: Pedro F. Felzenszwalb, Daniel P. Huttenlocher, Jon M. Kleinberg</p><p>Abstract: In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artiﬁcially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show an application to traﬃc analysis at a high-volume Web site. 1</p><p>same-paper 3 0.95200145 <a title="1-lda-3" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><p>4 0.68668115 <a title="1-lda-4" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>5 0.61780393 <a title="1-lda-5" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>Author: Claire Monteleoni, Tommi S. Jaakkola</p><p>Abstract: We consider an online learning scenario in which the learner can make predictions on the basis of a ﬁxed set of experts. We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts. On the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics. We demonstrate the new algorithm in the context of wireless networks.</p><p>6 0.61357582 <a title="1-lda-6" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>7 0.58787304 <a title="1-lda-7" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>8 0.57471097 <a title="1-lda-8" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>9 0.55283821 <a title="1-lda-9" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>10 0.55273139 <a title="1-lda-10" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>11 0.53936923 <a title="1-lda-11" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>12 0.53384215 <a title="1-lda-12" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>13 0.53019291 <a title="1-lda-13" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>14 0.52762401 <a title="1-lda-14" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>15 0.52661264 <a title="1-lda-15" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>16 0.52391499 <a title="1-lda-16" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>17 0.51516366 <a title="1-lda-17" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>18 0.51253444 <a title="1-lda-18" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>19 0.50481629 <a title="1-lda-19" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>20 0.50294924 <a title="1-lda-20" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
