<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-183" href="#">nips2003-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</h1>
<br/><p>Source: <a title="nips-2003-183-pdf" href="http://papers.nips.cc/paper/2375-synchrony-detection-by-analogue-vlsi-neurons-with-bimodal-stdp-synapses.pdf">pdf</a></p><p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>Reference: <a title="nips-2003-183-reference" href="../nips2003_reference/nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. [sent-10, score-0.365]
</p><p>2 The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. [sent-11, score-0.619]
</p><p>3 When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. [sent-13, score-0.748]
</p><p>4 We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. [sent-14, score-0.955]
</p><p>5 They are based on mean spike ﬁring rates correlations between presynaptic and postsynaptic spikes rather than upon precise timing diﬀerences between presynaptic and postsynaptic spikes. [sent-16, score-1.092]
</p><p>6 In recent years, new forms of synaptic plasticity that rely on precise spike-timing diﬀerences between presynaptic and postsynaptic spikes have been discovered in several biological systems[1][2][3]. [sent-17, score-0.556]
</p><p>7 These forms of plasticity, generally termed Spike Timing Dependent Plasticity (STDP), increase the synaptic eﬃcacy of a synapse when a presynaptic spike reaches the neuron a few milliseconds before the postsynaptic action potential. [sent-18, score-0.902]
</p><p>8 In contrast, when the postsynaptic neuron ﬁres immediately before the presynaptic neuron the strength of the synapse diminishes. [sent-19, score-0.797]
</p><p>9 The presence of weight dependence in the learning rule has been identiﬁed as having a dramatic eﬀect on the computational properties of STDP. [sent-21, score-0.262]
</p><p>10 When weight modiﬁcations are independent of the weight value, a strong competition takes places between the synapses. [sent-22, score-0.491]
</p><p>11 Hence, even when no spike-timing correlation is present in the input, synapses develop maximum or minimum strength so that a bimodal weight distribution emerges from learning[5]. [sent-23, score-0.82]
</p><p>12 Conversely, if the learning rule is strongly weight-dependent, such that strong synapses receive less potentiation than weaker ones while depression is independent of the synaptic strength,  a smooth unimodal weight distribution emerges from the learning process[6]. [sent-24, score-1.08]
</p><p>13 Hence, they are suited to analog VLSI implementation, as the main barrier to the implementation of on-chip learning, the long term storage of precise analog weight values, can be rendered unimportant. [sent-27, score-0.348]
</p><p>14 However, weight-independent STDP creates a highly unstable learning process that may hinder learning when only low levels of spike-timing correlations exist and neurons have few synapses. [sent-28, score-0.373]
</p><p>15 The circuits proposed here introduce a tunable weight dependence mechanism which stabilises the learning process. [sent-29, score-0.428]
</p><p>16 In the weight-dependent learning experiments reported here the weight-dependence is set at moderate levels such that bimodal weight distributions still result from learning. [sent-31, score-0.391]
</p><p>17 The authors used a weight-dependent scheme and concentrated on the weight normalisation properties of the learning rule. [sent-33, score-0.262]
</p><p>18 STDP synapses that contain an explicit bistable mechanism have been proposed in [10]. [sent-36, score-0.454]
</p><p>19 Long-term bistable synapses are a good technological solution for weight storage. [sent-37, score-0.638]
</p><p>20 However, the maximum and minimum weight limits in bimodal STDP already act as natural attractors. [sent-38, score-0.353]
</p><p>21 2  STDP circuits  The circuits in Figure 1 implement the asymmetric decaying learning window with the abrupt transition at the origin that is so characteristic of STDP. [sent-41, score-0.484]
</p><p>22 The weight of each synapse is represented by the charge stored on its weight capacitor C w . [sent-42, score-0.563]
</p><p>23 The strength of the weight is inversely proportional to Vw . [sent-43, score-0.287]
</p><p>24 Our silicon spiking neurons signal their ﬁring events with the sequence of pulses seen in Figure 1c. [sent-45, score-0.365]
</p><p>25 Signal post bp is back-propagated to the aﬀerent synapses of the neuron. [sent-46, score-0.428]
</p><p>26 Long is also sent to input synapses of following neurons in the activity path (see preLong in 1a). [sent-48, score-0.528]
</p><p>27 Finally, spikeOut is the presynaptic spike for the next receiving neuron (termed pre in Figure 1a). [sent-49, score-0.608]
</p><p>28 More details on the implementation of the silicon neuron can be found in [11] In Figure 1a, if preLong is long enough (a few µs) the voltage created by Ibpot on the diode connected transistor N5 is copied to the gate of N2. [sent-50, score-0.424]
</p><p>29 When the postsynaptic neuron ﬁres, a back propagation pulse post bp switches N3 on. [sent-52, score-0.42]
</p><p>30 Therefore, the weight is potentiated (Vw decreased) by an amount which reﬂects the time elapsed since the last presynaptic event. [sent-53, score-0.389]
</p><p>31 A weight dependence mechanism is introduced by the simple linearised V-I conﬁguration P5-P6 and current mirror N7-N6 (see Figure 1a). [sent-54, score-0.264]
</p><p>32 Thus, a current proportional to the value of the weight is subtracted from Ibpot . [sent-57, score-0.224]
</p><p>33 The resulting smaller current injected into N5 will cause a drop in the peak of potentiation for large weight values. [sent-58, score-0.413]
</p><p>34 The lower Vw , the smaller the weight of the synapse. [sent-61, score-0.224]
</p><p>35 This section of the weight change circuit detects causal spike correlations. [sent-62, score-0.571]
</p><p>36 (b) A single depression circuit present in the soma of the neuron creates the decaying shape of the depression side of the learning window. [sent-63, score-0.68]
</p><p>37 They are used to stimulate the weight change circuits. [sent-65, score-0.307]
</p><p>38 In a similar manner to potentiation, the weight is weakened by the circuit of Figure 1b when it detects a non-causal interaction between a presynaptic and a postsynaptic spike. [sent-66, score-0.623]
</p><p>39 When a postsynaptic spike event is generated a postLong pulse charges Cdep . [sent-67, score-0.419]
</p><p>40 A set of non-linear decaying currents (IdepX ) is sent to the weight change circuits placed in the input synapse (see Idep in Figure 1a). [sent-69, score-0.546]
</p><p>41 When a presynaptic spike reaches a synapse P1 is switched on. [sent-70, score-0.503]
</p><p>42 Only one depression circuit per neuron is required since the depression part of the learning rule is independent of the weight value. [sent-72, score-0.831]
</p><p>43 A chip including 5 spiking neurons with STDP synapses has been fabricated using a standard 0. [sent-73, score-0.58]
</p><p>44 Each neuron has 6 learning synapses, a single excitatory non-learning synapse and a single inhibitory one. [sent-75, score-0.306]
</p><p>45 Along with the silicon neuron circuits, the chip contains several voltage buﬀers that allow us to monitor the behaviour of the neuron. [sent-76, score-0.407]
</p><p>46 The testing setup uses a networked logic analysis system to stimulate the silicon neuron and to capture the results of on-chip learning. [sent-77, score-0.352]
</p><p>47 An externally addressable circuit creates preLong and pre pulses to stimulate the synapses. [sent-78, score-0.273]
</p><p>48 1  Weight-independent learning rule Characterisation  A weight-independent weight change regime is obtained by setting Vr to Vdd in the weight change circuit presented in Figure 1 . [sent-80, score-0.648]
</p><p>49 The resulting learning window on silicon can be seen in Figure 2. [sent-81, score-0.295]
</p><p>50 Each point in the curve was obtained from the stimulation of the ﬁx synapse and a learning synapse with a varying delay between them. [sent-82, score-0.268]
</p><p>51 Figure 2a shows that the peaks for potentiation and depression can be set independently. [sent-84, score-0.324]
</p><p>52 Also, as shown in Figure 2b the decay of the learning window for both sides of the curve can be set independently of the maximum weight change with Vbdep and Vbpot . [sent-85, score-0.434]
</p><p>53 Obviously, when the weight voltage Vw approaches  0. [sent-87, score-0.267]
</p><p>54 15 −40  (a)  −30  −20  −10 t  pre  0 −t  post  10 ( ms )  20  30  40  (b)  Figure 2: Experimental learning window for weight-independent STDP. [sent-103, score-0.279]
</p><p>55 The curves show the weight modiﬁcation induced in the weight of a learning synapse for diﬀerent time intervals between the presynaptic and the postsynaptic spike. [sent-104, score-0.914]
</p><p>56 For the results shown, the synapses were operated in a weight-independent mode. [sent-105, score-0.396]
</p><p>57 The peak for potentiation and depression are tuned independently with Ibpot and Ibdep (b) The rate of decay of the learning window for potentiation and depression can be set independently without aﬀecting the maximum weight change. [sent-107, score-1.074]
</p><p>58 any of the power supply rails a saturation eﬀect occurs as the transistors injecting current in the weight capacitor leave saturation. [sent-108, score-0.224]
</p><p>59 For the learning experiment with weight-independent weight change the area under the potentiation curve should be approximately 50% smaller than the area under the depression region. [sent-109, score-0.624]
</p><p>60 2  Learning spike-timing correlations with weight-independent learning  We stimulated a 6-synapse silicon neuron with 6 independent Poisson-distributed spike trains with a rate of 30Hz. [sent-111, score-0.771]
</p><p>61 Refractoriness helps break the temporal axis into disjoint segments so that presynaptic spikes can make less noisy ”predictions” of the postsynaptic time of ﬁring. [sent-113, score-0.365]
</p><p>62 We introduced spike-timing correlations between the inputs for synapses 1 and 2. [sent-114, score-0.524]
</p><p>63 As can be seen in Figure 3 the weights of synapses that receive correlated activity reach maximum strength (Vw close to GND) whereas the rest decay towards Vdd. [sent-121, score-0.61]
</p><p>64 Clearly, the bimodal weight distribution reﬂects the correlation pattern of the input signals. [sent-122, score-0.4]
</p><p>65 3  Hierarchical synchrony detection  To experiment with hierarchical synchrony detection we included in the chip a small 2-layered network of STDP silicon neurons with the conﬁguration shown in Figure 4. [sent-124, score-0.905]
</p><p>66 Neurons in the ﬁrst layer were stimulated with independent sets of Poisson-distributed spike trains with a mean spiking rate of 30Hz. [sent-125, score-0.42]
</p><p>67 A primary level of correlation was introduced for each neuron in the ﬁrst layer as signalled by the arrowed bridge between the  4. [sent-127, score-0.359]
</p><p>68 5  N3  N4  N5  Figure 4: Final weight values for a 2-layered network of STDP silicon neurons. [sent-138, score-0.378]
</p><p>69 For the results shown here these 2 inputs of each neuron shared 50% of the spike-timings (indicated with 0. [sent-140, score-0.249]
</p><p>70 A secondary level of correlation was introduced between the inputs of synapses 1 and 2 of both N1 and N2, as signalled by the arrow linking the ﬁrst level of correlations of N1 and N2. [sent-142, score-0.719]
</p><p>71 The weights corresponding to synapses 1 and 2 evolve towards the maximum value (i. [sent-148, score-0.393]
</p><p>72 The other neurons in the 1st layer have weight evolutions similar to that of N1. [sent-154, score-0.404]
</p><p>73 Synapses with synchronised activity corresponding to the 1st level of correlations win the competition imposed by STDP. [sent-155, score-0.254]
</p><p>74 Weights of the synapses receiving input from N1 and N2 are reinforced while the rest are decreased towards the minimum possible weight value (Vw = Vdd). [sent-157, score-0.581]
</p><p>75 In Figure 4, we have represented graphically the ﬁnal weight distribution for all synapses. [sent-159, score-0.224]
</p><p>76 As marked by ﬁlled circles, only synapses in the path of hierarchical  N1  4. [sent-160, score-0.405]
</p><p>77 5  0 0  2  4  6  8  10  0 0  time ( s )  5  10  15  20 25 time ( s )  30  35  40  Figure 5: Hierarchical synchrony detection. [sent-170, score-0.23]
</p><p>78 These correspond to synapses of ﬁrst layer neurons which received uncorrelated inputs or synapses of N5 which received inputs from neurons stimulated without a secondary level of correlations (N3-N4). [sent-175, score-1.44]
</p><p>79 1  Weight-dependent learning rule Characterisation  The STDP synapses presented can also be operated in weight-dependent mode. [sent-177, score-0.434]
</p><p>80 The weight dependent learning window implemented is similar to that which seems to underly some STDP recordings from biological neurons [6]. [sent-178, score-0.521]
</p><p>81 The weight change curve for potentiation is given for 3 diﬀerent weight values. [sent-180, score-0.645]
</p><p>82 The larger the weight value (low Vw ), the smaller the degree of potentiation induced in the synapse. [sent-181, score-0.383]
</p><p>83 The depression side of the learning window is unaﬀected by the weight value since the depression circuit shown in Figure 1b does not have an explicit weight-dependent mechanism. [sent-182, score-0.781]
</p><p>84 2  Learning spike-timing correlations with weight-dependent learning  Figure 6b shows the weight evolution for an experiment where the correlated activity between synapses 1 and 2 consisted of only 20% of common spike-timings. [sent-184, score-0.862]
</p><p>85 Finally, we stimulated a neuron in weight-dependent mode with a form of synchrony where spike-timings coincided in a time window (window of correlation) instead of being perfectly matched (syn0-1). [sent-186, score-0.543]
</p><p>86 The synchrony data was an inhomogeneous Poisson spike train with a rate modulated by a binary signal with random transition points. [sent-188, score-0.453]
</p><p>87 Figure 7 shows a normalised histogram of spike intervals between the correlated inputs for synapses 0 and 1 (Figure 7a) and the histogram of the uncorrelated inputs for synapses 2 and 3 (Figure 7b). [sent-189, score-1.3]
</p><p>88 Again, as can be seen in Figure 7c the neuron with weight-dependent STDP can detect this low-level of synchrony with non-coincident spikes. [sent-190, score-0.383]
</p><p>89 Clearly, the bimodal weight distribution identiﬁes the syn-  0. [sent-191, score-0.353]
</p><p>90 Correlated activity causes synapses to develop strong weights (Vw close to GND). [sent-209, score-0.438]
</p><p>91 The inﬂuence of weight-dependence on the ﬁnal weight distribution has been studied extensively[5][6]. [sent-212, score-0.224]
</p><p>92 In this paper, we have concentrated on the stabilising eﬀect that moderate weight-dependence can have on learning processes that develop bimodal weight distributions. [sent-213, score-0.391]
</p><p>93 We have also shown experimentally that a small feed-forward network of silicon neurons with STDP synapses can detect a hierarchical synchrony structure embedded in noisy spike trains. [sent-215, score-1.138]
</p><p>94 We are currently investigating the synchrony ampliﬁcation properties of silicon neurons with bimodal STDP. [sent-216, score-0.639]
</p><p>95 We are also working on a new chip that uses lateralinhibitory connections between neurons to classify data with complex synchrony patterns. [sent-217, score-0.413]
</p><p>96 Synaptic modiﬁcations in cultured hippocampal neurons; dependence on spike timing, synaptic strength and postsynaptic cell type. [sent-220, score-0.5]
</p><p>97 Regulation of synaptic eﬃcacy by coincidence of postsynaptic APs and EPSPs. [sent-239, score-0.244]
</p><p>98 5 0 0  10  20  30 time ( s )  40  50  60  (c)  Figure 7: Detection of non-coincident spike-timing synchrony with weight-dependent STDP. [sent-294, score-0.23]
</p><p>99 (a) Normalised spike interval histogram of the 2 correlated inputs (synapses 0 and 1). [sent-295, score-0.375]
</p><p>100 (b) Normalised spike interval histogram between 2 uncorrelated inputs (synapses 2-5) (c) Synapses 0 and 1 win the learning competition. [sent-296, score-0.456]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('synapses', 0.357), ('stdp', 0.339), ('vw', 0.331), ('synchrony', 0.23), ('weight', 0.224), ('spike', 0.223), ('presynaptic', 0.165), ('depression', 0.165), ('potentiation', 0.159), ('silicon', 0.154), ('neuron', 0.153), ('postsynaptic', 0.148), ('bimodal', 0.129), ('neurons', 0.126), ('circuits', 0.126), ('synapse', 0.115), ('window', 0.103), ('correlations', 0.1), ('plasticity', 0.09), ('circuit', 0.086), ('gnd', 0.082), ('ibpot', 0.082), ('postlong', 0.082), ('prelong', 0.082), ('vlsi', 0.074), ('post', 0.071), ('pre', 0.067), ('inputs', 0.067), ('synaptic', 0.066), ('vdd', 0.065), ('strength', 0.063), ('bo', 0.061), ('vbdep', 0.061), ('vbpot', 0.061), ('winit', 0.061), ('chip', 0.057), ('stimulated', 0.057), ('bistable', 0.057), ('uncorrelated', 0.056), ('timing', 0.056), ('layer', 0.054), ('evolution', 0.053), ('spikes', 0.052), ('edinburgh', 0.051), ('normalised', 0.048), ('refractory', 0.048), ('hierarchical', 0.048), ('pulse', 0.048), ('asymmetric', 0.048), ('correlation', 0.047), ('trains', 0.046), ('correlated', 0.045), ('pulses', 0.045), ('stimulate', 0.045), ('transistor', 0.045), ('activity', 0.045), ('voltage', 0.043), ('decaying', 0.043), ('competition', 0.043), ('cdep', 0.041), ('cpot', 0.041), ('hinder', 0.041), ('ibdep', 0.041), ('idep', 0.041), ('rossum', 0.041), ('scotland', 0.041), ('signalled', 0.041), ('spikeout', 0.041), ('erent', 0.041), ('mechanism', 0.04), ('histogram', 0.04), ('spiking', 0.04), ('hebbian', 0.039), ('operated', 0.039), ('secondary', 0.039), ('learning', 0.038), ('di', 0.038), ('change', 0.038), ('weights', 0.036), ('murray', 0.035), ('characterisation', 0.035), ('precise', 0.035), ('level', 0.034), ('receive', 0.033), ('termed', 0.032), ('klaus', 0.032), ('win', 0.032), ('ring', 0.031), ('decay', 0.031), ('dependent', 0.03), ('coincidence', 0.03), ('vr', 0.03), ('bridge', 0.03), ('peak', 0.03), ('creates', 0.03), ('analog', 0.03), ('detection', 0.03), ('shared', 0.029), ('implementation', 0.029), ('song', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="183-tfidf-1" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>2 0.34645614 <a title="183-tfidf-2" href="./nips-2003-A_Summating%2C_Exponentially-Decaying_CMOS_Synapse_for_Spiking_Neural_Systems.html">18 nips-2003-A Summating, Exponentially-Decaying CMOS Synapse for Spiking Neural Systems</a></p>
<p>Author: Rock Z. Shi, Timothy K. Horiuchi</p><p>Abstract: Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modiﬁcation. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5µm CMOS process. 1</p><p>3 0.25979951 <a title="183-tfidf-3" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>Author: Francesco Tenore, Ralph Etienne-Cummings, M. A. Lewis</p><p>Abstract: We have constructed a second generation CPG chip capable of generating the necessary timing to control the leg of a walking machine. We demonstrate improvements over a previous chip by moving toward a significantly more versatile device. This includes a larger number of silicon neurons, more sophisticated neurons including voltage dependent charging and relative and absolute refractory periods, and enhanced programmability of neural networks. This chip builds on the basic results achieved on a previous chip and expands its versatility to get closer to a self-contained locomotion controller for walking robots. 1</p><p>4 0.25046101 <a title="183-tfidf-4" href="./nips-2003-Analytical_Solution_of_Spike-timing_Dependent_Plasticity_Based_on_Synaptic_Biophysics.html">27 nips-2003-Analytical Solution of Spike-timing Dependent Plasticity Based on Synaptic Biophysics</a></p>
<p>Author: Bernd Porr, Ausra Saudargiene, Florentin Wörgötter</p><p>Abstract: Spike timing plasticity (STDP) is a special form of synaptic plasticity where the relative timing of post- and presynaptic activity determines the change of the synaptic weight. On the postsynaptic side, active backpropagating spikes in dendrites seem to play a crucial role in the induction of spike timing dependent plasticity. We argue that postsynaptically the temporal change of the membrane potential determines the weight change. Coming from the presynaptic side induction of STDP is closely related to the activation of NMDA channels. Therefore, we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the NMDA channel. Thus, for this calculation we utilise biophysical variables of the physiological cell. The ﬁnal result shows a weight change curve which conforms with measurements from biology. The positive part of the weight change curve is determined by the NMDA activation. The negative part of the weight change curve is determined by the membrane potential change. Therefore, the weight change curve should change its shape depending on the distance from the soma of the postsynaptic cell. We ﬁnd temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite. 1</p><p>5 0.19034266 <a title="183-tfidf-5" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 1</p><p>6 0.16587229 <a title="183-tfidf-6" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>7 0.1538434 <a title="183-tfidf-7" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>8 0.15374294 <a title="183-tfidf-8" href="./nips-2003-Plasticity_Kernels_and_Temporal_Statistics.html">157 nips-2003-Plasticity Kernels and Temporal Statistics</a></p>
<p>9 0.14773607 <a title="183-tfidf-9" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>10 0.1343915 <a title="183-tfidf-10" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>11 0.11678305 <a title="183-tfidf-11" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>12 0.11130781 <a title="183-tfidf-12" href="./nips-2003-A_Low-Power_Analog_VLSI_Visual_Collision_Detector.html">10 nips-2003-A Low-Power Analog VLSI Visual Collision Detector</a></p>
<p>13 0.09831176 <a title="183-tfidf-13" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>14 0.095753558 <a title="183-tfidf-14" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>15 0.094340131 <a title="183-tfidf-15" href="./nips-2003-Circuit_Optimization_Predicts_Dynamic_Networks_for_Chemosensory_Orientation_in_Nematode_C._elegans.html">45 nips-2003-Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans</a></p>
<p>16 0.086606838 <a title="183-tfidf-16" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>17 0.067177609 <a title="183-tfidf-17" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>18 0.056854464 <a title="183-tfidf-18" href="./nips-2003-A_Mixed-Signal_VLSI_for_Real-Time_Generation_of_Edge-Based_Image_Vectors.html">11 nips-2003-A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors</a></p>
<p>19 0.055481929 <a title="183-tfidf-19" href="./nips-2003-Reasoning_about_Time_and_Knowledge_in_Neural_Symbolic_Learning_Systems.html">165 nips-2003-Reasoning about Time and Knowledge in Neural Symbolic Learning Systems</a></p>
<p>20 0.051164638 <a title="183-tfidf-20" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.153), (1, 0.107), (2, 0.455), (3, 0.113), (4, 0.185), (5, -0.022), (6, -0.244), (7, 0.026), (8, 0.065), (9, -0.017), (10, -0.032), (11, -0.049), (12, -0.071), (13, -0.077), (14, -0.108), (15, -0.11), (16, -0.017), (17, 0.028), (18, 0.055), (19, -0.073), (20, 0.062), (21, 0.19), (22, -0.045), (23, 0.06), (24, -0.009), (25, -0.022), (26, 0.064), (27, 0.014), (28, -0.036), (29, 0.048), (30, 0.007), (31, -0.098), (32, 0.027), (33, -0.125), (34, -0.076), (35, -0.03), (36, 0.026), (37, -0.0), (38, -0.021), (39, 0.062), (40, 0.045), (41, -0.028), (42, 0.023), (43, -0.119), (44, 0.001), (45, -0.028), (46, -0.048), (47, 0.063), (48, -0.007), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97957277 <a title="183-lsi-1" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>2 0.86040908 <a title="183-lsi-2" href="./nips-2003-A_Summating%2C_Exponentially-Decaying_CMOS_Synapse_for_Spiking_Neural_Systems.html">18 nips-2003-A Summating, Exponentially-Decaying CMOS Synapse for Spiking Neural Systems</a></p>
<p>Author: Rock Z. Shi, Timothy K. Horiuchi</p><p>Abstract: Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modiﬁcation. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5µm CMOS process. 1</p><p>3 0.81653923 <a title="183-lsi-3" href="./nips-2003-Analytical_Solution_of_Spike-timing_Dependent_Plasticity_Based_on_Synaptic_Biophysics.html">27 nips-2003-Analytical Solution of Spike-timing Dependent Plasticity Based on Synaptic Biophysics</a></p>
<p>Author: Bernd Porr, Ausra Saudargiene, Florentin Wörgötter</p><p>Abstract: Spike timing plasticity (STDP) is a special form of synaptic plasticity where the relative timing of post- and presynaptic activity determines the change of the synaptic weight. On the postsynaptic side, active backpropagating spikes in dendrites seem to play a crucial role in the induction of spike timing dependent plasticity. We argue that postsynaptically the temporal change of the membrane potential determines the weight change. Coming from the presynaptic side induction of STDP is closely related to the activation of NMDA channels. Therefore, we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the NMDA channel. Thus, for this calculation we utilise biophysical variables of the physiological cell. The ﬁnal result shows a weight change curve which conforms with measurements from biology. The positive part of the weight change curve is determined by the NMDA activation. The negative part of the weight change curve is determined by the membrane potential change. Therefore, the weight change curve should change its shape depending on the distance from the soma of the postsynaptic cell. We ﬁnd temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite. 1</p><p>4 0.80021316 <a title="183-lsi-4" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>Author: Francesco Tenore, Ralph Etienne-Cummings, M. A. Lewis</p><p>Abstract: We have constructed a second generation CPG chip capable of generating the necessary timing to control the leg of a walking machine. We demonstrate improvements over a previous chip by moving toward a significantly more versatile device. This includes a larger number of silicon neurons, more sophisticated neurons including voltage dependent charging and relative and absolute refractory periods, and enhanced programmability of neural networks. This chip builds on the basic results achieved on a previous chip and expands its versatility to get closer to a self-contained locomotion controller for walking robots. 1</p><p>5 0.55624706 <a title="183-lsi-5" href="./nips-2003-Plasticity_Kernels_and_Temporal_Statistics.html">157 nips-2003-Plasticity Kernels and Temporal Statistics</a></p>
<p>Author: Peter Dayan, Michael Häusser, Michael London</p><p>Abstract: Computational mysteries surround the kernels relating the magnitude and sign of changes in efficacy as a function of the time difference between pre- and post-synaptic activity at a synapse. One important idea34 is that kernels result from filtering, ie an attempt by synapses to eliminate noise corrupting learning. This idea has hitherto been applied to trace learning rules; we apply it to experimentally-defined kernels, using it to reverse-engineer assumed signal statistics. We also extend it to consider the additional goal for filtering of weighting learning according to statistical surprise, as in the Z-score transform. This provides a fresh view of observed kernels and can lead to different, and more natural, signal statistics.</p><p>6 0.55270576 <a title="183-lsi-6" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>7 0.50368059 <a title="183-lsi-7" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>8 0.48378161 <a title="183-lsi-8" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>9 0.46854389 <a title="183-lsi-9" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>10 0.46447745 <a title="183-lsi-10" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>11 0.36717883 <a title="183-lsi-11" href="./nips-2003-Circuit_Optimization_Predicts_Dynamic_Networks_for_Chemosensory_Orientation_in_Nematode_C._elegans.html">45 nips-2003-Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans</a></p>
<p>12 0.35087076 <a title="183-lsi-12" href="./nips-2003-Reasoning_about_Time_and_Knowledge_in_Neural_Symbolic_Learning_Systems.html">165 nips-2003-Reasoning about Time and Knowledge in Neural Symbolic Learning Systems</a></p>
<p>13 0.33528715 <a title="183-lsi-13" href="./nips-2003-A_Low-Power_Analog_VLSI_Visual_Collision_Detector.html">10 nips-2003-A Low-Power Analog VLSI Visual Collision Detector</a></p>
<p>14 0.30516347 <a title="183-lsi-14" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>15 0.2976352 <a title="183-lsi-15" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>16 0.28360397 <a title="183-lsi-16" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>17 0.27675009 <a title="183-lsi-17" href="./nips-2003-A_Mixed-Signal_VLSI_for_Real-Time_Generation_of_Edge-Based_Image_Vectors.html">11 nips-2003-A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors</a></p>
<p>18 0.2688241 <a title="183-lsi-18" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>19 0.26563779 <a title="183-lsi-19" href="./nips-2003-The_Diffusion-Limited_Biochemical_Signal-Relay_Channel.html">184 nips-2003-The Diffusion-Limited Biochemical Signal-Relay Channel</a></p>
<p>20 0.26021597 <a title="183-lsi-20" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (9, 0.01), (11, 0.015), (29, 0.012), (30, 0.022), (35, 0.031), (53, 0.111), (59, 0.061), (63, 0.404), (71, 0.063), (76, 0.035), (85, 0.041), (91, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85827792 <a title="183-lda-1" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>2 0.83537334 <a title="183-lda-2" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>Author: Bertram E. Shi, Eric K. Tsang</p><p>Abstract: The relative depth of objects causes small shifts in the left and right retinal positions of these objects, called binocular disparity. Here, we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model, which has been proposed to model the response of disparity selective cells in the visual cortex. Our system consists of two silicon chips containing spiking neurons with monocular Gabor-type spatial receptive fields (RF) and circuits that combine the spike outputs to compute a disparity selective complex cell response. The disparity selectivity of the cell can be adjusted by both position and phase shifts between the monocular RF profiles, which are both used in biology. Our neuromorphic system performs better with phase encoding, because the relative responses of neurons tuned to different disparities by phase shifts are better matched than the responses of neurons tuned by position shifts.</p><p>3 0.68193597 <a title="183-lda-3" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>Author: Mark Girolami, Ata Kabán</p><p>Abstract: To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the deﬁnition of individual speciﬁc and global models. This paper proposes a linear-time distributed model for ﬁnite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained’ by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-speciﬁc proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efﬁcient representation scheme, reﬂected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.</p><p>4 0.5135476 <a title="183-lda-4" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>Author: Paul Merolla, Kwabena A. Boahen</p><p>Abstract: We describe a neuromorphic chip that utilizes transistor heterogeneity, introduced by the fabrication process, to generate orientation maps similar to those imaged in vivo. Our model consists of a recurrent network of excitatory and inhibitory cells in parallel with a push-pull stage. Similar to a previous model the recurrent network displays hotspots of activity that give rise to visual feature maps. Unlike previous work, however, the map for orientation does not depend on the sign of contrast. Instead, signindependent cells driven by both ON and OFF channels anchor the map, while push-pull interactions give rise to sign-preserving cells. These two groups of orientation-selective cells are similar to complex and simple cells observed in V1. 1 Orientation Maps Neurons in visual areas 1 and 2 (V1 and V2) are selectively tuned for a number of visual features, the most pronounced feature being orientation. Orientation preference of individual cells varies across the two-dimensional surface of the cortex in a stereotyped manner, as revealed by electrophysiology [1] and optical imaging studies [2]. The origin of these preferred orientation (PO) maps is debated, but experiments demonstrate that they exist in the absence of visual experience [3]. To the dismay of advocates of Hebbian learning, these results suggest that the initial appearance of PO maps rely on neural mechanisms oblivious to input correlations. Here, we propose a model that accounts for observed PO maps based on innate noise in neuron thresholds and synaptic currents. The network is implemented in silicon where heterogeneity is as ubiquitous as it is in biology. 2 Patterned Activity Model Ernst et al. have previously described a 2D rate model that can account for the origin of visual maps [4]. Individual units in their network receive isotropic feedforward input from the geniculate and recurrent connections from neighboring units in a Mexican hat profile, described by short-range excitation and long-range inhibition. If the recurrent connections are sufficiently strong, hotspots of activity (or ‘bumps’) form periodically across space. In a homogeneous network, these bumps of activity are equally stable at any position in the network and are free to wander. Introducing random jitter to the Mexican hat connectivity profiles breaks the symmetry and reduces the number of stable states for the bumps. Subsequently, the bumps are pinned down at the locations that maximize their net local recurrent feedback. In this regime, moving gratings are able to shift the bumps away from their stability points such that the responses of the network resemble PO maps. Therefore, the recurrent network, given an ample amount of noise, can innately generate its own orientation specificity without the need for specific hardwired connections or visually driven learning rules. 2.1 Criticisms of the Bump model We might posit that the brain uses a similar opportunistic model to derive and organize its feature maps – but the parallels between the primary visual cortex and the Ernst et al. bump model are unconvincing. For instance, the units in their model represent the collective activity of a column, reducing the network dynamics to a firing-rate approximation. But this simplification ignores the rich temporal dynamics of spiking networks, which are known to affect bump stability. More fundamentally, there is no role for functionally distinct neuron types. The primary criticism of the Ernst et al.’s bump model is that its input only consists of a luminance channel, and it is not obvious how to replace this channel with ON and OFF rectified channels to account for simple and complex cells. One possibility would be to segregate ON-driven and OFF-driven cells (referred to as simple cells in this paper) into two distinct recurrent networks. Because each network would have its own innate noise profile, bumps would form independently. Consequently, there is no guarantee that ON-driven maps would line up with OFF-driven maps, which would result in conflicting orientation signals when these simple cells converge onto sign-independent (complex) cells. 2.2 Simple Cells Solve a Complex Problem To ensure that both ON-driven and OFF-driven simple cells have the same orientation maps, both ON and OFF bumps must be computed in the same recurrent network so that they are subjected to the same noise profile. We achieve this by building our recurrent network out of cells that are sign-independent; that is both ON and OFF channels drive the network. These cells exhibit complex cell-like behavior (and are referred to as complex cells in this paper) because they are modulated at double the spatial frequency of a sinusoidal grating input. The simple cells subsequently derive their responses from two separate signals: an orientation selective feedback signal from the complex cells indicating the presence of either an ON or an OFF bump, and an ON–OFF selection signal that chooses the appropriate response flavor. Figure 1 left illustrates the formation of bumps (highlighted cells) by a recurrent network with a Mexican hat connectivity profile. Extending the Ernst et al. model, these complex bumps seed simple bumps when driven by a grating. Simple bumps that match the sign of the input survive, whereas out-of-phase bumps are extinguished (faded cells) by push-pull inhibition. Figure 1 right shows the local connections within a microcircuit. An EXC (excitatory) cell receives excitatory input from both ON and OFF channels, and projects to other EXC (not shown) and INH (inhibitory) cells. The INH cell projects back in a reciprocal configuration to EXC cells. The divergence is indicated in left. ON-driven and OFF-driven simple cells receive input in a push-pull configuration (i.e., ON cells are excited by ON inputs and inhibited by OFF inputs, and vise-versa), while additionally receiving input from the EXC–INH recurrent network. In this model, we implement our push-pull circuit using monosynaptic inhibitory connections, despite the fact that geniculate input is strictly excitatory. This simplification, while anatomically incorrect, yields a more efficient implementation that is functionally equivalent. ON Input Luminance OFF Input left right EXC EXC Divergence INH INH Simple Cells Complex Cells ON & OFF Input ON OFF OFF Space Figure 1: left, Complex and simple cell responses to a sinusoidal grating input. Luminance is transformed into ON (green) and OFF (red) pathways by retinal processing. Complex cells form a recurrent network through excitatory and inhibitory projections (yellow and blue lines, respectively), and clusters of activity occur at twice the spatial frequency of the grating. ON input activates ON-driven simple cells (bright green) and suppresses OFF-driven simple cells (faded red), and vise-versa. right, The bump model’s local microcircuit: circles represent neurons, curved lines represent axon arbors that end in excitatory synapses (v shape) or inhibitory synapses (open circles). For simplicity, inhibitory interneurons were omitted in our push-pull circuit. 2.3 Mathematical Description • The neurons in our network follow the equation CV = −∑ ∂(t − tn) + I syn − I KCa − I leak , • n where C is membrane capacitance, V is the temporal derivative of the membrane voltage, δ(·) is the Dirac delta function, which resets the membrane at the times tn when it crosses threshold, Isyn is synaptic current from the network, and Ileak is a constant leak current. Neurons receive synaptic current of the form: ON I syn = w+ I ON − w− I OFF + wEE I EXC − wEI I INH , EXC I syn = w+ ( I ON + I OFF ) + wEE I EXC − wEI I INH + I back , OFF INH I syn = w+ I OFF − w− I ON + wEE I EXC − wEI I INH , I syn = wIE I EXC where w+ is the excitatory synaptic strength for ON and OFF input synapses, w- is the strength of the push-pull inhibition, wEE is the synaptic strength for EXC cell projections to other EXC cells, wEI is the strength of INH cell projections to EXC cells, wIE is the strength of EXC cell projections to INH cells, Iback is a constant input current, and I{ON,OFF,EXC,INH} account for all impinging synapses from each of the four cell types. These terms are calculated for cell i using an arbor function that consists of a spatial weighting J(r) and a post-synaptic current waveform α(t): k ∑ J (i − k ) ⋅ α (t − t n ) , where k spans all cells of a given type and n indexes their spike k ,n times. The spatial weighting function is described by J (i − k ) = exp( − i − k σ ) , with σ as the space constant. The current waveform, which is non-zero for t>0, convolves a 1 t function with a decaying exponential: α (t ) = (t τ c + α 0 ) −1 ∗ exp(− t τ e ) , where τc is the decay-rate, and τe is the time constant of the exponential. Finally, we model spike-rate adaptation with a calcium-dependent potassium-channel (KCa), which integrates Ca triggered by spikes at times tn with a gain K and a time constant τk, as described by I KCa = ∑ K exp(tn − t τ k ) . n 3 Silicon Implementation We implemented our model in silicon using the TSMC (Taiwan Semiconductor Manufacturing Company) 0.25µm 5-metal layer CMOS process. The final chip consists of a 2-D core of 48x48 pixels, surrounded by asynchronous digital circuitry that transmits and receives spikes in real-time. Neurons that reach threshold within the array are encoded as address-events and sent off-chip, and concurrently, incoming address-events are sent to their appropriate synapse locations. This interface is compatible with other spike-based chips that use address-events [5]. The fabricated bump chip has close to 460,000 transistors packed in 10 mm2 of silicon area for a total of 9,216 neurons. 3.1 Circuit Design Our neural circuit was morphed into hardware using four building blocks. Figure 2 shows the transistor implementation for synapses, axonal arbors (diffuser), KCa analogs, and neurons. The circuits are designed to operate in the subthreshold region (except for the spiking mechanism of the neuron). Noise is not purposely designed into the circuits. Instead, random variations from the fabrication process introduce significant deviations in I-V curves of theoretically identical MOS transistors. The function of the synapse circuit is to convert a brief voltage pulse (neuron spike) into a postsynaptic current with biologically realistic temporal dynamics. Our synapse achieves this by cascading a current-mirror integrator with a log-domain low-pass filter. The current-mirror integrator has a current impulse response that decays as 1 t (with a decay rate set by the voltage τc and an amplitude set by A). This time-extended current pulse is fed into a log-domain low-pass filter (equivalent to a current-domain RC circuit) that imposes a rise-time on the post-synaptic current set by τe. ON and OFF input synapses receive presynaptic spikes from the off-chip link, whereas EXC and INH synapses receive presynaptic spikes from local on-chip neurons. Synapse Je Diffuser Ir A Ig Jc KCa Analog Neuron Jk Vmem Vspk K Figure 2: Transistor implementations are shown for a synapse, diffuser, KCa analog, and neuron (simplified), with circuit insignias in the top-left of each box. The circuits they interact with are indicated (e.g. the neuron receives synaptic current from the diffuser as well as adaptation current from the KCa analog; the neuron in turn drives the KCa analog). The far right shows layout for one pixel of the bump chip (vertical dimension is 83µm, horizontal is 30 µm). The diffuser circuit models axonal arbors that project to a local region of space with an exponential weighting. Analogous to resistive divider networks, diffusers [6] efficiently distribute synaptic currents to multiple targets. We use four diffusers to implement axonal projections for: the ON pathway, which excites ON and EXC cells and inhibits OFF cells; the OFF pathway, which excites OFF and EXC cells and inhibits ON cells; the EXC cells, which excite all cell types; and the INH cells, which inhibits EXC, ON, and OFF cells. Each diffuser node connects to its six neighbors through transistors that have a pseudo-conductance set by σr, and to its target site through a pseudo-conductance set by σg; the space-constant of the exponential synaptic decay is set by σr and σg’s relative levels. The neuron circuit integrates diffuser currents on its membrane capacitance. Diffusers either directly inject current (excitatory), or siphon off current (inhibitory) through a current-mirror. Spikes are generated by an inverter with positive feedback (modified from [7]), and the membrane is subsequently reset by the spike signal. We model a calcium concentration in the cell with a KCa analog. K controls the amount of calcium that enters the cell per spike; the concentration decays exponentially with a time constant set by τk. Elevated charge levels activate a KCa-like current that throttles the spike-rate of the neuron. 3.2 Experimental Setup Our setup uses either a silicon retina [8] or a National Instruments DIO (digital input–output) card as input to the bump chip. This allows us to test our V1 model with real-time visual stimuli, similar to the experimental paradigm of electrophysiologists. More specifically, the setup uses an address-event link [5] to establish virtual point-to-point connectivity between ON or OFF ganglion cells from the retina chip (or DIO card) with ON or OFF synapses on the bump chip. Both the input activity and the output activity of the bump chip is displayed in real-time using receiver chips, which integrate incoming spikes and displays their rates as pixel intensities on a monitor. A logic analyzer is used to capture spike output from the bump chip so it can be further analyzed. We investigated responses of the bump chip to gratings moving in sixteen different directions, both qualitatively and quantitatively. For the qualitative aspect, we created a PO map by taking each cell’s average activity for each stimulus direction and computing the vector sum. To obtain a quantitative measure, we looked at the normalized vector magnitude (NVM), which reveals the sharpness of a cell’s tuning. The NVM is calculated by dividing the vector sum by the magnitude sum for each cell. The NVM is 0 if a cell responds equally to all orientations, and 1 if a cell’s orientation selectivity is perfect such that it only responds at a single orientation. 4 Results We presented sixteen moving gratings to the network, with directions ranging from 0 to 360 degrees. The spatial frequency of the grating is tuned to match the size of the average bump, and the temporal frequency is 1 Hz. Figure 3a shows a resulting PO map for directions from 180 to 360 degrees, looking at the inhibitory cell population (the data looks similar for other cell types). Black contours represent stable bump regions, or equivalently, the regions that exceed a prescribed threshold (90 spikes) for all directions. The PO map from the bump chip reveals structure that resembles data from real cortex. Nearby cells tend to prefer similar orientations except at fractures. There are even regions that are similar to pinwheels (delimited by a white rectangle). A PO is a useful tool to describe a network’s selectivity, but it only paints part of the picture. So we have additionally computed a NVM map and a NVM histogram, shown in Figure 3b and 3c respectively. The NVM map shows that cells with sharp selectivity tend to cluster, particularly around the edge of the bumps. The histogram also reveals that the distribution of cell selectivity across the network varies considerably, skewed towards broadly tuned cells. We also looked at spike rasters from different cell-types to gain insight into their phase relationship with the stimulus. In particular, we present recordings for the site indicated by the arrow (see Figure 3a) for gratings moving in eight directions ranging from 0 to 360 degrees in 45-degree increments (this location was chosen because it is in the vicinity of a pinwheel, is reasonably selective, and shows considerable modulation in its firing rate). Figure 4 shows the luminance of the stimulus (bottom sinusoids), ON- (cyan) and OFF-input (magenta) spike trains, and the resulting spike trains from EXC (yellow), INH (blue), ON- (green), and OFFdriven (red) cell types for each of the eight directions. The center polar plot summarizes the orientation selectivity for each cell-type by showing the normalized number of spikes for each stimulus. Data is shown for one period. Even though all cells-types are selective for the same orientation (regardless of grating direction), complex cell responses tend to be phase-insensitive while the simple cell responses are modulated at the fundamental frequency. It is worth noting that the simple cells have sharper orientation selectivity compared to the complex cells. This trend is characteristic of our data. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 300 250 200 150 100 50 20 40 60 80 100 120 140 160 180 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure 3: (a) PO map for the inhibitory cell population stimulated with eight different directions from 180 to 360 degrees (black represents no activity, contours delineate regions that exceed 90 spikes for all stimuli). Normalized vector magnitude (NVM) data is presented as (b) a map and (c) a histogram. Figure 4: Spike rasters and polar plot for 8 directions ranging from 0 to 360 degrees. Each set of spike rasters represent from bottom to top, ON- (cyan) and OFF-input (magenta), INH (yellow), EXC (blue), and ON- (green) and OFF-driven (red). The stimulus period is 1 sec. 5 Discussion We have implemented a large-scale network of spiking neurons in a silicon chip that is based on layer 4 of the visual cortex. The initial testing of the network reveals a PO map, inherited from innate chip heterogeneities, resembling cortical maps. Our microcircuit proposes a novel function for complex-like cells; that is they create a sign-independent orientation selective signal, which through a push-pull circuit creates sharply tuned simple cells with the same orientation preference. Recently, Ringach et al. surveyed orientation selectivity in the macaque [9]. They observed that, in a population of V1 neurons (N=308) the distribution of orientation selectivity is quite broad, having a median NVM of 0.39. We have measured median NVM’s ranging from 0.25 to 0.32. Additionally, Ringach et al. found a negative correlation between spontaneous firing rate and NVM. This is consistent with our model because cells closer to the center of the bump have higher firing rates and broader tuning. While the results from the bump chip are promising, our maps are less consistent and noisier than the maps Ernst et al. have reported. We believe this is because our network is tuned to operate in a fluid state where bumps come on, travel a short distance and disappear (motivated by cortical imaging studies). But excessive fluidity can cause non-dominant bumps to briefly appear and adversely shift the PO maps. We are currently investigating the role of lateral connections between bumps as a means to suppress these spontaneous shifts. The neural mechanisms that underlie the orientation selectivity of V1 neurons are still highly debated. This may be because neuron responses are not only shaped by feedforward inputs, but are also influenced at the network level. If modeling is going to be a useful guide for electrophysiologists, we must model at the network level while retaining cell level detail. Our results demonstrate that a spike-based neuromorphic system is well suited to model layer 4 of the visual cortex. The same approach may be used to build large-scale models of other cortical regions. References 1. Hubel, D. and T. Wiesel, Receptive firelds, binocular interaction and functional architecture in the cat's visual cortex. J. Physiol, 1962. 160: p. 106-154. 2. Blasdel, G.G., Orientation selectivity, preference, and continuity in monkey striate cortex. J Neurosci, 1992. 12(8): p. 3139-61. 3. Crair, M.C., D.C. Gillespie, and M.P. Stryker, The role of visual experience in the development of columns in cat visual cortex. Science, 1998. 279(5350): p. 566-70. 4. Ernst, U.A., et al., Intracortical origin of visual maps. Nat Neurosci, 2001. 4(4): p. 431-6. 5. Boahen, K., Point-to-Point Connectivity. IEEE Transactions on Circuits & Systems II, 2000. vol 47 no 5: p. 416-434. 6. Boahen, K. and Andreou. A contrast sensitive silicon retina with reciprocal synapses. in NIPS91. 1992: IEEE. 7. Culurciello, E., R. Etienne-Cummings, and K. Boahen, A Biomorphic Digital Image Sensor. IEEE Journal of Solid State Circuits, 2003. vol 38 no 2: p. 281-294. 8. Zaghloul, K., A silicon implementation of a novel model for retinal processing, in Neuroscience. 2002, UPENN: Philadelphia. 9. Ringach, D.L., R.M. Shapley, and M.J. Hawken, Orientation selectivity in macaque V1: diversity and laminar dependence. J Neurosci, 2002. 22(13): p. 5639-51.</p><p>5 0.50922787 <a title="183-lda-5" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>6 0.4954102 <a title="183-lda-6" href="./nips-2003-Analytical_Solution_of_Spike-timing_Dependent_Plasticity_Based_on_Synaptic_Biophysics.html">27 nips-2003-Analytical Solution of Spike-timing Dependent Plasticity Based on Synaptic Biophysics</a></p>
<p>7 0.45458674 <a title="183-lda-7" href="./nips-2003-A_Summating%2C_Exponentially-Decaying_CMOS_Synapse_for_Spiking_Neural_Systems.html">18 nips-2003-A Summating, Exponentially-Decaying CMOS Synapse for Spiking Neural Systems</a></p>
<p>8 0.44542849 <a title="183-lda-8" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>9 0.40941352 <a title="183-lda-9" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>10 0.39736325 <a title="183-lda-10" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>11 0.39667684 <a title="183-lda-11" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>12 0.39519155 <a title="183-lda-12" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>13 0.3841697 <a title="183-lda-13" href="./nips-2003-A_Low-Power_Analog_VLSI_Visual_Collision_Detector.html">10 nips-2003-A Low-Power Analog VLSI Visual Collision Detector</a></p>
<p>14 0.38292414 <a title="183-lda-14" href="./nips-2003-Bounded_Invariance_and_the_Formation_of_Place_Fields.html">43 nips-2003-Bounded Invariance and the Formation of Place Fields</a></p>
<p>15 0.37360182 <a title="183-lda-15" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>16 0.3697933 <a title="183-lda-16" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>17 0.36912394 <a title="183-lda-17" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>18 0.36902788 <a title="183-lda-18" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>19 0.36573127 <a title="183-lda-19" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>20 0.36470306 <a title="183-lda-20" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
