<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-58" href="#">nips2003-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</h1>
<br/><p>Source: <a title="nips-2003-58-pdf" href="http://papers.nips.cc/paper/2435-efficient-multiscale-sampling-from-products-of-gaussian-mixtures.pdf">pdf</a></p><p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>Reference: <a title="nips-2003-58-reference" href="../nips2003_reference/nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. [sent-10, score-0.564]
</p><p>2 This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. [sent-11, score-0.59]
</p><p>3 The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. [sent-12, score-0.211]
</p><p>4 The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. [sent-13, score-0.356]
</p><p>5 We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. [sent-14, score-0.394]
</p><p>6 1  Introduction  Gaussian mixture densities are widely used to model complex, multimodal relationships. [sent-15, score-0.235]
</p><p>7 Although they are most commonly associated with parameter estimation procedures like the EM algorithm, kernel or Parzen window nonparametric density estimates [1] also take this form for Gaussian kernel functions. [sent-16, score-0.222]
</p><p>8 Products of Gaussian mixtures naturally arise whenever multiple sources of statistical information, each of which is individually modeled by a mixture density, are combined. [sent-17, score-0.35]
</p><p>9 In a recently proposed nonparametric belief propagation (NBP) [2, 3] inference algorithm for graphical models, Gaussian mixture products are the mechanism by which nodes fuse information from different parts of the graph. [sent-19, score-0.376]
</p><p>10 Product densities also arise in the product of experts (PoE) [4] framework, in which complex densities are modeled as the product of many “local” constraint densities. [sent-20, score-0.434]
</p><p>11 The primary difﬁculty associated with products of Gaussian mixtures is computational. [sent-21, score-0.294]
</p><p>12 The product of d mixtures of N Gaussians is itself a Gaussian mixture with N d components. [sent-22, score-0.488]
</p><p>13 In many practical applications, it is infeasible to explicitly construct these components, and therefore intractable to build a smaller approximating mixture using the EM algorithm. [sent-23, score-0.189]
</p><p>14 Mixture products are thus typically approximated by drawing samples from the product density. [sent-24, score-0.318]
</p><p>15 These samples can be used to either form a Monte Carlo estimate of a desired expectation [4], or construct a kernel density estimate approximating the true product [2]. [sent-25, score-0.342]
</p><p>16 Although exact sampling requires exponential cost, Gibbs sampling algorithms may often be used to produce good approximate samples [2, 4]. [sent-26, score-0.552]
</p><p>17 When accurate approximations are required, existing methods for sampling from products of Gaussian mixtures often require a large computational cost. [sent-27, score-0.55]
</p><p>18 In particular, sampling is the primary computational burden for both NBP and PoE. [sent-28, score-0.196]
</p><p>19 This paper develops a pair of new sampling algorithms which use multiscale, KD-Tree [5] representations to improve accuracy and reduce computation. [sent-29, score-0.25]
</p><p>20 The ﬁrst is a multiscale variant of existing Gibbs samplers [2, 4] with improved empirical convergence rate. [sent-30, score-0.394]
</p><p>21 The second makes use of approximate kernel density evaluation methods [6] to construct a fast -exact sampler which, in contrast with existing methods, is guaranteed to sample points to within a tunable parameter of their true probability. [sent-31, score-0.59]
</p><p>22 For simplicity, we assume that all mixtures are of equal size N , and that the variances Λi are uniform within each mixture, although the algorithms which follow may be readily extended to problems where this is not the case. [sent-37, score-0.182]
</p><p>23 Our goal is d to efﬁciently sample from the N d component mixture density p(x) ∝ i=1 pi (x). [sent-38, score-0.338]
</p><p>24 1  Exact Sampling  Sampling from the product density can be decomposed into two steps: randomly select one of the product density’s N d components, and then draw a sample from the corresponding Gaussian. [sent-40, score-0.466]
</p><p>25 Let each product density component be labeled as L = [l1 , . [sent-41, score-0.245]
</p><p>26 , ld ], where li labels one of the N components of pi (x). [sent-44, score-0.491]
</p><p>27 To form the product density, these weights are normalized by the weight partition function Z L wL . [sent-46, score-0.247]
</p><p>28 2  Importance Sampling  Importance sampling is a Monte Carlo method for approximately sampling from (or computing expectations of) an intractable distribution p(x), using a proposal distribution q(x) for which sampling is feasible [7]. [sent-50, score-0.588]
</p><p>29 To draw N samples from p(x), an importance sampler draws M ≥ N samples xi ∼ q(x), and assigns the ith sample weight wi ∝ p(xi )/q(xi ). [sent-51, score-0.556]
</p><p>30 The weights are then normalized by Z = i wi , and N samples are drawn (with replacement) from the discrete distribution p(xi ) = wi /Z. [sent-52, score-0.162]
</p><p>31 ¯ 1 Throughout this paper, we use lowercase letters (li ) to label input density components, and capital letters (L = [l1 , . [sent-53, score-0.27]
</p><p>32 , ld ]) to label the corresponding product density components. [sent-56, score-0.47]
</p><p>33 Parallel Gibbs Sampler  Mix 2  X  X Figure 1: Two possible Gibbs samplers for a product of 2 mixtures of 5 Gaussians. [sent-63, score-0.48]
</p><p>34 Bottom left: Alternate between sampling a data point X conditioned on the current labels, and resampling all labels in parallel. [sent-66, score-0.347]
</p><p>35 Right: After κ iterations, both Gibbs samplers identify mixture labels corresponding to a single kernel (solid) in the product density (dashed). [sent-67, score-0.72]
</p><p>36 The ﬁrst, which we refer to as mixture importance sampling, draws each sample by randomly selecting one of the d input mixtures, and sampling from its N components (q(x) = p i (x)). [sent-69, score-0.481]
</p><p>37 The remaining d − 1 mixtures then provide the importance weight (wi = j=i pj (xi )). [sent-70, score-0.256]
</p><p>38 Alternatively, we can approximate each input mixture pi (x) by a single Gaussian density qi (x), and choose q(x) ∝ i qi (x). [sent-72, score-0.371]
</p><p>39 3  Gibbs Sampling  Sampling from Gaussian mixture products is difﬁcult because the joint distribution over product density labels, as deﬁned by equation (2), is complicated. [sent-75, score-0.525]
</p><p>40 However, conditioned on the labels of all but one mixture, we can compute the conditional distribution over the remaining label in O(N ) operations, and easily sample from it. [sent-76, score-0.287]
</p><p>41 Thus, we may use a Gibbs sampler [9] to draw asymptotically unbiased samples, as illustrated in Figure 1. [sent-77, score-0.298]
</p><p>42 At each iteration, the labels {lj }j=i for d − 1 of the input mixtures are ﬁxed, and the ith label is sampled from the corresponding conditional density. [sent-78, score-0.429]
</p><p>43 The newly chosen li is then ﬁxed, and another label is updated. [sent-79, score-0.331]
</p><p>44 After a ﬁxed number of iterations κ, a single sample is drawn from the product mixture component identiﬁed by the ﬁnal labels. [sent-80, score-0.359]
</p><p>45 To draw N samples, the Gibbs sampler requires O(dκN 2 ) operations; see [2] for further details. [sent-81, score-0.298]
</p><p>46 The previously described sequential Gibbs sampler deﬁnes an iteration over the labels of the input mixtures. [sent-82, score-0.424]
</p><p>47 Another possibility uses the fact that, given a data point x in the product ¯ density space, the d input mixture labels are conditionally independent [4]. [sent-83, score-0.555]
</p><p>48 Thus, one can deﬁne a parallel Gibbs sampler which alternates between sampling a data point conditioned on the current input mixture labels, and parallel sampling of the mixture labels given the current data point (see Figure 1). [sent-84, score-1.204]
</p><p>49 The complexity of this sampler is also O(dκN 2 ). [sent-85, score-0.246]
</p><p>50 We use the variable l to denote the label of a leaf node (the index of a single point), and l to denote a set of leaf labels summarized at a node of the KD-tree. [sent-88, score-0.299]
</p><p>51 {1,2,3,4,5,6,7,8}  x xx  xx x  {1,2,3,4}  x xx {1,2}  x xx  x x  x xx  xx x  x x  x x  x xx  xx x  x x  x xx  xx x  x x  {5,6,7,8}  xx x {3,4}  {5,6}  {7,8}  xx x  x x  (a) (b) Figure 2: Two KD-tree representations of the same one-dim. [sent-89, score-2.0]
</p><p>52 The second (Figure 2(b)) precomputes means and variances of point clusters, providing a multi-scale Gaussian mixture representation used in Section 4. [sent-97, score-0.168]
</p><p>53 1  Dual Tree Evaluation  Multiscale representations have been effectively applied to kernel density estimation problems. [sent-100, score-0.173]
</p><p>54 Given a mixture of N Gaussians with means {µi }, we would like to evaluate wi N (xj ; µi , Λ) (3) p(xj ) = i  at a given set of M points {xj }. [sent-101, score-0.215]
</p><p>55 x xx  xx x Dmax  oooo  x x Dmin  o ooo  Figure 3: Two KD-tree representations may be combined to efﬁciently bound the maximum (Dmax ) and minimum (Dmin ) pairwise distances between subsets of the summarized points (bold). [sent-103, score-0.485]
</p><p>56 1  Sampling using Multiscale Representations Gibbs Sampling on KD-Trees  Although the pair of Gibbs samplers discussed in Section 2. [sent-107, score-0.16]
</p><p>57 The most difﬁcult densities are those for which there are multiple widely separated modes, each of which is associated with disjoint subsets of the input mixture labels. [sent-109, score-0.295]
</p><p>58 In this case, conditioned on a set of labels corresponding to one mode, it is very unlikely that a label or data point corresponding to a different mode will be sampled, leading to slow convergence. [sent-110, score-0.256]
</p><p>59 Similar problems have been observed with Gibbs samplers on Markov random ﬁelds [9]. [sent-111, score-0.16]
</p><p>60 In these cases, convergence can often be accelerated by constructing a series of “coarser  scale” approximate models in which the Gibbs sampler can move between modes more easily [10]. [sent-112, score-0.281]
</p><p>61 For Gaussian mixture products, KD-trees provide a simple, intuitive, and easily constructed set of coarser scale models. [sent-114, score-0.193]
</p><p>62 We start at the same coarse scale for all input mixtures, and perform standard Gibbs sampling on that scale’s summary Gaussians. [sent-116, score-0.255]
</p><p>63 After several iterations, we condition on a data sample (as in the parallel Gibbs sampler of Section 2. [sent-117, score-0.302]
</p><p>64 Intuitively, by gradually moving from coarse to ﬁne scales, multiscale sampling can better explore all of the product density’s important modes. [sent-119, score-0.575]
</p><p>65 As the number of sampling iterations approaches inﬁnity, multiscale samplers have the same asymptotic properties as standard Gibbs samplers. [sent-120, score-0.589]
</p><p>66 Unfortunately, there is no guarantee that multiscale sampling will improve performance. [sent-121, score-0.407]
</p><p>67 This leads to an -exact sampler for which a label L = [l1 , . [sent-126, score-0.351]
</p><p>68 We denote ˆ subsets of labels in the input densities with lowercase script (li ), and sets of labels in the product density by L = l1 × · · · × ld . [sent-130, score-0.747]
</p><p>69 The approximate sampling procedure is similar to the exact sampler of Section 2. [sent-131, score-0.56]
</p><p>70 We ﬁrst construct KD-tree representations of each input density (as in Figure 2(a)), and use a multi–tree recursion to approximate the partition ˆ function Z = wL by summarizing sets of labels L where possible. [sent-133, score-0.381]
</p><p>71 We use the KD-trees’ distance bounds to compute bounds on each of these pairwise distance terms for a collection of labels L = l1 ×· · ·×ld . [sent-138, score-0.236]
</p><p>72 The product of the upper (lower) pairwise bounds is itself an upper (lower) bound on the total distance contribution + − for any label L within the set; denote these bounds by KL and KL , respectively. [sent-139, score-0.398]
</p><p>73 If 1 (Kmax − Kmin ) ≤ Zmin δ, approximate this combination of label sets: 2 1 (a) wL = 2 (Kmax + Kmin ) ( wli ), where wli = li ∈li wli is cached by the KD-trees ˆ (b) Zmin = Zmin + Kmin ( wli ) ˆ ˆ (c) Z = Z + wL ˆ 4. [sent-151, score-1.201]
</p><p>74 , ld ]) where Nearer(Farther) returns the nearer (farther) of the ﬁrst two arguments to the third. [sent-167, score-0.164]
</p><p>75 Algorithm 1: Recursive multi-tree algorithm for approximately evaluating the partition function Z of the product of d Gaussian mixture densities represented by KD–trees. [sent-168, score-0.417]
</p><p>76 (c) If c ≤ Zuj < c + wL for any j, draw L ∈ L by sampling li ∈ li with weight wli /wli ˆ ˆ ˆ ˆ 3. [sent-172, score-0.941]
</p><p>77 ˆ ˆ  quantities required by this algorithm may be stored within the KD–trees, avoiding searches over the sets li . [sent-176, score-0.226]
</p><p>78 At the algorithm’s termination, the total error is bounded by 1 ˆ K+ − K− wl ≤ Zδ |Z − Z| ≤ |wL − wL | ≤ ˆ wl ≤ Zδ (7) 2  L  L  L  i  L  i  L  where the last inequality follows because each input mixture’s weights are normalized. [sent-177, score-0.905]
</p><p>79 We do not explicitly construct the cumulative distribution, but instead use the same approximate partial weight ˆ sums used to determine Z (see equation (6)) to ﬁnd the block of labels L = l1 × · · · × ld associated with each sample. [sent-182, score-0.377]
</p><p>80 Since all labels L ∈ L within this block have approximately ∗ equal distance contribution KL ≈ KL , we independently sample a label li within each set li proportionally to the weight wli . [sent-183, score-0.974]
</p><p>81 This algorithm is guaranteed to sample each label L with probability pL ∈ [pL − , pL + ]: ˆ wL 2δ wL ˆ − ≤ (8) |ˆL − pL | = p ˆ Z 1−δ Z ∗ |KL −KL | wli ≤ δ( wli ) ≤ δ and Z 1+δ δ. [sent-186, score-0.57]
</p><p>82 1 Products of One–Dimensional Gaussian Mixtures In this section, we compare the sampling methods discussed in this paper on three challenging one–dimensional examples, each involving products of mixtures of 100 Gaussians (see Figure 4). [sent-189, score-0.49]
</p><p>83 We measure performance by drawing 100 samples, constructing a kernel density estimate using likelihood cross–validation [1], and calculating the KL divergence from the true product density. [sent-190, score-0.34]
</p><p>84 For the product of three mixtures in Figure 4(a), the multiscale (MS) Gibbs samplers dramatically outperform standard Gibbs sampling. [sent-192, score-0.691]
</p><p>85 In addition, we see that sequential Gibbs sampling is more accurate than parallel. [sent-193, score-0.269]
</p><p>86 For a product of ﬁve densities (Figure 4(b)), the cost of exact sampling increases to 7. [sent-198, score-0.484]
</p><p>87 6 hours, but the –exact sampler matches its performance in less than one minute. [sent-199, score-0.246]
</p><p>88 For the previous two examples, mixture importance sampling (IS) is nearly as accurate as the best multiscale methods (Gaussian IS seems ineffective). [sent-202, score-0.647]
</p><p>89 However, in cases where all of the input densities have little overlap with the product density, mixture IS performs very poorly (see Figure 4(c)). [sent-203, score-0.402]
</p><p>90 In contrast, multiscale samplers perform very well in such situations, because they can discard large numbers of low weight product density kernels. [sent-204, score-0.655]
</p><p>91 2 Tracking an Object using Nonparametric Belief Propagation NBP [2] solves inference problems on non–Gaussian graphical models by propagating the results of local sampling computations. [sent-206, score-0.196]
</p><p>92 Using our multiscale samplers, we applied NBP to a simple tracking problem in which we observe a slowly moving object in a sea of randomly shifting clutter. [sent-207, score-0.233]
</p><p>93 –exact sampling matches the performance of exact sampling, but takes half as long. [sent-209, score-0.279]
</p><p>94 As in the previous section, multiscale Gibbs sampling is much more accurate than standard Gibbs sampling. [sent-211, score-0.444]
</p><p>95 6  Discussion  For products of a few mixtures, the –exact sampler is extremely fast, and is guaranteed to give good performance. [sent-212, score-0.388]
</p><p>96 As the number of mixtures grow, –exact sampling may become overly costly, but the sequential multiscale Gibbs sampler typically produces accurate samples with only a few iterations. [sent-213, score-0.95]
</p><p>97 Generalised Gibbs sampler and multigrid Monte Carlo for Bayesian computation. [sent-275, score-0.246]
</p><p>98 6  Computation Time (sec)  Figure 4: Comparison of average sampling accuracy versus computation time for different algorithms (see text). [sent-327, score-0.196]
</p><p>99 The particle ﬁlter and Gibbs samplers are allowed equal computation. [sent-338, score-0.197]
</p><p>100 (b) –exact sampling is very accurate, while a particle ﬁlter loses track. [sent-340, score-0.26]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wl', 0.425), ('gibbs', 0.384), ('sampler', 0.246), ('li', 0.226), ('multiscale', 0.211), ('wli', 0.202), ('sampling', 0.196), ('mixtures', 0.182), ('mixture', 0.168), ('xx', 0.164), ('nbp', 0.161), ('samplers', 0.16), ('kl', 0.16), ('product', 0.138), ('ld', 0.12), ('zmin', 0.118), ('labels', 0.113), ('products', 0.112), ('lj', 0.107), ('density', 0.107), ('label', 0.105), ('kmin', 0.088), ('kmax', 0.088), ('ms', 0.086), ('exact', 0.083), ('pl', 0.082), ('densities', 0.067), ('draw', 0.052), ('multitree', 0.051), ('cumulative', 0.049), ('wi', 0.047), ('nonparametric', 0.047), ('gaussian', 0.046), ('sec', 0.046), ('partition', 0.044), ('ihler', 0.044), ('nearer', 0.044), ('pairwise', 0.043), ('samples', 0.042), ('bounds', 0.04), ('evaluation', 0.04), ('monte', 0.039), ('mix', 0.039), ('weight', 0.039), ('carlo', 0.039), ('conditioned', 0.038), ('bounding', 0.038), ('farther', 0.037), ('accurate', 0.037), ('particle', 0.037), ('sequential', 0.036), ('importance', 0.035), ('divergence', 0.035), ('approximate', 0.035), ('kernel', 0.034), ('trees', 0.034), ('tree', 0.034), ('xli', 0.034), ('kd', 0.033), ('representations', 0.032), ('pi', 0.032), ('contribution', 0.032), ('subsets', 0.031), ('sample', 0.031), ('guaranteed', 0.03), ('coarse', 0.03), ('dmin', 0.029), ('xlj', 0.029), ('lowercase', 0.029), ('input', 0.029), ('summarized', 0.027), ('leaf', 0.027), ('cached', 0.027), ('sorting', 0.027), ('sudderth', 0.027), ('gauss', 0.027), ('loses', 0.027), ('willsky', 0.027), ('repeat', 0.026), ('weights', 0.026), ('drawing', 0.026), ('belief', 0.026), ('dmax', 0.025), ('parallel', 0.025), ('lx', 0.025), ('coarser', 0.025), ('distances', 0.024), ('experts', 0.024), ('tunable', 0.023), ('maintains', 0.023), ('gaussians', 0.023), ('xj', 0.023), ('existing', 0.023), ('propagation', 0.023), ('tracking', 0.022), ('develops', 0.022), ('boxes', 0.022), ('draws', 0.022), ('iterations', 0.022), ('construct', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="58-tfidf-1" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>2 0.16484004 <a title="58-tfidf-2" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>Author: Mark A. Paskin</p><p>Abstract: Rao–Blackwellization is an approximation technique for probabilistic inference that ﬂexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efﬁcient implementation of Rao–Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster’s variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems. 1</p><p>3 0.15941122 <a title="58-tfidf-3" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>Author: Maneesh Sahani</p><p>Abstract: Signiﬁcant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a signiﬁcant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generativemodelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way. 1</p><p>4 0.099638365 <a title="58-tfidf-4" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>5 0.096567877 <a title="58-tfidf-5" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>Author: Leonid Sigal, Michael Isard, Benjamin H. Sigelman, Michael J. Black</p><p>Abstract: The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuousvalued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle ﬁlter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter. 1</p><p>6 0.08986745 <a title="58-tfidf-6" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>7 0.080738224 <a title="58-tfidf-7" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>8 0.076882951 <a title="58-tfidf-8" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>9 0.071403019 <a title="58-tfidf-9" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>10 0.068784676 <a title="58-tfidf-10" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>11 0.066302754 <a title="58-tfidf-11" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>12 0.064217366 <a title="58-tfidf-12" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>13 0.064190738 <a title="58-tfidf-13" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>14 0.061506439 <a title="58-tfidf-14" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>15 0.060823053 <a title="58-tfidf-15" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>16 0.060728185 <a title="58-tfidf-16" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>17 0.059030525 <a title="58-tfidf-17" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>18 0.057719398 <a title="58-tfidf-18" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>19 0.056450274 <a title="58-tfidf-19" href="./nips-2003-Minimax_Embeddings.html">128 nips-2003-Minimax Embeddings</a></p>
<p>20 0.055401754 <a title="58-tfidf-20" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.188), (1, -0.038), (2, -0.044), (3, 0.111), (4, 0.062), (5, -0.034), (6, 0.11), (7, 0.014), (8, 0.003), (9, -0.006), (10, -0.028), (11, -0.052), (12, -0.032), (13, -0.073), (14, 0.132), (15, 0.028), (16, -0.061), (17, 0.079), (18, 0.117), (19, 0.052), (20, -0.129), (21, 0.082), (22, 0.029), (23, 0.114), (24, 0.05), (25, -0.139), (26, 0.192), (27, 0.009), (28, 0.156), (29, -0.138), (30, -0.091), (31, -0.088), (32, -0.069), (33, -0.105), (34, 0.04), (35, -0.018), (36, -0.114), (37, -0.183), (38, -0.009), (39, 0.028), (40, 0.069), (41, 0.077), (42, 0.012), (43, 0.168), (44, 0.006), (45, 0.149), (46, -0.004), (47, 0.084), (48, 0.03), (49, 0.204)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97181618 <a title="58-lsi-1" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>2 0.63879883 <a title="58-lsi-2" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>Author: Mark A. Paskin</p><p>Abstract: Rao–Blackwellization is an approximation technique for probabilistic inference that ﬂexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efﬁcient implementation of Rao–Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster’s variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems. 1</p><p>3 0.56633753 <a title="58-lsi-3" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>Author: Max Welling, Andriy Mnih, Geoffrey E. Hinton</p><p>Abstract: In models that deﬁne probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model’s distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate.</p><p>4 0.51400238 <a title="58-lsi-4" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>Author: Jaco Vermaak, Simon J. Godsill, Arnaud Doucet</p><p>Abstract: We propose a method for sequential Bayesian kernel regression. As is the case for the popular Relevance Vector Machine (RVM) [10, 11], the method automatically identiﬁes the number and locations of the kernels. Our algorithm overcomes some of the computational difﬁculties related to batch methods for kernel regression. It is non-iterative, and requires only a single pass over the data. It is thus applicable to truly sequential data sets and batch data sets alike. The algorithm is based on a generalisation of Importance Sampling, which allows the design of intuitively simple and efﬁcient proposal distributions for the model parameters. Comparative results on two standard data sets show our algorithm to compare favourably with existing batch estimation strategies.</p><p>5 0.51390827 <a title="58-lsi-5" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>Author: Maneesh Sahani</p><p>Abstract: Signiﬁcant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a signiﬁcant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generativemodelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way. 1</p><p>6 0.49837807 <a title="58-lsi-6" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>7 0.45166695 <a title="58-lsi-7" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>8 0.44062993 <a title="58-lsi-8" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>9 0.35347748 <a title="58-lsi-9" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>10 0.35083902 <a title="58-lsi-10" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>11 0.3133828 <a title="58-lsi-11" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>12 0.29114181 <a title="58-lsi-12" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>13 0.29051059 <a title="58-lsi-13" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>14 0.2833738 <a title="58-lsi-14" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>15 0.28178912 <a title="58-lsi-15" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>16 0.27299598 <a title="58-lsi-16" href="./nips-2003-Minimax_Embeddings.html">128 nips-2003-Minimax Embeddings</a></p>
<p>17 0.27068329 <a title="58-lsi-17" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>18 0.26158246 <a title="58-lsi-18" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>19 0.2585887 <a title="58-lsi-19" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>20 0.25856203 <a title="58-lsi-20" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.055), (11, 0.018), (29, 0.018), (33, 0.025), (34, 0.304), (35, 0.054), (53, 0.071), (69, 0.053), (71, 0.072), (76, 0.034), (85, 0.099), (91, 0.074), (99, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79500437 <a title="58-lda-1" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>2 0.50435513 <a title="58-lda-2" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>Author: Charles Kemp, Thomas L. Griffiths, Sean Stromsten, Joshua B. Tenenbaum</p><p>Abstract: We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efﬁcient computation of the optimal Bayesian classiﬁcation function from the labeled examples. We test our approach on eight real-world datasets. 1</p><p>3 0.50313604 <a title="58-lda-3" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri</p><p>Abstract: The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classiﬁcation algorithm. However, the objective function optimized in most of these algorithms is the error rate and not the AUC value. We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the ﬁrst exact expression of the expected value and the variance of the AUC for a ﬁxed error rate. Our results show that the average AUC is monotonically increasing as a function of the classiﬁcation accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable. Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values. We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets demonstrating the beneﬁts of an algorithm speciﬁcally designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC. 1 Motivation In many applications, the overall classiﬁcation error rate is not the most pertinent performance measure, criteria such as ordering or ranking seem more appropriate. Consider for example the list of relevant documents returned by a search engine for a speciﬁc query. That list may contain several thousand documents, but, in practice, only the top ﬁfty or so are examined by the user. Thus, a search engine’s ranking of the documents is more critical than the accuracy of its classiﬁcation of all documents as relevant or not. More generally, for a binary classiﬁer assigning a real-valued score to each object, a better correlation between output scores and the probability of correct classiﬁcation is highly desirable. A natural criterion or summary statistic often used to measure the ranking quality of a classiﬁer is the area under an ROC curve (AUC) [8].1 However, the objective function optimized by most classiﬁcation algorithms is the error rate and not the AUC. Recently, several algorithms have been proposed for maximizing the AUC value locally [4] or maximizing some approximations of the global AUC value [9, 15], but, in general, these algorithms do not obtain AUC values signiﬁcantly better than those obtained by an algorithm designed to minimize the error rates. Thus, it is important to determine the relationship between the AUC values and the error rate. ∗ This author’s new address is: Google Labs, 1440 Broadway, New York, NY 10018, corinna@google.com. 1 The AUC value is equivalent to the Wilcoxon-Mann-Whitney statistic [8] and closely related to the Gini index [1]. It has been re-invented under the name of L-measure by [11], as already pointed out by [2], and slightly modiﬁed under the name of Linear Ranking by [13, 14]. True positive rate ROC Curve. AUC=0.718 (1,1) True positive rate = (0,0) False positive rate = False positive rate correctly classiﬁed positive total positive incorrectly classiﬁed negative total negative Figure 1: An example of ROC curve. The line connecting (0, 0) and (1, 1), corresponding to random classiﬁcation, is drawn for reference. The true positive (negative) rate is sometimes referred to as the sensitivity (resp. speciﬁcity) in this context. In the following sections, we give a detailed statistical analysis of the relationship between the AUC and the error rate, including the ﬁrst exact expression of the expected value and the variance of the AUC for a ﬁxed error rate.2 We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets and demonstrate the beneﬁts of an algorithm speciﬁcally designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC. 2 Deﬁnition and properties of the AUC The Receiver Operating Characteristics (ROC) curves were originally developed in signal detection theory [3] in connection with radio signals, and have been used since then in many other applications, in particular for medical decision-making. Over the last few years, they have found increased interest in the machine learning and data mining communities for model evaluation and selection [12, 10, 4, 9, 15, 2]. The ROC curve for a binary classiﬁcation problem plots the true positive rate as a function of the false positive rate. The points of the curve are obtained by sweeping the classiﬁcation threshold from the most positive classiﬁcation value to the most negative. For a fully random classiﬁcation, the ROC curve is a straight line connecting the origin to (1, 1). Any improvement over random classiﬁcation results in an ROC curve at least partially above this straight line. Fig. (1) shows an example of ROC curve. The AUC is deﬁned as the area under the ROC curve and is closely related to the ranking quality of the classiﬁcation as shown more formally by Lemma 1 below. Consider a binary classiﬁcation task with m positive examples and n negative examples. We will assume that a classiﬁer outputs a strictly ordered list for these examples and will denote by 1X the indicator function of a set X. Lemma 1 ([8]) Let c be a ﬁxed classiﬁer. Let x1 , . . . , xm be the output of c on the positive examples and y1 , . . . , yn its output on the negative examples. Then, the AUC, A, associated to c is given by: m n i=1 j=1 1xi >yj (1) A= mn that is the value of the Wilcoxon-Mann-Whitney statistic [8]. Proof. The proof is based on the observation that the AUC value is exactly the probability P (X > Y ) where X is the random variable corresponding to the distribution of the outputs for the positive examples and Y the one corresponding to the negative examples [7]. The Wilcoxon-Mann-Whitney statistic is clearly the expression of that probability in the discrete case, which proves the lemma [8]. Thus, the AUC can be viewed as a measure based on pairwise comparisons between classiﬁcations of the two classes. With a perfect ranking, all positive examples are ranked higher than the negative ones and A = 1. Any deviation from this ranking decreases the AUC. 2 An attempt in that direction was made by [15], but, unfortunately, the authors’ analysis and the result are both wrong. Threshold θ k − x Positive examples x Negative examples n − x Negative examples m − (k − x) Positive examples Figure 2: For a ﬁxed number of errors k, there may be x, 0 ≤ x ≤ k, false negative examples. 3 The Expected Value of the AUC In this section, we compute exactly the expected value of the AUC over all classiﬁcations with a ﬁxed number of errors and compare that to the error rate. Different classiﬁers may have the same error rate but different AUC values. Indeed, for a given classiﬁcation threshold θ, an arbitrary reordering of the examples with outputs more than θ clearly does not affect the error rate but leads to different AUC values. Similarly, one may reorder the examples with output less than θ without changing the error rate. Assume that the number of errors k is ﬁxed. We wish to compute the average value of the AUC over all classiﬁcations with k errors. Our model is based on the simple assumption that all classiﬁcations or rankings with k errors are equiprobable. One could perhaps argue that errors are not necessarily evenly distributed, e.g., examples with very high or very low ranks are less likely to be errors, but we cannot justify such biases in general. For a given classiﬁcation, there may be x, 0 ≤ x ≤ k, false positive examples. Since the number of errors is ﬁxed, there are k − x false negative examples. Figure 3 shows the corresponding conﬁguration. The two regions of examples with classiﬁcation outputs above and below the threshold are separated by a vertical line. For a given x, the computation of the AUC, A, as given by Eq. (1) can be divided into the following three parts: A1 + A2 + A3 A= , with (2) mn A1 = the sum over all pairs (xi , yj ) with xi and yj in distinct regions; A2 = the sum over all pairs (xi , yj ) with xi and yj in the region above the threshold; A3 = the sum over all pairs (xi , yj ) with xi and yj in the region below the threshold. The ﬁrst term, A1 , is easy to compute. Since there are (m − (k − x)) positive examples above the threshold and n − x negative examples below the threshold, A1 is given by: A1 = (m − (k − x))(n − x) (3) To compute A2 , we can assign to each negative example above the threshold a position based on its classiﬁcation rank. Let position one be the ﬁrst position above the threshold and let α1 < . . . < αx denote the positions in increasing order of the x negative examples in the region above the threshold. The total number of examples classiﬁed as positive is N = m − (k − x) + x. Thus, by deﬁnition of A2 , x A2 = (N − αi ) − (x − i) (4) i=1 where the ﬁrst term N − αi represents the number of examples ranked higher than the ith example and the second term x − i discounts the number of negative examples incorrectly ranked higher than the ith example. Similarly, let α1 < . . . < αk−x denote the positions of the k − x positive examples below the threshold, counting positions in reverse by starting from the threshold. Then, A3 is given by: x A3 = (N − αj ) − (x − j) (5) j=1 with N = n − x + (k − x) and x = k − x. Combining the expressions of A1 , A2 , and A3 leads to: A= A1 + A2 + A3 (k − 2x)2 + k ( =1+ − mn 2mn x i=1 αi + mn x j=1 αj ) (6) Lemma 2 For a ﬁxed x, the average value of the AUC A is given by: < A >x = 1 − x n + k−x m 2 (7) x Proof. The proof is based on the computation of the average values of i=1 αi and x j=1 αj for a given x. We start by computing the average value < αi >x for a given i, 1 ≤ i ≤ x. Consider all the possible positions for α1 . . . αi−1 and αi+1 . . . αx , when the value of αi is ﬁxed at say αi = l. We have i ≤ l ≤ N − (x − i) since there need to be at least i − 1 positions before αi and N − (x − i) above. There are l − 1 possible positions for α1 . . . αi−1 and N − l possible positions for αi+1 . . . αx . Since the total number of ways of choosing the x positions for α1 . . . αx out of N is N , the average value < αi >x is: x N −(x−i) l=i < αi >x = l l−1 i−1 N −l x−i (8) N x Thus, x < αi >x = x i=1 i=1 Using the classical identity: x < αi >x = N −(x−i) l−1 l i−1 l=i N x u p1 +p2 =p p1 N l=1 l N −1 x−1 N x i=1 N −l x−i v p2 = = N l=1 = u+v p N (N + 1) 2 x l−1 i=1 i−1 N x l N −l x−i (9) , we can write: N −1 x−1 N x = x(N + 1) 2 (10) Similarly, we have: x < αj >x = j=1 x Replacing < i=1 αi >x and < Eq. (10) and Eq. (11) leads to: x j=1 x (N + 1) 2 (11) αj >x in Eq. (6) by the expressions given by (k − 2x)2 + k − x(N + 1) − x (N + 1) =1− 2mn which ends the proof of the lemma. < A >x = 1 + x n + k−x m 2 (12) Note that Eq. (7) shows that the average AUC value for a given x is simply one minus the average of the accuracy rates for the positive and negative classes. Proposition 1 Assume that a binary classiﬁcation task with m positive examples and n negative examples is given. Then, the expected value of the AUC A over all classiﬁcations with k errors is given by: < A >= 1 − k (n − m)2 (m + n + 1) − m+n 4mn k−1 m+n x=0 x k m+n+1 x=0 x k − m+n (13) Proof. Lemma 2 gives the average value of the AUC for a ﬁxed value of x. To compute the average over all possible values of x, we need to weight the expression of Eq. (7) with the total number of possible classiﬁcations for a given x. There are N possible ways of x choosing the positions of the x misclassiﬁed negative examples, and similarly N possible x ways of choosing the positions of the x = k − x misclassiﬁed positive examples. Thus, in view of Lemma 2, the average AUC is given by: < A >= k N x=0 x N x (1 − k N x=0 x N x k−x x n+ m 2 ) (14) r=0.05 r=0.01 r=0.1 r=0.25 0.0 0.1 0.2 r=0.5 0.3 Error rate 0.4 0.5 .00 .05 .10 .15 .20 .25 0.5 0.6 0.7 0.8 0.9 1.0 Mean value of the AUC Relative standard deviation r=0.01 r=0.05 r=0.1 0.0 0.1 r=0.25 0.2 0.3 Error rate r=0.5 0.4 0.5 Figure 3: Mean (left) and relative standard deviation (right) of the AUC as a function of the error rate. Each curve corresponds to a ﬁxed ratio of r = n/(n + m). The average AUC value monotonically increases with the accuracy. For n = m, as for the top curve in the left plot, the average AUC coincides with the accuracy. The standard deviation decreases with the accuracy, and the lowest curve corresponds to n = m. This expression can be simpliﬁed into Eq. (13)3 using the following novel identities: k X N x x=0 k X N x x x=0 ! N x ! ! N x ! = = ! k X n+m+1 x x=0 (15) ! k X (k − x)(m − n) + k n + m + 1 2 x x=0 (16) that we obtained by using Zeilberger’s algorithm4 and numerous combinatorial ’tricks’. From the expression of Eq. (13), it is clear that the average AUC value is identical to the accuracy of the classiﬁer only for even distributions (n = m). For n = m, the expected value of the AUC is a monotonic function of the accuracy, see Fig. (3)(left). For a ﬁxed ratio of n/(n + m), the curves are obtained by increasing the accuracy from n/(n + m) to 1. The average AUC varies monotonically in the range of accuracy between 0.5 and 1.0. In other words, on average, there seems nothing to be gained in designing speciﬁc learning algorithms for maximizing the AUC: a classiﬁcation algorithm minimizing the error rate also optimizes the AUC. However, this only holds for the average AUC. Indeed, we will show in the next section that the variance of the AUC value is not null for any ratio n/(n + m) when k = 0. 4 The Variance of the AUC 2 Let D = mn + (k−2x) +k , a = i=1 αi , a = j=1 αj , and α = a + a . Then, by 2 Eq. (6), mnA = D − α. Thus, the variance of the AUC, σ 2 (A), is given by: (mn)2 σ 2 (A) x x = < (D − α)2 − (< D > − < α >)2 > = < D2 > − < D >2 + < α2 > − < α >2 −2(< αD > − < α >< D >) (17) As before, to compute the average of a term X over all classiﬁcations, we can ﬁrst determine its average < X >x for a ﬁxed x, and then use the function F deﬁned by: F (Y ) = k N N x=0 x x k N N x=0 x x Y (18) and < X >= F (< X >x ). A crucial step in computing the exact value of the variance of x the AUC is to determine the value of the terms of the type < a2 >x =< ( i=1 αi )2 >x . 3 An essential difference between Eq. (14) and the expression given by [15] is the weighting by the number of conﬁgurations. The authors’ analysis leads them to the conclusion that the average AUC is identical to the accuracy for all ratios n/(n + m), which is false. 4 We thank Neil Sloane for having pointed us to Zeilberger’s algorithm and Maple package. x Lemma 3 For a ﬁxed x, the average of ( i=1 αi )2 is given by: x(N + 1) < a2 > x = (3N x + 2x + N ) 12 (19) Proof. By deﬁnition of a, < a2 >x = b + 2c with: x x α2 >x i b =< c =< αi αj >x (20) 1≤i</p><p>4 0.49587512 <a title="58-lda-4" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>5 0.49316514 <a title="58-lda-5" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>Author: Ben Taskar, Carlos Guestrin, Daphne Koller</p><p>Abstract: In typical classiﬁcation tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of conﬁdence of the classiﬁer, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efﬁciently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efﬁcient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classiﬁcation demonstrate very signiﬁcant gains over previous approaches. 1</p><p>6 0.49074537 <a title="58-lda-6" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>7 0.48992288 <a title="58-lda-7" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>8 0.48706743 <a title="58-lda-8" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>9 0.48697785 <a title="58-lda-9" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>10 0.48674238 <a title="58-lda-10" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>11 0.48476171 <a title="58-lda-11" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>12 0.48470026 <a title="58-lda-12" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>13 0.48449478 <a title="58-lda-13" href="./nips-2003-An_Infinity-sample_Theory_for_Multi-category_Large_Margin_Classification.html">23 nips-2003-An Infinity-sample Theory for Multi-category Large Margin Classification</a></p>
<p>14 0.48428988 <a title="58-lda-14" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>15 0.48254019 <a title="58-lda-15" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>16 0.48162901 <a title="58-lda-16" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>17 0.48121694 <a title="58-lda-17" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>18 0.48107177 <a title="58-lda-18" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>19 0.48089677 <a title="58-lda-19" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>20 0.48042381 <a title="58-lda-20" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
