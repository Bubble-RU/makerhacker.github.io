<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-93" href="#">nips2003-93</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</h1>
<br/><p>Source: <a title="nips-2003-93-pdf" href="http://papers.nips.cc/paper/2389-information-dynamics-and-emergent-computation-in-recurrent-circuits-of-spiking-neurons.pdf">pdf</a></p><p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 1</p><p>Reference: <a title="nips-2003-93-reference" href="../nips2003_reference/nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 at  Abstract We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. [sent-3, score-0.456]
</p><p>2 It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. [sent-4, score-1.045]
</p><p>3 1  Introduction  Common analytical tools of computational complexity theory cannot be applied to recurrent circuits with complex dynamic components, such as biologically realistic neuron models and dynamic synapses. [sent-5, score-0.232]
</p><p>4 In this article we explore the capability of information theoretic concepts to throw light on emergent computations in recurrent circuit of spiking neurons. [sent-6, score-1.158]
</p><p>5 Previous work on these methodological problems had focused on estimating the information in spike trains, i. [sent-9, score-0.262]
</p><p>6 We will deﬁne the speciﬁc circuit model used for our study in section 2 (although the methods that we apply appear to be useful for to a much wider class of analog and digital recurrent circuits). [sent-13, score-0.792]
</p><p>7 The results of applications of these methods to the analysis of the distribution and dynamics of information in a generic recurrent circuit of spiking neurons are presented in section 4. [sent-15, score-1.209]
</p><p>8 Applications of these methods to the analysis of emergent computations are discussed in section 5. [sent-16, score-0.232]
</p><p>9 A possible templates for spike train segments template  1. [sent-17, score-0.359]
</p><p>10 segment  0 1  B an example for resulting input spike trains (with noise) template 1 (s1=1)  0  0. [sent-21, score-0.528]
</p><p>11 Each input consists of 5 spike trains of length 800 ms generated from 4 segments of length 200 ms each. [sent-30, score-0.625]
</p><p>12 A For each segment 2 templates 0 and 1 were generated randomly (Poisson spike trains with a frequency of 20 Hz). [sent-31, score-0.441]
</p><p>13 B The actual input spike trains were generated by choosing randomly for each segment i, i = 1, . [sent-32, score-0.464]
</p><p>14 , 4, one of the two associated templates (si = 0 or si = 1), and then generating a noisy version by moving each spike by an amount drawn from a Gaussian distribution with mean 0 and SD 4 ms. [sent-35, score-0.316]
</p><p>15 The 800 neurons of the circuit were arranged on two 20 × 20 layers L1 and L2. [sent-37, score-0.783]
</p><p>16 Circuit inputs consisting of 5 spike trains were injected into a randomly chosen subset of neurons in layer L1 (the connection probability was set to 0. [sent-38, score-0.563]
</p><p>17 25 for each of the 5 input channels and each neuron in layer L1). [sent-39, score-0.143]
</p><p>18 We modeled the (short term) dynamics of synapses according to the model proposed in [1], with the synaptic parameters U (use), D (time constant for depression), F (time constant for facilitation) randomly chosen from Gaussian distributions that model empirical data for such connections. [sent-40, score-0.151]
</p><p>19 Parameters of neurons and synapses were chosen as in [2] to ﬁt data from microcircuits in rat somatosensory cortex (based on [3] and [1]). [sent-41, score-0.304]
</p><p>20 Since neural microcircuits in the nervous system often receive salient input in the form of spatio-temporal ﬁring patterns (e. [sent-42, score-0.202]
</p><p>21 from arrays of sensory neurons, or from other brain areas), we have concentrated on circuit inputs of this type. [sent-44, score-0.591]
</p><p>22 Information dynamics and emergent computation in recurrent circuits of spiking neurons were investigated for input streams over 800 ms consisting of sequences of noisy versions of 4 of such ﬁring patterns. [sent-46, score-0.962]
</p><p>23 We restricted our analysis to the case where in each of the four 200 ms segments one of two template patterns is possible, see Fig. [sent-47, score-0.281]
</p><p>24 In the following we write si = 1 (si = 0) if a noisy version of template 1 (0) is used in the i-th time segment of the circuit input. [sent-49, score-0.854]
</p><p>25 2 shows the response of a circuit of spiking neurons (drawn from the distribution speciﬁed above) to the input stream exhibited in Fig. [sent-51, score-0.965]
</p><p>26 2 shows the current ﬁring activity of one layer of the circuit at a particular point t in time. [sent-54, score-0.687]
</p><p>27 Since in such rather small circuit (compared for example with the estimated 105 neurons below a mm2 of cortical surface) very few neurons ﬁre at any given ms, we have replaced each spike by a pulse whose amplitude decays exponentially with a time constant of 30 ms. [sent-55, score-1.166]
</p><p>28 This models the impact of a spike on the membrane potential of a generic postsynaptic neuron. [sent-56, score-0.302]
</p><p>29 , r800 (t) consisting of 800 analog values from the  6 4 2 0  t=280 ms  t=290 ms  t=300 ms  t=310 ms  Figure 2: Snapshots of the ﬁrst 400 components of the circuit state r(t) (corresponding to the neurons in the layer L1) at various times t for the input shown at the bottom of ﬁg. [sent-60, score-1.608]
</p><p>30 A spike at time ts ≤ t adds a value of exp(−(t − ts )/(30ms)) to the corresponding component of the state r(t). [sent-63, score-0.268]
</p><p>31 800 neurons in the circuit is exactly the “liquid state” of the circuit at time t in the context of the abstract computational model introduced in [2]. [sent-64, score-1.374]
</p><p>32 In the subsequent sections we will analyze the temporal dynamics of the information contained in these momentary circuit states r(t). [sent-65, score-0.855]
</p><p>33 It is well known that empirical estimates of the entropy tend to underestimate the true entropy of a random variable (see e. [sent-69, score-0.276]
</p><p>34 The seriousness of this problem becomes obvious from results achieved for our study case of a generic neural microcircuit shown in Fig. [sent-76, score-0.287]
</p><p>35 The dashed line shows the dependence of “raw” estimates M Iraw of the mutual information M I(s2 , R) on the sample size2 N , which ranges here from 103 to 2 · 105 . [sent-78, score-0.41]
</p><p>36 For more components d of the current circuit state r(t), e. [sent-80, score-0.761]
</p><p>37 for estimating the mutual information M I(s2 , R) between the preceding circuit input s2 and the current ﬁring activity in a subcircuit consisting of d = 20 or more neurons, even sample sizes beyond 106 are likely to severely overestimate this mutual information. [sent-82, score-1.5]
</p><p>38 1  One should note that these circuit states do not reﬂect the complete current state of the underlying dynamical system, only those parts of the state of the dynamical system that are in principle “visible” for neurons outside the circuit. [sent-83, score-1.036]
</p><p>39 The current values of the membrane potential of neurons in the circuit and the current values of internal variables of dynamic synapses of the circuit are not visible in this sense. [sent-84, score-1.487]
</p><p>40 2 In our case the sample size N refers to the number of computer simulations of the circuit response to new drawings of circuit inputs, with new drawings of temporal jitter in the input spike trains and initial conditions of the neurons in the circuit. [sent-85, score-1.955]
</p><p>41 3 For direct estimates of the M I the analog value of each component of the circuit state r(t) has to be divided into discrete bins. [sent-86, score-0.802]
</p><p>42 35  0  E lower bounds (d=5, s3) 1 H(R)raw  F lower bounds (d=10, s3) 1  H(R|X)raw  7  MI [bit]  entropy [bit]  H(R)Ma  0. [sent-107, score-0.453]
</p><p>43 75 3  10  4  10  sample size  5  10  0 3 10  4  10  sample size  5  10  0 3 10  4  10  5  10  sample size  Figure 3: Estimated mutual information depends on sample size. [sent-117, score-0.677]
</p><p>44 In all panels d denotes the number of components of the circuit state r(t) at time t = 660 ms (or equivalently the number of neurons considered). [sent-118, score-1.042]
</p><p>45 A Dependence of the “raw” estimate M Iraw and two corrected estimates M Inaive and M Iinf inity of the mutual information M I(s2 , R) (see text). [sent-119, score-0.469]
</p><p>46 B Lower bounds M I(s2 , h(R)) for the mutual information obtained via classiﬁers h which are trained to predict the actual value of s2 given the circuit state r(t). [sent-120, score-1.059]
</p><p>47 In the case of the Bayes classiﬁer M I(s2 , h(R)) was estimated by employing a leave-one-out procedure (which is computationally efﬁcient for a Bayes classiﬁer), whereas for the linear classiﬁers a test set of size 5 · 104 was used (hence no results beyond a sample size of 1. [sent-124, score-0.139]
</p><p>48 The ﬁlled triangle marks the sample size from which on the Mabound is below the raw estimate. [sent-129, score-0.257]
</p><p>49 1 of [7] it is proposed to subtract one of two possible bias correction terms Bnaive and Bf ull from the raw estimate M Iraw of the mutual information. [sent-134, score-0.502]
</p><p>50 This correction is too optimistic for these applications, since the corrected estimate M Inaive = M Iraw − Bnaive at small sample sizes (e. [sent-137, score-0.262]
</p><p>51 104 ) is still substantially larger than the raw estimate M Iraw at large sample sizes (e. [sent-139, score-0.346]
</p><p>52 The subtraction of the second proposed term Bf ull is not applicable in our situation because it yields forM If ull = M Iraw − Bf ull values lower than zero for all considered sample sizes. [sent-142, score-0.496]
</p><p>53 The reason is, that Bf ull is proportional to the quotient “number of possible response bins” / N and the number of possible response bins is in the order of 3010 in this example. [sent-143, score-0.154]
</p><p>54 This approach is based on a series expansion of M I in 1/N [6] and is effectively a method to get an empirical estimate M Iinf inity of the mutual information for inﬁnite sample size (N → ∞). [sent-145, score-0.463]
</p><p>55 3A that for  moderate sample sizes M Iinf inity also yields too optimistic estimates for M I. [sent-147, score-0.321]
</p><p>56 3A shows that the raw estimate M Iraw is still too high for N = 9 · 103 since M Iraw assumes a substantially smaller value at N = 2 · 105 . [sent-157, score-0.2]
</p><p>57 In view of this unreliability of – even corrected – estimates for the mutual information we have employed standard methods from machine learning in order to derive lower bounds for the M I (see for example [8] and [9] for references to preceding related work). [sent-158, score-0.621]
</p><p>58 This method is computationally feasible and yields with not too large sample sizes reliable lower bounds for the M I even for large numbers of components of the circuit state. [sent-159, score-0.978]
</p><p>59 In fact, we will apply it in sections 4 and 5 even to the full 800-component circuit state r(t). [sent-160, score-0.668]
</p><p>60 Obviously M I(X, h(R)) is easier to estimate than M I(X, R) if the dimension of h(R) is substantially lower than that of R, especially if h(R) assumes just a few discrete values. [sent-163, score-0.13]
</p><p>61 Similar approaches for estimating a lower bound were motivated by the idea of predicting the stimulus (X) given the neural response (R) (see [8], [9] and the references therein). [sent-166, score-0.141]
</p><p>62 3B, 3C, 3E, and 3F show for 3 different predictors h how the resulting lower bounds for the M I depend on the sample size N . [sent-170, score-0.294]
</p><p>63 It is noteworthy that the lower bounds M I(X, h(R)) derived with the empirical Bayes classiﬁer5 increase signiﬁcantly with the sample size6 and converge quite well to the upper bounds M Iraw (X, R). [sent-171, score-0.407]
</p><p>64 Furthermore the computationally less demanding7 use of linear classiﬁers h also yields signiﬁcant lower bounds for M I(X, R), especially if the true value of M I(X, R) is not too small. [sent-173, score-0.182]
</p><p>65 In our application this does not even require high numerical precision, since a coarse binning (see footnote 3) of the analog components of r(t) sufﬁces, see Fig. [sent-174, score-0.163]
</p><p>66 All estimates of M I(X, R) in 4 These kind of results depend on a division of the space of circuit states into subspaces, which is required for the calculation of the Ma-bound. [sent-176, score-0.725]
</p><p>67 In our case we have chosen the subspaces such that the frequency counts of any two circuit states in the same subspace differ by at most 1. [sent-177, score-0.656]
</p><p>68 5 The empirical Bayes classiﬁer operates as follows: given observed (and discretized) d components r (d) (t) of the state r(t) it predicts the input which was observed most frequently for the given state components r (d) (t) (maximum a posterior classiﬁcation, see e. [sent-178, score-0.339]
</p><p>69 6 In fact, in the limit N → ∞ the Bayes classiﬁer is the optimal classiﬁer for the discretized data in the sense that it would yield the lowest classiﬁcation error — and hence the highest lower bound on mutual information — over all possible classiﬁers. [sent-182, score-0.386]
</p><p>70 7 In contrast to the Bayes classiﬁer the linear classiﬁers (both for analog and discrete data) yield already for relatively small sample sizes N good results which do not improve much with increasing N. [sent-183, score-0.241]
</p><p>71 Shown are lower bounds for mutual information M I(si , h(R)) obtained with a linear classiﬁer h operating on d components of the circuit state r(t). [sent-195, score-1.165]
</p><p>72 The numbers a × d to the right of each panel specify the number of components d used by the linear classiﬁer and for how many different choices a of such subsets of size d the results are plotted in that panel. [sent-196, score-0.119]
</p><p>73 the subsequent sections are lower bounds M I(X, h(R)) computed via linear classiﬁers h. [sent-197, score-0.182]
</p><p>74 Hence a high value of a lower bound M I(X, h(R)) for such h shows not only that information about X is present in the current circuit state R, but also that this information is in principle accessible for other neurons. [sent-199, score-0.855]
</p><p>75 4 shows the temporal dynamics of information (estimated every 20ms as described in section 3) about input bits si (encoded as described in section 2) for different components of the circuit state r(t) corresponding to different randomly drawn subsets of neurons in the circuit. [sent-202, score-1.356]
</p><p>76 One sees that even subsets of just 5 neurons absorb substantial information about the input bits si , however with a rather slow onset of the information uptake at the beginning of a segment and little memory retention when this information is overwritten by the next input segment. [sent-203, score-0.843]
</p><p>77 By merging the information from different subsets of neurons the uptake of new information gets faster and the memory retention grows. [sent-204, score-0.387]
</p><p>78 Note that for large sets of neurons (160 and 800) the information about each input bit si jumps up to its maximal value right at the beginning of the corresponding ith segment of the input trains. [sent-205, score-0.653]
</p><p>79 A Dynamics of information about input bits as in the bottom row of Fig. [sent-215, score-0.184]
</p><p>80 H(s) denotes the entropy of a segment si (which is 1 bit for i = 1, 2, 3, 4). [sent-217, score-0.344]
</p><p>81 B, C, D Lower bounds for the mutual information M I(f, h(R)) for various Boolean functions f (s1 , . [sent-218, score-0.363]
</p><p>82 , s4 ) obtained with a linear classiﬁer h operating on the full 800-component circuit state R = r(t). [sent-221, score-0.668]
</p><p>83 5B,C shows that all these Boolean functions f are autonomously computed by the circuit, in the sense that the current circuit state contains high mutual information with the target output f (s1 , s2 ) of this function f . [sent-233, score-0.986]
</p><p>84 Furthermore the information about the result f (s1 , s2 ) of this computation can be extracted linearly from the current circuit state r(t) (in spite of the fact that the computation of f (s1 , s2 ) from the spike patterns in the input requires highly nonlinear computational operations). [sent-234, score-1.101]
</p><p>85 There exist 5 other Boolean functions which are nontrivial in this sense, which are just the negations of the 5 Boolean functions shown (and for which the mutual information analysis therefore yields exactly the same result). [sent-237, score-0.256]
</p><p>86 5D corresponding results are shown for parity functions that depend on three of the 4 bits s1 , s2 , s3 , s4 . [sent-239, score-0.127]
</p><p>87 These Boolean functions are the most difﬁcult ones to compute in the sense that knowledge of just 1 or 2 of their input bits does not give any advantage in guessing the output bit. [sent-240, score-0.173]
</p><p>88 One noteworthy feature in all these emergent computations is that information about the result of the computation is already present in the current circuit state long before the complete spatio-temporal input patterns that encode the relevant input bits have been received by the circuit. [sent-241, score-1.316]
</p><p>89 In fact, the computation of f (s1 , s2 ) automatically just uses the temporal order of the ﬁrst spikes in the pattern encoding s2 , and merges information contained in the order of these spikes with the “context” deﬁned by the preceding input pattern. [sent-242, score-0.286]
</p><p>90 In this way the circuit automatically completes an ultra-rapid computation within just 20 ms of the beginning of the second pattern s2 . [sent-243, score-0.781]
</p><p>91 The existence of such ultra-rapid neural computations has previously already been inferred [12] but models that could explain the possibility of such ultra-rapid computations on the basis of generic models for recurrent neural microcircuits  have been missing. [sent-244, score-0.532]
</p><p>92 6  Discussion  We have analyzed the dynamics of information in high-dimensional circuit states of a generic neural microcircuit model. [sent-245, score-1.035]
</p><p>93 This approach also has the advantage that signiﬁcant lower bounds for the information content of high-dimensional circuit states can already be achieved for relatively small sample sizes. [sent-247, score-0.992]
</p><p>94 Our results show that information about current and preceding circuit inputs is spread throughout the circuit in a rather uniform manner. [sent-248, score-1.32]
</p><p>95 Such emergent computation in circuits of spiking neurons is extremely fast, and therefore provides an interesting alternative to models based on special-purpose constructions for explaining empirically observed [12] ultra-rapid computations in neural systems. [sent-250, score-0.659]
</p><p>96 The method for analyzing information contained in high-dimensional circuit states that we have explored in this article for a generic neural microcircuit model should also be applicable to biological data from multi-unit recordings, f M RI etc. [sent-251, score-1.077]
</p><p>97 , since signiﬁcant lower bounds for mutual information were achieved in our study case already for sample sizes in the range of a few hundred (see Fig. [sent-252, score-0.669]
</p><p>98 In this way one could get insight into the dynamics of information and emergent computations in biological neural systems. [sent-254, score-0.384]
</p><p>99 Estimating the errors on measured entropy and mutual information. [sent-289, score-0.306]
</p><p>100 A practical guide to information analysis of spike trains. [sent-303, score-0.257]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('circuit', 0.591), ('iraw', 0.223), ('mutual', 0.217), ('neurons', 0.192), ('spike', 0.191), ('emergent', 0.159), ('boolean', 0.155), ('raw', 0.145), ('ms', 0.123), ('microcircuit', 0.116), ('ull', 0.112), ('generic', 0.111), ('recurrent', 0.11), ('bounds', 0.107), ('classi', 0.095), ('template', 0.091), ('segment', 0.091), ('mi', 0.089), ('entropy', 0.089), ('trains', 0.088), ('spiking', 0.087), ('sample', 0.085), ('er', 0.084), ('bit', 0.083), ('si', 0.081), ('circuits', 0.08), ('dynamics', 0.079), ('bits', 0.078), ('bf', 0.078), ('state', 0.077), ('lower', 0.075), ('computations', 0.073), ('estimates', 0.069), ('input', 0.067), ('bnaive', 0.067), ('iinf', 0.067), ('inity', 0.067), ('maass', 0.067), ('microcircuits', 0.067), ('analog', 0.065), ('preceding', 0.065), ('states', 0.065), ('sizes', 0.061), ('components', 0.059), ('bayes', 0.058), ('ring', 0.057), ('parity', 0.049), ('corrected', 0.049), ('sec', 0.049), ('contained', 0.048), ('analyzing', 0.046), ('synapses', 0.045), ('drawings', 0.045), ('inaive', 0.045), ('miraw', 0.045), ('natschl', 0.045), ('uptake', 0.045), ('templates', 0.044), ('bins', 0.042), ('neuron', 0.042), ('ers', 0.042), ('retention', 0.039), ('footnote', 0.039), ('undersampling', 0.039), ('ger', 0.039), ('optimistic', 0.039), ('information', 0.039), ('theoretic', 0.038), ('markram', 0.035), ('graz', 0.035), ('capability', 0.034), ('patterns', 0.034), ('neural', 0.034), ('current', 0.034), ('computation', 0.034), ('layer', 0.034), ('overestimate', 0.033), ('subsets', 0.033), ('noteworthy', 0.033), ('beginning', 0.033), ('segments', 0.033), ('temporal', 0.033), ('estimating', 0.032), ('consisting', 0.031), ('already', 0.03), ('underestimate', 0.029), ('hundred', 0.029), ('sense', 0.028), ('activity', 0.028), ('stream', 0.028), ('therein', 0.028), ('trained', 0.028), ('estimate', 0.028), ('substantially', 0.027), ('guide', 0.027), ('article', 0.027), ('discretized', 0.027), ('size', 0.027), ('randomly', 0.027), ('study', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="93-tfidf-1" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 1</p><p>2 0.29544115 <a title="93-tfidf-2" href="./nips-2003-A_Summating%2C_Exponentially-Decaying_CMOS_Synapse_for_Spiking_Neural_Systems.html">18 nips-2003-A Summating, Exponentially-Decaying CMOS Synapse for Spiking Neural Systems</a></p>
<p>Author: Rock Z. Shi, Timothy K. Horiuchi</p><p>Abstract: Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modiﬁcation. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5µm CMOS process. 1</p><p>3 0.19034266 <a title="93-tfidf-3" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>Author: Adria Bofill-i-petit, Alan F. Murray</p><p>Abstract: We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with diﬀerent types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection. 1</p><p>4 0.18889233 <a title="93-tfidf-4" href="./nips-2003-A_Low-Power_Analog_VLSI_Visual_Collision_Detector.html">10 nips-2003-A Low-Power Analog VLSI Visual Collision Detector</a></p>
<p>Author: Reid R. Harrison</p><p>Abstract: We have designed and tested a single-chip analog VLSI sensor that detects imminent collisions by measuring radially expansive optic flow. The design of the chip is based on a model proposed to explain leg-extension behavior in flies during landing approaches. A new elementary motion detector (EMD) circuit was developed to measure optic flow. This EMD circuit models the bandpass nature of large monopolar cells (LMCs) immediately postsynaptic to photoreceptors in the fly visual system. A 16 × 16 array of 2-D motion detectors was fabricated on a 2.24 mm × 2.24 mm die in a standard 0.5-µm CMOS process. The chip consumes 140 µW of power from a 5 V supply. With the addition of wide-angle optics, the sensor is able to detect collisions around 500 ms before impact in complex, real-world scenes. 1</p><p>5 0.17584389 <a title="93-tfidf-5" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>Author: Hsin Chen, Patrice Fleury, Alan F. Murray</p><p>Abstract: This paper presents VLSI circuits with continuous-valued probabilistic behaviour realized by injecting noise into each computing unit(neuron). Interconnecting the noisy neurons forms a Continuous Restricted Boltzmann Machine (CRBM), which has shown promising performance in modelling and classifying noisy biomedical data. The Minimising-Contrastive-Divergence learning algorithm for CRBM is also implemented in mixed-mode VLSI, to adapt the noisy neurons’ parameters on-chip. 1</p><p>6 0.17046471 <a title="93-tfidf-6" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>7 0.14862221 <a title="93-tfidf-7" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>8 0.14126819 <a title="93-tfidf-8" href="./nips-2003-A_Mixed-Signal_VLSI_for_Real-Time_Generation_of_Edge-Based_Image_Vectors.html">11 nips-2003-A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors</a></p>
<p>9 0.12881765 <a title="93-tfidf-9" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>10 0.1236252 <a title="93-tfidf-10" href="./nips-2003-Circuit_Optimization_Predicts_Dynamic_Networks_for_Chemosensory_Orientation_in_Nematode_C._elegans.html">45 nips-2003-Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans</a></p>
<p>11 0.11240527 <a title="93-tfidf-11" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>12 0.098823003 <a title="93-tfidf-12" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<p>13 0.096144356 <a title="93-tfidf-13" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>14 0.086878359 <a title="93-tfidf-14" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>15 0.08535289 <a title="93-tfidf-15" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>16 0.077452436 <a title="93-tfidf-16" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>17 0.070958443 <a title="93-tfidf-17" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>18 0.064305365 <a title="93-tfidf-18" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>19 0.064294793 <a title="93-tfidf-19" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>20 0.063481413 <a title="93-tfidf-20" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.218), (1, 0.083), (2, 0.374), (3, 0.049), (4, 0.158), (5, -0.042), (6, -0.169), (7, -0.019), (8, 0.024), (9, -0.012), (10, -0.098), (11, -0.127), (12, -0.104), (13, -0.039), (14, 0.03), (15, -0.058), (16, -0.037), (17, 0.023), (18, 0.032), (19, -0.095), (20, 0.075), (21, 0.026), (22, 0.131), (23, -0.102), (24, 0.07), (25, -0.091), (26, 0.008), (27, 0.011), (28, 0.086), (29, 0.023), (30, -0.009), (31, 0.067), (32, -0.018), (33, 0.079), (34, 0.017), (35, -0.057), (36, 0.007), (37, -0.019), (38, 0.125), (39, -0.092), (40, -0.019), (41, 0.023), (42, -0.032), (43, 0.067), (44, -0.094), (45, 0.064), (46, 0.04), (47, -0.024), (48, -0.005), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95716238 <a title="93-lsi-1" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 1</p><p>2 0.86601561 <a title="93-lsi-2" href="./nips-2003-A_Summating%2C_Exponentially-Decaying_CMOS_Synapse_for_Spiking_Neural_Systems.html">18 nips-2003-A Summating, Exponentially-Decaying CMOS Synapse for Spiking Neural Systems</a></p>
<p>Author: Rock Z. Shi, Timothy K. Horiuchi</p><p>Abstract: Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modiﬁcation. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5µm CMOS process. 1</p><p>3 0.76841098 <a title="93-lsi-3" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>Author: Hsin Chen, Patrice Fleury, Alan F. Murray</p><p>Abstract: This paper presents VLSI circuits with continuous-valued probabilistic behaviour realized by injecting noise into each computing unit(neuron). Interconnecting the noisy neurons forms a Continuous Restricted Boltzmann Machine (CRBM), which has shown promising performance in modelling and classifying noisy biomedical data. The Minimising-Contrastive-Divergence learning algorithm for CRBM is also implemented in mixed-mode VLSI, to adapt the noisy neurons’ parameters on-chip. 1</p><p>4 0.68466073 <a title="93-lsi-4" href="./nips-2003-A_Low-Power_Analog_VLSI_Visual_Collision_Detector.html">10 nips-2003-A Low-Power Analog VLSI Visual Collision Detector</a></p>
<p>Author: Reid R. Harrison</p><p>Abstract: We have designed and tested a single-chip analog VLSI sensor that detects imminent collisions by measuring radially expansive optic flow. The design of the chip is based on a model proposed to explain leg-extension behavior in flies during landing approaches. A new elementary motion detector (EMD) circuit was developed to measure optic flow. This EMD circuit models the bandpass nature of large monopolar cells (LMCs) immediately postsynaptic to photoreceptors in the fly visual system. A 16 × 16 array of 2-D motion detectors was fabricated on a 2.24 mm × 2.24 mm die in a standard 0.5-µm CMOS process. The chip consumes 140 µW of power from a 5 V supply. With the addition of wide-angle optics, the sensor is able to detect collisions around 500 ms before impact in complex, real-world scenes. 1</p><p>5 0.59869474 <a title="93-lsi-5" href="./nips-2003-A_Mixed-Signal_VLSI_for_Real-Time_Generation_of_Edge-Based_Image_Vectors.html">11 nips-2003-A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors</a></p>
<p>Author: Masakazu Yagi, Hideo Yamasaki, Tadashi Shibata</p><p>Abstract: A mixed-signal image filtering VLSI has been developed aiming at real-time generation of edge-based image vectors for robust image recognition. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection, the key processing parameter in vector generation. As a result, a fully seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-µm double-polysilicon three-metal-layer CMOS technology and the concept was verified by the fabricated chip. The chip generates a 64-dimension feature vector from a 64x64-pixel gray scale image every 80µsec. This is about 104 times faster than the software computation, making a real-time image recognition system feasible. 1 In tro du c ti o n The development of human-like image recognition systems is a key issue in information technology. However, a number of algorithms developed for robust image recognition so far [1]-[3] are mostly implemented as software systems running on general-purpose computers. Since the algorithms are generally complex and include a lot of floating point operations, they are computationally too expensive to build real-time systems. Development of hardware-friendly algorithms and their direct VLSI implementation would be a promising solution for real-time response systems. Being inspired by the biological principle that edge information is firstly detected in the visual cortex, we have developed an edge-based image representation algorithm compatible to hardware processing. In this algorithm, multiple-direction edges extracted from an original gray scale image is utilized to form a feature vector. Since the spatial distribution of principal edges is represented by a vector, it was named Projected Principal-Edge Distribution (PPED) [4],[5], or formerly called Principal Axis Projection (PAP) [6],[7]. (The algorithm is explained later.) Since the PPED vectors very well represent the human perception of similarity among images, robust image recognition systems have been developed using PPED vectors in conjunction with the analog soft pattern classifier [4],[8], the digital VQ (Vector Quantization) processor [9], and support vector machines [10] . The robust nature of PPED representation is demonstrated in Fig. 1, where the system was applied to cephalometric landmark identification (identifying specific anatomical landmarks on medical radiographs) as an example, one of the most important clinical practices of expert dentists in orthodontics [6],[7]. Typical X-ray images to be experienced by apprentice doctors were converted to PPED vectors and utilized as templates for vector matching. The system performance has been proven for 250 head film samples regarding the fundamental 26 landmarks [11]. Important to note is the successful detection of the landmark on the soft tissue boundary (the tip of the lower lip) shown in Fig. 1(c). Landmarks on soft tissues are very difficult to detect as compared to landmarks on hard tissues (solid bones) because only faint images are captured on radiographs. The successful detection is due to the median algorithm that determines the threshold value for edge detection. Sella Nasion Orbitale By our system (a) By expert dentists Landmark on soft tissue (b) (c) Fig. 1: Image recognition using PPED vectors: (a,b) cephalometric landmark identification; (c) successful landmark detection on soft tissue. We have adopted the median value of spatial variance of luminance within the filtering kernel (5x5 pixels), which allows us to extract all essential features in a delicate gray scale image. However, the problem is the high computational cost in determining the median value. It takes about 0.6 sec to generate one PPED vector from a 64x64-pixel image (a standard image size for recognition in our system) on a SUN workstation, making real time processing unrealistic. About 90% of the computation time is for edge detection from an input image, in which most of the time is spent for median detection. Then the purpose of this work is to develop a new architecture median-filter VLSI subsystem for real-time PPED-vector generation. Special attention has been paid to realize a fully seamless pipeline processing from threshold detection to edge feature map generation by employing the four-stage asynchronous median detection architecture. 2 P r o je c t e d P r i n c i pa l E dg e Dis tribution (PPED ) Projected Principal Edge Distribution (PPED) algorithm [5],[6] is briefly explained using Fig. 2(a). A 5x5-pixel block taken from a 64x64-pixel target image is subjected to edge detection filtering in four principal directions, i.e. horizontal, vertical, and ±45-degree directions. In the figure, horizontal edge filtering is shown as an example. (The filtering kernels used for edge detection are given in Fig. 2(b).) In order to determine the threshold value for edge detection, all the absolute-value differences between two neighboring pixels are calculated in both vertical and horizontal directions and the median value is taken as the threshold. By scanning the 5x5-pixel filtering kernels in the target image, four 64x64 edge-flag maps are generated, which are called feature maps. In the horizontal feature map, for example, edge flags in every four rows are accumulated and spatial distribution of edge flags are represented by a histogram having 16 elements. Similar procedures are applied to other three directions to form respective histograms each having 16 elements. Finally, a 64-dimension vector is formed by series-connecting the four histograms in the order of horizontal, +45-degree, vertical, and –45-degree. Edge Detection 64x64 Feature Map (64x64) (Horizontal) 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 -1-1-1-1-1 0 0 0 0 0 (Horizontal) Threshold || Median Scan (16 elements) Edge Filter PPED Vector (Horizontal Section) 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 -1 -1 -1 -1 -1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 -1 0 1 0 -1 0 1 0 -1 -1 0 0 -1 0 0 0 Horizontal +45-degree 0 0 0 0 0 Threshold Detection Absolute value difference between neiboring pels. 1 1 1 1 1 0 -1 0 -1 0 -1 0 -1 0 -1 0 0 0 0 0 0 -1 0 0 0 1 0 -1 -1 0 0 1 0 -1 0 0 1 1 0 -1 0 0 0 1 0 Vertical (a) -45-degree (b) Fig. 2: PPED algorithm (a) and filtering kernels for edge detection (b). 3 Sy stem Orga ni za ti o n The system organization of the feature map generation VLSI is illustrated in Fig. 3. The system receives one column of data (8-b x 5 pixels) at each clock and stores the data in the last column of the 5x6 image buffer. The image buffer shifts all the stored data to the right at every clock. Before the edge filtering circuit (EFC) starts detecting four direction edges with respect to the center pixel in the 5x5 block, the threshold value calculated from all the pixel data in the 5x5 block must be ready in time for the processing. In order to keep the coherence of the threshold detection and the edge filtering processing, the two last-in data locating at column 5 and 6 are given to median filter circuit (MFC) in advance via absolute value circuit (AVC). AVC calculates all luminance differences between two neighboring pixels in columns 5 and 6. In this manner, a fully seamless pipeline processing from threshold detection to edge feature map generation has been established. The key requirement here is that MFC must determine the median value of the 40 luminance difference data from the 5x5-pixel block fast enough to carry out the seamless pipeline processing. For this purpose, a four-stage asynchronous median detection architecture has been developed which is explained in the following. Edge Filtering Circuit (EFC) 6 5 4 3 2 1 Edge flags H +45 V Image buffer 8-b x 5 pixels (One column) Absolute Value Circuit (AVC) Threshold value Median Filter Circuit (MFC) -45 Feature maps Fig. 3: System organization of feature map generation VLSI. The well-known binary search algorithm was adopted for fast execution of median detection. The median search processing for five 4-b data is illustrated in Fig. 4 for the purpose of explanation. In the beginning, majority voting is carried out for the MSB’s of all data. Namely, the number of 1’s is compared with the number of 0’s and the majority group wins. The majority group flag (“0” in this example) is stored as the MSB of the median value. In addition, the loser group is withdrawn in the following voting by changing all remaining bits to the loser MSB (“1” in this example). By repeating the processing, the median value is finally stored in the median value register. Elapse of time Median Register : 0 1 X X 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 MVC0 MVC1 MVC2 MVC3 MVC0 MVC1 MVC2 MVC3 MVC0 MVC1 MVC2 MVC3 MVC0 MVC1 MVC2 MVC3 Majority Flag : 0 0 X X X Majority Voting Circuit (MVC) Fig. 4: Hardware algorithm for median detection by binary search. How the median value is detected from all the 40 8-b data (20 horizontal luminance difference data and 20 vertical luminance difference data) is illustrated in Fig. 5. All the data are stored in the array of median detection units (MDU’s). At each clock, the array receives four vertical luminance difference data and five horizontal luminance difference data calculated from the data in column 5 and 6 in Fig. 3. The entire data are shifted downward at each clock. The median search is carried out for the upper four bits and the lower four bits separately in order to enhance the throughput by pipelining. For this purpose, the chip is equipped with eight majority voting circuits (MVC 0~7). The upper four bits from all the data are processed by MVC 4~7 in a single clock cycle to yield the median value. In the next clock cycle, the loser information is transferred to the lower four bits within each MDU and MVC0~3 carry out the median search for the lower four bits from all the data in the array. Vertical Luminance Difference AVC AVC AVC AVC Horizontal Luminance Difference AVC AVC AVC AVC AVC Shift Shift Median Detection Unit (MDU) x (40 Units) Lower 4bit Upper 4bit MVC0 MVC2 MVC1 MVC3 MVC4 MVC5 MVC6 MVC7 MVCs for upper 4bit MVCs for lower 4bit Fig. 5: Median detection architecture for all 40 luminance difference data. The majority voting circuit (MVC) is shown in Fig. 6. Output connected CMOS inverters are employed as preamplifiers for majority detection which was first proposed in Ref. [12]. In the present implementation, however, two preamps receiving input data and inverted input data are connected to a 2-stage differential amplifier. Although this doubles the area penalty, the instability in the threshold for majority detection due to process and temperature variations has been remarkably improved as compared to the single inverter thresholding in Ref. [12]. The MVC in Fig. 6 has 41 input terminals although 40 bits of data are inputted to the circuit at one time. Bit “0” is always given to the terminal IN40 to yield “0” as the majority when there is a tie in the majority voting. PREAMP IN0 PREAMP 2W/L IN0 2W/L OUT W/L ENBL W/L W/L IN1 IN1 2W/L 2W/L W/L ENBL IN40 W/L W/L IN40 Fig. 6: Majority voting circuit (MVC). The edge filtering circuit (EFC) in Fig. 3 is composed as a four-stage pipeline of regular CMOS digital logic. In the first two stages, four-direction edge gradients are computed, and in the succeeding two stages, the detection of the largest gradient and the thresholding is carried out to generate four edge flags. 4 E x p e r i m e n t a l R es u l t s The feature map generation VLSI was fabricated in a 0.35-µm double-poly three-metal-layer CMOS technology. A photomicrograph of the proof-of-concept chip is shown in Fig. 7. The measured waveforms of the MVC at operating frequencies of 10MHz and 90MHz are demonstrated in Fig. 8. The input condition is in the worst case. Namely, 21 “1” bits and 20 “0” bits were fed to the inputs. The observed computation time is about 12 nsec which is larger than the simulation result of 2.5 nsec. This was caused by the capacitance loading due to the probing of the test circuit. In the real circuit without external probing, we confirmed the average computation time of 4~5 nsec. Edge-detection Filtering Circuit Processing Technology 0.35µm CMOS 2-Poly 3-Metal Median Filter Control Unit Chip Size 4.5mm x 4.5mm MVC Majority Voting Circuit X8 Supply Voltage 3.3 V Operation Frequengy 50MHz Vector Generator Fig. 7: Photomicrograph and specification of the fabricated proof-of-concept chip. 1V/div 5ns/div MVC_Output 1V/div 8ns/div MVC_OUT IN IN 1 Majority Voting operation (a) Majority Voting operation (b) Fig. 8: Measured waveforms of majority voting circuit (MVC) at operation frequencies of 10MHz (a) and 90 MHz (b) for the worst-case input data. The feature maps generated by the chip at the operation frequency of 25 MHz are demonstrated in Fig. 9. The power dissipation was 224 mW. The difference between the flag bits detected by the chip and those obtained by computer simulation are also shown in the figure. The number of error flags was from 80 to 120 out of 16,384 flags, only a 0.6% of the total. The occurrence of such error bits is anticipated since we employed analog circuits for median detection. However, such error does not cause any serious problems in the PPED algorithm as demonstrated in Figs. 10 and 11. The template matching results with the top five PPED vector candidates in Sella identification are demonstrated in Fig. 11, where Manhattan distance was adopted as the dissimilarity measure. The error in the feature map generation processing yields a constant bias to the dissimilarity and does not affect the result of the maximum likelihood search. Generated Feature maps Difference as compared to computer simulation Sella Horizontal Plus 45-degrees Vertical Minus 45-degrees Fig. 9: Feature maps for Sella pattern generated by the chip. Generated PPED vector by the chip Sella Difference as compared to computer simulation Dissimilarity (by Manhattan Distance) Fig. 10: PPED vector for Sella pattern generated by the chip. The difference in the vector components between the PPED vector generated by the chip and that obtained by computer simulation is also shown. 1200 Measured Data 1000 800 Computer Simulation 600 400 200 0 1st (Correct) 2nd 3rd 4th 5th Candidates in Sella recognition Fig. 11: Comparison of template matching results. 5 Conclusion A mixed-signal median filter VLSI circuit for PPED vector generation is presented. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced. As a result, a fully seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-µm CMOS technology and the fab- ricated chip generates an edge based image vector every 80 µsec, which is about 10 4 times faster than the software computation. Acknowledgments The VLSI chip in this study was fabricated in the chip fabrication program of VLSI Design and Education Center (VDEC), the University of Tokyo with the collaboration by Rohm Corporation and Toppan Printing Corporation. The work is partially supported by the Ministry of Education, Science, Sports, and Culture under Grant-in-Aid for Scientific Research (No. 14205043) and by JST in the program of CREST. References [1] C. Liu and Harry Wechsler, “Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition”, IEEE Transactions on Image Processing, Vol. 11, No.4, Apr. 2002. [2] C. Yen-ting, C. Kuo-sheng, and L. Ja-kuang, “Improving cephalogram analysis through feature subimage extraction”, IEEE Engineering in Medicine and Biology Magazine, Vol. 18, No. 1, 1999, pp. 25-31. [3] H. Potlapalli and R. C. Luo, “Fractal-based classification of natural textures”, IEEE Transactions on Industrial Electronics, Vol. 45, No. 1, Feb. 1998. [4] T. Yamasaki and T. Shibata, “Analog Soft-Pattern-Matching Classifier Using Floating-Gate MOS Technology,” Advances in Neural Information Processing Systems 14, Vol. II, pp. 1131-1138. [5] Masakazu Yagi, Tadashi Shibata, “An Image Representation Algorithm Compatible to Neural-Associative-Processor-Based Hardware Recognition Systems,” IEEE Trans. Neural Networks, Vol. 14, No. 5, pp. 1144-1161, September (2003). [6] M. Yagi, M. Adachi, and T. Shibata,</p><p>6 0.59597254 <a title="93-lsi-6" href="./nips-2003-Synchrony_Detection_by_Analogue_VLSI_Neurons_with_Bimodal_STDP_Synapses.html">183 nips-2003-Synchrony Detection by Analogue VLSI Neurons with Bimodal STDP Synapses</a></p>
<p>7 0.58080822 <a title="93-lsi-7" href="./nips-2003-Entrainment_of_Silicon_Central_Pattern_Generators_for_Legged_Locomotory_Control.html">61 nips-2003-Entrainment of Silicon Central Pattern Generators for Legged Locomotory Control</a></p>
<p>8 0.52168667 <a title="93-lsi-8" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>9 0.41368037 <a title="93-lsi-9" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>10 0.40473253 <a title="93-lsi-10" href="./nips-2003-Circuit_Optimization_Predicts_Dynamic_Networks_for_Chemosensory_Orientation_in_Nematode_C._elegans.html">45 nips-2003-Circuit Optimization Predicts Dynamic Networks for Chemosensory Orientation in Nematode C. elegans</a></p>
<p>11 0.363372 <a title="93-lsi-11" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>12 0.34086215 <a title="93-lsi-12" href="./nips-2003-Mechanism_of_Neural_Interference_by_Transcranial_Magnetic_Stimulation%3A_Network_or_Single_Neuron%3F.html">127 nips-2003-Mechanism of Neural Interference by Transcranial Magnetic Stimulation: Network or Single Neuron?</a></p>
<p>13 0.33704939 <a title="93-lsi-13" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<p>14 0.29134393 <a title="93-lsi-14" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>15 0.2756916 <a title="93-lsi-15" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>16 0.27472311 <a title="93-lsi-16" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>17 0.27118877 <a title="93-lsi-17" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>18 0.26522768 <a title="93-lsi-18" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>19 0.2646189 <a title="93-lsi-19" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>20 0.26432523 <a title="93-lsi-20" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (11, 0.035), (29, 0.022), (30, 0.014), (35, 0.058), (53, 0.133), (59, 0.015), (63, 0.017), (65, 0.011), (66, 0.012), (69, 0.012), (71, 0.078), (76, 0.064), (82, 0.011), (85, 0.082), (90, 0.198), (91, 0.092), (99, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86248505 <a title="93-lda-1" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>Author: Darya Chudova, Christopher Hart, Eric Mjolsness, Padhraic Smyth</p><p>Abstract: We propose a functional mixture model for simultaneous clustering and alignment of sets of curves measured on a discrete time grid. The model is speciﬁcally tailored to gene expression time course data. Each functional cluster center is a nonlinear combination of solutions of a simple linear differential equation that describes the change of individual mRNA levels when the synthesis and decay rates are constant. The mixture of continuous time parametric functional forms allows one to (a) account for the heterogeneity in the observed proﬁles, (b) align the proﬁles in time by estimating real-valued time shifts, (c) capture the synthesis and decay of mRNA in the course of an experiment, and (d) regularize noisy proﬁles by enforcing smoothness in the mean curves. We derive an EM algorithm for estimating the parameters of the model, and apply the proposed approach to the set of cycling genes in yeast. The experiments show consistent improvement in predictive power and within cluster variance compared to regular Gaussian mixtures. 1</p><p>same-paper 2 0.85043621 <a title="93-lda-2" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>Author: Thomas Natschläger, Wolfgang Maass</p><p>Abstract: We employ an efﬁcient method using Bayesian and linear classiﬁers for analyzing the dynamics of information in high-dimensional states of generic cortical microcircuit models. It is shown that such recurrent circuits of spiking neurons have an inherent capability to carry out rapid computations on complex spike patterns, merging information contained in the order of spike arrival with previously acquired context information. 1</p><p>3 0.72421414 <a title="93-lda-3" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>4 0.72098404 <a title="93-lda-4" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>Author: Sanjiv Kumar, Martial Hebert</p><p>Abstract: In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classiﬁcation of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classiﬁcation problems using the graph min-cut algorithms. The performance of the model was veriﬁed on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments. 1</p><p>5 0.71928865 <a title="93-lda-5" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>Author: Pedro J. Moreno, Purdy P. Ho, Nuno Vasconcelos</p><p>Abstract: Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identiﬁcation/veriﬁcation and image classiﬁcation tasks and show that these new kernels have the best performance in speaker veriﬁcation and mostly outperform the Fisher kernel based SVM’s and the generative classiﬁers in speaker identiﬁcation and image classiﬁcation. 1</p><p>6 0.71927422 <a title="93-lda-6" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>7 0.71911418 <a title="93-lda-7" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>8 0.71904963 <a title="93-lda-8" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>9 0.7174961 <a title="93-lda-9" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>10 0.71607596 <a title="93-lda-10" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>11 0.71578181 <a title="93-lda-11" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>12 0.71458918 <a title="93-lda-12" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>13 0.71426809 <a title="93-lda-13" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>14 0.71384525 <a title="93-lda-14" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>15 0.71276671 <a title="93-lda-15" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>16 0.71213132 <a title="93-lda-16" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>17 0.71176738 <a title="93-lda-17" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>18 0.70947987 <a title="93-lda-18" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>19 0.70876998 <a title="93-lda-19" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>20 0.70814937 <a title="93-lda-20" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
