<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2003-Warped Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-194" href="#">nips2003-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 nips-2003-Warped Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2003-194-pdf" href="http://papers.nips.cc/paper/2481-warped-gaussian-processes.pdf">pdf</a></p><p>Author: Edward Snelson, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. 1</p><p>Reference: <a title="nips-2003-194-reference" href="../nips2003_reference/nips-2003-Warped_Gaussian_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. [sent-6, score-0.201]
</p><p>2 The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. [sent-8, score-0.128]
</p><p>3 This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. [sent-9, score-0.13]
</p><p>4 We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. [sent-10, score-0.195]
</p><p>5 Once this is done, GPs can be used as the basis for nonlinear nonparametric regression and classiﬁcation, showing excellent performance on a wide variety of datasets [1, 2, 3]. [sent-12, score-0.173]
</p><p>6 This simplicity enables predictions to be made easily using matrix manipulations, and of course the predictive distributions are Gaussian also. [sent-15, score-0.177]
</p><p>7 Often it is unreasonable to assume that, in the form the data is obtained, the noise will be Gaussian, and the data well modelled as a GP. [sent-16, score-0.109]
</p><p>8 Then modelling proceeds assuming that this transformed data has Gaussian noise and will be better modelled by the GP. [sent-19, score-0.138]
</p><p>9 The log is just one particular transformation that could be done; there is a con-  tinuum of transformations that could be applied to the observation space to bring the data into a form well modelled by a GP. [sent-20, score-0.317]
</p><p>10 Making such a transformation should really be a full part of the probabilistic modelling; it seems strange to ﬁrst make an ad-hoc transformation, and then use a principled Bayesian probabilistic model. [sent-21, score-0.101]
</p><p>11 In this paper we show how such a transformation or ‘warping’ of the observation space can be made entirely automatically, fully encompassed into the probabilistic framework of the GP. [sent-22, score-0.174]
</p><p>12 The warped GP makes a transformation from a latent space to the observation, such that the data is best modelled by a GP in the latent space. [sent-23, score-0.731]
</p><p>13 It can also be viewed as a generalisation of the GP, since in observation space it is a non-Gaussian process, with nonGaussian and asymmetric noise in general. [sent-24, score-0.134]
</p><p>14 For an excellent review of Gaussian processes for regression and classiﬁcation see [4]. [sent-26, score-0.109]
</p><p>15 We show in sections 4 and 5, with both toy and real data, that the warped GP can signiﬁcantly improve predictive performance over a variety of measures, especially with regard to the whole predictive distribution, rather than just a single point prediction such as the mean or median. [sent-28, score-0.679]
</p><p>16 2  Nonlinear regression with Gaussian processes  Suppose we are given a dataset D, consisting of N pairs of input vectors XN ≡ {x(n) }N n=1 and real-valued targets tN ≡ {tn }N . [sent-30, score-0.18]
</p><p>17 The covariance between the function value of y at two points x and x is modelled with a covariance function C(x, x ), which is usually assumed to have some simple parametric form. [sent-34, score-0.251]
</p><p>18 Often the noise model is taken to be input-independent, and the covariance function is taken to be a Gaussian function of the difference in the input vectors (a stationary covariance function), although many other possibilities exist, see e. [sent-36, score-0.215]
</p><p>19 In this paper we consider only this popular choice, in which case the entries in the covariance matrix are given by   D (m) (n) 2 xd − x d 1  + v0 δmn . [sent-39, score-0.106]
</p><p>20 Cmn = v1 exp − (2) 2 rd d=1  Here rd is a width parameter expressing the scale over which typical functions vary in the dth dimension, v1 is a size parameter expressing the typical size of the overall process in y-space, v0 is the noise variance of the observations, and Θ = {v0 , v1 , r1 , . [sent-40, score-0.164]
</p><p>21 It is simple to show that the predictive distribution for a new point given the observed data, P (tN +1 |tN , XN +1 ), is Gaussian. [sent-44, score-0.109]
</p><p>22 The calculation of the mean and variance of this  distribution involves doing a matrix inversion of the covariance matrix CN of the training inputs, which using standard exact methods incurs a computational cost of order N 3 . [sent-45, score-0.146]
</p><p>23 Learning, or ‘training’, in a GP is usually achieved by ﬁnding a local maximum in the likelihood using conjugate gradient methods with respect to the hyperparameters Θ of the covariance matrix. [sent-46, score-0.148]
</p><p>24 The negative log likelihood is given by 1 1 N log 2π . [sent-47, score-0.169]
</p><p>25 (3) L = − log P (tN |XN , Θ) = log det CN + tN C−1 tN + N 2 2 2 Once again, the evaluation of L, and its gradients with respect to Θ, involve computing the inverse covariance matrix, incurring an order N 3 cost. [sent-48, score-0.285]
</p><p>26 3  Warping the observation space  In this section we present a method of warping the observation space through a nonlinear monotonic function to a latent space, whilst retaining the full probabilistic framework to enable learning and prediction to take place consistently. [sent-50, score-0.618]
</p><p>27 Let us consider a vector of latent targets zN and suppose that this vector is modelled by a GP, 1 N 1 − log P (zN |XN , Θ) = log det CN + zN C−1 zN + log 2π . [sent-51, score-0.385]
</p><p>28 (4) N 2 2 2 Now we make a transformation from the true observation space to the latent space by mapping each observation through the same monotonic function f , zn = f (tn ; Ψ)  ∀n ,  (5)  where Ψ parameterises the transformation. [sent-52, score-0.427]
</p><p>29 We require f to be monotonic and mapping on to the whole of the real line; otherwise probability measure will not be conserved in the transformation, and we will not induce a valid distribution over the targets tN . [sent-53, score-0.141]
</p><p>30 Including the Jacobian term that takes the transformation into account, the negative log likelihood, − log P (tN |XN , Θ, Ψ), now becomes: N  L= 3. [sent-54, score-0.231]
</p><p>31 1  1 ∂f (t) 1 log log det CN + f (tN ) C−1 f (tN ) − N 2 2 ∂t n=1  + tn  N log 2π . [sent-55, score-0.452]
</p><p>32 2  (6)  Training the warped GP  Learning in this extended model is achieved by simply taking derivatives of the negative log likelihood function (6) with respect to both Θ and Ψ parameter vectors, and using a conjugate gradient method to compute ML parameter values. [sent-56, score-0.587]
</p><p>33 In this way the form of both the covariance matrix and the nonlinear transformation are learnt simultaneously under the same probabilistic framework. [sent-57, score-0.295]
</p><p>34 Since the computational limiter to a GP is inverting the covariance matrix, adding a few extra parameters into the likelihood is not really costing us anything. [sent-58, score-0.163]
</p><p>35 ˆ  (7)  To ﬁnd the distribution in the observation space we pass that Gaussian through the nonlinear warping function, giving P (tN +1 |x(N +1) , D, Θ, Ψ) =  f (tN +1 ) 2 2πσN +1  exp −  1 2  f (tN +1 ) − zN +1 ˆ σN +1  2  . [sent-63, score-0.464]
</p><p>36 (8)  The shape of this distribution depends on the form of the warping function f , but in general it may be asymmetric and multimodal. [sent-64, score-0.37]
</p><p>37 If our loss function is absolute error, then the median of the distribution should be predicted, whereas if our loss function is squared error, then it is the mean of the distribution. [sent-66, score-0.143]
</p><p>38 For a standard GP where the predictive distribution is Gaussian, the median and mean lie at the same point. [sent-67, score-0.21]
</p><p>39 For the warped GP in general they are at different points. [sent-68, score-0.435]
</p><p>40 The median is particularly easy to calculate: tmed = f −1 (ˆN +1 ) . [sent-69, score-0.112]
</p><p>41 z N +1  (9)  Notice we need to compute the inverse warping function. [sent-70, score-0.36]
</p><p>42 For example we may want to know the positions of ‘2σ’ either side of the median so that we can say that approximately 95% of the density lies between these bounds. [sent-74, score-0.115]
</p><p>43 These points in observation space are calculated in exactly the same way as the median - simply pass the values through the inverse function: tmed±2σ = f −1 (ˆN +1 ± 2σN +1 ) . [sent-75, score-0.195]
</p><p>44 Rewriting this integral back in latent space we get 2 dzf −1 (z)Nz (ˆN +1 , σN +1 ) = E(f −1 ) . [sent-77, score-0.105]
</p><p>45 3  Choosing a monotonic warping function  We wish to design a warping function that will allow for complex transformations, but we must constrain the function to be monotonic. [sent-80, score-0.706]
</p><p>46 There are various ways to do this, an obvious one being a neural-net style sum of tanh functions, I  f (t; Ψ) =  ai tanh (bi (t + ci ))  ai , bi ≥ 0 ∀i ,  (12)  i=1  where Ψ = {a, b, c}. [sent-81, score-0.202]
</p><p>47 The dotted lines show the true generating distribution, the dashed lines show a GP’s predictions, and the solid lines show the warped GP’s predictions. [sent-92, score-0.522]
</p><p>48 (a) The triplets of lines represent the median, and 2σ percentiles in each case. [sent-93, score-0.104]
</p><p>49 The derivatives of this function with respect to either t, or the warping parameters Ψ, are easy to compute. [sent-97, score-0.358]
</p><p>50 As explained earlier, this will not lead to a proper density in t space, because the density in z space is Gaussian, which covers the whole of the real line. [sent-100, score-0.139]
</p><p>51 We can ﬁx this up by using instead: I  ai tanh (bi (t + ci ))  f (t; Ψ) = t +  a i , bi ≥ 0  ∀i . [sent-101, score-0.106]
</p><p>52 In doing so, we have restricted ourselves to only making warping functions with f ≥ 1, but because the size of the covariance function v1 is free to vary, the effective gradient can be made arbitrarily small by simply making the range of the data in the latent space arbitrarily big. [sent-103, score-0.571]
</p><p>53 A more ﬂexible system of linear trends may be made by including, in addition to the neural1 net style function (12), some functions of the form β log eβm1 (t−d) + eβm2 (t−d) , where m1 , m2 ≥ 0. [sent-104, score-0.151]
</p><p>54 4  A simple 1D regression task  A simple 1D regression task was created to show a situation where the warped GP should, and does, perform signiﬁcantly better than the standard GP. [sent-107, score-0.581]
</p><p>55 101 points, regularly spaced from −π to π on the x axis, were generated with Gaussian noise about a sine function. [sent-108, score-0.097]
</p><p>56 These points were then warped through the function t = z 1/3 , to arrive at the dataset t which is shown as the dots in Figure 1(a). [sent-109, score-0.488]
</p><p>57 (a) sine  z  (c) abalone  (b) creep  z  z  t  t  (d) ailerons  z  t  t  Figure 2: Warping functions learnt for the four regression tasks carried out in this paper. [sent-110, score-0.553]
</p><p>58 Each plot is made over the range of the observation data, from tmin to tmax . [sent-111, score-0.182]
</p><p>59 A GP and a warped GP were trained independently on this dataset using a conjugate gradient minimisation procedure and randomly initialised parameters, to obtain maximum likelihood parameters. [sent-112, score-0.511]
</p><p>60 For the warped GP, the warping function (13) was used with just two tanh functions. [sent-113, score-0.842]
</p><p>61 For both models the covariance matrix (2) was used. [sent-114, score-0.106]
</p><p>62 Hybrid Monte Carlo was also implemented to integrate over all the parameters, or just the warping parameters (much faster since no matrix inversion is required with each step), but with this dataset (and the real datasets of section 5) no signiﬁcant differences were found from ML. [sent-115, score-0.46]
</p><p>63 Predictions from the GP and warped GP were made, using the ML parameters, for 401 points regularly spaced over the range of x. [sent-116, score-0.498]
</p><p>64 The predictions made were the median and 2σ percentiles in each case, and these are plotted as triplets of lines on Figure 1(a). [sent-117, score-0.247]
</p><p>65 The predictions from the warped GP are found to be much closer to the true generating distribution than the standard GP, especially with regard to the 2σ lines. [sent-118, score-0.494]
</p><p>66 The mean line was also computed, and found to lie close, but slightly skewed, from the median line. [sent-119, score-0.101]
</p><p>67 Figure 1(b) emphasises the point that the warped GP ﬁnds the shape of the whole predictive distribution much better, not just the median or mean. [sent-120, score-0.649]
</p><p>68 In this plot, one particular point on the x axis is chosen, x = −π/4, and the predictive densities from the GP and warped GP are plotted alongside the true density (which can be written down analytically). [sent-121, score-0.584]
</p><p>69 Figure 2(a) shows the warping function learnt for this regression task. [sent-123, score-0.467]
</p><p>70 The tanh functions have adjusted themselves so that they mimic a t3 nonlinearity over the range of the observation space, thus inverting the z 1/3 transformation imposed when generating the data. [sent-124, score-0.265]
</p><p>71 5  Results for some real datasets  It is not surprising that the method works well on the toy dataset of section 4 since it was generated from a known nonlinear warping of a smooth function with Gaussian noise. [sent-125, score-0.501]
</p><p>72 To demonstrate that nonlinear transformations also help on real data sets we have run the warped GP comparing its predictions to an ordinary GP on three regression problems. [sent-126, score-0.651]
</p><p>73 These datasets are summarised in the following table which shows the range of the targets (tmin , tmax ), the number of input dimensions (D), and the size of the training and test sets (Ntrain , Ntest ) that we used. [sent-127, score-0.158]
</p><p>74 Dataset creep abalone ailerons  D 30 8 40  tmin 18 MPa 1 yr −3. [sent-128, score-0.417]
</p><p>75 5 × 10−4  Ntrain 800 1000 1000  Ntest 1266 3177 6154  Dataset creep  abalone  ailerons  Model GP GP + log warped GP GP GP + log warped GP GP warped GP  Absolute error 16. [sent-130, score-1.802]
</p><p>76 45  Table 1: Results of testing the GP, warped GP, and GP with log transform, on three real datasets. [sent-151, score-0.518]
</p><p>77 The dataset creep is a materials science set, with the objective to predict creep rupture stress (in MPa) for steel given chemical composition and other inputs [7, 8]. [sent-153, score-0.452]
</p><p>78 With abalone the aim is to predict the the age of abalone from various physical inputs [9]. [sent-154, score-0.262]
</p><p>79 ailerons is a simulated control problem, with the aim to predict the control action on the ailerons of an F16 aircraft [10, 11]. [sent-155, score-0.228]
</p><p>80 For datasets creep and abalone, which consist of positive observations only, standard practice may be to model the log of the data with a GP. [sent-156, score-0.297]
</p><p>81 So for these datasets we have compared three models: a GP directly on the data, a GP on the ﬁxed log-transformed data, and the warped GP directly on the data. [sent-157, score-0.49]
</p><p>82 The predictive points and densities were always compared in the original data space, accounting for the Jacobian of both the log and the warped transforms. [sent-158, score-0.634]
</p><p>83 The models were run as in the 1D task: ML parameter estimates only, covariance matrix (2), and warping function (13) with three tanh functions. [sent-159, score-0.513]
</p><p>84 We show three measures of performance over independent test sets: mean absolute error, mean squared error, and the mean negative log predictive density evaluated at the test points. [sent-161, score-0.256]
</p><p>85 On these three sets, the warped GP always performs signiﬁcantly better than the standard GP. [sent-163, score-0.435]
</p><p>86 For creep and abalone, the ﬁxed log transform clearly works well too, but particularly in the case of creep, the warped GP learns a better transformation. [sent-164, score-0.65]
</p><p>87 Figure 2 shows the warping functions learnt, and indeed 2(b) and 2(c) are clearly log-like in character. [sent-165, score-0.356]
</p><p>88 On the other hand 2(d), for the ailerons set, is exponential-like. [sent-166, score-0.1]
</p><p>89 This shows the warped GP is able to ﬂexibly handle these different types of datasets. [sent-167, score-0.435]
</p><p>90 The shapes of the learnt warping functions were also found to be very robust to random initialisation of the parameters. [sent-168, score-0.417]
</p><p>91 Finally, the warped GP also makes a better job of predicting the distributions, as shown by the difference in values of the negative log density. [sent-169, score-0.518]
</p><p>92 6  Conclusions, extensions, and related work  We have shown that the warped GP is a useful extension to the standard GP for regression, capable of ﬁnding extra structure in the data through the transformations it learns. [sent-170, score-0.495]
</p><p>93 Of course some datasets are well modelled by a GP already, and applying the warped GP model simply results in a linear “warping” function. [sent-173, score-0.552]
</p><p>94 many observations at the edge of the range lie on a single point,  cause the warped GP problems. [sent-176, score-0.505]
</p><p>95 The warping function attempts to model the censoring by pushing those points far away from the rest of the data, and it suffers in performance especially for ML learning. [sent-177, score-0.354]
</p><p>96 As a further extension, one might consider warping the input space in some nonlinear fashion. [sent-179, score-0.401]
</p><p>97 In the context of geostatistics this has actually been dealt with by O’Hagan [12], where a transformation is made from an input space which can have non-stationary and non-isotropic covariance structure, to a latent space in which the usual conditions of stationarity and isotropy hold. [sent-180, score-0.3]
</p><p>98 Gaussian process classiﬁers can also be thought of as warping the outputs of a GP, through a mapping onto the (0, 1) probability interval. [sent-181, score-0.333]
</p><p>99 However, the observations in classiﬁcation are discrete, not points in this warped continuous space. [sent-182, score-0.483]
</p><p>100 Many thanks to David MacKay for useful discussions, suggestions of warping functions and datasets to try. [sent-194, score-0.411]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gp', 0.653), ('warped', 0.435), ('warping', 0.333), ('tn', 0.232), ('creep', 0.15), ('abalone', 0.117), ('ailerons', 0.1), ('predictive', 0.091), ('covariance', 0.084), ('transformation', 0.083), ('median', 0.079), ('zn', 0.075), ('tanh', 0.074), ('regression', 0.073), ('log', 0.065), ('latent', 0.064), ('modelled', 0.062), ('learnt', 0.061), ('ml', 0.057), ('datasets', 0.055), ('gaussian', 0.055), ('mpa', 0.05), ('percentiles', 0.05), ('tmin', 0.05), ('noise', 0.047), ('observation', 0.045), ('nonlinear', 0.045), ('tmax', 0.043), ('predictions', 0.041), ('monotonic', 0.04), ('targets', 0.039), ('transformations', 0.039), ('density', 0.036), ('processes', 0.036), ('cn', 0.035), ('xn', 0.034), ('diggle', 0.033), ('hagan', 0.033), ('ntrain', 0.033), ('rupture', 0.033), ('steel', 0.033), ('tmed', 0.033), ('gps', 0.033), ('bi', 0.032), ('dataset', 0.032), ('mn', 0.032), ('hybrid', 0.03), ('modelling', 0.029), ('lines', 0.029), ('cmn', 0.029), ('sine', 0.029), ('parameterises', 0.029), ('ntest', 0.029), ('predict', 0.028), ('inverse', 0.027), ('observations', 0.027), ('materials', 0.026), ('edward', 0.026), ('monte', 0.026), ('carlo', 0.026), ('whole', 0.026), ('absolute', 0.025), ('det', 0.025), ('derivatives', 0.025), ('triplets', 0.025), ('jacobian', 0.025), ('rd', 0.024), ('space', 0.023), ('functions', 0.023), ('expressing', 0.023), ('made', 0.023), ('conjugate', 0.023), ('densities', 0.022), ('carl', 0.022), ('style', 0.022), ('lie', 0.022), ('matrix', 0.022), ('regular', 0.021), ('phd', 0.021), ('squared', 0.021), ('regularly', 0.021), ('christopher', 0.021), ('likelihood', 0.021), ('extra', 0.021), ('points', 0.021), ('range', 0.021), ('hyperparameters', 0.02), ('gradients', 0.019), ('asymmetric', 0.019), ('inverting', 0.019), ('bayesian', 0.019), ('map', 0.019), ('distribution', 0.018), ('integral', 0.018), ('thesis', 0.018), ('real', 0.018), ('trends', 0.018), ('really', 0.018), ('negative', 0.018), ('toy', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="194-tfidf-1" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. 1</p><p>2 0.39119184 <a title="194-tfidf-2" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>Author: Christopher J. Paciorek, Mark J. Schervish</p><p>Abstract: We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP ﬁtting may allow for implementation of the method on larger datasets. 1</p><p>3 0.3603428 <a title="194-tfidf-3" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>4 0.11667972 <a title="194-tfidf-4" href="./nips-2003-GPPS%3A_A_Gaussian_Process_Positioning_System_for_Cellular_Networks.html">76 nips-2003-GPPS: A Gaussian Process Positioning System for Cellular Networks</a></p>
<p>Author: Anton Schwaighofer, Marian Grigoras, Volker Tresp, Clemens Hoffmann</p><p>Abstract: In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user’s position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user’s position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network. 1</p><p>5 0.089494355 <a title="194-tfidf-5" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: This paper reports on a family of computationally practical classiﬁers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classiﬁers are based on dyadic classiﬁcation trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classiﬁcation rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n−1/2 , the parametric rate. We are not aware of any other practical classiﬁers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed. 1</p><p>6 0.083074354 <a title="194-tfidf-6" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>7 0.065714382 <a title="194-tfidf-7" href="./nips-2003-A_Probabilistic_Model_of_Auditory_Space_Representation_in_the_Barn_Owl.html">15 nips-2003-A Probabilistic Model of Auditory Space Representation in the Barn Owl</a></p>
<p>8 0.065608412 <a title="194-tfidf-8" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>9 0.060816042 <a title="194-tfidf-9" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>10 0.058026813 <a title="194-tfidf-10" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>11 0.047768287 <a title="194-tfidf-11" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>12 0.041863229 <a title="194-tfidf-12" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>13 0.040590353 <a title="194-tfidf-13" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>14 0.038022738 <a title="194-tfidf-14" href="./nips-2003-Self-calibrating_Probability_Forecasting.html">170 nips-2003-Self-calibrating Probability Forecasting</a></p>
<p>15 0.037514068 <a title="194-tfidf-15" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>16 0.037320971 <a title="194-tfidf-16" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>17 0.037121281 <a title="194-tfidf-17" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>18 0.03614391 <a title="194-tfidf-18" href="./nips-2003-A_Mixed-Signal_VLSI_for_Real-Time_Generation_of_Edge-Based_Image_Vectors.html">11 nips-2003-A Mixed-Signal VLSI for Real-Time Generation of Edge-Based Image Vectors</a></p>
<p>19 0.034162298 <a title="194-tfidf-19" href="./nips-2003-Optimal_Manifold_Representation_of_Data%3A_An_Information_Theoretic_Approach.html">149 nips-2003-Optimal Manifold Representation of Data: An Information Theoretic Approach</a></p>
<p>20 0.033930581 <a title="194-tfidf-20" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.161), (1, 0.037), (2, -0.021), (3, 0.004), (4, -0.007), (5, 0.208), (6, 0.086), (7, -0.249), (8, 0.21), (9, 0.203), (10, -0.315), (11, 0.315), (12, 0.01), (13, -0.217), (14, 0.026), (15, -0.155), (16, -0.16), (17, 0.09), (18, -0.073), (19, -0.047), (20, -0.025), (21, -0.128), (22, -0.079), (23, -0.042), (24, 0.106), (25, 0.051), (26, -0.055), (27, 0.009), (28, 0.037), (29, 0.105), (30, 0.068), (31, 0.036), (32, 0.047), (33, -0.061), (34, 0.025), (35, -0.102), (36, 0.008), (37, -0.042), (38, -0.028), (39, 0.03), (40, -0.015), (41, 0.083), (42, 0.003), (43, -0.043), (44, 0.02), (45, -0.011), (46, 0.029), (47, -0.057), (48, 0.031), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96020085 <a title="194-lsi-1" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. 1</p><p>2 0.86105496 <a title="194-lsi-2" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>Author: Christopher J. Paciorek, Mark J. Schervish</p><p>Abstract: We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP ﬁtting may allow for implementation of the method on larger datasets. 1</p><p>3 0.62227559 <a title="194-lsi-3" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>4 0.60725665 <a title="194-lsi-4" href="./nips-2003-GPPS%3A_A_Gaussian_Process_Positioning_System_for_Cellular_Networks.html">76 nips-2003-GPPS: A Gaussian Process Positioning System for Cellular Networks</a></p>
<p>Author: Anton Schwaighofer, Marian Grigoras, Volker Tresp, Clemens Hoffmann</p><p>Abstract: In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user’s position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user’s position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network. 1</p><p>5 0.2513763 <a title="194-lsi-5" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>6 0.22088781 <a title="194-lsi-6" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>7 0.20304435 <a title="194-lsi-7" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>8 0.1997209 <a title="194-lsi-8" href="./nips-2003-A_Probabilistic_Model_of_Auditory_Space_Representation_in_the_Barn_Owl.html">15 nips-2003-A Probabilistic Model of Auditory Space Representation in the Barn Owl</a></p>
<p>9 0.19734776 <a title="194-lsi-9" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>10 0.19724287 <a title="194-lsi-10" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>11 0.19610333 <a title="194-lsi-11" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>12 0.1776239 <a title="194-lsi-12" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>13 0.17464401 <a title="194-lsi-13" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>14 0.17048514 <a title="194-lsi-14" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>15 0.16654335 <a title="194-lsi-15" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>16 0.16641316 <a title="194-lsi-16" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>17 0.16556226 <a title="194-lsi-17" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.16443697 <a title="194-lsi-18" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>19 0.16046154 <a title="194-lsi-19" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>20 0.16031161 <a title="194-lsi-20" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.029), (11, 0.024), (29, 0.017), (30, 0.015), (33, 0.012), (35, 0.055), (53, 0.106), (69, 0.031), (71, 0.059), (76, 0.062), (85, 0.069), (91, 0.08), (93, 0.315), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81370503 <a title="194-lda-1" href="./nips-2003-Local_Phase_Coherence_and_the_Perception_of_Blur.html">119 nips-2003-Local Phase Coherence and the Perception of Blur</a></p>
<p>Author: Zhou Wang, Eero P. Simoncelli</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.77197868 <a title="194-lda-2" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. 1</p><p>3 0.66579503 <a title="194-lda-3" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>Author: Claudio Gentile</p><p>Abstract: New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classiﬁcation of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with ﬁve known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 1</p><p>4 0.49350563 <a title="194-lda-4" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>5 0.49117297 <a title="194-lda-5" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>Author: Tong Zhang</p><p>Abstract: In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simpliﬁes and enhances previous results. Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution. This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist “bad” prior structures even at places far away from the true distribution. 1</p><p>6 0.49031547 <a title="194-lda-6" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>7 0.49006581 <a title="194-lda-7" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>8 0.48938122 <a title="194-lda-8" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>9 0.48850858 <a title="194-lda-9" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>10 0.488352 <a title="194-lda-10" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>11 0.48767373 <a title="194-lda-11" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>12 0.48761898 <a title="194-lda-12" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>13 0.48637885 <a title="194-lda-13" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>14 0.48633304 <a title="194-lda-14" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>15 0.48628414 <a title="194-lda-15" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>16 0.48413354 <a title="194-lda-16" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>17 0.48389572 <a title="194-lda-17" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>18 0.48372751 <a title="194-lda-18" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>19 0.48360908 <a title="194-lda-19" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>20 0.48300126 <a title="194-lda-20" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
