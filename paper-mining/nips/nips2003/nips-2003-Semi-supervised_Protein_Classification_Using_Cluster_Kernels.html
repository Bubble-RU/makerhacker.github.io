<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-173" href="#">nips2003-173</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</h1>
<br/><p>Source: <a title="nips-2003-173-pdf" href="http://papers.nips.cc/paper/2496-semi-supervised-protein-classification-using-cluster-kernels.pdf">pdf</a></p><p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>Reference: <a title="nips-2003-173-reference" href="../nips2003_reference/nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mismatch', 0.502), ('homolog', 0.437), ('protein', 0.35), ('kernel', 0.28), ('unlabel', 0.224), ('blast', 0.218), ('bag', 0.17), ('clust', 0.165), ('nbd', 0.151), ('pul', 0.131), ('neighb', 0.128), ('string', 0.117), ('roc', 0.109), ('transduc', 0.098), ('sequ', 0.087), ('walk', 0.086), ('scor', 0.08), ('kbag', 0.067), ('spect', 0.057), ('remot', 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="173-tfidf-1" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>2 0.20428027 <a title="173-tfidf-2" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>3 0.19637834 <a title="173-tfidf-3" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>4 0.15989445 <a title="173-tfidf-4" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>5 0.15207396 <a title="173-tfidf-5" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>Author: Woojae Kim, Daniel J. Navarro, Mark A. Pitt, In J. Myung</p><p>Abstract: Despite the popularity of connectionist models in cognitive science, their performance can often be diﬃcult to evaluate. Inspired by the geometric approach to statistical model selection, we introduce a conceptually similar method to examine the global behavior of a connectionist model, by counting the number and types of response patterns it can simulate. The Markov Chain Monte Carlo-based algorithm that we constructed Þnds these patterns eﬃciently. We demonstrate the approach using two localist network models of speech perception. 1</p><p>6 0.1429546 <a title="173-tfidf-6" href="./nips-2003-Clustering_with_the_Connectivity_Kernel.html">46 nips-2003-Clustering with the Connectivity Kernel</a></p>
<p>7 0.12591521 <a title="173-tfidf-7" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>8 0.11913104 <a title="173-tfidf-8" href="./nips-2003-Learning_the_k_in_k-means.html">111 nips-2003-Learning the k in k-means</a></p>
<p>9 0.11568561 <a title="173-tfidf-9" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>10 0.11030813 <a title="173-tfidf-10" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>11 0.10895272 <a title="173-tfidf-11" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>12 0.098645464 <a title="173-tfidf-12" href="./nips-2003-Identifying_Structure_across_Pre-partitioned_Data.html">87 nips-2003-Identifying Structure across Pre-partitioned Data</a></p>
<p>13 0.096064396 <a title="173-tfidf-13" href="./nips-2003-Out-of-Sample_Extensions_for_LLE%2C_Isomap%2C_MDS%2C_Eigenmaps%2C_and_Spectral_Clustering.html">150 nips-2003-Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering</a></p>
<p>14 0.095083877 <a title="173-tfidf-14" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>15 0.090688728 <a title="173-tfidf-15" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>16 0.089550771 <a title="173-tfidf-16" href="./nips-2003-Geometric_Clustering_Using_the_Information_Bottleneck_Method.html">82 nips-2003-Geometric Clustering Using the Information Bottleneck Method</a></p>
<p>17 0.088667944 <a title="173-tfidf-17" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>18 0.086194076 <a title="173-tfidf-18" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>19 0.079155378 <a title="173-tfidf-19" href="./nips-2003-ICA-based_Clustering_of_Genes_from_Microarray_Expression_Data.html">86 nips-2003-ICA-based Clustering of Genes from Microarray Expression Data</a></p>
<p>20 0.074827865 <a title="173-tfidf-20" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.209), (1, 0.168), (2, -0.02), (3, -0.086), (4, -0.015), (5, -0.098), (6, -0.088), (7, -0.137), (8, -0.003), (9, 0.036), (10, 0.173), (11, 0.102), (12, 0.013), (13, -0.184), (14, -0.046), (15, 0.055), (16, -0.103), (17, 0.083), (18, 0.01), (19, -0.003), (20, 0.053), (21, -0.097), (22, 0.105), (23, 0.006), (24, -0.057), (25, -0.051), (26, 0.244), (27, -0.054), (28, 0.13), (29, -0.017), (30, -0.153), (31, -0.007), (32, 0.04), (33, -0.018), (34, 0.073), (35, -0.039), (36, -0.114), (37, 0.013), (38, 0.137), (39, 0.017), (40, 0.124), (41, -0.07), (42, 0.063), (43, 0.019), (44, -0.145), (45, -0.038), (46, -0.035), (47, -0.094), (48, 0.054), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93493193 <a title="173-lsi-1" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>2 0.67642415 <a title="173-lsi-2" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>3 0.62621665 <a title="173-lsi-3" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>4 0.55693871 <a title="173-lsi-4" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>5 0.55005723 <a title="173-lsi-5" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>Author: Woojae Kim, Daniel J. Navarro, Mark A. Pitt, In J. Myung</p><p>Abstract: Despite the popularity of connectionist models in cognitive science, their performance can often be diﬃcult to evaluate. Inspired by the geometric approach to statistical model selection, we introduce a conceptually similar method to examine the global behavior of a connectionist model, by counting the number and types of response patterns it can simulate. The Markov Chain Monte Carlo-based algorithm that we constructed Þnds these patterns eﬃciently. We demonstrate the approach using two localist network models of speech perception. 1</p><p>6 0.49594417 <a title="173-lsi-6" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>7 0.48694071 <a title="173-lsi-7" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>8 0.48024267 <a title="173-lsi-8" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>9 0.45107529 <a title="173-lsi-9" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>10 0.45075041 <a title="173-lsi-10" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>11 0.44120446 <a title="173-lsi-11" href="./nips-2003-Clustering_with_the_Connectivity_Kernel.html">46 nips-2003-Clustering with the Connectivity Kernel</a></p>
<p>12 0.43837491 <a title="173-lsi-12" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>13 0.43625095 <a title="173-lsi-13" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>14 0.40071687 <a title="173-lsi-14" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>15 0.39831477 <a title="173-lsi-15" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>16 0.39080691 <a title="173-lsi-16" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>17 0.37258872 <a title="173-lsi-17" href="./nips-2003-ICA-based_Clustering_of_Genes_from_Microarray_Expression_Data.html">86 nips-2003-ICA-based Clustering of Genes from Microarray Expression Data</a></p>
<p>18 0.36920282 <a title="173-lsi-18" href="./nips-2003-Learning_the_k_in_k-means.html">111 nips-2003-Learning the k in k-means</a></p>
<p>19 0.35318634 <a title="173-lsi-19" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>20 0.35247174 <a title="173-lsi-20" href="./nips-2003-Plasticity_Kernels_and_Temporal_Statistics.html">157 nips-2003-Plasticity Kernels and Temporal Statistics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.096), (8, 0.012), (9, 0.012), (26, 0.018), (31, 0.08), (43, 0.318), (53, 0.068), (58, 0.131), (62, 0.036), (71, 0.018), (76, 0.088), (85, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71067768 <a title="173-lda-1" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>2 0.70219946 <a title="173-lda-2" href="./nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models.html">39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</a></p>
<p>Author: Charles Rosenberg, Alok Ladsariya, Tom Minka</p><p>Abstract: We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.</p><p>3 0.63512212 <a title="173-lda-3" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Andriy Kharechko, John S. Shawe-taylor</p><p>Abstract: We present a modiﬁed version of the perceptron learning algorithm (PLA) which solves semideﬁnite programs (SDPs) in polynomial time. The algorithm is based on the following three observations: (i) Semideﬁnite programs are linear programs with inﬁnitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in ﬁnitely many updates. Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time. We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods. 1</p><p>4 0.55327153 <a title="173-lda-4" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: Knowledge about local invariances with respect to given pattern transformations can greatly improve the accuracy of classiﬁcation. Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples. We develop a new framework for learning linear classiﬁers under known transformations based on semideﬁnite programming. We present a new learning algorithm— the Semideﬁnite Programming Machine (SDPM)—which is able to ﬁnd a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points. The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors. Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed. In experiments we use a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and ﬁnd improvements over known methods. 1</p><p>5 0.52577919 <a title="173-lda-5" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>6 0.52560484 <a title="173-lda-6" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>7 0.52274072 <a title="173-lda-7" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>8 0.52216446 <a title="173-lda-8" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>9 0.51967442 <a title="173-lda-9" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>10 0.51674062 <a title="173-lda-10" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>11 0.51572728 <a title="173-lda-11" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>12 0.51538545 <a title="173-lda-12" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>13 0.5146687 <a title="173-lda-13" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>14 0.51459444 <a title="173-lda-14" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>15 0.5138256 <a title="173-lda-15" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>16 0.51249981 <a title="173-lda-16" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>17 0.51159161 <a title="173-lda-17" href="./nips-2003-ICA-based_Clustering_of_Genes_from_Microarray_Expression_Data.html">86 nips-2003-ICA-based Clustering of Genes from Microarray Expression Data</a></p>
<p>18 0.51075703 <a title="173-lda-18" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>19 0.51025683 <a title="173-lda-19" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>20 0.50950581 <a title="173-lda-20" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
