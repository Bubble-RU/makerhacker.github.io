<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-9" href="#">nips2003-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</h1>
<br/><p>Source: <a title="nips-2003-9-pdf" href="http://papers.nips.cc/paper/2351-a-kullback-leibler-divergence-based-kernel-for-svm-classification-in-multimedia-applications.pdf">pdf</a></p><p>Author: Pedro J. Moreno, Purdy P. Ho, Nuno Vasconcelos</p><p>Abstract: Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identiﬁcation/veriﬁcation and image classiﬁcation tasks and show that these new kernels have the best performance in speaker veriﬁcation and mostly outperform the Fisher kernel based SVM’s and the generative classiﬁers in speaker identiﬁcation and image classiﬁcation. 1</p><p>Reference: <a title="nips-2003-9-reference" href="../nips2003_reference/nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. [sent-8, score-0.185]
</p><p>2 The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. [sent-9, score-0.235]
</p><p>3 In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. [sent-10, score-0.616]
</p><p>4 In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. [sent-11, score-0.578]
</p><p>5 We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. [sent-12, score-0.417]
</p><p>6 In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. [sent-13, score-0.213]
</p><p>7 1  Introduction  During the last years Support Vector Machines (SVM’s) [1] have become extremely successful discriminative approaches to pattern classiﬁcation and regression problems. [sent-15, score-0.141]
</p><p>8 While most research in the SVM community has focused on the underlying learning algorithms the study of kernels has also gained importance recently. [sent-19, score-0.165]
</p><p>9 Standard kernels such as linear, Gaussian, or polynomial do not take full advantage of the nuances of speciﬁc data sets. [sent-20, score-0.241]
</p><p>10 For example, [2] applies normalization factors to polynomial kernels for speaker identiﬁcation tasks. [sent-22, score-0.474]
</p><p>11 Similarly, [3] explores the use of heavy tailed Gaussian kernels in image classiﬁcation tasks. [sent-23, score-0.213]
</p><p>12 These approaches in general only try to tune standard kernels (linear, polynomial, Gaussian) to the nuances of multimedia data sets. [sent-24, score-0.471]
</p><p>13 They are simple to learn and estimate, and are well understood by the multimedia community. [sent-26, score-0.315]
</p><p>14 The Fisher kernel proposed by Jaakkola [4] effectively combines both generative and discriminative classiﬁers for variable length sequences. [sent-28, score-0.378]
</p><p>15 Besides its original application in genomic problems it has also been applied to multimedia domains, among others [5] applies it to audio classiﬁcation with good results; [6] also tries a variation on the Fisher kernel on phonetic classiﬁcation tasks. [sent-29, score-0.529]
</p><p>16 We propose a different approach to combine both discriminative and generative methods to classiﬁcation. [sent-30, score-0.239]
</p><p>17 Instead of using these standard kernels, we leverage on successful generative models used in the multimedia ﬁeld. [sent-31, score-0.477]
</p><p>18 We use diagonal covariance GMM’s and full covariance Gaussian models to better represent each individual audio and image object. [sent-32, score-0.54]
</p><p>19 We then use a metric derived from the symmetric Kullback-Leibler (KL) divergence to effectively compute inner products between multimedia objects. [sent-33, score-0.501]
</p><p>20 These kernel functions have two main disadvantages for multimedia signals. [sent-36, score-0.446]
</p><p>21 First they only model inner products between individual feature vectors as opposed to an ensemble of vectors which is the typical case for multimedia signals. [sent-37, score-0.477]
</p><p>22 Secondly these kernels are quite generic and do not take advantage of the statistics of the individual signals we are targeting. [sent-38, score-0.221]
</p><p>23 It assumes the existence of a generative model that explains well all possible data. [sent-40, score-0.14]
</p><p>24 For example, in the case of speech signals the generative model p(x|θ) is often a Gaussian mixture. [sent-41, score-0.264]
</p><p>25 Where the θ model parameters are priors, means, and diagonal covariance matrices. [sent-42, score-0.145]
</p><p>26 GMM’s are also quite popular in the image classiﬁcation and retrieval domains; [7] shows good results on image classiﬁcation and retrieval using Gaussian mixtures. [sent-43, score-0.278]
</p><p>27 For any given sequence of vectors deﬁning a multimedia object X = {x1 , x2 , . [sent-44, score-0.459]
</p><p>28 This new feature vector, the Fisher score, is deﬁned as UX =  θ log(P (X|θ))  (1)  Each component of UX is a derivative of the log-likelihood of the vector sequence X with respect to a particular parameter of the generative model. [sent-52, score-0.249]
</p><p>29 In our case the parameters θ of the generative model are chosen from either the prior probabilities, the mean vector or the diagonal covariance matrix of each individual Gaussian in the mixture model. [sent-53, score-0.376]
</p><p>30 , for θ = µk out of K possible mixtures, then the Fisher score is  m µk log(P (X|µk ))  P (k|xi )Σ−1 (xi − µk ) k  =  (2)  i=1  where P (k|xi ) represents the a posteriori probability of mixture k given the observed feature vector xi . [sent-56, score-0.232]
</p><p>31 Effectively we transform each multimedia object (audio or image) X of variable length into a single vector UX of ﬁxed dimension. [sent-57, score-0.42]
</p><p>32 , we estimate the parameters θ i of a generic probability density function (PDF) for each multimedia object (utterance or image) Xi = {x1 , x2 , . [sent-60, score-0.418]
</p><p>33 We pick PDF’s that have been shown over the years to be quite effective at modeling multimedia patterns. [sent-64, score-0.384]
</p><p>34 In particular we use diagonal Gaussian mixture models and single full covariance Gaussian models. [sent-65, score-0.29]
</p><p>35 In the ﬁrst case the parameters θ i are priors, mean vectors, and diagonal covariance matrices while in the second case the parameters θ i are the mean vector and full covariance matrix. [sent-66, score-0.333]
</p><p>36 In the case of diagonal mixture models there is no analytical solution for θ i and we use the Expectation Maximization algorithm. [sent-68, score-0.141]
</p><p>37 In the case of single full covariance Gaussian model there is a simple analytical solution for the mean vector and covariance matrix. [sent-69, score-0.357]
</p><p>38 Notice that if the number of vector in the Xi multimedia sequence is small and there is not enough data to accurately estimate θ i we can use regularization methods, or even replace the maximum likelihood solution for θ i by a maximum a posteriori solution. [sent-71, score-0.418]
</p><p>39 The next step is to deﬁne the kernel distance in this new feature space. [sent-73, score-0.207]
</p><p>40 In the case of single full covariance models the KL divergence has an analytical solution D(p(x|θ i ), p(x|θ j )) = tr(Σi Σ−1 ) + tr(Σj Σ−1 )− j i 2 S + tr((Σ−1 + Σ−1 ) (µi − µj )(µi − µj )T ) i j  (6)  where S is the dimensionality of the original feature data x. [sent-81, score-0.415]
</p><p>41 This distance is similar to the Arithmetic harmonic sphericity (AHS) distance quite popular in the speaker identiﬁcation and veriﬁcation research community [8]. [sent-82, score-0.522]
</p><p>42 Notice that there are signiﬁcant differences between our KL divergence based kernel and the Fisher kernel method. [sent-83, score-0.38]
</p><p>43 In our approach there is no underlying generative model to represent all the data. [sent-84, score-0.161]
</p><p>44 We do not use a single PDF (even if it encodes a latent variable indicative of class membership) as a way to map the multimedia object from the original feature vector space to a gradient log-likelihood vector space. [sent-85, score-0.49]
</p><p>45 Instead each individual object (consisting of a sequence of feature vectors) is modeled by its unique PDF. [sent-86, score-0.174]
</p><p>46 This represents a more localized version of the Fisher kernel underlying generative model. [sent-87, score-0.292]
</p><p>47 Effectively the modeling power is spent where it matters most, on each of the individual objects in the training and testing sets. [sent-88, score-0.164]
</p><p>48 As we will show in our experimental section a single full covariance Gaussian model produces extremely good results. [sent-90, score-0.225]
</p><p>49 4  Audio and Image Databases  We chose the 50 most frequent speakers from the HUB4-96 [9] News Broadcasting corpus and 50 speakers from the Narrowband version of the KING corpus [10] to train and test our new kernels on speaker identiﬁcation and veriﬁcation tasks. [sent-93, score-0.599]
</p><p>50 The HUB training set contains about 25 utterances (each 3-7 seconds long) from each speaker, resulting in 1198 utterances (or about 2 hours of speech). [sent-94, score-0.32]
</p><p>51 The HUB test set contains the rest of the utterances from these 50 speakers resulting in 15325 utterances (or about 21 hours of speech). [sent-95, score-0.324]
</p><p>52 The KING corpus is commonly used for speaker identiﬁcation and veriﬁcation in the speech community [11]. [sent-96, score-0.519]
</p><p>53 Its training set contains 4 utterances (each about 30 seconds long) from each speaker and the test set contains the remaining 6 from these 50 speakers. [sent-97, score-0.491]
</p><p>54 67 hours of speech) and 300 test utterances (about 2. [sent-99, score-0.161]
</p><p>55 Following standard practice in speech processing each utterance was transformed into a sequence of 13 dimensional Mel-Frequency Cepstral vectors. [sent-101, score-0.283]
</p><p>56 We did not do so for the HUB experiments since mean normalizing the audio would remove important speaker characteristics. [sent-104, score-0.415]
</p><p>57 There were 100 images per class – 66 for training and 34 for testing; thus, a total of 528 training images and 272 testing images were used. [sent-108, score-0.232]
</p><p>58 To extract feature vectors we followed standard practice in image processing. [sent-110, score-0.19]
</p><p>59 When training and testing our new GMM/KL Divergence based kernels, a sequence of feature vectors, {x1 , x2 , . [sent-115, score-0.2]
</p><p>60 , xm } from each utterance or image X was modeled by a single GMM of diagonal covariances. [sent-118, score-0.275]
</p><p>61 This resulted in kernel matrices for training and testing that could be feed directly into a SVM classiﬁer. [sent-122, score-0.244]
</p><p>62 For the experiments in which the object PDF was a single full covariance Gaussian we followed a similar procedure. [sent-125, score-0.249]
</p><p>63 The dimensions of the resulting training and testing kernel matrices are shown in Table 1. [sent-129, score-0.244]
</p><p>64 Table 1: Dimensions of the training and testing kernel matrices of both new probablisitic kernels on HUB, KING, and COREL databases. [sent-130, score-0.359]
</p><p>65 The underlying generative model was the same one used for the GMM classiﬁcation experiments. [sent-132, score-0.161]
</p><p>66 The task of speaker veriﬁcation is different from speaker identiﬁcation. [sent-133, score-0.664]
</p><p>67 Because we have trained SVM’s using the one-vs-all approach their output can be directly used in speaker veriﬁcation. [sent-135, score-0.332]
</p><p>68 , if we want to verify whether an utterance belongs to speaker A we need to compute a model for non-A speakers. [sent-140, score-0.422]
</p><p>69 This nonclass model can be computed by ﬁrst pooling the 49 non-class GMM’s together to form a super GMM with 256x49 mixtures, (each speaker GMM has 256 mixtures). [sent-141, score-0.359]
</p><p>70 Then the score produced by this super GMM is subtracted from the score produced by the claimed speaker GMM. [sent-142, score-0.519]
</p><p>71 In the case of AHS classiﬁers we estimate the non-class score as the arithmetic mean of the other 49 speaker scores. [sent-143, score-0.44]
</p><p>72 To compute the miss and false positive rates we compare the 1 Arithmetic harmonic sphericity classiﬁers pull together all vectors belonging to a class and ﬁt a single full covariance Gaussian model to the data. [sent-144, score-0.388]
</p><p>73 Similarly, a single full covariance model is ﬁtted to each testing utterance. [sent-145, score-0.263]
</p><p>74 The similarity between the testing utterances and the class models is measured according to Eq. [sent-146, score-0.218]
</p><p>75 We compare the performance of all the 5 classiﬁers in speaker veriﬁcation and speaker identiﬁcation tasks. [sent-152, score-0.664]
</p><p>76 Table 2 shows equal-error rates (EER’s) for speaker veriﬁcation and accuracies of speaker identiﬁcation for both speech corpora. [sent-153, score-0.834]
</p><p>77 9  Our results using the KL divergence based kernels in both multimedia data types are quite promising. [sent-185, score-0.586]
</p><p>78 In the case of the HUB experiments all classiﬁers perform similarly in both speaker veriﬁcation and identiﬁcation tasks with the exception of the SVM Fisher which performs signiﬁcantly worse. [sent-186, score-0.355]
</p><p>79 However, For the KING database, we can see that our KL based SVM kernels outperform all other classiﬁers in both identiﬁcation and veriﬁcation tasks. [sent-187, score-0.154]
</p><p>80 Looking at the DET plots for both corpora we can see that on the HUB experiments the new SVM kernels perform quite well and on the KING corpora they perform much better than any other veriﬁcation system. [sent-189, score-0.237]
</p><p>81 In image classiﬁcation experiments with the COREL database both KL based SVM kernels outperform the Fisher SVM; the GMM/KL kernel even outperforms the baseline GMM classiﬁer. [sent-190, score-0.466]
</p><p>82 6  Conclusion and Future Work  In this paper we have proposed a new method of combining generative models and discriminative classiﬁers (SVM’s). [sent-191, score-0.256]
</p><p>83 For every multimedia object represented by a sequence of vectors, a PDF is learned using maximum likelihood approaches. [sent-193, score-0.415]
</p><p>84 We have experimented with PDF’s that are commonly used in the multimedia  HUB DETs  0. [sent-194, score-0.315]
</p><p>85 In the case of a single full covariance Gaussian we directly estimate its parameters. [sent-229, score-0.188]
</p><p>86 In effect we replace the standard kernel distance on the original data K(Xi , Xj ) by a new kernel derived from the symmetric Kullback-Leibler (KL) divergence K(Xi , Xj ) −→ K(p(x|θ i ), p(x|θ j )). [sent-231, score-0.463]
</p><p>87 In our experiments we have validated this new approach in speaker identiﬁcation, veriﬁcation, and image classiﬁcation tasks by comparing its performance to Fisher kernel SVM’s and other well-known classiﬁcation algorithms: GMM and AHS methods. [sent-233, score-0.584]
</p><p>88 Our results show that our new method of combining generative models and SVM’s always outperform the SVM Fisher kernel and the AHS methods, and it often outperforms other classiﬁcation methods such as GMM’s and AHS. [sent-234, score-0.375]
</p><p>89 The equal error rates are consistently better with the new kernel SVM methods too. [sent-235, score-0.152]
</p><p>90 In the case of image classiﬁcation our GMM/KL divergence-based kernel has the best performance among the four classiﬁers while our single full covariance Gaussian distance based kernel outperforms most other classiﬁers and only do slightly worse than the baseline GMM. [sent-236, score-0.628]
</p><p>91 In both audio and image tasks we just take advantage of previous years of research in generative methods. [sent-238, score-0.375]
</p><p>92 The good results obtained using a full covariance single Gaussian KL kernel also make our algorithm a very attractive alternative as opposed to the more complex methods of tuning system parameters and combining generative classiﬁers and discriminative methods such as the Fisher SVM. [sent-239, score-0.553]
</p><p>93 This full covariance single Gaussian KL kernel’s performance is consistently good across all databases. [sent-240, score-0.188]
</p><p>94 We feel that this approach of combining generative classiﬁers via KL divergences of derived PDF’s is quite generic and can possibly be applied to other domains. [sent-242, score-0.294]
</p><p>95 We plan to explore its use in other multimedia related tasks. [sent-243, score-0.315]
</p><p>96 , “Support vector machines for speaker veriﬁcation and identiﬁcation,” IEEE Proceeding, 2000. [sent-249, score-0.354]
</p><p>97 , “Using the ﬁsher kernel method for web audio classiﬁcation,” ICASSP, 2000. [sent-265, score-0.214]
</p><p>98 , “Data dependent kernels in SVM classiﬁcation of speech patterns,” Tech. [sent-269, score-0.239]
</p><p>99 , “Second-order statistical measures for text-independent speaker identiﬁcation,” Speech Communication, vol. [sent-279, score-0.332]
</p><p>100 , “Towards better making a decision in speaker veriﬁcation,” Pattern Recognition, , no. [sent-291, score-0.332]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('speaker', 0.332), ('multimedia', 0.315), ('gmm', 0.285), ('hub', 0.264), ('fisher', 0.258), ('svm', 0.25), ('pdf', 0.215), ('king', 0.198), ('kl', 0.174), ('ahs', 0.163), ('classi', 0.15), ('generative', 0.14), ('veri', 0.138), ('kernel', 0.131), ('corel', 0.129), ('speech', 0.124), ('utterances', 0.121), ('divergence', 0.118), ('kernels', 0.115), ('covariance', 0.108), ('cation', 0.105), ('image', 0.098), ('ers', 0.097), ('utterance', 0.09), ('audio', 0.083), ('identi', 0.077), ('testing', 0.075), ('discriminative', 0.073), ('eer', 0.071), ('object', 0.061), ('score', 0.06), ('ux', 0.06), ('full', 0.058), ('acc', 0.053), ('divergences', 0.053), ('arithmetic', 0.048), ('feature', 0.048), ('gaussian', 0.045), ('miss', 0.045), ('vectors', 0.044), ('mixture', 0.043), ('speakers', 0.042), ('corpora', 0.042), ('generic', 0.042), ('dets', 0.041), ('moreno', 0.041), ('nuances', 0.041), ('nuno', 0.041), ('sphericity', 0.041), ('vasconcelos', 0.041), ('claimed', 0.04), ('hours', 0.04), ('sequence', 0.039), ('analytical', 0.039), ('outperform', 0.039), ('training', 0.038), ('xi', 0.038), ('quite', 0.038), ('diagonal', 0.037), ('extremely', 0.037), ('sher', 0.035), ('corpus', 0.034), ('effectively', 0.034), ('symmetric', 0.034), ('mixtures', 0.034), ('years', 0.031), ('database', 0.031), ('det', 0.03), ('transformed', 0.03), ('baseline', 0.03), ('community', 0.029), ('distance', 0.028), ('news', 0.028), ('domains', 0.028), ('xm', 0.028), ('tr', 0.028), ('polynomial', 0.027), ('super', 0.027), ('images', 0.027), ('combine', 0.026), ('harmonic', 0.026), ('individual', 0.026), ('objects', 0.025), ('scores', 0.025), ('accuracies', 0.025), ('tasks', 0.023), ('false', 0.023), ('vector', 0.022), ('outperforms', 0.022), ('vapnik', 0.022), ('models', 0.022), ('xj', 0.022), ('single', 0.022), ('retrieval', 0.022), ('combining', 0.021), ('replace', 0.021), ('posteriori', 0.021), ('rates', 0.021), ('table', 0.021), ('underlying', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="9-tfidf-1" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>Author: Pedro J. Moreno, Purdy P. Ho, Nuno Vasconcelos</p><p>Abstract: Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identiﬁcation/veriﬁcation and image classiﬁcation tasks and show that these new kernels have the best performance in speaker veriﬁcation and mostly outperform the Fisher kernel based SVM’s and the generative classiﬁers in speaker identiﬁcation and image classiﬁcation. 1</p><p>2 0.24051064 <a title="9-tfidf-2" href="./nips-2003-Phonetic_Speaker_Recognition_with_Support_Vector_Machines.html">156 nips-2003-Phonetic Speaker Recognition with Support Vector Machines</a></p>
<p>Author: William M. Campbell, Joseph P. Campbell, Douglas A. Reynolds, Douglas A. Jones, Timothy R. Leek</p><p>Abstract: A recent area of signiﬁcant progress in speaker recognition is the use of high level features—idiolect, phonetic relations, prosody, discourse structure, etc. A speaker not only has a distinctive acoustic sound but uses language in a characteristic manner. Large corpora of speech data available in recent years allow experimentation with long term statistics of phone patterns, word patterns, etc. of an individual. We propose the use of support vector machines and term frequency analysis of phone sequences to model a given speaker. To this end, we explore techniques for text categorization applied to the problem. We derive a new kernel based upon a linearization of likelihood ratio scoring. We introduce a new phone-based SVM speaker recognition approach that halves the error rate of conventional phone-based approaches.</p><p>3 0.1555437 <a title="9-tfidf-3" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>Author: James T. Kwok, Brian Mak, Simon Ho</p><p>Abstract: Eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available. At the heart of the method is principal component analysis (PCA) employed to ﬁnd the most important eigenvoices. In this paper, we postulate that nonlinear PCA, in particular kernel PCA, may be even more effective. One major challenge is to map the feature-space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding. Our solution is to compute kernel PCA using composite kernels, and we will call our new method kernel eigenvoice speaker adaptation. On the TIDIGITS corpus, we found that compared with a speaker-independent model, our kernel eigenvoice adaptation method can reduce the word error rate by 28–33% while the standard eigenvoice approach can only match the performance of the speaker-independent model. 1</p><p>4 0.13452928 <a title="9-tfidf-4" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><p>5 0.11758129 <a title="9-tfidf-5" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>6 0.11442865 <a title="9-tfidf-6" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>7 0.11341741 <a title="9-tfidf-7" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>8 0.10895456 <a title="9-tfidf-8" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>9 0.092926502 <a title="9-tfidf-9" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>10 0.085528515 <a title="9-tfidf-10" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>11 0.084962465 <a title="9-tfidf-11" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>12 0.081437103 <a title="9-tfidf-12" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>13 0.079554826 <a title="9-tfidf-13" href="./nips-2003-A_Classification-based_Cocktail-party_Processor.html">5 nips-2003-A Classification-based Cocktail-party Processor</a></p>
<p>14 0.079183593 <a title="9-tfidf-14" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>15 0.076919131 <a title="9-tfidf-15" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>16 0.075438097 <a title="9-tfidf-16" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>17 0.075222783 <a title="9-tfidf-17" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>18 0.07460542 <a title="9-tfidf-18" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>19 0.072197191 <a title="9-tfidf-19" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>20 0.068304017 <a title="9-tfidf-20" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.227), (1, -0.128), (2, 0.024), (3, -0.163), (4, -0.06), (5, 0.022), (6, 0.043), (7, -0.128), (8, 0.032), (9, 0.163), (10, 0.139), (11, -0.099), (12, -0.24), (13, -0.11), (14, 0.178), (15, -0.096), (16, 0.194), (17, 0.093), (18, 0.0), (19, -0.066), (20, -0.022), (21, -0.004), (22, 0.037), (23, 0.028), (24, -0.014), (25, -0.006), (26, 0.075), (27, 0.05), (28, -0.103), (29, 0.029), (30, 0.007), (31, 0.015), (32, -0.029), (33, 0.036), (34, 0.075), (35, 0.231), (36, -0.056), (37, -0.0), (38, 0.119), (39, 0.056), (40, 0.111), (41, 0.053), (42, 0.073), (43, -0.064), (44, 0.03), (45, -0.09), (46, 0.069), (47, -0.056), (48, -0.004), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93957478 <a title="9-lsi-1" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>Author: Pedro J. Moreno, Purdy P. Ho, Nuno Vasconcelos</p><p>Abstract: Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identiﬁcation/veriﬁcation and image classiﬁcation tasks and show that these new kernels have the best performance in speaker veriﬁcation and mostly outperform the Fisher kernel based SVM’s and the generative classiﬁers in speaker identiﬁcation and image classiﬁcation. 1</p><p>2 0.91437107 <a title="9-lsi-2" href="./nips-2003-Phonetic_Speaker_Recognition_with_Support_Vector_Machines.html">156 nips-2003-Phonetic Speaker Recognition with Support Vector Machines</a></p>
<p>Author: William M. Campbell, Joseph P. Campbell, Douglas A. Reynolds, Douglas A. Jones, Timothy R. Leek</p><p>Abstract: A recent area of signiﬁcant progress in speaker recognition is the use of high level features—idiolect, phonetic relations, prosody, discourse structure, etc. A speaker not only has a distinctive acoustic sound but uses language in a characteristic manner. Large corpora of speech data available in recent years allow experimentation with long term statistics of phone patterns, word patterns, etc. of an individual. We propose the use of support vector machines and term frequency analysis of phone sequences to model a given speaker. To this end, we explore techniques for text categorization applied to the problem. We derive a new kernel based upon a linearization of likelihood ratio scoring. We introduce a new phone-based SVM speaker recognition approach that halves the error rate of conventional phone-based approaches.</p><p>3 0.78225684 <a title="9-lsi-3" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>Author: James T. Kwok, Brian Mak, Simon Ho</p><p>Abstract: Eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available. At the heart of the method is principal component analysis (PCA) employed to ﬁnd the most important eigenvoices. In this paper, we postulate that nonlinear PCA, in particular kernel PCA, may be even more effective. One major challenge is to map the feature-space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding. Our solution is to compute kernel PCA using composite kernels, and we will call our new method kernel eigenvoice speaker adaptation. On the TIDIGITS corpus, we found that compared with a speaker-independent model, our kernel eigenvoice adaptation method can reduce the word error rate by 28–33% while the standard eigenvoice approach can only match the performance of the speaker-independent model. 1</p><p>4 0.56785578 <a title="9-lsi-4" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>Author: Michael J. Quinlan, Stephan K. Chalup, Richard H. Middleton</p><p>Abstract: This article addresses the issues of colour classiﬁcation and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classiﬁcation with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse ﬁtting for colour classiﬁcation and the statistical approach used for collision detection.</p><p>5 0.46079981 <a title="9-lsi-5" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>6 0.45965567 <a title="9-lsi-6" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>7 0.44832048 <a title="9-lsi-7" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>8 0.44093725 <a title="9-lsi-8" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>9 0.43816346 <a title="9-lsi-9" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>10 0.43795437 <a title="9-lsi-10" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>11 0.39254272 <a title="9-lsi-11" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>12 0.38852838 <a title="9-lsi-12" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>13 0.38738739 <a title="9-lsi-13" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>14 0.37469774 <a title="9-lsi-14" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>15 0.36015728 <a title="9-lsi-15" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>16 0.36008716 <a title="9-lsi-16" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>17 0.3576301 <a title="9-lsi-17" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>18 0.35243267 <a title="9-lsi-18" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>19 0.34944624 <a title="9-lsi-19" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>20 0.33209398 <a title="9-lsi-20" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.031), (11, 0.047), (17, 0.218), (30, 0.03), (35, 0.055), (53, 0.112), (71, 0.13), (76, 0.07), (79, 0.02), (85, 0.079), (91, 0.062), (99, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8415218 <a title="9-lda-1" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>Author: Pedro J. Moreno, Purdy P. Ho, Nuno Vasconcelos</p><p>Abstract: Over the last years signiﬁcant efforts have been made to develop kernels that can be applied to sequence data such as DNA, text, speech, video and images. The Fisher Kernel and similar variants have been suggested as good ways to combine an underlying generative model in the feature space and discriminant classiﬁers such as SVM’s. In this paper we suggest an alternative procedure to the Fisher kernel for systematically ﬁnding kernel functions that naturally handle variable length sequence data in multimedia domains. In particular for domains such as speech and images we explore the use of kernel functions that take full advantage of well known probabilistic models such as Gaussian Mixtures and single full covariance Gaussian models. We derive a kernel distance based on the Kullback-Leibler (KL) divergence between generative models. In effect our approach combines the best of both generative and discriminative methods and replaces the standard SVM kernels. We perform experiments on speaker identiﬁcation/veriﬁcation and image classiﬁcation tasks and show that these new kernels have the best performance in speaker veriﬁcation and mostly outperform the Fisher kernel based SVM’s and the generative classiﬁers in speaker identiﬁcation and image classiﬁcation. 1</p><p>2 0.67634147 <a title="9-lda-2" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Trevor J. Hastie</p><p>Abstract: Margin maximizing properties play an important role in the analysis of classi£cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a suf£cient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classi£cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines. 1</p><p>3 0.67321426 <a title="9-lda-3" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>Author: Yuan Qi, Tom Minka</p><p>Abstract: Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a “message” to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms. 1</p><p>4 0.67180395 <a title="9-lda-4" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>Author: Noam Shental, Aharon Bar-hillel, Tomer Hertz, Daphna Weinshall</p><p>Abstract: Density estimation with Gaussian Mixture Models is a popular generative technique used also for clustering. We develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure. Equivalence constraints are deﬁned on pairs of data points, indicating whether the points arise from the same source (positive constraints) or from different sources (negative constraints). Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others. For the estimation of model parameters we present a closed form EM procedure which handles positive constraints, and a Generalized EM procedure using a Markov net which handles negative constraints. Using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks, and that our algorithm is preferable to two other suggested methods using the same type of side information.</p><p>5 0.67101824 <a title="9-lda-5" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>Author: Ting-fan Wu, Chih-jen Lin, Ruby C. Weng</p><p>Abstract: Pairwise coupling is a popular multi-class classiﬁcation method that combines together all pairwise comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than two existing popular methods: voting and [3]. 1</p><p>6 0.66778755 <a title="9-lda-6" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>7 0.66381067 <a title="9-lda-7" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>8 0.66378891 <a title="9-lda-8" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>9 0.66287208 <a title="9-lda-9" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>10 0.66139704 <a title="9-lda-10" href="./nips-2003-An_Infinity-sample_Theory_for_Multi-category_Large_Margin_Classification.html">23 nips-2003-An Infinity-sample Theory for Multi-category Large Margin Classification</a></p>
<p>11 0.65933836 <a title="9-lda-11" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>12 0.65784764 <a title="9-lda-12" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>13 0.65739322 <a title="9-lda-13" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>14 0.65660506 <a title="9-lda-14" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>15 0.65542525 <a title="9-lda-15" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>16 0.65070695 <a title="9-lda-16" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>17 0.64920276 <a title="9-lda-17" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>18 0.64885122 <a title="9-lda-18" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>19 0.64590704 <a title="9-lda-19" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>20 0.6457423 <a title="9-lda-20" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
