<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-97" href="#">nips2003-97</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</h1>
<br/><p>Source: <a title="nips-2003-97-pdf" href="http://papers.nips.cc/paper/2376-iterative-scaled-trust-region-learning-in-krylov-subspaces-via-pearlmutters-implicit-sparse-hessian.pdf">pdf</a></p><p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>Reference: <a title="nips-2003-97-reference" href="../nips2003_reference/nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Iterative scaled trust-region learning in Krylov subspaces via Pearlmutter’s implicit sparse Hessian-vector multiply  Eiji Mizutani Department of Computer Science Tsing Hua University Hsinchu, 300 TAIWAN R. [sent-1, score-0.213]
</p><p>2 edu  Abstract The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. [sent-11, score-0.075]
</p><p>3 In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. [sent-12, score-0.06]
</p><p>4 Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. [sent-15, score-0.301]
</p><p>5 Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. [sent-16, score-0.079]
</p><p>6 1  Introduction  Our objective function to be minimized for optimizing the n-dimensional parameter vector „ of an F -output NN model is the sum over all the d data of squared 2 rk 2 . [sent-17, score-0.094]
</p><p>7 Here, m≡F d; r(„) is the mresiduals: E(„) = 1 r(„) 2 = 1 m ri = 1 F 2 2 2 2 2 i=1 k=1 dimensional residual vector composed of all m residual elements: ri (i = 1, . [sent-18, score-0.28]
</p><p>8 , m); and rk the d-dimensional residual vector evaluated at terminal node k. [sent-21, score-0.42]
</p><p>9 The gradient vector and the Hessian matrix of E(„) are given by g ≡ JT r and H ≡ JT J + S, respectively, where J, the m×n (residual) Jacobian matrix of r, is readily obtainable from backpropagation (BP) process, and S is the matrix of second-derivative  terms of r; i. [sent-22, score-0.177]
</p><p>10 Most nonlinear least squares algorithms take adi=1 vantage of information of J or its cross product called the Gauss-Newton (GN) Hessian JT J (or the Fisher information matrix for E(. [sent-25, score-0.131]
</p><p>11 ) in Amari’s natural-gradient learning [1]), which is the important portion of H because inﬂuence of S becomes weaker and weaker as residuals become smaller while learning progresses. [sent-26, score-0.107]
</p><p>12 With multiple F -output nonlinear models (except fully-connected NNs), J is known to have the m × n block angular matrix form (see [7, 6] and references therein). [sent-27, score-0.144]
</p><p>13 In other words, model’s parameters „ (n=F CA + nB in total) can separate as: „T = T T T T T T [„ A |„ B ] =[„ A , · · · , „ A , · · · , „ A |„ B ], where „ A is a vector of the kth subset of 1 F k k CA terminal parameters directly linked to terminal node k (k = 1, · · · ,F ). [sent-29, score-0.402]
</p><p>14 The associated residual Jacobian matrix J can be given in the block-angular form below left, and thus the (full) Hessian matrix H has the n × n sparse block arrow form below right (× denotes some non-zero block) as well as  GN-Hessian JT J: the  A1  × ×  B1 A2 B2 × ×          × ×  . [sent-30, score-0.268]
</p><p>15  × ×  m×n n×n AF  BF  ×  ×  ×  ×  ×  Here in J, Ak and Bk are d × CA and d × nB Jacobian matrices, respectively, of the d-dimensional residual vector rk evaluated at terminal node k. [sent-37, score-0.42]
</p><p>16 Notice that there are F diagonal Ak blocks [because (F − 1)CA terminal parameters excluding „ A have no eﬀect on rk ], and F vertical Bk blocks corresponding to the nB hidden k parameters „B that contribute to minimizing all the residuals rk (k=1, · · · , F ) evaluated at all F terminal nodes. [sent-38, score-0.729]
</p><p>17 Therefore, the posed problem is overdetermined when 1 “m > n” (namely, “d > CA + F nB ”) holds. [sent-39, score-0.117]
</p><p>18 In addition, when the terminal nodes have linear identity functions, terminal parameters „A are linear, and thus all Ak blocks become identical A1 = A2 = · · · = AF , with H + 1 hidden-node outputs (including one constant bias-node output) in each row. [sent-40, score-0.472]
</p><p>19 Notice that H−1 is dense even if H has a nice block-arrow sparsity structure. [sent-42, score-0.079]
</p><p>20 For large-scale problems, Krylov subspace methods, which circumvent the need to perform time-consuming and memory-intensive direct matrix factorizations, can be employed to realize what we call iterative batch-mode learning. [sent-43, score-0.095]
</p><p>21 1  Outer iteration process in trust-region methods  One might consider a convex combination of the Cauchy step ∆„Cauchy and the Newton step ∆„ N ewton such as (using a scalar parameter h): def  ∆„ Dogleg = (1 − h)∆„Cauchy + h∆„N ewton ,  (2)  which is known as the dogleg step [4, 9]. [sent-47, score-0.85]
</p><p>22 This step yields a good approximate solution to the so-called “scaled 2-norm” or “M -norm” trust-region subproblem (e. [sent-48, score-0.139]
</p><p>23 The posed constrained quadratic minimization can be solved with Lagrange multiplier µ: If ∆„ is a solution to the posed problem, then ∆„ satisﬁes the formula: (H + µM)∆„ = −g, with µ( ∆„ M − R) = 0, µ ≥ 0, and H + µM positive semideﬁnite. [sent-56, score-0.156]
</p><p>24 In nonlinear least squares context, the nonnegative scalar parameter µ is known as the Levenberg-Marquardt parameter. [sent-57, score-0.152]
</p><p>25 When µ = 0 (namely, R ≥ ∆„ N ewton M ), the trust-region step ∆„ becomes the Newton def step ∆„ N ewton = −H−1 g, and, as µ increases (i. [sent-58, score-0.586]
</p><p>26 , as R decreases), ∆„ gets closer to def the (full) Cauchy step ∆„ Cauchy : ∆„ Cauchy = − gT M−1 g/gT M−1 HM−1 g M−1 g. [sent-60, score-0.19]
</p><p>27 When R < ∆„ Cauchy M , the trust-region step ∆„ reduces to the restricted Cauchy def step ∆„ RC = −(R/ ∆„ Cauchy M )∆„ Cauchy . [sent-61, score-0.27]
</p><p>28 If ∆„ Cauchy M < R < ∆„ N ewton M , ∆„ is the “dogleg step,” intermediate between ∆„ Cauchy and ∆„ N ewton , as shown in Eq. [sent-62, score-0.316]
</p><p>29 (2), where scalar h (0 < h < 1) is the positive root of s + hp M = R: √ −sT Mp+ (sT Mp)2 +pT Mp(R2 −sT Ms) , (5) h= pT Mp def  def  with s = ∆„Cauchy and p = ∆„N ewton − ∆„Cauchy (when pT g < 0). [sent-63, score-0.434]
</p><p>30 In this way, the trial step ∆„ is subject to trust-region regularization. [sent-64, score-0.08]
</p><p>31 In large-scale problems, the linear-equation solution sequence {∆„k } is generated iteratively while seeking a trial step ∆„ in the inner iteration process, and the parameter sequence {„i }, whose two consecutive elements are denoted by „now and „ next , is produced by the outer iteration (i. [sent-65, score-0.359]
</p><p>32 For this purpose, the trust-region methods compute the gradient vector in batch mode or with (sufﬁciently large) data block (i. [sent-69, score-0.24]
</p><p>33 2  Inner iteration process with truncated preconditioned linear CG  We employ a preconditioned conjugate gradient (PCG) (among many Krylov subspace methods; see Section 6. [sent-73, score-0.31]
</p><p>34 6 in [3] and Chapter 5 in [2]) with our symmetric  positive deﬁnite preconditioner M for solving the M -norm trust-region subproblem (3). [sent-74, score-0.194]
</p><p>35 This is the truncated PCG (also known as Steihaug-Toint CG) applicable even to nonconvex problems for solving inexactly the Newton formula by the inner iterative process below (see pp. [sent-75, score-0.227]
</p><p>36 202–218 in [2]) based on the standard PCG algorithm (e. [sent-77, score-0.042]
</p><p>37 , see page 317 in [3]): Algorithm 1: The inner iteration process via preconditioned CG. [sent-79, score-0.299]
</p><p>38 Otherwise, compute h (> 0) such that ∆„ k−1 + hdk M = R, and terminate with ∆„ = ∆„ k−1 + hdk . [sent-87, score-0.533]
</p><p>39 (6) If ∆„ k M < R, go onto Step 6; else terminate with ∆„ = ∆„ k M  6. [sent-93, score-0.083]
</p><p>40 If k < klimit , set k = k + 1 and return to Step 2. [sent-105, score-0.09]
</p><p>41 (5) for ∆„ = ∆„ k−1 + hdk such that ∆„ k−1 + hdk M = R, but both computations become identical if R ≤ ∆„ Cauchy M ; otherwise, Eq. [sent-111, score-0.45]
</p><p>42 , stops at inner iteration k) when one of the next four conditions holds: (A) dT Hdk ≤ 0, (B) ∆„ k M ≥ R, (C) H∆„ k + g 2 ≤ ξ g 2 , (D) k=klimit . [sent-115, score-0.182]
</p><p>43 (7) k Condition (D) at Step 10 is least likely to be met since there would be no prior knowledge about preset limits klimit to inner iterations (usually, klimit =n). [sent-116, score-0.302]
</p><p>44 As long as dT Hdk > 0 holds, PCG works properly until the CG-trajectory hits the trustk region boundary [Condition (B) at Step 5], or till the 2-norm linear-system residuals become small [Condition (C) at Step 6], where ξ can be ﬁxed (e. [sent-117, score-0.107]
</p><p>45 That is, dk is a direction of zero or negative curvature; a typical exploitation of non-positive curvature is to set ∆„ equal to the “step to the trust-region boundary along that curvature segment (in Step 3)” as a model minimizer in the trust region. [sent-122, score-0.25]
</p><p>46 In this way, the terminated kth CG step yields an approximate solution to the trust-region subproblem (3), and it belongs to the Krylov sub1 1 1 1 1 1 1 space span {−M− 2 g, −(M− 2 HM− 2 )M− 2 g, . [sent-123, score-0.139]
</p><p>47 Since the matrix-vector product Hdk at Step 2 is dominant in operation cost of the entire inner-outer process, we can employ Pearlmutter’s method with no H explicitly required. [sent-128, score-0.036]
</p><p>48 To better understand the method, we ﬁrst describe a straightforward implicit sparse matrix-vector multiply when H = JT J; it evaluates JT Jdi (without forming JT J) in two-step implicit matrix-vector product as z=JT (Jdi ), exploiting block-angular J in Eq. [sent-129, score-0.183]
</p><p>49 , matrix-free) sparse matrix-vector multiplication step with an F -output NN model at inner iteration i starting with z = 0: for p = 1 to d (i. [sent-134, score-0.299]
</p><p>50 2 Here, Step (a) costs at least 2dn (see details in [8]); Step (b) costs at least 2mlu , where m=F d and lu =CA +nB < n=F CA +nB ; and Step (c) costs 4mlu ; overall, Algorithm 2 costs O(mlu ), linear in F . [sent-138, score-0.226]
</p><p>51 Note that if sparsity is ignored, the cost becomes O(mn), quadratic in F since mn = F d(F CA +nB ). [sent-139, score-0.079]
</p><p>52 Algorithm 2 can extract explicitly F pairs of row vectors (aT and bT ) of J (with F lu storage) on each datum, making it easier to apply other numerical linear algebra approaches such as preconditioning to reduce the number of inner iterations. [sent-140, score-0.24]
</p><p>53 Yet, if the row vectors are not needed explicitly, then Pearlmutter’s method is more eﬃcient, calculating αk [see Step (c)] in its forward pass (i. [sent-141, score-0.034]
</p><p>54 When H = JT J, it is easy to simplify its backward pass (see Eq. [sent-146, score-0.034]
</p><p>55 4) on page 152 in [11]), just by eliminating the terms involving residuals r and second-derivatives of node functions f (. [sent-148, score-0.202]
</p><p>56 ), so as to multiply vectors ak and bk through by scalar αk implicitly. [sent-149, score-0.369]
</p><p>57 Since mlu − dn = dF (CA + nB ) − d(F CA + nB ) = d(F − 1)nB , Pearlmutter’s method can be up to F times faster than Algorithm 2. [sent-151, score-0.281]
</p><p>58 (1), right] in the essentially same way as the standard BP deals with block-angular sparsity of J [see Eq. [sent-153, score-0.079]
</p><p>59 In Algorithms D and E, Algorithm 2 was only employed for obtaining a diagonal preconditioner M = diag(JT J) (or Jacobi preconditioner ) for Algorithm 1, whereas in Algorithms B and C, no preconditioning (M = I) was applied. [sent-160, score-0.338]
</p><p>60 The performance comparisons were made with a nonlinear regression task and a classiﬁcation benchmark, the letter recognition problem, from the UCI machine learning repository. [sent-161, score-0.101]
</p><p>61 The table below shows the results averaged over 20 trials with a single 16-82-10 MLP [n=2,224 (CA =83;nB =1,394;lu =1,477); hence, mlu =6. [sent-168, score-0.113]
</p><p>62 Clearly, the posed regression task is nontrivial because Algorithm A, online-BP, took roughly six days (averaged over only ten trials), nearly 280 (=8748. [sent-171, score-0.128]
</p><p>63 In generalization performance, all the posed algorithms were more or less equivalent. [sent-174, score-0.078]
</p><p>64 Model Algorithm Total time (min) Stopped epoch Time/epoch (sec) Inner itr. [sent-175, score-0.111]
</p><p>65 Remarkably, the time per inner iteration of Algorithm E did not diﬀer much from Algorithms C and D owing to Pearlmutter’s method; in fact, given preconditioner M, Algorithm E merely needed about 1. [sent-223, score-0.389]
</p><p>66 3 times more ﬂops ∗ per inner iteration than Algorithms C and D did, although Algorithm B needed nearly 3. [sent-224, score-0.219]
</p><p>67 For improving single-MLP performance, one might employ two layers of hidden nodes (rather than one large hidden layer; see the letter problem below), which increases nB while reducing nA , rendering Algorithm 2 less eﬃcient (i. [sent-227, score-0.087]
</p><p>68 Alternatively, one might introduce direct connections between the input and terminal output layers, which increases CA , the column size of Ak , retaining nice parameter separability. [sent-230, score-0.178]
</p><p>69 Note that each expert learns “residuals” rather than “desired outputs” (unlike in the committee method below) in the sense that only the ﬁnal combined outputs y must come close to the desired ones t. [sent-236, score-0.194]
</p><p>70 That is, there are strong coupling eﬀects (see page 80 in [5]) among all experts; hence, it is crucial to consider the global Hessian across all experts to optimize them simultaneously [7]. [sent-237, score-0.049]
</p><p>71 k k k k k k Here, the residual Jacobian portion for the parameters of the integrating unit was omitted because they were merely ﬁne-tuned with a steepest-descent type method owing to our knowledge-based design for input-partition to avoid (too many) local experts. [sent-240, score-0.174]
</p><p>72 Due to localized parameter-tunings, our ﬁve-MLP mixtures dn model was better in learning; see faster learning in table above. [sent-243, score-0.168]
</p><p>73 In particular, our model with Algorithm D worked 353 (≈ 123. [sent-244, score-0.053]
</p><p>74 9) times faster than with Algorithm A that took 123. [sent-247, score-0.094]
</p><p>75 9) times faster than the single MLP with Algorithm A. [sent-250, score-0.094]
</p><p>76 }-operator of Pearlmutter’s method is readily applicable; for instance, at terminal node k (k=1, · · · ,F ): R{rk } = R{yk } = Z R{oi,k }wi + Z R{wi }oi,k , where i i each R{oi,k } yields αk [see Algorithm 2(c)] for each expert-MLP i (i = 1, · · · ,Z ). [sent-252, score-0.224]
</p><p>77 The second letter classiﬁcation benchmark problem involves 16 inputs (features) and 26 outputs (alphabets) with 16,000 training data (F =26; d=16,000; m=416,000) plus 4,000 test data. [sent-253, score-0.128]
</p><p>78 We implemented block-mode learning (as well as batch mode) just by splitting the training data set into two or four equally-sized data blocks, and each data block alone is employed for Algorithms 1 and 2 except for computing ρ in Eq. [sent-257, score-0.148]
</p><p>79 Notice that two-block mode learning scheme updates model’s parameters „ twice per epoch, whereas onlineBP updates them on each datum (i. [sent-260, score-0.15]
</p><p>80 We observed that possible redundancy in the data set appeared to help reduce the number of inner iterations, speeding up our iterative batch-mode learning; therefore, we did not use preconditioning. [sent-263, score-0.182]
</p><p>81 The next table shows the average performance (over ten trials) when the best test-set performance was obtained by epoch 1,000 with online-BP (i. [sent-264, score-0.161]
</p><p>82 , Algorithm A) and by epoch 50 with Algorithm C in three learning modes: Average results Total time (min) Stopped epoch Time/epoch (sec) Avg. [sent-266, score-0.222]
</p><p>83 3%  On average, Algorithm C in four-block mode worked about three (≈ 63. [sent-299, score-0.153]
</p><p>84 4)  times faster than online-BP, and thus can work faster than batch-mode nonlinearCG algorithms, since, reported in [12], online-BP worked faster than nonlinear-CG. [sent-301, score-0.261]
</p><p>85 Here, we also tested the committee methods (see Chap. [sent-302, score-0.117]
</p><p>86 8 in [13]) that merely combined all (equally-weighted) outputs of the ten MLPs, which were optimized independently in this experiment. [sent-303, score-0.163]
</p><p>87 The committee error was better than the average error, as expected. [sent-304, score-0.117]
</p><p>88 Intriguingly, our block-mode learning schemes introduced small (harmless) bias, improving the test-data performance; speciﬁcally, the two-block mode yielded the best test error rate 2. [sent-305, score-0.1]
</p><p>89 4  Conclusion and Future Directions  Pearlmutter’s method can construct Krylov subspaces eﬃciently for implementing iterative batch- or block-mode learning. [sent-307, score-0.098]
</p><p>90 In our simulation examples, the simpler version of Pearlmutter’s method (see Algorithms C and D) worked excellently. [sent-308, score-0.053]
</p><p>91 Beside the simple committee method, it would be worth examining our algorithms for implementing other statistical learning methods (e. [sent-310, score-0.117]
</p><p>92 “On structure-exploiting trust-region regularized nonlinear least squares algorithms for neural-network learning. [sent-359, score-0.096]
</p><p>93 “On separable nonlinear least squares algorithms for neuro-fuzzy modular network learning. [sent-366, score-0.155]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cauchy', 0.313), ('jt', 0.299), ('nb', 0.266), ('hdk', 0.225), ('pearlmutter', 0.219), ('terminal', 0.178), ('ewton', 0.158), ('krylov', 0.158), ('newton', 0.141), ('preconditioner', 0.135), ('ak', 0.128), ('bk', 0.125), ('hessian', 0.123), ('inner', 0.122), ('committee', 0.117), ('eiji', 0.113), ('mlu', 0.113), ('epoch', 0.111), ('def', 0.11), ('residuals', 0.107), ('residual', 0.102), ('mode', 0.1), ('mlp', 0.098), ('rk', 0.094), ('klimit', 0.09), ('mizutani', 0.09), ('pcg', 0.09), ('ca', 0.089), ('terminate', 0.083), ('step', 0.08), ('sparsity', 0.079), ('posed', 0.078), ('outputs', 0.077), ('dn', 0.074), ('cg', 0.071), ('hm', 0.071), ('dk', 0.068), ('dogleg', 0.068), ('jdi', 0.068), ('preconditioned', 0.068), ('preconditioning', 0.068), ('jacobian', 0.066), ('mp', 0.066), ('curvature', 0.066), ('iterative', 0.06), ('iteration', 0.06), ('multiply', 0.06), ('block', 0.059), ('modular', 0.059), ('subproblem', 0.059), ('faster', 0.057), ('scalar', 0.056), ('james', 0.055), ('color', 0.053), ('worked', 0.053), ('letter', 0.051), ('datum', 0.05), ('lu', 0.05), ('trust', 0.05), ('ten', 0.05), ('nonlinear', 0.05), ('page', 0.049), ('batch', 0.048), ('node', 0.046), ('squares', 0.046), ('siam', 0.045), ('truncated', 0.045), ('demmel', 0.045), ('nns', 0.045), ('pseudoresiduals', 0.045), ('nn', 0.045), ('costs', 0.044), ('implicit', 0.043), ('dt', 0.043), ('bt', 0.043), ('algorithm', 0.042), ('alone', 0.041), ('ectance', 0.039), ('obtainable', 0.039), ('mlps', 0.039), ('overdetermined', 0.039), ('pth', 0.039), ('blocks', 0.039), ('ri', 0.038), ('subspaces', 0.038), ('mixtures', 0.037), ('sparse', 0.037), ('outer', 0.037), ('times', 0.037), ('employ', 0.036), ('merely', 0.036), ('rmse', 0.036), ('owing', 0.036), ('scaled', 0.035), ('matrix', 0.035), ('ve', 0.034), ('pass', 0.034), ('recipe', 0.033), ('pt', 0.033), ('gradient', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="97-tfidf-1" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>2 0.13658565 <a title="97-tfidf-2" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>3 0.099303804 <a title="97-tfidf-3" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>Author: Cynthia Rudin, Ingrid Daubechies, Robert E. Schapire</p><p>Abstract: In order to understand AdaBoost’s dynamics, especially its ability to maximize margins, we derive an associated simpliﬁed nonlinear iterated map and analyze its behavior in low-dimensional cases. We ﬁnd stable cycles for these cases, which can explicitly be used to solve for AdaBoost’s output. By considering AdaBoost as a dynamical system, we are able to prove R¨ tsch and Warmuth’s conjecture that AdaBoost may fail a to converge to a maximal-margin combined classiﬁer when given a ‘nonoptimal’ weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost∗ and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv. 1</p><p>4 0.091125593 <a title="97-tfidf-4" href="./nips-2003-Subject-Independent_Magnetoencephalographic_Source_Localization_by_a_Multilayer_Perceptron.html">182 nips-2003-Subject-Independent Magnetoencephalographic Source Localization by a Multilayer Perceptron</a></p>
<p>Author: Sung C. Jun, Barak A. Pearlmutter</p><p>Abstract: We describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic (MEG) measurements in real time. At its core is a multilayer perceptron (MLP) trained to map sensor signals and head position to dipole location. Including head position overcomes the previous need to retrain the MLP for each subject and session. The training dataset was generated by mapping randomly chosen dipoles and head positions through an analytic model and adding noise from real MEG recordings. After training, a localization took 0.7 ms with an average error of 0.90 cm. A few iterations of a Levenberg-Marquardt routine using the MLP’s output as its initial guess took 15 ms and improved the accuracy to 0.53 cm, only slightly above the statistical limits on accuracy imposed by the noise. We applied these methods to localize single dipole sources from MEG components isolated by blind source separation and compared the estimated locations to those generated by standard manually-assisted commercial software. 1</p><p>5 0.087727889 <a title="97-tfidf-5" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>Author: Léon Bottou, Yann L. Cun</p><p>Abstract: We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented. 1</p><p>6 0.07253772 <a title="97-tfidf-6" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>7 0.070731714 <a title="97-tfidf-7" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>8 0.070549622 <a title="97-tfidf-8" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>9 0.05867663 <a title="97-tfidf-9" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>10 0.05531064 <a title="97-tfidf-10" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>11 0.052523788 <a title="97-tfidf-11" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>12 0.052457552 <a title="97-tfidf-12" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>13 0.052388676 <a title="97-tfidf-13" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>14 0.052066579 <a title="97-tfidf-14" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>15 0.050928805 <a title="97-tfidf-15" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>16 0.05034915 <a title="97-tfidf-16" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>17 0.050250087 <a title="97-tfidf-17" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>18 0.048739508 <a title="97-tfidf-18" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>19 0.047740594 <a title="97-tfidf-19" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>20 0.04742486 <a title="97-tfidf-20" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.179), (1, -0.005), (2, -0.017), (3, -0.037), (4, 0.013), (5, -0.021), (6, 0.065), (7, 0.059), (8, 0.033), (9, 0.022), (10, -0.016), (11, -0.074), (12, 0.083), (13, 0.014), (14, -0.098), (15, -0.052), (16, -0.035), (17, -0.004), (18, 0.042), (19, 0.009), (20, 0.018), (21, 0.028), (22, -0.06), (23, -0.035), (24, -0.105), (25, -0.09), (26, -0.16), (27, 0.045), (28, -0.061), (29, -0.053), (30, -0.112), (31, 0.048), (32, 0.139), (33, 0.015), (34, -0.013), (35, 0.023), (36, -0.247), (37, 0.004), (38, 0.084), (39, 0.054), (40, -0.075), (41, 0.109), (42, -0.201), (43, -0.006), (44, 0.103), (45, 0.042), (46, 0.078), (47, -0.132), (48, -0.047), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93536788 <a title="97-lsi-1" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>2 0.48671526 <a title="97-lsi-2" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>Author: Léon Bottou, Yann L. Cun</p><p>Abstract: We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented. 1</p><p>3 0.48293158 <a title="97-lsi-3" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>Author: Bob Ricks, Dan Ventura</p><p>Abstract: Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms. 1</p><p>4 0.48160866 <a title="97-lsi-4" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>5 0.45810121 <a title="97-lsi-5" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><p>6 0.45340511 <a title="97-lsi-6" href="./nips-2003-Subject-Independent_Magnetoencephalographic_Source_Localization_by_a_Multilayer_Perceptron.html">182 nips-2003-Subject-Independent Magnetoencephalographic Source Localization by a Multilayer Perceptron</a></p>
<p>7 0.4476622 <a title="97-lsi-7" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>8 0.41693696 <a title="97-lsi-8" href="./nips-2003-Can_We_Learn_to_Beat_the_Best_Stock.html">44 nips-2003-Can We Learn to Beat the Best Stock</a></p>
<p>9 0.39190471 <a title="97-lsi-9" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>10 0.3803736 <a title="97-lsi-10" href="./nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models.html">39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</a></p>
<p>11 0.36137569 <a title="97-lsi-11" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>12 0.34606242 <a title="97-lsi-12" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>13 0.31065997 <a title="97-lsi-13" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>14 0.30577523 <a title="97-lsi-14" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>15 0.30089489 <a title="97-lsi-15" href="./nips-2003-Perception_of_the_Structure_of_the_Physical_World_Using_Unknown_Multimodal_Sensors_and_Effectors.html">154 nips-2003-Perception of the Structure of the Physical World Using Unknown Multimodal Sensors and Effectors</a></p>
<p>16 0.29973406 <a title="97-lsi-16" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>17 0.29670808 <a title="97-lsi-17" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>18 0.29084972 <a title="97-lsi-18" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>19 0.28713879 <a title="97-lsi-19" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>20 0.28079227 <a title="97-lsi-20" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (11, 0.022), (29, 0.011), (30, 0.016), (35, 0.084), (48, 0.019), (53, 0.095), (55, 0.331), (69, 0.023), (71, 0.052), (76, 0.04), (85, 0.056), (91, 0.117), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8884418 <a title="97-lda-1" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>same-paper 2 0.81422323 <a title="97-lda-2" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>3 0.65462714 <a title="97-lda-3" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>Author: Maneesh Sahani</p><p>Abstract: Signiﬁcant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a signiﬁcant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generativemodelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way. 1</p><p>4 0.49900207 <a title="97-lda-4" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>5 0.49775431 <a title="97-lda-5" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>6 0.49454817 <a title="97-lda-6" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>7 0.49425796 <a title="97-lda-7" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>8 0.4941268 <a title="97-lda-8" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>9 0.49409962 <a title="97-lda-9" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>10 0.49370372 <a title="97-lda-10" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>11 0.49266121 <a title="97-lda-11" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>12 0.49099928 <a title="97-lda-12" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>13 0.49064729 <a title="97-lda-13" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>14 0.49033755 <a title="97-lda-14" href="./nips-2003-Large_Scale_Online_Learning.html">102 nips-2003-Large Scale Online Learning</a></p>
<p>15 0.4896397 <a title="97-lda-15" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>16 0.48874408 <a title="97-lda-16" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>17 0.48785743 <a title="97-lda-17" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>18 0.48752651 <a title="97-lda-18" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>19 0.48695487 <a title="97-lda-19" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>20 0.48598006 <a title="97-lda-20" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
