<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-116" href="#">nips2003-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2003-116-pdf" href="http://papers.nips.cc/paper/2430-linear-program-approximations-for-factored-continuous-state-markov-decision-processes.pdf">pdf</a></p><p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>Reference: <a title="nips-2003-116-reference" href="../nips2003_reference/nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu    ¡  Abstract Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. [sent-3, score-0.628]
</p><p>2 In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). [sent-4, score-0.51]
</p><p>3 We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. [sent-5, score-0.096]
</p><p>4 We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. [sent-6, score-0.095]
</p><p>5 The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. [sent-7, score-0.16]
</p><p>6 1  Introduction  Markov decision processes (MDPs) offer an elegant mathematical framework for representing and solving decision problems in the presence of uncertainty. [sent-8, score-0.136]
</p><p>7 While standard solution techniques, such as value and policy iteration, scale-up well in terms of the number of states, the state space of more realistic MDP problems is factorized and thus becomes exponential in the number of state components. [sent-9, score-0.474]
</p><p>8 Much of the recent work in the AI community has focused on factored structured representations of ﬁnite-state MDPs and their efﬁcient solutions. [sent-10, score-0.312]
</p><p>9 Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with discrete state components. [sent-11, score-0.628]
</p><p>10 The approach uses a linear combination of local feature functions to model the value function. [sent-12, score-0.119]
</p><p>11 The coefﬁcients of the model are ﬁt using linear program methods. [sent-13, score-0.093]
</p><p>12 In this work we show how the same set of linear programming (LP) methods can be extended also to solutions of factored continuous-state MDPs. [sent-16, score-0.486]
</p><p>13 To address this problem, CMDPs and their solutions are usually approximated and solved either through state space discretization or by ﬁtting a surrogate and (often much simpler) parametric value function model. [sent-18, score-0.308]
</p><p>14 2 The disadvantage of discretizations is their accu1 2  We assume that action spaces stay ﬁnite. [sent-20, score-0.105]
</p><p>15 racy and the fact that higher accuracy solutions are paid for by the exponential increase in the complexity of discretizations. [sent-23, score-0.089]
</p><p>16 On the other hand, parametric value-function approximations may become unstable when combined with the dynamic programming methods and least squares error [1]. [sent-24, score-0.174]
</p><p>17 The ALP solution that is developed in this work eliminates the disadvantages of discretization and function approximation approaches while preserving their good properties. [sent-25, score-0.096]
</p><p>18 It extends the approach of Trick and Zin [17] to factored multidimensional continuous state spaces. [sent-26, score-0.472]
</p><p>19 Its main beneﬁts are good running time performance, stability of the solution, and good quality policies. [sent-27, score-0.081]
</p><p>20 In this work we study factored CMDPs with state spaces restricted to . [sent-30, score-0.481]
</p><p>21 We show that the solution for such a model can be approximated by an ALP with inﬁnite number of constraints that decompose locally. [sent-31, score-0.164]
</p><p>22 In addition, we show that by choosing transition models based on beta densities (or their mixtures) and basis functions deﬁned by products of polynomials one obtains an ALP in which both the objective function and constraints are in closed form. [sent-32, score-0.44]
</p><p>23 In order to alleviate the problem of inﬁnite number of constraints, we develop and study approximation based on constraint sampling [5, 6]. [sent-33, score-0.125]
</p><p>24 We show that even under a relatively simple random constraint sampling we are able to very quickly calculate solutions of a high quality that are comparable to other existing CMDP solution methods. [sent-34, score-0.253]
</p><p>25 First we review ﬁnite-state MDPs and approximate linear programming (ALP) methods developed for their factored reﬁnements. [sent-36, score-0.446]
</p><p>26 Next we show how to extend the LP approximations to factored continuous-state MDPs and discuss assumptions underlying the model. [sent-37, score-0.352]
</p><p>27   ¥ ¥  ¥   © § ¥ 876£ 54210)'& ¢ 3  (  (  %      3 CBA@9"  (  %  Given an MDP our objective is to ﬁnd the policy maximizing the inﬁnite, where is a discount factor horizon discounted reward criterion: and is a reward obtained in step . [sent-41, score-0.251]
</p><p>28 Given the value function , the optimal policy is deﬁned by the action optimizing Eqn 1. [sent-44, score-0.189]
</p><p>29 h    i    h  h   Vp  # i b  E  where  (1)    h  d  i  D  Methods for solving an MDP include value iteration, policy iteration, and linear programming [12, 2]. [sent-45, score-0.277]
</p><p>30 In the linear program (LP) formulation we solve the following problem:  h ¥ £ ¦fe6y0F d #  ¥ i  "  #  i    #  ¥ i R   i  i    a  h  # i Fb  for every state  are treated as variables. [sent-46, score-0.256]
</p><p>31  ¥ i hg     h # i b  where values of  h  subject to:  (2)  # i b  minimize  Factorizations and LP approximations In factored MDPs, the state space is deﬁned in terms of state variables . [sent-47, score-0.659]
</p><p>32 As a result, the state space becomes exponential in the number of variables. [sent-48, score-0.153]
</p><p>33 Compact parameterizations of MDPs based on dynamic belief networks [7] and decomposable reward functions are routinely used to represent such MDPs more efﬁciently. [sent-49, score-0.12]
</p><p>34 To address this problem Koller and Parr [9] and Guestrin at al [8] propose to use a linear model [13]:      ¡  m  #  W  m  W  i    ¥ k k k ¥ j flffRfB  ¥ i    W   p # i ! [sent-51, score-0.073]
</p><p>35 Here are the linear coefﬁcients to be found (ﬁt) and s denote feature functions deﬁned over subsets of state variables. [sent-53, score-0.248]
</p><p>36 Note that while the objective function can be computed efﬁciently, the number of constraints one has to satisfy remains exponential in the number of random variables. [sent-55, score-0.139]
</p><p>37 However, only a subset of these constraints becomes active and affect the solution. [sent-56, score-0.114]
</p><p>38 Guestrin et al [8] showed how to ﬁnd active constraints by solving a cost network problem. [sent-57, score-0.23]
</p><p>39 An alternative approach for ﬁnding active constraints was devised by Schuurmans and Patrascu [15]. [sent-59, score-0.172]
</p><p>40 The idea is to greedily search for maximally violated constraints which can be done efﬁciently by solving a linear optimization problem. [sent-61, score-0.195]
</p><p>41 These constraints are included in the linear program and the process is repeated until no violated constraints are found. [sent-62, score-0.292]
</p><p>42 #  ¥ i  y"  W    w W   i    #  6¥    w  i     w  "  #  ¥    w  i     w  "  i  i   X S  3  Factored continuous-state MDPs  Many stochastic controlled processes are more naturally deﬁned using continuous state variables. [sent-64, score-0.206]
</p><p>43 In this work we focus on continuous-state MDPs (CMDPs) where state spaces are restricted to . [sent-65, score-0.169]
</p><p>44 3 We assume factored representations where transition probabilities are deﬁned in terms of densities over state variable subspaces: where and denote the current and previous states. [sent-66, score-0.583]
</p><p>45 Rewards are represented compactly over subsets of state variables, similarly to factored ﬁnite-state MDPs. [sent-67, score-0.466]
</p><p>46 The solutions attempt to replace the value function or the optimal policy with a ﬁnite approximation. [sent-71, score-0.218]
</p><p>47 A typical solution is to discretize the state space to a set of grid points and approximate value functions over such points. [sent-73, score-0.342]
</p><p>48 Unfortunately, classic grid algorithms scale up exponentially with the number of state variables [4]. [sent-74, score-0.248]
</p><p>49 Let be a set of grid points over the state space . [sent-75, score-0.225]
</p><p>50 Then the Bellman operator can be approximated with an operator that is restricted to grid points . [sent-76, score-0.206]
</p><p>51 Equation 4 applied to grid points deﬁnes a ﬁnite state MDP with states. [sent-78, score-0.225]
</p><p>52 Convergence properties of the approximation scheme in Equation 4 for random or pseudo-random samples were analyzed by Rust [14]. [sent-80, score-0.092]
</p><p>53 An alternative way to solve a continuous-state with an appropriate parametric MDP is to approximate the optimal value function function model [3]. [sent-82, score-0.136]
</p><p>54 The parameters of the model are ﬁtted iteratively by applying one step Bellman backups to a ﬁnite set of state points arranged on a ﬁxed grid or obtained through Monte Carlo sampling. [sent-83, score-0.225]
</p><p>55 2 LP approximations of CMDPs Our objective is to develop an alternative to the above solutions that is based on ALP techniques and that takes advantage of model factorizations. [sent-88, score-0.165]
</p><p>56 It is easy to see that for a general continuous-state model the exact solution cannot be formulated as a linear program as was done in Equation 2 since the number of states is inﬁnite. [sent-89, score-0.159]
</p><p>57 However, using linear representations of the value functions we need to optimize only over a ﬁnite number of weights combining feature functions. [sent-90, score-0.119]
</p><p>58 First, the integrals may be improper and not computable. [sent-93, score-0.088]
</p><p>59 In the following we give   i  satisfy inﬁnite number of constraints (for all values of solutions to both issues. [sent-95, score-0.153]
</p><p>60 Closed form solutions Integrals in the objective function and constraints depend on the choice of transition models and basis functions. [sent-96, score-0.259]
</p><p>61 We want all these integrals to be proper Riemannian integrals. [sent-97, score-0.088]
</p><p>62 To this point, we have identiﬁed conjugate classes of transition models and basis functions leading to closed form expressions. [sent-99, score-0.125]
</p><p>63 w    Ì ¥ w  8#  j  i   w    i   Ì ¥ w  «# w    j  w    i   w  Ì   i  i   w        Ë Ê È »r'¥ É p #      È F  i   is the parent set of a variable under action , and deﬁne the parameters of the beta model. [sent-100, score-0.171]
</p><p>64 £ CÎ# Í  © § ¥ £ 876¤¢  w  Ì  i          Ñ ¹ Ð  Ï  w   i  w   d  Ï © § ¥ £ T¨¦¤¢  where for  we propose to use beta ¥ # «#  Beta transitions. [sent-101, score-0.133]
</p><p>65 To parameterize the transition model over densities or their mixtures. [sent-102, score-0.14]
</p><p>66 The beta transition is deﬁned as:  i  Feature functions. [sent-103, score-0.208]
</p><p>67 For example, assuming features with products of state , the ALP formulation becomes:     w  minimize    where variables:  º ¹ ¸  ALP solution. [sent-105, score-0.163]
</p><p>68 Existing ALP methods for factored ﬁnite-state MDPs search for this subset more efﬁciently by taking advantage of local constraint decompositions and various heuristics. [sent-107, score-0.381]
</p><p>69 However, at the end these methods always rely on the fact the decompositions are deﬁned on a ﬁnite state subspace. [sent-108, score-0.164]
</p><p>70 Unfortunately, constraints in our model decompose over smaller but still continuous subspaces, so the existing solutions for the ﬁnite-state MDPs cannot be applied directly. [sent-109, score-0.245]
</p><p>71 To avoid the problem of continuous state spaces we approximate the ALP solution using a ﬁnite set of constraints deﬁned by a ﬁnite set of state space points and actions in . [sent-111, score-0.502]
</p><p>72 These state space points can be deﬁned by regular grids on state subspaces or via random sampling of states . [sent-112, score-0.38]
</p><p>73 For the ﬁnite state spaces such a technique has been devised and analyzed by de Farias and Van Roy [5]. [sent-123, score-0.259]
</p><p>74 4 However, despite many possible heuristic improvements, we believe that the crucial beneﬁt comes from the ALP formulation that “ﬁts” the linear model and subsequent constraint and subspace decompositions. [sent-125, score-0.129]
</p><p>75 4  Experiments  To test the ALP method we use a continuous-state modiﬁcation of the computer network example proposed by Guestrin et al [8]. [sent-126, score-0.071]
</p><p>76 The state of a machine is represented by a number between 0 and 1 reﬂecting its processing capacity (the ability to process tasks). [sent-129, score-0.153]
</p><p>77 The network performance can be controlled through activities of a human operator: the operator can attend a machine (one at time) or do nothing. [sent-130, score-0.128]
</p><p>78 The transition model represents the dynamics of the computer network. [sent-135, score-0.075]
</p><p>79 The model is factorized and deﬁned in terms of beta densities: , where is the current state of the th describes the previous-step state of the computers affecting . [sent-136, score-0.446]
</p><p>80 We use: computer, and and for transitions when the human does not attend the computer, and and when the operator is present at the computer. [sent-137, score-0.119]
</p><p>81 To deﬁne the ALP approximation, we used a linear combination of linear (for every node) and quadratic (for every link) feature functions. [sent-142, score-0.104]
</p><p>82 To demonstrate the practical beneﬁt of the approach we have compared it to the grid-based approximation (Equation 4) and leastsquare value iteration approach (with the same linear value function model as in the ALP). [sent-143, score-0.122]
</p><p>83 Figure 2a illustrates the average quality (value) of a policy obtained by different approximation methods while varying the number of samples. [sent-151, score-0.197]
</p><p>84 The average is computed over 30 solutions obtained for 30 different sample sets and 100 different (random) start states. [sent-152, score-0.067]
</p><p>85 Figure 2b illustrates the scale-up potential of the methods in terms of running times. [sent-154, score-0.073]
</p><p>86 The ALP outperformed the grid-based approach (GMDP) in both the policy quality and running times. [sent-161, score-0.181]
</p><p>87 The gap in the policy quality was more pronounced for smaller sample sizes. [sent-162, score-0.135]
</p><p>88 This can be explained by the ability of the model to “cover” complete state space as opposed to individual grid points. [sent-163, score-0.225]
</p><p>89 Better running times for the ALP can be explained by the fact that the number of free variables to be optimized is ﬁxed (they are equal to weights ), while in grid methods free variables correspond to grid samples and their number grows linearly. [sent-164, score-0.303]
</p><p>90 î  5  Conclusions  We have extended the application of linear program approximation methods and their beneﬁts to factored MDPs with continuous states. [sent-165, score-0.469]
</p><p>91 5 We have proposed a factored transition model based on beta densities and identiﬁed feature functions that match well such a model. [sent-166, score-0.642]
</p><p>92 compared to grid methods and provides a better way of “smoothing” value function to unseen examples; (4) its running time scales up better than grid methods. [sent-168, score-0.259]
</p><p>93 First, the random sampling of constraints can be improved using various heuristics. [sent-171, score-0.14]
</p><p>94 We report results of some heuristic solutions in a separate work [10]. [sent-172, score-0.067]
</p><p>95 Second, we did not give any complexity bounds for the random constraint sampling approach. [sent-173, score-0.09]
</p><p>96 On constraint sampling for the linear programming approach to approximate dynamic programming. [sent-206, score-0.257]
</p><p>97 Computing factored value functions for policies in structured MDPs. [sent-228, score-0.364]
</p><p>98 Heuristics reﬁnements of approximate linear programming for factored continuous-state Markov decision processes. [sent-233, score-0.48]
</p><p>99 Learning and value function approximation in complex decision problems. [sent-249, score-0.094]
</p><p>100 A linear programming approach to solving stochastic dynamic programs, TR, 1993. [sent-270, score-0.208]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alp', 0.715), ('factored', 0.312), ('mdps', 0.213), ('cmdp', 0.189), ('beta', 0.133), ('state', 0.131), ('cmdps', 0.126), ('mdp', 0.122), ('farias', 0.11), ('policy', 0.1), ('grid', 0.094), ('integrals', 0.088), ('constraints', 0.086), ('gmdp', 0.084), ('transition', 0.075), ('guestrin', 0.073), ('programming', 0.07), ('solutions', 0.067), ('densities', 0.065), ('bellman', 0.064), ('rust', 0.063), ('ls', 0.062), ('lp', 0.061), ('van', 0.061), ('reward', 0.06), ('roy', 0.058), ('operator', 0.056), ('program', 0.056), ('nite', 0.055), ('patrascu', 0.055), ('sampling', 0.054), ('running', 0.046), ('solving', 0.045), ('kveton', 0.042), ('ir', 0.042), ('schuurmans', 0.042), ('approximations', 0.04), ('koller', 0.04), ('decompose', 0.04), ('action', 0.038), ('spaces', 0.038), ('solution', 0.038), ('linear', 0.037), ('nes', 0.037), ('attend', 0.037), ('fb', 0.037), ('al', 0.036), ('constraint', 0.036), ('subspaces', 0.036), ('quality', 0.035), ('approximation', 0.035), ('network', 0.035), ('analyzed', 0.034), ('decision', 0.034), ('emerged', 0.033), ('nements', 0.033), ('factorizations', 0.033), ('decompositions', 0.033), ('dynamic', 0.033), ('formulation', 0.032), ('bene', 0.031), ('parametric', 0.031), ('athena', 0.031), ('surrogate', 0.031), ('devised', 0.031), ('objective', 0.031), ('feature', 0.03), ('disadvantage', 0.029), ('xl', 0.029), ('continuous', 0.029), ('active', 0.028), ('states', 0.028), ('equation', 0.028), ('illustrates', 0.027), ('alternative', 0.027), ('approximate', 0.027), ('functions', 0.027), ('factorized', 0.027), ('violated', 0.027), ('transitions', 0.026), ('optimal', 0.026), ('de', 0.025), ('parents', 0.025), ('value', 0.025), ('subspace', 0.024), ('computers', 0.024), ('closed', 0.023), ('compact', 0.023), ('subsets', 0.023), ('stochastic', 0.023), ('offers', 0.023), ('discretization', 0.023), ('existing', 0.023), ('processes', 0.023), ('variables', 0.023), ('samples', 0.023), ('capacity', 0.022), ('actions', 0.022), ('exponential', 0.022), ('subject', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="116-tfidf-1" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>2 0.13142265 <a title="116-tfidf-2" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>3 0.12044165 <a title="116-tfidf-3" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>Author: Arnab Nilim, Laurent El Ghaoui</p><p>Abstract: Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to realworld problems. We propose an algorithm for solving ﬁnite-state and ﬁnite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efﬁcient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.</p><p>4 0.11890547 <a title="116-tfidf-4" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efﬁciently.</p><p>5 0.11388678 <a title="116-tfidf-5" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>6 0.1097085 <a title="116-tfidf-6" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>7 0.10691212 <a title="116-tfidf-7" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>8 0.10672526 <a title="116-tfidf-8" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>9 0.088874787 <a title="116-tfidf-9" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>10 0.081841357 <a title="116-tfidf-10" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>11 0.077485941 <a title="116-tfidf-11" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>12 0.074048802 <a title="116-tfidf-12" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>13 0.073299617 <a title="116-tfidf-13" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>14 0.066367865 <a title="116-tfidf-14" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>15 0.061958823 <a title="116-tfidf-15" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>16 0.058957599 <a title="116-tfidf-16" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>17 0.057787996 <a title="116-tfidf-17" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>18 0.055982485 <a title="116-tfidf-18" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>19 0.055965181 <a title="116-tfidf-19" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<p>20 0.053339168 <a title="116-tfidf-20" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.177), (1, 0.178), (2, -0.108), (3, 0.025), (4, -0.029), (5, 0.018), (6, -0.061), (7, -0.086), (8, 0.082), (9, 0.022), (10, -0.007), (11, -0.096), (12, 0.012), (13, 0.097), (14, -0.077), (15, 0.041), (16, 0.009), (17, -0.021), (18, -0.016), (19, -0.005), (20, -0.045), (21, 0.03), (22, -0.006), (23, -0.05), (24, -0.046), (25, -0.074), (26, 0.03), (27, -0.096), (28, 0.007), (29, -0.03), (30, -0.087), (31, -0.013), (32, -0.019), (33, 0.024), (34, 0.001), (35, 0.003), (36, -0.037), (37, -0.088), (38, -0.016), (39, -0.099), (40, 0.05), (41, -0.009), (42, 0.086), (43, 0.022), (44, -0.05), (45, 0.01), (46, -0.046), (47, 0.026), (48, 0.008), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94261324 <a title="116-lsi-1" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>2 0.78995997 <a title="116-lsi-2" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>Author: J. A. Bagnell, Sham M. Kakade, Jeff G. Schneider, Andrew Y. Ng</p><p>Abstract: We consider the policy search approach to reinforcement learning. We show that if a “baseline distribution” is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a ﬁnite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem. 1</p><p>3 0.76573819 <a title="116-lsi-3" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>4 0.69433767 <a title="116-lsi-4" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>Author: Arnab Nilim, Laurent El Ghaoui</p><p>Abstract: Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to realworld problems. We propose an algorithm for solving ﬁnite-state and ﬁnite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efﬁcient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.</p><p>5 0.69286084 <a title="116-lsi-5" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>Author: Natalia H. Gardiol, Leslie P. Kaelbling</p><p>Abstract: A mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes. Indeed, almost all interesting sequential decision-making domains involve large state spaces and large, stochastic action sets. We investigate a way to act intelligently as quickly as possible in domains where ﬁnding a complete policy would take a hopelessly long time. This approach, Relational Envelopebased Planning (REBP) tackles large, noisy problems along two axes. First, describing a domain as a relational MDP (instead of as an atomic or propositionally-factored MDP) allows problem structure and dynamics to be captured compactly with a small set of probabilistic, relational rules. Second, an envelope-based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit. 1</p><p>6 0.66836369 <a title="116-lsi-6" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>7 0.59840745 <a title="116-lsi-7" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>8 0.57649553 <a title="116-lsi-8" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>9 0.55885404 <a title="116-lsi-9" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>10 0.52705675 <a title="116-lsi-10" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>11 0.52492249 <a title="116-lsi-11" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>12 0.52346617 <a title="116-lsi-12" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>13 0.50903159 <a title="116-lsi-13" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>14 0.45860785 <a title="116-lsi-14" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>15 0.45734957 <a title="116-lsi-15" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>16 0.43096787 <a title="116-lsi-16" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>17 0.42394653 <a title="116-lsi-17" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>18 0.41553453 <a title="116-lsi-18" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>19 0.37710336 <a title="116-lsi-19" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>20 0.34788907 <a title="116-lsi-20" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.068), (11, 0.03), (21, 0.274), (29, 0.019), (30, 0.028), (35, 0.069), (53, 0.069), (69, 0.016), (71, 0.069), (76, 0.047), (85, 0.063), (91, 0.091), (99, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83298105 <a title="116-lda-1" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>Author: Curt Bererton, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 1</p><p>same-paper 2 0.77227032 <a title="116-lda-2" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>3 0.55143231 <a title="116-lda-3" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>Author: J. A. Bagnell, Sham M. Kakade, Jeff G. Schneider, Andrew Y. Ng</p><p>Abstract: We consider the policy search approach to reinforcement learning. We show that if a “baseline distribution” is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a ﬁnite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem. 1</p><p>4 0.54654539 <a title="116-lda-4" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>5 0.54398561 <a title="116-lda-5" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>6 0.53939867 <a title="116-lda-6" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>7 0.53152013 <a title="116-lda-7" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>8 0.53113496 <a title="116-lda-8" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>9 0.53034776 <a title="116-lda-9" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>10 0.52874374 <a title="116-lda-10" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>11 0.52841747 <a title="116-lda-11" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>12 0.52436388 <a title="116-lda-12" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>13 0.52321124 <a title="116-lda-13" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>14 0.52227855 <a title="116-lda-14" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>15 0.52196473 <a title="116-lda-15" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>16 0.52119619 <a title="116-lda-16" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>17 0.52110541 <a title="116-lda-17" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>18 0.52095389 <a title="116-lda-18" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>19 0.52079207 <a title="116-lda-19" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>20 0.52070796 <a title="116-lda-20" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
