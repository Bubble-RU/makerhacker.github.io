<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-49" href="#">nips2003-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</h1>
<br/><p>Source: <a title="nips-2003-49-pdf" href="http://papers.nips.cc/paper/2491-decoding-v1-neuronal-activity-using-particle-filtering-with-volterra-kernels.pdf">pdf</a></p><p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>Reference: <a title="nips-2003-49-reference" href="../nips2003_reference/nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. [sent-5, score-0.18]
</p><p>2 In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. [sent-6, score-0.732]
</p><p>3 This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. [sent-8, score-0.559]
</p><p>4 Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. [sent-9, score-1.089]
</p><p>5 The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. [sent-10, score-0.257]
</p><p>6 1  Introduction  Cells in the primary visual cortex perform nonlinear operations on visual stimuli. [sent-11, score-0.189]
</p><p>7 This nonlinearity introduces ambiguity in the response of the neurons. [sent-12, score-0.195]
</p><p>8 Given a neuronal response, an optimal linear decoder cannot accurately reconstruct the visual stimulus due to nonlinearities. [sent-13, score-0.512]
</p><p>9 Is there a strategy to resolve this ambiguity and recover the information that is encoded in the response of these neurons? [sent-14, score-0.195]
</p><p>10 Bayesian decoding schemes, which are nonlinear, might be useful in this context . [sent-15, score-0.327]
</p><p>11 Bayesian sequential updating or belief propagation, implemented in the form of particle ﬁltering, has recently been used in estimating the hand trajectories of monkeys based on M1 neuron’s responses [4] and the location of a rat based on the responses of the place cells in the hippocampus[3]. [sent-16, score-0.772]
</p><p>12 However, linear methods have been shown to be quite adequate for decoding LGN, motor cortical, or hippocampal place cells’ signals using population vectors or the optimal linear decoder [10, 5, 8]. [sent-17, score-0.661]
</p><p>13 These methods may be more useful or important in the decoding of nonlinear visual neuronal responses. [sent-19, score-0.524]
</p><p>14 Here, we implement an algorithm based on Bayesian sequential updating in the form particle ﬁltering to decode nonlinear visual neurons in awake behaving monkeys. [sent-20, score-0.769]
</p><p>15 [1] in their decoding of hippocampus place neurons or M1 neurons, except that we introduced the use of Volterra kernels [6, 7, 9] to ﬁlter the hypothesis particle to generate feedback messages. [sent-23, score-1.228]
</p><p>16 This window allows us to backtrack and update the hypotheses within a 200 ms window, so the hypothesis space does not grow beyond 200ms for lengthy signals. [sent-25, score-0.367]
</p><p>17 We demonstrated that this method is feasible practically and indeed useful for decoding a temporal variable in the stimulus input based on cells’ responses and that it succeeds even when the optimal linear decoder fails. [sent-26, score-0.829]
</p><p>18 2  The Approach  Our objective is to infer the time-series of a scene variable based on the ongoing response of one or a set of visual neurons. [sent-27, score-0.416]
</p><p>19 A hypothesis particle is then the entire history of the scene variable of interest up to the present time t, i. [sent-28, score-0.758]
</p><p>20 , xt ) given the observed neuronal activity (y1 , y2 , . [sent-33, score-0.358]
</p><p>21 A key feature of our algorithm is the use of a decoded or estimated hypothesis to predict the response of the neurons at the next time step. [sent-37, score-0.569]
</p><p>22 The premise is that the scene variable we are inferring is sufﬁcient to predict the activity of the neuron. [sent-38, score-0.265]
</p><p>23 Since visual neurons have a temporal receptive ﬁeld and typically integrate information from the past 100-200 ms to produce a response, we cannot make the Markovian assumption made in other Bayesian decoding studies [1, 2, 3, 4]. [sent-39, score-0.699]
</p><p>24 Instead, we will use the receptive ﬁeld (kernel) to ﬁlter each hypothesis particle to generate a prediction of the neural response. [sent-40, score-0.772]
</p><p>25 We propose to use the Volterra kernels, which have been used in previous studies [6, 7, 9] to characterize the transfer function or receptive ﬁeld of a neuron, to ﬁlter the hypothesis (ˆt , . [sent-41, score-0.233]
</p><p>26 Ongoing observation of the activity of neurons is compared to the predicted response or proposal to yield a likelihood measure. [sent-46, score-0.422]
</p><p>27 The likelihood measure of each hypothesis particle is proportional to how close the hypothesis’s predicted response is to the actual observed neural response. [sent-47, score-0.936]
</p><p>28 As all the existing hypotheses are weighted by their likelihood measures, the posterior distribution of the hypothesis is effectively resampled. [sent-48, score-0.289]
</p><p>29 hypotheses that give predicted responses close to the actual response values will not only be kept alive, but also be allowed to give birth to offspring particles in its vicinity in the hypothesis space, allowing the algorithm to zoom in to the correct hypothesis more precisely. [sent-51, score-0.986]
</p><p>30 After weighting, resampling and reproducing, the hypothesis particles are propagated forward according to the prior statistical distribution on how the scene variable tends to progress. [sent-52, score-0.639]
</p><p>31 That is, p(xt |xt−1 ) yields a proposed hypothesis about the stimulus at time t + 1 based on the existing hypothesis which is deﬁned at t and earlier times. [sent-53, score-0.539]
</p><p>32 These hypotheses are then ﬁltered though the Volterra kernels to predict the distribution p(yt |xt−200,. [sent-54, score-0.259]
</p><p>33 This experiment sought to understand the encoding and decoding of temporal visual information by V1 neurons. [sent-61, score-0.442]
</p><p>34 2 seconds per trial) of a sinewave grating stimulus was presented while the monkey had to maintain ﬁxation on a spot within a 0. [sent-63, score-0.277]
</p><p>35 The sinewave grating was constrained to move along one dimension in a Figure 2: A sample time series of the scene direction perpendicular to the grating with a step size in phase drawn from a random variable, with a sample spike train below. [sent-66, score-0.442]
</p><p>36 In decoding the cos(phase), a hidden variable, was the scene variable inferred. [sent-69, score-0.463]
</p><p>37 This scene variable, through the Volterra kernel procedure, can predict the neural responses of this class of stimulus reasonably well. [sent-71, score-0.443]
</p><p>38 The known pair sequences of stimulus and response in these trials were used to estimate the Volterra kernels by correlating the input x with the neural response y. [sent-73, score-0.743]
</p><p>39 In addition, one particular stimulus sequence is repeated 60-80 trials to obtain a PSTH, which is smoothed with a 10 ms window to give an estimate of the instantaneous ﬁring rate. [sent-74, score-0.351]
</p><p>40 In our decoding work, we take the PSTH as input to our algorithm; this is considered equivalent to assuming simultaneous access to a number of identical, independent neurons. [sent-75, score-0.35]
</p><p>41 When the neurons are different, a kernel derivation for each neuron is necessary. [sent-76, score-0.21]
</p><p>42 That is, the parameters of the kernels are derived using the regression technique by correlating the input and the output, and are compensated by the covariance in the input, i. [sent-82, score-0.171]
</p><p>43 05  0  0  500  1000  1500  2000  2500  Time(ms)  Figure 3: The ﬁrst and second order Volterra kernels of a V1 cell (left) and a typical prediction of the neuronal response compared to the actual response (right). [sent-88, score-0.667]
</p><p>44 Because of the correlations in the input signal xt , the matrix (X X) is ill conditioned. [sent-90, score-0.221]
</p><p>45 Figure 3 depicts an example of the ﬁrst and second order Volterra kernels and also shows a typical example of their accuracy in predicting the response PSTH yt . [sent-93, score-0.557]
</p><p>46 For a majority of these neurons, the Volterra kernels recovered are capable of predicting the neural response to the input stimulus with a high level of accuracy. [sent-94, score-0.528]
</p><p>47 This observation forms the basis of success for our scheme of particle ﬁltering. [sent-95, score-0.473]
</p><p>48 5  Decoding Scheme  We apply Bayesian decoding to the problem of determining the visual stimulus variable xt for some time step t, given observations of the neural responses (y1 , y2 , . [sent-96, score-0.972]
</p><p>49 1  Particle Prediction  At each time step of of decoding scheme, we can now ﬁlter a hypothesis particle (ˆ1 , x2 , . [sent-102, score-0.98]
</p><p>50 , xt ) by the Volterra kernels to generate a prediction of the response of the x ˆ ˆ neuron to test the validity of the hypothesis. [sent-105, score-0.638]
</p><p>51 , yt ) remains the observed neural activity of a V1 neuron up to time t, and yt is the predicted neural activity at time t based on ˆi hypothesis particle i. [sent-109, score-1.485]
</p><p>52 This gives us a set of predicted responses at time t, {ˆt , yt , . [sent-110, score-0.404]
</p><p>53 , yt }, y 1 ˆ2 ˆN where the subscript is the particle index, and N is the number of particles. [sent-113, score-0.681]
</p><p>54 2  Particle Resampling  The actual observed response of the neuron at time t is compared to each particle’s prediction as a means to evaluate the likelihood or ﬁtness of the particle. [sent-115, score-0.397]
</p><p>55 The  Figure 4: Flow chart of the PF decoding scheme. [sent-117, score-0.327]
</p><p>56 The thicknesses of the lines are proportional to the number of particles with the corresponding values. [sent-120, score-0.227]
</p><p>57 Notice the change in the distribution of particles after sampling. [sent-121, score-0.227]
</p><p>58 After the resampling there are many more particles concentrated around 1 instead of -1. [sent-122, score-0.318]
</p><p>59 relative likelihood of an observation given each particle is then given by i  p(yt |ˆi , . [sent-123, score-0.457]
</p><p>60 All the particles together provide a representation of the particle-conditional distribution, p(yt |ˆt , xt−1 , . [sent-127, score-0.227]
</p><p>61 3  Particle Propagation  The next step in the decoding scheme is to generate new value xt+1 and append it to the ˆ hypothesis particle p(ˆt+1 |y1 , y2 , . [sent-139, score-1.02]
</p><p>62 yt )dxt , x x x  where p(ˆt+1 |ˆt ) is the state propagation model that provides the prior on how the stimulus x x changes over time. [sent-145, score-0.463]
</p><p>63 For the state propagation model used in this study, all initial positions for the stimulus are equally likely. [sent-146, score-0.24]
</p><p>64 The range of the stimulus (-1 to 1) is divided into 60 equally spaced intervals. [sent-147, score-0.189]
</p><p>65 Besides, the  hypothesis space is enormous as there are 60 possible values at each time point, and information from a 200 ms window (20 time points at 10 ms intervals) is being integrated to predict yt . [sent-150, score-0.734]
</p><p>66 The particle ﬁltering algorithm is basically a way to approximate the distributions efﬁciently. [sent-151, score-0.434]
</p><p>67 Prediction step: Filter all particles by the Volterra kernels to generate the prediction of neural responses. [sent-156, score-0.448]
</p><p>68 Resampling step: Compare actual neural response with the predicted response of each particle to assign a likelihood value to each particle. [sent-158, score-0.941]
</p><p>69 Resample (with replacement) the posterior distribution of the particles based on their likelihood. [sent-159, score-0.248]
</p><p>70 Propagation step: Sample from the state model to randomly postulate a new stimulus value xt for each particle and add this ˆ value to the end of the particle’s sequence to obtain (ˆ1 , x2 , . [sent-161, score-0.797]
</p><p>71 In the propagation step, the state model will move the stimulus in ways that it has typically been seen to move. [sent-166, score-0.216]
</p><p>72 In the prediction step, particles that predict a neural response close to the actual observed response will be highly valued and will likely be duplicated in the resampled set. [sent-167, score-0.781]
</p><p>73 Conversely, particles that predict a response which is not close to the actual response will not be highly valued and thus will likely be removed from the resultant set. [sent-168, score-0.702]
</p><p>74 sk (t) is the binary spike response of a neuron during trial k. [sent-173, score-0.348]
</p><p>75 In general, for cells that respond well to a stimulus, this ﬁrst order and the second order kernel can predict the response well. [sent-175, score-0.359]
</p><p>76 Over all cells tested (n=33), the average error ratio ey in the energy of the actual response is 18. [sent-176, score-0.353]
</p><p>77 Each of the cells was decoded using the particle ﬁltering algorithm with 1000 particles. [sent-178, score-0.587]
</p><p>78 A correlation exists between the encoding and decoding errors across trials as shown in Figure 5. [sent-181, score-0.368]
</p><p>79 ey =  − yt )2 , ex = 2 t yt  y t (ˆt  − xt )2 , 2 t (xt + 1)  x t (ˆt  Figure 6: Reconstruction error when input PSTH is constructed from fewer trials. [sent-182, score-0.775]
</p><p>80 Figure 7: Particle ﬁltering (PF) and optimal linear decoder (O. [sent-184, score-0.171]
</p><p>81 σ affects the rate at which the particle hypothesis space collapses around the correct solution. [sent-189, score-0.597]
</p><p>82 If σ is too large, all particles will become equally likely, while if σ is too small, only a few particles will survive each time step. [sent-190, score-0.503]
</p><p>83 Ideally, the particles will converge on a value for a number of time steps equal to the kernel’s length. [sent-191, score-0.252]
</p><p>84 When the kernel is unable to predict the neuronal response, particularly for cells that have low ﬁring rates, any decoding scheme will suffer because of insufﬁcient information. [sent-195, score-0.649]
</p><p>85 This idea is consistent with the error correlation between the particle ﬁlter and kernel in Figure 5. [sent-197, score-0.467]
</p><p>86 These cells do not provide enough relevant information about the visual stimulus in their spiking activities. [sent-198, score-0.346]
</p><p>87 Figure 6 shows that reconstruction based on the Figure 8: A scatter plot comparing PSTH constructed from as few as 5-10 spike trains the two decoding methods. [sent-199, score-0.526]
</p><p>88 This suggests that as few as 10 independent but similar cells recorded simultaneously might be sufﬁcient for decoding this scene variable. [sent-201, score-0.513]
</p><p>89 We ﬁnd that the optimal linear decoder does not decode these cells well. [sent-202, score-0.316]
</p><p>90 The problem for the optimal linear decoder is that at any single moment in time it can only propose a single hypothesis, but there exist multiple signals that can produce the response. [sent-204, score-0.22]
</p><p>91 The optimal linear decoder tends to average in these cases. [sent-205, score-0.193]
</p><p>92 The particle ﬁlter keeps alive many independent hypotheses and can thus choose the most likely candidate by integrating information. [sent-206, score-0.581]
</p><p>93 The success of the particle ﬁlter relies mainly on three factors. [sent-207, score-0.434]
</p><p>94 First, in the particle prediction step, the Volterra kernels allow the particles to make reasonably accurate proposals based on the observed neural activities. [sent-208, score-0.897]
</p><p>95 Second, in the resampling step, the weight of each particle embodies all the earlier observations, and because our particle ﬁlter keeps track of all proposals within the last 200 ms, earlier hypotheses can continue to be reevaluated and reﬁned. [sent-210, score-1.15]
</p><p>96 Finally, in the propagation step, the particle ﬁlter utilizes prior knowledge about the manner in which the stimulus moves. [sent-211, score-0.65]
</p><p>97 Bayesian decoding of motor cortical signals by particle ﬁltering. [sent-222, score-0.815]
</p><p>98 A statistical paradigm for neural spike train decoding applied to position prediction from ensemble ﬁring patterns of rat hippocampal place cells. [sent-231, score-0.584]
</p><p>99 Particle ﬁltering algorithms for neural decoding and adaptive estimation of receptive ﬁeld plasticity. [sent-242, score-0.427]
</p><p>100 Interpreting neuronal population activity by reconstruction: Uniﬁed framework with application to hippocampal place cells. [sent-297, score-0.269]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('particle', 0.434), ('volterra', 0.408), ('decoding', 0.327), ('yt', 0.247), ('particles', 0.227), ('xt', 0.198), ('response', 0.168), ('stimulus', 0.165), ('hypothesis', 0.163), ('decoder', 0.149), ('psth', 0.131), ('kernels', 0.116), ('cells', 0.097), ('neurons', 0.096), ('neuronal', 0.092), ('resampling', 0.091), ('ms', 0.091), ('scene', 0.089), ('visual', 0.084), ('hypotheses', 0.082), ('neuron', 0.081), ('spike', 0.071), ('lter', 0.07), ('receptive', 0.07), ('activity', 0.068), ('predicted', 0.067), ('ring', 0.066), ('responses', 0.065), ('brockwell', 0.064), ('sinewave', 0.064), ('ltering', 0.061), ('predict', 0.061), ('reconstruction', 0.059), ('decoded', 0.056), ('pf', 0.052), ('actual', 0.051), ('propagation', 0.051), ('prediction', 0.049), ('trains', 0.048), ('decode', 0.048), ('grating', 0.048), ('variable', 0.047), ('hippocampal', 0.045), ('bayesian', 0.045), ('alive', 0.043), ('romero', 0.043), ('tai', 0.043), ('tness', 0.043), ('trials', 0.041), ('proposals', 0.041), ('scheme', 0.039), ('awake', 0.037), ('ey', 0.037), ('hippocampus', 0.034), ('frank', 0.034), ('resample', 0.034), ('kernel', 0.033), ('place', 0.032), ('population', 0.032), ('correlating', 0.032), ('step', 0.031), ('temporal', 0.031), ('window', 0.031), ('rat', 0.03), ('motor', 0.03), ('neural', 0.03), ('ongoing', 0.028), ('yu', 0.028), ('trial', 0.028), ('ambiguity', 0.027), ('valued', 0.027), ('cognition', 0.027), ('eld', 0.026), ('predicting', 0.026), ('neurophysiology', 0.026), ('updating', 0.026), ('generate', 0.026), ('time', 0.025), ('nih', 0.025), ('phase', 0.024), ('cos', 0.024), ('equally', 0.024), ('signals', 0.024), ('ex', 0.023), ('neuroscience', 0.023), ('input', 0.023), ('likelihood', 0.023), ('earlier', 0.023), ('sequential', 0.023), ('instantaneous', 0.023), ('cell', 0.023), ('keeps', 0.022), ('optimal', 0.022), ('tends', 0.022), ('brown', 0.021), ('scatter', 0.021), ('conversely', 0.021), ('posterior', 0.021), ('sample', 0.021), ('nonlinear', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="49-tfidf-1" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>2 0.28643188 <a title="49-tfidf-2" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><p>3 0.17280203 <a title="49-tfidf-3" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>4 0.16159476 <a title="49-tfidf-4" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. 1</p><p>5 0.16037409 <a title="49-tfidf-5" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>6 0.16024606 <a title="49-tfidf-6" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>7 0.12578598 <a title="49-tfidf-7" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>8 0.12033593 <a title="49-tfidf-8" href="./nips-2003-Bounded_Invariance_and_the_Formation_of_Place_Fields.html">43 nips-2003-Bounded Invariance and the Formation of Place Fields</a></p>
<p>9 0.10923147 <a title="49-tfidf-9" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>10 0.10806535 <a title="49-tfidf-10" href="./nips-2003-Nonlinear_Processing_in_LGN_Neurons.html">140 nips-2003-Nonlinear Processing in LGN Neurons</a></p>
<p>11 0.092061691 <a title="49-tfidf-11" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>12 0.090612069 <a title="49-tfidf-12" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>13 0.088550046 <a title="49-tfidf-13" href="./nips-2003-Online_Passive-Aggressive_Algorithms.html">148 nips-2003-Online Passive-Aggressive Algorithms</a></p>
<p>14 0.088411845 <a title="49-tfidf-14" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>15 0.08535289 <a title="49-tfidf-15" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>16 0.083118081 <a title="49-tfidf-16" href="./nips-2003-Eye_Micro-movements_Improve_Stimulus_Detection_Beyond_the_Nyquist_Limit_in_the_Peripheral_Retina.html">67 nips-2003-Eye Micro-movements Improve Stimulus Detection Beyond the Nyquist Limit in the Peripheral Retina</a></p>
<p>17 0.074890494 <a title="49-tfidf-17" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>18 0.073808827 <a title="49-tfidf-18" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>19 0.072167665 <a title="49-tfidf-19" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>20 0.069265842 <a title="49-tfidf-20" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.214), (1, 0.113), (2, 0.187), (3, -0.07), (4, 0.138), (5, 0.021), (6, 0.206), (7, 0.239), (8, 0.133), (9, -0.025), (10, 0.03), (11, 0.028), (12, 0.055), (13, -0.011), (14, 0.136), (15, 0.11), (16, 0.053), (17, 0.037), (18, -0.15), (19, 0.071), (20, -0.163), (21, 0.044), (22, 0.075), (23, -0.14), (24, 0.063), (25, -0.086), (26, -0.021), (27, 0.074), (28, -0.057), (29, -0.072), (30, -0.029), (31, -0.114), (32, 0.11), (33, 0.013), (34, 0.005), (35, -0.094), (36, 0.121), (37, -0.002), (38, 0.045), (39, 0.012), (40, 0.05), (41, -0.018), (42, 0.094), (43, -0.007), (44, 0.033), (45, -0.074), (46, 0.058), (47, -0.119), (48, -0.027), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97580624 <a title="49-lsi-1" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>2 0.57760906 <a title="49-lsi-2" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><p>3 0.54447669 <a title="49-lsi-3" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>Author: Jaco Vermaak, Simon J. Godsill, Arnaud Doucet</p><p>Abstract: We propose a method for sequential Bayesian kernel regression. As is the case for the popular Relevance Vector Machine (RVM) [10, 11], the method automatically identiﬁes the number and locations of the kernels. Our algorithm overcomes some of the computational difﬁculties related to batch methods for kernel regression. It is non-iterative, and requires only a single pass over the data. It is thus applicable to truly sequential data sets and batch data sets alike. The algorithm is based on a generalisation of Importance Sampling, which allows the design of intuitively simple and efﬁcient proposal distributions for the model parameters. Comparative results on two standard data sets show our algorithm to compare favourably with existing batch estimation strategies.</p><p>4 0.53462958 <a title="49-lsi-4" href="./nips-2003-Inferring_State_Sequences_for_Non-linear_Systems_with_Embedded_Hidden_Markov_Models.html">91 nips-2003-Inferring State Sequences for Non-linear Systems with Embedded Hidden Markov Models</a></p>
<p>Author: Radford M. Neal, Matthew J. Beal, Sam T. Roweis</p><p>Abstract: We describe a Markov chain method for sampling from the distribution of the hidden state sequence in a non-linear dynamical system, given a sequence of observations. This method updates all states in the sequence simultaneously using an embedded Hidden Markov Model (HMM). An update begins with the creation of “pools” of candidate states at each time. We then deﬁne an embedded HMM whose states are indexes within these pools. Using a forward-backward dynamic programming algorithm, we can efﬁciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools. We illustrate the method in a simple one-dimensional example, and in an example showing how an embedded HMM can be used to in effect discretize the state space without any discretization error. We also compare the embedded HMM to a particle smoother on a more substantial problem of inferring human motion from 2D traces of markers. 1</p><p>5 0.45686245 <a title="49-lsi-5" href="./nips-2003-Bounded_Invariance_and_the_Formation_of_Place_Fields.html">43 nips-2003-Bounded Invariance and the Formation of Place Fields</a></p>
<p>Author: Reto Wyss, Paul F. Verschure</p><p>Abstract: One current explanation of the view independent representation of space by the place-cells of the hippocampus is that they arise out of the summation of view dependent Gaussians. This proposal assumes that visual representations show bounded invariance. Here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representations. Our analysis is based on the behavior of a simulated robot in a virtual environment containing speciﬁc visual cues. Our results show that the temporal population code provides a representational substrate that can naturally account for the formation of place ﬁelds. 1</p><p>6 0.44520906 <a title="49-lsi-6" href="./nips-2003-Nonlinear_Processing_in_LGN_Neurons.html">140 nips-2003-Nonlinear Processing in LGN Neurons</a></p>
<p>7 0.43853608 <a title="49-lsi-7" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>8 0.43489817 <a title="49-lsi-8" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>9 0.42765436 <a title="49-lsi-9" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>10 0.40218109 <a title="49-lsi-10" href="./nips-2003-A_Neuromorphic_Multi-chip_Model_of_a_Disparity_Selective_Complex_Cell.html">13 nips-2003-A Neuromorphic Multi-chip Model of a Disparity Selective Complex Cell</a></p>
<p>11 0.38381204 <a title="49-lsi-11" href="./nips-2003-A_Recurrent_Model_of_Orientation_Maps_with_Simple_and_Complex_Cells.html">16 nips-2003-A Recurrent Model of Orientation Maps with Simple and Complex Cells</a></p>
<p>12 0.38272908 <a title="49-lsi-12" href="./nips-2003-Eye_Micro-movements_Improve_Stimulus_Detection_Beyond_the_Nyquist_Limit_in_the_Peripheral_Retina.html">67 nips-2003-Eye Micro-movements Improve Stimulus Detection Beyond the Nyquist Limit in the Peripheral Retina</a></p>
<p>13 0.37703469 <a title="49-lsi-13" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>14 0.3375861 <a title="49-lsi-14" href="./nips-2003-Dopamine_Modulation_in_a_Basal_Ganglio-Cortical_Network_of_Working_Memory.html">56 nips-2003-Dopamine Modulation in a Basal Ganglio-Cortical Network of Working Memory</a></p>
<p>15 0.31163502 <a title="49-lsi-15" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>16 0.29369199 <a title="49-lsi-16" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>17 0.28821757 <a title="49-lsi-17" href="./nips-2003-Online_Passive-Aggressive_Algorithms.html">148 nips-2003-Online Passive-Aggressive Algorithms</a></p>
<p>18 0.28553611 <a title="49-lsi-18" href="./nips-2003-A_Probabilistic_Model_of_Auditory_Space_Representation_in_the_Barn_Owl.html">15 nips-2003-A Probabilistic Model of Auditory Space Representation in the Barn Owl</a></p>
<p>19 0.28243953 <a title="49-lsi-19" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>20 0.27346563 <a title="49-lsi-20" href="./nips-2003-The_Diffusion-Limited_Biochemical_Signal-Relay_Channel.html">184 nips-2003-The Diffusion-Limited Biochemical Signal-Relay Channel</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (11, 0.025), (29, 0.019), (30, 0.025), (33, 0.018), (35, 0.045), (41, 0.015), (53, 0.142), (71, 0.035), (75, 0.238), (76, 0.1), (85, 0.118), (91, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85408169 <a title="49-lda-1" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>2 0.73168129 <a title="49-lda-2" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>3 0.65854794 <a title="49-lda-3" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>Author: Sanjiv Kumar, Martial Hebert</p><p>Abstract: In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classiﬁcation of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classiﬁcation problems using the graph min-cut algorithms. The performance of the model was veriﬁed on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments. 1</p><p>4 0.65854287 <a title="49-lda-4" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. 1</p><p>5 0.65811419 <a title="49-lda-5" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>6 0.65555918 <a title="49-lda-6" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>7 0.6555205 <a title="49-lda-7" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>8 0.65231335 <a title="49-lda-8" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>9 0.65071511 <a title="49-lda-9" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>10 0.64865941 <a title="49-lda-10" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>11 0.64850044 <a title="49-lda-11" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>12 0.64793605 <a title="49-lda-12" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>13 0.6471957 <a title="49-lda-13" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>14 0.64601243 <a title="49-lda-14" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>15 0.64497858 <a title="49-lda-15" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>16 0.6449517 <a title="49-lda-16" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>17 0.64436686 <a title="49-lda-17" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>18 0.64386576 <a title="49-lda-18" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>19 0.64364862 <a title="49-lda-19" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>20 0.64311492 <a title="49-lda-20" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
