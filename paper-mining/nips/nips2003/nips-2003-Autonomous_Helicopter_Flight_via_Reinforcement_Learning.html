<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-38" href="#">nips2003-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2003-38-pdf" href="http://papers.nips.cc/paper/2455-autonomous-helicopter-flight-via-reinforcement-learning.pdf">pdf</a></p><p>Author: H. J. Kim, Michael I. Jordan, Shankar Sastry, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition.</p><p>Reference: <a title="nips-2003-38-reference" href="../nips2003_reference/nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Autonomous helicopter ﬂight via Reinforcement Learning Andrew Y. [sent-1, score-0.892]
</p><p>2 Jordan, and Shankar Sastry University of California Berkeley, CA 94720  Abstract Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. [sent-4, score-0.968]
</p><p>3 In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. [sent-5, score-0.984]
</p><p>4 We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. [sent-6, score-0.892]
</p><p>5 We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition. [sent-7, score-1.046]
</p><p>6 1 Introduction Helicopters represent a challenging control problem with high-dimensional, complex, asymmetric, noisy, non-linear, dynamics, and are widely regarded as signiﬁcantly more difﬁcult to control than ﬁxed-wing aircraft. [sent-8, score-0.133]
</p><p>7 [7] Consider, for instance, the problem of designing a helicopter that hovers in place. [sent-9, score-0.892]
</p><p>8 We begin with a single, horizontally-oriented main rotor attached to the helicopter via the rotor shaft. [sent-10, score-1.299]
</p><p>9 Suppose the main rotor rotates clockwise (viewed from above), blowing air downwards and hence generating upward thrust. [sent-11, score-0.285]
</p><p>10 By applying clockwise torque to the main rotor to make it rotate, our helicopter experiences an anti-torque that tends to cause the main chassis to spin anti-clockwise. [sent-12, score-1.141]
</p><p>11 Thus, in the invention of the helicopter, it was necessary to add a tail rotor, which blows air sideways/rightwards to generate an appropriate moment to counteract the spin. [sent-13, score-0.083]
</p><p>12 But, this sideways force now causes the helicopter to drift leftwards. [sent-14, score-0.955]
</p><p>13 So, for a helicopter to hover in place, it must actually be tilted slightly to the right, so that the main rotor’s thrust is directed downwards and slightly to the left, to counteract this tendency to drift sideways. [sent-15, score-1.082]
</p><p>14 The history of helicopters is rife with such tales of ingenious solutions to problems caused by solutions to other problems, and of complex, nonintuitive dynamics that make helicopters challenging to control. [sent-16, score-0.168]
</p><p>15 In this paper, we describe the successful application of reinforcement learning to designing a controller for autonomous helicopter ﬂight. [sent-17, score-1.042]
</p><p>16 2 Autonomous Helicopter The helicopter used in this work was a Yamaha R-50 helicopter, which is approximately 3. [sent-20, score-0.892]
</p><p>17 The helicopter carries an Inertial Navigation System (INS) consisting of 3 accelerometers and 3 rate gyroscopes installed in exactly orthogonal x,y,z directions, and a differential GPS system, which with the assistance of a ground station, gives position estimates with a resolution of 2cm. [sent-23, score-0.941]
</p><p>18 Most Helicopters are controlled via a 4-dimensional action space: : The longtitudinal (front-back) and latitudinal (left-right) cyclic pitch controls. [sent-27, score-0.112]
</p><p>19 The rotor plane is the plane in which the helicopter’s rotors rotate. [sent-28, score-0.229]
</p><p>20 By tilting this plane either forwards/backwards or sideways, these controls cause the helicopter to accelerate forward/backwards or sideways. [sent-29, score-0.945]
</p><p>21 As the helicopter main-rotor’s blades sweep through the air, they generate an amount of upward thrust that (generally) increases with the angle at which the rotor blades are tilted. [sent-31, score-1.195]
</p><p>22 By varying the tilt angle of the rotor blades, the collective pitch control affects the main rotor’s thrust. [sent-32, score-0.425]
</p><p>23 Using a mechanism similar to the main rotor collective pitch control, this controls the tail rotor’s thrust. [sent-34, score-0.41]
</p><p>24 Using the position estimates given by the Kalman ﬁlter, our task is to pick good control actions every 50th of a second. [sent-35, score-0.106]
</p><p>25 ¦ £¡ §¥¡¡ ¤¢  ¨¡ §¢  ©¡ ¢   3 Model identiﬁcation To ﬁt a model of the helicopter’s dynamics, we began by asking a human pilot to ﬂy the helicopter for several minutes, and recorded the 12-dimensional helicopter state and 4dimensional helicopter control inputs as it was ﬂown. [sent-36, score-2.831]
</p><p>26 For instance, a helicopter at (0,0,0) facing east behaves in a way related only by a translation and rotation to one at (10,10,50) facing north, if we command each to accelerate forwards. [sent-39, score-0.951]
</p><p>27 Thus, model identiﬁcation is typically done not in the spatial (world) coordinates , but instead in the helicopter body coordinates, in which the , , and axes are forwards, sideways, and down relative to the current position of the helicopter. [sent-41, score-0.974]
</p><p>28 Each point plotted shows the mean-squared error between the predicted value of a state variable—when a model is used to the simulate the helicopter’s dynamics for a certain duration indicated on the -axis—and the true value of that state variable (as measured on test data) after the same duration. [sent-79, score-0.117]
</p><p>29 (b) The solid line is the true helicopter state on 10s of test data. [sent-84, score-0.944]
</p><p>30 The dash-dot line is the helicopter state predicted by our model, given the initial state at time 0 and all the intermediate control inputs. [sent-85, score-1.039]
</p><p>31 The solid lines show the hovering policy class (Section 5). [sent-92, score-0.243]
</p><p>32 The dashed lines show the extra weights added for trajectory following (Section 6). [sent-93, score-0.1]
</p><p>33 Similarly, we know that the roll angle of the helicopter should have no direct effect on forward velocity . [sent-100, score-0.964]
</p><p>34 £  4 53  danger and expense (about $70,000) of autonomous helicopters, we wanted to verify the ﬁtted model carefully, so as to be reasonably conﬁdent that a controller tested successfully in simulation will also be safe in real life. [sent-124, score-0.105]
</p><p>35 ) To check against this, we examined many plots such as shown in Figure 2, to check that the helicopter state “rarely” goes outside the errorbars predicted by our model at various time scales (see caption). [sent-127, score-0.945]
</p><p>36 Consider an MDP with state space , initial state , action space , state transition probabilities , reward function , and discount . [sent-129, score-0.144]
</p><p>37 Also let some family of policies be given, and suppose our goal is to ﬁnd a policy in with high utility, where the policy of is deﬁned to be ¦      £ ¡ ¥¤¢  ! [sent-130, score-0.236]
</p><p>38 Then a standard way to deﬁne an estimate of is via Monte Carlo: We can use the simulator to sample a trajectory , and by taking the on this sequence, we obtain empirical sum of discounted rewards one “sample” with which to estimate . [sent-136, score-0.084]
</p><p>39 This succeeded in ﬂying the helicopter in simulation, but not on the actual helicopter (Shim, pers. [sent-153, score-1.806]
</p><p>40 Similarly, preliminary experiments using and controllers to ﬂy a similar helicopter were also unsuccessful. [sent-156, score-0.892]
</p><p>41 These comments should not be taken as conclusive of the viability of any of these methods; rather, we take them to be indicative of the difﬁculty and subtlety involved in learning a helicopter controller. [sent-157, score-0.892]
</p><p>42 5 0  30  5  10  15  Figure 3: Comparison of hovering performance of learned controller (solid line) vs. [sent-177, score-0.16]
</p><p>43 ¢   ¨ ¡     ¢   ¨ ¡   We began by learning a policy for hovering in place. [sent-181, score-0.208]
</p><p>44 We want a controller that, given the current helicopter state and a desired hovering position and orientation , computes controls to make it hover stably there. [sent-182, score-1.213]
</p><p>45 For our policy class , we chose the simple neural network depicted in Figure 2c (solid edges only). [sent-183, score-0.128]
</p><p>46 Each of the edges in the ﬁgure represents a weight, and the connections were chosen via simple reasoning about which control channel should be used to control which state variables. [sent-184, score-0.151]
</p><p>47 For instance, consider the the longitudinal (forward/backward) cyclic pitch control , which causes the rotor plane to tilt forward/backward, thus causing the helicopter to pitch (and/or accelerate) forward or backward. [sent-185, score-1.399]
</p><p>48 From Figure 2c, we can read off the control control as  (£ "   ¡ ¦¤¡ ¥¤¡ ¤  $ £ ¦ £ £ £  © ( ¡ ( `  £  ¡  £¡  6  `     21wu S u 0 f ) x £ § ! [sent-186, score-0.114]
</p><p>49 ¨ Y1wu S © ¨$ ¥w¨ ¨ Cwu S ¦ ¨ f £ © £ § f ( u   x f u  Here, the ’s are the tunable parameters (weights) of the network, and is deﬁned to be the error in the -position (forward direction, in body coordinates) between where the helicopter currently is and where we wish it to hover. [sent-191, score-0.931]
</p><p>50 BgSeX'V ¡ ¢ cXV 2XXaV ¡ cX'7D ¨ G ¨ aSX7D ¨ G ¨ SX'7D G TSQIHE(B@ A ` P W V A Y P W V A R PA G F D CA  (  $  This encourages the helicopter to hover near , while also keeping the velocity small and not making abrupt movements. [sent-193, score-1.02]
</p><p>51 (distinct from the weights parameterizing our policy class) were chosen to scale each of the terms to be roughly the same order of magnitude. [sent-195, score-0.129]
</p><p>52 To encourage small actions and smooth control of the helicopter, we also used a quadratic penalty for actions: , and the overall reward was . [sent-196, score-0.104]
</p><p>53 One last component of the reward that we did not mention earlier was that, if in performing the locally weighted regression, the matrix is singular to numerical precision, then we declare the helicopter to have “crashed,” terminate the simulation, and give it a huge negative (-50000) reward. [sent-203, score-0.942]
</p><p>54 5  −80 −81  Figure 4: Top row: Maneuver diagrams from RC helicopter competition. [sent-220, score-0.906]
</p><p>55 The most expensive step in policy search was the repeated Monte Carlo evaluation to obtain . [sent-228, score-0.127]
</p><p>56 Figure 1b shows the result of implementing and of ﬂying time each, and running the resulting policy on the helicopter. [sent-231, score-0.111]
</p><p>57 On its maiden ﬂight, our learned policy was successful in keeping the helicopter stabilized in the air. [sent-232, score-1.054]
</p><p>58 (We note that [1] was also successful at using our P EGASUS algorithm to control a subset, the cyclic pitch controls, of a helicopter’s dynamics. [sent-233, score-0.183]
</p><p>59 ) We also compare the performance of our learned policy against that of our human pilot trained and licensed by Yamaha to ﬂy the R-50 helicopter. [sent-234, score-0.177]
</p><p>60 Figure 5 shows the velocities and positions of the helicopter under our learned policy and under the human pilot’s control. [sent-235, score-1.051]
</p><p>61 As we see, our controller was able to keep the helicopter ﬂying more stably than was a human pilot. [sent-236, score-0.984]
</p><p>62 Videos of the helicopter ﬂying are available at http://www. [sent-237, score-0.892]
</p><p>63 8  ( D#$  8  ( D#$  '  '  ( (#$  8  '  '  6 Flying competition maneuvers We were next interested in making the helicopter learn to ﬂy several challenging maneuvers. [sent-241, score-1.021]
</p><p>64 The Academy of Model Aeronautics (AMA) (to our knowledge the largest RC helicopter organization) holds an annual RC helicopter competition, in which helicopters have to be accurately ﬂown through a number of maneuvers. [sent-242, score-1.845]
</p><p>65 We took the ﬁrst three maneuvers from the most challenging, Class III, segment of their competition. [sent-244, score-0.082]
</p><p>66 Figure 4 shows maneuver diagrams from the AMA web site. [sent-245, score-0.096]
</p><p>67 In the ﬁrst of these maneuvers 4  A problem exacerbated by the discontinuities described in the previous footnote. [sent-246, score-0.082]
</p><p>68 1), the helicopter starts from the middle of the base of a triangle, ﬂies backwards to the lower-right corner, performs a pirouette (turning in place), ﬂies backwards up an edge of the triangle, backwards down the other edge, performs another pirouette, and ﬂies backwards to its starting position. [sent-248, score-1.066]
</p><p>69 Flying backwards is a signiﬁcantly less stable maneuver than ﬂying forwards, which makes this maneuver interesting and challenging. [sent-249, score-0.203]
</p><p>70 2), the helicopter has to perform a nose-in turn, in which it ﬂies backwards out to the edge of a circle, pauses, and then ﬂies in a circle but always keeping the nose of the helicopter pointed at center of rotation. [sent-251, score-1.84]
</p><p>71 Many human pilots seem to ﬁnd this second maneuver particularly challenging. [sent-253, score-0.116]
</p><p>72 3 involves ﬂying the helicopter in a vertical rectangle, with two pirouettes in opposite directions halfway along the rectangle’s vertical segments. [sent-255, score-0.922]
</p><p>73 Given a controller for keeping a system’s state at a point , one standard way to make the system move through a particular trajectory is to slowly vary along a sequence of set points on that trajectory. [sent-257, score-0.191]
</p><p>74 ) For instance, if we ask our helicopter to hover at , then a fraction of a second later ask it to hover at , then at and so on, our helicopter will slowly ﬂy in the -direction. [sent-261, score-1.943]
</p><p>75 By taking this procedure and “wrapping” it around our old policy class from Figure 2c, we thus obtain a computer program—that is, a new policy class—not just for hovering, but also for ﬂying arbitrary trajectories. [sent-262, score-0.239]
</p><p>76 , we now have a family of policies that take as input a trajectory, and that attempt to make the helicopter ﬂy that trajectory. [sent-265, score-0.906]
</p><p>77 Since we are now ﬂying trajectories and not only hovering, we also augmented the policy class to take into account more of the coupling between the helicopter’s different subdynamics. [sent-267, score-0.153]
</p><p>78 For instance, the simplest way to turn is to change the tail rotor collective pitch/thrust, so that it yaws either left or right. [sent-268, score-0.285]
</p><p>79 This works well for small turns, but for large turns, the thrust from the tail rotor also tends to cause the helicopter to drift sideways. [sent-269, score-1.193]
</p><p>80 Thus, we enriched the policy class to allow it to correct for this drift by applying the appropriate cyclic pitch controls. [sent-270, score-0.272]
</p><p>81 Also, having a helicopter climb or descend changes the amount of work done by the main rotor, and hence the amount of torque/anti-torque generated, which can cause the helicopter to turn. [sent-271, score-1.842]
</p><p>82 So, we also added a link between the collective pitch control and the tail rotor control. [sent-272, score-0.433]
</p><p>83 We also needed to specify a reward function for trajectory following. [sent-274, score-0.097]
</p><p>84 Speciﬁcally, consider making the helicopter ﬂy in the increasing -direction, so that starts off as (say), and has its ﬁrst coordinate slowly increased over time. [sent-277, score-0.907]
</p><p>85 Then, while will indeed increase, it will also almost certainly lag conthe actual helicopter position sistently behind . [sent-278, score-0.949]
</p><p>86 This is because the hovering controller is always trying to “catch up” to the moving . [sent-279, score-0.14]
</p><p>87 Thus, may remain large, and the helicopter will cost, even if it is in fact ﬂying a very accurate trajectory in the continuously incur a increasing -direction exactly as desired. [sent-280, score-0.956]
</p><p>88 It would be undesirable to have the helicopter risk trying to ﬂy more aggressively to reduce this fake “error,” particularly if it is at the cost of increased error in the other coordinates. [sent-281, score-0.892]
</p><p>89 (In our example of ﬂying in a straight line, for a helicopter at , we easily see . [sent-283, score-0.892]
</p><p>90 We also needed to make sure the helicopter is rewarded for making progress along the   '  ¢   £ ¡(   '  ¢   ¥ ¤(   '  ¢ § © ¨¦  (  ¡  ¡  ¡ (  6 F$ (£  (£ ! [sent-285, score-0.892]
</p><p>91 Since, we are already tracking where along the desired trajectory the helicopter is, we chose a potential function that increases along the trajectory. [sent-289, score-0.956]
</p><p>92 So, we are now also free to consider allowing to evolve in a way that is different from the path of the desired trajectory, but nonetheless in way that allows the helicopter to follow the actual, desired trajectory more accurately. [sent-293, score-0.956]
</p><p>93 (In control theory, there is a related practice of using the inverse dynamics to obtain better tracking behavior. [sent-294, score-0.084]
</p><p>94 Speciﬁcally, it turns out that the (vertical)-response of the helicopter is very fast: To climb, we need only increase the collective pitch control, which almost immediately causes the helicopter to start accelerating upwards. [sent-297, score-1.922]
</p><p>95 1, the helicopter will tend to track the -component of the trajectory much more quickly, so that it accelerates into a climb steeper than , resulting in a “bowed-out” trajectory. [sent-300, score-0.997]
</p><p>96 Using this setup and retraining our policy class’ parameters for accurate trajectory following, we were able to learn a policy that ﬂies all three of the competition maneuvers fairly accurately. [sent-303, score-0.396]
</p><p>97 Figure 4 (bottom) shows actual trajectories taken by the helicopter while ﬂying these maneuvers. [sent-304, score-0.939]
</p><p>98 Videos of the helicopter ﬂying these maneuvers are also available at the URL given at the end of Section 5. [sent-305, score-0.974]
</p><p>99 Autonomous helicopter control using reinforcement learning policy search methods. [sent-309, score-1.107]
</p><p>100 P EGASUS: A policy search method for large MDPs and POMDPs. [sent-362, score-0.127]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('helicopter', 0.892), ('rotor', 0.195), ('ying', 0.143), ('policy', 0.111), ('pitch', 0.091), ('hovering', 0.082), ('maneuver', 0.082), ('maneuvers', 0.082), ('hover', 0.072), ('egasus', 0.071), ('trajectory', 0.064), ('helicopters', 0.061), ('controller', 0.058), ('control', 0.057), ('ight', 0.053), ('ies', 0.049), ('autonomous', 0.047), ('collective', 0.047), ('tail', 0.043), ('climb', 0.041), ('seconds', 0.039), ('velocity', 0.039), ('backwards', 0.039), ('state', 0.037), ('utilities', 0.036), ('position', 0.035), ('reward', 0.033), ('drift', 0.032), ('pilot', 0.032), ('reinforcement', 0.031), ('blades', 0.031), ('sideways', 0.031), ('thrust', 0.031), ('xdot', 0.031), ('yamaha', 0.031), ('competition', 0.028), ('dynamics', 0.027), ('symmetries', 0.027), ('rad', 0.027), ('rc', 0.026), ('regression', 0.025), ('trajectories', 0.025), ('body', 0.025), ('idealized', 0.024), ('coordinates', 0.022), ('actual', 0.022), ('cyclic', 0.021), ('ama', 0.02), ('clockwise', 0.02), ('counteract', 0.02), ('flying', 0.02), ('ins', 0.02), ('pilots', 0.02), ('stably', 0.02), ('air', 0.02), ('facing', 0.02), ('simulator', 0.02), ('evaluations', 0.02), ('learned', 0.02), ('monte', 0.02), ('carlo', 0.02), ('accelerate', 0.019), ('challenging', 0.019), ('weights', 0.018), ('lines', 0.018), ('tilt', 0.018), ('downwards', 0.018), ('pirouette', 0.018), ('wrapping', 0.018), ('plane', 0.017), ('keeping', 0.017), ('class', 0.017), ('main', 0.017), ('locally', 0.017), ('forward', 0.017), ('controls', 0.017), ('mdp', 0.016), ('roll', 0.016), ('angled', 0.016), ('videos', 0.016), ('predicted', 0.016), ('andrew', 0.016), ('search', 0.016), ('triangle', 0.015), ('upward', 0.015), ('forwards', 0.015), ('began', 0.015), ('intercept', 0.015), ('solid', 0.015), ('slowly', 0.015), ('vertical', 0.015), ('human', 0.014), ('actions', 0.014), ('carries', 0.014), ('shaping', 0.014), ('velocities', 0.014), ('diagrams', 0.014), ('tunable', 0.014), ('policies', 0.014), ('successful', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="38-tfidf-1" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>Author: H. J. Kim, Michael I. Jordan, Shankar Sastry, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition.</p><p>2 0.094253302 <a title="38-tfidf-2" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>3 0.087515332 <a title="38-tfidf-3" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>Author: J. A. Bagnell, Sham M. Kakade, Jeff G. Schneider, Andrew Y. Ng</p><p>Abstract: We consider the policy search approach to reinforcement learning. We show that if a “baseline distribution” is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a ﬁnite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem. 1</p><p>4 0.063874774 <a title="38-tfidf-4" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>5 0.061576273 <a title="38-tfidf-5" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>6 0.060139794 <a title="38-tfidf-6" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>7 0.053475551 <a title="38-tfidf-7" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>8 0.050555062 <a title="38-tfidf-8" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>9 0.045062061 <a title="38-tfidf-9" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>10 0.043611933 <a title="38-tfidf-10" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>11 0.040693235 <a title="38-tfidf-11" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>12 0.034752283 <a title="38-tfidf-12" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>13 0.033500414 <a title="38-tfidf-13" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>14 0.032227792 <a title="38-tfidf-14" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>15 0.031894997 <a title="38-tfidf-15" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<p>16 0.030881623 <a title="38-tfidf-16" href="./nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</a></p>
<p>17 0.02855441 <a title="38-tfidf-17" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>18 0.02587671 <a title="38-tfidf-18" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>19 0.024523553 <a title="38-tfidf-19" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>20 0.024344014 <a title="38-tfidf-20" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.091), (1, 0.116), (2, -0.037), (3, 0.004), (4, -0.035), (5, 0.022), (6, -0.03), (7, -0.056), (8, 0.056), (9, 0.024), (10, -0.036), (11, -0.007), (12, 0.018), (13, 0.03), (14, -0.023), (15, 0.011), (16, -0.009), (17, -0.032), (18, -0.023), (19, -0.023), (20, 0.01), (21, -0.012), (22, 0.053), (23, 0.013), (24, -0.109), (25, -0.038), (26, 0.039), (27, -0.031), (28, 0.027), (29, -0.028), (30, -0.057), (31, 0.013), (32, 0.016), (33, 0.028), (34, -0.031), (35, 0.011), (36, -0.005), (37, 0.024), (38, -0.026), (39, -0.001), (40, -0.005), (41, 0.032), (42, -0.019), (43, -0.001), (44, 0.001), (45, -0.155), (46, -0.004), (47, 0.055), (48, -0.003), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9083907 <a title="38-lsi-1" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>Author: H. J. Kim, Michael I. Jordan, Shankar Sastry, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition.</p><p>2 0.76836663 <a title="38-lsi-2" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>Author: J. A. Bagnell, Sham M. Kakade, Jeff G. Schneider, Andrew Y. Ng</p><p>Abstract: We consider the policy search approach to reinforcement learning. We show that if a “baseline distribution” is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a ﬁnite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem. 1</p><p>3 0.62640595 <a title="38-lsi-3" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>Author: Ciamac C. Moallemi, Benjamin V. Roy</p><p>Abstract: We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overﬂow in a sensor network. Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective. 1</p><p>4 0.61024195 <a title="38-lsi-4" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>5 0.59716505 <a title="38-lsi-5" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>Author: Arnab Nilim, Laurent El Ghaoui</p><p>Abstract: Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to realworld problems. We propose an algorithm for solving ﬁnite-state and ﬁnite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efﬁcient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.</p><p>6 0.59188849 <a title="38-lsi-6" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>7 0.53206396 <a title="38-lsi-7" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>8 0.52316666 <a title="38-lsi-8" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>9 0.40855405 <a title="38-lsi-9" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>10 0.38915506 <a title="38-lsi-10" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>11 0.38175824 <a title="38-lsi-11" href="./nips-2003-Different_Cortico-Basal_Ganglia_Loops_Specialize_in_Reward_Prediction_at_Different_Time_Scales.html">52 nips-2003-Different Cortico-Basal Ganglia Loops Specialize in Reward Prediction at Different Time Scales</a></p>
<p>12 0.3578555 <a title="38-lsi-12" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>13 0.32717365 <a title="38-lsi-13" href="./nips-2003-Salient_Boundary_Detection_using_Ratio_Contour.html">168 nips-2003-Salient Boundary Detection using Ratio Contour</a></p>
<p>14 0.32278609 <a title="38-lsi-14" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>15 0.31597748 <a title="38-lsi-15" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>16 0.29059941 <a title="38-lsi-16" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>17 0.28480023 <a title="38-lsi-17" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>18 0.26885936 <a title="38-lsi-18" href="./nips-2003-Online_Learning_of_Non-stationary_Sequences.html">146 nips-2003-Online Learning of Non-stationary Sequences</a></p>
<p>19 0.24330266 <a title="38-lsi-19" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>20 0.23811515 <a title="38-lsi-20" href="./nips-2003-Parameterized_Novelty_Detectors_for_Environmental_Sensor_Monitoring.html">153 nips-2003-Parameterized Novelty Detectors for Environmental Sensor Monitoring</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.043), (11, 0.023), (29, 0.026), (30, 0.016), (35, 0.044), (53, 0.079), (69, 0.011), (71, 0.052), (76, 0.036), (82, 0.024), (85, 0.068), (91, 0.092), (92, 0.312), (99, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75418377 <a title="38-lda-1" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>Author: H. J. Kim, Michael I. Jordan, Shankar Sastry, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition.</p><p>2 0.46917766 <a title="38-lda-2" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>3 0.46430752 <a title="38-lda-3" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><p>4 0.46134979 <a title="38-lda-4" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>Author: J. A. Bagnell, Sham M. Kakade, Jeff G. Schneider, Andrew Y. Ng</p><p>Abstract: We consider the policy search approach to reinforcement learning. We show that if a “baseline distribution” is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a ﬁnite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem. 1</p><p>5 0.45818099 <a title="38-lda-5" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>6 0.45720908 <a title="38-lda-6" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>7 0.45662409 <a title="38-lda-7" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>8 0.45627189 <a title="38-lda-8" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>9 0.45562008 <a title="38-lda-9" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>10 0.45554453 <a title="38-lda-10" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>11 0.45551354 <a title="38-lda-11" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>12 0.45533982 <a title="38-lda-12" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>13 0.45281109 <a title="38-lda-13" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>14 0.45251793 <a title="38-lda-14" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>15 0.4524323 <a title="38-lda-15" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>16 0.45230559 <a title="38-lda-16" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>17 0.45185807 <a title="38-lda-17" href="./nips-2003-On_the_Dynamics_of_Boosting.html">143 nips-2003-On the Dynamics of Boosting</a></p>
<p>18 0.45065102 <a title="38-lda-18" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>19 0.45024547 <a title="38-lda-19" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>20 0.44960836 <a title="38-lda-20" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
