<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-134" href="#">nips2003-134</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</h1>
<br/><p>Source: <a title="nips-2003-134-pdf" href="http://papers.nips.cc/paper/2364-near-minimax-optimal-classification-with-dyadic-classification-trees.pdf">pdf</a></p><p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: This paper reports on a family of computationally practical classiﬁers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classiﬁers are based on dyadic classiﬁcation trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classiﬁcation rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n−1/2 , the parametric rate. We are not aware of any other practical classiﬁers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed. 1</p><p>Reference: <a title="nips-2003-134-reference" href="../nips2003_reference/nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The classiﬁers are based on dyadic classiﬁcation trees (DCTs), which involve adaptively pruned partitions of the feature space. [sent-5, score-0.617]
</p><p>2 A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. [sent-6, score-0.153]
</p><p>3 Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. [sent-7, score-0.492]
</p><p>4 For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. [sent-8, score-0.696]
</p><p>5 We also study DCTs equipped with polynomial classiﬁcation rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n−1/2 , the parametric rate. [sent-9, score-0.433]
</p><p>6 We are not aware of any other practical classiﬁers that provide similar rate of convergence guarantees. [sent-10, score-0.201]
</p><p>7 1  Introduction  We previously studied dyadic classiﬁcation trees, equipped with simple binary decision rules at each leaf, in [1]. [sent-12, score-0.497]
</p><p>8 There we applied standard structural risk minimization to derive a pruning rule that minimizes the empirical error plus a complexity penalty proportional to the square root of the size of the tree. [sent-13, score-0.629]
</p><p>9 Our main result concerned the rate of convergence of the expected error probability of our pruned dyadic classiﬁcation tree to the Bayes error for a certain class of problems. [sent-14, score-0.928]
</p><p>10 This class, which essentially requires the Bayes decision boundary to be locally Lipschitz, had previously been studied by Mammen and Tsybakov [2]. [sent-15, score-0.238]
</p><p>11 They showed the minimax rate of convergence for this class to be n−1/d , where n is the number of labeled training samples, and d is the dimension of each sample. [sent-16, score-0.307]
</p><p>12 They also demonstrated a classiﬁcation rule achieving this rate, but the rule requires minimization of the empirical error over the entire class of decision boundaries, an infeasible task in practice. [sent-17, score-0.322]
</p><p>13 In this paper we exhibit a new pruning strategy that is both computationally efﬁcient and realizes the minimax rate to within a log factor. [sent-19, score-0.598]
</p><p>14 Those works develop a theory of local uniform convergence, which allows the error to be decomposed in a spatially adaptive way (unlike conventional structural risk minimization). [sent-21, score-0.272]
</p><p>15 In essence, the associated pruning rules allow a more reﬁned partition in a region where the classiﬁcation problem is harder (i. [sent-22, score-0.397]
</p><p>16 Heuristic arguments and anecdotal evidence in both [3] and [4] suggest that spatially adaptive penalties lead to improved performance compared to “global” penalties. [sent-25, score-0.186]
</p><p>17 In this work, we give theoretical support to this claim (for a speciﬁc kind of classiﬁcation tree, the DCT) by showing a superior rate of convergence for DCTs pruned according to spatially adaptive penalties. [sent-26, score-0.486]
</p><p>18 We go on to study DCTs equipped with polynomial classiﬁcation rules at each leaf. [sent-27, score-0.174]
</p><p>19 PDCTs can be practically implemented by employing polynomial kernel SVMs at each leaf node of a pruned DCT. [sent-30, score-0.533]
</p><p>20 For any distribution whose Bayes decision boundary behaves locally like a H¨ lder-γ smooth function, we show that the PDCT error converges to the o Bayes error at a rate no slower than O((log n/n)γ/(d+2γ−2) ). [sent-31, score-0.543]
</p><p>21 As γ → ∞ the rate tends to within a log factor of the parametric rate, n−1/2 . [sent-32, score-0.251]
</p><p>22 Perceptron trees, tree classiﬁers having linear splits at each node, have been investigated by many authors and in particular we point to the works [5,6]. [sent-33, score-0.146]
</p><p>23 A key aspect of PDCTs is their spatial adaptivity, which enables local (rather than global) polynomial ﬁtting of the decision boundary. [sent-35, score-0.249]
</p><p>24 Traditional polynomial kernel-based methods are not capable of achieving such rates of convergence due to their lack of spatial adaptivity, and it is unlikely that other kernels can solve this problem for the same reason. [sent-36, score-0.31]
</p><p>25 Then the error in approximation is O(1), a constant, which is the best one could hope for in learning a H¨ lder smooth boundo ary with a traditional polynomial kernel scheme. [sent-38, score-0.266]
</p><p>26 On the other hand, if we partition the domain into hypercubes of side length O(1/m) and ﬁt an individual polynomial on each hypercube, then the approximation error decays like O(m−γ ). [sent-39, score-0.229]
</p><p>27 On the other hand, pruning back the partition helps to avoid overﬁtting. [sent-41, score-0.355]
</p><p>28 2  Dyadic Classiﬁcation Trees  In this section we review our earlier results on dyadic classiﬁcation trees. [sent-43, score-0.311]
</p><p>29 DCTs are based on the concept of a cyclic dyadic partition (CDP). [sent-47, score-0.397]
</p><p>30 We deﬁne a dyadic classiﬁcation tree (DCT) to be a cyclic dyadic partition with  (a)  (b)  (c)  Figure 1: Example of a dyadic classiﬁcation tree when d = 2. [sent-68, score-1.311]
</p><p>31 Polynomial-decorated DCTs, discussed in Section 4, are similar in structure, but a polynomial decision rule is employed at each leaf of the pruned tree, instead of a simple binary label. [sent-72, score-0.603]
</p><p>32 Previously we presented a rule for pruning DCTs with consistency and rate of convergence properties. [sent-76, score-0.527]
</p><p>33 Let m = 2J be a dyadic integer, and deﬁne T0 to be the DCT that has every leaf node at depth dJ. [sent-78, score-0.652]
</p><p>34 Then each leaf of T0 corresponds to a cube of side length 1/m, and T0 has md total leaf nodes. [sent-79, score-0.487]
</p><p>35 A subtree T of T0 is referred to as a pruned subtree, denoted T ≤ T0 , if T includes the root of T0 , if every internal node of T has both its children in T , and if the nodes of T inherit their labels from T0 . [sent-81, score-0.503]
</p><p>36 The size of a tree T , denoted |T |, is the number of leaf nodes. [sent-82, score-0.321]
</p><p>37 We deﬁned the complexity penalized dyadic classiﬁcation tree Tn to be the solution of Tn = arg min ˆ(T ) + αn T ≤T0  |T |,  (1)  where αn = 32 log(en)/n, and ˆ(T ) is the empirical error, i. [sent-83, score-0.521]
</p><p>38 (The solution to this pruning problem can be computed efﬁciently [7]. [sent-86, score-0.301]
</p><p>39 ) We showed that if X ∈ [0, 1]d with probability one, and md = o(n/ log n), then E{ (Tn )} → ∗ with probability one (i. [sent-87, score-0.165]
</p><p>40 We also demonstrated a rate of convergence result for Tn , under certain assumptions on the distribution of (X, Y ). [sent-93, score-0.174]
</p><p>41 Deﬁnition 1 Let c1 , c2 > 0, and let m0 be a dyadic integer. [sent-96, score-0.349]
</p><p>42 A2 (Regularity): For all dyadic integers m ≥ m0 , if we subdivide the unit cube into cubes of side length 1/m, The Bayes decision boundary passes through at most c2 md−1 of the resulting md cubes. [sent-98, score-0.715]
</p><p>43 These assumptions are satisﬁed when the density of X is essentially bounded with respect to Lebesgue measure, and when the Bayes decision boundary for the distribution on (X, Y ) behaves locally like a Lipschitz function. [sent-99, score-0.321]
</p><p>44 See, for example, the boundary fragment class of [2] with γ = 1 therein. [sent-100, score-0.148]
</p><p>45 In [1], we showed that if the distribution of (X, Y ) belongs to F, and m ∼ 1/(d+1) (n/ log n)1/(d+1) , then E{ (Tn )} − ∗ = O((log n/n) ). [sent-101, score-0.132]
</p><p>46 However, this upper bound on the rate of convergence is not tight. [sent-102, score-0.22]
</p><p>47 The results of Mammen and Tsybakov [2] show that the minimax rate of convergence, inf φn supF E{ (φn )} − ∗ , is on the order of n−1/d (here φn ranges over all possible discrimination rules). [sent-103, score-0.195]
</p><p>48 In the next section, we introduce a new strategy for pruning DCTs, which leads to an improved rate of convergence of (log n/n)1/d (i. [sent-104, score-0.475]
</p><p>49 3  Improved Tree Pruning with Spatially Adaptive Penalties  An improved rate of convergence is achieved by pruning the initial tree T0 using a new complexity penalty. [sent-108, score-0.621]
</p><p>50 Given a node v in a tree T , let Tv denote the subtree of T rooted at v. [sent-109, score-0.418]
</p><p>51 Let S denote the training data, and let nv denote the number of training samples reaching node v. [sent-110, score-0.193]
</p><p>52 Consider the pruning rule that selects Tn = arg min ˆ(T ) + min ∆(T, S, R) , where ∆(T, S, R) = v∈L(R)  (2)  R≤T  T ≤T0  1 n  48nv |Tv | log(2n) +  48nv d log(m) . [sent-114, score-0.481]
</p><p>53 Observe that the penalty is data-dependent (since nv depends on S) and spatially adaptive (choosing R ≤ T to minimize ∆). [sent-115, score-0.275]
</p><p>54 The ﬁrst term in the penalty is written v∈L(R) pv 48|Tv | log(2n)/nv , where pv = nv /n. [sent-117, score-0.193]
</p><p>55 The second term can be interpreted as the “cost” of spatially decomposing the bound on the generalization error. [sent-119, score-0.163]
</p><p>56 Consider pruning one of two subtrees, both with the same size, and assume that both options result in the same increase in the empirical error. [sent-121, score-0.301]
</p><p>57 Then the subtree with more data is selected for pruning. [sent-122, score-0.144]
</p><p>58 Since deeper nodes typically have less data, this shows that the penalty favors unbalanced trees, which may promote higher resolution (deeper leaf nodes) in the vicinity of the decision boundary. [sent-123, score-0.479]
</p><p>59 In contrast, the pruning rule (1) penalizes balanced and unbalanced trees (with the same size) equally. [sent-124, score-0.533]
</p><p>60 The following theorem bounds the expected error of Tn . [sent-125, score-0.131]
</p><p>61 Recall that m speciﬁes the depth of the initial tree T0 . [sent-127, score-0.222]
</p><p>62 Theorem 1 If m ∼ (n/ log n)1/d , then E{ (Tn ) −  ∗  } ≤ min  T ≤T0  ( (T ) −  ∗  ) + E min ∆(T, S, R) R≤T  +O  log n n  . [sent-128, score-0.332]
</p><p>63 Since the bound holds for all T , one feature of the pruning rule (2) is that Tn performs at least as well as the subtree T ≤ T0 that minimizes the bound. [sent-131, score-0.57]
</p><p>64 This theorem may be applied to give us our desired rate of convergence result. [sent-132, score-0.257]
</p><p>65 In other words, the pruning rule (2) comes within a log factor of the minimax rate. [sent-135, score-0.587]
</p><p>66 Within each cube the Bayes decision boundary is described by a function (one coordinate is a function of the others) with H¨ lder regularity γ. [sent-141, score-0.376]
</p><p>67 o The collection G contains all distributions whose Bayes decision boundaries behave locally like the graph of a function with H¨ lder regularity γ. [sent-142, score-0.352]
</p><p>68 We propose a classiﬁer, called a polynomial-decorated dyadic classiﬁcation tree (PDCT), that achieves fast rates of convergence for distributions satisfying A3. [sent-144, score-0.593]
</p><p>69 Given a positive integer r, a PDCT of degree r is a DCT, with class labels at each leaf node assigned by a degree r polynomial classiﬁer. [sent-145, score-0.495]
</p><p>70 Consider the pruning rule that selects Tn,r = arg min ˆ(T ) + min ∆r (T, S, R) , R≤T  T ≤T0  (3)  where ∆r (T, S, R) = v  1 n  48nv Vd,r |Tv | log(2n) +  48nv (d + γ) log(m) . [sent-146, score-0.481]
</p><p>71 Here Vd,r = d+r is the V C dimension of the collection of degree r polynomial classiﬁers r in d dimensions. [sent-147, score-0.162]
</p><p>72 We actually consider a search over all pruned subtrees of T0 , and with all possible conﬁgurations of degree r polynomial classiﬁers at the leaf nodes. [sent-149, score-0.536]
</p><p>73 Moreover, If r = γ − 1, then a decision boundary with H¨ lder regularity γ is well approximated by o a PDCT of degree r. [sent-151, score-0.372]
</p><p>74 In this case, Tn,r converges to the Bayes risk at rates bounded by the next theorem. [sent-152, score-0.14]
</p><p>75 Also notice that as γ → ∞, the rate of convergence comes within a logarithmic factor of the parametric rate n−1/2 . [sent-156, score-0.371]
</p><p>76 5  Efﬁcient Algorithms  The optimally pruned subtree Tn of rule (2) can be computed exactly in O(|T0 |2 ) operations. [sent-158, score-0.368]
</p><p>77 This follows from a simple bottom-up dynamic programming algorithm, which we  describe below, and uses a method for “square-root” pruning studied in [7]. [sent-159, score-0.301]
</p><p>78 Given a node ∗ v ∈ T0 , let Tv be the subtree of T0 rooted at v that minimizes the objective function of (2), ∗ ∗ and let Rv be the associated subtree that minimizes ∆(Tv , S, R). [sent-163, score-0.508]
</p><p>79 ∗ ∗ If v is a leaf node of T0 , then clearly Tv = Rv = {v}. [sent-165, score-0.265]
</p><p>80 Using the ﬁrst algorithm in [7], the overall pruning procedure may be accomplished in (|T0 |2 ) operations. [sent-169, score-0.301]
</p><p>81 Determining the optimally pruned degree r PDCT is more challenging. [sent-170, score-0.211]
</p><p>82 The problem requires the construction, at each node of T0 , a polynomial classiﬁer of degree r having minimum empirical error. [sent-171, score-0.225]
</p><p>83 Moreover, linear SVMs in perceptron trees have been shown to work well [6]. [sent-175, score-0.175]
</p><p>84 6  Conclusions  A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. [sent-176, score-0.153]
</p><p>85 Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion that promotes unbalanced trees that focus on the decision boundary. [sent-177, score-0.78]
</p><p>86 For distributions on (X, Y ) whose Bayes decision boundary behave locally like a H¨ lder-γ smooth o function, we show that the PDCT error converges to the Bayes error at a rate no slower than O((log n/n)γ/(d+2γ−2) ). [sent-178, score-0.491]
</p><p>87 Polynomial kernel methods are not capable of achieving such rates due to their lack of spatial adaptivity. [sent-179, score-0.131]
</p><p>88 When γ = 1, the DCT convergence rate is within a logarithmic factor of the minimax optimal rate. [sent-180, score-0.354]
</p><p>89 As γ → ∞ the rate tends to within a log factor of n−1/2 , the parametric rate. [sent-181, score-0.251]
</p><p>90 However, the rates for γ > 1 are not within a logarithmic factor of the minimax rate [2]. [sent-182, score-0.324]
</p><p>91 m Given S ∈ Ωm , we know (Tn ) ≤ = ≤  ˆ(Tn ) + min f (Tn , S, R, 3m−d ) R≤Tn  4d log(m) R≤Tn n 4d log(m) ˆ(T ) + min ∆(T, S, R) + , R≤T n ˆ(Tn ) + min ∆(Tn , S, R) +  where the last inequality comes from the deﬁnition of Tn . [sent-193, score-0.192]
</p><p>92 1  Proof of Theorem 2  By Theorem 1, it sufﬁces to ﬁnd a tree T ∗ ≤ T0 such that E min∗ ∆(T ∗ , S, R) + ( (T ∗ ) − R≤T  ∗  )=O  log n n  1 d  . [sent-197, score-0.248]
</p><p>93 Deﬁne T ∗ to be the tree obtained by pruning back T0 at every node (thought of as a region of space) that does not intersect the Bayes decision boundary. [sent-198, score-0.645]
</p><p>94 Deﬁne R∗ to be the pruned subtree of T ∗ consisting of all nodes in T ∗ up to depth j0 d, where j0 = J − (1/d) log2 (J) (truncated if necessary). [sent-202, score-0.442]
</p><p>95 The ﬁrst term, consisting of a sum of terms over the leaf nodes of R∗ , is dominated by the sum of those terms over the leaf nodes of R∗ at depth j0 d. [sent-212, score-0.526]
</p><p>96 Recall the leaf nodes of T ∗ at maximum depth are cells of side length 1/m. [sent-219, score-0.332]
</p><p>97 Nowak, “Dyadic classiﬁcation trees via structural risk minimization,” in Advances in Neural Information Processing Systems 14, S. [sent-228, score-0.218]
</p><p>98 Mansour, “A fast, bottom-up decision tree pruning algorithm with nearoptimal generalization,” in International Conference on Machine Learning, 1998, pp. [sent-241, score-0.555]
</p><p>99 Wu, “Enlarging the margins in perceptron decision trees,” Machine Learning, vol. [sent-258, score-0.149]
</p><p>100 Nowak, “Complexity-regularized dyadic classiﬁcation trees: Efﬁcient pruning and rates of convergence,” Tech. [sent-277, score-0.665]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tn', 0.313), ('dyadic', 0.311), ('tv', 0.305), ('pruning', 0.301), ('dcts', 0.248), ('rv', 0.207), ('leaf', 0.175), ('pruned', 0.172), ('tree', 0.146), ('pdct', 0.145), ('subtree', 0.144), ('dct', 0.144), ('trees', 0.134), ('bayes', 0.128), ('decision', 0.108), ('minimax', 0.104), ('classi', 0.104), ('log', 0.102), ('polynomial', 0.096), ('rate', 0.091), ('node', 0.09), ('spatially', 0.09), ('boundary', 0.09), ('theorem', 0.083), ('convergence', 0.083), ('cdp', 0.083), ('lder', 0.083), ('mammen', 0.083), ('nowak', 0.083), ('ri', 0.079), ('depth', 0.076), ('mansour', 0.072), ('tsybakov', 0.072), ('adaptivity', 0.072), ('penalty', 0.07), ('nv', 0.065), ('min', 0.064), ('md', 0.063), ('rice', 0.062), ('ers', 0.061), ('risk', 0.056), ('scott', 0.055), ('partition', 0.054), ('subtrees', 0.054), ('rates', 0.053), ('regularity', 0.052), ('behaves', 0.052), ('rule', 0.052), ('adaptive', 0.05), ('nodes', 0.05), ('lemma', 0.049), ('logarithmic', 0.048), ('error', 0.048), ('root', 0.047), ('penalties', 0.046), ('mcallester', 0.046), ('lipschitz', 0.046), ('unbalanced', 0.046), ('bound', 0.046), ('spatial', 0.045), ('rk', 0.043), ('cube', 0.043), ('boundaries', 0.042), ('rules', 0.042), ('cation', 0.042), ('perceptron', 0.041), ('pdcts', 0.041), ('resolvability', 0.041), ('locally', 0.04), ('degree', 0.039), ('smooth', 0.039), ('let', 0.038), ('equipped', 0.036), ('subdivide', 0.036), ('okamoto', 0.036), ('achieving', 0.033), ('cubes', 0.033), ('collections', 0.033), ('cyclic', 0.032), ('inequalities', 0.032), ('bounded', 0.031), ('side', 0.031), ('parametric', 0.03), ('deeper', 0.03), ('belongs', 0.03), ('proof', 0.029), ('class', 0.029), ('bennett', 0.029), ('fragment', 0.029), ('pv', 0.029), ('lebesgue', 0.029), ('factor', 0.028), ('structural', 0.028), ('collection', 0.027), ('minimizes', 0.027), ('slower', 0.027), ('aware', 0.027), ('svms', 0.027), ('integer', 0.027), ('generalization', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="134-tfidf-1" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: This paper reports on a family of computationally practical classiﬁers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classiﬁers are based on dyadic classiﬁcation trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classiﬁcation rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n−1/2 , the parametric rate. We are not aware of any other practical classiﬁers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed. 1</p><p>2 0.11207295 <a title="134-tfidf-2" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>Author: David Kauchak, Sanjoy Dasgupta</p><p>Abstract: We describe a procedure which ﬁnds a hierarchical clustering by hillclimbing. The cost function we use is a hierarchical extension of the k-means cost; our local moves are tree restructurings and node reorderings. We show these can be accomplished efﬁciently, by exploiting special properties of squared Euclidean distances and by using techniques from scheduling algorithms. 1</p><p>3 0.10585114 <a title="134-tfidf-3" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><p>4 0.10490619 <a title="134-tfidf-4" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>5 0.098512203 <a title="134-tfidf-5" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>6 0.089494355 <a title="134-tfidf-6" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>7 0.088718504 <a title="134-tfidf-7" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>8 0.081217803 <a title="134-tfidf-8" href="./nips-2003-A_Probabilistic_Model_of_Auditory_Space_Representation_in_the_Barn_Owl.html">15 nips-2003-A Probabilistic Model of Auditory Space Representation in the Barn Owl</a></p>
<p>9 0.078463167 <a title="134-tfidf-9" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>10 0.070045583 <a title="134-tfidf-10" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>11 0.066400029 <a title="134-tfidf-11" href="./nips-2003-Minimax_Embeddings.html">128 nips-2003-Minimax Embeddings</a></p>
<p>12 0.064199284 <a title="134-tfidf-12" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>13 0.063787602 <a title="134-tfidf-13" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>14 0.061404828 <a title="134-tfidf-14" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>15 0.061156433 <a title="134-tfidf-15" href="./nips-2003-An_Infinity-sample_Theory_for_Multi-category_Large_Margin_Classification.html">23 nips-2003-An Infinity-sample Theory for Multi-category Large Margin Classification</a></p>
<p>16 0.061018191 <a title="134-tfidf-16" href="./nips-2003-Salient_Boundary_Detection_using_Ratio_Contour.html">168 nips-2003-Salient Boundary Detection using Ratio Contour</a></p>
<p>17 0.060097985 <a title="134-tfidf-17" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>18 0.059643768 <a title="134-tfidf-18" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>19 0.059373621 <a title="134-tfidf-19" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>20 0.05752394 <a title="134-tfidf-20" href="./nips-2003-PAC-Bayesian_Generic_Chaining.html">151 nips-2003-PAC-Bayesian Generic Chaining</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.194), (1, -0.041), (2, -0.056), (3, -0.003), (4, 0.061), (5, -0.064), (6, -0.054), (7, -0.032), (8, -0.043), (9, 0.144), (10, 0.018), (11, 0.015), (12, 0.045), (13, 0.036), (14, 0.028), (15, -0.06), (16, -0.057), (17, 0.114), (18, 0.169), (19, -0.02), (20, -0.023), (21, -0.103), (22, -0.06), (23, 0.004), (24, 0.005), (25, 0.103), (26, 0.085), (27, 0.153), (28, 0.147), (29, -0.036), (30, 0.04), (31, 0.104), (32, 0.173), (33, 0.121), (34, -0.141), (35, -0.131), (36, -0.085), (37, 0.043), (38, -0.031), (39, 0.025), (40, -0.01), (41, 0.091), (42, 0.117), (43, -0.133), (44, 0.075), (45, -0.076), (46, -0.027), (47, -0.02), (48, -0.031), (49, -0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9501487 <a title="134-lsi-1" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: This paper reports on a family of computationally practical classiﬁers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classiﬁers are based on dyadic classiﬁcation trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classiﬁcation rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n−1/2 , the parametric rate. We are not aware of any other practical classiﬁers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed. 1</p><p>2 0.72886616 <a title="134-lsi-2" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><p>3 0.63748837 <a title="134-lsi-3" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>Author: David Kauchak, Sanjoy Dasgupta</p><p>Abstract: We describe a procedure which ﬁnds a hierarchical clustering by hillclimbing. The cost function we use is a hierarchical extension of the k-means cost; our local moves are tree restructurings and node reorderings. We show these can be accomplished efﬁciently, by exploiting special properties of squared Euclidean distances and by using techniques from scheduling algorithms. 1</p><p>4 0.63073659 <a title="134-lsi-4" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>Author: Charles Kemp, Thomas L. Griffiths, Sean Stromsten, Joshua B. Tenenbaum</p><p>Abstract: We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efﬁcient computation of the optimal Bayesian classiﬁcation function from the labeled examples. We test our approach on eight real-world datasets. 1</p><p>5 0.50087577 <a title="134-lsi-5" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><p>6 0.44161585 <a title="134-lsi-6" href="./nips-2003-An_Infinity-sample_Theory_for_Multi-category_Large_Margin_Classification.html">23 nips-2003-An Infinity-sample Theory for Multi-category Large Margin Classification</a></p>
<p>7 0.43186438 <a title="134-lsi-7" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>8 0.39860681 <a title="134-lsi-8" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>9 0.38720408 <a title="134-lsi-9" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>10 0.37986505 <a title="134-lsi-10" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>11 0.35846967 <a title="134-lsi-11" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>12 0.35571325 <a title="134-lsi-12" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>13 0.3557086 <a title="134-lsi-13" href="./nips-2003-Reasoning_about_Time_and_Knowledge_in_Neural_Symbolic_Learning_Systems.html">165 nips-2003-Reasoning about Time and Knowledge in Neural Symbolic Learning Systems</a></p>
<p>14 0.34882116 <a title="134-lsi-14" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>15 0.340754 <a title="134-lsi-15" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>16 0.3405219 <a title="134-lsi-16" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>17 0.33925122 <a title="134-lsi-17" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>18 0.33348337 <a title="134-lsi-18" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>19 0.32639745 <a title="134-lsi-19" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>20 0.31588364 <a title="134-lsi-20" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (1, 0.315), (11, 0.02), (29, 0.01), (30, 0.015), (35, 0.047), (53, 0.075), (69, 0.026), (71, 0.066), (76, 0.029), (85, 0.134), (91, 0.078), (99, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78713983 <a title="134-lda-1" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: This paper reports on a family of computationally practical classiﬁers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classiﬁers are based on dyadic classiﬁcation trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) ﬁtting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y ) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classiﬁcation rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n−1/2 , the parametric rate. We are not aware of any other practical classiﬁers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed. 1</p><p>2 0.68819761 <a title="134-lda-2" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>Author: Liva Ralaivola, Florence D'alché-buc</p><p>Abstract: We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: ﬁrst, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions. 1</p><p>3 0.53221786 <a title="134-lda-3" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>Author: Ben Taskar, Carlos Guestrin, Daphne Koller</p><p>Abstract: In typical classiﬁcation tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of conﬁdence of the classiﬁer, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efﬁciently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efﬁcient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classiﬁcation demonstrate very signiﬁcant gains over previous approaches. 1</p><p>4 0.52458876 <a title="134-lda-4" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>Author: Kevin P. Murphy, Antonio Torralba, William T. Freeman</p><p>Abstract: Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not. We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities. We present a conditional random ﬁeld for jointly solving the tasks of object detection and scene classiﬁcation. 1</p><p>5 0.52042919 <a title="134-lda-5" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>Author: Corinna Cortes, Mehryar Mohri</p><p>Abstract: The area under an ROC curve (AUC) is a criterion used in many applications to measure the quality of a classiﬁcation algorithm. However, the objective function optimized in most of these algorithms is the error rate and not the AUC value. We give a detailed statistical analysis of the relationship between the AUC and the error rate, including the ﬁrst exact expression of the expected value and the variance of the AUC for a ﬁxed error rate. Our results show that the average AUC is monotonically increasing as a function of the classiﬁcation accuracy, but that the standard deviation for uneven distributions and higher error rates is noticeable. Thus, algorithms designed to minimize the error rate may not lead to the best possible AUC values. We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets demonstrating the beneﬁts of an algorithm speciﬁcally designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC. 1 Motivation In many applications, the overall classiﬁcation error rate is not the most pertinent performance measure, criteria such as ordering or ranking seem more appropriate. Consider for example the list of relevant documents returned by a search engine for a speciﬁc query. That list may contain several thousand documents, but, in practice, only the top ﬁfty or so are examined by the user. Thus, a search engine’s ranking of the documents is more critical than the accuracy of its classiﬁcation of all documents as relevant or not. More generally, for a binary classiﬁer assigning a real-valued score to each object, a better correlation between output scores and the probability of correct classiﬁcation is highly desirable. A natural criterion or summary statistic often used to measure the ranking quality of a classiﬁer is the area under an ROC curve (AUC) [8].1 However, the objective function optimized by most classiﬁcation algorithms is the error rate and not the AUC. Recently, several algorithms have been proposed for maximizing the AUC value locally [4] or maximizing some approximations of the global AUC value [9, 15], but, in general, these algorithms do not obtain AUC values signiﬁcantly better than those obtained by an algorithm designed to minimize the error rates. Thus, it is important to determine the relationship between the AUC values and the error rate. ∗ This author’s new address is: Google Labs, 1440 Broadway, New York, NY 10018, corinna@google.com. 1 The AUC value is equivalent to the Wilcoxon-Mann-Whitney statistic [8] and closely related to the Gini index [1]. It has been re-invented under the name of L-measure by [11], as already pointed out by [2], and slightly modiﬁed under the name of Linear Ranking by [13, 14]. True positive rate ROC Curve. AUC=0.718 (1,1) True positive rate = (0,0) False positive rate = False positive rate correctly classiﬁed positive total positive incorrectly classiﬁed negative total negative Figure 1: An example of ROC curve. The line connecting (0, 0) and (1, 1), corresponding to random classiﬁcation, is drawn for reference. The true positive (negative) rate is sometimes referred to as the sensitivity (resp. speciﬁcity) in this context. In the following sections, we give a detailed statistical analysis of the relationship between the AUC and the error rate, including the ﬁrst exact expression of the expected value and the variance of the AUC for a ﬁxed error rate.2 We show that, under certain conditions, the global function optimized by the RankBoost algorithm is exactly the AUC. We report the results of our experiments with RankBoost in several datasets and demonstrate the beneﬁts of an algorithm speciﬁcally designed to globally optimize the AUC over other existing algorithms optimizing an approximation of the AUC or only locally optimizing the AUC. 2 Deﬁnition and properties of the AUC The Receiver Operating Characteristics (ROC) curves were originally developed in signal detection theory [3] in connection with radio signals, and have been used since then in many other applications, in particular for medical decision-making. Over the last few years, they have found increased interest in the machine learning and data mining communities for model evaluation and selection [12, 10, 4, 9, 15, 2]. The ROC curve for a binary classiﬁcation problem plots the true positive rate as a function of the false positive rate. The points of the curve are obtained by sweeping the classiﬁcation threshold from the most positive classiﬁcation value to the most negative. For a fully random classiﬁcation, the ROC curve is a straight line connecting the origin to (1, 1). Any improvement over random classiﬁcation results in an ROC curve at least partially above this straight line. Fig. (1) shows an example of ROC curve. The AUC is deﬁned as the area under the ROC curve and is closely related to the ranking quality of the classiﬁcation as shown more formally by Lemma 1 below. Consider a binary classiﬁcation task with m positive examples and n negative examples. We will assume that a classiﬁer outputs a strictly ordered list for these examples and will denote by 1X the indicator function of a set X. Lemma 1 ([8]) Let c be a ﬁxed classiﬁer. Let x1 , . . . , xm be the output of c on the positive examples and y1 , . . . , yn its output on the negative examples. Then, the AUC, A, associated to c is given by: m n i=1 j=1 1xi >yj (1) A= mn that is the value of the Wilcoxon-Mann-Whitney statistic [8]. Proof. The proof is based on the observation that the AUC value is exactly the probability P (X > Y ) where X is the random variable corresponding to the distribution of the outputs for the positive examples and Y the one corresponding to the negative examples [7]. The Wilcoxon-Mann-Whitney statistic is clearly the expression of that probability in the discrete case, which proves the lemma [8]. Thus, the AUC can be viewed as a measure based on pairwise comparisons between classiﬁcations of the two classes. With a perfect ranking, all positive examples are ranked higher than the negative ones and A = 1. Any deviation from this ranking decreases the AUC. 2 An attempt in that direction was made by [15], but, unfortunately, the authors’ analysis and the result are both wrong. Threshold θ k − x Positive examples x Negative examples n − x Negative examples m − (k − x) Positive examples Figure 2: For a ﬁxed number of errors k, there may be x, 0 ≤ x ≤ k, false negative examples. 3 The Expected Value of the AUC In this section, we compute exactly the expected value of the AUC over all classiﬁcations with a ﬁxed number of errors and compare that to the error rate. Different classiﬁers may have the same error rate but different AUC values. Indeed, for a given classiﬁcation threshold θ, an arbitrary reordering of the examples with outputs more than θ clearly does not affect the error rate but leads to different AUC values. Similarly, one may reorder the examples with output less than θ without changing the error rate. Assume that the number of errors k is ﬁxed. We wish to compute the average value of the AUC over all classiﬁcations with k errors. Our model is based on the simple assumption that all classiﬁcations or rankings with k errors are equiprobable. One could perhaps argue that errors are not necessarily evenly distributed, e.g., examples with very high or very low ranks are less likely to be errors, but we cannot justify such biases in general. For a given classiﬁcation, there may be x, 0 ≤ x ≤ k, false positive examples. Since the number of errors is ﬁxed, there are k − x false negative examples. Figure 3 shows the corresponding conﬁguration. The two regions of examples with classiﬁcation outputs above and below the threshold are separated by a vertical line. For a given x, the computation of the AUC, A, as given by Eq. (1) can be divided into the following three parts: A1 + A2 + A3 A= , with (2) mn A1 = the sum over all pairs (xi , yj ) with xi and yj in distinct regions; A2 = the sum over all pairs (xi , yj ) with xi and yj in the region above the threshold; A3 = the sum over all pairs (xi , yj ) with xi and yj in the region below the threshold. The ﬁrst term, A1 , is easy to compute. Since there are (m − (k − x)) positive examples above the threshold and n − x negative examples below the threshold, A1 is given by: A1 = (m − (k − x))(n − x) (3) To compute A2 , we can assign to each negative example above the threshold a position based on its classiﬁcation rank. Let position one be the ﬁrst position above the threshold and let α1 < . . . < αx denote the positions in increasing order of the x negative examples in the region above the threshold. The total number of examples classiﬁed as positive is N = m − (k − x) + x. Thus, by deﬁnition of A2 , x A2 = (N − αi ) − (x − i) (4) i=1 where the ﬁrst term N − αi represents the number of examples ranked higher than the ith example and the second term x − i discounts the number of negative examples incorrectly ranked higher than the ith example. Similarly, let α1 < . . . < αk−x denote the positions of the k − x positive examples below the threshold, counting positions in reverse by starting from the threshold. Then, A3 is given by: x A3 = (N − αj ) − (x − j) (5) j=1 with N = n − x + (k − x) and x = k − x. Combining the expressions of A1 , A2 , and A3 leads to: A= A1 + A2 + A3 (k − 2x)2 + k ( =1+ − mn 2mn x i=1 αi + mn x j=1 αj ) (6) Lemma 2 For a ﬁxed x, the average value of the AUC A is given by: < A >x = 1 − x n + k−x m 2 (7) x Proof. The proof is based on the computation of the average values of i=1 αi and x j=1 αj for a given x. We start by computing the average value < αi >x for a given i, 1 ≤ i ≤ x. Consider all the possible positions for α1 . . . αi−1 and αi+1 . . . αx , when the value of αi is ﬁxed at say αi = l. We have i ≤ l ≤ N − (x − i) since there need to be at least i − 1 positions before αi and N − (x − i) above. There are l − 1 possible positions for α1 . . . αi−1 and N − l possible positions for αi+1 . . . αx . Since the total number of ways of choosing the x positions for α1 . . . αx out of N is N , the average value < αi >x is: x N −(x−i) l=i < αi >x = l l−1 i−1 N −l x−i (8) N x Thus, x < αi >x = x i=1 i=1 Using the classical identity: x < αi >x = N −(x−i) l−1 l i−1 l=i N x u p1 +p2 =p p1 N l=1 l N −1 x−1 N x i=1 N −l x−i v p2 = = N l=1 = u+v p N (N + 1) 2 x l−1 i=1 i−1 N x l N −l x−i (9) , we can write: N −1 x−1 N x = x(N + 1) 2 (10) Similarly, we have: x < αj >x = j=1 x Replacing < i=1 αi >x and < Eq. (10) and Eq. (11) leads to: x j=1 x (N + 1) 2 (11) αj >x in Eq. (6) by the expressions given by (k − 2x)2 + k − x(N + 1) − x (N + 1) =1− 2mn which ends the proof of the lemma. < A >x = 1 + x n + k−x m 2 (12) Note that Eq. (7) shows that the average AUC value for a given x is simply one minus the average of the accuracy rates for the positive and negative classes. Proposition 1 Assume that a binary classiﬁcation task with m positive examples and n negative examples is given. Then, the expected value of the AUC A over all classiﬁcations with k errors is given by: < A >= 1 − k (n − m)2 (m + n + 1) − m+n 4mn k−1 m+n x=0 x k m+n+1 x=0 x k − m+n (13) Proof. Lemma 2 gives the average value of the AUC for a ﬁxed value of x. To compute the average over all possible values of x, we need to weight the expression of Eq. (7) with the total number of possible classiﬁcations for a given x. There are N possible ways of x choosing the positions of the x misclassiﬁed negative examples, and similarly N possible x ways of choosing the positions of the x = k − x misclassiﬁed positive examples. Thus, in view of Lemma 2, the average AUC is given by: < A >= k N x=0 x N x (1 − k N x=0 x N x k−x x n+ m 2 ) (14) r=0.05 r=0.01 r=0.1 r=0.25 0.0 0.1 0.2 r=0.5 0.3 Error rate 0.4 0.5 .00 .05 .10 .15 .20 .25 0.5 0.6 0.7 0.8 0.9 1.0 Mean value of the AUC Relative standard deviation r=0.01 r=0.05 r=0.1 0.0 0.1 r=0.25 0.2 0.3 Error rate r=0.5 0.4 0.5 Figure 3: Mean (left) and relative standard deviation (right) of the AUC as a function of the error rate. Each curve corresponds to a ﬁxed ratio of r = n/(n + m). The average AUC value monotonically increases with the accuracy. For n = m, as for the top curve in the left plot, the average AUC coincides with the accuracy. The standard deviation decreases with the accuracy, and the lowest curve corresponds to n = m. This expression can be simpliﬁed into Eq. (13)3 using the following novel identities: k X N x x=0 k X N x x x=0 ! N x ! ! N x ! = = ! k X n+m+1 x x=0 (15) ! k X (k − x)(m − n) + k n + m + 1 2 x x=0 (16) that we obtained by using Zeilberger’s algorithm4 and numerous combinatorial ’tricks’. From the expression of Eq. (13), it is clear that the average AUC value is identical to the accuracy of the classiﬁer only for even distributions (n = m). For n = m, the expected value of the AUC is a monotonic function of the accuracy, see Fig. (3)(left). For a ﬁxed ratio of n/(n + m), the curves are obtained by increasing the accuracy from n/(n + m) to 1. The average AUC varies monotonically in the range of accuracy between 0.5 and 1.0. In other words, on average, there seems nothing to be gained in designing speciﬁc learning algorithms for maximizing the AUC: a classiﬁcation algorithm minimizing the error rate also optimizes the AUC. However, this only holds for the average AUC. Indeed, we will show in the next section that the variance of the AUC value is not null for any ratio n/(n + m) when k = 0. 4 The Variance of the AUC 2 Let D = mn + (k−2x) +k , a = i=1 αi , a = j=1 αj , and α = a + a . Then, by 2 Eq. (6), mnA = D − α. Thus, the variance of the AUC, σ 2 (A), is given by: (mn)2 σ 2 (A) x x = < (D − α)2 − (< D > − < α >)2 > = < D2 > − < D >2 + < α2 > − < α >2 −2(< αD > − < α >< D >) (17) As before, to compute the average of a term X over all classiﬁcations, we can ﬁrst determine its average < X >x for a ﬁxed x, and then use the function F deﬁned by: F (Y ) = k N N x=0 x x k N N x=0 x x Y (18) and < X >= F (< X >x ). A crucial step in computing the exact value of the variance of x the AUC is to determine the value of the terms of the type < a2 >x =< ( i=1 αi )2 >x . 3 An essential difference between Eq. (14) and the expression given by [15] is the weighting by the number of conﬁgurations. The authors’ analysis leads them to the conclusion that the average AUC is identical to the accuracy for all ratios n/(n + m), which is false. 4 We thank Neil Sloane for having pointed us to Zeilberger’s algorithm and Maple package. x Lemma 3 For a ﬁxed x, the average of ( i=1 αi )2 is given by: x(N + 1) < a2 > x = (3N x + 2x + N ) 12 (19) Proof. By deﬁnition of a, < a2 >x = b + 2c with: x x α2 >x i b =< c =< αi αj >x (20) 1≤i</p><p>6 0.51564687 <a title="134-lda-6" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>7 0.50933367 <a title="134-lda-7" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>8 0.50896114 <a title="134-lda-8" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>9 0.50800198 <a title="134-lda-9" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>10 0.5064593 <a title="134-lda-10" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>11 0.50568491 <a title="134-lda-11" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>12 0.50433171 <a title="134-lda-12" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>13 0.50427973 <a title="134-lda-13" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>14 0.50250107 <a title="134-lda-14" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>15 0.50240874 <a title="134-lda-15" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>16 0.50069249 <a title="134-lda-16" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>17 0.4977012 <a title="134-lda-17" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>18 0.49561793 <a title="134-lda-18" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>19 0.49471369 <a title="134-lda-19" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>20 0.49426118 <a title="134-lda-20" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
