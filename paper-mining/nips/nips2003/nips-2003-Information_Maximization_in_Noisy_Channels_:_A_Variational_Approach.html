<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-94" href="#">nips2003-94</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</h1>
<br/><p>Source: <a title="nips-2003-94-pdf" href="http://papers.nips.cc/paper/2410-information-maximization-in-noisy-channels-a-variational-approach.pdf">pdf</a></p><p>Author: David Barber, Felix V. Agakov</p><p>Abstract: The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. 1</p><p>Reference: <a title="nips-2003-94-reference" href="../nips2003_reference/nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The IM Algorithm : A variational approach to Information Maximization  David Barber Felix Agakov Institute for Adaptive and Neural Computation : www. [sent-1, score-0.079]
</p><p>2 Abstract The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. [sent-7, score-0.248]
</p><p>3 We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. [sent-8, score-0.278]
</p><p>4 The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. [sent-9, score-0.133]
</p><p>5 1  Introduction  The reliable communication of information over noisy channels is a widespread issue, ranging from the construction of good error-correcting codes to feature extraction[3, 12]. [sent-11, score-0.111]
</p><p>6 The goal is to adjust parameters of the mapping p(y|x) to maximise I(x, y). [sent-14, score-0.146]
</p><p>7 The key diﬃculty lies in the computation of the entropy of p(y) (a mixture). [sent-16, score-0.086]
</p><p>8 One such tractable special case is if the mapping y = g(x; Θ) is deterministic and invertible, for which the diﬃcult entropy term trivially becomes H(y) = log |J|  p(y)  + const. [sent-17, score-0.266]
</p><p>9 Another tractable special case is if the source distribution p(x) is Gaussian and the mapping p(y|x) is Gaussian. [sent-20, score-0.159]
</p><p>10 x p(x)  source  y p(y|x) encoder z p(z) x q(x|y,z) decoder  Figure 1: An illustration of the form of a more general mixture decoder. [sent-21, score-0.672]
</p><p>11 In neural coding, a popular alternative is to maximise the Fisher ‘Information’[5]. [sent-26, score-0.146]
</p><p>12 Other approaches use diﬀerent objective criteria, such as average reconstruction error. [sent-27, score-0.088]
</p><p>13 2  Variational Lower Bound on Mutual Information  Since the MI is a measure of information transmission, our central aim is to maximise a lower bound on the MI. [sent-28, score-0.411]
</p><p>14 Since we shall generally be interested in optimising MI with respect to the parameters of p(y|x), and p(x) is simply the data distribution, we need to bound H(x|y) suitably. [sent-30, score-0.315]
</p><p>15 The KullbackLeibler bound x p(x|y) log p(x|y) − p(x|y) log q(x|y) ≥ 0 gives I(x, y) ≥ H(x) + log q(x|y) “entropy  def p(x,y)  ˜ = I(x, y). [sent-31, score-0.588]
</p><p>16 (3)  “energy  where q(x|y) is an arbitrary variational distribution. [sent-32, score-0.079]
</p><p>17 The form of this bound is convenient since it explicitly includes both the encoder p(y|x) and decoder q(x|y), see ﬁg(1). [sent-34, score-0.741]
</p><p>18 However, our current experience suggests that the bound considered above is particularly computationally convenient. [sent-36, score-0.199]
</p><p>19 Since the bound is based on the KL divergence, it is equivalent to a moment matching approximation of p(x|y) by q(x|y). [sent-37, score-0.229]
</p><p>20 The IM algorithm To maximise the MI with respect to any parameters θ of p(y|x, θ), we aim to push up the lower bound (3). [sent-40, score-0.411]
</p><p>21 First one needs to choose a class of variational distributions q(x|y) ∈ Q for which the energy term is tractable. [sent-41, score-0.11]
</p><p>22 Then a natural ˜ recursive procedure for maximising I(X, Y ) for given p(x), is ˜ 1. [sent-42, score-0.094]
</p><p>23 This procedure is analogous to the (G)EM algorithm which maximises a lower bound on the likelihood[9]. [sent-46, score-0.278]
</p><p>24 Note that if |y| is large, the posterior p(x|y) will typically be sharply peaked around its mode. [sent-48, score-0.091]
</p><p>25 This would motivate a simple approximation q(x|y) to the posterior,  Figure 2: The MI optimal linear projection of data x (dots) is not always given by PCA. [sent-49, score-0.071]
</p><p>26 PCA projects data onto the vertical line, for which the entropy conditional on the projection H(x|y) is large. [sent-50, score-0.081]
</p><p>27 A simple approximation would then be to use a Laplace approximation to p(x|y) with 2 log covariance elements [Σ−1 ]ij = ∂ ∂xi p(x|y) . [sent-54, score-0.15]
</p><p>28 The bound presented here is arguably more general and appropriate than presented in [5] since, whilst it also tends to the exact value of the MI in the limit of a large number of responses, it is a principled bound for any response dimension. [sent-56, score-0.572]
</p><p>29 Even though we do not directly maximise the MI, we also indirectly maximise the probability of a correct reconstruction – a form of autoencoder. [sent-59, score-0.38]
</p><p>30 Generalisation to Mixture Decoders A straightforward application of Jensen’s inequality leads to the more general result: I(X, Y ) ≥ H(X) + log q(x|y, z)  p(y|x)p(x)q(z)  ˜ ≡ I(X, Y )  where q(x|y, z) and q(z) are variational distributions. [sent-60, score-0.189]
</p><p>31 The aim is to choose q(x|y, z) such that the bound is tractably computable. [sent-61, score-0.276]
</p><p>32 The classical solution to this problem (and minimizes the linear reconstruction error) is given by PCA. [sent-64, score-0.088]
</p><p>33 To see how we might improve on the PCA approach, we consider optimising our bound with respect to linear mappings. [sent-66, score-0.283]
</p><p>34 For a decoder q(x|y) = N (m(y), Σ(y)), maximising the bound on MI is equivalent to minimising P  (x − m(y))T Σ−1 (y)(x − m(y)) + log det Σ(y)  p(y|xµ )  µ=1  For constant diagonal matrices Σ(y), this reduces to minimal mean square reconstruction error autoencoder training in the limit s2 → 0. [sent-70, score-1.145]
</p><p>35 Linear Gaussian Decoder A simple decoder is given by q(x|y) ∼ N (Uy, σ 2 I), for which ˜ I(x, y) ∝ 2tr(UWS) − tr(UMUT ), where S = xx  T  =  µ  µ  (4)  µ T  x (x ) /P is the sample covariance of the data, and M = Is2 + WSWT  (5)  is the covariance of the mixture distribution p(y). [sent-72, score-0.566]
</p><p>36 For noisy channels, unconstrained optimization of (6) leads to a divergence of the matrix norm WWT ∞ ; a norm-constrained optimisation in general produces a diﬀerent result to PCA. [sent-75, score-0.179]
</p><p>37 The simplicity of the linear decoder in this case severely limits any potential improvement over PCA, and certainly would not resolve the issue in ﬁg(2). [sent-76, score-0.443]
</p><p>38 For this, a non-linear decoder q(x|y) is required, for which the integrals become more complex. [sent-77, score-0.482]
</p><p>39 Non-linear Encoders and Kernel PCA An alternative to using non-linear decoders to improve on PCA is to use a non-linear encoder. [sent-78, score-0.123]
</p><p>40 In the zero-noise limit the optimal solution for the encoder results in non-linear PCA on the covariance Φ(x)Φ(x)T of the transformed data. [sent-80, score-0.217]
</p><p>41 By Mercer’s theorem, the elements of the covariance matrix may be replaced by a Kernel function of the users choice[8]. [sent-81, score-0.1]
</p><p>42 An advantage of our framework is that our bound enables the principled comparison of embedding functions/kernels. [sent-82, score-0.242]
</p><p>43 4  Binary Responses (Neural Coding)  In a neurobiological context, a popular issue is how to encode real-valued stimuli in a population of spiking neurons. [sent-83, score-0.087]
</p><p>44 Here we look brieﬂy at a simple case in which each neuron ﬁres (yi = 1) with increasing probability the further the membrane T potential wi x is above threshold −bi . [sent-84, score-0.25]
</p><p>45 Independent neural ﬁring suggests: def  p(yi |x) =  p(y|x) = i  T σ(yi (wi x + bi )). [sent-85, score-0.14]
</p><p>46 (7)  Figure 3: Top row: a subset of the original real-valued source data. [sent-86, score-0.087]
</p><p>47 Middle row: after training, 20 samples from each of the 7 output units, for each of the corresponding source inputs. [sent-87, score-0.087]
</p><p>48 Bottom row: Reconstruction of the source data from 50 samples of the output units. [sent-88, score-0.087]
</p><p>49 This results in a visibly larger bottom loop of the 8th reconstructed pattern, which agrees with the original source data. [sent-90, score-0.116]
</p><p>50 In this case, exact evaluation of the bound (3) is straightforward, since it only involves computations of the second-order moments of y over the factorized distribution. [sent-94, score-0.233]
</p><p>51 A reasonable reconstruction of the source x from its representation y will be given ˜ by the mean x = x q(x|y) of the learned approximate posterior. [sent-95, score-0.207]
</p><p>52 In noisy channels ˜ we need to average over multiple possible representations, i. [sent-96, score-0.111]
</p><p>53 We performed reconstruction of continuous source data from stochastic binary responses for |x| = 196 input and |y| = 7 output units. [sent-99, score-0.207]
</p><p>54 The bound was optimized with respect to the parameters of p(y|x) and q(x|y) with isotropic norm constraints on W and b for 30 instances of digits 1 and 8 (15 of each class). [sent-100, score-0.199]
</p><p>55 The source variables were reconstructed from 50 samples of the corresponding binary representations at the mean of the learned q(x|y), see ﬁg(3). [sent-101, score-0.148]
</p><p>56 , M wishes to send a bit sj ∈ {0, 1} of information to a base station. [sent-105, score-0.209]
</p><p>57 To send sj = 1, she transmits an N dimensional realvalued vector gj , which represents a time-discretised waveform (sj = 0 corresponds to no transmission). [sent-106, score-0.168]
</p><p>58 The simultaneous transmissions from all users results in a received signal at the base station of j gi sj + ηi ,  ri =  i = 1, . [sent-107, score-0.2]
</p><p>59 The task for the base station (which knows G) is to decode the received vector r so that s can be recovered reliably. [sent-112, score-0.071]
</p><p>60 Using Bayes’ rule, p(s|r) ∝ p(r|s)p(s), and assuming a ﬂat prior on s, p(s|r) ∝ exp − −2rT Gs + sT GT Gs /(2σ 2 ) (8) Computing either the MAP solution arg maxs p(s|r) or the MPM solution arg maxsj p(sj |r), j = 1, . [sent-114, score-0.08]
</p><p>61 If GT G is diagonal, optimal decoding is easy, since the posterior factorises, with p(sj |r) ∝ exp  ri Gji − Djj  2  sj /(2σ 2 )  i T  where the diagonal matrix D = G G (and we used s2 ≡ si for si ∈ {0, 1}). [sent-118, score-0.73]
</p><p>62 For i suitably randomly chosen matrices G, GT G will be approximately diagonal in the limit of large N . [sent-119, score-0.08]
</p><p>63 However, ideally, one would like to construct decoders that perform near-optimal decoding without recourse to the approximate diagonality of GT G. [sent-120, score-0.315]
</p><p>64 The MAP decoder solves the problem mins∈{0,1}N sT GT Gs − 2sT GT r ≡ mins∈{0,1}N s − G−1 r  T  GT G s − G−1 r  and hence the MAP solution is that s which is closest to the vector G−1 r. [sent-121, score-0.474]
</p><p>65 The diﬃculty lies in the meaning of ‘closest’ since the space is non-isotropically warped by the matrix GT G. [sent-122, score-0.065]
</p><p>66 A useful guess for the decoder is that it is the closest in the Euclidean sense to the vector G−1 r. [sent-123, score-0.474]
</p><p>67 Computing the Mutual Information Of prime interest in CDMA is the evaluation of decoders in the case of nonorthogonal matrices G[11]. [sent-125, score-0.123]
</p><p>68 In this respect, a principled comparison of decoders can be obtained by evaluating the corresponding bound on the MI1 , I(r, s) ≡ H(s) − H(s|r) ≥ H(s) +  p(s)p(r|s) log q(s|r) r  (9)  s  where H(s) is trivially given by M (bits). [sent-126, score-0.508]
</p><p>69 We make the speciﬁc assumption in the following that our decoding algorithm takes the factorised form q(s|r) = i q(si |r) and, without loss of generality, we may write q(si |r) = σ ((2si − 1)fi (r))  (10)  for some decoding function fi (r). [sent-128, score-0.499]
</p><p>70 We restrict interest here to the case of simple linear decoding functions wij rj . [sent-129, score-0.192]
</p><p>71 fi (r) = ai + j  Since p(r|s) is Gaussian, (2si − 1)fi (r) ≡ xi is also Gaussian, p(xi |s) = N (µi (s), vari ),  T µi (s) ≡ (2si − 1)(ai + wi Gs),  T vari ≡ σ 2 wi wi  T where wi is the ith row of the matrix [W ]ij ≡ wij . [sent-130, score-1.265]
</p><p>72 Hence  1  −H(s|r) ≥ i  T 2πσ 2 wi wi  ∞  T  [log σ (x)] e−[x−(2si −1)(ai +wi Gs)]  2  T /(2σ 2 wi wi )  x=−∞  p(s)  (11) In general, the average over the factorised distribution p(s) can be evaluated by using the Fourier Transform [1]. [sent-131, score-1.057]
</p><p>73 However, to retain clarity here, we constrain the T decoding matrix W so that wi Gs = bi si , i. [sent-132, score-0.709]
</p><p>74 Figure 4: The bound given by the decoder W ∝ G−1 r plotted against the optimised bound (for the same G) found using 50 updates of conjugate gradients. [sent-136, score-0.904]
</p><p>75 For clarity, a small number of poor results (in which the bound is negative) have been omitted. [sent-138, score-0.199]
</p><p>76 8  1  MI bound for Inverse Decoder  a sum of one dimensional integrals, each of which can be evaluated numerically. [sent-156, score-0.229]
</p><p>77 In the case of an orthogonal matrix GT G = D the decoding function is optimal and the MI bound is exact with the parameters in (12) set to ai = −[GT G]ii /(2σ 2 )  W = GT /σ 2  bi = [GT G]ii /σ 2 . [sent-157, score-0.628]
</p><p>78 Optimising the linear decoder In the case that GT G is non-diagonal, what is the optimal linear decoder? [sent-158, score-0.478]
</p><p>79 A partial answer is given by numerically optimising the bound from (11). [sent-159, score-0.283]
</p><p>80 Using W = diag(b)G−1 , ([G−1 ]ij )2 ,  T σ 2 wi wi = σ 2 b2 i j  and the bound depends only on a and b. [sent-161, score-0.699]
</p><p>81 Under this constraint the bound can be numerically optimised as a function of a and b, given a ﬁxed vector j ([G−1 ]ij )2 . [sent-162, score-0.262]
</p><p>82 As an alternative we can employ the decorrelation decoder, W = G−1 /σ 2 , with ai = −1/(2σ 2 ). [sent-163, score-0.124]
</p><p>83 In ﬁg(4) we see that, according to our bound, the decorrelation or T (‘inverse’) decoder is suboptimal versus the linear decoder fi (r) = ai + wi r with −1 W = diag(b)G , optimised over a and b. [sent-164, score-1.381]
</p><p>84 These initial results are encouraging, and motivate further investigations, for example, using syndrome decoding for CDMA. [sent-165, score-0.228]
</p><p>85 6  Posterior Approximations  There is an interesting relationship between maximising the bound on the MI and computing an optimal estimate q(s|r) of an intractable posterior p(s|r). [sent-166, score-0.419]
</p><p>86 The optimal bit error solution sets q(si |r) to the mean of the exact posterior marginal p(si |r). [sent-167, score-0.298]
</p><p>87 Mean Field Theory approximates the posterior marginal by minimising the KL divergence: KL(q||p) = s (q(s|r) log q(s|r) − q(s|r) log p(s|r)), where q(s|r) = i q(si |r). [sent-168, score-0.42]
</p><p>88 In this case, the KL divergence is tractably computable (up to a neglectable prefactor). [sent-169, score-0.115]
</p><p>89 However, this form of the KL divergence chooses q(si |r) to be any one of a very large number of local modes of the posterior distribution p(si |r). [sent-170, score-0.161]
</p><p>90 Since the optimal choice is to choose the posterior marginal mean, this is why using Mean Field decoding is generally suboptimal. [sent-171, score-0.417]
</p><p>91 Alternatively, consider (p(s|r) log p(s|r) − p(s|r) log q(s|r)) = −  KL(p||q) = s  p(s|r) log q(s|r) + const. [sent-172, score-0.33]
</p><p>92 s  This is the correct KL divergence in the sense that, optimally, q(si |r) = p(si |r), that is, the posterior marginal is correctly calculated. [sent-173, score-0.228]
</p><p>93 The diﬃculty lies in performing  averages with respect to p(s|r), which are generally intractable. [sent-174, score-0.067]
</p><p>94 Since we will have a distribution p(r) it is reasonable to provide an averaged objective function, p(r)p(s|r) log q(s|r) = r  s  p(s)p(r|s) log q(s|r). [sent-175, score-0.22]
</p><p>95 r  (13)  s  Whilst, for any given r, we cannot calculate the best posterior marginal estimate, we may be able to calculate the best posterior marginal estimate on average. [sent-176, score-0.316]
</p><p>96 Whilst the bound is straightforward, it appears to have attracted little previous attention as a practical tool for MI optimisation. [sent-180, score-0.199]
</p><p>97 It is a more direct approach to optimal coding than using the Fisher ‘Information’ in neurobiological population encoding. [sent-182, score-0.17]
</p><p>98 Our bound enables a principled comparison of diﬀerent information maximisation algorithms, and may have applications in other areas of machine learning and Information Theory, such as error-correction. [sent-183, score-0.287]
</p><p>99 , Improving the mean ﬁeld approximation via the use of mixture distributions, Proceedings of the NATO ASI on Learning in Graphical Models, Kluwer, 1997. [sent-207, score-0.075]
</p><p>100 Willsky, A new class of upper bounds on the log partition function, Uncertainty in Artiﬁcial Intelligence, 2002. [sent-248, score-0.11]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('decoder', 0.443), ('mi', 0.284), ('gt', 0.276), ('wi', 0.25), ('gs', 0.227), ('bound', 0.199), ('decoding', 0.192), ('maximise', 0.146), ('cdma', 0.142), ('di', 0.123), ('si', 0.123), ('decoders', 0.123), ('log', 0.11), ('sj', 0.099), ('encoder', 0.099), ('pca', 0.096), ('kl', 0.094), ('maximising', 0.094), ('posterior', 0.091), ('reconstruction', 0.088), ('mutual', 0.088), ('source', 0.087), ('optimising', 0.084), ('bi', 0.081), ('variational', 0.079), ('divergence', 0.07), ('decorrelation', 0.067), ('marginal', 0.067), ('channels', 0.064), ('optimised', 0.063), ('field', 0.06), ('fisher', 0.06), ('transmission', 0.06), ('def', 0.059), ('im', 0.059), ('ij', 0.058), ('fi', 0.058), ('ai', 0.057), ('autoencoder', 0.057), ('factorised', 0.057), ('swt', 0.057), ('uy', 0.057), ('whilst', 0.054), ('diag', 0.053), ('entropy', 0.051), ('barber', 0.049), ('coding', 0.048), ('noisy', 0.047), ('erent', 0.045), ('mins', 0.045), ('neurobiological', 0.045), ('tractably', 0.045), ('maximises', 0.045), ('maximisation', 0.045), ('vari', 0.045), ('principled', 0.043), ('limit', 0.043), ('mixture', 0.043), ('population', 0.042), ('tractable', 0.042), ('minimising', 0.042), ('opper', 0.042), ('saad', 0.042), ('arg', 0.04), ('covariance', 0.04), ('send', 0.039), ('station', 0.039), ('integrals', 0.039), ('bit', 0.039), ('culty', 0.039), ('diagonal', 0.037), ('compression', 0.037), ('motivate', 0.036), ('optimal', 0.035), ('lies', 0.035), ('lower', 0.034), ('exact', 0.034), ('clarity', 0.033), ('trivially', 0.033), ('generally', 0.032), ('mean', 0.032), ('optimisation', 0.032), ('base', 0.032), ('em', 0.032), ('responses', 0.032), ('aim', 0.032), ('energy', 0.031), ('closest', 0.031), ('jensen', 0.031), ('special', 0.03), ('vertical', 0.03), ('matching', 0.03), ('row', 0.03), ('matrix', 0.03), ('dimensional', 0.03), ('advanced', 0.03), ('users', 0.03), ('blind', 0.029), ('reconstructed', 0.029), ('ring', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="94-tfidf-1" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. 1</p><p>2 0.19707127 <a title="94-tfidf-2" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>Author: Jason Palmer, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a speciﬁc approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice. 1</p><p>3 0.18096314 <a title="94-tfidf-3" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean ﬁeld approximations.</p><p>4 0.16159476 <a title="94-tfidf-4" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>Author: Ryan C. Kelly, Tai Sing Lee</p><p>Abstract: Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle ﬁltering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to ﬁlter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufﬁcient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels. 1</p><p>5 0.12531817 <a title="94-tfidf-5" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>Author: Haifeng Li, Tao Jiang, Keshu Zhang</p><p>Abstract: A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix Sw . Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efﬁcient and stable. 1</p><p>6 0.10632102 <a title="94-tfidf-6" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>7 0.10556773 <a title="94-tfidf-7" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>8 0.098731562 <a title="94-tfidf-8" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>9 0.096144356 <a title="94-tfidf-9" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>10 0.093887165 <a title="94-tfidf-10" href="./nips-2003-Reconstructing_MEG_Sources_with_Unknown_Correlations.html">166 nips-2003-Reconstructing MEG Sources with Unknown Correlations</a></p>
<p>11 0.090661354 <a title="94-tfidf-11" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>12 0.089722022 <a title="94-tfidf-12" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>13 0.087024495 <a title="94-tfidf-13" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>14 0.085382111 <a title="94-tfidf-14" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>15 0.082099892 <a title="94-tfidf-15" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>16 0.078820556 <a title="94-tfidf-16" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>17 0.077159084 <a title="94-tfidf-17" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>18 0.072970279 <a title="94-tfidf-18" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>19 0.072660439 <a title="94-tfidf-19" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>20 0.072486199 <a title="94-tfidf-20" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.253), (1, -0.048), (2, 0.034), (3, 0.08), (4, 0.154), (5, 0.046), (6, 0.208), (7, -0.113), (8, -0.063), (9, -0.116), (10, -0.077), (11, -0.183), (12, -0.047), (13, -0.025), (14, 0.024), (15, 0.085), (16, -0.04), (17, 0.033), (18, -0.024), (19, -0.029), (20, -0.054), (21, -0.109), (22, 0.139), (23, -0.057), (24, 0.141), (25, -0.01), (26, -0.112), (27, 0.104), (28, -0.14), (29, -0.091), (30, 0.001), (31, -0.086), (32, 0.112), (33, 0.172), (34, -0.023), (35, 0.005), (36, 0.06), (37, 0.042), (38, -0.054), (39, 0.066), (40, 0.051), (41, -0.042), (42, 0.018), (43, -0.041), (44, -0.16), (45, -0.042), (46, 0.034), (47, -0.036), (48, -0.026), (49, -0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96287918 <a title="94-lsi-1" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. 1</p><p>2 0.75053841 <a title="94-lsi-2" href="./nips-2003-Variational_Linear_Response.html">193 nips-2003-Variational Linear Response</a></p>
<p>Author: Manfred Opper, Ole Winther</p><p>Abstract: A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean ﬁeld approximations.</p><p>3 0.72159326 <a title="94-lsi-3" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>Author: Jason Palmer, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a speciﬁc approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice. 1</p><p>4 0.45209593 <a title="94-lsi-4" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>Author: Hsin Chen, Patrice Fleury, Alan F. Murray</p><p>Abstract: This paper presents VLSI circuits with continuous-valued probabilistic behaviour realized by injecting noise into each computing unit(neuron). Interconnecting the noisy neurons forms a Continuous Restricted Boltzmann Machine (CRBM), which has shown promising performance in modelling and classifying noisy biomedical data. The Minimising-Contrastive-Divergence learning algorithm for CRBM is also implemented in mixed-mode VLSI, to adapt the noisy neurons’ parameters on-chip. 1</p><p>5 0.44094375 <a title="94-lsi-5" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>Author: Michael I. Jordan, Martin J. Wainwright</p><p>Abstract: We present a new method for calculating approximate marginals for probability distributions deﬁned by graphs with cycles, based on a Gaussian entropy bound combined with a semideﬁnite outer bound on the marginal polytope. This combination leads to a log-determinant maximization problem that can be solved by efﬁcient interior point methods [8]. As with the Bethe approximation and its generalizations [12], the optimizing arguments of this problem can be taken as approximations to the exact marginals. In contrast to Bethe/Kikuchi approaches, our variational problem is strictly convex and so has a unique global optimum. An additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function. In experimental trials, the performance of the log-determinant relaxation is comparable to or better than the sum-product algorithm, and by a substantial margin for certain problem classes. Finally, the zero-temperature limit of our log-determinant relaxation recovers a class of well-known semideﬁnite relaxations for integer programming [e.g., 3]. 1</p><p>6 0.44057009 <a title="94-lsi-6" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>7 0.43330136 <a title="94-lsi-7" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>8 0.4269208 <a title="94-lsi-8" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>9 0.41303 <a title="94-lsi-9" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>10 0.40737358 <a title="94-lsi-10" href="./nips-2003-Reconstructing_MEG_Sources_with_Unknown_Correlations.html">166 nips-2003-Reconstructing MEG Sources with Unknown Correlations</a></p>
<p>11 0.40035766 <a title="94-lsi-11" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>12 0.39636555 <a title="94-lsi-12" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>13 0.37495258 <a title="94-lsi-13" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>14 0.3748402 <a title="94-lsi-14" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>15 0.35804155 <a title="94-lsi-15" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>16 0.340747 <a title="94-lsi-16" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>17 0.33790198 <a title="94-lsi-17" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.33468768 <a title="94-lsi-18" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>19 0.33338737 <a title="94-lsi-19" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>20 0.3315233 <a title="94-lsi-20" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (11, 0.024), (29, 0.023), (30, 0.016), (35, 0.084), (49, 0.011), (53, 0.156), (69, 0.014), (71, 0.065), (76, 0.069), (84, 0.245), (85, 0.102), (91, 0.067), (99, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85347819 <a title="94-lda-1" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>Author: David Barber, Felix V. Agakov</p><p>Abstract: The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA. 1</p><p>2 0.8424027 <a title="94-lda-2" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>Author: David Kauchak, Sanjoy Dasgupta</p><p>Abstract: We describe a procedure which ﬁnds a hierarchical clustering by hillclimbing. The cost function we use is a hierarchical extension of the k-means cost; our local moves are tree restructurings and node reorderings. We show these can be accomplished efﬁciently, by exploiting special properties of squared Euclidean distances and by using techniques from scheduling algorithms. 1</p><p>3 0.66405851 <a title="94-lda-3" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Trevor J. Hastie</p><p>Abstract: Margin maximizing properties play an important role in the analysis of classi£cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a suf£cient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss. We also generalize it to multi-class classi£cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines. 1</p><p>4 0.66388816 <a title="94-lda-4" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>5 0.66351211 <a title="94-lda-5" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>Author: Yuanqing Li, Shun-ichi Amari, Sergei Shishkin, Jianting Cao, Fanji Gu, Andrzej S. Cichocki</p><p>Abstract: In this paper, sparse representation (factorization) of a data matrix is ﬁrst discussed. An overcomplete basis matrix is estimated by using the K−means method. We have proved that for the estimated overcomplete basis matrix, the sparse solution (coefﬁcient matrix) with minimum l1 −norm is unique with probability of one, which can be obtained using a linear programming algorithm. The comparisons of the l1 −norm solution and the l0 −norm solution are also presented, which can be used in recoverability analysis of blind source separation (BSS). Next, we apply the sparse matrix factorization approach to BSS in the overcomplete case. Generally, if the sources are not sufﬁciently sparse, we perform blind separation in the time-frequency domain after preprocessing the observed data using the wavelet packets transformation. Third, an EEG experimental data analysis example is presented to illustrate the usefulness of the proposed approach and demonstrate its performance. Two almost independent components obtained by the sparse representation method are selected for phase synchronization analysis, and their periods of signiﬁcant phase synchronization are found which are related to tasks. Finally, concluding remarks review the approach and state areas that require further study. 1</p><p>6 0.66313446 <a title="94-lda-6" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>7 0.66256946 <a title="94-lda-7" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>8 0.66215271 <a title="94-lda-8" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>9 0.65890086 <a title="94-lda-9" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>10 0.65874815 <a title="94-lda-10" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>11 0.65873307 <a title="94-lda-11" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>12 0.65781879 <a title="94-lda-12" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>13 0.65677106 <a title="94-lda-13" href="./nips-2003-Bias-Corrected_Bootstrap_and_Model_Uncertainty.html">40 nips-2003-Bias-Corrected Bootstrap and Model Uncertainty</a></p>
<p>14 0.65643108 <a title="94-lda-14" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>15 0.65609163 <a title="94-lda-15" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>16 0.65587515 <a title="94-lda-16" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>17 0.65389043 <a title="94-lda-17" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>18 0.65359986 <a title="94-lda-18" href="./nips-2003-Necessary_Intransitive_Likelihood-Ratio_Classifiers.html">135 nips-2003-Necessary Intransitive Likelihood-Ratio Classifiers</a></p>
<p>19 0.65261078 <a title="94-lda-19" href="./nips-2003-Decoding_V1_Neuronal_Activity_using_Particle_Filtering_with_Volterra_Kernels.html">49 nips-2003-Decoding V1 Neuronal Activity using Particle Filtering with Volterra Kernels</a></p>
<p>20 0.65256017 <a title="94-lda-20" href="./nips-2003-An_Infinity-sample_Theory_for_Multi-category_Large_Margin_Classification.html">23 nips-2003-An Infinity-sample Theory for Multi-category Large Margin Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
