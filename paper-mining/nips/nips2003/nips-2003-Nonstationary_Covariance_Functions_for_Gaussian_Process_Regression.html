<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-141" href="#">nips2003-141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</h1>
<br/><p>Source: <a title="nips-2003-141-pdf" href="http://papers.nips.cc/paper/2350-nonstationary-covariance-functions-for-gaussian-process-regression.pdf">pdf</a></p><p>Author: Christopher J. Paciorek, Mark J. Schervish</p><p>Abstract: We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP ﬁtting may allow for implementation of the method on larger datasets. 1</p><p>Reference: <a title="nips-2003-141-reference" href="../nips2003_reference/nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. [sent-7, score-0.838]
</p><p>2 Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. [sent-8, score-0.548]
</p><p>3 The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. [sent-9, score-1.177]
</p><p>4 1  Introduction  Gaussian processes (GPs) have been used successfully for regression and classiﬁcation tasks. [sent-13, score-0.152]
</p><p>5 Standard GP models use a stationary covariance, in which the covariance between any two points is a function of Euclidean distance. [sent-14, score-0.397]
</p><p>6 However, stationary GPs fail to adapt to variable smoothness in the function of interest [1, 2]. [sent-15, score-0.304]
</p><p>7 This is of particular importance in geophysical and other spatial datasets, in which domain knowledge suggests that the function may vary more quickly in some parts of the input space than in others. [sent-16, score-0.197]
</p><p>8 Spatial statistics researchers have made some progress in deﬁning nonstationary covariance structures for kriging, a form of GP regression. [sent-18, score-0.77]
</p><p>9 We extend the nonstationary covariance structure of [3], of which [1] gives a special case, to a class of nonstationary covariance functions. [sent-19, score-1.47]
</p><p>10 The class includes a Matérn form, which in contrast to most covariance functions has the added ﬂexibility of a parameter that controls the differentiability of sample functions drawn from the GP distribution. [sent-20, score-0.639]
</p><p>11 We use the nonstationary covariance structure for one, two, and three dimensional input spaces in a standard GP regression model, as done previously only for one-dimensional input spaces [1]. [sent-21, score-0.976]
</p><p>12 The issue has been addressed in regression spline models by choosing the knot locations during the ﬁtting [6] and in smoothing splines by choosing an adaptive penalizer on the integrated squared derivative [7]. [sent-23, score-0.457]
</p><p>13 The general approach in spline and other models involves learning the underlying basis functions, either explicitly or implicitly, rather than ﬁxing the functions in advance. [sent-24, score-0.208]
</p><p>14 One alternative to a nonstationary GP model is mixtures of stationary GPs [8, 9]. [sent-25, score-0.72]
</p><p>15 Such methods adapt to variable smoothness by using different stationary GPs in different parts of the input space. [sent-26, score-0.343]
</p><p>16 The main difﬁculty is that the class membership is a function of the inputs; this involves additional unknown functions in the hierarchy of the model. [sent-27, score-0.174]
</p><p>17 One possibility is to use stationary GPs for these additional unknown functions [8], while [9] reduce computational complexity by using a local estimate of the class membership, but do not know if the resulting model is well-deﬁned probabilistically. [sent-28, score-0.31]
</p><p>18 In our model, there are unknown functions in the hierarchy of the model that determine the nonstationary covariance structure. [sent-30, score-0.906]
</p><p>19 We choose to fully model the functions as Gaussian processes themselves, but recognize the computational cost and suggest that simpler representations are worth investigating. [sent-31, score-0.151]
</p><p>20 2  Covariance functions and sample function differentiability  The covariance function is crucial in GP regression because it controls how much the data are smoothed in estimating the unknown function. [sent-32, score-0.7]
</p><p>21 GP distributions are distributions over functions; the covariance function determines the properties of sample functions drawn from the distribution. [sent-33, score-0.358]
</p><p>22 The stochastic process literature gives conditions for determining sample function properties of GPs based on the covariance function of the process, summarized in [10] for several common covariance functions. [sent-34, score-0.526]
</p><p>23 Stationary, isotropic covariance functions are functions only of Euclidean distance, τ . [sent-35, score-0.393]
</p><p>24 Of particular note, the squared exponential (also called the Gaussian) covariance function, C(τ ) = σ 2 exp −(τ /κ)2 , where σ 2 is the variance and κ is a correlation scale parameter, has sample functions with inﬁnitely many derivatives. [sent-36, score-0.49]
</p><p>25 In contrast, spline regression models have sample functions that are typically only twice differentiable. [sent-37, score-0.36]
</p><p>26 In addition to being of theoretical concern from an asymptotic perspective [11], other covariance forms might better ﬁt real data for which it is unlikely that the unknown function is so highly differentiable. [sent-38, score-0.267]
</p><p>27 In spatial statistics, the exponential covariance, C(τ ) = σ 2 exp (−τ /κ) , is commonly used, but this form gives sample functions that, while continuous, are not differentiable. [sent-39, score-0.236]
</p><p>28 Recent work√ spatial statistics has in √ ν 1 focused on the Matérn form, C(τ ) = σ 2 Γ(ν)2ν−1 (2 ντ /κ) Kν (2 ντ /κ) , where Kν (·) is the modiﬁed Bessel function of the second kind, whose order is the differentiability parameter, ν > 0. [sent-40, score-0.311]
</p><p>29 This form has the desirable property that sample functions are ν − 1 times differentiable. [sent-41, score-0.129]
</p><p>30 Standard covariance functions require one to place all of one’s prior probability on a particular degree of differentiability; use of the Matérn allows one to more accurately, yet easily, express prior lack of knowledge about sample function differentiability. [sent-44, score-0.406]
</p><p>31 [12] suggest using the squared exponential covariance but with anisotropic distance, τ (xi , xj ) = (xi − xj )T ∆−1 (xi − xj ), where ∆ is an arbitrary positive deﬁnite matrix, rather than the standard diagonal matrix. [sent-46, score-0.527]
</p><p>32 The nonstationary covariance function we introduce next builds on this more general form. [sent-48, score-0.735]
</p><p>33 3  Nonstationary covariance functions  One nonstationary covariance function, introduced by [3], is C(xi , xj ) = 2 , and kx (·) is a ker2 kxi (u)kxj (u)du, where xi , xj , and u are locations in nel function centered at x. [sent-49, score-1.263]
</p><p>34 The form (1) is a squared exponential correlation function, but in place of a ﬁxed matrix, ∆, in the quadratic form, we average the kernel matrices for the two locations. [sent-55, score-0.286]
</p><p>35 The evolution of the kernel matrices in space produces nonstationary covariance, with kernels that drop off quickly producing locally short correlation scales. [sent-56, score-0.748]
</p><p>36 Independently, [1] derived a special case in which the kernel matrices are diagonal. [sent-57, score-0.154]
</p><p>37 Unfortunately, so long as the kernel matrices vary smoothly in the input space, sample functions from GPs with the covariance (1) are inﬁnitely differentiable [10], just as for the stationary squared exponential. [sent-58, score-0.874]
</p><p>38 To generalize (1) and introduce functions for which sample path differentiability varies, we extend (1) as proven in [10]: Theorem 1 Let Qij be deﬁned as in (2). [sent-59, score-0.328]
</p><p>39 If a stationary correlation function, R S (τ ), is positive deﬁnite on p for every p = 1, 2, . [sent-60, score-0.234]
</p><p>40 , then 1  1  RN S (xi , xj ) = |Σi | 4 |Σj | 4 |(Σi + Σj ) /2| is a nonstationary correlation function, positive deﬁnite on  1 −2  p  RS  Qij  (3)  , p = 1, 2, . [sent-63, score-0.629]
</p><p>41 One example of nonstationary covariance functions constructed in this way is a nonstationary version of the Matérn covariance, 1  C  NS  1  σ 2 |Σi | 4 |Σj | 4 Σi + Σj (xi , xj ) = Γ(ν)2ν−1 2  −1 2  ν  2  νQij  Kν 2  νQij . [sent-67, score-1.38]
</p><p>42 (4)  Provided the kernel matrices vary smoothly in space, the sample function differentiability of the nonstationary form follows that of the stationary form, so for the nonstationary Matérn, the sample function differentiability increases with ν [10]. [sent-68, score-1.945]
</p><p>43 4  Bayesian regression model and implementation  Assume independent observations, Y1 , . [sent-69, score-0.154]
</p><p>44 , Yn , indexed by a vector of input or feature values, xi ∈ P , with Yi ∼ N (f (xi ), η 2 ), where η 2 is the noise variance. [sent-72, score-0.117]
</p><p>45 Specify a Gaussian  N N process prior, f (·) ∼ GP µf , Cf S (·, ·) , where Cf S (·, ·) is the nonstationary Matérn covariance function (4) constructed from a set of Gaussian kernels as described below. [sent-73, score-0.778]
</p><p>46 For the differentiability parameter, we use the prior, νf ∼ U(0. [sent-74, score-0.199]
</p><p>47 The main challenge is to parameterize the kernel matrices, since their evolution determines how quickly the covariance structure changes in the input space and the degree to which the model adapts to variable smoothness in the unknown function. [sent-78, score-0.534]
</p><p>48 In many problems, it seems natural that the covariance structure would evolve smoothly; if so, the differentiability of the regression function will be determined by νf . [sent-79, score-0.558]
</p><p>49 We put a prior distribution on the kernel matrices as follows. [sent-80, score-0.178]
</p><p>50 Any location in the input space, xi , has a Gaussian kernel with mean xi and covariance (kernel) matrix, Σi . [sent-81, score-0.502]
</p><p>51 When the input space is one-dimensional, each kernel ’matrix’ is just a scalar, the variance of the kernel, and we use a stationary Matérn GP prior on the log variance so that the variances evolve smoothly in the input space. [sent-82, score-0.444]
</p><p>52 Next consider multi-dimensional input spaces; since there are (implicitly) kernel matrices at each location in the input space, we have a multivariate process, the matrix-valued function, Σ(·). [sent-83, score-0.276]
</p><p>53 We use the spectral decomposition of an individual covariance matrix, Σi , Σi = Γ(γ1 (xi ), . [sent-85, score-0.229]
</p><p>54 , Q, which are functions on the input space, construct Σ(·). [sent-101, score-0.121]
</p><p>55 To have the kernel matrices vary smoothly, we ensure that their eigenvalues and eigenvectors vary smoothly by taking each φ(·) to have a GP prior with a single stationary, anisotropic Matérn correlation function, common to all the processes and described later. [sent-110, score-0.549]
</p><p>56 Parameterizing the eigenvectors of the kernel matrices using Givens angles, with each angle a function on P , the input space, is difﬁcult, because the angle functions have range [0, 2π) ≡ S 1 , which is not compatible with the range of a GP. [sent-114, score-0.305]
</p><p>57 Here, we demonstrate the construction of the eigenvectors for xi ∈ 2 and xi ∈ 3 ; a similar approach, albeit with more parameters, applies to higher-dimensional spaces, but is probably infeasible in dimensions larger than ﬁve or so. [sent-116, score-0.212]
</p><p>58 In 3 , we construct an eigenvector matrix for an individual location as Γ = Γ3 Γ2 , where     a −b −ac 1 0 0 labc lab lab labc  b  u −v  a −bc Γ3 =  labc lab lab labc  , Γ2 =  0 luv luv . [sent-117, score-0.996]
</p><p>59 v u lab c 0 luv luv 0 labc labc The elements of Γ3 are functions of three random variables, {A, B, C}, where labc = √ √ a2 + b2 + c2 and lab = a2 + b2 . [sent-118, score-0.76]
</p><p>60 To have the matrices, Σ(·), vary smoothly in space, a, b, c, u and v, are the values of the processes, γ1 (·), . [sent-121, score-0.119]
</p><p>61 In the stationary GP model, the marginal posterior contains a small number of hyperparameters to either optimize or sample via MCMC. [sent-126, score-0.25]
</p><p>62 In the nonstationary case, the presence of the additional GPs for the kernel matrices (5) precludes straightforward optimization, leaving MCMC. [sent-127, score-0.66]
</p><p>63 The parameter vector θ, involving P correlation scale parameters and P (P − 1)/2 Givens angles, is used to construct an anisotropic distance matrix, ∆(θ), shared by the φ vectors, creating a stationary, anisotropic correlation structure common to all the eigenprocesses. [sent-130, score-0.281]
</p><p>64 L(∆(θ)) is a generalized Cholesky decomposition of the correlation matrix shared by the φ vectors that deals  12 6 0  8 0. [sent-132, score-0.127]
</p><p>65 0  Figure 1: On the left are the three test functions in one dimension, with one simulated set of observations (of the 50 used in the evaluation), while the right shows the test function with two inputs. [sent-156, score-0.138]
</p><p>66 with numerically singular correlation matrices by setting the ith column of the matrix to all zeroes when φi is numerically a linear combination of φ1 , . [sent-157, score-0.176]
</p><p>67 One never calculates L(∆(θ))−1 or |L(∆(θ))|, which are not deﬁned, and does not need to introduce jitter, and therefore discontinuity in φ(·), into the covariance structure. [sent-161, score-0.229]
</p><p>68 We use three test functions [6]: a smoothly-varying function, a spatially inhomogeneous function, and a function with a sharp jump (Figure 1a). [sent-163, score-0.178]
</p><p>69 For each, we generate 50 sets of noisy data and compare the models using the means, ˆ ¯ ˆ averaged over the 50 sets, of the standardized MSE, i (fi − fi )2 / i (fi − f )2 , where fi ¯ is the mean of the true values. [sent-164, score-0.171]
</p><p>70 In the non-Bayesian neural is the posterior mean at xi , and f ˆ network model, fi is the ﬁtted value and, as a simpliﬁcation, we use a network with the optimal number of hidden units (3, 3, and 8 for the three functions), thereby giving an overly optimistic assessment of the performance. [sent-165, score-0.307]
</p><p>71 For higher-dimensional inputs, we compare the nonstationary GP to the stationary GP, the neural network models, and two free-knot spline methods, Bayesian multivariate linear splines (BMLS) [14] and Bayesian multivariate automatic regression splines (BMARS) [15], a Bayesian version of MARS [16]. [sent-167, score-1.354]
</p><p>72 We choose to compare to neural networks and 1 N We implement the stationary GP model by replacing Cf S (·, ·) with the Matérn stationary correlation, still using a differentiability parameter, νf , that is allowed to vary. [sent-168, score-0.583]
</p><p>73 Table 1: Mean (over 50 data samples) and 95% conﬁdence interval for standardized MSE for the ﬁve methods on the three test functions with one-dimensional input. [sent-173, score-0.187]
</p><p>74 The second is a real dataset of air temperature as a function of latitude and longitude [17] that allows assessment on a spatial dataset with distinct variable smoothness. [sent-226, score-0.209]
</p><p>75 Table 1 shows that the nonstationary GP does as well or better than the stationary GP, but that BARS does as well or better than the other methods on all three datasets with one input. [sent-236, score-0.712]
</p><p>76 Part of the difﬁculty for the nonstationary GP with the third function, which has the sharp jump, is that our parameterization forces smoothly-varying kernel matrices, which prevents our particular implementation from picking up sharp jumps. [sent-237, score-0.671]
</p><p>77 A potential improvement would be to parameterize kernel matrices that do not vary so smoothly. [sent-238, score-0.242]
</p><p>78 Table 2 shows that for the known function on two dimensions, the GP models outperform both the spline models and the non-Bayesian neural network, but not the Bayesian network. [sent-239, score-0.152]
</p><p>79 The stationary and nonstationary GPs are very similar, indicative of the relative homogeneity of the function. [sent-240, score-0.674]
</p><p>80 For the two real datasets, the nonstationary GP model outperforms the other methods, except the Bayesian network on the temperature dataset. [sent-241, score-0.642]
</p><p>81 Predictive density calculations that assess the ﬁts of the functions drawn during the MCMC are similar to the point estimate MSE calculations in terms of model comparison, although we do not have predictive density values for the non-Bayesian neural network implementation. [sent-242, score-0.193]
</p><p>82 Take f (·) to have a nonstationary GP prior; it cannot be integrated out of the model because of the lack of conjugacy, which causes slow MCMC mixing. [sent-244, score-0.551]
</p><p>83 Table 2: For test function with two inputs, mean (over 50 data samples) and 95% conﬁdence interval for standardized MSE at 225 test locations, and for the temperature and ozone datasets, cross-validated standardized MSE, for the six methods. [sent-246, score-0.357]
</p><p>84 07 for a neural network implementation We ﬁt the model to the Tokyo rainfall dataset [19]. [sent-281, score-0.253]
</p><p>85 The data are the presence of rainfall greater than 1 mm for every calendar day in 1983 and 1984. [sent-282, score-0.238]
</p><p>86 Assuming independence between years [19], conditional on f (·) = logit(p(·)), the likelihood for a given calendar day, xi , is binomial with two trials and unknown probability of rainfall, p(xi ). [sent-283, score-0.235]
</p><p>87 The model detects inhomogeneity in the function, with more smoothness in the ﬁrst few months and less smoothness later (Figure 2b). [sent-285, score-0.146]
</p><p>88 8  (a)  (b)  0  7  100  200 calendar day  300  Figure 2. [sent-290, score-0.123]
</p><p>89 (a) Posterior mean estimate, from nonstationary GP model, of p(·), the probability of rainfall as a function of calendar day, with 95% pointwise credible intervals. [sent-291, score-0.698]
</p><p>90 Dots are empirical probabilities of rainfall based on the two binomial trials. [sent-292, score-0.157]
</p><p>91 (b) Posterior geometric mean kernel size (square root of geometric mean kernel eigenvalue). [sent-293, score-0.156]
</p><p>92 Discussion  We introduce a class of nonstationary covariance functions that can be used in GP regression (and classiﬁcation) models and allow the model to adapt to variable smoothness in the unknown function. [sent-294, score-1.118]
</p><p>93 The nonstationary GPs improve on stationary GP models on several test datasets. [sent-295, score-0.702]
</p><p>94 The nonstationary GP may be of particular interest for data indexed by spatial coordinates, where the low dimensionality keeps the parameter complexity manageable. [sent-297, score-0.583]
</p><p>95 Unfortunately, the nonstationary GP requires many more parameters than a stationary GP, particularly as the dimension grows, losing the attractive simplicity of the stationary GP model. [sent-298, score-0.842]
</p><p>96 Use of GP priors in the hierarchy of the model to parameterize the nonstationary covariance results in slow computation, limiting the feasibility of the model to approximately n < 1000, because the Cholesky decomposition is O(n3 ). [sent-299, score-0.871]
</p><p>97 Also, approaches that use low-rank approximations to  the covariance matrix [20, 21] may speed ﬁtting. [sent-301, score-0.263]
</p><p>98 Bayesian inference for nonstationary spatial covariance structure via spatial deformations. [sent-339, score-0.889]
</p><p>99 Bayesian mixture of splines for spatially adaptive nonparametric regression. [sent-448, score-0.165]
</p><p>100 Adaptive Bayesian regression splines in semiparametric generalized linear models. [sent-462, score-0.241]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nonstationary', 0.506), ('gp', 0.495), ('covariance', 0.229), ('differentiability', 0.199), ('mat', 0.182), ('stationary', 0.168), ('splines', 0.136), ('labc', 0.134), ('gps', 0.126), ('spline', 0.126), ('rainfall', 0.115), ('regression', 0.105), ('bayesian', 0.102), ('rn', 0.097), ('ozone', 0.096), ('mse', 0.088), ('qij', 0.085), ('functions', 0.082), ('kernel', 0.078), ('xi', 0.078), ('spatial', 0.077), ('calendar', 0.077), ('luv', 0.077), ('standardized', 0.077), ('matrices', 0.076), ('smoothly', 0.071), ('correlation', 0.066), ('network', 0.063), ('smoothness', 0.062), ('lab', 0.061), ('anisotropic', 0.061), ('bmars', 0.057), ('bmls', 0.057), ('xj', 0.057), ('temperature', 0.051), ('vary', 0.048), ('adapt', 0.048), ('sample', 0.047), ('fi', 0.047), ('processes', 0.047), ('day', 0.046), ('cf', 0.045), ('multivariate', 0.044), ('binomial', 0.042), ('parameterize', 0.04), ('input', 0.039), ('gaussian', 0.039), ('inputs', 0.038), ('unknown', 0.038), ('eigenprocesses', 0.038), ('givens', 0.038), ('paciorek', 0.038), ('radiation', 0.038), ('datasets', 0.038), ('jump', 0.038), ('squared', 0.036), ('statistics', 0.035), ('hyperparameters', 0.035), ('matrix', 0.034), ('schervish', 0.033), ('logit', 0.033), ('geophysical', 0.033), ('eigenvectors', 0.03), ('assessment', 0.03), ('exponential', 0.03), ('sharp', 0.03), ('xing', 0.03), ('adaptive', 0.029), ('spaces', 0.029), ('hierarchy', 0.029), ('cholesky', 0.028), ('eigenvector', 0.028), ('test', 0.028), ('shared', 0.027), ('dietterich', 0.027), ('editors', 0.027), ('implementation', 0.027), ('neural', 0.026), ('dimensions', 0.026), ('variable', 0.026), ('membership', 0.025), ('air', 0.025), ('evolve', 0.025), ('parameterizing', 0.025), ('locations', 0.025), ('prior', 0.024), ('volker', 0.024), ('biometrika', 0.024), ('bars', 0.024), ('mixtures', 0.024), ('varies', 0.023), ('slow', 0.023), ('angles', 0.022), ('mcmc', 0.022), ('model', 0.022), ('kernels', 0.022), ('massachusetts', 0.022), ('tresp', 0.022), ('competitive', 0.022), ('process', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="141-tfidf-1" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>Author: Christopher J. Paciorek, Mark J. Schervish</p><p>Abstract: We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP ﬁtting may allow for implementation of the method on larger datasets. 1</p><p>2 0.39119184 <a title="141-tfidf-2" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. 1</p><p>3 0.29148892 <a title="141-tfidf-3" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>4 0.13006499 <a title="141-tfidf-4" href="./nips-2003-GPPS%3A_A_Gaussian_Process_Positioning_System_for_Cellular_Networks.html">76 nips-2003-GPPS: A Gaussian Process Positioning System for Cellular Networks</a></p>
<p>Author: Anton Schwaighofer, Marian Grigoras, Volker Tresp, Clemens Hoffmann</p><p>Abstract: In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user’s position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user’s position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network. 1</p><p>5 0.11431948 <a title="141-tfidf-5" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>6 0.10124949 <a title="141-tfidf-6" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>7 0.079466678 <a title="141-tfidf-7" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>8 0.073854014 <a title="141-tfidf-8" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>9 0.069228783 <a title="141-tfidf-9" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>10 0.067563176 <a title="141-tfidf-10" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>11 0.066634759 <a title="141-tfidf-11" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>12 0.063555844 <a title="141-tfidf-12" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>13 0.062484626 <a title="141-tfidf-13" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>14 0.05970883 <a title="141-tfidf-14" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>15 0.058539577 <a title="141-tfidf-15" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>16 0.056788385 <a title="141-tfidf-16" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>17 0.055388886 <a title="141-tfidf-17" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>18 0.055206724 <a title="141-tfidf-18" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>19 0.054485846 <a title="141-tfidf-19" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>20 0.052146181 <a title="141-tfidf-20" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.206), (1, 0.005), (2, -0.034), (3, 0.01), (4, 0.013), (5, 0.227), (6, 0.078), (7, -0.217), (8, 0.202), (9, 0.151), (10, -0.276), (11, 0.32), (12, 0.001), (13, -0.239), (14, 0.012), (15, -0.109), (16, -0.14), (17, 0.091), (18, -0.098), (19, -0.044), (20, -0.012), (21, -0.135), (22, -0.075), (23, -0.025), (24, 0.04), (25, 0.031), (26, -0.052), (27, 0.009), (28, -0.014), (29, 0.093), (30, 0.085), (31, -0.009), (32, -0.01), (33, -0.07), (34, 0.048), (35, -0.051), (36, -0.008), (37, -0.077), (38, -0.013), (39, 0.046), (40, 0.062), (41, 0.024), (42, -0.002), (43, 0.022), (44, -0.019), (45, 0.071), (46, 0.039), (47, -0.036), (48, -0.011), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94636256 <a title="141-lsi-1" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>Author: Christopher J. Paciorek, Mark J. Schervish</p><p>Abstract: We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP ﬁtting may allow for implementation of the method on larger datasets. 1</p><p>2 0.93773043 <a title="141-lsi-2" href="./nips-2003-Warped_Gaussian_Processes.html">194 nips-2003-Warped Gaussian Processes</a></p>
<p>Author: Edward Snelson, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to signiﬁcantly better performance than using a regular GP, or a GP with a ﬁxed transformation. 1</p><p>3 0.65069765 <a title="141-lsi-3" href="./nips-2003-GPPS%3A_A_Gaussian_Process_Positioning_System_for_Cellular_Networks.html">76 nips-2003-GPPS: A Gaussian Process Positioning System for Cellular Networks</a></p>
<p>Author: Anton Schwaighofer, Marian Grigoras, Volker Tresp, Clemens Hoffmann</p><p>Abstract: In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user’s position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user’s position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network. 1</p><p>4 0.62393445 <a title="141-lsi-4" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>5 0.32322457 <a title="141-lsi-5" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>Author: Jan Eichhorn, Andreas Tolias, Alexander Zien, Malte Kuss, Jason Weston, Nikos Logothetis, Bernhard Schölkopf, Carl E. Rasmussen</p><p>Abstract: We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms. 1</p><p>6 0.28939873 <a title="141-lsi-6" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>7 0.26746738 <a title="141-lsi-7" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>8 0.26691815 <a title="141-lsi-8" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>9 0.26151168 <a title="141-lsi-9" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>10 0.25470391 <a title="141-lsi-10" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>11 0.24432071 <a title="141-lsi-11" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>12 0.24272364 <a title="141-lsi-12" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>13 0.24198028 <a title="141-lsi-13" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>14 0.22957693 <a title="141-lsi-14" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>15 0.22487926 <a title="141-lsi-15" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>16 0.22197196 <a title="141-lsi-16" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>17 0.22153156 <a title="141-lsi-17" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>18 0.2213883 <a title="141-lsi-18" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>19 0.21589802 <a title="141-lsi-19" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>20 0.21526542 <a title="141-lsi-20" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (11, 0.016), (27, 0.307), (29, 0.022), (30, 0.013), (35, 0.049), (48, 0.015), (53, 0.133), (69, 0.027), (71, 0.068), (76, 0.071), (85, 0.064), (91, 0.084), (99, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79198134 <a title="141-lda-1" href="./nips-2003-An_MDP-Based_Approach_to_Online_Mechanism_Design.html">26 nips-2003-An MDP-Based Approach to Online Mechanism Design</a></p>
<p>Author: David C. Parkes, Satinder P. Singh</p><p>Abstract: Online mechanism design (MD) considers the problem of providing incentives to implement desired system-wide outcomes in systems with self-interested agents that arrive and depart dynamically. Agents can choose to misrepresent their arrival and departure times, in addition to information about their value for diﬀerent outcomes. We consider the problem of maximizing the total longterm value of the system despite the self-interest of agents. The online MD problem induces a Markov Decision Process (MDP), which when solved can be used to implement optimal policies in a truth-revealing Bayesian-Nash equilibrium. 1</p><p>same-paper 2 0.78461719 <a title="141-lda-2" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>Author: Christopher J. Paciorek, Mark J. Schervish</p><p>Abstract: We introduce a class of nonstationary covariance functions for Gaussian process (GP) regression. Nonstationary covariance functions allow the model to adapt to functions whose smoothness varies with the inputs. The class includes a nonstationary version of the Matérn stationary covariance, in which the differentiability of the regression function is controlled by a parameter, freeing one from ﬁxing the differentiability in advance. In experiments, the nonstationary GP regression model performs well when the input space is two or three dimensions, outperforming a neural network model and Bayesian free-knot spline models, and competitive with a Bayesian neural network, but is outperformed in one dimension by a state-of-the-art Bayesian free-knot spline model. The model readily generalizes to non-Gaussian data. Use of computational methods for speeding GP ﬁtting may allow for implementation of the method on larger datasets. 1</p><p>3 0.74408025 <a title="141-lda-3" href="./nips-2003-Laplace_Propagation.html">100 nips-2003-Laplace Propagation</a></p>
<p>Author: Eleazar Eskin, Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: We present a novel method for approximate inference in Bayesian models and regularized risk functionals. It is based on the propagation of mean and variance derived from the Laplace approximation of conditional probabilities in factorizing distributions, much akin to Minka’s Expectation Propagation. In the jointly normal case, it coincides with the latter and belief propagation, whereas in the general case, it provides an optimization strategy containing Support Vector chunking, the Bayes Committee Machine, and Gaussian Process chunking as special cases. 1</p><p>4 0.5413546 <a title="141-lda-4" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>5 0.53853244 <a title="141-lda-5" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors. 1</p><p>6 0.53772342 <a title="141-lda-6" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>7 0.53714579 <a title="141-lda-7" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>8 0.53700978 <a title="141-lda-8" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>9 0.53678793 <a title="141-lda-9" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>10 0.53631389 <a title="141-lda-10" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>11 0.5352841 <a title="141-lda-11" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>12 0.53506118 <a title="141-lda-12" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>13 0.53456724 <a title="141-lda-13" href="./nips-2003-Geometric_Clustering_Using_the_Information_Bottleneck_Method.html">82 nips-2003-Geometric Clustering Using the Information Bottleneck Method</a></p>
<p>14 0.53391701 <a title="141-lda-14" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>15 0.53362232 <a title="141-lda-15" href="./nips-2003-Locality_Preserving_Projections.html">120 nips-2003-Locality Preserving Projections</a></p>
<p>16 0.53340495 <a title="141-lda-16" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>17 0.5329318 <a title="141-lda-17" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>18 0.5326997 <a title="141-lda-18" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>19 0.53199798 <a title="141-lda-19" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>20 0.53119415 <a title="141-lda-20" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
