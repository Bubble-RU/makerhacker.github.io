<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-29" href="#">nips2003-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</h1>
<br/><p>Source: <a title="nips-2003-29-pdf" href="http://papers.nips.cc/paper/2437-applying-metric-trees-to-belief-point-pomdps.pdf">pdf</a></p><p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>Reference: <a title="nips-2003-29-reference" href="../nips2003_reference/nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These approaches operate on sets of belief points by individually learning a value function for each point. [sent-5, score-0.749]
</p><p>2 In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. [sent-6, score-0.883]
</p><p>3 This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. [sent-7, score-1.035]
</p><p>4 However the practical use of POMDPs has been severely limited by the computational requirement of planning in such a rich representation. [sent-11, score-0.221]
</p><p>5 POMDP planning is difﬁcult because it involves learning action selection strategies contingent on all possible types of state uncertainty. [sent-12, score-0.275]
</p><p>6 This means that whenever the robot’s world state cannot be observed, the planner must maintain a belief (namely a probability distribution over possible states) to summarize the robot’s recent history of actions taken and observations received. [sent-13, score-0.635]
</p><p>7 The POMDP planner then learns an optimal future action selection for each possible belief. [sent-14, score-0.092]
</p><p>8 As the planning horizon grows (linearly), so does the number of possible beliefs (exponentially), which causes the computational intractability of exact POMDP planning. [sent-15, score-0.357]
</p><p>9 In recent years, a number of approximate algorithms have been proposed which overcome this issue by simply refusing to consider all possible beliefs, and instead selecting (and planning for) a small set of representative belief points. [sent-16, score-0.776]
</p><p>10 During execution, should the robot encounter a belief for which it has no plan, it ﬁnds the nearest known belief point and follows its plan. [sent-17, score-1.216]
</p><p>11 Such approaches, often known as grid-based [1, 4, 13], or point-based [8, 9] algorithms, have had signiﬁcant success with increasingly large planning domains. [sent-18, score-0.221]
</p><p>12 They formulate the plan optimization problem as a value iteration procedure, and estimate the cost/reward of applying a sequence of actions from a given belief point. [sent-19, score-0.775]
</p><p>13 The value of  each action sequence can be expressed as an α-vector, and a key step in many algorithms consists of evaluating many candidate α-vectors (set Γ) at each belief point (set B). [sent-20, score-0.711]
</p><p>14 Recent work has shown that for these problems, one can signiﬁcantly reduce the number of necessary comparisons by using appropriate metric data structures, such as KD-trees and ball-trees [3, 6, 12]. [sent-22, score-0.287]
</p><p>15 This paper describes our algorithm for building and searching a metric-tree over belief points. [sent-24, score-0.649]
</p><p>16 For example, when using trees for POMDPs, we move away from point-to-point search procedures for which the trees are typically used, and leverage metric constraints to prune point-to-vector comparisons. [sent-26, score-0.328]
</p><p>17 We show how it is often possible to evaluate the usefulness of an α-vector over an entire sub-region of the belief simplex without explicitly evaluating it at each belief point in that sub-region. [sent-27, score-1.197]
</p><p>18 While our new metric-tree approach offers signiﬁcant potential for all point-based approaches, in this paper we apply it in the context of the PBVI algorithm [8], and show that it can effectively reduce computation without compromising plan quality. [sent-28, score-0.121]
</p><p>19 An |S|-dimensional vector, bt , represents the agent’s belief about the state of the world at time t, and is expressed as a probability distribution over states. [sent-31, score-0.611]
</p><p>20 This belief is updated after each time step—to reﬂect the latest pair (at−1 , zt )—using a Bayesian ﬁlter: bt (s ) := c O(s , at−1 , zt ) s∈S T (s, at−1 , s )bt−1 (s), where c is a normalizing constant. [sent-32, score-0.679]
</p><p>21 The goal of POMDP planning is to ﬁnd a sequence of actions maximizing the expected sum of rewards E[ t γ t R(st , at )], for all belief. [sent-33, score-0.263]
</p><p>22 The corresponding value function can be formulated as a Bellman equation: V (b) = maxa∈A R(b, a) + γ b ∈B T (b, a, b )V (b ) By deﬁnition there exist an inﬁnite number of belief points. [sent-34, score-0.62]
</p><p>23 However when optimized exactly, the value function is always piecewise linear and convex in the belief (Fig. [sent-35, score-0.667]
</p><p>24 Each α-vector represents an |S|-dimensional hyper-plane, and deﬁnes the value function over a bounded region of the belief: Vn (b) = maxα∈Vn s∈S α(s)b(s). [sent-41, score-0.098]
</p><p>25 When performing exact value updates, the set of α-vectors can (and often does) grow exponentially with the planning horizon. [sent-42, score-0.292]
</p><p>26 We leave out a full discussion of exact POMDP planning (see [5] for more) and focus instead on the much more tractable point-based approximate algorithm. [sent-44, score-0.256]
</p><p>27 3  Point-based value iteration for POMDPs  The main motivation behind the point-based algorithm is to exploit the fact that most beliefs are never, or very rarely, encountered, and thus resources are better spent planning  for those beliefs that are most likely to be reached. [sent-45, score-0.514]
</p><p>28 Point-based value iteration algorithms on the other hand apply value backups only to a ﬁnite set of pre-selected (and likely to be encountered) belief points B = {b0 , b1 , . [sent-47, score-0.918]
</p><p>29 As shown in Figure 1b, by maintaining a full α-vector for each belief point, we can preserve the piecewise linearity and convexity of the value function, and deﬁne a value function over the entire belief simplex. [sent-52, score-1.333]
</p><p>30 This is an approximation, as some vectors may be missed, but by appropriately selecting points, we can bound the approximation error (see [8] for details). [sent-53, score-0.084]
</p><p>31 V={ α 0 ,α 1 ,α 2 ,α 3 }  V={ α 0 ,α 1 ,α 3 }  b2  b1  (a)  b0  b3  (b)  Figure 1: (a) Value iteration with exact updates. [sent-54, score-0.092]
</p><p>32 First, a set of belief points is selected, and second, a series of backup operations are applied over α-vectors for that set of points. [sent-57, score-0.757]
</p><p>33 In practice, steps of value iteration and steps of belief set expansion can be repeatedly interleaved to produce an anytime algorithm that can gradually trade-off computation time and solution quality. [sent-58, score-0.692]
</p><p>34 The question of how to best select belief points is somewhat orthogonal to the ideas in this paper and is discussed in detail in [8]. [sent-59, score-0.713]
</p><p>35 We therefore focus on describing how to do point-based value backups, before showing how this step can be signiﬁcantly accelerated by the use of appropriate metric data structures. [sent-60, score-0.174]
</p><p>36 It also completely ignores the highly structured nature of the belief space. [sent-70, score-0.555]
</p><p>37 Belief points exist in a metric space and there is much to be gained from exploiting this property. [sent-71, score-0.294]
</p><p>38 For example, given the piecewise linearity and convexity of the value function, it is more likely that two nearby points will share similar values (and policies) than points that are far away. [sent-72, score-0.504]
</p><p>39 Consequently it could be much more efﬁcient to evaluate an α-vector over sets of nearby points, rather than by exhaustively looking at all the points separately. [sent-73, score-0.202]
</p><p>40 In the next section, we describe a new type of metric-tree which structures data points based on a distance metric over the belief simplex. [sent-74, score-0.856]
</p><p>41 We then show how this kind of tree can be used to efﬁciently evaluate α-vectors over sets of belief points (or belief regions). [sent-75, score-1.479]
</p><p>42 4  Metric-trees for belief spaces  Metric data structures offer a way to organize large sets of data points according to distances between the points. [sent-76, score-0.749]
</p><p>43 Instances of metric data structures such as KD-trees, ball-trees and metric-trees have been shown to be useful for a wide range of learning tasks (e. [sent-78, score-0.143]
</p><p>44 It consists of a hierarchical tree built by recursively splitting the set of points into spatially tighter subsets, assuming only that the distance between points is a metric. [sent-82, score-0.622]
</p><p>45 1  Building a metric-tree from belief points  Each node η in a metric-tree is represented by its center ηc , its radius ηr , and a set of points ηB that fall within its radius. [sent-84, score-1.097]
</p><p>46 To recursively construct the tree—starting with node η and building children nodes η 1 and η 2 —we ﬁrst pick two candidate centers (one per child) at 1 2 1 the extremes of the η’s region: ηc = maxb∈ηD D(ηc , b), and ηc = maxb∈ηD D(ηc , b). [sent-85, score-0.392]
</p><p>47 For the centers, the most common choice is the centroid of the points and this is what we use when building a tree over belief points. [sent-90, score-0.962]
</p><p>48 For the distance metric, we select the max-norm: D(ηc , b) = ||ηc −b||∞ , which allows for fast searching as described in the next section. [sent-92, score-0.086]
</p><p>49 While the radius determines the size of the region enclosed by each node, the choice of distance metric determines its shape (e. [sent-93, score-0.243]
</p><p>50 with Euclidean distance, we would get hyper-balls of radius η r ). [sent-95, score-0.074]
</p><p>51 In the case of the max-norm, each node deﬁnes an |S|-dimensional hyper-cube of length 2∗η r . [sent-96, score-0.122]
</p><p>52 Figure 2 shows how the ﬁrst two-levels of a tree are built, assuming a 3-state problem. [sent-97, score-0.211]
</p><p>53 (d) Corresponding tree While we need to compute the center and radius for each node to build the tree, there are additional statistics which we also store about each node. [sent-104, score-0.437]
</p><p>54 These are speciﬁc to using trees in the context of belief-state planning, and are necessary to evaluate α vectors over regions of the belief simplex. [sent-105, score-0.706]
</p><p>55 For a given node η containing data points ηB , we compute ηmin and ηmax , the vectors containing respectively the min and max belief in each dimension: ηmin (s) = min b(s), ∀s ∈ S b∈ηB  4. [sent-106, score-1.099]
</p><p>56 2  ηmax (s) = max b(s), ∀s ∈ S b∈ηB  (8)  Searching over sub-regions of the simplex  Once the tree is built, it can be used for fast statistical queries. [sent-107, score-0.419]
</p><p>57 In our case, the goal is to compute argmaxα∈Γa,z (α · b) for all belief points. [sent-108, score-0.555]
</p><p>58 To do this, we consider the α vectors one at a time, and decide whether a new candidate αi is better than any of the previous vectors {α0 . [sent-109, score-0.143]
</p><p>59 With the belief points organized in a tree, we can often assess this over sets of points by consulting a high-level node η, rather than by assessing this for each belief point separately. [sent-113, score-1.548]
</p><p>60 There are four different situations we can encounter as we traverse the tree: ﬁrst, there might be no single previous α-vector that is best for all belief points below the current node (Fig. [sent-115, score-0.887]
</p><p>61 In this case we proceed to the children of the current node without performing any tests. [sent-117, score-0.185]
</p><p>62 In the other three cases there is a single dominant alpha-vector at the current node; the cases are that the newest vector αi dominates it (Fig. [sent-118, score-0.113]
</p><p>63 If we can prove that α i dominates or is dominated by the previous one, we can prune the search and avoid checking the current node’s children; otherwise we must check the children recursively. [sent-122, score-0.402]
</p><p>64 We seek an efﬁcient test to determine whether one vector, αi , dominates another, αj , over the belief points contained within a node. [sent-123, score-0.896]
</p><p>65 The test must be conservative: it must never erroneously say that one vector dominates another. [sent-124, score-0.153]
</p><p>66 It is acceptable for the test to miss some pruning opportunities—the consequence is an increase in run-time as we check more nodes than necessary—but this is best avoided if possible. [sent-125, score-0.179]
</p><p>67 All positive would mean that αi dominates αj , all negative the reverse, and mixed positive and negative would mean that neither dominates the other. [sent-132, score-0.226]
</p><p>68 Of course, this test renders the tree useless, since all points are checked individually. [sent-133, score-0.409]
</p><p>69 Instead, we test whether ∆·b is positive or negative over a convex region R which includes all of the belief samples that belong to the current node. [sent-134, score-0.718]
</p><p>70 The smaller the region, the more accurate our test will be; on the other hand, if the region is too complicated we won’t be able to carry out the test efﬁciently. [sent-135, score-0.142]
</p><p>71 (Note that we can always test some region R by solving one linear program to ﬁnd l = minb∈R b · ∆, another to ﬁnd h = maxb∈R b · ∆, and testing whether l < 0 < h. [sent-136, score-0.166]
</p><p>72 ) P(s2)  ) (s 3  ax  ηm  ηmax(s1)  ηmax(s2)  (b)  ) (s 3  (a)  ηmin(s1)  in  P(s1)  ηm  ηmin(s2)  (c)  (d)  Figure 4: Several possible convex regions over subsets of belief points, assuming a 3-state domain. [sent-138, score-0.626]
</p><p>73 The simplest type is an axis-parallel bounding box (Fig. [sent-140, score-0.086]
</p><p>74 We also tested the simplex deﬁned by b ≥ ηmin and s∈S b(s) = 1 (Fig. [sent-143, score-0.087]
</p><p>75 4b), as well as the simplex deﬁned by b ≤ ηmax and s∈S b(s) = 1 (Fig. [sent-144, score-0.087]
</p><p>76 The most effective test we discovered assumes R is the intersection of the bounding box ηmin ≤ b ≤ ηmax with the plane s∈S b(s) = 1 (Fig. [sent-146, score-0.126]
</p><p>77 4a) we check each dimension independently, and for the simplices (Figs 4b, 4c) we check each corner exhaustively. [sent-149, score-0.128]
</p><p>78 Empirical results show that checking the corners of regions (b) and (c) and taking the tightest bounds provides the fastest algorithm. [sent-156, score-0.078]
</p><p>79 5  Results and Discussion  We have conducted a set of experiments to test the effectiveness of the tree structure in reducing computations. [sent-158, score-0.312]
</p><p>80 5(a)-(f) we show the number of B × Γ (point-to-vector) comparisons required, with and without a tree, for different numbers of belief points. [sent-163, score-0.7]
</p><p>81 5(g)-(h) we show the computation time (as a function of the number of belief points) required for two of the problems. [sent-165, score-0.555]
</p><p>82 The Tree results (which count comparisons on both internal and leaf nodes) were generated by embedding the tree searching procedure described in Section 4. [sent-167, score-0.443]
</p><p>83 6  4  x 10  7  4  x 10  7  x 10  2  x 10  6 # comparisons  1. [sent-173, score-0.145]
</p><p>84 5  5  # comparisons  8 # comparisons  2  No Tree Tree Epsilon−Tree  # comparisons  10  1  0. [sent-176, score-0.435]
</p><p>85 5 4 x 10  (g) SACI, |S|=12  0 0  200  400 600 # belief points  800  1000  (h) Tag, |S|=870  Figure 5: Results of PBVI algorithm with and without metric-tree. [sent-180, score-0.713]
</p><p>86 These early results show that, in various proportions, the tree can cut down on the number of comparisons. [sent-181, score-0.211]
</p><p>87 The -tree is particularly effective at reducing the number of comparisons in some domains (e. [sent-183, score-0.178]
</p><p>88 In keeping with other metric-tree applications, our results show that computational savings increase with the number of belief points. [sent-189, score-0.555]
</p><p>89 What is more surprising is to see the trees paying off with so few data points (most applications of KD-trees start seeing beneﬁts with 1000+ data points. [sent-190, score-0.23]
</p><p>90 ) This may be partially attributed to the compactness of our convex test region (Fig. [sent-191, score-0.217]
</p><p>91 4d), and to the fact that we do not search on split nodes (Fig. [sent-192, score-0.079]
</p><p>92 3a); however, it is most likely due to the nature of our search problem: many α vectors are accepted/rejected before visiting any leaf nodes, which is different from typical metric-tree applications. [sent-193, score-0.141]
</p><p>93 We are particularly encouraged to see trees having a noticeable effect with very few data points because, in some domains, good control policies can also be extracted with few data points. [sent-194, score-0.23]
</p><p>94 We notice that the effect of using trees is negligible in some larger problems (e. [sent-195, score-0.136]
</p><p>95 This is  likely due to the intrinsic dimensionality of each problem. [sent-200, score-0.14]
</p><p>96 While this suggests that our current algorithm is not as effective in problems with intrinsic high-dimensionality, a slightly different tree structure or search procedure may well help in those cases. [sent-202, score-0.353]
</p><p>97 6  Conclusion  We have described a new type of metric-tree which can be used for sorting belief points and accelerating value updates in POMDPs. [sent-204, score-0.831]
</p><p>98 Early experiments indicate that the tree structure, by appropriately pruning unnecessary α-vectors over large regions of the belief, can accelerate planning for a range problems. [sent-205, score-0.552]
</p><p>99 3  The coffee domain is known to have an intrinsic dimensionality of 7 [10]. [sent-284, score-0.242]
</p><p>100 We do not know the intrinsic dimensionality of the Tag domain, but many robot applications produce belief points that exist in sub-dimensional manifolds [11]. [sent-285, score-0.904]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('belief', 0.555), ('pomdp', 0.376), ('planning', 0.221), ('tree', 0.211), ('points', 0.158), ('tag', 0.154), ('comparisons', 0.145), ('node', 0.122), ('dominates', 0.113), ('pomdps', 0.113), ('metric', 0.107), ('coffee', 0.103), ('max', 0.091), ('saci', 0.089), ('simplex', 0.087), ('maxb', 0.078), ('pbvi', 0.078), ('radius', 0.074), ('intrinsic', 0.074), ('trees', 0.072), ('min', 0.067), ('beliefs', 0.067), ('check', 0.064), ('children', 0.063), ('centers', 0.063), ('region', 0.062), ('argmax', 0.059), ('iteration', 0.057), ('searching', 0.056), ('bt', 0.056), ('plan', 0.054), ('robot', 0.054), ('observable', 0.054), ('action', 0.054), ('box', 0.052), ('vn', 0.052), ('encounter', 0.052), ('child', 0.05), ('partially', 0.048), ('dominated', 0.047), ('secs', 0.047), ('pineau', 0.047), ('sorting', 0.047), ('appropriately', 0.045), ('piecewise', 0.045), ('anytime', 0.044), ('backup', 0.044), ('backups', 0.044), ('exhaustively', 0.044), ('actions', 0.042), ('convexity', 0.041), ('regions', 0.04), ('test', 0.04), ('nodes', 0.04), ('queries', 0.039), ('vectors', 0.039), ('search', 0.039), ('building', 0.038), ('planner', 0.038), ('prune', 0.038), ('checking', 0.038), ('bottleneck', 0.038), ('structures', 0.036), ('value', 0.036), ('attributed', 0.036), ('exact', 0.035), ('updates', 0.035), ('candidate', 0.035), ('reduce', 0.035), ('pruning', 0.035), ('negligible', 0.035), ('program', 0.034), ('dimensionality', 0.034), ('linearity', 0.034), ('gordon', 0.034), ('bounding', 0.034), ('ijcai', 0.034), ('zt', 0.034), ('horizon', 0.034), ('exploit', 0.034), ('reducing', 0.033), ('spatially', 0.032), ('offers', 0.032), ('built', 0.032), ('intelligence', 0.032), ('likely', 0.032), ('maintaining', 0.031), ('encountered', 0.031), ('leaf', 0.031), ('convex', 0.031), ('domain', 0.031), ('step', 0.031), ('applying', 0.031), ('recursively', 0.031), ('fast', 0.03), ('center', 0.03), ('whether', 0.03), ('exist', 0.029), ('problems', 0.029), ('effectiveness', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="29-tfidf-1" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>2 0.40365198 <a title="29-tfidf-2" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efﬁciently.</p><p>3 0.25630203 <a title="29-tfidf-3" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>Author: Pascal Poupart, Craig Boutilier</p><p>Abstract: We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic ﬁnite state controllers, combining several advantages of gradient ascent (efﬁciency, search through restricted controller space) and policy iteration (less vulnerability to local optima).</p><p>4 0.16824514 <a title="29-tfidf-4" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>5 0.15309238 <a title="29-tfidf-5" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>Author: Yuan Qi, Tom Minka</p><p>Abstract: Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a “message” to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms. 1</p><p>6 0.14603689 <a title="29-tfidf-6" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>7 0.14366706 <a title="29-tfidf-7" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>8 0.14047423 <a title="29-tfidf-8" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>9 0.11843538 <a title="29-tfidf-9" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>10 0.11649421 <a title="29-tfidf-10" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<p>11 0.11220861 <a title="29-tfidf-11" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>12 0.10905416 <a title="29-tfidf-12" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>13 0.098512203 <a title="29-tfidf-13" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>14 0.094918169 <a title="29-tfidf-14" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>15 0.08901035 <a title="29-tfidf-15" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>16 0.086980045 <a title="29-tfidf-16" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>17 0.084765874 <a title="29-tfidf-17" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>18 0.081354119 <a title="29-tfidf-18" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>19 0.080775134 <a title="29-tfidf-19" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>20 0.080661982 <a title="29-tfidf-20" href="./nips-2003-Learning_the_k_in_k-means.html">111 nips-2003-Learning the k in k-means</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.267), (1, 0.156), (2, -0.204), (3, 0.233), (4, -0.072), (5, -0.221), (6, -0.111), (7, 0.026), (8, 0.182), (9, 0.118), (10, 0.076), (11, -0.078), (12, 0.093), (13, 0.096), (14, 0.013), (15, 0.011), (16, 0.105), (17, 0.115), (18, 0.209), (19, 0.083), (20, 0.002), (21, 0.013), (22, 0.016), (23, 0.023), (24, 0.258), (25, 0.112), (26, -0.127), (27, 0.062), (28, 0.013), (29, 0.058), (30, 0.002), (31, 0.094), (32, 0.024), (33, -0.06), (34, -0.046), (35, 0.031), (36, 0.156), (37, 0.004), (38, 0.1), (39, 0.075), (40, 0.06), (41, 0.009), (42, -0.071), (43, 0.023), (44, -0.012), (45, 0.043), (46, 0.036), (47, 0.034), (48, -0.001), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98160267 <a title="29-lsi-1" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>2 0.86057943 <a title="29-lsi-2" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efﬁciently.</p><p>3 0.78147262 <a title="29-lsi-3" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>Author: Pascal Poupart, Craig Boutilier</p><p>Abstract: We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic ﬁnite state controllers, combining several advantages of gradient ascent (efﬁciency, search through restricted controller space) and policy iteration (less vulnerability to local optima).</p><p>4 0.62110937 <a title="29-lsi-4" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>Author: Matthew R. Rudary, Satinder P. Singh</p><p>Abstract: Predictive state representations (PSRs) use predictions of a set of tests to represent the state of controlled dynamical systems. One reason why this representation is exciting as an alternative to partially observable Markov decision processes (POMDPs) is that PSR models of dynamical systems may be much more compact than POMDP models. Empirical work on PSRs to date has focused on linear PSRs, which have not allowed for compression relative to POMDPs. We introduce a new notion of tests which allows us to deﬁne a new type of PSR that is nonlinear in general and allows for exponential compression in some deterministic dynamical systems. These new tests, called e-tests, are related to the tests used by Rivest and Schapire [1] in their work with the diversity representation, but our PSR avoids some of the pitfalls of their representation—in particular, its potential to be exponentially larger than the equivalent POMDP. 1</p><p>5 0.58510172 <a title="29-lsi-5" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>Author: Amos J. Storkey</p><p>Abstract: Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coeﬃcients. Furthermore eﬃcient generalised belief propagation methods between clusters of four nodes enable the Fourier coeﬃcients to be inferred and the missing data to be estimated in near to O(n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal. 1</p><p>6 0.56572491 <a title="29-lsi-6" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>7 0.52607214 <a title="29-lsi-7" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>8 0.4618232 <a title="29-lsi-8" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>9 0.42514294 <a title="29-lsi-9" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>10 0.40381646 <a title="29-lsi-10" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>11 0.3840864 <a title="29-lsi-11" href="./nips-2003-An_Iterative_Improvement_Procedure_for_Hierarchical_Clustering.html">24 nips-2003-An Iterative Improvement Procedure for Hierarchical Clustering</a></p>
<p>12 0.38204029 <a title="29-lsi-12" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>13 0.36587229 <a title="29-lsi-13" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>14 0.36442918 <a title="29-lsi-14" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>15 0.35556605 <a title="29-lsi-15" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>16 0.34825888 <a title="29-lsi-16" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>17 0.34492674 <a title="29-lsi-17" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>18 0.33225456 <a title="29-lsi-18" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>19 0.32517782 <a title="29-lsi-19" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>20 0.31273082 <a title="29-lsi-20" href="./nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">62 nips-2003-Envelope-based Planning in Relational MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.469), (11, 0.017), (29, 0.011), (30, 0.019), (35, 0.057), (53, 0.076), (71, 0.047), (76, 0.033), (85, 0.1), (91, 0.079), (99, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95623404 <a title="29-lda-1" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>2 0.94845498 <a title="29-lda-2" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>Author: Thore Graepel, Ralf Herbrich, Andriy Kharechko, John S. Shawe-taylor</p><p>Abstract: We present a modiﬁed version of the perceptron learning algorithm (PLA) which solves semideﬁnite programs (SDPs) in polynomial time. The algorithm is based on the following three observations: (i) Semideﬁnite programs are linear programs with inﬁnitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in ﬁnitely many updates. Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time. We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods. 1</p><p>3 0.9435454 <a title="29-lda-3" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>Author: Salvador Ruiz-correa, Linda G. Shapiro, Marina Meila, Gabriel Berson</p><p>Abstract: We present and empirically test a novel approach for categorizing 3-D free form object shapes represented by range data . In contrast to traditional surface-signature based systems that use alignment to match speciﬁc objects, we adapted the newly introduced symbolic-signature representation to classify deformable shapes [10]. Our approach constructs an abstract description of shape classes using an ensemble of classiﬁers that learn object class parts and their corresponding geometrical relationships from a set of numeric and symbolic descriptors. We used our classiﬁcation engine in a series of large scale discrimination experiments on two well-deﬁned classes that share many common distinctive features. The experimental results suggest that our method outperforms traditional numeric signature-based methodologies. 1 1</p><p>4 0.87183857 <a title="29-lda-4" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>Author: Claudio Fanti, Marzia Polito, Pietro Perona</p><p>Abstract: Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can eﬀortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body’s centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more signiﬁcantly, to the introduction of the centroid variable. 1</p><p>5 0.69924003 <a title="29-lda-5" href="./nips-2003-Bounded_Finite_State_Controllers.html">42 nips-2003-Bounded Finite State Controllers</a></p>
<p>Author: Pascal Poupart, Craig Boutilier</p><p>Abstract: We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic ﬁnite state controllers, combining several advantages of gradient ascent (efﬁciency, search through restricted controller space) and policy iteration (less vulnerability to local optima).</p><p>6 0.65302956 <a title="29-lda-6" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>7 0.5939945 <a title="29-lda-7" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>8 0.58426905 <a title="29-lda-8" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>9 0.57929415 <a title="29-lda-9" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>10 0.57903492 <a title="29-lda-10" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>11 0.55550915 <a title="29-lda-11" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>12 0.55191541 <a title="29-lda-12" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<p>13 0.54870504 <a title="29-lda-13" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>14 0.54757327 <a title="29-lda-14" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>15 0.54744774 <a title="29-lda-15" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>16 0.54371774 <a title="29-lda-16" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>17 0.54268759 <a title="29-lda-17" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>18 0.53992677 <a title="29-lda-18" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>19 0.53924072 <a title="29-lda-19" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>20 0.53695428 <a title="29-lda-20" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
