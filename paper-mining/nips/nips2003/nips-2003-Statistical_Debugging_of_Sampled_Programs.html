<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2003-Statistical Debugging of Sampled Programs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-181" href="#">nips2003-181</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>181 nips-2003-Statistical Debugging of Sampled Programs</h1>
<br/><p>Source: <a title="nips-2003-181-pdf" href="http://papers.nips.cc/paper/2371-statistical-debugging-of-sampled-programs.pdf">pdf</a></p><p>Author: Alice X. Zheng, Michael I. Jordan, Ben Liblit, Alex Aiken</p><p>Abstract: We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately deﬁned utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs. 1</p><p>Reference: <a title="nips-2003-181-reference" href="../nips2003_reference/nips-2003-Statistical_Debugging_of_Sampled_Programs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. [sent-11, score-0.302]
</p><p>2 This is accomplished by maximizing an appropriately deﬁned utility function. [sent-13, score-0.211]
</p><p>3 It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs. [sent-14, score-0.528]
</p><p>4 Most users take software bugs for granted, and willingly run buggy programs every day with little complaint. [sent-16, score-0.681]
</p><p>5 In some sense, these user runs of the program are the ideal test suite any software engineer could hope for. [sent-17, score-0.489]
</p><p>6 User crash reports are used to direct debugging efforts toward those bugs which seem to affect the most people. [sent-19, score-0.72]
</p><p>7 In earlier work [1] we present a program sampling framework that collects data from users at minimal cost; the aggregated runs are then analyzed to isolate the bugs. [sent-22, score-0.527]
</p><p>8 In this paper, we describe how to design a single classiﬁcation utility function that integrates the various debugging heuristics. [sent-25, score-0.33]
</p><p>9 In particular, determinism of some features is a signiﬁcant issue in this domain, and an additional penalty term for false positives is included to deal with this aspect. [sent-26, score-0.593]
</p><p>10 Furthermore, utility levels, while subjective, are robust: we offer simple guidelines for their selection, and demonstrate that results remain stable and strong across a wide range of reasonable parameter settings. [sent-27, score-0.247]
</p><p>11 We start by brieﬂy describing the program sampling framework in Section 2, and present the feature selection framework in Section 3. [sent-28, score-0.452]
</p><p>12 2  Program Sampling Framework  Our approach relies on being able to collect information about program behavior at runtime. [sent-30, score-0.244]
</p><p>13 We scatter a large number of checks in the program code, but do not execute all of them during any single run. [sent-32, score-0.294]
</p><p>14 At runtime, the program tosses a coin (with low heads probability) independently for each assertion it encounters, and decides whether or not to execute the assertion. [sent-38, score-0.449]
</p><p>15 However, while it is not expensive to generate a random coin toss, doing so separately for each assertion would incur a very large overhead; the program will run even slower than just executing every assertion. [sent-39, score-0.5]
</p><p>16 Each assertion decrements this countdown by 1; when it reaches 0, we perform the assertion and generate another geometric random variable. [sent-46, score-0.368]
</p><p>17 1 However, checking to see if the counter has reached 0 at every assertion is still an expensive procedure. [sent-47, score-0.275]
</p><p>18 For further code optimization, we analyze each contiguous acyclic code region (loops- and recursion-free) at compile time and count the maximum number of assertions on any path through that region. [sent-48, score-0.229]
</p><p>19 Samples are taken in chronological order as the program runs. [sent-50, score-0.244]
</p><p>20 To save space, we instead record only the counts of how often each assertion is found to be true or false. [sent-52, score-0.258]
</p><p>21 When the program ﬁnishes, these counts, along with the program exit status, are sent back to the central server for further analysis. [sent-53, score-0.528]
</p><p>22 The program sampling framework is a non-trivial software analysis effort. [sent-54, score-0.365]
</p><p>23 Knowing the ﬁnal program exit status (crashed or successful) leaves us in 1 The sampling density h controls the tradeoff between runtime overhead and data sparsity. [sent-59, score-0.497]
</p><p>24 This is not a problem for large programs like Mozilla and Windows with thousands of crash reports a day. [sent-61, score-0.343]
</p><p>25 Good feature selection should be corroborated by classiﬁcation performance, though in our case, we only care about features that correctly predict one of the two classes. [sent-64, score-0.246]
</p><p>26 Hence, instead of working in the usual maximum likelihood setting for classiﬁcation and regularization, we deﬁne and maximize a more appropriate utility function. [sent-65, score-0.211]
</p><p>27 1  Some characteristics of the problem  We concentrate on isolating the bugs that are caused by the occurrence of a small set of features, i. [sent-71, score-0.364]
</p><p>28 assertions that are always true when a crash occurs. [sent-73, score-0.328]
</p><p>29 2 We want to identify the predicate counts that are positively correlated with the program crashing. [sent-74, score-0.367]
</p><p>30 Due to sampling effects, it is quite possible that a feature responsible for the ultimate crash may not have been observed in a given run. [sent-77, score-0.392]
</p><p>31 This is especially true in the case of “quick and painless” deaths, where a program crashes very soon after the actual bug occurs. [sent-78, score-0.659]
</p><p>32 Normally this would be an easy bug to ﬁnd, because one wouldn’t have to look very far beyond the crashing point at the top of the stack. [sent-79, score-0.343]
</p><p>33 However, this is a challenge for our approach, because there may be only a single opportunity to sample the buggy feature before the program dies. [sent-80, score-0.39]
</p><p>34 Thus many crashes may have an input feature proﬁle that is very similar to that of a successful run. [sent-81, score-0.314]
</p><p>35 At the other end of the spectrum, if we are dealing with a deterministic bug3 , false positives should have a probability of zero: if the buggy feature is observed to be true, then the program has to crash; if the program did not crash, then the bug must not have occurred. [sent-83, score-1.207]
</p><p>36 Therefore, for a deterministic bug, any false positives during the training process should incur a much larger penalty compared to any false negatives. [sent-84, score-0.56]
</p><p>37 2  Designing the utility function  Let (x, y) denote a data point, where x is an input vector of non-negative integer counts, and y ∈ {0, 1} is the output label. [sent-86, score-0.241]
</p><p>38 The last two cases represent false negative and false positive, respectively. [sent-89, score-0.314]
</p><p>39 In the general form of utility maximization for classiﬁcation (see, e. [sent-90, score-0.211]
</p><p>40 We do not focus on this type of bugs in this paper. [sent-93, score-0.364]
</p><p>41 3 A bug is deterministic if it crashes the program every time it is observed. [sent-94, score-0.732]
</p><p>42 For example, dereferencing a null pointer would crash the program without exception. [sent-95, score-0.527]
</p><p>43 Note that this notion of determinism is data-dependent: it is always predicated on the trial runs that we have seen. [sent-96, score-0.3]
</p><p>44 To slightly simplify the 1 formula, we choose the same functional form for u1 and u2 , but add an extra penalty term for false positives: u1 (x; θ) := u2 (x; θ) u3 (x; θ) u4 (x; θ)  := δ1 (log2 µ(x; θ) + 1) := δ2 (log2 (1 − µ(x; θ)) + 1)  (4) (5)  := δ2 (log2 (1 − µ(x; θ)) + 1) − δ3 θT x . [sent-107, score-0.244]
</p><p>45 Also, we can fold any multiplicative constants of the utility functions into δi , so the base of the log function is freely exchangeable. [sent-109, score-0.248]
</p><p>46 We ﬁnd that the expected utility function is equivalent to: E U = δ1 y log µ + δ2 (1 − y) log(1 − µ) − δ3 θT x(1 − y)I{µ>1/2} − λ θ  1 1  . [sent-110, score-0.248]
</p><p>47 In general, this expected utility function weighs each class separately using δi , and has an additional penalty term for false positives. [sent-113, score-0.455]
</p><p>48 3  Interpretation of the utility functions  Let us closely examine the utility functions deﬁned in Eqns. [sent-125, score-0.422]
</p><p>49 It is positive when z is positive, and approaches 4  Assuming that the more abnormalities there are, the more likely it is for the program to crash, it is reasonable to use a classiﬁer based on a linear combination of features. [sent-129, score-0.275]
</p><p>50 5  −2 −2  −1  0 z  1  2  −2 −2  −z/2ln2 −z/ln2  −1  0 z  1  2  Figure 1: (a) Plot of the true positive indicator function and the utility function log2 µ(z) + 1. [sent-144, score-0.291]
</p><p>51 (b) Plot of the true negative indicator function, utility function log2 (1 − µ(z)) + 1, and its asymptotic slopes −z/ log 2 and −z/2 log 2. [sent-145, score-0.334]
</p><p>52 On the other hand, when z is negative, the utility function is negative, acting as a penalty for false negatives. [sent-148, score-0.455]
</p><p>53 Hence, when the false positive is close to the decision boundary, the additional penalty of θT x = z in Eqn. [sent-156, score-0.275]
</p><p>54 Most of the time a program exits successfully without crashing, so we have to deal with having many more successful runs than crashed runs (see Section 5). [sent-160, score-0.714]
</p><p>55 Finally, δ3 is the knob of determinism: if the bug is deterministic, then setting δ3 to a large value will severely penalize false positives; if the bug is not deterministic, then a small value for δ3 affords the necessary slack to accommodate runs which should have failed but did not. [sent-162, score-0.795]
</p><p>56 As we shall see in Section 5, if the bug is truly deterministic, then the quality of the ﬁnal features selected will be higher for large δ3 values. [sent-163, score-0.37]
</p><p>57 In a previous paper [1], we outlined some simple feature elimination heuristics that can be used in the case of a deterministic bug. [sent-164, score-0.276]
</p><p>58 Elimination by universal falsehood discards any counter that is always zero, because it likely represents an assertion that can never be true. [sent-165, score-0.325]
</p><p>59 Elimination by lack of failing example discards any counter that is zero on all crashes, because what never happens cannot have caused the crash. [sent-167, score-0.221]
</p><p>60 Elimination by successful counterexample discards any counter that is non-zero on any successful run, because these are assertions that can be true without a subsequent program failure. [sent-168, score-0.718]
</p><p>61 Also, if a heavily weighted feature xi is positive on a successful run in the training set, then the classiﬁer is more likely to result in a false positive. [sent-172, score-0.395]
</p><p>62 The false positive penalty term will then decrease the weight θi , so that such a feature is unlikely to be chosen at the end. [sent-173, score-0.353]
</p><p>63 Thus utility maximization also handles elimination by successful counterexample . [sent-174, score-0.452]
</p><p>64 4  Two Case Studies  As examples, we present two cases studies of C programs with bugs that are at the opposite ends of the determinism spectrum. [sent-176, score-0.607]
</p><p>65 2 is known to contain a bug that involves overwriting existing ﬁles. [sent-179, score-0.257]
</p><p>66 If the user responds to a conﬁrmation prompt with EOF rather than yes or no, ccrypt consistently crashes. [sent-180, score-0.327]
</p><p>67 We ﬁnd that feeding bc nine megabytes of random input causes it to crash roughly one time in four while calling malloc() — a strong indication of heap corruption. [sent-183, score-0.45]
</p><p>68 Such bugs are inherently difﬁcult to ﬁx because they are inherently non-deterministic: there is no guarantee that a mangled heap will cause a crash soon or indeed at all. [sent-184, score-0.723]
</p><p>69 Our instrumented program adds instrumentation after each function call to sample and record the number of times the return value is negative, zero, or positive. [sent-187, score-0.339]
</p><p>70 Each run uses a randomly selected set of present or absent ﬁles, randomized command line ﬂags, and randomized responses to ccrypt prompts including the occasional EOF. [sent-190, score-0.365]
</p><p>71 We have collected 7204 trial runs at a sampling rate of 1/100, 1162 of which result in a crash. [sent-191, score-0.24]
</p><p>72 Out of the 1710 counter features, 1542 are constant across all runs, leaving 168 counters to be considered in the training process. [sent-193, score-0.228]
</p><p>73 Our bc data set consists of 3051 runs with distinct random inputs at a sampling rate of 1/1000. [sent-203, score-0.339]
</p><p>74 The smaller ccrypt dataset requires just under 8 seconds. [sent-214, score-0.25]
</p><p>75 The more important knobs are δ1 and δ3 : the former controls the relative importance of classiﬁcation performance on crashed runs, the latter adjusts the believed level of determinism of the bug. [sent-217, score-0.27]
</p><p>76 (1) In order to counter the effects of imbalanced datasets, the ratio of δ1 /δ2 should be at least around the range of the ratio of successful to crashed runs. [sent-219, score-0.354]
</p><p>77 This is especially crucial for the ccrypt data set, which contains roughly 32 successful runs for every crash. [sent-220, score-0.487]
</p><p>78 (2) δ3 should not be higher than δ1 , because it is ultimately more important  (a) ccrypt  (b) ccrypt, δ1 = 30  (d) bc, δ1 = 5  (c) bc  1. [sent-221, score-0.388]
</p><p>79 2  0  5 10 15 20 25 δ3  1  1  5  10 δ1  15  20  1  0  1  2  δ3  3  4  5  Figure 2: (a,b) Cross-validation scores for the ccrypt data set; (c,d) Cross-validation scores for the bc data set. [sent-237, score-0.462]
</p><p>80 to correctly classify crashes than to not have any false positives. [sent-239, score-0.315]
</p><p>81 2(a) shows a plot of cross-validation score (maximum over a number of settings for δ2 and δ3 ) for the ccrypt data set at various δ1 values. [sent-242, score-0.38]
</p><p>82 The “smoking gun” which directly indicates the ccrypt bug is: traverse. [sent-248, score-0.507]
</p><p>83 In all of the above mentioned safe settings for δ1 and δ3 , this feature is returned as the top feature. [sent-250, score-0.217]
</p><p>84 This is to be expected: the bug in bc is non-deterministic, and therefore false positives do indeed exist in the training set. [sent-257, score-0.638]
</p><p>85 As for the feature selection results for bc, for all reasonable parameter settings (and even those that do not have the best classiﬁcation performance), the top features are a group of correlated counters that all point to the index of an array being abnormally big. [sent-259, score-0.566]
</p><p>86 c:176:  more more more more more  arrays(): arrays(): arrays(): arrays(): arrays():  indx indx indx indx indx  > > > > >  optopt opterr use math quiet f count  In Fig. [sent-270, score-1.057]
</p><p>87 The top feature becomes a necessary but not sufﬁcient condition for a crash – a false positive-inducing feature! [sent-273, score-0.512]
</p><p>88 Hence the lesson is that if the bug is believed to be deterministic then δ3 should always be positive. [sent-274, score-0.364]
</p><p>89 They also indicate that the variable indx seems to be abnormally big. [sent-277, score-0.273]
</p><p>90 Indeed, indx is the array index that runs over the actual array length, which is contained in the integer variable a count. [sent-278, score-0.475]
</p><p>91 The program may crash long after the ﬁrst array bound violation, which means that there are many opportunities for the sampling framework to observe the abnormally big value of indx. [sent-279, score-0.684]
</p><p>92 Since there are many comparisons between indx and other integer variables, there is a large set of inter-correlated counters, any subset of which may be picked by our algorithm as the top features. [sent-280, score-0.275]
</p><p>93 In the training run shown above, the smoking gun of indx > a count is ranked number 8. [sent-281, score-0.38]
</p><p>94 But in general its rank could be much smaller, because the top features already sufﬁce for predicting crashes and pointing us to the right line in the code. [sent-282, score-0.279]
</p><p>95 6  Conclusions and Future Work  Our goal is a system that automatically pinpoints the location of bugs in widely deployed software. [sent-283, score-0.364]
</p><p>96 We tackle different types of bugs using a custom-designed utility function with a “determinism level” knob. [sent-284, score-0.575]
</p><p>97 Our methods are shown to work on two real-world programs, and are able to locate the bugs in a range of parameter settings. [sent-285, score-0.364]
</p><p>98 In on-going research, we are extending our approach to deal with the problem of multiple bugs in larger programs. [sent-288, score-0.409]
</p><p>99 We are also working on modifying the program sampling framework to allow denser sampling in more important regions of the code. [sent-289, score-0.398]
</p><p>100 This should alleviate the sparsity of features while reducing the number of runs required to yield useful results. [sent-290, score-0.237]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bugs', 0.364), ('bug', 0.257), ('ccrypt', 0.25), ('program', 0.244), ('crash', 0.237), ('utility', 0.211), ('indx', 0.205), ('crashes', 0.158), ('false', 0.157), ('assertion', 0.138), ('bc', 0.138), ('counter', 0.137), ('determinism', 0.137), ('runs', 0.124), ('debugging', 0.119), ('programs', 0.106), ('crashed', 0.099), ('classi', 0.097), ('elimination', 0.093), ('assertions', 0.091), ('counters', 0.091), ('subgradient', 0.091), ('penalty', 0.087), ('positives', 0.086), ('features', 0.081), ('arrays', 0.079), ('successful', 0.078), ('feature', 0.078), ('user', 0.077), ('sampling', 0.077), ('deterministic', 0.073), ('abnormally', 0.068), ('aiken', 0.068), ('buggy', 0.068), ('liblit', 0.068), ('score', 0.067), ('coin', 0.067), ('settings', 0.063), ('record', 0.063), ('pointers', 0.059), ('array', 0.058), ('counts', 0.057), ('runtime', 0.054), ('code', 0.053), ('selection', 0.053), ('run', 0.051), ('overhead', 0.05), ('checks', 0.05), ('discards', 0.05), ('uc', 0.05), ('indicator', 0.049), ('ascent', 0.048), ('users', 0.048), ('cation', 0.046), ('countdown', 0.046), ('crashing', 0.046), ('decrements', 0.046), ('eof', 0.046), ('gun', 0.046), ('pinpoint', 0.046), ('pointer', 0.046), ('smoking', 0.046), ('xreadline', 0.046), ('deal', 0.045), ('software', 0.044), ('inherently', 0.041), ('berkeley', 0.04), ('regularization', 0.04), ('top', 0.04), ('guesses', 0.04), ('heap', 0.04), ('counterexample', 0.04), ('imbalanced', 0.04), ('exit', 0.04), ('trial', 0.039), ('division', 0.039), ('cs', 0.038), ('log', 0.037), ('scores', 0.037), ('safe', 0.036), ('wild', 0.036), ('guidelines', 0.036), ('zheng', 0.036), ('roughly', 0.035), ('care', 0.034), ('correlated', 0.034), ('aggregated', 0.034), ('failing', 0.034), ('believed', 0.034), ('heuristics', 0.032), ('count', 0.032), ('selected', 0.032), ('identify', 0.032), ('return', 0.032), ('status', 0.032), ('command', 0.032), ('alleviate', 0.032), ('positive', 0.031), ('handles', 0.03), ('integer', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="181-tfidf-1" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>Author: Alice X. Zheng, Michael I. Jordan, Ben Liblit, Alex Aiken</p><p>Abstract: We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately deﬁned utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs. 1</p><p>2 0.0985962 <a title="181-tfidf-2" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>Author: Jianxin Wu, James M. Rehg, Matthew D. Mullin</p><p>Abstract: Face detection is a canonical example of a rare event detection problem, in which target patterns occur with much lower frequency than nontargets. Out of millions of face-sized windows in an input image, for example, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classiﬁers of equivalent quality. This faster method could be used for more demanding classiﬁcation tasks, such as on-line learning. 1</p><p>3 0.076490305 <a title="181-tfidf-3" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><p>4 0.075295359 <a title="181-tfidf-4" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>Author: Volker Roth, Tilman Lange</p><p>Abstract: A novel approach to combining clustering and feature selection is presented. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. On the technical side, we present an efﬁcient optimization algorithm with guaranteed local convergence property. The only free parameter of this method is selected by a resampling-based stability analysis. Experiments with real-world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features. 1</p><p>5 0.053596251 <a title="181-tfidf-5" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>Author: Koji Tsuda, Gunnar Rätsch</p><p>Abstract: A common way of image denoising is to project a noisy image to the subspace of admissible images made for instance by PCA. However, a major drawback of this method is that all pixels are updated by the projection, even when only a few pixels are corrupted by noise or occlusion. We propose a new method to identify the noisy pixels by 1 -norm penalization and update the identiﬁed pixels only. The identiﬁcation and updating of noisy pixels are formulated as one linear program which can be solved efﬁciently. Especially, one can apply the ν-trick to directly specify the fraction of pixels to be reconstructed. Moreover, we extend the linear program to be able to exploit prior knowledge that occlusions often appear in contiguous blocks (e.g. sunglasses on faces). The basic idea is to penalize boundary points and interior points of the occluded area differently. We are able to show the ν-property also for this extended LP leading a method which is easy to use. Experimental results impressively demonstrate the power of our approach.</p><p>6 0.052622054 <a title="181-tfidf-6" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>7 0.051638354 <a title="181-tfidf-7" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>8 0.051628649 <a title="181-tfidf-8" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>9 0.050073087 <a title="181-tfidf-9" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>10 0.049132798 <a title="181-tfidf-10" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>11 0.048737872 <a title="181-tfidf-11" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>12 0.048640881 <a title="181-tfidf-12" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>13 0.047764488 <a title="181-tfidf-13" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>14 0.047598518 <a title="181-tfidf-14" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>15 0.047539588 <a title="181-tfidf-15" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>16 0.046364646 <a title="181-tfidf-16" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>17 0.045683295 <a title="181-tfidf-17" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>18 0.044632711 <a title="181-tfidf-18" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>19 0.044237062 <a title="181-tfidf-19" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>20 0.044204302 <a title="181-tfidf-20" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.171), (1, -0.032), (2, -0.004), (3, -0.076), (4, -0.021), (5, -0.036), (6, -0.047), (7, -0.021), (8, -0.005), (9, 0.086), (10, -0.007), (11, -0.037), (12, -0.017), (13, 0.01), (14, 0.043), (15, 0.044), (16, -0.07), (17, 0.007), (18, 0.089), (19, 0.007), (20, 0.016), (21, 0.007), (22, -0.035), (23, -0.029), (24, -0.002), (25, -0.023), (26, 0.041), (27, 0.008), (28, -0.036), (29, -0.015), (30, 0.0), (31, 0.036), (32, -0.038), (33, 0.039), (34, -0.003), (35, 0.023), (36, -0.024), (37, -0.081), (38, -0.023), (39, 0.1), (40, -0.094), (41, 0.003), (42, 0.056), (43, -0.023), (44, -0.04), (45, 0.098), (46, -0.046), (47, -0.037), (48, 0.022), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93923402 <a title="181-lsi-1" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>Author: Alice X. Zheng, Michael I. Jordan, Ben Liblit, Alex Aiken</p><p>Abstract: We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately deﬁned utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs. 1</p><p>2 0.64097995 <a title="181-lsi-2" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore</p><p>Abstract: Given an N ×N grid of squares, where each square has a count and an underlying population, our goal is to ﬁnd the square region with the highest density, and to calculate its signiﬁcance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff’s spatial scan statistic DK to ﬁnd the most signiﬁcant spatial disease cluster. A naive approach to ﬁnding the maximum density region requires O(N 3 ) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufﬁciently dense regions, this method ﬁnds the maximum density region in optimal O(N 2 ) time, in practice resulting in signiﬁcant (10-200x) speedups. 1</p><p>3 0.62823027 <a title="181-lsi-3" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>Author: Thomas R. Strohmann, Andrei Belitski, Gregory Z. Grudic, Dennis DeCoste</p><p>Abstract: The Minimax Probability Machine Classiﬁcation (MPMC) framework [Lanckriet et al., 2002] builds classiﬁers by minimizing the maximum probability of misclassiﬁcation, and gives direct estimates of the probabilistic accuracy bound Ω. The only assumptions that MPMC makes is that good estimates of means and covariance matrices of the classes exist. However, as with Support Vector Machines, MPMC is computationally expensive and requires extensive cross validation experiments to choose kernels and kernel parameters that give good performance. In this paper we address the computational cost of MPMC by proposing an algorithm that constructs nonlinear sparse MPMC (SMPMC) models by incrementally adding basis functions (i.e. kernels) one at a time – greedily selecting the next one that maximizes the accuracy bound Ω. SMPMC automatically chooses both kernel parameters and feature weights without using computationally expensive cross validation. Therefore the SMPMC algorithm simultaneously addresses the problem of kernel selection and feature selection (i.e. feature weighting), based solely on maximizing the accuracy bound Ω. Experimental results indicate that we can obtain reliable bounds Ω, as well as test set accuracies that are comparable to state of the art classiﬁcation algorithms.</p><p>4 0.62622786 <a title="181-lsi-4" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>Author: Michael J. Quinlan, Stephan K. Chalup, Richard H. Middleton</p><p>Abstract: This article addresses the issues of colour classiﬁcation and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classiﬁcation with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse ﬁtting for colour classiﬁcation and the statistical approach used for collision detection.</p><p>5 0.61627066 <a title="181-lsi-5" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>Author: Jianxin Wu, James M. Rehg, Matthew D. Mullin</p><p>Abstract: Face detection is a canonical example of a rare event detection problem, in which target patterns occur with much lower frequency than nontargets. Out of millions of face-sized windows in an input image, for example, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classiﬁers of equivalent quality. This faster method could be used for more demanding classiﬁcation tasks, such as on-line learning. 1</p><p>6 0.6111455 <a title="181-lsi-6" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>7 0.61015964 <a title="181-lsi-7" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>8 0.56208318 <a title="181-lsi-8" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>9 0.52320951 <a title="181-lsi-9" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>10 0.52011639 <a title="181-lsi-10" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<p>11 0.5184018 <a title="181-lsi-11" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>12 0.51827657 <a title="181-lsi-12" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>13 0.51734531 <a title="181-lsi-13" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>14 0.51229352 <a title="181-lsi-14" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>15 0.50758529 <a title="181-lsi-15" href="./nips-2003-Insights_from_Machine_Learning_Applied_to_Human_Visual_Classification.html">95 nips-2003-Insights from Machine Learning Applied to Human Visual Classification</a></p>
<p>16 0.48050222 <a title="181-lsi-16" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>17 0.47582799 <a title="181-lsi-17" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>18 0.46930286 <a title="181-lsi-18" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>19 0.46477193 <a title="181-lsi-19" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>20 0.46355587 <a title="181-lsi-20" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.05), (11, 0.029), (29, 0.015), (30, 0.017), (35, 0.059), (53, 0.089), (69, 0.023), (71, 0.039), (76, 0.046), (82, 0.369), (85, 0.078), (91, 0.105), (99, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87253577 <a title="181-lda-1" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>Author: David Ferguson, Aaron Morris, Dirk Hähnel, Christopher Baker, Zachary Omohundro, Carlos Reverte, Scott Thayer, Charles Whittaker, William Whittaker, Wolfram Burgard, Sebastian Thrun</p><p>Abstract: We present the software architecture of a robotic system for mapping abandoned mines. The software is capable of acquiring consistent 2D maps of large mines with many cycles, represented as Markov random £elds. 3D C-space maps are acquired from local 3D range scans, which are used to identify navigable paths using A* search. Our system has been deployed in three abandoned mines, two of which inaccessible to people, where it has acquired maps of unprecedented detail and accuracy. 1</p><p>same-paper 2 0.80069375 <a title="181-lda-2" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>Author: Alice X. Zheng, Michael I. Jordan, Ben Liblit, Alex Aiken</p><p>Abstract: We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately deﬁned utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs. 1</p><p>3 0.72696829 <a title="181-lda-3" href="./nips-2003-Nonlinear_Filtering_of_Electron_Micrographs_by_Means_of_Support_Vector_Regression.html">139 nips-2003-Nonlinear Filtering of Electron Micrographs by Means of Support Vector Regression</a></p>
<p>Author: Roland Vollgraf, Michael Scholz, Ian A. Meinertzhagen, Klaus Obermayer</p><p>Abstract: Nonlinear ﬁltering can solve very complex problems, but typically involve very time consuming calculations. Here we show that for ﬁlters that are constructed as a RBF network with Gaussian basis functions, a decomposition into linear ﬁlters exists, which can be computed eﬃciently in the frequency domain, yielding dramatic improvement in speed. We present an application of this idea to image processing. In electron micrograph images of photoreceptor terminals of the fruit ﬂy, Drosophila, synaptic vesicles containing neurotransmitter should be detected and labeled automatically. We use hand labels, provided by human experts, to learn a RBF ﬁlter using Support Vector Regression with Gaussian kernels. We will show that the resulting nonlinear ﬁlter solves the task to a degree of accuracy, which is close to what can be achieved by human experts. This allows the very time consuming task of data evaluation to be done eﬃciently. 1</p><p>4 0.4577677 <a title="181-lda-4" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>Author: H. J. Kim, Michael I. Jordan, Shankar Sastry, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition.</p><p>5 0.45775852 <a title="181-lda-5" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>Author: Liva Ralaivola, Florence D'alché-buc</p><p>Abstract: We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: ﬁrst, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions. 1</p><p>6 0.44815275 <a title="181-lda-6" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>7 0.44516626 <a title="181-lda-7" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>8 0.44180983 <a title="181-lda-8" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>9 0.43903828 <a title="181-lda-9" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>10 0.43759936 <a title="181-lda-10" href="./nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</a></p>
<p>11 0.43683815 <a title="181-lda-11" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>12 0.43539003 <a title="181-lda-12" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>13 0.43445429 <a title="181-lda-13" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>14 0.43361595 <a title="181-lda-14" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>15 0.43328467 <a title="181-lda-15" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>16 0.43294913 <a title="181-lda-16" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>17 0.4321638 <a title="181-lda-17" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>18 0.43131343 <a title="181-lda-18" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>19 0.43114668 <a title="181-lda-19" href="./nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</a></p>
<p>20 0.43107289 <a title="181-lda-20" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
