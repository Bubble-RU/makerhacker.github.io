<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-8" href="#">nips2003-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</h1>
<br/><p>Source: <a title="nips-2003-8-pdf" href="http://papers.nips.cc/paper/2383-a-holistic-approach-to-compositional-semantics-a-connectionist-model-and-robot-experiments.pdf">pdf</a></p><p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>Reference: <a title="nips-2003-8-reference" href="../nips2003_reference/nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. [sent-9, score-0.63]
</p><p>2 We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. [sent-10, score-0.358]
</p><p>3 We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. [sent-11, score-0.278]
</p><p>4 The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. [sent-12, score-1.092]
</p><p>5 This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. [sent-13, score-0.922]
</p><p>6 1  Introduction  Implementing language acquisition systems is one of the most diﬃcult problems, since not only the complexity of the syntactical structure, but also the diversity in the domain of meaning make this problem complicated and intractable. [sent-14, score-0.27]
</p><p>7 In particular, how linguistic meaning can be represented in the system is crucial, and this problem has been investigated for many years. [sent-15, score-0.288]
</p><p>8 In this paper, we introduce a connectionist model to acquire the semantics of language with respect to the behavioral patterns of a real robot. [sent-16, score-0.684]
</p><p>9 An essential question is how embodied compositional semantics can be acquired in the proposed connectionist model without providing any representations of the meaning of a word or behavior routines a priori. [sent-17, score-0.675]
</p><p>10 By “compositionality”, we refer to the fundamental human ability to understand a sentence from (1) the meanings of its constituents, and (2) the way in which they are put together. [sent-18, score-0.235]
</p><p>11 It is possible for a language acquisition system that acquires compositional semantics to derive the meaning of an unknown sentence from the meanings of known sentences. [sent-19, score-0.713]
</p><p>12 ” That is to say, generalization of meaning can be achieved through compositional semantics. [sent-24, score-0.298]
</p><p>13 From the point of view of compositionality, the symbolic representation of word meaning has much aﬃnity with processing the linguistic meaning of sentences [4]. [sent-25, score-0.804]
</p><p>14 For example, some models learn semantics in the form of correspondences between sentences and non-linguistic objects, i. [sent-27, score-0.521]
</p><p>15 , lexicon) is acquired independently of the usages of words in sentences (i. [sent-33, score-0.396]
</p><p>16 A priori separation of lexicon and syntax requires a pre-deﬁned manner of combining word meanings into the meaning of a sentence. [sent-37, score-0.429]
</p><p>17 In Iwahashi’s model, the class of a word is assumed to be given prior to learning its meaning because diﬀerent acquisition algorithms are required for nouns and verbs (c. [sent-38, score-0.317]
</p><p>18 Moreover, the meaning of a sentence is obtained by ﬁlling a pre-deﬁned template with meanings of words. [sent-41, score-0.332]
</p><p>19 Roy’s model does not require a priori knowledge of word classes, but requires the strong assumption, that the meaning of a word can be assigned to some pre-deﬁned attributes of non-linguistic objects. [sent-42, score-0.306]
</p><p>20 In this paper, we discuss an essential mechanism for self-organizing embodied compositional semantic representations, in which separate treatments of words and syntax are not required. [sent-44, score-0.352]
</p><p>21 Our model implements compositional semantics by utilizing the generalization capability of an RNN, where the meaning of each word cannot exist independently, but emerges from the relations with others (c. [sent-45, score-0.509]
</p><p>22 In this situation, a sort of generalization can be expected, such that the meanings of novel sentences can be inferred by analogy with learned ones. [sent-48, score-0.477]
</p><p>23 A ﬁnite set of two-word sentences consisting of a verb followed by a noun was considered. [sent-50, score-0.314]
</p><p>24 Our analysis will clarify what sorts of internal neural structures should be self-organized for achieving compositional semantics grounded to a robot’s behavioral experiences. [sent-51, score-0.718]
</p><p>25 Although our experimental design is limited, the current study will suggest an essential mechanism for acquiring grounded compositional semantics, with the minimal combinatorial structure of this ﬁnite language [2]. [sent-52, score-0.33]
</p><p>26 2  Task Design  The aim of our experimental task is to discuss an essential mechanism for self-organizing compositional semantics based on the behavior of a robot. [sent-53, score-0.309]
</p><p>27 In the training phase, our robot learns the relationships between sentences and the corresponding behavioral sensory-motor sequences of a robot in a supervised manner. [sent-54, score-1.139]
</p><p>28 It is then tested to generate behavioral sequences from a given sentence. [sent-55, score-0.532]
</p><p>29 We regard compositional semantics as being acquired if appropriate behavioral sequences can be generated from unlearned sentences by analogy with learned data. [sent-56, score-1.241]
</p><p>30 Our mobile robot has three actuators, with two wheels and a joint on the arm; a colored vision sensor; and two torque sensors, on the wheel and the arm (Figure 1a). [sent-57, score-0.233]
</p><p>31 The positions of these objects can be varied so long as the robot sees the red object on the left side of its ﬁeld of view, the green object in the middle, and the blue object on the right at the start of every trial of behavioral sequences. [sent-59, score-0.9]
</p><p>32 The robot thus learns nine categories of behavioral patterns, consisting of pointing at, pushing, and hitting each of the three objects, in a supervised manner. [sent-60, score-0.648]
</p><p>33 The meanings of these 18 possible sentences are given in terms of ﬁxed correspondences with the 9 behavioral categories (Figure 2). [sent-66, score-0.986]
</p><p>34 These synonyms are introduced to observe how the behavioral similarity aﬀects the acquired linguistic semantic structure. [sent-69, score-0.684]
</p><p>35 3  Proposed Model  Our model employs two RNNs with parametric bias nodes (RNNPBs) [15] in order to implement a linguistic module and a behavioral module (Figure 3). [sent-70, score-1.139]
</p><p>36 The linguistic module learns the above sentences represented as time sequences of words [1], while the behavioral module learns the behavioral sensory-motor sequences of the robot. [sent-72, score-2.034]
</p><p>37 To acquire the correspondences between the sentences and behavioral sequences, these two modules are connected to each other by using the parametric bias binding method. [sent-73, score-1.134]
</p><p>38 The diﬀerence is that in the RNNPB, the vectors that encode the time sequences are self-organized in PB nodes during the learning process. [sent-83, score-0.233]
</p><p>39 The common structural properties of all the training time sequences are acquired as connection weight values by using the  back-propagation through time (BPTT) algorithm, as used also in the conventional RNN [8, 11]. [sent-84, score-0.236]
</p><p>40 For each of n training time sequences of real-numbered vectors x0 , · · · , xn−1 , the back-propagated errors with respect to the PB nodes are accumulated for all time steps to update the PB vectors. [sent-90, score-0.274]
</p><p>41 Here, we introduce an abstracted operational notation for the RNNPB to facilitate a later explanation of our proposed method of binding language and behavior. [sent-100, score-0.237]
</p><p>42 (5)  The other important characteristic nature of the RNNPB is that the relational structure among the training time sequences can be acquired in the PB space through the learning process. [sent-105, score-0.218]
</p><p>43 For instance, by learning several cyclic time sequences of diﬀerent frequency, novel time sequences of intermediate frequency can be generated [6]. [sent-107, score-0.246]
</p><p>44 2 Binding In the proposed model, corresponding sentences and behavioral sequences are constrained to have the same PB vectors in both modules. [sent-109, score-0.867]
</p><p>45 Under this condition, corresponding behavioral sequences can be generated naturally from sentences. [sent-110, score-0.532]
</p><p>46 When a sentence si and its corresponding behavioral sequence bi have the same PB vector, we can obtain bi from si as follows: RNNPBB (RNNPB−1 (si )) → L  bi  (6)  where RNNPBL and RNNPBB are abstracted operators for the linguistic module and the behavioral module, respectively. [sent-111, score-1.548]
</p><p>47 Because of the constraint that corresponding sentences and behavioral sequences must have the same PB vectors, pbi is equal to p si . [sent-113, score-0.933]
</p><p>48 Therefore, we can obtain the corresponding behavioral sequence bi by utilizing the behavioral module with pbi . [sent-114, score-1.175]
</p><p>49 p si  =  pold + δp si + γL · (pold − pold ) si si bi  pbi  =  pold bi  + δpbi + γB ·  (pold si  −  pold ) bi  (7) (8)  where γL and γB are positive coeﬃcients that determine the strength of the binding. [sent-116, score-0.783]
</p><p>50 Equations (7) and (8) are the constrained update rules for the linguistic module and the behavior module, respectively. [sent-117, score-0.42]
</p><p>51 Under these rules, the PB vectors of a corresponding sentence si and behavioral sequence bi attract each other. [sent-118, score-0.647]
</p><p>52 3 Generalization of Correspondences As noted above, our model enables a robot to understand a sentence by means of a generated behavior as if the meaning of the sentence were composed of the meanings of the constituents. [sent-122, score-0.596]
</p><p>53 That is to say, the robot can generate appropriate behavioral sequences from all sentences without learning all correspondences. [sent-123, score-0.959]
</p><p>54 To achieve this, an unlearned sentence and its corresponding behavioral sequences must have the same PB vector. [sent-124, score-0.673]
</p><p>55 Nevertheless, the PB binding method only equalizes the PB vectors for given corresponding sentences and behavioral sequences (c. [sent-125, score-1.007]
</p><p>56 Finally, both PB structures converge into a common PB structure, and therefore, all corresponding sentences and behavioral sequences then share the same PB vectors automatically. [sent-131, score-0.867]
</p><p>57 4  Experiments  In the learning phase, the robot learned 14 of 18 correspondences between sentences and behavioral patterns (c. [sent-132, score-0.996]
</p><p>58 It was then tested to generate behavioral sequences from each of the remaining 4 sentences (“point green”, “point right”, “push red”, and “push left”). [sent-135, score-0.828]
</p><p>59 To enable a robot to learn correspondences robustly, ﬁve corresponding sentences and behavioral sequences were associated by using the PB binding method for each of the 14 training correspondences. [sent-136, score-1.236]
</p><p>60 Thus, the linguistic module learned 70 sentences with PB binding. [sent-137, score-0.703]
</p><p>61 Meanwhile, the behavioral module learned the behavioral sequences of the 9 categories, including 2 categories which had no corresponding sentences in the training set. [sent-138, score-1.543]
</p><p>62 The behavioral module learned 10 diﬀerent sensory-motor sequences for each behavioral category. [sent-139, score-1.183]
</p><p>63 It therefore learned 70 behavioral sequences corresponding to the training sentences with PB binding and the remaining 20 sequences independently. [sent-140, score-1.141]
</p><p>64 In addition, the behavioral module learned the same 90 behavioral sequences without binding. [sent-141, score-1.183]
</p><p>65 Each word is locally represented, such that each input node of the module corre-  sponds to a speciﬁc word. [sent-143, score-0.293]
</p><p>66 The linguistic module has 10 input nodes for each of 9 words and a starting symbol. [sent-147, score-0.477]
</p><p>67 The module also has 6 parametric bias nodes, 4 context nodes, 50 hidden nodes, and 10 prediction output nodes. [sent-148, score-0.28]
</p><p>68 A training behavioral sequence was created by sampling three sensory-motor vectors per second during a trial of the robot’s human-guided behavior. [sent-150, score-0.493]
</p><p>69 For robust learning of behavior, each training behavioral sequence was generated under a slightly diﬀerent environment in which object positions were varied. [sent-151, score-0.482]
</p><p>70 Typical behavioral sequences are about 5 to 25 seconds long, and therefore have about 15 to 75 sensory-motor vectors. [sent-155, score-0.532]
</p><p>71 The behavioral module had 26 input nodes for sensory-motor input, 6 parametric bias nodes, 6 context nodes, 70 hidden nodes, and 6 output nodes for motor commands and partial prediction of the sensory image at the next time step. [sent-158, score-0.831]
</p><p>72 The analysis reveals that the inter-module generalization realized by the PB binding method could ﬁll an essential role in self-organizing the compositional semantics of the simple language through the behavioral experiences of the robot. [sent-160, score-0.943]
</p><p>73 As a result, although the behavioral module was trained with the behavioral sequences of all behavioral categories, those in two of the categories, whose corresponding sentences were not in the linguistic training set, could not be bound. [sent-162, score-2.047]
</p><p>74 The most important result was that these dangling behavioral sequences could be bound with appropriate sentences. [sent-163, score-0.532]
</p><p>75 The linguistic PB vectors are computed by recognizing all the possible 18 sentences including 4 unseen ones (Figure 4a), and the behavioral PB vectors are computed at the learning phase for all the corresponding 90 behavioral sequences in the training data (Figure 4b). [sent-167, score-1.519]
</p><p>76 The acquired correspondences between sentences and behavioral sequences can be examined according to equation (6). [sent-168, score-1.016]
</p><p>77 In particular, the implicit binding of the four unlearned correspondences (“point green”↔POINTG, “point right”↔POINT-G, “push red”↔PUSH-R, and “push left”↔PUSH-R) demonstrates acquisition of the underlying semantics, or the generalized correspondences. [sent-169, score-0.358]
</p><p>78 The acquired common structure has two striking characteristics: (1) the combinatorial structure originated from the linguistic module, and (2) the metric based on the behavioral similarity originated from the behavioral module. [sent-170, score-1.114]
</p><p>79 8  POINT-R POINT-B  The second principal component  point red point left point blue point center point green point right push red push left push blue push center push green push right hit red hit left hit blue hit center hit green hit right  0. [sent-177, score-2.658]
</p><p>80 8  (b) Behavioral module  Figure 4: Plots of the bound linguistic module (a) and the bound behavioral module (b). [sent-181, score-1.216]
</p><p>81 Unlearned sentences and their corresponding behavioral categories are underlined. [sent-184, score-0.747]
</p><p>82 ” This predictable geometric regularity could be acquired by independent learning of the linguistic module. [sent-187, score-0.238]
</p><p>83 However it could not be acquired by independent learning of the behavioral module because these behavioral sequences can not be decomposed into plausible primitives, unlike the sentences which can be broken down into words. [sent-188, score-1.524]
</p><p>84 We can also see a metric reﬂecting the similarity of behavioral sequences not only in the behavioral modules but also in the linguistic module. [sent-189, score-1.177]
</p><p>85 The PB vectors of sentences that correspond to the same behavioral category take the similar values. [sent-190, score-0.744]
</p><p>86 For example, the two sentences corresponding to POINT-R (“point red” and “point left”) are encoded in similar PB vectors. [sent-191, score-0.296]
</p><p>87 Such a metric nature could not be observed in the independent learning of the linguistic module, in which all nouns were plotted symmetrically in the PB space by means of the syntactical constraints. [sent-192, score-0.221]
</p><p>88 The above observation thus conﬁrms that the embodied compositional semantics was selforganized through the uniﬁcation of both modules, which was implemented by the PB binding method. [sent-193, score-0.454]
</p><p>89 6  Discussion and Summary  Our simple experiments showed that the minimal grounded compositional semantics of our language can be acquired by generalizing the correspondences between sentences and the behavioral sensory-motor sequences of a robot. [sent-195, score-1.403]
</p><p>90 That is to say, the robot could understand relatively simple sentences in a systematic way, and could understand novel sentences. [sent-197, score-0.479]
</p><p>91 We claim that the acquisition of word meaning and syntax can not be separated from the standpoint of the symbol grounding problem [5]. [sent-199, score-0.327]
</p><p>92 The meanings of words depend on each other to compose the meanings of sentences [16]. [sent-200, score-0.571]
</p><p>93 ” The meaning of “red” must be something which combines with the meaning of “point”, “push” or “hit” to form the grounded meanings of sentences. [sent-202, score-0.423]
</p><p>94 This means that it is inevitably diﬃcult to explicitly extract the meaning of a word from the meaning of a sentence. [sent-204, score-0.325]
</p><p>95 Our model avoids this diﬃculty by implementing the grounded meaning of a word implicitly in terms of the relationships among the meanings of sentences based on behavioral experiences. [sent-205, score-1.084]
</p><p>96 Instead, the essential structures accounting for compositionality are fully self-organized in the iterative dynamics of the RNN, through the structural interactions between language and behavior using the PB binding method. [sent-207, score-0.339]
</p><p>97 Thus, the robot can understand “red” through its behavioral interactions in the designed tasks in a bottom-up way [14]. [sent-208, score-0.566]
</p><p>98 For example, the robot understands “point” through pointing at red, blue, and green objects. [sent-210, score-0.241]
</p><p>99 To the summary, the current study has shown the importance of generalization of the correspondences between sentences and behavioral patterns in the acquisition of an embodied language. [sent-211, score-0.971]
</p><p>100 However, we think that our model requires relatively fewer fraction of sentences to learn a larger language set, for a given degree of syntactic complexity. [sent-215, score-0.396]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pb', 0.553), ('behavioral', 0.409), ('sentences', 0.296), ('module', 0.214), ('rnnpb', 0.179), ('linguistic', 0.165), ('push', 0.159), ('compositional', 0.146), ('binding', 0.14), ('robot', 0.131), ('meanings', 0.124), ('meaning', 0.123), ('sequences', 0.123), ('hit', 0.118), ('correspondences', 0.115), ('semantics', 0.11), ('red', 0.098), ('pold', 0.089), ('green', 0.089), ('blue', 0.089), ('sentence', 0.085), ('word', 0.079), ('language', 0.078), ('acquired', 0.073), ('nodes', 0.071), ('modules', 0.071), ('compositionality', 0.068), ('embodied', 0.058), ('rnn', 0.058), ('pbi', 0.056), ('unlearned', 0.056), ('grounded', 0.053), ('syntax', 0.053), ('si', 0.049), ('acquisition', 0.047), ('likes', 0.045), ('bi', 0.042), ('categories', 0.042), ('vectors', 0.039), ('parametric', 0.037), ('acquire', 0.037), ('semantic', 0.037), ('xi', 0.036), ('arm', 0.035), ('nouns', 0.034), ('pxi', 0.034), ('verbs', 0.034), ('connectionist', 0.033), ('di', 0.031), ('essential', 0.031), ('bias', 0.029), ('generalization', 0.029), ('learned', 0.028), ('object', 0.028), ('words', 0.027), ('learns', 0.027), ('riken', 0.027), ('wheels', 0.027), ('understand', 0.026), ('grounding', 0.025), ('lexicon', 0.025), ('priori', 0.025), ('sequence', 0.023), ('training', 0.022), ('bptt', 0.022), ('bsi', 0.022), ('hirosawa', 0.022), ('rnnpbb', 0.022), ('rnnpbs', 0.022), ('rnns', 0.022), ('sugita', 0.022), ('syntactical', 0.022), ('systematicity', 0.022), ('tani', 0.022), ('behavior', 0.022), ('erent', 0.022), ('syntactic', 0.022), ('utilizing', 0.022), ('generation', 0.022), ('mobile', 0.022), ('combinatorial', 0.022), ('pointing', 0.021), ('center', 0.02), ('abstracted', 0.019), ('congruent', 0.019), ('mary', 0.019), ('meanwhile', 0.019), ('li', 0.019), ('update', 0.019), ('conventional', 0.018), ('point', 0.018), ('hitting', 0.018), ('originated', 0.018), ('rumelhart', 0.018), ('saitama', 0.018), ('torque', 0.018), ('verb', 0.018), ('sensors', 0.017), ('unseen', 0.017), ('patterns', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="8-tfidf-1" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>2 0.14516844 <a title="8-tfidf-2" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>3 0.064043529 <a title="8-tfidf-3" href="./nips-2003-Bounded_Invariance_and_the_Formation_of_Place_Fields.html">43 nips-2003-Bounded Invariance and the Formation of Place Fields</a></p>
<p>Author: Reto Wyss, Paul F. Verschure</p><p>Abstract: One current explanation of the view independent representation of space by the place-cells of the hippocampus is that they arise out of the summation of view dependent Gaussians. This proposal assumes that visual representations show bounded invariance. Here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representations. Our analysis is based on the behavior of a simulated robot in a virtual environment containing speciﬁc visual cues. Our results show that the temporal population code provides a representational substrate that can naturally account for the formation of place ﬁelds. 1</p><p>4 0.063865401 <a title="8-tfidf-4" href="./nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</a></p>
<p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><p>5 0.056926105 <a title="8-tfidf-5" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>Author: Curt Bererton, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball. 1</p><p>6 0.055307645 <a title="8-tfidf-6" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>7 0.055305276 <a title="8-tfidf-7" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>8 0.054233618 <a title="8-tfidf-8" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>9 0.048826952 <a title="8-tfidf-9" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>10 0.046374641 <a title="8-tfidf-10" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>11 0.04453541 <a title="8-tfidf-11" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>12 0.044332292 <a title="8-tfidf-12" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<p>13 0.042306703 <a title="8-tfidf-13" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>14 0.04068283 <a title="8-tfidf-14" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>15 0.039769106 <a title="8-tfidf-15" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>16 0.036409516 <a title="8-tfidf-16" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>17 0.035744406 <a title="8-tfidf-17" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>18 0.034064718 <a title="8-tfidf-18" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>19 0.032187369 <a title="8-tfidf-19" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>20 0.031431235 <a title="8-tfidf-20" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.108), (1, 0.027), (2, 0.022), (3, 0.005), (4, -0.043), (5, -0.058), (6, 0.017), (7, -0.022), (8, 0.021), (9, -0.019), (10, 0.072), (11, -0.063), (12, 0.008), (13, -0.008), (14, 0.149), (15, 0.055), (16, -0.042), (17, -0.036), (18, 0.025), (19, 0.012), (20, 0.035), (21, -0.043), (22, -0.101), (23, 0.007), (24, -0.062), (25, -0.063), (26, -0.068), (27, 0.007), (28, -0.094), (29, 0.191), (30, 0.217), (31, -0.082), (32, 0.046), (33, -0.029), (34, 0.072), (35, -0.162), (36, -0.085), (37, 0.067), (38, 0.009), (39, -0.138), (40, 0.132), (41, -0.021), (42, -0.145), (43, 0.207), (44, 0.006), (45, -0.09), (46, -0.106), (47, 0.077), (48, 0.104), (49, -0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97213274 <a title="8-lsi-1" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>2 0.77289885 <a title="8-lsi-2" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>3 0.52285427 <a title="8-lsi-3" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>Author: Xavier Carreras, Lluís Màrquez</p><p>Abstract: This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a ﬁltering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reﬂects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently. 1</p><p>4 0.48886263 <a title="8-lsi-4" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>Author: David Ferguson, Aaron Morris, Dirk Hähnel, Christopher Baker, Zachary Omohundro, Carlos Reverte, Scott Thayer, Charles Whittaker, William Whittaker, Wolfram Burgard, Sebastian Thrun</p><p>Abstract: We present the software architecture of a robotic system for mapping abandoned mines. The software is capable of acquiring consistent 2D maps of large mines with many cycles, represented as Markov random £elds. 3D C-space maps are acquired from local 3D range scans, which are used to identify navigable paths using A* search. Our system has been deployed in three abandoned mines, two of which inaccessible to people, where it has acquired maps of unprecedented detail and accuracy. 1</p><p>5 0.37869489 <a title="8-lsi-5" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>Author: Maxim Likhachev, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: In real world planning problems, time for deliberation is often limited. Anytime planners are well suited for these problems: they ﬁnd a feasible solution quickly and then continually work on improving it until time runs out. In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time. It starts by ﬁnding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows. Given enough time it ﬁnds a provably optimal solution. While improving its bound, ARA* reuses previous search efforts and, as a result, is signiﬁcantly more efﬁcient than other anytime search methods. In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover. 1</p><p>6 0.35449207 <a title="8-lsi-6" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>7 0.35319042 <a title="8-lsi-7" href="./nips-2003-Bounded_Invariance_and_the_Formation_of_Place_Fields.html">43 nips-2003-Bounded Invariance and the Formation of Place Fields</a></p>
<p>8 0.34670284 <a title="8-lsi-8" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>9 0.29782778 <a title="8-lsi-9" href="./nips-2003-Kernels_for_Structured_Natural_Language_Data.html">99 nips-2003-Kernels for Structured Natural Language Data</a></p>
<p>10 0.28210789 <a title="8-lsi-10" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>11 0.24575397 <a title="8-lsi-11" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<p>12 0.21025604 <a title="8-lsi-12" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>13 0.20795406 <a title="8-lsi-13" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>14 0.19542512 <a title="8-lsi-14" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>15 0.1921321 <a title="8-lsi-15" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>16 0.17996901 <a title="8-lsi-16" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>17 0.17703058 <a title="8-lsi-17" href="./nips-2003-On_the_Concentration_of_Expectation_and_Approximate_Inference_in_Layered_Networks.html">142 nips-2003-On the Concentration of Expectation and Approximate Inference in Layered Networks</a></p>
<p>18 0.16982208 <a title="8-lsi-18" href="./nips-2003-Minimising_Contrastive_Divergence_in_Noisy%2C_Mixed-mode_VLSI_Neurons.html">129 nips-2003-Minimising Contrastive Divergence in Noisy, Mixed-mode VLSI Neurons</a></p>
<p>19 0.16976894 <a title="8-lsi-19" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>20 0.16528437 <a title="8-lsi-20" href="./nips-2003-The_Doubly_Balanced_Network_of_Spiking_Neurons%3A_A_Memory_Model_with_High_Capacity.html">185 nips-2003-The Doubly Balanced Network of Spiking Neurons: A Memory Model with High Capacity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (11, 0.04), (29, 0.013), (30, 0.017), (35, 0.033), (53, 0.049), (55, 0.416), (66, 0.028), (71, 0.087), (76, 0.019), (85, 0.07), (91, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84181124 <a title="8-lda-1" href="./nips-2003-A_Holistic_Approach_to_Compositional_Semantics%3A_a_connectionist_model_and_robot_experiments.html">8 nips-2003-A Holistic Approach to Compositional Semantics: a connectionist model and robot experiments</a></p>
<p>Author: Yuuya Sugita, Jun Tani</p><p>Abstract: We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the “compositionality” of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the “embodiment” of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and conﬁrmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors. 1</p><p>2 0.66645885 <a title="8-lda-2" href="./nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</a></p>
<p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><p>3 0.50989741 <a title="8-lda-3" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>Author: Maneesh Sahani</p><p>Abstract: Signiﬁcant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a signiﬁcant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generativemodelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way. 1</p><p>4 0.35921958 <a title="8-lda-4" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>5 0.35564438 <a title="8-lda-5" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>Author: Leonid Sigal, Michael Isard, Benjamin H. Sigelman, Michael J. Black</p><p>Abstract: The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuousvalued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle ﬁlter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter. 1</p><p>6 0.35051283 <a title="8-lda-6" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>7 0.34497923 <a title="8-lda-7" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>8 0.34250277 <a title="8-lda-8" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>9 0.33993286 <a title="8-lda-9" href="./nips-2003-Linear_Response_for_Approximate_Inference.html">117 nips-2003-Linear Response for Approximate Inference</a></p>
<p>10 0.33972028 <a title="8-lda-10" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>11 0.33873197 <a title="8-lda-11" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>12 0.33724481 <a title="8-lda-12" href="./nips-2003-Boosting_versus_Covering.html">41 nips-2003-Boosting versus Covering</a></p>
<p>13 0.33625728 <a title="8-lda-13" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>14 0.33608136 <a title="8-lda-14" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>15 0.33532575 <a title="8-lda-15" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>16 0.33378166 <a title="8-lda-16" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>17 0.33372518 <a title="8-lda-17" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>18 0.33371753 <a title="8-lda-18" href="./nips-2003-Online_Classification_on_a_Budget.html">145 nips-2003-Online Classification on a Budget</a></p>
<p>19 0.33288208 <a title="8-lda-19" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>20 0.33285597 <a title="8-lda-20" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
