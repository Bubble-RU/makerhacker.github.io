<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-6" href="#">nips2003-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</h1>
<br/><p>Source: <a title="nips-2003-6-pdf" href="http://papers.nips.cc/paper/2529-a-fast-multi-resolution-method-for-detection-of-significant-spatial-disease-clusters.pdf">pdf</a></p><p>Author: Daniel B. Neill, Andrew W. Moore</p><p>Abstract: Given an N ×N grid of squares, where each square has a count and an underlying population, our goal is to ﬁnd the square region with the highest density, and to calculate its signiﬁcance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff’s spatial scan statistic DK to ﬁnd the most signiﬁcant spatial disease cluster. A naive approach to ﬁnding the maximum density region requires O(N 3 ) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufﬁciently dense regions, this method ﬁnds the maximum density region in optimal O(N 2 ) time, in practice resulting in signiﬁcant (10-200x) speedups. 1</p><p>Reference: <a title="nips-2003-6-reference" href="../nips2003_reference/nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Given an N ×N grid of squares, where each square has a count and an underlying population, our goal is to ﬁnd the square region with the highest density, and to calculate its signiﬁcance by randomization. [sent-7, score-0.781]
</p><p>2 Any density measure D, dependent on the total count and total population of a region, can be used. [sent-8, score-0.536]
</p><p>3 For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff’s spatial scan statistic DK to ﬁnd the most signiﬁcant spatial disease cluster. [sent-9, score-1.021]
</p><p>4 A naive approach to ﬁnding the maximum density region requires O(N 3 ) time, and is generally computationally infeasible. [sent-10, score-0.614]
</p><p>5 We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. [sent-11, score-0.697]
</p><p>6 For sufﬁciently dense regions, this method ﬁnds the maximum density region in optimal O(N 2 ) time, in practice resulting in signiﬁcant (10-200x) speedups. [sent-12, score-0.621]
</p><p>7 A major application is in identifying clusters of disease cases, for purposes ranging from detection of bioterrorism (ex. [sent-14, score-0.343]
</p><p>8 Assume an N × N grid of squares G, where each square si j ∈ G is associated with a count ci j and an underlying population pi j . [sent-18, score-0.519]
</p><p>9 For example, a square’s count may be the number of disease cases in that geographical region in a given time period, while its population may be the total number of people “at-risk” for the disease. [sent-19, score-0.916]
</p><p>10 Our goal is to ﬁnd the square region S ∗ ⊆ G with the highest density according to a density measure D: S∗ = arg maxS D(S). [sent-20, score-0.875]
</p><p>11 We use the abbreviations mdr for the “maximum density region” S∗ , and mrd for the “maximum region density” D(S∗ ), throughout. [sent-21, score-1.002]
</p><p>12 The density measure D must be an increasing function of the total count of the region, C(S) = ∑S ci j , and a decreasing function of the total population of the region, P(S) = ∑S pi j . [sent-22, score-0.594]
</p><p>13 In the case of a uniform underlying population, P(S) ∝ k 2 , where k is the size of region S. [sent-23, score-0.402]
</p><p>14 Our method is applicable to any density measure D, while the other algorithms are speciﬁc to the “standard” density measure D1 (S) = C(S) . [sent-27, score-0.464]
</p><p>15 The D1 measure is the P(S) number of points per unit population, for example this corresponds to the region with the highest observed disease rate. [sent-28, score-0.664]
</p><p>16 Unlike many other density measures, D 1 is monotonic: if a region S with density d is partitioned into any set of disjoint subregions, at least one subregion will have density d ≥ d. [sent-29, score-1.187]
</p><p>17 For a non-monotonic measure such as Kulldorff’s, it is possible to have a large dense region where none of its subregions are themselves dense, so bottom-up can fail. [sent-32, score-0.522]
</p><p>18 Our method deals with non-uniform underlying populations: this is particularly important for real-world epidemiological applications, in which an overdensity of disease cases is more signiﬁcant if the underlying population is large. [sent-35, score-0.575]
</p><p>19 Our goal is not only to ﬁnd the highest scoring region, but also to test whether that region is a true cluster or if it is likely to have occurred by chance. [sent-37, score-0.412]
</p><p>20 1 The spatial scan statistic A non-monotonic density measure which is of great interest to epidemiologists is Kulldorff’s spatial scan statistic [8], which we denote by DK . [sent-39, score-0.694]
</p><p>21 We then calculate the log of the likelihood ratio of two possibilities: that the disease rate q is higher in the region than outside the region, and that the disease rate is identical inside and outside the region. [sent-41, score-1.009]
</p><p>22 For a region with count C and population P, in a grid with total count Ctot and population Ptot , we can caltot −C tot tot culate DK = C log C + (Ctot − C) log Ctot −P − Ctot log Ctot , if C > Ctot , and 0 otherwise. [sent-42, score-1.169]
</p><p>23 [8] P P P P P proved that the spatial scan statistic is individually most powerful for ﬁnding a signiﬁcant region of elevated disease rate: it is more likely to detect the overdensity than any other test statistic. [sent-43, score-0.933]
</p><p>24 Once we have found the maximum density region (mdr) of grid G according to our density measure, we must still determine the statistical signiﬁcance of this region. [sent-46, score-0.873]
</p><p>25 Since the exact distribution of the test statistic is only known in special cases (such as D1 density with a uniform underlying population), in general we must perform Monte Carlo simulation for our hypothesis test. [sent-47, score-0.358]
</p><p>26 To do so, we run a large number R of random replications, where a replica has the same underlying populations pi j as G, but assumes a uniform disease tot (G) rate qrep = Ctot (G) for all squares. [sent-48, score-0.709]
</p><p>27 For each replica G , we ﬁrst generate all counts ci j P randomly from an inhomogeneous Poisson distribution with mean qrep pi j , then compute the maximum region density (mrd) of G and compare this to mrd(G). [sent-49, score-0.815]
</p><p>28 The number of replicas G with mrd(G ) ≥ mrd(G), divided by the total number of replications R, gives us the p-value for our maximum density region. [sent-50, score-0.33]
</p><p>29 05, we can conclude that the discovered region is statistically signiﬁcant (unlikely to have occurred  by chance) and is thus a “spatial overdensity. [sent-52, score-0.341]
</p><p>30 ” If the test fails, we have still discovered the maximum density region of G, but there is not sufﬁcient evidence that this is an overdensity. [sent-53, score-0.591]
</p><p>31 2 The naive approach The simplest method of ﬁnding the maximum density region is to compute the density of all square regions of sizes k = kmin . [sent-55, score-1.09]
</p><p>32 We can compute the density of any region S in O(1), by ﬁrst ﬁnding the count C(S) and population P(S), then applying our density measure D(C, P). [sent-60, score-0.986]
</p><p>33 However, signiﬁcance testing by Monte Carlo replication also requires us to ﬁnd the mrd for each replica G , and compare this to mrd(G). [sent-62, score-0.441]
</p><p>34 Since calculation of the mrd takes O(N 3 ) time for each replica, the total complexity is O(RN 3 ), and R is typically large (we assume R = 1000). [sent-63, score-0.357]
</p><p>35 First, we can stop examining a replica G immediately if we ﬁnd a region with density greater than mrd(G). [sent-65, score-0.633]
</p><p>36 Second, we can use the Central Limit Theorem to halt our Monte Carlo testing early if, after a number of replications R < R, we can conclude with high conﬁdence that the region is not signiﬁcant. [sent-66, score-0.364]
</p><p>37 The main difference of our problem from kernel density estimation, however, is that we are only interested in the maximum density region; thus, we do not necessarily need to build a space-partitioning tree at all resolutions. [sent-69, score-0.449]
</p><p>38 However, a simple partitioning approach fails because of the non-monotonicity of our density measure: a dense region may be split into two or more separate subregions, none of which is as dense as the original region. [sent-73, score-0.726]
</p><p>39 We denote a region S by an ordered triple (x, y, k), where (x, y) is the upper left corner of the region and k is its size. [sent-76, score-0.626]
</p><p>40 Next, we deﬁne the ω-children of a region S = (x, y, k) as the four overlapping subregions of size k − ω corresponding to the top left, top right, bottom left, and bottom right corners of S: (x, y, k − ω), (x + ω, y, k − ω), (x, y + ω, k − ω), and (x + ω, y + ω, k − ω). [sent-77, score-0.469]
</p><p>41 Next, we deﬁne a region as “even” if its size is 2k for some k ≥ 2, and “odd” if its size is 3 × 2k for some k ≥ 0. [sent-78, score-0.389]
</p><p>42 Thus the four g-children of an even region are odd, and each 2 overlaps 3 with the directly adjacent child regions. [sent-80, score-0.396]
</p><p>43 Similarly, we deﬁne the g-children of k an odd region S = (x, y, k) as its ω-children for ω = 3 . [sent-81, score-0.391]
</p><p>44 Thus the four g-children of an odd 1 region are even, and each overlaps 2 with the directly adjacent child regions. [sent-82, score-0.474]
</p><p>45 Note that 1 We  assume that a region must have size at least kmin to be signiﬁcant: here kmin = 3. [sent-83, score-0.505]
</p><p>46 2 An old trick allows us to compute the count of any k × k region in O(1): we ﬁrst form a matrix of  the cumulative counts, then compute each region’s count by adding at most four cumulative counts. [sent-84, score-0.495]
</p><p>47 even though a region has four g-children, and each of its g-children has four g-children, it has only nine (not 16) distinct grandchildren, several of which are the child of multiple regions. [sent-85, score-0.346]
</p><p>48 Each node represents a gridded region (denoted by a thick square) of the entire dataset (thin square and dots). [sent-88, score-0.67]
</p><p>49 Our algorithm focuses its search on the set of gridded regions, only searching non-gridded regions when necessary. [sent-92, score-0.422]
</p><p>50 This technique is useful because the total number of gridded regions is O(N 2 ), as in the simple quadtree partitioning method. [sent-93, score-0.525]
</p><p>51 This implies that, if only gridded regions need to be searched, our total time to ﬁnd the mdr of a grid is O(N 2 ). [sent-94, score-0.683]
</p><p>52 1 Top-down pruning So when can we search only gridded regions, or alternatively, when does a given nongridded region need to be searched? [sent-97, score-0.679]
</p><p>53 Our ﬁrst step is to derive an upper bound Dmax (S, k) on the density of subregions of minimum size k contained in a given region S (Section 2. [sent-99, score-0.765]
</p><p>54 Then we can compare Dmax (S, k) to the density D(S∗ ) of the best region found so far: if Dmax (S, k) < D(S∗ ), we know that no subregion of S with size k or more can be the mdr. [sent-101, score-0.819]
</p><p>55 First, if Dmax (S, kmin ) < D(S∗ ), we know that no subregion of S can be optimal; we can prune the region completely, and not search its (gridded or non-gridded) children. [sent-103, score-0.7]
</p><p>56 Second, we can show that (for 0 < k < n) any 3 region of size 2k + 1 or less is contained entirely in an odd gridded region of size 2 × 2k . [sent-104, score-1.092]
</p><p>57 n−1 + 2) < D(S∗ ) for the entire grid G, any optimal non-gridded region Thus, if Dmax (G, 2 must be contained in an odd gridded region. [sent-105, score-0.814]
</p><p>58 Similarly, if Dmax (S, 2k + 2) < D(S∗ ) for an odd gridded region S of size 3 × 2k , any optimal non-gridded subregion of S must be within an odd gridded subregion of S. [sent-106, score-1.575]
</p><p>59 Thus we can search only gridded regions if two conditions hold: 1) no subregion of G of size 2n−1 + 2 or more can be optimal, and 2) for each odd gridded region of size 3 × 2k , no subregion of size 2k + 2 or more can be optimal. [sent-107, score-1.726]
</p><p>60 2 Bounding subregion density To bound the maximum subregion density Dmax (S, k), we must ﬁnd the highest possible score D(S ) of a subregion S ⊆ S of size k or more. [sent-109, score-1.371]
</p><p>61 We assume that these are known, as well as lower and upper bounds [d min , dmax ] on the D1 density of subregions of S. [sent-111, score-0.686]
</p><p>62 We can prove that, if D(S ) > D(S), the maximum value of D(S ) occurs when S has the maximum allowable D1 density dmax , and S − S has the minimum allowable D1 density dmin : this gives us pdmax + (P − p)dmin = C. [sent-113, score-1.001]
</p><p>63 Thus p = dC−Pdmin and c = dmax p = max −dmin C−Pdmin 1−dmin /dmax . [sent-114, score-0.337]
</p><p>64 First, if we know the minimum population ps,min of a single square s ∈ S, then pmin ≥ k2 ps,min . [sent-118, score-0.325]
</p><p>65 Second, if we know the maximum population ps,max of a single square s ∈ S, then pmin ≥ P − (K 2 − k2 )ps,max . [sent-119, score-0.368]
</p><p>66 At the beginning of our algorithm, we calculate ps,max (S) = max pi j and ps,min (S) = min pi j (where si j ∈ S) for each gridded region S. [sent-120, score-0.675]
</p><p>67 For non-gridded regions, we use the population statistics of the region’s gridded parent (either an odd gridded region or the entire grid G); these bounds will be looser for the child region than for the parent, but are still correct. [sent-123, score-1.588]
</p><p>68 This is done simply by ﬁnding the global maximum ) and minimum values of the D1 density: dmax = max C(S ) (where S ⊆ G and size(S ) = P(S c  kmin ), and dmin = min pii jj (where si j ∈ G). [sent-125, score-0.614]
</p><p>69 3 Alternatively, we could compute dmax and dmin recursively (bottom-up) for each gridded region S, but in practice we ﬁnd that the global values are sufﬁcient for good performance on most test cases. [sent-126, score-1.079]
</p><p>70 3 The algorithm Our algorithm, based on the overlap-multires partitioning scheme above, is a top-down, best-ﬁrst search of the set of gridded regions, followed by a top-down, best-ﬁrst search of any non-gridded regions as necessary. [sent-128, score-0.553]
</p><p>71 highest density) region from a queue, examines it, and (if necessary) adds its children to queues. [sent-131, score-0.352]
</p><p>72 We also assume that regions are “marked” once added to a queue, so that a region will not be searched more than once. [sent-133, score-0.464]
</p><p>73 Finally, we use the rules and density bounds derived above to speed up our search, by pruning subregions when Dmax (S, k) ≤ D(S∗ ). [sent-134, score-0.401]
</p><p>74 We then perform these steps to calculate the mrd of each replica; however, several techniques allow us to reduce the amount of computation necessary for a replica. [sent-145, score-0.351]
</p><p>75 First, we can stop examining a replica G immediately if we ﬁnd a region with density greater than mrd(G). [sent-146, score-0.633]
</p><p>76 Second, we can use mrd(G) for pruning our search on a replica G : if Dmax (S, k) < mrd(G) for some S ⊆ G , we know that no subregion of S of size k or more can have a greater density than the mdr of the original grid, and thus we do not need to examine any of those subregions. [sent-148, score-0.882]
</p><p>77 This is especially useful where there is a signiﬁcant spatial overdensity in G: a high mrd will allow large amounts of pruning on the replica grids. [sent-149, score-0.677]
</p><p>78 We can derive tighter bounds on D max in two ways: by using a closer approximation to the D1 density of S − S , and by using a tighter lower bound on the population of S . [sent-153, score-0.544]
</p><p>79 Thus, if Sdc is contained entirely in the region under consideration S, we would expect that the maximum density subregion S of S is Sdc , and that the disease rate of S − S is equal to the C−c tot −C disease rate outside S: E P−p = Ctot −P = dout . [sent-157, score-1.678]
</p><p>80 Assuming that the D1 density of S − S is P equal to its expected value dout , we obtain the equation pdmax + (P − p)dout = C. [sent-158, score-0.335]
</p><p>81 max −dout The problem with this approach is that we have not compensated for the variance in densities: our calculated value of Dmax is an upper bound for the maximum subregion density D(S ) only in the most approximate probabilistic sense. [sent-161, score-0.561]
</p><p>82 We would expect the D1 density of S − S to be less than its expected value half the time, and thus we would expect D(S ) to be less than Dmax at least half the time; in practice, our bound will be correct more often, since we are still using a conservative approximation of the D1 density of S . [sent-162, score-0.485]
</p><p>83 Note also that we expect to underestimate Dmax if the disease cluster Sdc is not contained entirely in S: this is acceptable (and desirable) since a region not containing Sdc does not need to be expanded. [sent-163, score-0.668]
</p><p>84 From  Then we can compute p by solving  pdmax + (P − p)(dout − bσ) = C, and obtain c = dmax p and Dmax = D(c, p) as before. [sent-168, score-0.381]
</p><p>85 By adjusting our approximation of the minimum density in this manner, we compute a higher score Dmax , reducing the likelihood that we will underestimate the maximum subregion density and prune a region that should not necessarily be pruned. [sent-169, score-1.056]
</p><p>86 For b = 2, there is an 98% chance that we will underestimate D1 (S −S ), giving a guaranteed correct upper bound for the maximum subregion density. [sent-171, score-0.387]
</p><p>87 In practice, the maximum subregion density will be lower than our computed value of Dmax more often, since our estimates for dmax and q are conservative. [sent-172, score-0.848]
</p><p>88 In fact, our experiments demonstrate that b = 1 is sufﬁcient to obtain the correct region with over 90% probability, approaching 100% for sufﬁciently dense regions. [sent-174, score-0.375]
</p><p>89 2 Cached population statistics A ﬁnal step in making the algorithm tractable is to cache certain statistics about the minimum populations of subsquares of gridded regions. [sent-176, score-0.548]
</p><p>90 Finally, the count of each square is chosen randomly from a Poisson distribution with parameter qpi j , where q = q inside the test region and q = q outside the test region. [sent-184, score-0.664]
</p><p>91 The approximate algorithm was tested for grids of size N = 512; test region sizes of k = 16 and k = 4 were used, and the disease rate q was set to . [sent-186, score-0.696]
</p><p>92 Next, we tested accuracy by generating 50 artiﬁcial grids for each population distribution, and computing the percentage of test grids on which the algorithm was able to ﬁnd the correct mdr (see Table 2). [sent-195, score-0.43]
</p><p>93 For the large test region (k = 16), all variants were able to ﬁnd the correct mdr with high (97-100%) accuracy. [sent-196, score-0.544]
</p><p>94 These results demonstrate that the approximate algorithm (with variance adjustment and cached population statistics) is able to achieve high performance and accuracy even for very small test regions and highly non-uniform populations. [sent-198, score-0.351]
</p><p>95 We tested for spatial clustering of “recent” disease cases: the “count” of each square was the number of ED visits in that square in the last two months, and the “population” of that square was the total number of ED visits in that square. [sent-206, score-0.69]
</p><p>96 All three variants (b = 0, 1, 2), as well as the naive algorithm, found the maximum density region (of size 101) and found it statistically signiﬁcant (p-value 0/1000). [sent-209, score-0.689]
</p><p>97 Thus we can see that all three variants ﬁnd the correct region in much less time than  Figure 2: The left picture shows the “population” distribution within Western PA and the right picture shows the “counts” distribution. [sent-214, score-0.404]
</p><p>98 This is very important for applications such as real-time detection of disease outbreaks: if a system is able to detect an outbreak in minutes rather than days, preventive measures or treatments can be administered earlier, possibly saving many lives. [sent-219, score-0.391]
</p><p>99 We are currently applying this algorithm to national-level hospital and pharmacy data, attempting to detect statistically significant indications of a disease outbreak based on changes in the spatial clustering of disease cases. [sent-221, score-0.705]
</p><p>100 Application of a fast partitioning method using the techniques presented here may allow us to achieve the difﬁcult goal of automatic real-time detection of disease outbreaks. [sent-222, score-0.403]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dmax', 0.337), ('mrd', 0.324), ('region', 0.313), ('disease', 0.283), ('gridded', 0.269), ('subregion', 0.265), ('density', 0.203), ('ctot', 0.192), ('mdr', 0.162), ('population', 0.147), ('dmin', 0.128), ('subregions', 0.118), ('tot', 0.118), ('replica', 0.117), ('grid', 0.111), ('spatial', 0.11), ('regions', 0.108), ('sdc', 0.103), ('city', 0.097), ('count', 0.091), ('pmin', 0.09), ('dout', 0.088), ('ptot', 0.088), ('square', 0.088), ('partitioning', 0.086), ('odd', 0.078), ('populations', 0.078), ('kmin', 0.077), ('kulldorff', 0.074), ('overdensity', 0.074), ('scan', 0.072), ('dense', 0.062), ('speedup', 0.061), ('overdensities', 0.059), ('std', 0.058), ('tighter', 0.058), ('naive', 0.055), ('pruning', 0.052), ('counts', 0.052), ('replications', 0.051), ('bound', 0.05), ('statistic', 0.049), ('search', 0.045), ('minutes', 0.045), ('mafia', 0.044), ('pdmax', 0.044), ('qpi', 0.044), ('searched', 0.043), ('contained', 0.043), ('maximum', 0.043), ('cant', 0.04), ('outside', 0.039), ('highest', 0.039), ('size', 0.038), ('variants', 0.037), ('cached', 0.035), ('detection', 0.034), ('signi', 0.034), ('child', 0.033), ('pi', 0.033), ('total', 0.033), ('cance', 0.032), ('test', 0.032), ('grids', 0.03), ('emergency', 0.029), ('lmin', 0.029), ('neill', 0.029), ('outbreak', 0.029), ('pdmin', 0.029), ('pii', 0.029), ('qrep', 0.029), ('quadtree', 0.029), ('reps', 0.029), ('sting', 0.029), ('conservative', 0.029), ('underestimate', 0.029), ('measure', 0.029), ('accuracy', 0.029), ('occurred', 0.028), ('bounds', 0.028), ('adjacent', 0.027), ('poisson', 0.027), ('calculate', 0.027), ('statistics', 0.027), ('dk', 0.027), ('picture', 0.027), ('uniform', 0.027), ('clusters', 0.026), ('geographical', 0.026), ('resolutions', 0.026), ('western', 0.026), ('ci', 0.025), ('inside', 0.025), ('underlying', 0.024), ('latitude', 0.023), ('longitude', 0.023), ('orig', 0.023), ('overlaps', 0.023), ('speedups', 0.023), ('cases', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="6-tfidf-1" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore</p><p>Abstract: Given an N ×N grid of squares, where each square has a count and an underlying population, our goal is to ﬁnd the square region with the highest density, and to calculate its signiﬁcance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff’s spatial scan statistic DK to ﬁnd the most signiﬁcant spatial disease cluster. A naive approach to ﬁnding the maximum density region requires O(N 3 ) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufﬁciently dense regions, this method ﬁnds the maximum density region in optimal O(N 2 ) time, in practice resulting in signiﬁcant (10-200x) speedups. 1</p><p>2 0.060823053 <a title="6-tfidf-2" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><p>3 0.058902886 <a title="6-tfidf-3" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>Author: Joelle Pineau, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems. 1</p><p>4 0.056799337 <a title="6-tfidf-4" href="./nips-2003-Learning_the_k_in_k-means.html">111 nips-2003-Learning the k in k-means</a></p>
<p>Author: Greg Hamerly, Charles Elkan</p><p>Abstract: When clustering a dataset, the right number k of clusters to use is often not obvious, and choosing k automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning k while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs k-means with increasing k in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each k-means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical signiﬁcance level α. We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model’s complexity. 1 Introduction and related work Clustering algorithms are useful tools for data mining, compression, probability density estimation, and many other important tasks. However, most clustering algorithms require the user to specify the number of clusters (called k), and it is not always clear what is the best value for k. Figure 1 shows examples where k has been improperly chosen. Choosing k is often an ad hoc decision based on prior knowledge, assumptions, and practical experience. Choosing k is made more difﬁcult when the data has many dimensions, even when clusters are well-separated. Center-based clustering algorithms (in particular k-means and Gaussian expectationmaximization) usually assume that each cluster adheres to a unimodal distribution, such as Gaussian. With these methods, only one center should be used to model each subset of data that follows a unimodal distribution. If multiple centers are used to describe data drawn from one mode, the centers are a needlessly complex description of the data, and in fact the multiple centers capture the truth about the subset less well than one center. In this paper we present a simple algorithm called G-means that discovers an appropriate k using a statistical test for deciding whether to split a k-means center into two centers. We describe examples and present experimental results that show that the new algorithm 0.9 4 0.8 3 0.7 2 0.6 1 0.5 0 0.4 −1 0.3 −2 −3 0.2 0.1 −0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 −4 −3 −2 −1 0 1 2 3 Figure 1: Two clusterings where k was improperly chosen. Dark crosses are k-means centers. On the left, there are too few centers; ﬁve should be used. On the right, too many centers are used; one center is sufﬁcient for representing the data. In general, one center should be used to represent one Gaussian cluster. is successful. This technique is useful and applicable for many clustering algorithms other than k-means, but here we consider only the k-means algorithm for simplicity. Several algorithms have been proposed previously to determine k automatically. Like our method, most previous methods are wrappers around k-means or some other clustering algorithm for ﬁxed k. Wrapper methods use splitting and/or merging rules for centers to increase or decrease k as the algorithm proceeds. Pelleg and Moore [14] proposed a regularization framework for learning k, which they call X-means. The algorithm searches over many values of k and scores each clustering model using the so-called Bayesian Information Criterion [10]: BIC(C|X) = L(X|C) − p log n 2 where L(X|C) is the log-likelihood of the dataset X according to model C, p = k(d + 1) is the number of parameters in the model C with dimensionality d and k cluster centers, and n is the number of points in the dataset. X-means chooses the model with the best BIC score on the data. Aside from the BIC, other scoring functions are also available. Bischof et al. [1] use a minimum description length (MDL) framework, where the description length is a measure of how well the data are ﬁt by the model. Their algorithm starts with a large value for k and removes centers (reduces k) whenever that choice reduces the description length. Between steps of reducing k, they use the k-means algorithm to optimize the model ﬁt to the data. With hierarchical clustering algorithms, other methods may be employed to determine the best number of clusters. One is to build a merging tree (“dendrogram”) of the data based on a cluster distance metric, and search for areas of the tree that are stable with respect to inter- and intra-cluster distances [9, Section 5.1]. This method of estimating k is best applied with domain-speciﬁc knowledge and human intuition. 2 The Gaussian-means (G-means) algorithm The G-means algorithm starts with a small number of k-means centers, and grows the number of centers. Each iteration of the algorithm splits into two those centers whose data appear not to come from a Gaussian distribution. Between each round of splitting, we run k-means on the entire dataset and all the centers to reﬁne the current solution. We can initialize with just k = 1, or we can choose some larger value of k if we have some prior knowledge about the range of k. G-means repeatedly makes decisions based on a statistical test for the data assigned to each center. If the data currently assigned to a k-means center appear to be Gaussian, then we want to represent that data with only one center. However, if the same data do not appear Algorithm 1 G-means(X, α) 1: Let C be the initial set of centers (usually C ← {¯}). x 2: C ← kmeans(C, X). 3: Let {xi |class(xi ) = j} be the set of datapoints assigned to center cj . 4: Use a statistical test to detect if each {xi |class(xi ) = j} follow a Gaussian distribution (at conﬁdence level α). 5: If the data look Gaussian, keep cj . Otherwise replace cj with two centers. 6: Repeat from step 2 until no more centers are added. to be Gaussian, then we want to use multiple centers to model the data properly. The algorithm will run k-means multiple times (up to k times when ﬁnding k centers), so the time complexity is at most O(k) times that of k-means. The k-means algorithm implicitly assumes that the datapoints in each cluster are spherically distributed around the center. Less restrictively, the Gaussian expectation-maximization algorithm assumes that the datapoints in each cluster have a multidimensional Gaussian distribution with a covariance matrix that may or may not be ﬁxed, or shared. The Gaussian distribution test that we present below are valid for either covariance matrix assumption. The test also accounts for the number of datapoints n tested by incorporating n in the calculation of the critical value of the test (see Equation 2). This prevents the G-means algorithm from making bad decisions about clusters with few datapoints. 2.1 Testing clusters for Gaussian ﬁt To specify the G-means algorithm fully we need a test to detect whether the data assigned to a center are sampled from a Gaussian. The alternative hypotheses are • H0 : The data around the center are sampled from a Gaussian. • H1 : The data around the center are not sampled from a Gaussian. If we accept the null hypothesis H0 , then we believe that the one center is sufﬁcient to model its data, and we should not split the cluster into two sub-clusters. If we reject H0 and accept H1 , then we want to split the cluster. The test we use is based on the Anderson-Darling statistic. This one-dimensional test has been shown empirically to be the most powerful normality test that is based on the empirical cumulative distribution function (ECDF). Given a list of values xi that have been converted to mean 0 and variance 1, let x(i) be the ith ordered value. Let zi = F (x(i) ), where F is the N (0, 1) cumulative distribution function. Then the statistic is A2 (Z) = − 1 n n (2i − 1) [log(zi ) + log(1 − zn+1−i )] − n (1) i=1 Stephens [17] showed that for the case where µ and σ are estimated from the data (as in clustering), we must correct the statistic according to A2 (Z) ∗ = A2 (Z)(1 + 4/n − 25/(n2 )) (2) Given a subset of data X in d dimensions that belongs to center c, the hypothesis test proceeds as follows: 1. Choose a signiﬁcance level α for the test. 2. Initialize two centers, called “children” of c. See the text for good ways to do this. 3. Run k-means on these two centers in X. This can be run to completion, or to some early stopping point if desired. Let c1 , c2 be the child centers chosen by k-means. 4. Let v = c1 − c2 be a d-dimensional vector that connects the two centers. This is the direction that k-means believes to be important for clustering. Then project X onto v: xi = xi , v /||v||2 . X is a 1-dimensional representation of the data projected onto v. Transform X so that it has mean 0 and variance 1. 5. Let zi = F (x(i) ). If A2 (Z) is in the range of non-critical values at conﬁdence ∗ level α, then accept H0 , keep the original center, and discard {c1 , c2 }. Otherwise, reject H0 and keep {c1 , c2 } in place of the original center. A primary contribution of this work is simplifying the test for Gaussian ﬁt by projecting the data to one dimension where the test is simple to apply. The authors of [5] also use this approach for online dimensionality reduction during clustering. The one-dimensional representation of the data allows us to consider only the data along the direction that kmeans has found to be important for separating the data. This is related to the problem of projection pursuit [7], where here k-means searches for a direction in which the data appears non-Gaussian. We must choose the signiﬁcance level of the test, α, which is the desired probability of making a Type I error (i.e. incorrectly rejecting H0 ). It is appropriate to use a Bonferroni adjustment to reduce the chance of making Type I errors over multiple tests. For example, if we want a 0.01 chance of making a Type I error in 100 tests, we should apply a Bonferroni adjustment to make each test use α = 0.01/100 = 0.0001. To ﬁnd k ﬁnal centers the G-means algorithm makes k statistical tests, so the Bonferroni correction does not need to be extreme. In our tests, we always use α = 0.0001. We consider two ways to initialize the two child centers. Both approaches initialize with c ± m, where c is a center and m is chosen. The ﬁrst method chooses m as a random d-dimensional vector such that ||m|| is small compared to the distortion of the data. A second method ﬁnds the main principal component s of the data (having eigenvalue λ), and chooses m = s 2λ/π. This deterministic method places the two centers in their expected locations under H0 . The principal component calculations require O(nd2 + d3 ) time and O(d2 ) space, but since we only want the main principal component, we can use fast methods like the power method, which takes time that is at most linear in the ratio of the two largest eigenvalues [4]. In this paper we use principal-component-based splitting. 2.2 An example Figure 2 shows a run of the G-means algorithm on a synthetic dataset with two true clusters and 1000 points, using α = 0.0001. The critical value for the Anderson-Darling test is 1.8692 for this conﬁdence level. Starting with one center, after one iteration of G-means, we have 2 centers and the A2 statistic is 38.103. This is much larger than the critical value, ∗ so we reject H0 and accept this split. On the next iteration, we split each new center and repeat the statistical test. The A2 values for the two splits are 0.386 and 0.496, both of ∗ which are well below the critical value. Therefore we accept H0 for both tests, and discard these splits. Thus G-means gives a ﬁnal answer of k = 2. 2.3 Statistical power Figure 3 shows the power of the Anderson-Darling test, as compared to the BIC. Lower is better for both plots. We run 1000 tests for each data point plotted for both plots. In the left 14 14 14 13 13 13 12 12 12 11 11 11 10 10 10 9 9 9 8 8 8 7 7 7 6 6 6 5 5 4 4 0 2 4 6 8 10 12 5 4 0 2 4 6 8 10 12 0 2 4 6 8 10 12 Figure 2: An example of running G-means for three iterations on a 2-dimensional dataset with two true clusters and 1000 points. Starting with one center (left plot), G-means splits into two centers (middle). The test for normality is signiﬁcant, so G-means rejects H0 and keeps the split. After splitting each center again (right), the test values are not signiﬁcant, so G-means accepts H0 for both tests and does not accept these splits. The middle plot is the G-means answer. See the text for further details. 1 1 G-means X-means 0.8 P(Type II error) P(Type I error) 0.8 G-means X-means 0.6 0.4 0.2 0.6 0.4 0.2 0 0 0 30 60 90 120 150 number of datapoints 180 210 0 30 60 90 120 150 number of datapoints 180 210 Figure 3: A comparison of the power of the Anderson-Darling test versus the BIC. For the AD test we ﬁx the signiﬁcance level (α = 0.0001), while the BIC’s signiﬁcance level depends on n. The left plot shows the probability of incorrectly splitting (Type I error) one true 2-d cluster that is 5% elliptical. The right plot shows the probability of incorrectly not splitting two true clusters separated by 5σ (Type II error). Both plots are functions of n. Both plots show that the BIC overﬁts (splits clusters) when n is small. plot, for each test we generate n datapoints from a single true Gaussian distribution, and then plot the frequency with which BIC and G-means will choose k = 2 rather than k = 1 (i.e. commit a Type I error). BIC tends to overﬁt by choosing too many centers when the data is not strictly spherical, while G-means does not. This is consistent with the tests of real-world data in the next section. While G-means commits more Type II errors when n is small, this prevents it from overﬁtting the data. The BIC can be considered a likelihood ratio test, but with a signiﬁcance level that cannot be ﬁxed. The signiﬁcance level instead varies depending on n and ∆k (the change in the number of model parameters between two models). As n or ∆k decrease, the signiﬁcance level increases (the BIC becomes weaker as a statistical test) [10]. Figure 3 shows this effect for varying n. In [11] the authors show that penalty-based methods require problemspeciﬁc tuning and don’t generalize as well as other methods, such as cross validation. 3 Experiments Table 1 shows the results from running G-means and X-means on many large synthetic. On synthetic datasets with spherically distributed clusters, G-means and X-means do equally Table 1: Results for many synthetic datasets. We report distortion relative to the optimum distortion for the correct clustering (closer to one is better), and time is reported relative to k-means run with the correct k. For BIC, larger values are better, but it is clear that ﬁnding the correct clustering does not always coincide with ﬁnding a larger BIC. Items with a star are where X-means always chose the largest number of centers we allowed. dataset synthetic k=5 synthetic k=20 synthetic k=80 synthetic k=5 synthetic k=20 synthetic k=80 synthetic k=5 synthetic k=20 synthetic k=80 d 2 k found 9.1± 9.9 18.1± 3.2 20.1± 0.6 70.5±11.6 80.0± 0.2 171.7±23.7 5.0± 0.0 *20.0± 0.0 20.0± 0.1 *80.0± 0.0 80.2± 0.5 229.2±36.8 5.0± 0.0 *20.0± 0.0 20.0± 0.0 *80.0± 0.0 80.0± 0.0 171.5±10.9 method G-means X-means G-means X-means G-means X-means G-means X-means G-means X-means G-means X-means G-means X-means G-means X-means G-means X-means 2 2 8 8 8 32 32 32 BIC(×104 ) -0.19±2.70 0.70±0.93 0.21±0.18 14.83±3.50 1.84±0.12 40.16±6.59 -0.74±0.16 -2.28±0.20 -0.18±0.17 14.36±0.21 1.45±0.20 52.28±9.26 -3.36±0.21 -27.92±0.22 -2.73±0.22 -11.13±0.23 -1.10±0.16 11.78±2.74 distortion(× optimal) 0.89± 0.23 0.37± 0.12 0.99± 0.01 9.45±28.02 1.00± 0.01 48.49±70.04 1.00± 0.00 0.47± 0.03 0.99± 0.00 0.47± 0.01 0.99± 0.00 0.57± 0.06 1.00± 0.00 0.76± 0.00 1.00± 0.00 0.76± 0.01 1.00± 0.00 0.84± 0.01 7 7 6 6 5 5 4 4 3 3 2 2 1 time(× k-means) 13.2 2.8 2.1 1.2 2.2 1.8 4.6 11.0 2.6 4.0 2.9 6.5 4.4 29.9 2.3 21.2 2.8 53.3 1 0 0 2 4 6 8 10 12 0 0 2 4 6 8 10 12 Figure 4: 2-d synthetic dataset with 5 true clusters. On the left, G-means correctly chooses 5 centers and deals well with non-spherical data. On the right, the BIC causes X-means to overﬁt the data, choosing 20 unevenly distributed clusters. well at ﬁnding the correct k and maximizing the BIC statistic, so we don’t show these results here. Most real-world data is not spherical, however. The synthetic datasets used here each have 5000 datapoints in d = 2/8/32 dimensions. The true ks are 5, 20, and 80. For each synthetic dataset type, we generate 30 datasets with the true center means chosen uniformly randomly from the unit hypercube, and choosing σ so that no two clusters are closer than 3σ apart. Each cluster is also given a transformation to make it non-spherical, by multiplying the data by a randomly chosen scaling and rotation matrix. We run G-means starting with one center. We allow X-means to search between 2 and 4k centers (where here k is the true number of clusters). The G-means algorithm clearly does better at ﬁnding the correct k on non-spherical data. Its results are closer to the true distortions and the correct ks. The BIC statistic that X-means uses has been formulated to maximize the likelihood for spherically-distributed data. Thus it overestimates the number of true clusters in non-spherical data. This is especially evident when the number of points per cluster is small, as in datasets with 80 true clusters. 1 2 2 3 3 4 4 Digit 0 1 Digit 0 5 5 6 6 7 7 8 8 9 9 5 10 15 20 25 30 Cluster 10 20 30 40 50 60 Cluster Figure 5: NIST and Pendigits datasets: correspondence between each digit (row) and each cluster (column) found by G-means. G-means did not have the labels, yet it found meaningful clusters corresponding with the labels. Because of this overestimation, X-means often hits our limit of 4k centers. Figure 4 shows an example of overﬁtting on a dataset with 5 true clusters. X-means chooses k = 20 while G-means ﬁnds all 5 true cluster centers. Also of note is that X-means does not distribute centers evenly among clusters; some clusters receive one center, but others receive many. G-means runs faster than X-means for 8 and 32 dimensions, which we expect, since the kd-tree structures which make X-means fast in low dimensions take time exponential in d, making them slow for more than 8 to 12 dimensions. All our code is written in Matlab; X-means is written in C. 3.1 Discovering true clusters in labeled data We tested these algorithms on two real-world datasets for handwritten digit recognition: the NIST dataset [12] and the Pendigits dataset [2]. The goal is to cluster the data without knowledge of the labels and measure how well the clustering captures the true labels. Both datasets have 10 true classes (digits 0-9). NIST has 60000 training examples and 784 dimensions (28×28 pixels). We use 6000 randomly chosen examples and we reduce the dimension to 50 by random projection (following [3]). The Pendigits dataset has 7984 examples and 16 dimensions; we did not change the data in any way. We cluster each dataset with G-means and X-means, and measure performance by comparing the cluster labels Lc with the true labels Lt . We deﬁne the partition quality (PQ) as kt kc kt 2 2 pq = i=1 j=1 p(i, j) i=1 p(i) where kt is the true number of classes, and kc is the number of clusters found by the algorithm. This metric is maximized when Lc induces the same partition of the data as Lt ; in other words, when all points in each cluster have the same true label, and the estimated k is the true k. The p(i, j) term is the frequency-based probability that a datapoint will be labeled i by Lt and j by Lc . This quality is normalized by the sum of true probabilities, squared. This statistic is related to the Rand statistic for comparing partitions [8]. For the NIST dataset, G-means ﬁnds 31 clusters in 30 seconds with a PQ score of 0.177. X-means ﬁnds 715 clusters in 4149 seconds, and 369 of these clusters contain only one point, indicating an overestimation problem with the BIC. X-means receives a PQ score of 0.024. For the Pendigits dataset, G-means ﬁnds 69 clusters in 30 seconds, with a PQ score of 0.196; X-means ﬁnds 235 clusters in 287 seconds, with a PQ score of 0.057. Figure 5 shows Hinton diagrams of the G-means clusterings of both datasets, showing that G-means succeeds at identifying the true clusters concisely, without aid of the labels. The confusions between different digits in the NIST dataset (seen in the off-diagonal elements) are common for other researchers using more sophisticated techniques, see [3]. 4 Discussion and conclusions We have introduced the new G-means algorithm for learning k based on a statistical test for determining whether datapoints are a random sample from a Gaussian distribution with arbitrary dimension and covariance matrix. The splitting uses dimension reduction and a powerful test for Gaussian ﬁtness. G-means uses this statistical test as a wrapper around k-means to discover the number of clusters automatically. The only parameter supplied to the algorithm is the signiﬁcance level of the statistical test, which can easily be set in a standard way. The G-means algorithm takes linear time and space (plus the cost of the splitting heuristic and test) in the number of datapoints and dimension, since k-means is itself linear in time and space. Empirically, the G-means algorithm works well at ﬁnding the correct number of clusters and the locations of genuine cluster centers, and we have shown it works well in moderately high dimensions. Clustering in high dimensions has been an open problem for many years. Recent research has shown that it may be preferable to use dimensionality reduction techniques before clustering, and then use a low-dimensional clustering algorithm such as k-means, rather than clustering in the high dimension directly. In [3] the author shows that using a simple, inexpensive linear projection preserves many of the properties of data (such as cluster distances), while making it easier to ﬁnd the clusters. Thus there is a need for good-quality, fast clustering algorithms for low-dimensional data. Our work is a step in this direction. Additionally, recent image segmentation algorithms such as normalized cut [16, 13] are based on eigenvector computations on distance matrices. These “spectral” clustering algorithms still use k-means as a post-processing step to ﬁnd the actual segmentation and they require k to be speciﬁed. Thus we expect G-means will be useful in combination with spectral clustering. References [1] Horst Bischof, Aleˇ Leonardis, and Alexander Selb. MDL principle for robust vector quantisation. Pattern analysis and applications, 2:59–72, s 1999. [2] C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html. [3] Sanjoy Dasgupta. Experiments with random projection. In Uncertainty in Artiﬁcial Intelligence: Proceedings of the Sixteenth Conference (UAI-2000), pages 143–151, San Francisco, CA, 2000. Morgan Kaufmann Publishers. [4] Gianna M. Del Corso. Estimating an eigenvector by the power method with a random start. SIAM Journal on Matrix Analysis and Applications, 18(4):913–937, 1997. [5] Chris Ding, Xiaofeng He, Hongyuan Zha, and Horst Simon. Adaptive dimension reduction for clustering high dimensional data. In Proceedings of the 2nd IEEE International Conference on Data Mining, 2002. [6] Fredrik Farnstrom, James Lewis, and Charles Elkan. Scalability for clustering algorithms revisited. SIGKDD Explorations, 2(1):51–57, 2000. [7] Peter J. Huber. Projection pursuit. Annals of Statistics, 13(2):435–475, June 1985. [8] L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation, 2:193–218, 1985. [9] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Computing Surveys, 31(3):264–323, 1999. [10] Robert E. Kass and Larry Wasserman. A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion. Journal of the American Statistical Association, 90(431):928–934, 1995. [11] Michael J. Kearns, Yishay Mansour, Andrew Y. Ng, and Dana Ron. An experimental and theoretical comparison of model selection methods. In Computational Learing Theory (COLT), pages 21–30, 1995. [12] Yann LeCun, L´ on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the e IEEE, 86(11):2278–2324, 1998. [13] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Neural Information Processing Systems, 14, 2002. [14] Dan Pelleg and Andrew Moore. X-means: Extending K-means with efﬁcient estimation of the number of clusters. In Proceedings of the 17th International Conf. on Machine Learning, pages 727–734. Morgan Kaufmann, San Francisco, CA, 2000. [15] Peter Sand and Andrew Moore. Repairing faulty mixture models using density estimation. In Proceedings of the 18th International Conf. on Machine Learning. Morgan Kaufmann, San Francisco, CA, 2001. [16] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000. [17] M. A. Stephens. EDF statistics for goodness of ﬁt and some comparisons. American Statistical Association, 69(347):730–737, September 1974.</p><p>5 0.05384415 <a title="6-tfidf-5" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>Author: Victor Lavrenko, R. Manmatha, Jiwoon Jeon</p><p>Abstract: We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model signiﬁcantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval. 1</p><p>6 0.051609006 <a title="6-tfidf-6" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>7 0.050082523 <a title="6-tfidf-7" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>8 0.049974255 <a title="6-tfidf-8" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>9 0.049802605 <a title="6-tfidf-9" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>10 0.048126373 <a title="6-tfidf-10" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>11 0.047941394 <a title="6-tfidf-11" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>12 0.047010351 <a title="6-tfidf-12" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>13 0.045832116 <a title="6-tfidf-13" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>14 0.042988978 <a title="6-tfidf-14" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>15 0.042885143 <a title="6-tfidf-15" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>16 0.042170946 <a title="6-tfidf-16" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>17 0.042069536 <a title="6-tfidf-17" href="./nips-2003-Gene_Expression_Clustering_with_Functional_Mixture_Models.html">79 nips-2003-Gene Expression Clustering with Functional Mixture Models</a></p>
<p>18 0.0379005 <a title="6-tfidf-18" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>19 0.036779974 <a title="6-tfidf-19" href="./nips-2003-Clustering_with_the_Connectivity_Kernel.html">46 nips-2003-Clustering with the Connectivity Kernel</a></p>
<p>20 0.035874005 <a title="6-tfidf-20" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.135), (1, -0.011), (2, -0.001), (3, 0.03), (4, -0.031), (5, -0.014), (6, 0.007), (7, 0.008), (8, 0.006), (9, 0.05), (10, -0.034), (11, -0.021), (12, -0.002), (13, 0.031), (14, 0.059), (15, 0.003), (16, -0.015), (17, 0.019), (18, 0.103), (19, 0.041), (20, -0.037), (21, -0.065), (22, -0.04), (23, -0.068), (24, 0.025), (25, 0.02), (26, -0.019), (27, 0.086), (28, 0.056), (29, -0.002), (30, -0.062), (31, -0.028), (32, 0.041), (33, 0.014), (34, -0.006), (35, -0.019), (36, -0.014), (37, -0.163), (38, 0.033), (39, 0.115), (40, -0.044), (41, -0.006), (42, 0.056), (43, 0.073), (44, -0.126), (45, 0.025), (46, -0.143), (47, 0.069), (48, 0.01), (49, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96161819 <a title="6-lsi-1" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore</p><p>Abstract: Given an N ×N grid of squares, where each square has a count and an underlying population, our goal is to ﬁnd the square region with the highest density, and to calculate its signiﬁcance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff’s spatial scan statistic DK to ﬁnd the most signiﬁcant spatial disease cluster. A naive approach to ﬁnding the maximum density region requires O(N 3 ) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufﬁciently dense regions, this method ﬁnds the maximum density region in optimal O(N 2 ) time, in practice resulting in signiﬁcant (10-200x) speedups. 1</p><p>2 0.63532406 <a title="6-lsi-2" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>Author: Max Welling, Andriy Mnih, Geoffrey E. Hinton</p><p>Abstract: In models that deﬁne probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model’s distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate.</p><p>3 0.54733777 <a title="6-lsi-3" href="./nips-2003-An_MCMC-Based_Method_of_Comparing_Connectionist_Models_in_Cognitive_Science.html">25 nips-2003-An MCMC-Based Method of Comparing Connectionist Models in Cognitive Science</a></p>
<p>Author: Woojae Kim, Daniel J. Navarro, Mark A. Pitt, In J. Myung</p><p>Abstract: Despite the popularity of connectionist models in cognitive science, their performance can often be diﬃcult to evaluate. Inspired by the geometric approach to statistical model selection, we introduce a conceptually similar method to examine the global behavior of a connectionist model, by counting the number and types of response patterns it can simulate. The Markov Chain Monte Carlo-based algorithm that we constructed Þnds these patterns eﬃciently. We demonstrate the approach using two localist network models of speech perception. 1</p><p>4 0.51960224 <a title="6-lsi-4" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>Author: Alice X. Zheng, Michael I. Jordan, Ben Liblit, Alex Aiken</p><p>Abstract: We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately deﬁned utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs. 1</p><p>5 0.49944693 <a title="6-lsi-5" href="./nips-2003-ARA%2A%3A_Anytime_A%2A_with_Provable_Bounds_on_Sub-Optimality.html">2 nips-2003-ARA*: Anytime A* with Provable Bounds on Sub-Optimality</a></p>
<p>Author: Maxim Likhachev, Geoffrey J. Gordon, Sebastian Thrun</p><p>Abstract: In real world planning problems, time for deliberation is often limited. Anytime planners are well suited for these problems: they ﬁnd a feasible solution quickly and then continually work on improving it until time runs out. In this paper we propose an anytime heuristic search, ARA*, which tunes its performance bound based on available search time. It starts by ﬁnding a suboptimal solution quickly using a loose bound, then tightens the bound progressively as time allows. Given enough time it ﬁnds a provably optimal solution. While improving its bound, ARA* reuses previous search efforts and, as a result, is signiﬁcantly more efﬁcient than other anytime search methods. In addition to our theoretical analysis, we demonstrate the practical utility of ARA* with experiments on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover. 1</p><p>6 0.4828057 <a title="6-lsi-6" href="./nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</a></p>
<p>7 0.46225852 <a title="6-lsi-7" href="./nips-2003-An_Autonomous_Robotic_System_for_Mapping_Abandoned_Mines.html">21 nips-2003-An Autonomous Robotic System for Mapping Abandoned Mines</a></p>
<p>8 0.40675327 <a title="6-lsi-8" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>9 0.40645766 <a title="6-lsi-9" href="./nips-2003-Human_and_Ideal_Observers_for_Detecting_Image_Curves.html">85 nips-2003-Human and Ideal Observers for Detecting Image Curves</a></p>
<p>10 0.39512703 <a title="6-lsi-10" href="./nips-2003-Markov_Models_for_Automated_ECG_Interval_Analysis.html">123 nips-2003-Markov Models for Automated ECG Interval Analysis</a></p>
<p>11 0.39072937 <a title="6-lsi-11" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>12 0.37281621 <a title="6-lsi-12" href="./nips-2003-Training_a_Quantum_Neural_Network.html">187 nips-2003-Training a Quantum Neural Network</a></p>
<p>13 0.36147693 <a title="6-lsi-13" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>14 0.35463563 <a title="6-lsi-14" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>15 0.34878933 <a title="6-lsi-15" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>16 0.34685418 <a title="6-lsi-16" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>17 0.3457059 <a title="6-lsi-17" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>18 0.34322914 <a title="6-lsi-18" href="./nips-2003-Near-Minimax_Optimal_Classification_with_Dyadic_Classification_Trees.html">134 nips-2003-Near-Minimax Optimal Classification with Dyadic Classification Trees</a></p>
<p>19 0.33117485 <a title="6-lsi-19" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>20 0.32418236 <a title="6-lsi-20" href="./nips-2003-Auction_Mechanism_Design_for_Multi-Robot_Coordination.html">36 nips-2003-Auction Mechanism Design for Multi-Robot Coordination</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.073), (10, 0.372), (11, 0.013), (29, 0.011), (30, 0.016), (35, 0.066), (53, 0.09), (66, 0.011), (71, 0.041), (76, 0.056), (82, 0.013), (85, 0.074), (91, 0.065), (99, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74380815 <a title="6-lda-1" href="./nips-2003-A_Fast_Multi-Resolution_Method_for_Detection_of_Significant_Spatial_Disease_Clusters.html">6 nips-2003-A Fast Multi-Resolution Method for Detection of Significant Spatial Disease Clusters</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore</p><p>Abstract: Given an N ×N grid of squares, where each square has a count and an underlying population, our goal is to ﬁnd the square region with the highest density, and to calculate its signiﬁcance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff’s spatial scan statistic DK to ﬁnd the most signiﬁcant spatial disease cluster. A naive approach to ﬁnding the maximum density region requires O(N 3 ) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufﬁciently dense regions, this method ﬁnds the maximum density region in optimal O(N 2 ) time, in practice resulting in signiﬁcant (10-200x) speedups. 1</p><p>2 0.41983569 <a title="6-lda-2" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><p>3 0.41376695 <a title="6-lda-3" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>4 0.410707 <a title="6-lda-4" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>Author: Yuan Qi, Tom Minka</p><p>Abstract: Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a “message” to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms. 1</p><p>5 0.40994591 <a title="6-lda-5" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><p>6 0.40842974 <a title="6-lda-6" href="./nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</a></p>
<p>7 0.40774012 <a title="6-lda-7" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>8 0.4071995 <a title="6-lda-8" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>9 0.4053646 <a title="6-lda-9" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>10 0.40488976 <a title="6-lda-10" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>11 0.40465179 <a title="6-lda-11" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>12 0.40413883 <a title="6-lda-12" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>13 0.40336955 <a title="6-lda-13" href="./nips-2003-Margin_Maximizing_Loss_Functions.html">122 nips-2003-Margin Maximizing Loss Functions</a></p>
<p>14 0.40264621 <a title="6-lda-14" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>15 0.40139928 <a title="6-lda-15" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>16 0.4012337 <a title="6-lda-16" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>17 0.40047252 <a title="6-lda-17" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>18 0.40044945 <a title="6-lda-18" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>19 0.40040666 <a title="6-lda-19" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>20 0.4003965 <a title="6-lda-20" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
