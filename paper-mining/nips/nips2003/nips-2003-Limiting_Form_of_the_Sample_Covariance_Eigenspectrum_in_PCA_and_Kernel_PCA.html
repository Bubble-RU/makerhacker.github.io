<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-114" href="#">nips2003-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</h1>
<br/><p>Source: <a title="nips-2003-114-pdf" href="http://papers.nips.cc/paper/2501-limiting-form-of-the-sample-covariance-eigenspectrum-in-pca-and-kernel-pca.pdf">pdf</a></p><p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><p>Reference: <a title="nips-2003-114-reference" href="../nips2003_reference/nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Limiting form of the sample covariance eigenspectrum in PCA and kernel PCA  David C. [sent-1, score-0.446]
</p><p>2 uk  Abstract We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. [sent-10, score-1.018]
</p><p>3 The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. [sent-12, score-0.512]
</p><p>4 Results are derived in the limit of large data dimension while keeping α ﬁxed. [sent-13, score-0.207]
</p><p>5 As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. [sent-14, score-0.303]
</p><p>6 For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. [sent-15, score-0.669]
</p><p>7 The principal components are eigenvectors of the sample covariance matrix ordered according to the size of the corresponding eigenvalues. [sent-20, score-0.453]
</p><p>8 Most methods for model selection require only the eigenvalue spectrum of the sample covariance matrix. [sent-22, score-0.95]
</p><p>9 It is therefore useful to understand how the sample covariance spectrum behaves given a particular data distribution. [sent-23, score-0.742]
</p><p>10 Much is known about the asymptotic properties of the spectrum in the case where the data distribution is isotropic, e. [sent-24, score-0.466]
</p><p>11 However, it is also instructive to consider the limiting behaviour in the case where the data does contain some low-dimensional structure. [sent-29, score-0.264]
</p><p>12 This is interesting as it allows us to understand the limits of learnability and previous studies have already shown phase-transition behaviour in PCA learning from data containing a single symmetry-breaking direction [3]. [sent-30, score-0.272]
</p><p>13 The analysis of  data models which include a signal component are also useful if we are to correct for bias in the estimated eigenvalues corresponding to retained components. [sent-31, score-0.541]
</p><p>14 A promising nonlinear alternative is kernel PCA [4] in which data is projected into a high-dimensional feature space and PCA is carried out in this feature space. [sent-33, score-0.36]
</p><p>15 The kernel trick allows all computations to be carried out efﬁciently so that the method is practical even when the feature space has a very high, or even inﬁnite, dimension. [sent-34, score-0.22]
</p><p>16 In this case we are interested in properties of the eigenvalue spectrum of the sample covariance matrix in feature space. [sent-35, score-1.053]
</p><p>17 The covariance of the features will typically be non-isotropic even when the data itself has independently distributed components with equal variance. [sent-36, score-0.427]
</p><p>18 The sample covariance spectrum will therefore show quite rich behaviour even when the data itself has no structure. [sent-37, score-0.807]
</p><p>19 It is important to understand the expected behaviour in order to develop model selection methods for kernel PCA analogous to those used for standard PCA. [sent-38, score-0.323]
</p><p>20 Model selection methods based on data models with isotropic noise (e. [sent-39, score-0.268]
</p><p>21 In this paper we apply methods from statistical mechanics and random matrix theory to determine the limiting form of eigenvalue spectrum for sample covariance matrices produced from data containing symmetry-breaking structure. [sent-42, score-1.303]
</p><p>22 We ﬁrst show how the replica method can be used to derive the spectrum for Gaussian data with a ﬁnite number a symmetrybreaking directions. [sent-43, score-0.584]
</p><p>23 This result is conﬁrmed and generalised by studying the Stieltjes transform of the eigenvalue spectrum, suggesting that it may be insensitive to details of the data distribution. [sent-44, score-0.456]
</p><p>24 We then show how the results can be used to derive the limiting form of eigenvalue spectrum of the feature covariance matrix (or Gram matrix) in kernel PCA for the case of a polynomial kernel. [sent-45, score-1.163]
</p><p>25 2  Statistical mechanics theory for Gaussian data  We ﬁrst consider a data set of N -dimensional data vectors {xµ }p containing a signal µ=1 and noise component. [sent-46, score-0.401]
</p><p>26 Initially we restrict ourselves to the case where xµ is drawn from a Gaussian distribution whose covariance matrix C is isotropic except for a small number of orthogonal symmetry-breaking directions, i. [sent-47, score-0.428]
</p><p>27 (1)  m=1  ˆ We deﬁne the sample covariance C = p−1 µ xµ xT and study its eigenvalue spectrum µ in the limit N → ∞ when the ratio α = p/N is held ﬁxed and the number of symmetryˆ breaking directions S is ﬁnite. [sent-50, score-1.067]
</p><p>28 We work with the trace of the resolvent G(λ) = (λI − C)−1 from which the density of eigenvalues ρ(λ) can be calculated, N  ρ(λ) = lim (N π)−1 Im trG(λ − i ) +  where  trG(λ) =  →0  i=1  1 λ − λi  ˆ and λi are eigenvalues of C. [sent-51, score-0.835]
</p><p>29 (4)  We assume that the eigenvalue spectrum is self-averaging, so that the calculation for a speciﬁc realisation of the sample covariance can be replaced by an ensemble average for large N that can be performed using the replica method (see e. [sent-54, score-1.086]
</p><p>30 (6) αA The ﬁrst term in equation (5) sets a proportion 1 − α eigenvalues to zero when the rank of ˆ C is less than N , i. [sent-62, score-0.42]
</p><p>31 The last term represents the bulk of the spectrum and is identical to the well-known Marˇ enko-Pastur law for isotropic data with variance σ 2 [8, 9]. [sent-65, score-0.802]
</p><p>32 The mth symmetry-breaking term in the data covariance C only contributes to the spectrum if α > A−2 . [sent-68, score-0.582]
</p><p>33 Above this transition the sample covariance eigenvalue over-estimates the true variance corresponding to this component by a factor 1 + 1/(αAm ) which indicates a signiﬁcant bias when the data set is small or the signal is relatively weak. [sent-73, score-0.905]
</p><p>34 Our result provides a method of bias correction for the top eigenvalues in this case. [sent-74, score-0.52]
</p><p>35 On the left we show how the top eigenvalues separate from the bulk while the inset compares the density of the bulk with the theoretical result, showing excellent agreement. [sent-76, score-1.036]
</p><p>36 On the right we show convergence to the theoretical result for λu (A, σ 2 ) in equation (6) as the data dimension N is increased for ﬁxed α. [sent-77, score-0.289]
</p><p>37 3  Analysis of the Stieltjes transform  The statistical mechanics approach is useful because it allows the derivation of results from ﬁrst principles and it is possible to use this method to determine other self-averaging quantities of interest, e. [sent-78, score-0.198]
</p><p>38 Marˇ enko and Pastur [8] studied the case of data with a general covariance matrix. [sent-84, score-0.417]
</p><p>39 25  0  8  1  2  10  3 4 Eigenvalue  12  14  5  16  6  18  -3  (b)  -4  -5  20  -6  5  7  6  8  Log N  Index  Figure 1: In (a) we show eigenvalues of the sample covariance matrix for Gaussian data with σ 2 = 1, N = 2000 and α = 0. [sent-92, score-0.753]
</p><p>40 The data contains three symmetry-breaking directions with strengths A2 = 20, A2 = 15 and A2 = 10 all above the transition point. [sent-94, score-0.203]
</p><p>41 The 3 2 1 inset shows the distribution of all non-zero eigenvalues except for the largest three with the solid line showing the theoretical result. [sent-95, score-0.554]
</p><p>42 In (b) we show the fractional difference between the three largest eigenvalues λi and the theoretical value λu (Ai , σ 2 ) for i = 1, 2, 3. [sent-96, score-0.445]
</p><p>43 The above equation is therefore exactly equivalent to equation (2) and we see that this approach starts from the same point as the statistical mechanics theory. [sent-100, score-0.215]
</p><p>44 Marˇ enko and Pastur showed that c the Stieltjes transform satisﬁes the following relationship, z(mρ ) = −  1 + α−1 mρ  dH(t) . [sent-101, score-0.182]
</p><p>45 + mρ  t−1  (8)  The measure H(t) is deﬁned such that N −1 i dk converges to tk dH(t) ∀k where i di are the eigenvalues of C. [sent-102, score-0.338]
</p><p>46 An equivalent result is also derived by Wachter [10] and more recently by Sengupta and Mitra using the replica method [11] (for Gaussian data). [sent-103, score-0.212]
</p><p>47 Silverstein and Choi have shown that the support of ρ(λ) can be determined by the intervals between extrema of z(mρ ) [12] and this has been used to determine the signal component of a spectrum when O(N ) equal strength symmetry-breaking directions are present [13]. [sent-104, score-0.548]
</p><p>48 Thus, in this limit the eigenvalue density would appear to be identical to the isotropic case. [sent-106, score-0.636]
</p><p>49 However, it is the behaviour of the largest eigenvalues that we are most interested in, even though these may have vanishing measure. [sent-107, score-0.438]
</p><p>50 Again this is in agreement with our previous replica analysis of the resolvent. [sent-120, score-0.196]
</p><p>51 [14]) while Marˇ enko and Pastur show that the data vector components do not have c to be independently distributed for the relation to hold and they give sufﬁcient conditions on the 4th order cross-moments of the data vector components [8]. [sent-128, score-0.444]
</p><p>52 In [7] we study PCA on some examples of non-Gaussian data with symmetry-breaking structure (non-Gaussian signal and noise) and show that the separated eigenvalues behave similarly to ﬁgure 1. [sent-129, score-0.538]
</p><p>53 4  Eigenvalue spectra for kernel PCA  Equation (8) holds under quite weak conditions on the data distribution. [sent-130, score-0.231]
</p><p>54 It is therefore hoped that we can apply these results to the feature space of kernel PCA [4]. [sent-131, score-0.211]
</p><p>55 In kernel PCA the data x is transformed into a feature vector φ(x) and standard PCA is carried out in the feature space. [sent-132, score-0.36]
</p><p>56 The eigenvalues of the sample covariance in feature space are identical to eigenvalues of the Gram matrix Kµν with entries k(xµ , xν ) and the eigenvalues can therefore be computed efﬁciently for arbitrary feature-space dimension as long as the number of samples p is not too large (NB. [sent-134, score-1.583]
</p><p>57 The Gram matrix ﬁrst has to be centred [4] so that the data has zero mean in the feature space). [sent-135, score-0.242]
</p><p>58 One common choice of kernel function is the polynomial kernel k(x, y) = (c + x · y) d in which case, for integer d, the features are all possible monomials up to order d involving components of x. [sent-136, score-0.273]
</p><p>59 We limit our attention here to the quadratic kernel (d = 2). [sent-137, score-0.193]
</p><p>60 We consider data vectors with components that are independently and symmetrically distributed √ with equal variance σ 2 and choose a set of features φ(x) = ( 2cx, Vec[xxT ]) where Vec[xxT ]j+N (i−1) = xi xj . [sent-138, score-0.271]
</p><p>61 The covariance in feature space is block diagonal, 2c xxT C= 0    0  Vec[xxT ]Vec[xxT ]T − Vec[xxT ] Vec[xxT ]T  di 2cσ 2 2σ 4 2σ 4 + κi 4  number N N (N − 1)/2 N  where angled brackets denote expectations over the data distribution. [sent-139, score-0.337]
</p><p>62 The non-zero eigenvalues of C are shown on the right where κi = x4 − 3σ 4 is the 4th cumulant of the 4 i ith component of x. [sent-140, score-0.38]
</p><p>63 We see that although each component of the data is independently distributed with equal variance, the covariance structure in feature space may be quite complex. [sent-141, score-0.484]
</p><p>64 05 6 0  0  1  2  3  4  Eigenvalue  5  6  7  0  10000  20000  p  30000  40000  Figure 2: On the left we show the Gram matrix eigenspectrum for a sample data set and compare it to the theoretical result. [sent-149, score-0.325]
</p><p>65 The kernel is purely quadratic (c = 0) and we use isotropic Gaussian data with 2σ 4 = 1, N = 63 and p = 1000 so that α 0. [sent-150, score-0.416]
</p><p>66 On the right we show the averaged top eigenvalue against p for ﬁxed α. [sent-152, score-0.459]
</p><p>67 On the left of ﬁgure 2 we compare the spectra for a single sample data set to the theory for p = 1000 and N = 63 which corresponds to α 0. [sent-159, score-0.257]
</p><p>68 50 and the theoretical curve is almost identical to the one used in the inset to ﬁgure 1(a). [sent-160, score-0.222]
</p><p>69 The ﬁnite size effects are much larger than would be observed for PCA with isotropic data and on the right of ﬁgure 2 we show the average of the top eigenvalue for this value of α as p is increased, showing a very slow convergence to the asymptotic result. [sent-161, score-0.761]
</p><p>70 • Gaussian data, c > 0  For isotropic Gaussian data and c > 0 there are two eigenvalues of C with degeneracy N and N (N + 1)/2 respectively. [sent-162, score-0.615]
</p><p>71 For large N and c > σ 2 the top N eigenvalues play an analogous role to the top S eigenvalues in the PCA data model deﬁned in section 2. [sent-163, score-0.993]
</p><p>72 A similar perturbative expansion to the one described in section 3 shows that when α < (c/σ 2 − 1)−2 (where α 2p/N 2 is deﬁned relative to the feature space) the distribution is identical to the c = 0 case. [sent-164, score-0.179]
</p><p>73 For α above this transition point the N top eigenvalues separate from the bulk. [sent-165, score-0.509]
</p><p>74 In the limit N → ∞ with p = O(N 2 ) the spread of the upper N eigenvalues will tend to zero and they will become localised at λu (c/σ 2 − 1, 2σ 4 ) as deﬁned by equation (6). [sent-166, score-0.471]
</p><p>75 For ﬁnite N and when the two components of the spectra are well separated, we can approximate the eigenvalue spectrum of the top N eigenvalues as though the data only contains these components, i. [sent-167, score-1.252]
</p><p>76 we model this cluster as isotropic data with α = p/N and variance 2cσ 2 . [sent-169, score-0.304]
</p><p>77 On the left of ﬁgure 3 we compare this approximation to the Gram matrix spectrum averaged over 300 data sets for large c, with the inset showing the separated cluster. [sent-171, score-0.744]
</p><p>78 For the bulk we believe these to be due to ﬁnite size effects but the theory for the spread of the upper N eigenvalues is only approximate since the spread of this cluster will vanish as N → ∞ for ﬁxed c and p = O(N 2 ). [sent-173, score-0.639]
</p><p>79 On the right of ﬁgure 3 we plot the average of the top N eigenvalues against c, showing good agreement with the theory. [sent-174, score-0.541]
</p><p>80 The top eigenvalue of the population covariance is shown by the line and the theory accurately predicts the bias in the sample estimate. [sent-175, score-0.778]
</p><p>81 05  0  0  2  4  20  15  10  5  0  8  6  Simulation Theory Unbiased Eigenvalue  25  0  10  5  20  15  c  Eigenvalue  Figure 3: On the left we show the Gram matrix eigenvalue spectrum averaged over 300 data sets and compare it to the theoretical result. [sent-185, score-0.86]
</p><p>82 The inset shows the density of the top N eigenvalues which are separated from the bulk. [sent-186, score-0.711]
</p><p>83 On the right we show the average of the top N eigenvalues against the theoretical result as a function of c. [sent-188, score-0.532]
</p><p>84 2  7 0  0  1  2  3  4  5  6  0  7  0  1  Eigenvalue  2  3  4  5  6  7  Eigenvalue  6  6  <λ2>Theory 5  0  5  10  Rank  15  20  5  0  5  10  15  20  Rank  Figure 4: Results from a purely quadratic kernel (c = 0) on data containing a single dimension having positive kurtosis. [sent-197, score-0.341]
</p><p>85 We show the top 20 eigenvalues of the Gram matrix with the bulk spectrum as an inset. [sent-198, score-0.977]
</p><p>86 On the left κ4 = 5 and we are above the transition where the top eigenvalue is separated from the bulk. [sent-199, score-0.557]
</p><p>87 This is analogous to the case for PCA studied in section 2 and the result for the limiting spectrum carries over. [sent-203, score-0.545]
</p><p>88 For each component of the data with κi > 2σ 4 / α there will be a 4 delta function in the spectrum at λu (κi /2σ 4 , 2σ 4 ) as deﬁned by equation (6). [sent-205, score-0.502]
</p><p>89 4 In ﬁgure 4 we show the Gram matrix eigenvalues for a data set containing a single dimension having positive kurtosis. [sent-206, score-0.556]
</p><p>90 We have indicated with arrows the theoretical prediction for the top two eigenvalues and we see that there is a signiﬁcant difference, although the separation is quite well described by the theory. [sent-208, score-0.501]
</p><p>91 On the right we have κ4 = 1 which is below the transition and the spectrum is very similar to the case for isotropic Gaussian data. [sent-210, score-0.563]
</p><p>92 5  Conclusion  We studied the asymptotic form of the sample covariance eigenvalue spectrum from data with symmetry-breaking structure. [sent-211, score-1.099]
</p><p>93 For standard PCA the asymptotic results are very accurate even for moderate data dimension, but for kernel PCA with a quadratic kernel we found that convergence to the asymptotic result was slow. [sent-212, score-0.509]
</p><p>94 The limiting form of sample covariance spectra has previously been studied in the neural networks literature where it can be used in order to determine the optimal batch learning rate for large linear perceptrons. [sent-213, score-0.5]
</p><p>95 Indeed, the results derived in section 2 for Gaussian data can also be derived by adapting an elegant method developed by Sollich [15], without recourse to the replica method. [sent-214, score-0.284]
</p><p>96 Halkjær & Winther used this approach to compute the spectral density for the case of a single symmetry breaking direction and obtained a similar result to us, except that the position of the separated eigenvalue was at σ 2 (1 + A) which differs from our result [16]. [sent-215, score-0.681]
</p><p>97 In fact they assumed a large signal in their derivation and their derivation can easily be adapted to obtain an identical result to ours. [sent-216, score-0.214]
</p><p>98 However this method, as well as the replica approach used here, is limited because it only applies to Gaussian data, while the Stieltjes transform relationship in equation (8) has been derived under much weaker conditions on the data distribution. [sent-217, score-0.385]
</p><p>99 Our current work is focussed on extending the analysis to more general kernels, such as the radial basis function (RBF) kernel where the feature space dimension is inﬁnite. [sent-218, score-0.243]
</p><p>100 In the general case we ﬁnd that the Stieltjes transform can be derived by a variational mean ﬁeld theory and therefore provides a principled approximation to the average spectral density. [sent-219, score-0.218]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eigenvalues', 0.338), ('spectrum', 0.323), ('pca', 0.299), ('eigenvalue', 0.299), ('covariance', 0.197), ('stieltjes', 0.177), ('isotropic', 0.172), ('bulk', 0.154), ('xxt', 0.152), ('gram', 0.149), ('replica', 0.14), ('vec', 0.14), ('enko', 0.118), ('trg', 0.118), ('mar', 0.117), ('inset', 0.112), ('kernel', 0.105), ('top', 0.103), ('limiting', 0.102), ('behaviour', 0.1), ('sample', 0.097), ('mechanics', 0.093), ('pastur', 0.088), ('silverstein', 0.088), ('separated', 0.087), ('asymptotic', 0.081), ('feature', 0.078), ('directions', 0.073), ('density', 0.071), ('dh', 0.07), ('transition', 0.068), ('bm', 0.065), ('spectra', 0.064), ('transform', 0.064), ('components', 0.063), ('data', 0.062), ('gure', 0.06), ('theoretical', 0.06), ('dimension', 0.06), ('matrix', 0.059), ('halkj', 0.059), ('hoyle', 0.059), ('magnus', 0.059), ('manchester', 0.059), ('reimann', 0.059), ('resolvent', 0.059), ('symmetrybreaking', 0.059), ('averaged', 0.057), ('gaussian', 0.056), ('agreement', 0.056), ('log', 0.053), ('spectral', 0.051), ('perturbative', 0.051), ('sengupta', 0.051), ('signal', 0.051), ('identical', 0.05), ('analogous', 0.049), ('bias', 0.048), ('equation', 0.047), ('fractional', 0.047), ('den', 0.047), ('eigenspectrum', 0.047), ('limit', 0.044), ('quadratic', 0.044), ('showing', 0.044), ('degeneracy', 0.043), ('centred', 0.043), ('spread', 0.042), ('component', 0.042), ('im', 0.041), ('derivation', 0.041), ('variance', 0.041), ('derived', 0.041), ('studied', 0.04), ('symmetry', 0.039), ('independently', 0.038), ('direction', 0.038), ('distributed', 0.038), ('principal', 0.037), ('carried', 0.037), ('containing', 0.037), ('understand', 0.035), ('rank', 0.035), ('breaking', 0.034), ('theory', 0.034), ('selection', 0.034), ('stationary', 0.034), ('purely', 0.033), ('weaker', 0.031), ('nite', 0.031), ('result', 0.031), ('strength', 0.03), ('calculation', 0.03), ('det', 0.029), ('cluster', 0.029), ('increased', 0.029), ('equal', 0.029), ('lim', 0.029), ('delta', 0.028), ('therefore', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="114-tfidf-1" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><p>2 0.21698602 <a title="114-tfidf-2" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>Author: Max Welling, Christopher Williams, Felix V. Agakov</p><p>Abstract: Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for “extreme components analysis” (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efﬁcient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.</p><p>3 0.13448876 <a title="114-tfidf-3" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>4 0.1032846 <a title="114-tfidf-4" href="./nips-2003-Out-of-Sample_Extensions_for_LLE%2C_Isomap%2C_MDS%2C_Eigenmaps%2C_and_Spectral_Clustering.html">150 nips-2003-Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering</a></p>
<p>Author: Yoshua Bengio, Jean-françcois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas L. Roux, Marie Ouimet</p><p>Abstract: Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a uniﬁed framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data. 1</p><p>5 0.10180434 <a title="114-tfidf-5" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>6 0.10124949 <a title="114-tfidf-6" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>7 0.097001903 <a title="114-tfidf-7" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<p>8 0.094879553 <a title="114-tfidf-8" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>9 0.09234646 <a title="114-tfidf-9" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>10 0.08934211 <a title="114-tfidf-10" href="./nips-2003-Minimax_Embeddings.html">128 nips-2003-Minimax Embeddings</a></p>
<p>11 0.088082828 <a title="114-tfidf-11" href="./nips-2003-Clustering_with_the_Connectivity_Kernel.html">46 nips-2003-Clustering with the Connectivity Kernel</a></p>
<p>12 0.082099892 <a title="114-tfidf-12" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>13 0.07762403 <a title="114-tfidf-13" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>14 0.077536181 <a title="114-tfidf-14" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>15 0.076727949 <a title="114-tfidf-15" href="./nips-2003-Locality_Preserving_Projections.html">120 nips-2003-Locality Preserving Projections</a></p>
<p>16 0.07526613 <a title="114-tfidf-16" href="./nips-2003-Approximate_Analytical_Bootstrap_Averages_for_Support_Vector_Classifiers.html">31 nips-2003-Approximate Analytical Bootstrap Averages for Support Vector Classifiers</a></p>
<p>17 0.075146608 <a title="114-tfidf-17" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>18 0.074965701 <a title="114-tfidf-18" href="./nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</a></p>
<p>19 0.068611033 <a title="114-tfidf-19" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>20 0.068368837 <a title="114-tfidf-20" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.223), (1, -0.095), (2, 0.008), (3, -0.004), (4, -0.048), (5, 0.217), (6, 0.072), (7, -0.031), (8, 0.083), (9, -0.087), (10, -0.052), (11, -0.044), (12, 0.023), (13, -0.081), (14, -0.047), (15, -0.08), (16, 0.164), (17, 0.004), (18, 0.173), (19, 0.008), (20, 0.019), (21, 0.01), (22, 0.034), (23, -0.127), (24, 0.008), (25, 0.078), (26, -0.045), (27, 0.044), (28, -0.115), (29, -0.195), (30, 0.079), (31, -0.023), (32, -0.143), (33, 0.035), (34, -0.032), (35, -0.049), (36, -0.057), (37, 0.054), (38, -0.1), (39, 0.034), (40, -0.072), (41, -0.084), (42, 0.059), (43, 0.122), (44, -0.058), (45, 0.036), (46, 0.137), (47, 0.12), (48, -0.195), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96989387 <a title="114-lsi-1" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><p>2 0.9270581 <a title="114-lsi-2" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>Author: Max Welling, Christopher Williams, Felix V. Agakov</p><p>Abstract: Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for “extreme components analysis” (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efﬁcient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.</p><p>3 0.62849587 <a title="114-lsi-3" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>4 0.49226508 <a title="114-lsi-4" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>Author: Jason Weston, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: We consider the problem of reconstructing patterns from a feature map. Learning algorithms using kernels to operate in a reproducing kernel Hilbert space (RKHS) express their solutions in terms of input points mapped into the RKHS. We introduce a technique based on kernel principal component analysis and regression to reconstruct corresponding patterns in the input space (aka pre-images) and review its performance in several applications requiring the construction of pre-images. The introduced technique avoids difﬁcult and/or unstable numerical optimization, is easy to implement and, unlike previous methods, permits the computation of pre-images in discrete input spaces. 1</p><p>5 0.47344461 <a title="114-lsi-5" href="./nips-2003-Efficient_and_Robust_Feature_Extraction_by_Maximum_Margin_Criterion.html">59 nips-2003-Efficient and Robust Feature Extraction by Maximum Margin Criterion</a></p>
<p>Author: Haifeng Li, Tao Jiang, Keshu Zhang</p><p>Abstract: A new feature extraction criterion, maximum margin criterion (MMC), is proposed in this paper. This new criterion is general in the sense that, when combined with a suitable constraint, it can actually give rise to the most popular feature extractor in the literature, linear discriminate analysis (LDA). We derive a new feature extractor based on MMC using a different constraint that does not depend on the nonsingularity of the within-class scatter matrix Sw . Such a dependence is a major drawback of LDA especially when the sample size is small. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in this paper. Our preliminary experimental results on face images demonstrate that the new feature extractors are efﬁcient and stable. 1</p><p>6 0.42455688 <a title="114-lsi-6" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>7 0.40917823 <a title="114-lsi-7" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>8 0.39388898 <a title="114-lsi-8" href="./nips-2003-The_Diffusion-Limited_Biochemical_Signal-Relay_Channel.html">184 nips-2003-The Diffusion-Limited Biochemical Signal-Relay Channel</a></p>
<p>9 0.38965887 <a title="114-lsi-9" href="./nips-2003-Information_Maximization_in_Noisy_Channels_%3A_A_Variational_Approach.html">94 nips-2003-Information Maximization in Noisy Channels : A Variational Approach</a></p>
<p>10 0.38802519 <a title="114-lsi-10" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>11 0.38564941 <a title="114-lsi-11" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>12 0.37087443 <a title="114-lsi-12" href="./nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</a></p>
<p>13 0.35773209 <a title="114-lsi-13" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>14 0.35627314 <a title="114-lsi-14" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>15 0.35094684 <a title="114-lsi-15" href="./nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</a></p>
<p>16 0.35007721 <a title="114-lsi-16" href="./nips-2003-Out-of-Sample_Extensions_for_LLE%2C_Isomap%2C_MDS%2C_Eigenmaps%2C_and_Spectral_Clustering.html">150 nips-2003-Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering</a></p>
<p>17 0.33711812 <a title="114-lsi-17" href="./nips-2003-Clustering_with_the_Connectivity_Kernel.html">46 nips-2003-Clustering with the Connectivity Kernel</a></p>
<p>18 0.33710822 <a title="114-lsi-18" href="./nips-2003-Plasticity_Kernels_and_Temporal_Statistics.html">157 nips-2003-Plasticity Kernels and Temporal Statistics</a></p>
<p>19 0.3356851 <a title="114-lsi-19" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>20 0.33363807 <a title="114-lsi-20" href="./nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">92 nips-2003-Information Bottleneck for Gaussian Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.032), (29, 0.378), (30, 0.024), (35, 0.036), (53, 0.173), (69, 0.016), (71, 0.052), (76, 0.058), (85, 0.054), (91, 0.081), (99, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94554675 <a title="114-lda-1" href="./nips-2003-Model_Uncertainty_in_Classical_Conditioning.html">130 nips-2003-Model Uncertainty in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, Geoffrey J. Gordon, David S. Touretzky, Nathaniel D. Daw</p><p>Abstract: We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments. Traditional accounts of conditioning ﬁt parameters within a ﬁxed generative model of reinforcer delivery; uncertainty over the model structure is not considered. We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes. According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justiﬁed by additional experience. 1</p><p>same-paper 2 0.8446185 <a title="114-lda-2" href="./nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</a></p>
<p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><p>3 0.74713796 <a title="114-lda-3" href="./nips-2003-Approximate_Planning_in_POMDPs_with_Macro-Actions.html">33 nips-2003-Approximate Planning in POMDPs with Macro-Actions</a></p>
<p>Author: Georgios Theocharous, Leslie P. Kaelbling</p><p>Abstract: Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efﬁciently.</p><p>4 0.61466199 <a title="114-lda-4" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>Author: Max Welling, Christopher Williams, Felix V. Agakov</p><p>Abstract: Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for “extreme components analysis” (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efﬁcient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.</p><p>5 0.59305567 <a title="114-lda-5" href="./nips-2003-Probabilistic_Inference_in_Human_Sensorimotor_Processing.html">161 nips-2003-Probabilistic Inference in Human Sensorimotor Processing</a></p>
<p>Author: Konrad P. Körding, Daniel M. Wolpert</p><p>Abstract: When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.</p><p>6 0.56213409 <a title="114-lda-6" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>7 0.56116396 <a title="114-lda-7" href="./nips-2003-Probabilistic_Inference_of_Speech_Signals_from_Phaseless_Spectrograms.html">162 nips-2003-Probabilistic Inference of Speech Signals from Phaseless Spectrograms</a></p>
<p>8 0.56047994 <a title="114-lda-8" href="./nips-2003-Non-linear_CCA_and_PCA_by_Alignment_of_Local_Models.html">138 nips-2003-Non-linear CCA and PCA by Alignment of Local Models</a></p>
<p>9 0.55908197 <a title="114-lda-9" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<p>10 0.55740821 <a title="114-lda-10" href="./nips-2003-Maximum_Likelihood_Estimation_of_a_Stochastic_Integrate-and-Fire_Neural_Model.html">125 nips-2003-Maximum Likelihood Estimation of a Stochastic Integrate-and-Fire Neural Model</a></p>
<p>11 0.54762673 <a title="114-lda-11" href="./nips-2003-Bias-Corrected_Bootstrap_and_Model_Uncertainty.html">40 nips-2003-Bias-Corrected Bootstrap and Model Uncertainty</a></p>
<p>12 0.54576367 <a title="114-lda-12" href="./nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</a></p>
<p>13 0.54281825 <a title="114-lda-13" href="./nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</a></p>
<p>14 0.5411399 <a title="114-lda-14" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>15 0.54064125 <a title="114-lda-15" href="./nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</a></p>
<p>16 0.54061407 <a title="114-lda-16" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>17 0.53917438 <a title="114-lda-17" href="./nips-2003-Optimal_Manifold_Representation_of_Data%3A_An_Information_Theoretic_Approach.html">149 nips-2003-Optimal Manifold Representation of Data: An Information Theoretic Approach</a></p>
<p>18 0.53735805 <a title="114-lda-18" href="./nips-2003-Information_Dynamics_and_Emergent_Computation_in_Recurrent_Circuits_of_Spiking_Neurons.html">93 nips-2003-Information Dynamics and Emergent Computation in Recurrent Circuits of Spiking Neurons</a></p>
<p>19 0.53560299 <a title="114-lda-19" href="./nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</a></p>
<p>20 0.53546828 <a title="114-lda-20" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
