<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2003-Learning with Local and Global Consistency</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-113" href="#">nips2003-113</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2003-Learning with Local and Global Consistency</h1>
<br/><p>Source: <a title="nips-2003-113-pdf" href="http://papers.nips.cc/paper/2506-learning-with-local-and-global-consistency.pdf">pdf</a></p><p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>Reference: <a title="nips-2003-113-reference" href="../nips2003_reference/nips-2003-Learning_with_Local_and_Global_Consistency_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('unlabel', 0.361), ('label', 0.358), ('harmon', 0.313), ('moon', 0.294), ('rbf', 0.222), ('toy', 0.197), ('svm', 0.183), ('nity', 0.145), ('af', 0.141), ('kernel', 0.136), ('digit', 0.128), ('diffus', 0.125), ('sf', 0.122), ('smoo', 0.116), ('xl', 0.115), ('spreading', 0.108), ('zhu', 0.096), ('class', 0.086), ('orthodox', 0.083), ('nn', 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="113-tfidf-1" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>2 0.23086682 <a title="113-tfidf-2" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>3 0.213429 <a title="113-tfidf-3" href="./nips-2003-Ranking_on_Data_Manifolds.html">164 nips-2003-Ranking on Data Manifolds</a></p>
<p>Author: Dengyong Zhou, Jason Weston, Arthur Gretton, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method. 1</p><p>4 0.20428027 <a title="113-tfidf-4" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>5 0.16457829 <a title="113-tfidf-5" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<p>Author: Noam Shental, Assaf Zomet, Tomer Hertz, Yair Weiss</p><p>Abstract: Signiﬁcant progress in clustering has been achieved by algorithms that are based on pairwise afﬁnities between the datapoints. In particular, spectral clustering methods have the advantage of being able to divide arbitrarily shaped clusters and are based on efﬁcient eigenvector calculations. However, spectral methods lack a straightforward probabilistic interpretation which makes it difﬁcult to automatically set parameters using training data. In this paper we use the previously proposed typical cut framework for pairwise clustering. We show an equivalence between calculating the typical cut and inference in an undirected graphical model. We show that for clustering problems with hundreds of datapoints exact inference may still be possible. For more complicated datasets, we show that loopy belief propagation (BP) and generalized belief propagation (GBP) can give excellent results on challenging clustering problems. We also use graphical models to derive a learning algorithm for afﬁnity matrices based on labeled data. 1</p><p>6 0.16185775 <a title="113-tfidf-6" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>7 0.15710044 <a title="113-tfidf-7" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>8 0.14473245 <a title="113-tfidf-8" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>9 0.14423659 <a title="113-tfidf-9" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>10 0.14366168 <a title="113-tfidf-10" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>11 0.13512775 <a title="113-tfidf-11" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>12 0.12599581 <a title="113-tfidf-12" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>13 0.12209871 <a title="113-tfidf-13" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>14 0.11768895 <a title="113-tfidf-14" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>15 0.11711365 <a title="113-tfidf-15" href="./nips-2003-Prediction_on_Spike_Data_Using_Kernel_Algorithms.html">160 nips-2003-Prediction on Spike Data Using Kernel Algorithms</a></p>
<p>16 0.11063342 <a title="113-tfidf-16" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>17 0.10954491 <a title="113-tfidf-17" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>18 0.10643054 <a title="113-tfidf-18" href="./nips-2003-Learning_the_k_in_k-means.html">111 nips-2003-Learning the k in k-means</a></p>
<p>19 0.1054761 <a title="113-tfidf-19" href="./nips-2003-Clustering_with_the_Connectivity_Kernel.html">46 nips-2003-Clustering with the Connectivity Kernel</a></p>
<p>20 0.098865911 <a title="113-tfidf-20" href="./nips-2003-Out-of-Sample_Extensions_for_LLE%2C_Isomap%2C_MDS%2C_Eigenmaps%2C_and_Spectral_Clustering.html">150 nips-2003-Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.295), (1, 0.198), (2, -0.048), (3, 0.038), (4, -0.076), (5, -0.021), (6, -0.084), (7, -0.185), (8, 0.062), (9, 0.104), (10, 0.207), (11, 0.103), (12, -0.068), (13, -0.112), (14, -0.093), (15, 0.001), (16, 0.003), (17, 0.192), (18, 0.062), (19, 0.072), (20, -0.084), (21, -0.114), (22, 0.128), (23, 0.06), (24, -0.024), (25, -0.009), (26, 0.186), (27, -0.006), (28, 0.027), (29, -0.143), (30, -0.051), (31, 0.063), (32, 0.034), (33, -0.146), (34, -0.051), (35, 0.032), (36, -0.007), (37, 0.074), (38, 0.009), (39, 0.018), (40, 0.073), (41, 0.035), (42, -0.03), (43, -0.066), (44, 0.031), (45, 0.155), (46, 0.069), (47, 0.015), (48, 0.045), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94620144 <a title="113-lsi-1" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>2 0.70872635 <a title="113-lsi-2" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>Author: Charles Kemp, Thomas L. Griffiths, Sean Stromsten, Joshua B. Tenenbaum</p><p>Abstract: We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efﬁcient computation of the optimal Bayesian classiﬁcation function from the labeled examples. We test our approach on eight real-world datasets. 1</p><p>3 0.65496582 <a title="113-lsi-3" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>Author: Jason Weston, Dengyong Zhou, André Elisseeff, William S. Noble, Christina S. Leslie</p><p>Abstract: A key issue in supervised protein classiﬁcation is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classiﬁcation performance. However, such representations are based only on labeled data — examples with known 3D structures, organized into structural classes — while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classiﬁcation performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efﬁciency. 1</p><p>4 0.63145339 <a title="113-lsi-4" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>Author: Tijl D. Bie, Nello Cristianini</p><p>Abstract: The 2-class transduction problem, as formulated by Vapnik [1], involves ﬁnding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3]. In this paper we present a relaxation of this task based on semideﬁnite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated. 1</p><p>5 0.58643997 <a title="113-lsi-5" href="./nips-2003-Measure_Based_Regularization.html">126 nips-2003-Measure Based Regularization</a></p>
<p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><p>6 0.57113624 <a title="113-lsi-6" href="./nips-2003-Ranking_on_Data_Manifolds.html">164 nips-2003-Ranking on Data Manifolds</a></p>
<p>7 0.56037945 <a title="113-lsi-7" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>8 0.5267185 <a title="113-lsi-8" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>9 0.49393874 <a title="113-lsi-9" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>10 0.48977607 <a title="113-lsi-10" href="./nips-2003-AUC_Optimization_vs._Error_Rate_Minimization.html">3 nips-2003-AUC Optimization vs. Error Rate Minimization</a></p>
<p>11 0.48336703 <a title="113-lsi-11" href="./nips-2003-1-norm_Support_Vector_Machines.html">1 nips-2003-1-norm Support Vector Machines</a></p>
<p>12 0.48120165 <a title="113-lsi-12" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>13 0.46721068 <a title="113-lsi-13" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>14 0.46282989 <a title="113-lsi-14" href="./nips-2003-Application_of_SVMs_for_Colour_Classification_and_Collision_Detection_with_AIBO_Robots.html">28 nips-2003-Application of SVMs for Colour Classification and Collision Detection with AIBO Robots</a></p>
<p>15 0.46082133 <a title="113-lsi-15" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>16 0.43047336 <a title="113-lsi-16" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>17 0.43018189 <a title="113-lsi-17" href="./nips-2003-Sparse_Greedy_Minimax_Probability_Machine_Classification.html">178 nips-2003-Sparse Greedy Minimax Probability Machine Classification</a></p>
<p>18 0.41835994 <a title="113-lsi-18" href="./nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</a></p>
<p>19 0.41637474 <a title="113-lsi-19" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>20 0.40377504 <a title="113-lsi-20" href="./nips-2003-Pairwise_Clustering_and_Graphical_Models.html">152 nips-2003-Pairwise Clustering and Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.092), (8, 0.233), (9, 0.019), (11, 0.03), (26, 0.017), (31, 0.12), (53, 0.101), (58, 0.102), (62, 0.075), (76, 0.11), (85, 0.015), (95, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79073191 <a title="113-lda-1" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><p>2 0.78834313 <a title="113-lda-2" href="./nips-2003-A_Probabilistic_Model_of_Auditory_Space_Representation_in_the_Barn_Owl.html">15 nips-2003-A Probabilistic Model of Auditory Space Representation in the Barn Owl</a></p>
<p>Author: Brian J. Fischer, Charles H. Anderson</p><p>Abstract: The barn owl is a nocturnal hunter, capable of capturing prey using auditory information alone [1]. The neural basis for this localization behavior is the existence of auditory neurons with spatial receptive ﬁelds [2]. We provide a mathematical description of the operations performed on auditory input signals by the barn owl that facilitate the creation of a representation of auditory space. To develop our model, we ﬁrst formulate the sound localization problem solved by the barn owl as a statistical estimation problem. The implementation of the solution is constrained by the known neurobiology.</p><p>3 0.71555686 <a title="113-lda-3" href="./nips-2003-Ranking_on_Data_Manifolds.html">164 nips-2003-Ranking on Data Manifolds</a></p>
<p>Author: Dengyong Zhou, Jason Weston, Arthur Gretton, Olivier Bousquet, Bernhard Schölkopf</p><p>Abstract: The Google search engine has enjoyed huge success with its web page ranking algorithm, which exploits global, rather than local, hyperlink structure of the web using random walks. Here we propose a simple universal ranking algorithm for data lying in the Euclidean space, such as text or image data. The core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data. Encouraging experimental results from synthetic, image, and text data illustrate the validity of our method. 1</p><p>4 0.70521063 <a title="113-lda-4" href="./nips-2003-Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data.html">77 nips-2003-Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data</a></p>
<p>Author: Neil D. Lawrence</p><p>Abstract: In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.</p><p>5 0.69862103 <a title="113-lda-5" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: Knowledge about local invariances with respect to given pattern transformations can greatly improve the accuracy of classiﬁcation. Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples. We develop a new framework for learning linear classiﬁers under known transformations based on semideﬁnite programming. We present a new learning algorithm— the Semideﬁnite Programming Machine (SDPM)—which is able to ﬁnd a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points. The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors. Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed. In experiments we use a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and ﬁnd improvements over known methods. 1</p><p>6 0.69215691 <a title="113-lda-6" href="./nips-2003-Learning_to_Find_Pre-Images.html">112 nips-2003-Learning to Find Pre-Images</a></p>
<p>7 0.68827122 <a title="113-lda-7" href="./nips-2003-Semi-supervised_Protein_Classification_Using_Cluster_Kernels.html">173 nips-2003-Semi-supervised Protein Classification Using Cluster Kernels</a></p>
<p>8 0.68808025 <a title="113-lda-8" href="./nips-2003-Probability_Estimates_for_Multi-Class_Classification_by_Pairwise_Coupling.html">163 nips-2003-Probability Estimates for Multi-Class Classification by Pairwise Coupling</a></p>
<p>9 0.68719846 <a title="113-lda-9" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>10 0.68664092 <a title="113-lda-10" href="./nips-2003-Semi-Supervised_Learning_with_Trees.html">172 nips-2003-Semi-Supervised Learning with Trees</a></p>
<p>11 0.68259692 <a title="113-lda-11" href="./nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</a></p>
<p>12 0.68137091 <a title="113-lda-12" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>13 0.68135279 <a title="113-lda-13" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>14 0.68077618 <a title="113-lda-14" href="./nips-2003-Applying_Metric-Trees_to_Belief-Point_POMDPs.html">29 nips-2003-Applying Metric-Trees to Belief-Point POMDPs</a></p>
<p>15 0.67991012 <a title="113-lda-15" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>16 0.67945868 <a title="113-lda-16" href="./nips-2003-Statistical_Debugging_of_Sampled_Programs.html">181 nips-2003-Statistical Debugging of Sampled Programs</a></p>
<p>17 0.67804778 <a title="113-lda-17" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>18 0.67783725 <a title="113-lda-18" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>19 0.67552489 <a title="113-lda-19" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>20 0.67544687 <a title="113-lda-20" href="./nips-2003-Nonstationary_Covariance_Functions_for_Gaussian_Process_Regression.html">141 nips-2003-Nonstationary Covariance Functions for Gaussian Process Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
