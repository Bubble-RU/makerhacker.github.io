<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-22" href="#">nips2003-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</h1>
<br/><p>Source: <a title="nips-2003-22-pdf" href="http://papers.nips.cc/paper/2368-an-improved-scheme-for-detection-and-labelling-in-johansson-displays.pdf">pdf</a></p><p>Author: Claudio Fanti, Marzia Polito, Pietro Perona</p><p>Abstract: Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can eﬀortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body’s centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more signiﬁcantly, to the introduction of the centroid variable. 1</p><p>Reference: <a title="nips-2003-22-reference" href="../nips2003_reference/nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fy', 0.762), ('body', 0.237), ('detect', 0.173), ('lbp', 0.158), ('loop', 0.126), ('display', 0.124), ('clut', 0.122), ('label', 0.107), ('foreground', 0.105), ('goncalv', 0.105), ('johannson', 0.105), ('johansson', 0.105), ('occlud', 0.104), ('transl', 0.09), ('hum', 0.09), ('peron', 0.087), ('veloc', 0.086), ('graph', 0.083), ('inv', 0.08), ('rp', 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="22-tfidf-1" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>Author: Claudio Fanti, Marzia Polito, Pietro Perona</p><p>Abstract: Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can eﬀortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body’s centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more signiﬁcantly, to the introduction of the centroid variable. 1</p><p>2 0.20446609 <a title="22-tfidf-2" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>Author: Leonid Sigal, Michael Isard, Benjamin H. Sigelman, Michael J. Black</p><p>Abstract: The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuousvalued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle ﬁlter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter. 1</p><p>3 0.10638319 <a title="22-tfidf-3" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>Author: Kevin P. Murphy, Antonio Torralba, William T. Freeman</p><p>Abstract: Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not. We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities. We present a conditional random ﬁeld for jointly solving the tasks of object detection and scene classiﬁcation. 1</p><p>4 0.10559756 <a title="22-tfidf-4" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<p>Author: Michael Fink, Pietro Perona</p><p>Abstract: Mutual Boosting is a method aimed at incorporating contextual information to augment object detection. When multiple detectors of objects and parts are trained in parallel using AdaBoost [1], object detectors might use the remaining intermediate detectors to enrich the weak learner set. This method generalizes the efficient features suggested by Viola and Jones [2] thus enabling information inference between parts and objects in a compositional hierarchy. In our experiments eye-, nose-, mouth- and face detectors are trained using the Mutual Boosting framework. Results show that the method outperforms applications overlooking contextual information. We suggest that achieving contextual integration is a step toward human-like detection capabilities. 1 In trod u ction Classification of multiple objects in complex scenes is one of the next challenges facing the machine learning and computer vision communities. Although, real-time detection of single object classes has been recently demonstrated [2], naïve duplication of these detectors to the multiclass case would be unfeasible. Our goal is to propose an efficient method for detection of multiple objects in natural scenes. Hand-in-hand with the challenges entailing multiclass detection, some distinct advantages emerge as well. Knowledge on position of several objects might shed light on the entire scene (Figure 1). Detection systems that do not exploit the information provided by objects on the neighboring scene will be suboptimal. A B Figure 1: Contextual spatial relationships assist detection A. in absence of facial components (whitened blocking box) faces can be detected by context (alignment of neighboring faces). B. keyboards can be detected when they appear under monitors. Many human and computer vision models postulate explicitly or implicitly that vision follows a compositional hierarchy. Grounded features (that are innate/hardwired and are available prior to learning) are used to detect salient parts, these parts in turn enable detection of complex objects [3, 4], and finally objects are used to recognize the semantics of the entire scene. Yet, a more accurate assessment of human performance reveals that the visual system often violates this strictly hierarchical structure in two ways. First, part and whole detection are often evidently interacting [5, 6]. Second, several layers of the hierarchy are occasionally bypassed to enable swift direct detection. This phenomenon is demonstrated by gist recognition experiments where the semantic classification of an entire scene is performed using only minimal low level feature information [7]. The insights emerging from observing human perception were adopted by the object detection community. Many object detection algorithms bypass stages of a strict compositional hierarchy. The Viola & Jones (VJ) detector [2] is able to perform robust online face detection by directly agglomerating very low-level features (rectangle contrasts), without explicitly referring to facial parts. Gist detection from low-level spatial frequencies was demonstrated by Oliva and Torralba [8]. Recurrent optimization of parts and object constellation is also common in modern detection schemes [9]. Although Latent Semantic Analysis (making use of object cooccurrence information) has been adapted to images [10], the existing state of object detection methods is still far from unifying all the sources of visual contextual information integrated by the human perceptual system. Tackling the context integration problem and achieving robust multiclass object detection is a vital step for applications like image-content database indexing and autonomous robot navigation. We will propose a method termed Mutual Boosting to incorporate contextual information for object detection. Section 2 will start by posing the multiclass detection problem from labeled images. In Section 3 we characterize the feature sets implemented by Mutual Boosting and define an object's contextual neighborhood. Section 4 presents the Mutual Boosting framework aimed at integrating contextual information and inspired by the recurrent inferences dominating the human perceptual system. An application of the Mutual Boosting framework to facial component detection is presented in Section 5. We conclude with a discussion on the scope and limitations of the proposed framework. 2 Problem setting and basic notation Suppose we wish to detect multiple objects in natural scenes, and that these scenes are characterized by certain mutual positions between the composing objects. Could we make use of these objects' contextual relations to improve detection? Perceptual context might include multiple sources of information: information originating from the presence of existing parts, information derived from other objects in the perceptual vicinity and finally general visual knowledge on the scene. In order to incorporate these various sources of visual contextual information Mutual Boosting will treat parts, objects and scenes identically. We will therefore use the term object as a general term while referring to any entity in the compositional hierarchy. Let M denote the cardinality of the object set we wish to detect in natural scenes. Our goal is to optimize detection by exploiting contextual information while maintaining detection time comparable to M individual detectors trained without such information. We define the goal of the multiclass detection algorithm as generating M intensity maps Hm=1,..,M indicating the likelihood of object m appearing at different positions in a target image. We will use the following notation (Figure 2): • H0+/H0-: raw image input with/without the trained objects (A1 & A2) • Cm[i]: labeled position of instance i of object m in image H0+ • Hm: intensity map output indicating the likelihood of object m appearing in different positions in the image H0 (B) B. Hm A2. H0- A1. H0+ Cm[1] Cm[2] Cm[1] Cm[2] Figure 2: A1 & A2. Input: position of positive and negative examples of eyes in natural images. B. Output: Eye intensity (eyeness) detection map of image H0+ 3 F e a t u r e se t a n d c o n t e x t u a l wi n d o w g e n e r a l i za t i o n s The VJ method for real-time object-detection included three basic innovations. First, they presented the rectangle contrast-features, features that are evaluated efficiently, using an integral-image. Second, VJ introduced AdaBoost [1] to object detection using rectangle features as weak learners. Finally a cascade method was developed to chain a sequence of increasingly complex AdaBoost learners to enable rapid filtering of non-relevant sections in the target image. The resulting cascade of AdaBoost face detectors achieves a 15 frame per second detection speed, with 90% detection rate and 2x10-6 false alarms. This detection speed is currently unmatched. In order to maintain efficient detection and in order to benchmark the performance of Mutual Boosting we will adopt the rectangle contrast feature framework suggested by VJ. It should be noted that the grayscale rectangle features could be naturally extended to any image channel that preserves the semantics of summation. A diversified feature set (including color features, texture features, etc.) might saturate later than a homogeneous channel feature set. By making use of features that capture the object regularities well, one can improve performance or reduce detection time. VJ extract training windows that capture the exact area of the training faces. We term this the local window approach. A second approach, in line with our attempt to incorporate information from neighboring parts or objects, would be to make use of training windows that capture wide regions around the object (Figure 3)1. A B Figure 3: A local window (VJ) and a contextual window that captures relative position information from objects or parts around and within the detected object. 1 Contextual neighborhoods emerge by downscaling larger regions in the original image to a PxP resolution window. The contextual neighborhood approach contributes to detection when the applied channels require a wide contextual range as will be demonstrated in the Mutual Boosting scheme presented in the following section2. 4 Mutual Boosting The AdaBoost algorithm maintains a clear distinction between the boosting level and the weak-learner training level. The basic insight guiding the Mutual Boosting method reexamines this distinction, stipulating that when multiple objects and parts are trained simultaneously using AdaBoost; any object detector might combine the previously evolving intermediate detectors to generate new weak learners. In order to elaborate this insight it should first be noted that while training a strong learner using 100 iterations of AdaBoost (abbreviated AB100) one could calculate an intermediate strong learner at each step on the way (AB2 - AB99). To apply this observation for our multiclass detection problem we simultaneously train M object detectors. At each boosting iteration t the M detectors (ABmt-1) emerging at the previous stage t-1, are used to filter positive and negative3 training images, thus producing intermediate m-detection maps Hmt-1 (likelihood of object m in the images4). Next, the Mutual Boosting stage takes place and all the existing Hmt-1 maps are used as additional channels out of which new contrast features are selected. This process gradually enriches the initial grounded features with composite contextual features. The composite features are searched on a PxP wide contextual neighborhood region rather than the PxP local window (Figure 3). Following a dynamic programming approach in training and detection, Hm=1,..,M detection maps are constantly maintained and updated so that the recalculation of Hmt only requires the last chosen weak learner WLmn*t to be evaluated on channel Hn*t-1 of the training image (Figure 4). This evaluation produces a binary detection layer that will be weighted by the AdaBoost weak-learner weighting scheme and added to the previous stage map5. Although Mutual Boosting examines a larger feature set during training, an iteration of Mutual Boosting detection of M objects is as time-consuming as performing an AdaBoost detection iteration for M individual objects. The advantage of Mutual Boosting emerges from introducing highly informative feature sets that can enhance detection or require fewer boosting iterations. While most object detection applications extract a local window containing the object information and discard the remaining image (including the object positional information). Mutual Boosting processes the entire image during training and detection and makes constant use of the information characterizing objects’ relative-position in the training images. As we have previously stated, the detected objects might be in various levels of a compositional hierarchy (e.g. complex objects or parts of other objects). Nevertheless, Mutual Boosting provides a similar treatment to objects, parts and scenes enabling any compositional structure of the data to naturally emerge. We will term any contextual reference that is not directly grounded to the basic features, as a cross referencing of objects6. 2 The most efficient size of the contextual neighborhoods might vary, from the immediate to the entire image, and therefore should be empirically learned. 3 Images without target objects (see experimental section below) 4 Unlike the weak learners, the intermediate strong learners do not apply a threshold 5 In order to optimize the number of detection map integral image recalculations these maps might be updated every k (e.g. 50) iterations rather than at each iteration. 6 Scenes can be crossed referenced as well if scene labels are available (office/lab etc.). H0+/0- positive / negative raw images Cm[i] position of instance i of object m=1,..,M in image H0+ initialize boosting-weights of instances i of object m to 1 initialize detection maps Hm+0/Hm-0 to 0 Input Initialization For t=1,…,T For m=1,..,M and n=0,..,M (A) cutout & downscale local (n=0) or contextual (n>0) windows (WINm) of instances i of object m (at Cm[i]), from all existing images Hnt-1 For m=1,..,M normalize boosting-weights of object m instances [1] (B1&2) select map Hn*t-1 and weak learner WLmn* that minimize error on WINm decrease boosting-weights of instances that WLmn* labeled correctly [1] (C) DetectionLayermn* ← WLmn*(Hn*t-1) calculate α mt the weak learner contribution factor from the empirical error [1] (D) update m-detection map Hmt ← Hmt-1 + αmt DetectionLayermn * Return strong learner ABmT including WLmn*1,..,T and αm1,..,T (m=1,..,M) H0± raw image H1± . . . Hn*± (A) WIN m0 WL m0 (B1) . . . Hm± (A) WIN m1 (B2) WL m1 (B1) (B2) m detection map (A) WIN mn* WL (B1) (D) (C) Detection Layer mn* mn* Figure 4: Mutual Boosting Diagram & Pseudo code. Each raw image H0 is analyzed by M object detection maps Hm=1,.,M, updated by iterating through four steps: (A) cutout & downscale from existing maps H n=0,..,M t-1 a local (n=0) or contextual (n>0) PxP window containing a neighborhood of object m (B1&2) select best performing map Hn* and weak learner WLmn* that optimize object m detection (C) run WLmn* on Hn* map to generate a new binary m-detection layer (D) add m-detection layer to existing detection map Hm. [1] Standard AdaBoost stages are not elaborated To maintain local and global natural scene statistics, negative training examples are generated by pairing each image with an image of equal size that does not contain the target objects and by centering the local and contextual windows of the positive and negative examples on the object positions in the positive images (see Figure 2). By using parallel boosting and efficient rectangle contrast features, Mutual Boosting is capable of incorporating many information inferences (references in Figure 5): • Features could be used to directly detect parts and objects (A & B) • Objects could be used to detect other (or identical) objects in the image (C) • Parts could be used to detect other (or identical) nearby parts (D & E) • Parts could be used to detect objects (F) • Objects could be used to detect parts A. eye feature from raw image B. face feature from raw image C. face E. mouth feature from eye feature from face detection image detection image F. face feature from mouth D. eye feature from eye detection image detection image Figure 5: A-E. Emerging features of eyes, mouths and faces (presented on windows of raw images for legibility). The windows’ scale is defined by the detected object size and by the map mode (local or contextual). C. faces are detected using face detection maps HFace, exploiting the fact that faces tend to be horizontally aligned. 5 Experiments A. Pd In order to test the contribution of the Mutual Boosting process we focused on detection of objects in what we term a face-scene (right eye, left eye, nose, mouth and face). We chose to perform contextual detection in the face-scene for two main reasons. First as detailed in Figure 5, face scenes demonstrate a range of potential part and object cross references. Second, faces have been the focus of object detection research for many years, thus enabling a systematic result comparison. Experiment 1 was aimed at comparing the performance of Mutual Boosting to that of naïve independently trained object detectors using local windows. Pfa Figure 6: A. Two examples of the CMU/MIT face database. B. Mutual Boosting and AdaBoost ROCs on the CMU/MIT face database. Face-scene images were downloaded from the web and manually labeled7. Training relied on 450 positive and negative examples (~4% of the images used by VJ). 400 iterations of local window AdaBoost and contextual window Mutual Boosting were performed on the same image set. Contextual windows encompassed a region five times larger in width and height than the local windows8 (see Figure 3). 7 By following CMU database conventions (R-eye, L-eye, Nose & Mouth positions) we derive both the local window position and the relative position of objects in the image 8 Local windows were created by downscaling objects to 25x25 grids Test image detection maps emerge from iteratively summing T m-detection layers (Mutual Boosting stages C&D;). ROC performance on the CMU/MIT face database (see sample images in Figure 6A) was assessed using a threshold on position Cm[i] that best discriminated the final positive and negative detection maps Hm+/-T. Figure 6B demonstrates the superiority of Mutual Boosting to grounded feature AdaBoost. A. COV 0.25 COV 1.00 COV 4.00 Equal error performance Our second experiment was aimed at assessing the performance of Mutual Boosting as we change the detected configurations’ variance. Assuming normal distribution of face configurations we estimated (from our existing labeled set) the spatial covariance between four facial components (noses, mouths and both eyes). We then modified the covariance matrix, multiplying it by 0.25, 1 or 4 and generated 100 artificial configurations by positioning four contrasting rectangles in the estimated position of facial components. Although both Mutual Boosting and AdaBoost performance degraded as the configuration variance increased, the advantage of Mutual Boosting persists both in rigid and in varying configurations9 (Figure 7). MB sigma=0.25 MB sigma=1.00 MB sigma=4.00 AB sigma=0.25 AB sigma=1.00 AB sigma=4.00 Boosting iteration Figure 7: A. Artificial face configurations with increasing covariance B. MB and AB Equal error rate performance on configurations with varying covariance as a function of boosting iterations. 6 D i s c u s s i on While evaluating the performance of Mutual Boosting it should be emphasized that we did not implement the VJ cascade approach; therefore we only attempt to demonstrate that the power of a single AdaBoost learner could be augmented by Mutual Boosting. The VJ detector is rescaled in order to perform efficient detection of objects in multiple scales. For simplicity, scale of neighboring objects and parts was assumed to be fixed so that a similar detector-rescaling approach could be followed. This assumption holds well for face-scenes, but if neighboring objects may vary in scale a single m-detection map will not suffice. However, by transforming each m-detection image to an m-detection cube, (having scale as the third dimension) multi-scale context detection could be achieved10. The dynamic programming characteristic of Mutual Boosting (simply reusing the multiple position and scale detections already performed by VJ) will ensure that the running time of varying scale context will only be doubled. It should be noted that the facescene is highly structured and therefore it is a good candidate for demonstrating 9 In this experiment the resolution of the MB windows (and the number of training features) was decreased so that information derived from the higher resolution of the parts would be ruled out as an explaining factor for the Mutual Boosting advantage. This procedure explains the superior AdaBoost performance in the first boosting iteration. 10 By using an integral cube, calculating the sum of a cube feature (of any size) requires 8 access operations (only double than the 4 operations required in the integral image case). Mutual Boosting; however as suggested by Figure 7B Mutual Boosting can handle highly varying configurations and the proposed method needs no modification when applied to other scenes, like the office scene in Figure 111. Notice that Mutual Boosting does not require a-priori knowledge of the compositional structure but rather permits structure to naturally emerge in the cross referencing pattern (see examples in Figure 5). Mutual Boosting could be enhanced by unifying the selection of weak-learners rather than selecting an individual weak learner for each object detector. Unified selection is aimed at choosing weak learners that maximize the entire object set detection rate, thus maximizing feature reuse [11]. This approach is optimal when many objects with common characteristics are trained. Is Mutual Boosting specific for image object detection? Indeed it requires labeled input of multiple objects in a scene supplying a local description of the objects as well as information on their contextual mutual positioning. But these criterions are shared by other complex</p><p>5 0.098722309 <a title="22-tfidf-5" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>Author: Tom Heskes, Onno Zoeter, Wim Wiegerinck</p><p>Abstract: We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach. 1</p><p>6 0.085555241 <a title="22-tfidf-6" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>7 0.084422246 <a title="22-tfidf-7" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>8 0.083139807 <a title="22-tfidf-8" href="./nips-2003-Perception_of_the_Structure_of_the_Physical_World_Using_Unknown_Multimodal_Sensors_and_Effectors.html">154 nips-2003-Perception of the Structure of the Physical World Using Unknown Multimodal Sensors and Effectors</a></p>
<p>9 0.077218175 <a title="22-tfidf-9" href="./nips-2003-Max-Margin_Markov_Networks.html">124 nips-2003-Max-Margin Markov Networks</a></p>
<p>10 0.077037655 <a title="22-tfidf-10" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>11 0.070660651 <a title="22-tfidf-11" href="./nips-2003-Human_and_Ideal_Observers_for_Detecting_Image_Curves.html">85 nips-2003-Human and Ideal Observers for Detecting Image Curves</a></p>
<p>12 0.069814578 <a title="22-tfidf-12" href="./nips-2003-Automatic_Annotation_of_Everyday_Movements.html">37 nips-2003-Automatic Annotation of Everyday Movements</a></p>
<p>13 0.067613252 <a title="22-tfidf-13" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>14 0.067344688 <a title="22-tfidf-14" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>15 0.061768085 <a title="22-tfidf-15" href="./nips-2003-Log-Linear_Models_for_Label_Ranking.html">121 nips-2003-Log-Linear Models for Label Ranking</a></p>
<p>16 0.061442886 <a title="22-tfidf-16" href="./nips-2003-Generalised_Propagation_for_Fast_Fourier_Transforms_with_Partial_or_Missing_Data.html">80 nips-2003-Generalised Propagation for Fast Fourier Transforms with Partial or Missing Data</a></p>
<p>17 0.05782887 <a title="22-tfidf-17" href="./nips-2003-Salient_Boundary_Detection_using_Ratio_Contour.html">168 nips-2003-Salient Boundary Detection using Ratio Contour</a></p>
<p>18 0.054600164 <a title="22-tfidf-18" href="./nips-2003-Learning_the_k_in_k-means.html">111 nips-2003-Learning the k in k-means</a></p>
<p>19 0.054454651 <a title="22-tfidf-19" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>20 0.053603292 <a title="22-tfidf-20" href="./nips-2003-Towards_Social_Robots%3A_Automatic_Evaluation_of_Human-Robot_Interaction_by_Facial_Expression_Classification.html">186 nips-2003-Towards Social Robots: Automatic Evaluation of Human-Robot Interaction by Facial Expression Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.183), (1, 0.039), (2, -0.003), (3, 0.077), (4, 0.143), (5, 0.117), (6, 0.123), (7, 0.053), (8, -0.041), (9, 0.036), (10, -0.032), (11, 0.096), (12, -0.063), (13, 0.024), (14, 0.024), (15, -0.024), (16, 0.028), (17, 0.01), (18, 0.02), (19, 0.092), (20, -0.077), (21, -0.032), (22, 0.14), (23, 0.05), (24, 0.046), (25, -0.056), (26, 0.035), (27, -0.075), (28, 0.025), (29, 0.215), (30, -0.154), (31, 0.083), (32, 0.081), (33, -0.006), (34, -0.027), (35, 0.047), (36, 0.07), (37, -0.015), (38, 0.074), (39, -0.028), (40, -0.165), (41, 0.043), (42, -0.01), (43, 0.011), (44, 0.063), (45, 0.03), (46, -0.034), (47, 0.024), (48, 0.093), (49, -0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91220051 <a title="22-lsi-1" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>Author: Claudio Fanti, Marzia Polito, Pietro Perona</p><p>Abstract: Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can eﬀortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body’s centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more signiﬁcantly, to the introduction of the centroid variable. 1</p><p>2 0.78968352 <a title="22-lsi-2" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>Author: Leonid Sigal, Michael Isard, Benjamin H. Sigelman, Michael J. Black</p><p>Abstract: The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuousvalued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle ﬁlter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter. 1</p><p>3 0.57414317 <a title="22-lsi-3" href="./nips-2003-Perception_of_the_Structure_of_the_Physical_World_Using_Unknown_Multimodal_Sensors_and_Effectors.html">154 nips-2003-Perception of the Structure of the Physical World Using Unknown Multimodal Sensors and Effectors</a></p>
<p>Author: D. Philipona, J.k. O'regan, J.-p. Nadal, Olivier Coenen</p><p>Abstract: Is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in? Taking the case of space for example, is there a way for this algorithm to realize that its body is in a three dimensional world? Is it possible for this algorithm to discover how to move in a straight line? And more basically: do these questions make any sense at all given that the algorithm only has access to the very high-dimensional data consisting of its sensory inputs and motor outputs? We demonstrate in this article how these questions can be given a positive answer. We show that it is possible to make an algorithm that, by analyzing the law that links its motor outputs to its sensory inputs, discovers information about the structure of the world regardless of the devices constituting the body it is linked to. We present results from simulations demonstrating a way to issue motor orders resulting in “fundamental” movements of the body as regards the structure of the physical world. 1</p><p>4 0.5606401 <a title="22-lsi-4" href="./nips-2003-Parameterized_Novelty_Detectors_for_Environmental_Sensor_Monitoring.html">153 nips-2003-Parameterized Novelty Detectors for Environmental Sensor Monitoring</a></p>
<p>Author: Cynthia Archer, Todd K. Leen, António M. Baptista</p><p>Abstract: As part of an environmental observation and forecasting system, sensors deployed in the Columbia RIver Estuary (CORIE) gather information on physical dynamics and changes in estuary habitat. Of these, salinity sensors are particularly susceptible to biofouling, which gradually degrades sensor response and corrupts critical data. Automatic fault detectors have the capability to identify bio-fouling early and minimize data loss. Complicating the development of discriminatory classiﬁers is the scarcity of bio-fouling onset examples and the variability of the bio-fouling signature. To solve these problems, we take a novelty detection approach that incorporates a parameterized bio-fouling model. These detectors identify the occurrence of bio-fouling, and its onset time as reliably as human experts. Real-time detectors installed during the summer of 2001 produced no false alarms, yet detected all episodes of sensor degradation before the ﬁeld staﬀ scheduled these sensors for cleaning. From this initial deployment through February 2003, our bio-fouling detectors have essentially doubled the amount of useful data coming from the CORIE sensors. 1</p><p>5 0.44804379 <a title="22-lsi-5" href="./nips-2003-Approximate_Expectation_Maximization.html">32 nips-2003-Approximate Expectation Maximization</a></p>
<p>Author: Tom Heskes, Onno Zoeter, Wim Wiegerinck</p><p>Abstract: We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach. 1</p><p>6 0.44532642 <a title="22-lsi-6" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>7 0.44315884 <a title="22-lsi-7" href="./nips-2003-Salient_Boundary_Detection_using_Ratio_Contour.html">168 nips-2003-Salient Boundary Detection using Ratio Contour</a></p>
<p>8 0.43397239 <a title="22-lsi-8" href="./nips-2003-Denoising_and_Untangling_Graphs_Using_Degree_Priors.html">50 nips-2003-Denoising and Untangling Graphs Using Degree Priors</a></p>
<p>9 0.43230921 <a title="22-lsi-9" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<p>10 0.40884531 <a title="22-lsi-10" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>11 0.39286432 <a title="22-lsi-11" href="./nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</a></p>
<p>12 0.37931219 <a title="22-lsi-12" href="./nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F.html">195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</a></p>
<p>13 0.36488307 <a title="22-lsi-13" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>14 0.36333215 <a title="22-lsi-14" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>15 0.35380697 <a title="22-lsi-15" href="./nips-2003-Automatic_Annotation_of_Everyday_Movements.html">37 nips-2003-Automatic Annotation of Everyday Movements</a></p>
<p>16 0.35355842 <a title="22-lsi-16" href="./nips-2003-Sample_Propagation.html">169 nips-2003-Sample Propagation</a></p>
<p>17 0.35006472 <a title="22-lsi-17" href="./nips-2003-Sensory_Modality_Segregation.html">175 nips-2003-Sensory Modality Segregation</a></p>
<p>18 0.34057987 <a title="22-lsi-18" href="./nips-2003-Approximability_of_Probability_Distributions.html">30 nips-2003-Approximability of Probability Distributions</a></p>
<p>19 0.33837381 <a title="22-lsi-19" href="./nips-2003-Tree-structured_Approximations_by_Expectation_Propagation.html">189 nips-2003-Tree-structured Approximations by Expectation Propagation</a></p>
<p>20 0.32501066 <a title="22-lsi-20" href="./nips-2003-Link_Prediction_in_Relational_Data.html">118 nips-2003-Link Prediction in Relational Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.109), (11, 0.01), (23, 0.273), (26, 0.026), (31, 0.068), (53, 0.05), (56, 0.033), (58, 0.042), (62, 0.062), (71, 0.01), (76, 0.142), (85, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78511566 <a title="22-lda-1" href="./nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</a></p>
<p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><p>same-paper 2 0.73342115 <a title="22-lda-2" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>Author: Claudio Fanti, Marzia Polito, Pietro Perona</p><p>Abstract: Consider a number of moving points, where each point is attached to a joint of the human body and projected onto an image plane. Johannson showed that humans can eﬀortlessly detect and recognize the presence of other humans from such displays. This is true even when some of the body points are missing (e.g. because of occlusion) and unrelated clutter points are added to the display. We are interested in replicating this ability in a machine. To this end, we present a labelling and detection scheme in a probabilistic framework. Our method is based on representing the joint probability density of positions and velocities of body points with a graphical model, and using Loopy Belief Propagation to calculate a likely interpretation of the scene. Furthermore, we introduce a global variable representing the body’s centroid. Experiments on one motion-captured sequence suggest that our scheme improves on the accuracy of a previous approach based on triangulated graphical models, especially when very few parts are visible. The improvement is due both to the more general graph structure we use and, more signiﬁcantly, to the introduction of the centroid variable. 1</p><p>3 0.6728639 <a title="22-lda-3" href="./nips-2003-Extreme_Components_Analysis.html">66 nips-2003-Extreme Components Analysis</a></p>
<p>Author: Max Welling, Christopher Williams, Felix V. Agakov</p><p>Abstract: Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for “extreme components analysis” (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efﬁcient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.</p><p>4 0.59521866 <a title="22-lda-4" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>Author: Alan Fern, Sungwook Yoon, Robert Givan</p><p>Abstract: We explore approximate policy iteration, replacing the usual costfunction learning step with a learning step in policy space. We give policy-language biases that enable solution of very large relational Markov decision processes (MDPs) that no previous technique can solve. In particular, we induce high-quality domain-speciﬁc planners for classical planning domains (both deterministic and stochastic variants) by solving such domains as extremely large MDPs. 1</p><p>5 0.59515941 <a title="22-lda-5" href="./nips-2003-Discriminating_Deformable_Shape_Classes.html">53 nips-2003-Discriminating Deformable Shape Classes</a></p>
<p>Author: Salvador Ruiz-correa, Linda G. Shapiro, Marina Meila, Gabriel Berson</p><p>Abstract: We present and empirically test a novel approach for categorizing 3-D free form object shapes represented by range data . In contrast to traditional surface-signature based systems that use alignment to match speciﬁc objects, we adapted the newly introduced symbolic-signature representation to classify deformable shapes [10]. Our approach constructs an abstract description of shape classes using an ensemble of classiﬁers that learn object class parts and their corresponding geometrical relationships from a set of numeric and symbolic descriptors. We used our classiﬁcation engine in a series of large scale discrimination experiments on two well-deﬁned classes that share many common distinctive features. The experimental results suggest that our method outperforms traditional numeric signature-based methodologies. 1 1</p><p>6 0.59500349 <a title="22-lda-6" href="./nips-2003-Using_the_Forest_to_See_the_Trees%3A_A_Graphical_Model_Relating_Features%2C_Objects%2C_and_Scenes.html">192 nips-2003-Using the Forest to See the Trees: A Graphical Model Relating Features, Objects, and Scenes</a></p>
<p>7 0.59493953 <a title="22-lda-7" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>8 0.59489632 <a title="22-lda-8" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>9 0.59466869 <a title="22-lda-9" href="./nips-2003-Increase_Information_Transfer_Rates_in_BCI_by_CSP_Extension_to_Multi-class.html">90 nips-2003-Increase Information Transfer Rates in BCI by CSP Extension to Multi-class</a></p>
<p>10 0.59450328 <a title="22-lda-10" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>11 0.59242648 <a title="22-lda-11" href="./nips-2003-Learning_a_World_Model_and_Planning_with_a_Self-Organizing%2C_Dynamic_Neural_System.html">110 nips-2003-Learning a World Model and Planning with a Self-Organizing, Dynamic Neural System</a></p>
<p>12 0.59223443 <a title="22-lda-12" href="./nips-2003-Wormholes_Improve_Contrastive_Divergence.html">196 nips-2003-Wormholes Improve Contrastive Divergence</a></p>
<p>13 0.59092671 <a title="22-lda-13" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>14 0.59079432 <a title="22-lda-14" href="./nips-2003-Learning_a_Rare_Event_Detection_Cascade_by_Direct_Feature_Selection.html">109 nips-2003-Learning a Rare Event Detection Cascade by Direct Feature Selection</a></p>
<p>15 0.59006768 <a title="22-lda-15" href="./nips-2003-Discriminative_Fields_for_Modeling_Spatial_Dependencies_in_Natural_Images.html">54 nips-2003-Discriminative Fields for Modeling Spatial Dependencies in Natural Images</a></p>
<p>16 0.58944881 <a title="22-lda-16" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>17 0.58883351 <a title="22-lda-17" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>18 0.58861446 <a title="22-lda-18" href="./nips-2003-Sequential_Bayesian_Kernel_Regression.html">176 nips-2003-Sequential Bayesian Kernel Regression</a></p>
<p>19 0.58791596 <a title="22-lda-19" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<p>20 0.58783376 <a title="22-lda-20" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
