<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-195" href="#">nips2003-195</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</h1>
<br/><p>Source: <a title="nips-2003-195-pdf" href="http://papers.nips.cc/paper/2463-when-does-non-negative-matrix-factorization-give-a-correct-decomposition-into-parts.pdf">pdf</a></p><p>Author: David Donoho, Victoria Stodden</p><p>Abstract: We interpret non-negative matrix factorization geometrically, as the problem of ﬁnding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of ‘parts’ and NMF correctly identiﬁes the ‘parts’. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases. 1</p><p>Reference: <a title="nips-2003-195-reference" href="../nips2003_reference/nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We interpret non-negative matrix factorization geometrically, as the problem of ﬁnding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. [sent-6, score-1.273]
</p><p>2 We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. [sent-7, score-0.646]
</p><p>3 We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. [sent-8, score-0.384]
</p><p>4 We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases. [sent-10, score-0.236]
</p><p>5 1  Introduction  In a recent article in Nature [4], Lee and Seung proposed the notion of non-negative matrix factorization (NMF) as a way to ﬁnd a set of basis functions for representing non-negative data. [sent-11, score-0.224]
</p><p>6 They claimed that the notion is particularly applicable to image articulation libraries made up of images showing a composite object in many articulations and poses. [sent-12, score-0.402]
</p><p>7 NMF is akin to other matrix decompositions which have been proposed previously, such as positive matrix factorization (PMF) of Juvela, Lehtinen, and Paatero [3], [2] and various minimum-volume transforms used in the analysis of remote-sensing data [1]. [sent-14, score-0.239]
</p><p>8 Despite all the literature and discussion of this method, two fundamental questions appear not to have been posed clearly, let alone answered: • Under what assumptions is the notion of non-negative matrix factorization welldeﬁned, for example is the factorization in some sense unique? [sent-16, score-0.377]
</p><p>9 • Under what assumptions is the factorization correct, recovering the ‘right answer’? [sent-17, score-0.15]
</p><p>10 In this paper, we develop a geometric view of the setting underlying NMF factorization and derive geometric conditions under which the factorization is essentially unique, so NMF makes sense no matter what algorithm is being employed. [sent-18, score-0.401]
</p><p>11 We then consider those conditions in the setting of image articulation libraries. [sent-19, score-0.234]
</p><p>12 We describe a class of image libraries which are created by an NMF-style generative model, where different parts have separate support, and where all different combinations of parts are exhaustively sampled. [sent-20, score-0.335]
</p><p>13 Our theory shows that, in such Separable Factorial Articulation Families, non-negative factorization is effectively unique. [sent-21, score-0.15]
</p><p>14 We construct such a library, showing a stick ﬁgure with four limbs going through a range of various motions, and verify that our theoretical analysis is predictive of the actual performance of the Lee and Seung algorithm on this image library. [sent-23, score-0.147]
</p><p>15 Our viewpoint also explains relations between NMF and other ideas for obtaining non-negative factorizations and explains why uniqueness and stability may fail under other conditions. [sent-24, score-0.14]
</p><p>16 We note that Plumbley [5] has in some sense already validated NMF for datasets which are not only non-negative but which obey an independent components model. [sent-25, score-0.06]
</p><p>17 For example, for the kinds of image articulation families where each part is viewed in one of many positions, the underlying exclusion principle – that a certain part can only be present in one particular articulation – guarantees that an ICA model does not apply. [sent-27, score-0.422]
</p><p>18 2  Non-Negative Matrix Factorization  NMF seeks to decompose a non-negative n × p matrix X, where each row contains the p pixel values for one of the n images, into X = AΨ  (1)  where A is n × r and Ψ is r × p, and both A and Ψ have non-negative entries. [sent-29, score-0.061]
</p><p>19 The rows of Ψ, denoted (ψj )r , are basis elements in Rp and the rows of A, (αi )n , belong to Rr and j=1 i=1 can be thought of as coefﬁcient sequences representing the images in that basis. [sent-30, score-0.09]
</p><p>20 Recalling that the rows of X, (xi ), are individual images stored as row vectors, the representation takes the form r  i  i αj ψj . [sent-31, score-0.062]
</p><p>21 What is less clear is whether, when the generative model actually holds and we generate a synthetic dataset based on that model, the NMF matrix factorization of the dataset will yield underlying basis elements which have some connection to the true generative elements. [sent-51, score-0.395]
</p><p>22 In this paper we investigate this question and exhibit conditions under which NMF will in fact successfully recover the true generative elements. [sent-52, score-0.08]
</p><p>23 3  Geometric Interpretation of the NMF Setting  We now describe a geometric viewpoint which will help explain the issues involved. [sent-53, score-0.072]
</p><p>24 Each image in our database of images can be thought of as a point in a p-dimensional space, whose p coordinates are given by the intensity values in each of the p pixels. [sent-54, score-0.151]
</p><p>25 The fact that image data are non-negative means that every such point lies in the positive orthant P of Rp . [sent-55, score-0.241]
</p><p>26 The factorization X = AΨ says that there are vectors ψj in Rp such that all the data points xi have a representation as non-negative linear combinations of the ψj . [sent-56, score-0.288]
</p><p>27 The simplicial cone generated by vectors Φ = (φj )r is j=1 Γ = ΓΦ = {x : x =  αj φj , αj ≥ 0}. [sent-59, score-0.966]
</p><p>28 j  The factorization (1) tells us geometrically that the (xi ) all lie in the simplicial cone ΣΨ generated by the (ψj ). [sent-60, score-1.152]
</p><p>29 Now in general, for a given dataset (xi ), there will be many possible simplicial cones containing the points in that dataset. [sent-61, score-0.719]
</p><p>30 Indeed, if ΓΨ is a simplicial cone containing the data, and ΓΦ is another cone containing the ﬁrst, so that ΓΨ ⊂ ΓΦ , then the corresponding vectors Φ = (φj ) also can furnish a representation of the dataset (xi ). [sent-62, score-1.498]
</p><p>31 Now for any simplicial cone, there can always be another cone containing it strictly, so there are an inﬁnite number of factorizations X = AΨ with non-negative A, and various Ψ which are nontrivially different. [sent-63, score-1.013]
</p><p>32 However, the geometric viewpoint we are developing does not so far include the positivity constraint Ψ ≥ 0 on the generating vectors of the simplicial cone. [sent-65, score-0.719]
</p><p>33 Geometrically, this constraint demands that the simplicial cone ΓΨ lies inside the positive orthant P. [sent-66, score-1.19]
</p><p>34 (3)  Geometrically, this condition places the data points xi well inside the interior of the positive orthant P. [sent-69, score-0.276]
</p><p>35 It is then evident by visual inspection that there will be many simplicial cones containing the data. [sent-70, score-0.653]
</p><p>36 For example, P itself is a simplicial cone, and it contains the data points. [sent-71, score-0.571]
</p><p>37 However, many other cones will also contain the data points. [sent-72, score-0.072]
</p><p>38 Indeed, for δ > 0 consider the collection of vectors Φδ with individual vectors φδ = ej + δ1 j where ej denotes the usual vector in the standard basis, and 1 denotes the vector of all ones. [sent-73, score-0.14]
</p><p>39 Then, for δ < , the cone ΓΦδ also contains all the data points. [sent-74, score-0.438]
</p><p>40 Geometrically ΓΦδ is a dilation of the positive orthant that shrinks it slightly towards the main diagonal. [sent-75, score-0.164]
</p><p>41 Since the positivity constraint (3) places all the data well inside the interior of the positive orthant, for slight enough shrinkage it will still contain the data. [sent-76, score-0.182]
</p><p>42 In short, we must look for situations where the data do not obey strict positivity in order to have uniqueness. [sent-78, score-0.117]
</p><p>43 4  An Example of Uniqueness  When we take the non-negativity constraint on the generating elements (the extreme rays of the simplicial cone) into account, it can happen that there will only be one simplicial cone containing the data. [sent-79, score-1.923]
</p><p>44 What is perhaps surprising is that uniqueness can hold even when the data only ‘ﬁll out’ a proper subset of the positive orthant. [sent-81, score-0.116]
</p><p>45 There is a unique simplicial cone which both contains C and is itself contained in the positive orthant. [sent-85, score-1.119]
</p><p>46 Indeed that unique cone is P itself; no simplicial cone contained inside P contains all of C! [sent-86, score-1.523]
</p><p>47 To give a full proof, we introduce notions from the subject of convex duality [8]. [sent-87, score-0.128]
</p><p>48 Associated with the primal domain of points x we have been dealing with so far, there is also the dual domain of linear functionals ξ acting on points x via ξ x. [sent-88, score-0.229]
</p><p>49 If we have a convex set C, its dual C ∗ is deﬁned as a collection of linear functionals which are positive on C: C ∗ = {ξ : ξ x ≥ 0 ∀x ∈ C} The following facts are easily veriﬁed: Lemma 2. [sent-89, score-0.236]
</p><p>50 • The dual of a simplicial cone with p linearly independent generators, is another simplicial cone with p generators. [sent-91, score-1.993]
</p><p>51 Given a pointset (xi ), its conical hull is the simplicial hull generated by the vectors (xi ) themselves. [sent-95, score-0.823]
</p><p>52 An abstraction of the NMF problem is: Primal-Simplicial-Cone(r, X ) Find a simplicial cone with r generators contained in P and containing X . [sent-97, score-1.301]
</p><p>53 Consider now a problem in the dual domain, posed with reversed inclusions: Dual-Simplicial-Cone(r, Ξ) Find a simplicial cone with r generators contained in Ξ and containing P . [sent-98, score-1.406]
</p><p>54 Every solution to Primal-Simplicial-Cone(r, X ) is dual to a solution of DualSimplicial-Cone(r, X ∗ ), and vice-versa. [sent-100, score-0.08]
</p><p>55 Suppose we ﬁnd a simplicial cone Γ obeying X ⊂ Γ ⊂ P. [sent-103, score-1.018]
</p><p>56 Then (4) says that  P ∗ ⊂ Γ∗ ⊂ X ∗ , and so a solution to the primal solves the dual. [sent-104, score-0.106]
</p><p>57 In the other direction, if we ﬁnd a simplicial cone Γ∗ obeying P ∗ ⊂ Γ∗ ⊂ X ∗ then we have by (4) (X ∗ )∗ ⊂ (Γ∗ )∗ ⊂ (P ∗ )∗ ; ∗ ∗ we simply apply (K ) = K three times to see that a solution to the dual corresponds to a solution to the primal. [sent-105, score-1.098]
</p><p>58 QED Our motivation in introducing duality is to see something we couldn’t in the primal: we can see that even if X is properly contained in P , there can be a unique simplicial hull for X which lies inside P. [sent-106, score-0.914]
</p><p>59 This follows from a simple observation about simplicial cones contained in convex cones. [sent-107, score-0.715]
</p><p>60 An extreme ray of a convex cone Γ is a ray Rx = {ax : a ≥ 0} where x ∈ Γ cannot be represented as a proper convex combination of two points x0 and x1 which belong to Γ but not Rx . [sent-109, score-0.804]
</p><p>61 For example, a simplicial cone with r linearly independent generators has r extreme rays; each ray consists of all positive multiples of one generator. [sent-110, score-1.456]
</p><p>62 Suppose that Γ and G are convex cones, that Γ ⊂ G ⊂ Rr , that Γ is a simplicial cone with r generators and that G intersects Γ in exactly r rays which are extreme rays of G. [sent-112, score-1.909]
</p><p>63 Then (a) these rays are also extreme rays of Γ and (b) no simplicial cone with r generators Γ = Γ can satisfy Γ ⊂ Γ ⊂ G. [sent-113, score-1.825]
</p><p>64 (a) Since the rays in question are extreme rays of G, which contains Γ, they are also extreme rays of Γ. [sent-115, score-1.039]
</p><p>65 (b) Any simplicial cone Γ with r generators and lying ‘in between’ Γ and G would have to also intersect G in the same r rays as Γ does. [sent-116, score-1.449]
</p><p>66 Those r rays would also have to be extreme rays for Γ , because they are extreme rays for G, which by hypothesis contains Γ . [sent-117, score-1.039]
</p><p>67 But a simplicial cone with r generators is completely determined by its r extreme rays. [sent-118, score-1.321]
</p><p>68 Its dual is C ∗ = {ξ : ξ 1 ≥ ||ξ||} Note (a) that every boundary ray of C ∗ is extreme; and (b) that C ∗ intersects P ∗ on the n unit vectors ej . [sent-122, score-0.237]
</p><p>69 5  Uniqueness for Separable Factorial Articulation Families  We now describe families of articulated images which have at least a few ‘realistic’ features, and which, because of the relevant convex geometry, offer an essentially unique NMF. [sent-125, score-0.21]
</p><p>70 The families of images we have in mind consist of black-and-white images with P parts, each exercised systematically through A articulations. [sent-126, score-0.175]
</p><p>71 As an illustration, Figure 1 shows some sample images from the Swimmer dataset, which depicts a ﬁgure with four moving parts (limbs), each able to exhibit four articulations (different positions). [sent-127, score-0.191]
</p><p>72 A Separable Factorial Articulation Family is a collection X of points x obeying these rules: [R1] Generative Model. [sent-129, score-0.13]
</p><p>73 Each image x in the database has a representation P  A  x=  αq,a ψq,a q=1 a=1  where the generators ψq,a ∈ Rp obey the non-negativity constraint ψq,a ≥ 0 along with the coefﬁcients αq,a ≥ 0. [sent-130, score-0.434]
</p><p>74 The dataset contains all AP images in which the P parts appear in all combinations of A articulations. [sent-137, score-0.235]
</p><p>75 Figure 1: Sample images from the Swimmer database depicting four stick ﬁgures with four limbs; the panels illustrate different articulations of the limbs. [sent-138, score-0.179]
</p><p>76 The Swimmer dataset obeys these rules except for one disagreement: every image contains an invariant region (the torso). [sent-139, score-0.16]
</p><p>77 We note that assumption [R2] forces the generators ψq,a to be linearly independent, which forces p > A·P . [sent-141, score-0.335]
</p><p>78 Consequently, the linear span of the generators is some subspace V ⊂ Rp . [sent-142, score-0.258]
</p><p>79 Given a database obeying rules [R1]-[R3], there is a unique simplicial hull with r = A · P generators which contains all the points of the database, and is contained in P ∩ V . [sent-144, score-1.211]
</p><p>80 Any factorization obeying (1) and (2) must recover the correct generators (ψq,a ) modulo permutation of labels and rescaling. [sent-148, score-0.487]
</p><p>81 We need to introduce the notion of duality relative to a vector space V ⊂ Rp . [sent-150, score-0.106]
</p><p>82 In the case of V ≡ Rp this is just the notion of duality already introduced. [sent-151, score-0.106]
</p><p>83 Suppose that we have a set K ⊂ V ; its relative dual K v is the set of linear functionals ξ which, viewed as members of Rp also belong to V , and which obey ξ x ≥ 0 for x ∈ K. [sent-152, score-0.215]
</p><p>84 In effect, the relative dual is the ordinary dual taken within V rather than Rp . [sent-153, score-0.16]
</p><p>85 As a result, all the properties of Lemma 2 hold for relative duality provided we talk about sets which are subsets of V ; e. [sent-154, score-0.08]
</p><p>86 Deﬁne PV = V ∩ P; this is a simplicial cone in V with r generators. [sent-157, score-0.939]
</p><p>87 Let again X denote the conical hull of X = (xi ) and suppose that every (r−1)-dimensional face of PV contains r − 1 linearly independent points from X. [sent-158, score-0.3]
</p><p>88 Since the face of a cone is a linear subspace, the face is uniquely determined by these r − 1 points. [sent-159, score-0.453]
</p><p>89 The face is part of a supporting hyperplane to PV which is also a supporting hyperplane to X . [sent-160, score-0.183]
</p><p>90 The v supporting hyperplane deﬁnes a point ξ ∈ V which is in common between the duals PV and X v . [sent-161, score-0.079]
</p><p>91 Because of the linear independence mentioned above, the different supporting hyperplanes in primal space correspond in fact to extreme rays in dual space – extreme rays for both v PV and X v . [sent-164, score-0.933]
</p><p>92 This gives the conclusion that PV is the v v unique simplicial cone with r generators contained in X and containing PV . [sent-166, score-1.35]
</p><p>93 QED  7  Empirical Veriﬁcation  We built the Swimmer image library of 256 32×32 images. [sent-177, score-0.089]
</p><p>94 Each image contains a ‘torso’ of 12 pixels in the center and four ‘arms’ of 6 pixels that can be in one of 4 positions. [sent-178, score-0.09]
</p><p>95 Figure 2 shows that the 16 different part/articulation pairs are properly resolved, but that the torso is not properly resolved. [sent-183, score-0.13]
</p><p>96 The 16 images shown agree well with the known list of generators (4 ‘limbs’ in 4 positions each). [sent-185, score-0.351]
</p><p>97 an invariant region) violates our conditions for a Factorial Separable Articulation Library, and, not unexpectedly, ghosts of the torso contaminate several of the reconstructed generators. [sent-188, score-0.097]
</p><p>98 The use of positive matrix factorization in the analysis of molecular line spectra from the thumbprint nebula. [sent-200, score-0.213]
</p><p>99 The use of positive matrix factorization in the analysis of molecular line spectra. [sent-209, score-0.213]
</p><p>100 Learning the parts of objects by non-negative matrix factorization. [sent-214, score-0.101]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('simplicial', 0.536), ('cone', 0.403), ('nmf', 0.299), ('generators', 0.258), ('rays', 0.252), ('articulation', 0.158), ('factorization', 0.15), ('orthant', 0.127), ('pv', 0.126), ('extreme', 0.124), ('hull', 0.103), ('rp', 0.099), ('swimmer', 0.091), ('factorial', 0.088), ('dual', 0.08), ('duality', 0.08), ('obeying', 0.079), ('uniqueness', 0.079), ('torso', 0.076), ('parts', 0.075), ('cones', 0.072), ('geometrically', 0.063), ('limbs', 0.063), ('ray', 0.063), ('images', 0.062), ('obey', 0.06), ('generative', 0.059), ('contained', 0.059), ('positivity', 0.057), ('image', 0.055), ('articulations', 0.054), ('conical', 0.054), ('juvela', 0.054), ('lehtinen', 0.054), ('qed', 0.054), ('supporting', 0.053), ('seung', 0.053), ('families', 0.051), ('unique', 0.049), ('lemma', 0.049), ('primal', 0.048), ('convex', 0.048), ('libraries', 0.047), ('functionals', 0.047), ('containing', 0.045), ('separable', 0.044), ('geometric', 0.04), ('dataset', 0.039), ('inside', 0.038), ('positive', 0.037), ('aerosol', 0.036), ('hopke', 0.036), ('intersects', 0.036), ('malm', 0.036), ('polissar', 0.036), ('says', 0.036), ('contains', 0.035), ('linearly', 0.035), ('library', 0.034), ('database', 0.034), ('stanford', 0.033), ('viewpoint', 0.032), ('alaska', 0.032), ('geophysical', 0.032), ('donoho', 0.032), ('ej', 0.031), ('positions', 0.031), ('rules', 0.031), ('lee', 0.029), ('rr', 0.029), ('factorizations', 0.029), ('stick', 0.029), ('atmospheric', 0.029), ('belong', 0.028), ('constraint', 0.027), ('properly', 0.027), ('vectors', 0.027), ('points', 0.027), ('hyperplane', 0.026), ('notion', 0.026), ('matrix', 0.026), ('face', 0.025), ('posed', 0.025), ('rx', 0.025), ('ap', 0.025), ('combinations', 0.024), ('collection', 0.024), ('xi', 0.024), ('faces', 0.024), ('interior', 0.023), ('synthetic', 0.023), ('lies', 0.022), ('solves', 0.022), ('article', 0.022), ('inclusion', 0.022), ('suppose', 0.021), ('forces', 0.021), ('conditions', 0.021), ('indeed', 0.021), ('ll', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="195-tfidf-1" href="./nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F.html">195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</a></p>
<p>Author: David Donoho, Victoria Stodden</p><p>Abstract: We interpret non-negative matrix factorization geometrically, as the problem of ﬁnding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of ‘parts’ and NMF correctly identiﬁes the ‘parts’. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases. 1</p><p>2 0.24048972 <a title="195-tfidf-2" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>Author: Mark Girolami, Ata Kabán</p><p>Abstract: To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the deﬁnition of individual speciﬁc and global models. This paper proposes a linear-time distributed model for ﬁnite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained’ by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-speciﬁc proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efﬁcient representation scheme, reﬂected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.</p><p>3 0.12064378 <a title="195-tfidf-3" href="./nips-2003-Unsupervised_Color_Decomposition_Of_Histologically_Stained_Tissue_Samples.html">190 nips-2003-Unsupervised Color Decomposition Of Histologically Stained Tissue Samples</a></p>
<p>Author: Andrew Rabinovich, Sameer Agarwal, Casey Laris, Jeffrey H. Price, Serge J. Belongie</p><p>Abstract: Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the ﬁrst automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results. 1</p><p>4 0.0656102 <a title="195-tfidf-4" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>Author: Amit Gruber, Yair Weiss</p><p>Abstract: The problem of “Structure From Motion” is a central problem in vision: given the 2D locations of certain points we wish to recover the camera motion and the 3D coordinates of the points. Under simpliﬁed camera models, the problem reduces to factorizing a measurement matrix into the product of two low rank matrices. Each element of the measurement matrix contains the position of a point in a particular image. When all elements are observed, the problem can be solved trivially using SVD, but in any realistic situation many elements of the matrix are missing and the ones that are observed have a diﬀerent directional uncertainty. Under these conditions, most existing factorization algorithms fail while human perception is relatively unchanged. In this paper we use the well known EM algorithm for factor analysis to perform factorization. This allows us to easily handle missing data and measurement uncertainty and more importantly allows us to place a prior on the temporal trajectory of the latent variables (the camera position). We show that incorporating this prior gives a signiﬁcant improvement in performance in challenging image sequences. 1</p><p>5 0.063411549 <a title="195-tfidf-5" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>Author: Victor Lavrenko, R. Manmatha, Jiwoon Jeon</p><p>Abstract: We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model signiﬁcantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval. 1</p><p>6 0.053731896 <a title="195-tfidf-6" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<p>7 0.051120769 <a title="195-tfidf-7" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>8 0.049446102 <a title="195-tfidf-8" href="./nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</a></p>
<p>9 0.047113668 <a title="195-tfidf-9" href="./nips-2003-Image_Reconstruction_by_Linear_Programming.html">88 nips-2003-Image Reconstruction by Linear Programming</a></p>
<p>10 0.037730046 <a title="195-tfidf-10" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>11 0.037241813 <a title="195-tfidf-11" href="./nips-2003-Feature_Selection_in_Clustering_Problems.html">73 nips-2003-Feature Selection in Clustering Problems</a></p>
<p>12 0.036535427 <a title="195-tfidf-12" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<p>13 0.035542771 <a title="195-tfidf-13" href="./nips-2003-Eye_Micro-movements_Improve_Stimulus_Detection_Beyond_the_Nyquist_Limit_in_the_Peripheral_Retina.html">67 nips-2003-Eye Micro-movements Improve Stimulus Detection Beyond the Nyquist Limit in the Peripheral Retina</a></p>
<p>14 0.034564879 <a title="195-tfidf-14" href="./nips-2003-Semi-Definite_Programming_by_Perceptron_Learning.html">171 nips-2003-Semi-Definite Programming by Perceptron Learning</a></p>
<p>15 0.033098426 <a title="195-tfidf-15" href="./nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</a></p>
<p>16 0.03272865 <a title="195-tfidf-16" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>17 0.032350987 <a title="195-tfidf-17" href="./nips-2003-Sparse_Representation_and_Its_Applications_in_Blind_Source_Separation.html">179 nips-2003-Sparse Representation and Its Applications in Blind Source Separation</a></p>
<p>18 0.031893063 <a title="195-tfidf-18" href="./nips-2003-Mutual_Boosting_for_Contextual_Inference.html">133 nips-2003-Mutual Boosting for Contextual Inference</a></p>
<p>19 0.031534009 <a title="195-tfidf-19" href="./nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">155 nips-2003-Perspectives on Sparse Bayesian Learning</a></p>
<p>20 0.031303767 <a title="195-tfidf-20" href="./nips-2003-Design_of_Experiments_via_Information_Theory.html">51 nips-2003-Design of Experiments via Information Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2003_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.114), (1, -0.045), (2, 0.007), (3, -0.015), (4, -0.093), (5, -0.007), (6, 0.046), (7, -0.033), (8, -0.005), (9, -0.085), (10, -0.024), (11, -0.073), (12, -0.108), (13, 0.156), (14, -0.063), (15, 0.022), (16, -0.074), (17, 0.028), (18, -0.026), (19, -0.076), (20, -0.13), (21, 0.067), (22, -0.16), (23, -0.073), (24, -0.02), (25, 0.001), (26, -0.01), (27, 0.031), (28, 0.153), (29, 0.146), (30, 0.231), (31, 0.01), (32, -0.157), (33, -0.084), (34, -0.124), (35, 0.121), (36, 0.04), (37, 0.208), (38, 0.048), (39, -0.029), (40, -0.125), (41, 0.188), (42, -0.109), (43, -0.069), (44, -0.014), (45, -0.009), (46, -0.06), (47, -0.026), (48, -0.218), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95531046 <a title="195-lsi-1" href="./nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F.html">195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</a></p>
<p>Author: David Donoho, Victoria Stodden</p><p>Abstract: We interpret non-negative matrix factorization geometrically, as the problem of ﬁnding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of ‘parts’ and NMF correctly identiﬁes the ‘parts’. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases. 1</p><p>2 0.66270018 <a title="195-lsi-2" href="./nips-2003-Simplicial_Mixtures_of_Markov_Chains%3A_Distributed_Modelling_of_Dynamic_User_Profiles.html">177 nips-2003-Simplicial Mixtures of Markov Chains: Distributed Modelling of Dynamic User Profiles</a></p>
<p>Author: Mark Girolami, Ata Kabán</p><p>Abstract: To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the deﬁnition of individual speciﬁc and global models. This paper proposes a linear-time distributed model for ﬁnite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained’ by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-speciﬁc proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efﬁcient representation scheme, reﬂected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations.</p><p>3 0.50197375 <a title="195-lsi-3" href="./nips-2003-Unsupervised_Color_Decomposition_Of_Histologically_Stained_Tissue_Samples.html">190 nips-2003-Unsupervised Color Decomposition Of Histologically Stained Tissue Samples</a></p>
<p>Author: Andrew Rabinovich, Sameer Agarwal, Casey Laris, Jeffrey H. Price, Serge J. Belongie</p><p>Abstract: Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the ﬁrst automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results. 1</p><p>4 0.37030062 <a title="195-lsi-4" href="./nips-2003-Modeling_User_Rating_Profiles_For_Collaborative_Filtering.html">131 nips-2003-Modeling User Rating Profiles For Collaborative Filtering</a></p>
<p>Author: Benjamin M. Marlin</p><p>Abstract: In this paper we present a generative latent variable model for rating-based collaborative ﬁltering called the User Rating Proﬁle model (URP). The generative process which underlies URP is designed to produce complete user rating proﬁles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each. 1</p><p>5 0.33565283 <a title="195-lsi-5" href="./nips-2003-Bayesian_Color_Constancy_with_Non-Gaussian_Models.html">39 nips-2003-Bayesian Color Constancy with Non-Gaussian Models</a></p>
<p>Author: Charles Rosenberg, Alok Ladsariya, Tom Minka</p><p>Abstract: We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reﬂectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.</p><p>6 0.28702784 <a title="195-lsi-6" href="./nips-2003-A_Model_for_Learning_the_Semantics_of_Pictures.html">12 nips-2003-A Model for Learning the Semantics of Pictures</a></p>
<p>7 0.24355522 <a title="195-lsi-7" href="./nips-2003-From_Algorithmic_to_Subjective_Randomness.html">75 nips-2003-From Algorithmic to Subjective Randomness</a></p>
<p>8 0.23204806 <a title="195-lsi-8" href="./nips-2003-A_Nonlinear_Predictive_State_Representation.html">14 nips-2003-A Nonlinear Predictive State Representation</a></p>
<p>9 0.22572684 <a title="195-lsi-9" href="./nips-2003-Factorization_with_Uncertainty_and_Missing_Data%3A_Exploiting_Temporal_Coherence.html">69 nips-2003-Factorization with Uncertainty and Missing Data: Exploiting Temporal Coherence</a></p>
<p>10 0.22007471 <a title="195-lsi-10" href="./nips-2003-Eye_Movements_for_Reward_Maximization.html">68 nips-2003-Eye Movements for Reward Maximization</a></p>
<p>11 0.20398876 <a title="195-lsi-11" href="./nips-2003-A_Sampled_Texture_Prior_for_Image_Super-Resolution.html">17 nips-2003-A Sampled Texture Prior for Image Super-Resolution</a></p>
<p>12 0.20316282 <a title="195-lsi-12" href="./nips-2003-Error_Bounds_for_Transductive_Learning_via_Compression_and_Clustering.html">63 nips-2003-Error Bounds for Transductive Learning via Compression and Clustering</a></p>
<p>13 0.20277126 <a title="195-lsi-13" href="./nips-2003-Sparseness_of_Support_Vector_Machines---Some_Asymptotically_Sharp_Bounds.html">180 nips-2003-Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds</a></p>
<p>14 0.19665229 <a title="195-lsi-14" href="./nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</a></p>
<p>15 0.19380502 <a title="195-lsi-15" href="./nips-2003-Perception_of_the_Structure_of_the_Physical_World_Using_Unknown_Multimodal_Sensors_and_Effectors.html">154 nips-2003-Perception of the Structure of the Physical World Using Unknown Multimodal Sensors and Effectors</a></p>
<p>16 0.18192424 <a title="195-lsi-16" href="./nips-2003-Semidefinite_Relaxations_for_Approximate_Inference_on_Graphs_with_Cycles.html">174 nips-2003-Semidefinite Relaxations for Approximate Inference on Graphs with Cycles</a></p>
<p>17 0.18093018 <a title="195-lsi-17" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>18 0.17820857 <a title="195-lsi-18" href="./nips-2003-Fast_Algorithms_for_Large-State-Space_HMMs_with_Applications_to_Web_Usage_Analysis.html">70 nips-2003-Fast Algorithms for Large-State-Space HMMs with Applications to Web Usage Analysis</a></p>
<p>19 0.1753567 <a title="195-lsi-19" href="./nips-2003-An_Improved_Scheme_for_Detection_and_Labelling_in_Johansson_Displays.html">22 nips-2003-An Improved Scheme for Detection and Labelling in Johansson Displays</a></p>
<p>20 0.16991447 <a title="195-lsi-20" href="./nips-2003-Convex_Methods_for_Transduction.html">48 nips-2003-Convex Methods for Transduction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2003_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.049), (11, 0.034), (30, 0.018), (35, 0.042), (53, 0.063), (66, 0.462), (69, 0.011), (71, 0.039), (76, 0.024), (85, 0.051), (91, 0.086), (99, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85375118 <a title="195-lda-1" href="./nips-2003-When_Does_Non-Negative_Matrix_Factorization_Give_a_Correct_Decomposition_into_Parts%3F.html">195 nips-2003-When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts?</a></p>
<p>Author: David Donoho, Victoria Stodden</p><p>Abstract: We interpret non-negative matrix factorization geometrically, as the problem of ﬁnding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that under certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sampling. For such databases there is a generative model in terms of ‘parts’ and NMF correctly identiﬁes the ‘parts’. We show that our theoretical results are predictive of the performance of published NMF code, by running the published algorithms on one of our synthetic image articulation databases. 1</p><p>2 0.63934249 <a title="195-lda-2" href="./nips-2003-Training_fMRI_Classifiers_to_Detect_Cognitive_States_across_Multiple_Human_Subjects.html">188 nips-2003-Training fMRI Classifiers to Detect Cognitive States across Multiple Human Subjects</a></p>
<p>Author: Xuerui Wang, Rebecca Hutchinson, Tom M. Mitchell</p><p>Abstract: We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classiﬁers constitute “virtual sensors” of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classiﬁers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classiﬁers that can be applied across multiple human subjects, including subjects who were not involved in training the classiﬁer. We describe the design of several machine learning approaches to training multiple-subject classiﬁers, and report experimental results demonstrating the success of these methods in learning cross-subject classiﬁers for two different fMRI data sets. 1</p><p>3 0.63500637 <a title="195-lda-3" href="./nips-2003-Attractive_People%3A_Assembling_Loose-Limbed_Models_using_Non-parametric_Belief_Propagation.html">35 nips-2003-Attractive People: Assembling Loose-Limbed Models using Non-parametric Belief Propagation</a></p>
<p>Author: Leonid Sigal, Michael Isard, Benjamin H. Sigelman, Michael J. Black</p><p>Abstract: The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuousvalued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle ﬁlter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter. 1</p><p>4 0.59747094 <a title="195-lda-4" href="./nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Michael I. Jordan</p><p>Abstract: We propose a novel method of dimensionality reduction for supervised learning. Given a regression or classiﬁcation problem in which we wish to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of ﬁnding a low-dimensional “effective subspace” of X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem, we characterize the notion of conditional independence using covariance operators on reproducing kernel Hilbert spaces; this allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . 1</p><p>5 0.42729452 <a title="195-lda-5" href="./nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</a></p>
<p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><p>6 0.3344332 <a title="195-lda-6" href="./nips-2003-Linear_Dependent_Dimensionality_Reduction.html">115 nips-2003-Linear Dependent Dimensionality Reduction</a></p>
<p>7 0.3314108 <a title="195-lda-7" href="./nips-2003-Learning_Spectral_Clustering.html">107 nips-2003-Learning Spectral Clustering</a></p>
<p>8 0.32116029 <a title="195-lda-8" href="./nips-2003-Unsupervised_Color_Decomposition_Of_Histologically_Stained_Tissue_Samples.html">190 nips-2003-Unsupervised Color Decomposition Of Histologically Stained Tissue Samples</a></p>
<p>9 0.3179931 <a title="195-lda-9" href="./nips-2003-Online_Learning_via_Global_Feedback_for_Phrase_Recognition.html">147 nips-2003-Online Learning via Global Feedback for Phrase Recognition</a></p>
<p>10 0.30760306 <a title="195-lda-10" href="./nips-2003-Learning_Non-Rigid_3D_Shape_from_2D_Motion.html">106 nips-2003-Learning Non-Rigid 3D Shape from 2D Motion</a></p>
<p>11 0.30647972 <a title="195-lda-11" href="./nips-2003-A_Kullback-Leibler_Divergence_Based_Kernel_for_SVM_Classification_in_Multimedia_Applications.html">9 nips-2003-A Kullback-Leibler Divergence Based Kernel for SVM Classification in Multimedia Applications</a></p>
<p>12 0.30457392 <a title="195-lda-12" href="./nips-2003-Computing_Gaussian_Mixture_Models_with_EM_Using_Equivalence_Constraints.html">47 nips-2003-Computing Gaussian Mixture Models with EM Using Equivalence Constraints</a></p>
<p>13 0.29662541 <a title="195-lda-13" href="./nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">78 nips-2003-Gaussian Processes in Reinforcement Learning</a></p>
<p>14 0.29375526 <a title="195-lda-14" href="./nips-2003-Policy_Search_by_Dynamic_Programming.html">158 nips-2003-Policy Search by Dynamic Programming</a></p>
<p>15 0.29233614 <a title="195-lda-15" href="./nips-2003-Learning_with_Local_and_Global_Consistency.html">113 nips-2003-Learning with Local and Global Consistency</a></p>
<p>16 0.29152671 <a title="195-lda-16" href="./nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</a></p>
<p>17 0.29066333 <a title="195-lda-17" href="./nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</a></p>
<p>18 0.2888051 <a title="195-lda-18" href="./nips-2003-Approximate_Policy_Iteration_with_a_Policy_Language_Bias.html">34 nips-2003-Approximate Policy Iteration with a Policy Language Bias</a></p>
<p>19 0.28816676 <a title="195-lda-19" href="./nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">55 nips-2003-Distributed Optimization in Adaptive Networks</a></p>
<p>20 0.28778937 <a title="195-lda-20" href="./nips-2003-Geometric_Analysis_of_Constrained_Curves.html">81 nips-2003-Geometric Analysis of Constrained Curves</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
