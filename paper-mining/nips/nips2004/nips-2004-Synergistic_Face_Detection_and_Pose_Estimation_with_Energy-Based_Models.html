<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-182" href="#">nips2004-182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</h1>
<br/><p>Source: <a title="nips-2004-182-pdf" href="http://papers.nips.cc/paper/2638-synergistic-face-detection-and-pose-estimation-with-energy-based-models.pdf">pdf</a></p><p>Author: Margarita Osadchy, Matthew L. Miller, Yann L. Cun</p><p>Abstract: We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. We test the resulting system, in a single conﬁguration, on three standard data sets – one for frontal pose, one for rotated faces, and one for proﬁles – and ﬁnd that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. We also show experimentally that the system’s accuracy on both face detection and pose estimation is improved by training for the two tasks together.</p><p>Reference: <a title="nips-2004-182-reference" href="../nips2004_reference/nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. [sent-6, score-0.824]
</p><p>2 The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. [sent-7, score-0.612]
</p><p>3 This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. [sent-8, score-0.154]
</p><p>4 We test the resulting system, in a single conﬁguration, on three standard data sets – one for frontal pose, one for rotated faces, and one for proﬁles – and ﬁnd that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. [sent-9, score-0.752]
</p><p>5 We also show experimentally that the system’s accuracy on both face detection and pose estimation is improved by training for the two tasks together. [sent-10, score-0.867]
</p><p>6 1 Introduction The detection of human faces in natural images and videos is a key component in a wide variety of applications of human-computer interaction, search and indexing, security, and surveillance. [sent-11, score-0.468]
</p><p>7 Many real-world applications would proﬁt from multi-view detectors that can detect faces under a wide range of poses: looking left or right (yaw axis), up or down (pitch axis), or tilting left or right (roll axis). [sent-12, score-0.312]
</p><p>8 In this paper we describe a novel method that not only detects faces independently of their poses, but simultaneously estimates those poses. [sent-13, score-0.283]
</p><p>9 The system is highly-reliable, runs at near real time (5 frames per second on standard hardware), and is robust against variations in yaw (±90◦ ), roll (±45◦ ), and pitch (±60◦ ). [sent-14, score-0.46]
</p><p>10 The method is motivated by the idea that multi-view face detection and pose estimation are so closely related that they should not be performed separately. [sent-15, score-0.801]
</p><p>11 The tasks are related in the sense that they must be robust against the same sorts of variation: skin color, glasses, facial hair, lighting, scale, expressions, etc. [sent-16, score-0.109]
</p><p>12 To exploit the synergy between these two tasks, we train a convolutional network to map face images to points on a face manifold, and non-face images to points far away from that manifold. [sent-18, score-0.983]
</p><p>13 Conceptually, we can view the pose parameter as a latent variable that can be inferred through an energy-minimization process [4]. [sent-20, score-0.327]
</p><p>14 To train the machine we derive a new type of discriminative loss function that is tailored to such detection tasks. [sent-21, score-0.254]
</p><p>15 Previous Work: Learning-based approaches to face detection abound, including real-time methods [16], and approaches based on convolutional networks [15, 3]. [sent-22, score-0.571]
</p><p>16 Most multi-view systems take a view-based approach, which involves building separate detectors for different views and either applying them in parallel [10, 14, 13, 7] or using a pose estimator to select a detector [5]. [sent-23, score-0.502]
</p><p>17 Another approach is to estimate and correct in-plane rotations before applying a single pose-speciﬁc detector [12]. [sent-24, score-0.121]
</p><p>18 Closer to our approach is that of [8], in which a number of Support Vector Regressors are trained to approximate smooth functions, each of which has a maximum for a face at a particular pose. [sent-25, score-0.317]
</p><p>19 Another machine is trained to convert the resulting values to estimates of poses, and a third is trained to convert the values into a face/non-face score. [sent-26, score-0.172]
</p><p>20 2 Integrating face detection and pose estimation To exploit the posited synergy between face detection and pose estimation, we must design a system that integrates the solutions to the two problems. [sent-28, score-1.722]
</p><p>21 Our approach is to build a trainable system that can map raw images X to points in a low-dimensional space. [sent-31, score-0.166]
</p><p>22 In that space, we pre-deﬁne a face manifold F (Z) that we parameterize by the pose Z. [sent-32, score-0.727]
</p><p>23 We train the system to map face images with known poses to the corresponding points on the manifold. [sent-33, score-0.516]
</p><p>24 We also train it to map non-face images to points far away from the manifold. [sent-34, score-0.11]
</p><p>25 Proximity to the manifold then tells us whether or not an image is a face, and projection to the manifold yields an estimate of the pose. [sent-35, score-0.334]
</p><p>26 Parameterizing the Face Manifold: We will now describe the details of the parameterizations of the face manifold. [sent-36, score-0.295]
</p><p>27 Let’s start with the simplest case of one pose parameter Z = θ, representing, say, yaw. [sent-37, score-0.327]
</p><p>28 If we want to preserve the natural topology and geometry of the problem, the face manifold under yaw variations in the interval [−90◦ , 90◦ ] should be a half circle (with constant curvature). [sent-38, score-0.593]
</p><p>29 The same idea can be applied to any number of pose parameters. [sent-41, score-0.327]
</p><p>30 Let us consider the set of all faces with yaw in [−90, 90] and roll in [−45, 45]. [sent-42, score-0.471]
</p><p>31 Consequently, we encode the pose with the product of the cosines of the two angles: Fij (θ, φ) = cos(θ − αi ) cos(φ − βj ); i, j = 1, 2, 3; (3) For convenience we rescale the roll angles to the range of [−90, 90]. [sent-44, score-0.432]
</p><p>32 With these parameterizations, the manifold has constant curvature, which ensures that the effect of errors will be the same regardless of pose. [sent-45, score-0.134]
</p><p>33 Given nine components of the network’s output Gij (X), we compute the corresponding pose angles as follows: cc = ij Gij (X) cos(αi ) cos(βj ); cs = ij Gij (X) cos(αi ) sin(βj ) sc = Gij (X) sin(αi ) cos(βj ); ss = ij ij Gij (X) sin(αi ) sin(βj ) (4) θ = 0. [sent-46, score-0.673]
</p><p>34 5(atan2(cs + sc, cc − ss) + atan2(sc − cs, cc + ss)) φ = 0. [sent-47, score-0.144]
</p><p>35 5(atan2(cs + sc, cc − ss) − atan2(sc − cs, cc + ss)) Note that the dimension of the face manifold is much lower than that of the embedding space. [sent-48, score-0.544]
</p><p>36 If X is a face with pose Z, then we want: EW (1, Z, X) EW (0, Z , X) for any pose Z , and EW (1, Z , X) EW (1, Z, X) for any pose Z = Z. [sent-53, score-1.247]
</p><p>37 Operating the machine consists in clamping X to the observed value (the image), and ﬁnding the values of Z and Y that minimize EW (Y, Z, X): (Y , Z) = argminY ∈{Y }, Z∈{Z} EW (Y, Z, X)  (5)  where {Y } = {0, 1} and {Z} = [−90, 90]×[−45, 45] for yaw and roll variables. [sent-54, score-0.234]
</p><p>38 The complete energy function is: EW (Y, Z, X) = Y GW (X) − F (Z) + (1 − Y )T  (7)  The architecture of the machine is depicted in Figure 1. [sent-61, score-0.143]
</p><p>39 Operating this machine (ﬁnding the output label and pose with the smallest energy) comes down to ﬁrst ﬁnding: Z = argminZ∈{Z} ||GW (X) − F (Z)||, and then comparing this minimum distance, GW (X) − F (Z) , to the threshold T . [sent-62, score-0.327]
</p><p>40 Convolutional networks [6] are “endto-end” trainable system that can operate on raw pixel images and learn low-level features and high-level representation in an integrated fashion. [sent-66, score-0.142]
</p><p>41 We employ a network architecture similar to LeNet5 [6]. [sent-69, score-0.135]
</p><p>42 In our architecture we have 8 feature maps in the bottom convolutional and subsampling layers and 20 maps in the next two layers. [sent-71, score-0.226]
</p><p>43 The last layer has 9 outputs to encode two pose parameters. [sent-72, score-0.327]
</p><p>44 where S1 is the set of training faces, S0 the set of non-faces, L1 (W, Z i , X i ) and L0 (W, X i ) are loss functions for a face sample (with a known pose) and non-face, respectively1 . [sent-74, score-0.34]
</p><p>45 To cause the machine to achieve the desired behavior, we need the parameter update to decrease the difference between the energy of the desired label and the energy of the undesired label. [sent-80, score-0.173]
</p><p>46 for a face example (X, Z, 1), we must have: EW (1, Z, X) < EW (1, Z, X) For a non-face example (X, 1), we must have: EW (1, Z, X) > EW (1, Z, X) We choose the following forms for L1 and L0 : L1 (W, 1, Z, X) = EW (1, Z, X)2 ;  L0 (W, 0, X) = K exp[−E(1, Z, X)]  (9)  where K is a positive constant. [sent-82, score-0.266]
</p><p>47 1 Although face samples whose pose is unknown can easily be accommodated, we will not discuss this possibility here. [sent-89, score-0.593]
</p><p>48 Running the Machine: Our detection system works on grayscale images and it applies √ the network to each image at a range of scales, stepping by a factor of 2. [sent-95, score-0.456]
</p><p>49 The network is replicated over the image at each scale, stepping by 4 pixels in x and y (this step size is a consequence of having two, 2x2 subsampling layers). [sent-96, score-0.219]
</p><p>50 At each scale and location, the network outputs are compared to the closest point on the manifold, and the system collects a list of all instances closer than our detection threshold. [sent-97, score-0.316]
</p><p>51 The system can detect, locate, and estimate the pose of faces that are between 40 and 250 pixels high in a 640 × 480 image at roughly 5 frames per second on a 2. [sent-101, score-0.741]
</p><p>52 4 Experiments and results Using the above architecture, we built a detector to locate faces and estimate two pose parameters: yaw from left to right proﬁle, and in-plane rotation from −45 to 45 degrees. [sent-103, score-0.898]
</p><p>53 The machine was trained to be robust against pitch variation. [sent-104, score-0.131]
</p><p>54 The ﬁrst set of experiments tests whether training for the two tasks together improves performance on both. [sent-106, score-0.116]
</p><p>55 Training: Our training set consisted of 52, 850, 32x32-pixel faces from natural images collected at NEC Labs and hand annotated with appropriate facial poses (see [9] for a description of how the annotation was done). [sent-108, score-0.504]
</p><p>56 These faces were selected from a much larger annotated set to yield a roughly uniform distribution of poses from left proﬁle to right proﬁle, with as much variation in pitch as we could obtain. [sent-109, score-0.403]
</p><p>57 Our initial negative training data consisted of 52, 850 image patches chosen randomly from non-face areas of a variety of images. [sent-110, score-0.105]
</p><p>58 For our second set of tests, we replaced half of these with image patches obtained by running the initial version of the detector on our training images and collecting false detections. [sent-111, score-0.324]
</p><p>59 Each training image was used 5 times during training, with random variations  95  Percentage of poses correctly estimated  100  95  Percentage of faces detected  100  90 85 80 75 70 65 Frontal Rotated in plane Profile  60 55 50  0  0. [sent-112, score-0.528]
</p><p>60 Left: ROC curves for our detector on the three data sets. [sent-118, score-0.1]
</p><p>61 The x axis is the average number of false positives per image over all three sets, so each point corresponds to a single detection threshold. [sent-119, score-0.407]
</p><p>62 Right: frequency with which yaw and roll are estimated within various error tolerances. [sent-120, score-0.259]
</p><p>63 At the end of training, the network had converged to an equal error rate of 5% on the training data and 6% on a separate test set of 90,000 images. [sent-126, score-0.107]
</p><p>64 Synergy tests: The goal of the synergy test was to verify that both face detection and pose estimation beneﬁt from learning and running in parallel. [sent-127, score-0.88]
</p><p>65 The ﬁrst one was trained for simultaneous face detection and pose estimation (combined), the second was trained for detection only and the third for pose estimation only. [sent-129, score-1.438]
</p><p>66 The “pose only” network was identical to the combined network, but trained on faces only (no negative examples). [sent-131, score-0.356]
</p><p>67 In both these graphs, we see that the pose-plus-detection network had better performance, conﬁrming that training for each task beneﬁts the other. [sent-133, score-0.107]
</p><p>68 Standard data sets: There is no standard data set that tests all the poses our system is designed to detect. [sent-134, score-0.215]
</p><p>69 There are, however, data sets that have been used to test more restricted face detectors, each set focusing on a particular variation in pose. [sent-135, score-0.29]
</p><p>70 The details of these sets are described below: • MIT+CMU [14, 11] – 130 images for testing frontal face detectors. [sent-138, score-0.389]
</p><p>71 We count 517 faces in this set, but the standard tests only use a subset of 507 faces, because 10 faces are in the wrong pose or otherwise not suitable for the test. [sent-139, score-0.851]
</p><p>72 (Note: about 2% of the faces in the standard subset are badly-drawn cartoons, which we do not intend our system to detect. [sent-140, score-0.292]
</p><p>73 ) • TILTED [12] – 50 images of frontal faces with in-plane rotations. [sent-142, score-0.336]
</p><p>74 (Note: about 20% of the faces in the standard subset are outside of the ±45◦ rotation range for which our system is designed. [sent-144, score-0.343]
</p><p>75 There seems to be some disagreement about the number of faces in the standard set of annotations: [13] reports using 347 faces of the 462 that we found, [5] reports using 355, and we found 353 annotations. [sent-147, score-0.474]
</p><p>76 We counted a face as being detected if 1) at least one detection lay within a circle centered on the midpoint between the eyes, with a radius equal to 1. [sent-149, score-0.539]
</p><p>77 25 times the distance from that point to the midpoint of the mouth, and 2) that detection came at a scale within a factor of  Figure 4: Some example face detections. [sent-150, score-0.469]
</p><p>78 Data set → False positives per image → Our detector Jones & Viola [5] (tilted) Jones & Viola [5] (proﬁle) Rowley et al [11] Schneiderman & Kanade [13]  TILTED 4. [sent-154, score-0.247]
</p><p>79 Each column shows the detection rates for a given average number of false positives per image (these rates correspond to those for which other authors have reported results). [sent-161, score-0.377]
</p><p>80 Note that ours is the only single detector that can be tested on all data sets simultaneously. [sent-163, score-0.124]
</p><p>81 We counted a detection as a false positive if it did not lie within this range for any of the faces in the image, including those faces not in the standard subset. [sent-165, score-0.733]
</p><p>82 Table 1 shows our detection rates compared against other systems for which results were given on these data sets. [sent-168, score-0.171]
</p><p>83 Those detectors, however, are not designed to handle all variations in pose, and do not yield pose estimates. [sent-170, score-0.387]
</p><p>84 The right side of Figure 3 shows our performance at pose estimation. [sent-171, score-0.327]
</p><p>85 To make this graph, we ﬁxed the detection threshold at a value that resulted in about 0. [sent-172, score-0.171]
</p><p>86 5 false positives per image over all three data sets. [sent-173, score-0.206]
</p><p>87 We then compared the pose estimates for all detected faces (including those not in the standard subsets) against our manual pose annotations. [sent-174, score-0.977]
</p><p>88 Note that this test is more difﬁcult than typical tests of pose estimation systems, where faces are ﬁrst localized by hand. [sent-175, score-0.651]
</p><p>89 5 Conclusion The system we have presented here integrates detection and pose estimation by training a convolutional network to map faces to points on a manifold, parameterized by pose, and non-faces to points far from the manifold. [sent-177, score-1.115]
</p><p>90 The network is trained by optimizing a loss function of three variables – image, pose, and face/non-face label. [sent-178, score-0.154]
</p><p>91 When the three variables match, the energy function is trained to have a small value, when they do not match, it is  trained to have a large value. [sent-179, score-0.178]
</p><p>92 This system has several desirable properties: • The use of a convolutional network makes it fast. [sent-180, score-0.257]
</p><p>93 • It is robust to a wide range of poses, including variations in yaw up to ±90◦ , in-plane rotation up to ±45◦ , and pitch up to ±60◦ . [sent-183, score-0.324]
</p><p>94 This has been veriﬁed with tests on three standard data sets, each designed to test robustness against a single dimension of pose variation. [sent-184, score-0.402]
</p><p>95 On the standard data sets, the estimates of yaw and in-plane rotation are within 15◦ of manual estimates over 80% and 95% of the time, respectively. [sent-186, score-0.278]
</p><p>96 We have shown experimentally that our system’s accuracy at both pose estimation and face detection is increased by training for the two tasks together. [sent-187, score-0.867]
</p><p>97 A neural architecture for fast and robust face detection. [sent-202, score-0.355]
</p><p>98 Support vector regression and classiﬁcation based multi-view face detection and recognition. [sent-237, score-0.437]
</p><p>99 A statistical method for 3d object detection applied to faces and cars. [sent-268, score-0.408]
</p><p>100 Rapid object detection using a boosted cascade of simple features. [sent-284, score-0.171]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ew', 0.663), ('pose', 0.327), ('face', 0.266), ('faces', 0.237), ('detection', 0.171), ('yaw', 0.158), ('convolutional', 0.134), ('manifold', 0.134), ('gw', 0.109), ('detector', 0.1), ('poses', 0.085), ('synergy', 0.079), ('energy', 0.076), ('roll', 0.076), ('detectors', 0.075), ('cos', 0.073), ('profile', 0.073), ('cc', 0.072), ('network', 0.068), ('architecture', 0.067), ('image', 0.066), ('gij', 0.063), ('tilted', 0.063), ('facial', 0.06), ('images', 0.06), ('false', 0.059), ('pro', 0.058), ('ss', 0.058), ('pitch', 0.058), ('sc', 0.055), ('system', 0.055), ('yaws', 0.054), ('rotation', 0.051), ('trained', 0.051), ('rowley', 0.051), ('tests', 0.05), ('sin', 0.05), ('positives', 0.05), ('cs', 0.048), ('detected', 0.041), ('nec', 0.04), ('jones', 0.04), ('le', 0.04), ('training', 0.039), ('frontal', 0.039), ('viola', 0.039), ('estimation', 0.037), ('lush', 0.036), ('stepping', 0.036), ('labs', 0.036), ('loss', 0.035), ('variations', 0.035), ('pentium', 0.033), ('kanade', 0.032), ('midpoint', 0.032), ('yann', 0.032), ('percentage', 0.031), ('per', 0.031), ('axis', 0.03), ('angles', 0.029), ('parameterizations', 0.029), ('courant', 0.029), ('counted', 0.029), ('schneiderman', 0.029), ('trainable', 0.027), ('tasks', 0.027), ('roc', 0.026), ('train', 0.026), ('locate', 0.025), ('subsampling', 0.025), ('baluja', 0.025), ('estimated', 0.025), ('frames', 0.025), ('designed', 0.025), ('sets', 0.024), ('estimates', 0.024), ('curvature', 0.024), ('bottou', 0.024), ('replicated', 0.024), ('cmu', 0.024), ('map', 0.024), ('integrates', 0.023), ('convert', 0.023), ('annotated', 0.023), ('detections', 0.023), ('discriminative', 0.022), ('closest', 0.022), ('america', 0.022), ('tolerance', 0.022), ('detects', 0.022), ('princeton', 0.022), ('box', 0.022), ('robust', 0.022), ('manual', 0.021), ('rotated', 0.021), ('pami', 0.021), ('rotations', 0.021), ('ij', 0.021), ('recognition', 0.021), ('update', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="182-tfidf-1" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Matthew L. Miller, Yann L. Cun</p><p>Abstract: We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. We test the resulting system, in a single conﬁguration, on three standard data sets – one for frontal pose, one for rotated faces, and one for proﬁles – and ﬁnd that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. We also show experimentally that the system’s accuracy on both face detection and pose estimation is improved by training for the two tasks together.</p><p>2 0.12937868 <a title="182-tfidf-2" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>Author: Wolf Kienzle, Matthias O. Franz, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: This paper proposes a method for computing fast approximations to support vector decision functions in the ﬁeld of object detection. In the present approach we are building on an existing algorithm where the set of support vectors is replaced by a smaller, so-called reduced set of synthesized input space points. In contrast to the existing method that ﬁnds the reduced set via unconstrained optimization, we impose a structural constraint on the synthetic points such that the resulting approximations can be evaluated via separable ﬁlters. For applications that require scanning large images, this decreases the computational complexity by a signiﬁcant amount. Experimental results show that in face detection, rank deﬁcient approximations are 4 to 6 times faster than unconstrained reduced set systems. 1</p><p>3 0.12103352 <a title="182-tfidf-3" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>Author: Shai Avidan, Moshe Butman</p><p>Abstract: We give a fast rejection scheme that is based on image segments and demonstrate it on the canonical example of face detection. However, instead of focusing on the detection step we focus on the rejection step and show that our method is simple and fast to be learned, thus making it an excellent pre-processing step to accelerate standard machine learning classiﬁers, such as neural-networks, Bayes classiﬁers or SVM. We decompose a collection of face images into regions of pixels with similar behavior over the image set. The relationships between the mean and variance of image segments are used to form a cascade of rejectors that can reject over 99.8% of image patches, thus only a small fraction of the image patches must be passed to a full-scale classiﬁer. Moreover, the training time for our method is much less than an hour, on a standard PC. The shape of the features (i.e. image segments) we use is data-driven, they are very cheap to compute and they form a very low dimensional feature space in which exhaustive search for the best features is tractable. 1</p><p>4 0.11018725 <a title="182-tfidf-4" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>Author: Tamara L. Berg, Alexander C. Berg, Jaety Edwards, David A. Forsyth</p><p>Abstract: The context in which a name appears in a caption provides powerful cues as to who is depicted in the associated image. We obtain 44,773 face images, using a face detector, from approximately half a million captioned news images and automatically link names, obtained using a named entity recognizer, with these faces. A simple clustering method can produce fair results. We improve these results signiﬁcantly by combining the clustering process with a model of the probability that an individual is depicted given its context. Once the labeling procedure is over, we have an accurately labeled set of faces, an appearance model for each individual depicted, and a natural language model that can produce accurate results on captions in isolation. 1</p><p>5 0.10957612 <a title="182-tfidf-5" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>Author: Yoshua Bengio, Martin Monperrus</p><p>Abstract: We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation suggests to explore non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions. A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails. 1</p><p>6 0.10444531 <a title="182-tfidf-6" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>7 0.10054053 <a title="182-tfidf-7" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>8 0.095425889 <a title="182-tfidf-8" href="./nips-2004-Joint_Tracking_of_Pose%2C_Expression%2C_and_Texture_using_Conditionally_Gaussian_Filters.html">91 nips-2004-Joint Tracking of Pose, Expression, and Texture using Conditionally Gaussian Filters</a></p>
<p>9 0.08594881 <a title="182-tfidf-9" href="./nips-2004-Incremental_Learning_for_Visual_Tracking.html">83 nips-2004-Incremental Learning for Visual Tracking</a></p>
<p>10 0.083639659 <a title="182-tfidf-10" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>11 0.074595265 <a title="182-tfidf-11" href="./nips-2004-Multiple_Relational_Embedding.html">125 nips-2004-Multiple Relational Embedding</a></p>
<p>12 0.074032046 <a title="182-tfidf-12" href="./nips-2004-Maximum_Likelihood_Estimation_of_Intrinsic_Dimension.html">114 nips-2004-Maximum Likelihood Estimation of Intrinsic Dimension</a></p>
<p>13 0.067358211 <a title="182-tfidf-13" href="./nips-2004-Discrete_profile_alignment_via_constrained_information_bottleneck.html">52 nips-2004-Discrete profile alignment via constrained information bottleneck</a></p>
<p>14 0.063000306 <a title="182-tfidf-14" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>15 0.062144291 <a title="182-tfidf-15" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<p>16 0.062070943 <a title="182-tfidf-16" href="./nips-2004-Seeing_through_water.html">160 nips-2004-Seeing through water</a></p>
<p>17 0.059522584 <a title="182-tfidf-17" href="./nips-2004-Learning_Hyper-Features_for_Visual_Identification.html">99 nips-2004-Learning Hyper-Features for Visual Identification</a></p>
<p>18 0.059366271 <a title="182-tfidf-18" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>19 0.056907095 <a title="182-tfidf-19" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<p>20 0.054772682 <a title="182-tfidf-20" href="./nips-2004-Joint_MRI_Bias_Removal_Using_Entropy_Minimization_Across_Images.html">89 nips-2004-Joint MRI Bias Removal Using Entropy Minimization Across Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.161), (1, 0.034), (2, -0.053), (3, -0.168), (4, 0.123), (5, 0.029), (6, 0.054), (7, -0.106), (8, 0.002), (9, 0.019), (10, -0.041), (11, -0.086), (12, -0.04), (13, -0.003), (14, -0.097), (15, -0.035), (16, -0.072), (17, 0.045), (18, 0.046), (19, -0.029), (20, -0.041), (21, 0.028), (22, -0.103), (23, -0.102), (24, 0.154), (25, 0.129), (26, 0.004), (27, 0.003), (28, -0.074), (29, -0.015), (30, 0.16), (31, -0.082), (32, 0.209), (33, 0.031), (34, 0.021), (35, -0.146), (36, 0.07), (37, 0.094), (38, 0.038), (39, -0.038), (40, -0.035), (41, 0.101), (42, 0.026), (43, -0.018), (44, 0.002), (45, -0.069), (46, 0.041), (47, 0.127), (48, 0.109), (49, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96007377 <a title="182-lsi-1" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Matthew L. Miller, Yann L. Cun</p><p>Abstract: We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. We test the resulting system, in a single conﬁguration, on three standard data sets – one for frontal pose, one for rotated faces, and one for proﬁles – and ﬁnd that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. We also show experimentally that the system’s accuracy on both face detection and pose estimation is improved by training for the two tasks together.</p><p>2 0.5571391 <a title="182-lsi-2" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>Author: Shai Avidan, Moshe Butman</p><p>Abstract: We give a fast rejection scheme that is based on image segments and demonstrate it on the canonical example of face detection. However, instead of focusing on the detection step we focus on the rejection step and show that our method is simple and fast to be learned, thus making it an excellent pre-processing step to accelerate standard machine learning classiﬁers, such as neural-networks, Bayes classiﬁers or SVM. We decompose a collection of face images into regions of pixels with similar behavior over the image set. The relationships between the mean and variance of image segments are used to form a cascade of rejectors that can reject over 99.8% of image patches, thus only a small fraction of the image patches must be passed to a full-scale classiﬁer. Moreover, the training time for our method is much less than an hour, on a standard PC. The shape of the features (i.e. image segments) we use is data-driven, they are very cheap to compute and they form a very low dimensional feature space in which exhaustive search for the best features is tractable. 1</p><p>3 0.5426302 <a title="182-lsi-3" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>Author: Wolf Kienzle, Matthias O. Franz, Bernhard Schölkopf, Gökhan H. Bakir</p><p>Abstract: This paper proposes a method for computing fast approximations to support vector decision functions in the ﬁeld of object detection. In the present approach we are building on an existing algorithm where the set of support vectors is replaced by a smaller, so-called reduced set of synthesized input space points. In contrast to the existing method that ﬁnds the reduced set via unconstrained optimization, we impose a structural constraint on the synthetic points such that the resulting approximations can be evaluated via separable ﬁlters. For applications that require scanning large images, this decreases the computational complexity by a signiﬁcant amount. Experimental results show that in face detection, rank deﬁcient approximations are 4 to 6 times faster than unconstrained reduced set systems. 1</p><p>4 0.53829283 <a title="182-lsi-4" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>Author: Oliver Williams, Andrew Blake, Roberto Cipolla</p><p>Abstract: There has been substantial progress in the past decade in the development of object classiﬁers for images, for example of faces, humans and vehicles. Here we address the problem of contaminations (e.g. occlusion, shadows) in test images which have not explicitly been encountered in training data. The Variational Ising Classiﬁer (VIC) algorithm models contamination as a mask (a ﬁeld of binary variables) with a strong spatial coherence prior. Variational inference is used to marginalize over contamination and obtain robust classiﬁcation. In this way the VIC approach can turn a kernel classiﬁer for clean data into one that can tolerate contamination, without any speciﬁc training on contaminated positives. 1</p><p>5 0.52522653 <a title="182-lsi-5" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>Author: Tamara L. Berg, Alexander C. Berg, Jaety Edwards, David A. Forsyth</p><p>Abstract: The context in which a name appears in a caption provides powerful cues as to who is depicted in the associated image. We obtain 44,773 face images, using a face detector, from approximately half a million captioned news images and automatically link names, obtained using a named entity recognizer, with these faces. A simple clustering method can produce fair results. We improve these results signiﬁcantly by combining the clustering process with a model of the probability that an individual is depicted given its context. Once the labeling procedure is over, we have an accurately labeled set of faces, an appearance model for each individual depicted, and a natural language model that can produce accurate results on captions in isolation. 1</p><p>6 0.47301337 <a title="182-lsi-6" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>7 0.4161005 <a title="182-lsi-7" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<p>8 0.40701175 <a title="182-lsi-8" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>9 0.39649856 <a title="182-lsi-9" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>10 0.38689721 <a title="182-lsi-10" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>11 0.3496649 <a title="182-lsi-11" href="./nips-2004-Maximum_Likelihood_Estimation_of_Intrinsic_Dimension.html">114 nips-2004-Maximum Likelihood Estimation of Intrinsic Dimension</a></p>
<p>12 0.33441088 <a title="182-lsi-12" href="./nips-2004-Seeing_through_water.html">160 nips-2004-Seeing through water</a></p>
<p>13 0.33233798 <a title="182-lsi-13" href="./nips-2004-Joint_Tracking_of_Pose%2C_Expression%2C_and_Texture_using_Conditionally_Gaussian_Filters.html">91 nips-2004-Joint Tracking of Pose, Expression, and Texture using Conditionally Gaussian Filters</a></p>
<p>14 0.32929954 <a title="182-lsi-14" href="./nips-2004-Modeling_Conversational_Dynamics_as_a_Mixed-Memory_Markov_Process.html">120 nips-2004-Modeling Conversational Dynamics as a Mixed-Memory Markov Process</a></p>
<p>15 0.30569023 <a title="182-lsi-15" href="./nips-2004-Contextual_Models_for_Object_Detection_Using_Boosted_Random_Fields.html">47 nips-2004-Contextual Models for Object Detection Using Boosted Random Fields</a></p>
<p>16 0.29918391 <a title="182-lsi-16" href="./nips-2004-Proximity_Graphs_for_Clustering_and_Manifold_Learning.html">150 nips-2004-Proximity Graphs for Clustering and Manifold Learning</a></p>
<p>17 0.29586032 <a title="182-lsi-17" href="./nips-2004-Learning_Hyper-Features_for_Visual_Identification.html">99 nips-2004-Learning Hyper-Features for Visual Identification</a></p>
<p>18 0.2931397 <a title="182-lsi-18" href="./nips-2004-Generative_Affine_Localisation_and_Tracking.html">73 nips-2004-Generative Affine Localisation and Tracking</a></p>
<p>19 0.2683537 <a title="182-lsi-19" href="./nips-2004-Discrete_profile_alignment_via_constrained_information_bottleneck.html">52 nips-2004-Discrete profile alignment via constrained information bottleneck</a></p>
<p>20 0.26296547 <a title="182-lsi-20" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.239), (13, 0.101), (15, 0.132), (17, 0.033), (25, 0.014), (26, 0.059), (31, 0.021), (33, 0.179), (35, 0.025), (50, 0.034), (51, 0.013), (76, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88239551 <a title="182-lda-1" href="./nips-2004-Sharing_Clusters_among_Related_Groups%3A_Hierarchical_Dirichlet_Processes.html">169 nips-2004-Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes</a></p>
<p>Author: Yee W. Teh, Michael I. Jordan, Matthew J. Beal, David M. Blei</p><p>Abstract: We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.</p><p>2 0.86463052 <a title="182-lda-2" href="./nips-2004-Schema_Learning%3A_Experience-Based_Construction_of_Predictive_Action_Models.html">159 nips-2004-Schema Learning: Experience-Based Construction of Predictive Action Models</a></p>
<p>Author: Michael P. Holmes, Charles Jr.</p><p>Abstract: Schema learning is a way to discover probabilistic, constructivist, predictive action models (schemas) from experience. It includes methods for ﬁnding and using hidden state to make predictions more accurate. We extend the original schema mechanism [1] to handle arbitrary discrete-valued sensors, improve the original learning criteria to handle POMDP domains, and better maintain hidden state by using schema predictions. These extensions show large improvement over the original schema mechanism in several rewardless POMDPs, and achieve very low prediction error in a difﬁcult speech modeling task. Further, we compare extended schema learning to the recently introduced predictive state representations [2], and ﬁnd their predictions of next-step action effects to be approximately equal in accuracy. This work lays the foundation for a schema-based system of integrated learning and planning. 1</p><p>same-paper 3 0.84066415 <a title="182-lda-3" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Matthew L. Miller, Yann L. Cun</p><p>Abstract: We describe a novel method for real-time, simultaneous multi-view face detection and facial pose estimation. The method employs a convolutional network to map face images to points on a manifold, parametrized by pose, and non-face images to points far from that manifold. This network is trained by optimizing a loss function of three variables: image, pose, and face/non-face label. We test the resulting system, in a single conﬁguration, on three standard data sets – one for frontal pose, one for rotated faces, and one for proﬁles – and ﬁnd that its performance on each set is comparable to previous multi-view face detectors that can only handle one form of pose variation. We also show experimentally that the system’s accuracy on both face detection and pose estimation is improved by training for the two tasks together.</p><p>4 0.72420579 <a title="182-lda-4" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. 1</p><p>5 0.71677196 <a title="182-lda-5" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><p>6 0.7154026 <a title="182-lda-6" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>7 0.71427435 <a title="182-lda-7" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>8 0.71414542 <a title="182-lda-8" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>9 0.71328449 <a title="182-lda-9" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>10 0.71101487 <a title="182-lda-10" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>11 0.70949173 <a title="182-lda-11" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>12 0.70915681 <a title="182-lda-12" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>13 0.70903617 <a title="182-lda-13" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>14 0.70897549 <a title="182-lda-14" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>15 0.70779556 <a title="182-lda-15" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>16 0.7072646 <a title="182-lda-16" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>17 0.70705074 <a title="182-lda-17" href="./nips-2004-Newscast_EM.html">130 nips-2004-Newscast EM</a></p>
<p>18 0.70656985 <a title="182-lda-18" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>19 0.70651042 <a title="182-lda-19" href="./nips-2004-Message_Errors_in_Belief_Propagation.html">116 nips-2004-Message Errors in Belief Propagation</a></p>
<p>20 0.70641857 <a title="182-lda-20" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
