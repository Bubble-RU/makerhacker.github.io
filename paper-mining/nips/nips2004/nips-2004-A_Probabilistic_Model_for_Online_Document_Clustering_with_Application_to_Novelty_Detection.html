<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-10" href="#">nips2004-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</h1>
<br/><p>Source: <a title="nips-2004-10-pdf" href="http://papers.nips.cc/paper/2639-a-probabilistic-model-for-online-document-clustering-with-application-to-novelty-detection.pdf">pdf</a></p><p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. 1</p><p>Reference: <a title="nips-2004-10-reference" href="../nips2004_reference/nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we propose a probabilistic model for online document clustering. [sent-9, score-0.481]
</p><p>2 We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. [sent-10, score-0.374]
</p><p>3 Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. [sent-11, score-0.258]
</p><p>4 We use empirical Bayes method to estimate hyperparameters based on a historical dataset. [sent-12, score-0.152]
</p><p>5 Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. [sent-13, score-0.571]
</p><p>6 1  Introduction  The task of online document clustering is to group documents into clusters as long as they arrive in a temporal sequence. [sent-14, score-0.709]
</p><p>7 Generally speaking, it is difﬁcult for several reasons: First, it is unsupervised learning and the learning has to be done in an online fashion, which imposes constraints on both strategy and efﬁciency. [sent-15, score-0.117]
</p><p>8 Second, similar to other learning problems in text, we have to deal with a high-dimensional space with tens of thousands of features. [sent-16, score-0.03]
</p><p>9 And ﬁnally, the number of clusters can be as large as thousands in newswire data. [sent-17, score-0.098]
</p><p>10 The objective of novelty detection is to identify the novel objects from a sequence of data, where “novel” is usually deﬁned as dissimilar to previous seen instances. [sent-18, score-0.485]
</p><p>11 Here we are interested in novelty detection in the text domain, where we want to identify the earliest report of every new event in a sequence of news stories. [sent-19, score-0.602]
</p><p>12 Applying online document clustering to the novelty detection task is straightforward by assigning the ﬁrst seed of every cluster as novel and all its remaining ones as non-novel. [sent-20, score-1.3]
</p><p>13 The most obvious application of novelty detection is that, by detecting novel events, systems can automatically alert people when new events happen, for example. [sent-21, score-0.549]
</p><p>14 In this paper we apply Dirichlet process prior to model the growing number of clusters, and propose to use a General English language model as a basis of newly generated clusters. [sent-22, score-0.245]
</p><p>15 In particular, the new clusters will be generated according to the prior and a background General English model, and each document cluster is modeled using a Bayesian Dirichletmultinomial language model. [sent-23, score-0.758]
</p><p>16 The Bayesian inference can be easily carried out due to conjugacy, and model hyperparameters are estimated using a historical dataset by the empirical Bayes method. [sent-24, score-0.212]
</p><p>17 We evaluate our online clustering algorithm (as well as its variants) on the novelty detection task in TDT, which has been regarded as the hardest task in that literature [2]. [sent-25, score-0.818]
</p><p>18 We ﬁrst introduce our probabilistic model in Section 2, and in Section 3 we give detailed information on how to estimate model hyperparameters. [sent-27, score-0.091]
</p><p>19 2  A Probabilistic Model for Online Document Clustering  In this section we will describe the generative probabilistic model for online document (x) (x) (x) clustering. [sent-30, score-0.481]
</p><p>20 , nV ) to represent a document vector where each (x) element nv denotes the term frequency of the v th corresponding word in the document x, and V is the total size of the vocabulary. [sent-34, score-0.798]
</p><p>21 1  Dirichlet-Multinomial Model  The multinomial distribution has been one of the most frequently used language models for modeling documents in information retrieval. [sent-36, score-0.242]
</p><p>22 , θV ), a document x is generated with the following probability: p(x|θ) =  (  V (x) V n(x) v=1 nv )! [sent-40, score-0.457]
</p><p>23 v=1  From the formula we can see the so-called naive assumption: words are assumed to be independent of each other. [sent-43, score-0.024]
</p><p>24 Given a collection of documents generated from the same model, the parameter θ can be estimated with Maximum Likelihood Estimation (MLE). [sent-44, score-0.196]
</p><p>25 In a Bayesian approach we would like to put a Dirichlet prior over the parameter (θ ∼ Dir(α)) such that the probability of generating a document is obtained by integrating over the parameter space: p(x) = p(θ|α)p(x|θ)dθ. [sent-45, score-0.444]
</p><p>26 This integration can be easily written down due to the conjugacy between Dirichlet and multinomial distributions. [sent-46, score-0.126]
</p><p>27 The key difference between the Bayesian approach and the MLE is that the former uses a distribution to model the uncertainty of the parameter θ, while the latter gives only a point estimation. [sent-47, score-0.059]
</p><p>28 2  Online Document Clustering with Dirichlet Process Mixture Model  In our system documents are grouped into clusters in an online fashion. [sent-49, score-0.335]
</p><p>29 Each cluster is modeled with a multinomial distribution whose parameter θ follows a Dirichlet prior. [sent-50, score-0.365]
</p><p>30 First, a cluster is chosen based on a Dirichlet process prior (can be either a new or existing cluster), and then a document is drawn from that cluster. [sent-51, score-0.643]
</p><p>31 We use Dirichlet Process (DP) to model the prior distribution of θ’s, and our hierarchical model is as follows: xi |ci ∼ M ul(. [sent-52, score-0.192]
</p><p>32 ∼ ∼  G DP (λ, G0 )  (1)  where ci is the cluster indicator variable, θ i is the multinomial parameter 1 for each document, and θ (ci ) is the unique θ for the cluster ci . [sent-54, score-0.863]
</p><p>33 G is a random distribution generated from the Dirichlet process DP (λ, G0 ) [4], which has a precision parameter λ and a base distribution G0 . [sent-55, score-0.147]
</p><p>34 Here our base distribution G0 is a Dirichlet distribution Dir(γπ1 , γπ2 , . [sent-56, score-0.043]
</p><p>35 Intuitively, our G0 distribution can be treated as the prior over general English word frequencies, which has been used in information retrieval literature [6] to model general English documents. [sent-60, score-0.1]
</p><p>36 The exact cluster-document generation process can be described as follows: 1. [sent-61, score-0.056]
</p><p>37 Let xi be the current document under processing (the ith document in the input sequence), and C1 , C2 , . [sent-62, score-0.71]
</p><p>38 Draw a cluster ci based on the following Dirichlet process prior [4]: |Cj | p(ci = Cj ) = (j = 1, 2, . [sent-67, score-0.462]
</p><p>39 , m) m λ + j=1 |Cj | p(ci = Cm+1 )  =  λ λ+  m j=1  (2)  |Cj | m  where |Cj | stands for the cardinality of cluster j with j=1 |Cj | = i − 1, and with certain probability a new cluster Cm+1 will be generated. [sent-70, score-0.464]
</p><p>40 3  Model Updating  Our models for each cluster need to be updated based on incoming documents. [sent-74, score-0.29]
</p><p>41 We can write down the probability that the current document xi is generated by any cluster as p(xi |Cj ) =  p(θ(Cj ) |Cj )p(xi |θ(Cj ) )dθ(Cj ) (j = 1, 2, . [sent-75, score-0.658]
</p><p>42 , m, m + 1)  where p(θ (Cj ) |Cj ) is the posterior distribution of parameters of the j th cluster (j = 1, 2, . [sent-78, score-0.289]
</p><p>43 , m) and we use p(θ (Cm+1 ) |Cm+1 ) = p(θ (Cm+1 ) ) to represent the prior distribution of the parameters of the new cluster for convenience. [sent-81, score-0.284]
</p><p>44 Once the conditional probabilities p(xi |Cj ) are computed, the probabilities p(Cj |xi ) can be easily calculated using the Bayes rule: p(Cj |xi ) =  p(Cj )p(xi |Cj ) m+1 j =1  p(Cj )p(xi |Cj )  where the prior probability of each cluster is calculated using equation (2). [sent-83, score-0.378]
</p><p>45 Now there are several choices we can consider on how to update the cluster models. [sent-84, score-0.232]
</p><p>46 The second choice is to make a hard decision by assigning the current document xi to the cluster with the maximum probability: ci = arg max p(Cj |xi ) = Cj  p(Cj )p(xi |Cj ) m+1 j =1  p(Cj )p(xi |Cj )  . [sent-86, score-0.834]
</p><p>47 For θ we use θv to denote the v th element in the vector, θ i to denote the parameter vector that generates the ith document, and θ (j) to denote the parameter vector for the j th cluster. [sent-87, score-0.208]
</p><p>48 1  The third choice is to use a soft probabilistic updating, which is similar in spirit to the Assumed Density Filtering (ADF) [7] in the literature. [sent-88, score-0.043]
</p><p>49 Instead, we will update all existing clusters as above, and new cluster will be generated only if c i = Cm+1 . [sent-90, score-0.367]
</p><p>50 We will use HD and PD (hard decision and probabilistic decision) to denote the last two candidates in our experiments. [sent-91, score-0.068]
</p><p>51 3  Learning Model Parameters  In the above probabilistic model there are still several hyperparameters not speciﬁed, namely the π and γ in the base distribution G0 = Dir(γπ1 , γπ2 , . [sent-92, score-0.155]
</p><p>52 , γπV ), and the precision parameter λ in the DP (λ, G0 ). [sent-95, score-0.035]
</p><p>53 Since we can obtain a partially labeled historical dataset 2 , we now discuss how to estimate those parameters respectively. [sent-96, score-0.143]
</p><p>54 We will mainly use the empirical Bayes method [5] to estimate those parameters instead of taking a full Bayesian approach, since it is easier to compute and generally reliable when the number of data points is relatively large compared to the number of parameters. [sent-97, score-0.025]
</p><p>55 from the random distribution G, by integrating out the G we get θi |θ1 , θ2 , . [sent-99, score-0.025]
</p><p>56 ˆ b, ˆ a/i+b+ci+i  4  Experiments  We apply the above online clustering model to the novelty detection task in Topic Detection and Tracking (TDT). [sent-103, score-0.682]
</p><p>57 TDT has been a research community since its 1997 pilot study, which is a research initiative that aims at techniques to automatically process news documents in terms of events. [sent-104, score-0.266]
</p><p>58 First Story Detection or New Event Detection) has been regarded as the hardest task in this area [2]. [sent-108, score-0.13]
</p><p>59 The objective of the novelty detection task is to detect the earliest report for each event as soon as that report arrives in the temporal sequence of news stories. [sent-109, score-0.638]
</p><p>60 1  Dataset  We use the TDT2 corpus as our historical dataset for estimating parameters, and use the TDT3 corpus to evaluate our model 5 . [sent-111, score-0.277]
</p><p>61 Notice that we have a subset of documents in the historical dataset (TDT2) for which events labels are given. [sent-112, score-0.304]
</p><p>62 The TDT2 corpus used for novelty detection task consists of 62,962 documents, among them 8,401 documents are labeled in 96 clusters. [sent-113, score-0.638]
</p><p>63 Stopwords are removed and words are stemmed, and after that there are on average 180 words per document. [sent-114, score-0.048]
</p><p>64 2  Evaluation Measure  In our experiments we use the standard TDT evaluation measure [1] to evaluate our results. [sent-117, score-0.082]
</p><p>65 The performance is characterized in terms of the probability of two types of errors: miss and false alarm (PM iss and PF A ). [sent-118, score-0.431]
</p><p>66 These two error probabilities are then combined into a single detection cost, Cdet , by assigning costs to miss and false alarm errors: Cdet = CM iss · PM iss · Ptarget + CF A · PF A · Pnon−target where 1. [sent-119, score-0.89]
</p><p>67 CM iss and CF A are the costs of a miss and a false alarm, respectively, 2. [sent-120, score-0.359]
</p><p>68 PM iss and PF A are the conditional probabilities of a miss and a false alarm, respectively, and 3. [sent-121, score-0.38]
</p><p>69 Ptarget and Pnon−target is the priori target probabilities (Ptarget = 1 − Pnon−target ). [sent-122, score-0.083]
</p><p>70 It is the following normalized cost that is actually used in evaluating various TDT systems: (Cdet )norm =  Cdet min(CM iss · Ptarget , CF A · Pnon−target )  where the denominator is the minimum of two trivial systems. [sent-123, score-0.215]
</p><p>71 Besides, two types of evaluations are used in TDT, namely macro-averaged (topic-weighted) and micro-averaged 5 Strictly speaking we only used the subsets of TDT2 and TDT3 that is designated for the novelty detection task. [sent-124, score-0.464]
</p><p>72 In macro-averaged evaluation, the cost is computed for every event, and then the average is taken. [sent-126, score-0.03]
</p><p>73 In micro-averaged evaluation the cost is averaged over all documents’ decisions generated by the system, thus large event will have bigger impact on the overall performance. [sent-127, score-0.181]
</p><p>74 Note that macro-averaged evaluation is used as the primary evaluation measure in TDT. [sent-128, score-0.104]
</p><p>75 In addition to the binary decision “novel” or “non-novel”, each system is required to generated a conﬁdence score for each test document. [sent-129, score-0.132]
</p><p>76 The higher the score is, the more likely the document is novel. [sent-130, score-0.341]
</p><p>77 Here we mainly use the minimum cost to evaluate systems by varying the threshold, which is independent of the threshold setting. [sent-131, score-0.085]
</p><p>78 3  Methods  One simple but effective method is the “GAC-INCR” clustering method [9] with cosine similarity metric and TFIDF term weighting, which has remained to be the top performing system in TDT 2002 & 2003 ofﬁcial evaluations. [sent-133, score-0.093]
</p><p>79 For this method the novelty conﬁdence score we used is one minus the similarity score between the current cluster xi and its nearest neighbor cluster: s(xi ) = 1. [sent-134, score-0.691]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cj', 0.488), ('document', 0.297), ('novelty', 0.279), ('tdt', 0.277), ('cluster', 0.232), ('dirichlet', 0.186), ('iss', 0.185), ('cm', 0.181), ('detection', 0.159), ('ci', 0.146), ('documents', 0.124), ('cdet', 0.123), ('nv', 0.123), ('pnon', 0.123), ('ptarget', 0.123), ('online', 0.117), ('historical', 0.107), ('alarm', 0.098), ('miss', 0.098), ('english', 0.098), ('xi', 0.092), ('dirichletmultinomial', 0.092), ('dir', 0.073), ('multinomial', 0.072), ('clusters', 0.068), ('pf', 0.068), ('clustering', 0.067), ('event', 0.062), ('cargenie', 0.062), ('hardest', 0.062), ('yiming', 0.062), ('pm', 0.061), ('dp', 0.059), ('cf', 0.059), ('th', 0.057), ('news', 0.056), ('conjugacy', 0.054), ('prior', 0.052), ('evaluation', 0.052), ('false', 0.05), ('mle', 0.049), ('probabilities', 0.047), ('novel', 0.047), ('language', 0.046), ('earliest', 0.046), ('zoubin', 0.046), ('hyperparameters', 0.045), ('score', 0.044), ('probabilistic', 0.043), ('base', 0.043), ('bayesian', 0.042), ('assigning', 0.042), ('corpus', 0.04), ('generated', 0.037), ('events', 0.037), ('target', 0.036), ('dataset', 0.036), ('task', 0.036), ('parameter', 0.035), ('bayes', 0.033), ('process', 0.032), ('regarded', 0.032), ('mellon', 0.031), ('cost', 0.03), ('children', 0.03), ('incoming', 0.03), ('pittsburgh', 0.03), ('thousands', 0.03), ('existing', 0.03), ('evaluate', 0.03), ('growing', 0.03), ('topic', 0.03), ('draw', 0.028), ('updated', 0.028), ('adf', 0.027), ('alert', 0.027), ('hd', 0.027), ('initiative', 0.027), ('pilot', 0.027), ('stemmed', 0.027), ('stopwords', 0.027), ('story', 0.027), ('modeled', 0.026), ('costs', 0.026), ('london', 0.026), ('speaking', 0.026), ('system', 0.026), ('integrating', 0.025), ('mainly', 0.025), ('decision', 0.025), ('seed', 0.024), ('specially', 0.024), ('tfidf', 0.024), ('ul', 0.024), ('pa', 0.024), ('word', 0.024), ('model', 0.024), ('words', 0.024), ('generation', 0.024), ('ith', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="10-tfidf-1" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. 1</p><p>2 0.12033612 <a title="10-tfidf-2" href="./nips-2004-Integrating_Topics_and_Syntax.html">87 nips-2004-Integrating Topics and Syntax</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers, David M. Blei, Joshua B. Tenenbaum</p><p>Abstract: Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously ﬁnd syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classiﬁcation with models that exclusively use short- and long-range dependencies respectively. 1</p><p>3 0.10732338 <a title="10-tfidf-3" href="./nips-2004-Sharing_Clusters_among_Related_Groups%3A_Hierarchical_Dirichlet_Processes.html">169 nips-2004-Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes</a></p>
<p>Author: Yee W. Teh, Michael I. Jordan, Matthew J. Beal, David M. Blei</p><p>Abstract: We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.</p><p>4 0.088602059 <a title="10-tfidf-4" href="./nips-2004-Euclidean_Embedding_of_Co-Occurrence_Data.html">62 nips-2004-Euclidean Embedding of Co-Occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for low dimensional structure in complex data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of our embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text datasets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling and correspondence analysis. 1</p><p>5 0.08676134 <a title="10-tfidf-5" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>Author: Adrian Corduneanu, Tommi S. Jaakkola</p><p>Abstract: We provide a principle for semi-supervised learning based on optimizing the rate of communicating labels for unlabeled points with side information. The side information is expressed in terms of identities of sets of points or regions with the purpose of biasing the labels in each region to be the same. The resulting regularization objective is convex, has a unique solution, and the solution can be found with a pair of local propagation operations on graphs induced by the regions. We analyze the properties of the algorithm and demonstrate its performance on document classiﬁcation tasks. 1</p><p>6 0.076326936 <a title="10-tfidf-6" href="./nips-2004-Making_Latin_Manuscripts_Searchable_using_gHMM%27s.html">107 nips-2004-Making Latin Manuscripts Searchable using gHMM's</a></p>
<p>7 0.074410729 <a title="10-tfidf-7" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>8 0.072221868 <a title="10-tfidf-8" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>9 0.071271442 <a title="10-tfidf-9" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>10 0.069559664 <a title="10-tfidf-10" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>11 0.066667669 <a title="10-tfidf-11" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>12 0.065694354 <a title="10-tfidf-12" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>13 0.064714834 <a title="10-tfidf-13" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>14 0.06467966 <a title="10-tfidf-14" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>15 0.059478629 <a title="10-tfidf-15" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>16 0.057973616 <a title="10-tfidf-16" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>17 0.057235353 <a title="10-tfidf-17" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>18 0.05493639 <a title="10-tfidf-18" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>19 0.053944528 <a title="10-tfidf-19" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>20 0.053601593 <a title="10-tfidf-20" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.163), (1, 0.041), (2, -0.008), (3, -0.043), (4, 0.021), (5, 0.065), (6, -0.103), (7, 0.157), (8, -0.026), (9, 0.034), (10, 0.029), (11, 0.079), (12, -0.127), (13, -0.012), (14, 0.034), (15, -0.06), (16, 0.041), (17, -0.034), (18, -0.024), (19, 0.031), (20, -0.063), (21, -0.115), (22, 0.038), (23, -0.051), (24, -0.0), (25, -0.053), (26, 0.053), (27, -0.102), (28, -0.022), (29, 0.065), (30, -0.006), (31, -0.1), (32, 0.039), (33, 0.008), (34, 0.152), (35, -0.171), (36, -0.097), (37, 0.171), (38, -0.067), (39, -0.089), (40, 0.054), (41, 0.134), (42, -0.207), (43, 0.003), (44, -0.002), (45, 0.008), (46, 0.063), (47, 0.029), (48, -0.047), (49, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95567912 <a title="10-lsi-1" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. 1</p><p>2 0.60880125 <a title="10-lsi-2" href="./nips-2004-Sharing_Clusters_among_Related_Groups%3A_Hierarchical_Dirichlet_Processes.html">169 nips-2004-Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes</a></p>
<p>Author: Yee W. Teh, Michael I. Jordan, Matthew J. Beal, David M. Blei</p><p>Abstract: We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.</p><p>3 0.54056752 <a title="10-lsi-3" href="./nips-2004-Integrating_Topics_and_Syntax.html">87 nips-2004-Integrating Topics and Syntax</a></p>
<p>Author: Thomas L. Griffiths, Mark Steyvers, David M. Blei, Joshua B. Tenenbaum</p><p>Abstract: Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously ﬁnd syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classiﬁcation with models that exclusively use short- and long-range dependencies respectively. 1</p><p>4 0.51309621 <a title="10-lsi-4" href="./nips-2004-Making_Latin_Manuscripts_Searchable_using_gHMM%27s.html">107 nips-2004-Making Latin Manuscripts Searchable using gHMM's</a></p>
<p>Author: Jaety Edwards, Yee W. Teh, Roger Bock, Michael Maire, Grace Vesom, David A. Forsyth</p><p>Abstract: We describe a method that can make a scanned, handwritten mediaeval latin manuscript accessible to full text search. A generalized HMM is ﬁtted, using transcribed latin to obtain a transition model and one example each of 22 letters to obtain an emission model. We show results for unigram, bigram and trigram models. Our method transcribes 25 pages of a manuscript of Terence with fair accuracy (75% of letters correctly transcribed). Search results are very strong; we use examples of variant spellings to demonstrate that the search respects the ink of the document. Furthermore, our model produces fair searches on a document from which we obtained no training data. 1. Intoduction There are many large corpora of handwritten scanned documents, and their number is growing rapidly. Collections range from the complete works of Mark Twain to thousands of pages of zoological notes spanning two centuries. Large scale analyses of such corpora is currently very difﬁcult, because handwriting recognition works poorly. Recently, Rath and Manmatha have demonstrated that one can use small bodies of aligned material as supervised data to train a word spotting mechanism [7]. The result can make scanned handwritten documents searchable. Current techniques assume a closed vocabulary — one can search only for words in the training set — and search for instances of whole words. This approach is particularly unattractive for an inﬂected language, because individual words can take so many forms that one is unlikely to see all in the training set. Furthermore, one would like the method used to require very little aligned training data, so that it is possible to process documents written by different scribes with little overhead. Mediaeval Latin manuscripts are a natural ﬁrst corpus for studying this problem, because there are many scanned manuscripts and because the handwriting is relatively regular. We expect the primary user need to be search over a large body of documents — to allow comparisons between documents — rather than transcription of a particular document (which is usually relatively easy to do by hand). Desirable features for a system are: First, that it use little or no aligned training data (an ideal, which we believe may be attainable, is an unsupervised learning system). Second, that one can search the document for an arbitrary string (rather than, say, only complete words that appear in the training data). This would allow a user to determine whether a document contains curious or distinctive spellings, for example (ﬁgure 7). We show that, using a statistical model based on a generalized HMM, we can search a medieval manuscript with considerable accuracy, using only one instance each of each letter in the manuscript to train the method (22 instances in total; Latin has no j, k, w, or z). Furthermore, our method allows fairly accurate transcription of the manuscript. We train our system on 22 glyphs taken from a a 12th century latin manuscript of Terence’s Comedies (obtained from a repository of over 80 scanned medieval works maintained by Oxford University [1]). We evaluate searches using a considerable portion of this manuscript aligned by hand; we then show that fair search results are available on a different manuscript (MS. Auct. D. 2. 16, Latin Gospels with beast-headed evangelist portraits made at Landvennec, Brittany, late 9th or early 10th century, from [1]) without change of letter templates. 1.1. Previous Work Handwriting recognition is a traditional problem, too well studied to review in detail here (see [6]). Typically, online handwriting recognition (where strokes can be recorded) works better than ofﬂine handwriting recognition. Handwritten digits can now be recognized with high accuracy [2, 5]. Handwritten amounts can be read with fair accuracy, which is signiﬁcantly improved if one segments the amount into digits at the same time as one recognizes it [4, 5]. Recently several authors have proposed new techniques for search and translation in this unrestricted setting. Manmatha et al [7] introduce the technique of “word spotting,” which segments text into word images, rectiﬁes the word images, and then uses an aligned training set to learn correspondences between rectiﬁed word images and strings. The method is not suitable for a heavily inﬂected language, because words take so many forms. In an inﬂected language, the natural unit to match to is a subset of a word, rather than a whole word, implying that one should segment the text into blocks — which may be smaller than words — while recognizing. Vinciarelli et al [8] introduce a method for line by line recognition based around an HMM and quite similar to techniques used in the speech recognition community. Their method uses a window that slides along the text to obtain features; this has the difﬁculty that the same window is in some places too small (and so uninformative) and in others too big (and so spans more than one letter, and is confusing). Their method requires a substantial body of aligned training data, which makes it impractical for our applications. Close in spirit to our work is the approach to machine translation of Koehn and Knight [3]. They demonstrate that the statistics of unaligned corpora may provide as powerful constraints for training models as aligned bitexts. 2. The Model Our models for both search and transcription are based on the generalized HMM and differ only in their choice of transition model. In an HMM, each hidden node ct emits a single evidence node xt . In a generalized HMM, we allow each ct to emit a series of x’s whose length is itself a random variable. In our model, the hidden nodes correspond to letters and each xt is a single column of pixels. Allowing letters to emit sets of columns lets us accomodate letter templates of variable width. In particular, this means that we can unify segmenting ink into letters and recognizing blocks of ink; ﬁgure 3 shows an example of how useful this is. 2.1. Generating a line of text Our hidden state consists of a character label c, width w and vertical position y. The statespace of c contains the characters ‘a’-‘z’, a space ‘ ’, and a special end state Ω. Let T c be the template associated with character c, Tch , Tcw be respectively the height and width of that template, and m be the height of the image. Figure 1: Left, a full page of our manuscript, a 12’th century manuscript of Terence’s Comedies obtained from [1]. Top right, a set of lines from a page from that document and bottom right, some words in higher resolution. Note: (a) the richness of page layout; (b) the clear spacing of the lines; (c) the relatively regular handwriting. Figure 2: Left, the 22 instances, one per letter, used to train our emission model. These templates are extracted by hand from the Terence document. Right, the ﬁve image channels for a single letter. Beginning at image column 1 (and assuming a dummy space before the ﬁrst character), • • • • choose character c ∼ p(c|c−1...−n ) (an n-gram letter model) choose length w ∼ Uniform(Tcw − k, Tcw + k) (for some small k) choose vertical position y ∼ Uniform(1, m − Tch ) z,y and Tch now deﬁne a bounding box b of pixels. Let i and j be indexed from the top left of that bounding box. – draw pixel (i, j) ∼ N (Tcij , σcij ) for each pixel in b – draw all pixels above and below b from background gaussian N (µ0 , σ0 ) (See 2.2 for greater detail on pixel emission model) • move to column w + 1 and repeat until we enter the end state Ω. Inference on a gHMM is a relatively straighforward business of dynamic programming. We have used unigram, bigram and trigram models, with each transition model ﬁtted using an electronic version of Caesar’s Gallic Wars, obtained from http://www.thelatinlibrary.com. We do not believe that the choice of author should signiﬁcantly affect the ﬁtted transition model — which is at the level of characters — but have not experimented with this point. The important matter is the emission model. 2.2. The Emission Model Our emission model is as follows: Given the character c and width w, we generate a template of the required length. Each pixel in this template becomes the mean of a gaussian which generates the corresponding pixel in the image. This template has a separate mean image for each pixel channel. The channels are assumed independent given the means. We train the model by cutting out by hand a single instance of each letter from our corpus (ﬁgure 2). This forms the central portion of the template. Pixels above and below this Model Perfect transcription unigram bigram trigram matching chars 21019 14603 15572 15788 substitutions 0 5487 4597 4410 insertions 0 534 541 507 deletions 0 773 718 695 Table 1: Edit distance between our transcribed Terence and the editor’s version. Note the trigram model produces signiﬁcantly fewer letter errors than the unigram model, but that the error rate is still a substantial 25%. central box are generated from a single gaussian used to model background pixels (basically white pixels). We add a third variable yt to our hidden state indicating the vertical position of the central box. However, since we are uninterested in actually recovering this variable, during inference we sum it out of the model. The width of a character is constrained to be close to the width (tw ) of our hand cut example by setting p(w|c) = 0 for w < tw − k and w > tw + k. Here k is a small, user deﬁned integer. Within this range, p(w|c) is distributed uniformly, larger templates are created by appending pixels from the background model to the template and smaller ones by simply removing the right k-most columns of the hand cut example. For features, we generate ﬁve image representations, shown in ﬁgure 2. The ﬁrst is a grayscale version of the original color image. The second and third are generated by convolving the grayscale image with a vertical derivative of gaussian ﬁlter, separating the positive and negative components of this response, and smoothing each of these gradient images separately. The fourth and ﬁfth are generated similarly but with a horizontal derivative of gaussian ﬁlter. We have experimented with different weightings of these 5 channels. In practice we use the gray scale channel and the horizontal gradient channels. We emphasize the horizontal pieces since these seem the more discriminative. 2.3. Transcription For transcription, we model letters as coming from an n-gram language model, with no dependencies between words. Thus, the probability of a letter depends on the k letters before it, where k = n unless this would cross a word boundary in which case the history terminates at this boundary. We chose not to model word to word transition probabilities since, unlike in English, word order in Latin is highly arbitrary. This transition model is ﬁt from a corpus of ascii encoded latin. We have experimented with unigram (i.e. uniform transition probabilities), bigram and trigram letter models. We can perform transcription by ﬁtting the maximum likelihood path through any given line. Some results of this technique are shown in ﬁgure 3. 2.4. Search For search, we rank lines by the probability that they contain our search word. We set up a ﬁnite state machine like that in ﬁgure 4. In this ﬁgure, ‘bg’ represents our background model for that portion of the line not generated by our search word. We can use any of the n-gram letter models described for transcription as the transition model for ‘bg’. The probability that the line contains the search word is the probability that this FSM takes path 1. We use this FSM as the transition model for our gHMM, and output the posterior probability of the two arrows leading into the end state. 1 and 2 are user deﬁned weights, but in practice the algorithm does not appear to be particular sensitive to the choice of these parameters. The results presented here use the unigram model. Editorial translation Orator ad vos venio ornatu prologi: unigram b u rt o r a d u o s u em o o r n a t u p r o l o g r b u rt o r a d v o s v em o o r u a t u p r o l o g r fo r a t o r a d v o s v en i o o r n a t u p r o l o g i bigram trigram Figure 3: We transcribe the text by ﬁnding the maximum likelihood path through the gHMM. The top line shows the standard version of the line (obtained by consensus among editors who have consulted various manuscripts; we obtained this information in electronic form from http://www.thelatinlibrary.com). Below, we show the line as segmented and transcribed by unigram, bigram and trigram models; the unigram and bigram models transcribe one word as “vemo”, but the stronger trigram model forces the two letters to be segmented and correctly transcribes the word as “venio”, illustrating the considerable beneﬁt to be obtained by segmenting only at recognition time. 1 − ε1 Path 1 1 − ε2 a b bg ε1 Ω bg Path 2 ε2 Figure 4: The ﬁnite state machine to search for the word ‘ab.’ ‘bg’ is a place holder for the larger ﬁnite state machine deﬁned by our language model’s transition matrix. 3. Results Figure 1 shows a page from our collection. This is a scanned 12th century manuscript of Terence’s Comedies, obtained from the collection at [1]. In preprocessing, we extract individual lines of text by rotating the image to various degrees and projecting the sum of the pixel values onto the y-axis. We choose the orientation whose projection vector has the lowest entropy, and then segment lines by cutting at minima of this projection. Transcription is not our primary task, but methods that produce good transcriptions are going to support good searches. The gHMM can produce a surprisingly good transcription, given how little training data is used to train the emission model. We aligned an editors version of Terence with 25 pages from the manuscript by hand, and computed the edit distance between the transcribed text and the aligned text; as table 1 indicates, approximately 75% of letters are read correctly. Search results are strong. We show results for two documents. The ﬁrst set of results refers to the edition of Terence’s Comedies, from which we took the 22 letter instances. In particular, for any given search term, our process ranks the complete set of lines. We used a hand alignment of the manuscript to determine which lines contained each term; ﬁgure 5 shows an overview of searches performed using every word that appears in the 50 100 150 200 250 300 350 400 450 500 550 Figure 5: Our search ranks 587 manuscript lines, with higher ranking lines more likely to contain the relevant term. This ﬁgure shows complete search results for each term that appears more than three times in the 587 lines. Each row represents the ranked search results for a term, and a black mark appears if the search term is actually in the line; a successful search will therefore appear as a row which is wholly dark to the left, and then wholly light. All 587 lines are represented. More common terms are represented by lower rows. More detailed results appear in ﬁgure 5 and ﬁgure 6; this summary ﬁgure suggests almost all searches are highly successful. document more than three times, in particular, showing which of the ranked set of lines actually contained the search term. For almost every search, the term appears mainly in the lines with higher rank. Figure 6 contains more detailed information for a smaller set of words. We do not score the position of a word in a line (for practical reasons). Figure 7 demonstrates (a) that our search respects the ink of the document and (b) that for the Terence document, word positions are accurately estimated. The spelling of mediaeval documents is typically cleaned up by editors; in our manuscript, the scribe reliably spells “michi” for the standard “mihi”. A search on “michi” produces many instances; a search on “mihi” produces none, because the ink doesn’t have any. Notice this phenomenon also in the bottom right line of ﬁgure 7, the scribe writes “habet, ut consumat nunc cum nichil obsint doli” and the editor gives “habet, ut consumat nunc quom nil obsint doli.” Figure 8 shows that searches on short strings produce many words containing that string as one would wish. 4. Discussion We have shown that it is possible to make at least some handwritten mediaeval manuscripts accessible to full text search, without requiring an aligned text or much supervisory data. Our documents have very regular letters, and letter frequencies — which can be obtained from transcribed Latin — appear to provide so powerful a cue that relatively little detailed information about letter shapes is required. Linking letter segmentation and recognition has thoroughly beneﬁcial effects. This suggests that the pool of manuscripts that can be made accessible in this way is large. In particular, we have used our method, trained on 22 instances of letters from one document, to search another document. Figure 9 shows the results from two searches of our second document (MS. Auct. D. 2. 16, Latin Gospels with beast-headed evangelist portraits made at Landvennec, Brittany, late 9th or early 10th century, from [1]). No information from this document was used in training at all; but letter 1tu arbitror pater etiam nisi factum primum siet vero illi inter hic michi ibi qui tu ibi michi 0.9 0.8 0.7 qui hic 0.6 inter 0.5 illi 0.4 siet 0.3 vero 0.2 nisi 0.1 50 100 150 200 250 300 350 400 450 500 550 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure 6: On the left, search results for selected words (indicated on the leftmost column). Each row represents the ranked search results for a term, and a black mark appears if the search term is actually in the line; a successful search will therefore appear as a row which is wholly dark to the left, and then wholly light. Note only the top 300 results are represented, and that lines containing the search term are almost always at or close to the top of the search results (black marks to the left). On the right, we plot precision against recall for a set of different words by taking the top 10, 20, ... lines returned from the search, and checking them against the aligned manuscript. Note that, once all cases have been found, if the size of the pool is increased the precision will fall with 100% recall; many words work well, with most of the ﬁrst 20 or so lines returned containing the search term. shapes are sufﬁciently well shared that the search is still useful. All this suggests that one might be able to use EM to link three processes: one that clusters to determine letter shapes; one that segments letters; and one that imposes a language model. Such a system might be able to make handwritten Latin searchable with no training data. References [1] Early Manuscripts at Oxford University. Bodleian library ms. auct. f. 2.13. http://image.ox.ac.uk/. [2] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape matching and object recognition using shape contexts. IEEE T. Pattern Analysis and Machine Intelligence, 24(4):509–522, 2002. [3] Philipp Koehn and Kevin Knight. Estimating word translation probabilities from unrelated monolingual corpora. In Proc. of the 17th National Conf. on AI, pages 711–715. AAAI Press / The MIT Press, 2000. [4] Y. LeCun, L. Bottou, and Y. Bengio. Reading checks with graph transformer networks. In International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 151–154, Munich, 1997. IEEE. [5] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [6] R. Plamondon and S.N. Srihari. Online and off-line handwriting recognition: a comprehensive survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(1):63–84, 2000. [7] T. M. Rath and R. Manmatha. Word image matching using dynamic time warping. In Proc. of the Conf. on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 521–527, 2003. [8] Alessandro Vinciarelli, Samy Bengio, and Horst Bunke. Ofﬂine recognition of unconstrained handwritten texts using hmms and statistical language models. IEEE Trans. Pattern Anal. Mach. Intell., 26(6):709–720, 2004. michi: Spe incerta certum mihi laborem sustuli, mihi: Faciuntne intellegendo ut nil intellegant? michi: Nonnumquam conlacrumabat. placuit tum id mihi. mihi: Placuit: despondi. hic nuptiis dictust dies. michi: Sto exspectans siquid mi imperent. venit una,</p><p>5 0.47284049 <a title="10-lsi-5" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>Author: David H. Stern, Thore Graepel, David MacKay</p><p>Abstract: Go is an ancient oriental game whose complexity has defeated attempts to automate it. We suggest using probability in a Bayesian sense to model the uncertainty arising from the vast complexity of the game tree. We present a simple conditional Markov random ﬁeld model for predicting the pointwise territory outcome of a game. The topology of the model reﬂects the spatial structure of the Go board. We describe a version of the Swendsen-Wang process for sampling from the model during learning and apply loopy belief propagation for rapid inference and prediction. The model is trained on several hundred records of professional games. Our experimental results indicate that the model successfully learns to predict territory despite its simplicity. 1</p><p>6 0.38582289 <a title="10-lsi-6" href="./nips-2004-Euclidean_Embedding_of_Co-Occurrence_Data.html">62 nips-2004-Euclidean Embedding of Co-Occurrence Data</a></p>
<p>7 0.36293826 <a title="10-lsi-7" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>8 0.36259511 <a title="10-lsi-8" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>9 0.36120582 <a title="10-lsi-9" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>10 0.34226012 <a title="10-lsi-10" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>11 0.33687058 <a title="10-lsi-11" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>12 0.32886347 <a title="10-lsi-12" href="./nips-2004-Detecting_Significant_Multidimensional_Spatial_Clusters.html">51 nips-2004-Detecting Significant Multidimensional Spatial Clusters</a></p>
<p>13 0.31786406 <a title="10-lsi-13" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>14 0.31039494 <a title="10-lsi-14" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>15 0.30201465 <a title="10-lsi-15" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>16 0.29831186 <a title="10-lsi-16" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>17 0.29261518 <a title="10-lsi-17" href="./nips-2004-Theories_of_Access_Consciousness.html">193 nips-2004-Theories of Access Consciousness</a></p>
<p>18 0.28801996 <a title="10-lsi-18" href="./nips-2004-The_Rescorla-Wagner_Algorithm_and_Maximum_Likelihood_Estimation_of_Causal_Parameters.html">190 nips-2004-The Rescorla-Wagner Algorithm and Maximum Likelihood Estimation of Causal Parameters</a></p>
<p>19 0.28329778 <a title="10-lsi-19" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>20 0.28201303 <a title="10-lsi-20" href="./nips-2004-Hierarchical_Distributed_Representations_for_Statistical_Language_Modeling.html">78 nips-2004-Hierarchical Distributed Representations for Statistical Language Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.023), (9, 0.015), (13, 0.082), (15, 0.115), (25, 0.21), (26, 0.035), (31, 0.05), (33, 0.176), (34, 0.112), (35, 0.014), (39, 0.014), (50, 0.049), (87, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91318905 <a title="10-lda-1" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>Author: Finn \. Nielsen</p><p>Abstract: We provide a method for mass meta-analysis in a neuroinformatics database containing stereotaxic Talairach coordinates from neuroimaging experiments. Database labels are used to group the individual experiments, e.g., according to cognitive function, and the consistent pattern of the experiments within the groups are determined. The method voxelizes each group of experiments via a kernel density estimation, forming probability density volumes. The values in the probability density volumes are compared to null-hypothesis distributions generated by resamplings from the entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. This allows for mass meta-analysis, with the construction of a list with the most prominent associations between brain areas and group labels. Furthermore, the method can be used for functional labeling of voxels. 1</p><p>2 0.8997587 <a title="10-lda-2" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>3 0.89695042 <a title="10-lda-3" href="./nips-2004-Discrete_profile_alignment_via_constrained_information_bottleneck.html">52 nips-2004-Discrete profile alignment via constrained information bottleneck</a></p>
<p>Author: Sean O'rourke, Gal Chechik, Robin Friedman, Eleazar Eskin</p><p>Abstract: Amino acid proﬁles, which capture position-speciﬁc mutation probabilities, are a richer encoding of biological sequences than the individual sequences themselves. However, proﬁle comparisons are much more computationally expensive than discrete symbol comparisons, making proﬁles impractical for many large datasets. Furthermore, because they are such a rich representation, proﬁles can be diﬃcult to visualize. To overcome these problems, we propose a discretization for proﬁles using an expanded alphabet representing not just individual amino acids, but common proﬁles. By using an extension of information bottleneck (IB) incorporating constraints and priors on the class distributions, we ﬁnd an informationally optimal alphabet. This discretization yields a concise, informative textual representation for proﬁle sequences. Also alignments between these sequences, while nearly as accurate as the full proﬁleproﬁle alignments, can be computed almost as quickly as those between individual or consensus sequences. A full pairwise alignment of SwissProt would take years using proﬁles, but less than 3 days using a discrete IB encoding, illustrating how discrete encoding can expand the range of sequence problems to which proﬁle information can be applied. 1</p><p>same-paper 4 0.83703279 <a title="10-lda-4" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. 1</p><p>5 0.78041387 <a title="10-lda-5" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: We provide a worst-case analysis of selective sampling algorithms for learning linear threshold functions. The algorithms considered in this paper are Perceptron-like algorithms, i.e., algorithms which can be efﬁciently run in any reproducing kernel Hilbert space. Our algorithms exploit a simple margin-based randomized rule to decide whether to query the current label. We obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic counterparts, but using much fewer labels. We complement our theoretical ﬁndings with an empirical comparison on two text categorization tasks. The outcome of these experiments is largely predicted by our theoretical results: Our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classiﬁcation, while observing in practice substantially fewer labels. 1</p><p>6 0.75579822 <a title="10-lda-6" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>7 0.72290647 <a title="10-lda-7" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<p>8 0.6984629 <a title="10-lda-8" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>9 0.69736767 <a title="10-lda-9" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>10 0.69499964 <a title="10-lda-10" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>11 0.68991458 <a title="10-lda-11" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>12 0.68900275 <a title="10-lda-12" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>13 0.68861306 <a title="10-lda-13" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>14 0.68834668 <a title="10-lda-14" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>15 0.68833429 <a title="10-lda-15" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>16 0.68824595 <a title="10-lda-16" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>17 0.68776476 <a title="10-lda-17" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>18 0.68671334 <a title="10-lda-18" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>19 0.68513012 <a title="10-lda-19" href="./nips-2004-Sharing_Clusters_among_Related_Groups%3A_Hierarchical_Dirichlet_Processes.html">169 nips-2004-Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes</a></p>
<p>20 0.6845867 <a title="10-lda-20" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
