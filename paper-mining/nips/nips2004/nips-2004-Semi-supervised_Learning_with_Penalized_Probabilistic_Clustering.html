<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-167" href="#">nips2004-167</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</h1>
<br/><p>Source: <a title="nips-2004-167-pdf" href="http://papers.nips.cc/paper/2610-semi-supervised-learning-with-penalized-probabilistic-clustering.pdf">pdf</a></p><p>Author: Zhengdong Lu, Todd K. Leen</p><p>Abstract: While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. We express clustering preferences in the prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. We ﬁt the model parameters with EM. Experiments on a variety of data sets show that PPC can consistently improve clustering results.</p><p>Reference: <a title="nips-2004-167-reference" href="../nips2004_reference/nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. [sent-4, score-0.358]
</p><p>2 We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. [sent-5, score-0.899]
</p><p>3 Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. [sent-6, score-0.278]
</p><p>4 We express clustering preferences in the prior distribution over assignments of data points to clusters. [sent-7, score-0.649]
</p><p>5 This prior penalizes cluster assignments according to the degree with which they violate the preferences. [sent-8, score-0.291]
</p><p>6 Experiments on a variety of data sets show that PPC can consistently improve clustering results. [sent-10, score-0.307]
</p><p>7 1 Introduction While clustering is usually executed completely unsupervised, there are circumstances in which we have prior belief that pairs of samples should (or should not) be assigned to the same cluster. [sent-11, score-0.476]
</p><p>8 Such pairwise relations may arise from a perceived similarity (or dissimilarity) between samples, or from a desire that the algorithmically generated clusters match the geometric cluster structure perceived by the experimenter in the original data. [sent-12, score-0.743]
</p><p>9 Continuity, which suggests that neighboring pairs of samples in a time series or in an image are likely to belong to the same class of object, is also a source of clustering preferences. [sent-13, score-0.388]
</p><p>10 We would like these preferences to be incorporated into the cluster structure so that the assignment of out-of-sample data to clusters captures the concept(s) that give rise to the preferences expressed in the training data. [sent-14, score-0.823]
</p><p>11 Some work [1, 2, 3] has been done on adopting traditional clustering methods, such as Kmeans, to incorporate pairwise relations. [sent-15, score-0.48]
</p><p>12 These models are based on hard clustering and the clustering preferences are expressed as hard pairwise constraints that must be satisﬁed. [sent-16, score-1.111]
</p><p>13 [4] who propose a Gaussian mixture model (GMM) for clustering that incorporates hard pairwise constraints. [sent-18, score-0.589]
</p><p>14 In this paper, we propose a soft clustering algorithm based on GMM that expresses cluster-  ing preferences (in the form of pairwise relations) in the prior probability on assignments of data points to clusters. [sent-19, score-0.967]
</p><p>15 This framework naturally accommodates both hard constraints and soft preferences in a framework in which the preferences are expressed as a Bayesian probability that pairs of points should (or should not) be assigned to the same cluster. [sent-20, score-0.781]
</p><p>16 Experiments on several datasets demonstrate that PPC can consistently improve the clustering result by incorporating reliable prior knowledge. [sent-22, score-0.353]
</p><p>17 N with (latent) cluster assignments Z = z(xi ), i = 1, . [sent-33, score-0.22]
</p><p>18 The complete data likelihood is P (X, Z|Θ) = P (X|Z, Θ)P (Z|Θ). [sent-37, score-0.099]
</p><p>19 1 Prior distribution in latent space We incorporate our clustering preferences by manipulating the prior probability P (Z|Θ). [sent-39, score-0.526]
</p><p>20 In the standard Gaussian mixture model, the prior distribution is trivial: P (Z|Θ) = i πzi . [sent-40, score-0.109]
</p><p>21 We incorporate prior knowledge (our clustering preferences) through a weighting function g(Z) that has large values when the assignment of data points to clusters Z conforms to our preferences, and low values when Z conﬂicts with our preferences. [sent-41, score-0.582]
</p><p>22 Hence we write i  P (Z|Θ, G) = Z  πzi g(Z) 1 ≡ K j πzj g(Z)  πzi g(Z)  (2)  i  where the sum is over all possible assignments of the data to clusters. [sent-42, score-0.126]
</p><p>23 The likelihood of the data, given a speciﬁc cluster assignment, is independent of the cluster assignment preferences, and so the complete data likelihood is P (X, Z|Θ, G) = P (X|Z, Θ)  1 K  πzi g(Z) = i  1 P (X, Z|Θ)g(Z), K  (3)  where P (X, Z|Θ) is the complete data likelihood for a standard GMM. [sent-43, score-0.592]
</p><p>24 The data likelihood is the sum of complete data likelihood over all possible Z, that is, L(X|Θ) = P (X|Θ, G) = Z P (X, Z|Θ, G), which can be maximized with the EM algorithm. [sent-44, score-0.161]
</p><p>25 Once the model parameters are ﬁt, we do soft clustering according to the posterior probabilities for new data p(α|x, Θ). [sent-45, score-0.423]
</p><p>26 (Note that cluster assignment preferences are not expressed for the new data, only for the training data. [sent-46, score-0.515]
</p><p>27 2 Pairwise relations Pairwise relations provide a special case of the framework discussed above. [sent-48, score-0.508]
</p><p>28 We specify two types of pairwise relations: • link: two sample should be assigned into one cluster • do-not-link: two samples should be assigned into different clusters. [sent-49, score-0.585]
</p><p>29 The weighting factor given to the cluster assignment conﬁguration Z is simple: p exp(Wij δ(zi , zj )),  g(Z) = i,j  p where δ is the Kronecker δ-function and Wij is the weight associated with sample pair (xi , xj ). [sent-50, score-0.541]
</p><p>30 p The weight Wij reﬂects our preference and conﬁdence in assigning xi and xj into one p cluster. [sent-52, score-0.147]
</p><p>31 We use a positive Wij when we prefer to assign xi and xj into one cluster (link), p and a negative Wij when we prefer to assign them into different clusters (do-not-link). [sent-53, score-0.363]
</p><p>32 If Wij = 0, we have no prior p knowledge on the assignment relevancy of xi and xj . [sent-55, score-0.415]
</p><p>33 In the extreme cases where |Wij | → ∞, the Z violating the pairwise relations about xi and xj have zero prior probability, since for those assignments n  P (Z|Θ, G) = Z  πz n n  i,j  πzn  p exp(Wij δ(zi , zj )) i,j  p exp(Wij δ(zi , zj ))  → 0. [sent-56, score-1.124]
</p><p>34 p Then the relations become hard constraints, while the relations with |Wij | < ∞ are called soft preferences. [sent-57, score-0.662]
</p><p>35 In the remainder of this paper, we will use W p to denote the prior knowledge on pairwise relations, that is  P (X, Z|Θ, W p ) =  1 P (X, Z|Θ) K  p exp(Wij δ(zi , zj ))  (4)  i,j  2. [sent-58, score-0.518]
</p><p>36 However, the update of prior probability of each component is more difﬁcult than for the standard GMM, we need to ﬁnd M  N  log πl P (l|xi , Θ(t−1) , G) − log K(π). [sent-61, score-0.071]
</p><p>37 4 Posterior Inference and Gibbs sampling The M-step requires the cluster membership posterior. [sent-67, score-0.153]
</p><p>38 Computing this posterior is simple for the standard GMM since each data point xi can be assigned to a cluster independent of the other data points and we have the familiar cluster origin posterior p(zi = k|xi , Θ). [sent-68, score-0.65]
</p><p>39 If two sample points, xi and xj participate in a pairwise relations, equation (4) tells us P (zi , zj |X, Θ, W p ) = P (zi |X, Θ, W p )P (zj |X, Θ, W p ) . [sent-70, score-0.613]
</p><p>40 and the posterior probability of xi and xj cannot be computed separately. [sent-71, score-0.249]
</p><p>41 For pairwise relations, the joint posterior distribution must be calculated over the entire transitive closure of the “link” or “do-not-link” relations. [sent-72, score-0.361]
</p><p>42 p Z P (ZT , XT |Θ, W ) T  Computing the posterior probability of a sample in clique T requires time complexity O(M |T | ), where |T | is the size of clique T and M is the number of components in the mixture model. [sent-77, score-0.414]
</p><p>43 In some circumstances it is natural to limit ourselves to the special case of pairwise relation with |T | ≤ 2, called non-overlapping relations. [sent-80, score-0.281]
</p><p>44 More generally, we can avoid the expensive computation in posterior inference by breaking large clique into many small ones. [sent-83, score-0.221]
</p><p>45 For some choices of g(Z), the posterior probability can be given in a simple form even when the clique is big. [sent-87, score-0.221]
</p><p>46 This case is useful when we are sure that a group of samples are from one source. [sent-89, score-0.099]
</p><p>47 For more general cases, where exact inference is computationally prohibitive, we propose to use Gibbs sampling [6] to estimate the posterior probability. [sent-90, score-0.136]
</p><p>48 (b)  (a)  Figure 2: (a) Overlapping pairwise relations; (b) Non-overlapping pairwise relations. [sent-91, score-0.474]
</p><p>49 In Gibbs sampling, we estimate P (zi |X, Θ, G) as a sample mean P (zi = k|X, Θ, G) = E(δ(zi , k)|X, Θ, G) ≈  1 S  S  (t)  δ(zi , k) t=1  where the sum is over a sequence of S samples from P (Z|X, Θ, G) generated by the Gibbs MCMC. [sent-92, score-0.135]
</p><p>50 , zN −1 , X, G, Θ) For pairwise relations it is helpful to introduce some notation. [sent-102, score-0.491]
</p><p>51 Let Z−i denote an assignment of data points to clusters that leaves out the assignment of xi . [sent-103, score-0.397]
</p><p>52 Let U (i) be the indices of the set of samples that participate in a pairwise relation with sample xi , p U (i) = {j : Wij = 0}. [sent-104, score-0.468]
</p><p>53 P (zi |Z−i , X, Θ, W p ) ∝ P (xi , zi |Θ)  (5)  j∈U (i)  When W p is sparse, the size of U (i) is small, thus calculating P (zi |Z−i , X, Θ, W p ) is very cheap and Gibbs sampling can effectively estimate the posterior probability. [sent-106, score-0.416]
</p><p>54 1 Clustering with different number of hard pairwise constraints In this experiment, we demonstrate how the number of pairwise relations affects the performance of clustering. [sent-108, score-0.852]
</p><p>55 Iris data set has 150 samples and three classes, 50 samples in each class; Waveform data set has 5000 samples and three classes, 33% samples in each class; Pendigits data set includes four classes (digits 0,6,8,9), each with 750 samples. [sent-110, score-0.511]
</p><p>56 All data sets have labels for all samples, which are used to generate the relations and to evaluate performance. [sent-111, score-0.279]
</p><p>57 We try PPC (with component number same as the number of classes) with various number of pairwise relations. [sent-112, score-0.237]
</p><p>58 For each relations number, we conduct 100 runs and calculate the averaged classiﬁcation accuracy. [sent-113, score-0.28]
</p><p>59 The pairwise relations are generated as follows: we randomly pick two samples from the training set without replacement and check their labels. [sent-115, score-0.691]
</p><p>60 Note the generated pairwise relations are non-overlapping, as described in section 2. [sent-117, score-0.491]
</p><p>61 3 indicates, PPC can consistently improve its clustering accuracy on the training set when more pairwise constraints are added; also, the effect brought by constraints generalizes to the test set. [sent-123, score-0.684]
</p><p>62 7 0  200  400 600 800 1000 Number of relations  1200  (c) on Pendigits data  Figure 3: The performance of PPC with various number of relations  3. [sent-141, score-0.533]
</p><p>63 2 Hard pairwise constraints for encoding partial label The experiment in this subsection shows the application of pairwise constraints on partially labeled data. [sent-142, score-0.638]
</p><p>64 The samples are partially labeled in the sense that we are told which class-set a sample is from, but not which speciﬁc class it is from. [sent-148, score-0.181]
</p><p>65 We can logically derive a do-not-link constraint between any pair of samples known to belong to different class-sets, while no link constraint can be derived if each class-set has more than one class in it. [sent-149, score-0.162]
</p><p>66 4 (a) is a 120x400 region from Greenland ice sheet from NASA Langley DAAC. [sent-151, score-0.073]
</p><p>67 This region is partially labeled into snow area and non-snow area, as indicated in Fig. [sent-152, score-0.273]
</p><p>68 The snow area can be ice, melting snow or dry snow, while the non-snow area can be bare land, water or cloud. [sent-154, score-0.498]
</p><p>69 To segment the image, we ﬁrst divide the image into 5x5x7 blocks (175 dim vectors). [sent-156, score-0.139]
</p><p>70 For PPC, we use half of data samples for training set and the rest for test. [sent-158, score-0.154]
</p><p>71 Hard do-not-link constraints (only on training set) are generated as follows: for each block in the non-snow area, we randomly choose (without replacement) six blocks from the snow area to build do-not-link constraints. [sent-159, score-0.467]
</p><p>72 By doing this, we achieve cliques with size seven (1 non-snow block + 6 snow blocks). [sent-160, score-0.325]
</p><p>73 1, we apply the model ﬁtted with PPC to test set and combine the clustering results on both data sets into a complete picture. [sent-162, score-0.277]
</p><p>74 A typical clustering result of 3-component standard GMM and 3-component PPC are shown as Fig. [sent-163, score-0.215]
</p><p>75 4, standard GMM gives a clustering that is clearly in disagreement with the human labeling in Fig. [sent-166, score-0.215]
</p><p>76 The PPC segmentation makes far fewer mis-assignments of snow areas (tagged white and gray) to non-snow (black) than does the GMM. [sent-168, score-0.312]
</p><p>77 The PPC segmentation properly labels almost all of the non-snow regions as non-snow. [sent-169, score-0.081]
</p><p>78 Furthermore, the segmentation of the snow areas into the two classes (not labeled) tagged white and gray in Fig. [sent-170, score-0.39]
</p><p>79 4 (d) reﬂects subtle differences in the snow regions captured by the gray-scale image from spectral channel 2, as shown in Fig. [sent-171, score-0.304]
</p><p>80 Figure 4: (a) Gray-scale image from the ﬁrst spectral channel 2. [sent-173, score-0.1]
</p><p>81 (b) Partial label given by expert, black pixels denote non-snow area and white pixels denote snow area. [sent-174, score-0.276]
</p><p>82 (c) and (d) are colored according to image blocks’ assignment. [sent-176, score-0.074]
</p><p>83 3 Soft pairwise preferences for texture image segmentation In this subsection, we propose an unsupervised texture image segmentation algorithm as an application of PPC model. [sent-178, score-0.893]
</p><p>84 2, the image is divided into blocks and rearranged into feature vectors. [sent-180, score-0.19]
</p><p>85 However, standard GMM often fails to give a good segmentation because it cannot make use of the spatial continuity of image, which is essential in many image segmentation models, such as random ﬁeld [7]. [sent-182, score-0.281]
</p><p>86 In our algorithm, the spatial continuity is incorporated as the soft link preferences with uniform weight between each block and its neighbors. [sent-183, score-0.466]
</p><p>87 The complete data likelihood is P (X, Z|Θ, W p ) =  1 P (X, Z|Θ) K  exp(w δ(zi , zj )),  (6)  i j∈U (i)  where U (i) means the neighbors of the ith block. [sent-184, score-0.256]
</p><p>88 The EM algorithm can be roughly interpreted as iterating on two steps: 1) estimating the texture description (parameters of mixture model) based on segmentation, and 2) segmenting the image based on the texture description given by step 1. [sent-185, score-0.246]
</p><p>89 Gibbs sampling is used to estimate the posterior probability in each EM iteration. [sent-186, score-0.136]
</p><p>90 Equation (5) is reduced to P (zi |Z−i , X, Θ, W p ) ∝ P (xi , zi |Θ)  exp(2w δ(zi , zj )). [sent-187, score-0.411]
</p><p>91 This image is divided into 7x7 blocks and then rearranged to 49-dim vectors. [sent-190, score-0.19]
</p><p>92 For PPC model, the soft links 1  Downloaded from http://sipi. [sent-192, score-0.129]
</p><p>93 A typical clustering result of 4-component standard GMM and 4-component PPC with w = 2 are shown in Fig. [sent-197, score-0.215]
</p><p>94 Obviously, PPC achieves a better segmentation after incorporating spatial continuity. [sent-200, score-0.081]
</p><p>95 (c) and (d) are shaded according to the blocks assignments to clusters. [sent-204, score-0.166]
</p><p>96 4 Conclusion and Discussion We have proposed a probabilistic clustering model that incorporates prior knowledge in the form of pairwise relations between samples. [sent-205, score-0.83]
</p><p>97 Unlike previous work in semi-supervised clustering, this work formulates clustering preferences as a Bayesian prior over the assignment of data points to clusters, and so naturally accommodates both hard constraints and soft preferences. [sent-206, score-0.939]
</p><p>98 Experiments on different data sets show that pairwise relations can consistently improve the performance of the clustering process. [sent-208, score-0.798]
</p><p>99 From instance Level to space-level constraints: making the most of prior knowledge in data clustering. [sent-228, score-0.123]
</p><p>100 A multiscale random ﬁeld model for Bayesian image segmentation. [sent-250, score-0.074]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ppc', 0.513), ('gmm', 0.265), ('relations', 0.254), ('zi', 0.254), ('pairwise', 0.237), ('clustering', 0.215), ('preferences', 0.212), ('snow', 0.204), ('wij', 0.185), ('zj', 0.157), ('clique', 0.119), ('cluster', 0.119), ('assignment', 0.119), ('zn', 0.102), ('posterior', 0.102), ('assignments', 0.101), ('samples', 0.099), ('xj', 0.087), ('gibbs', 0.084), ('soft', 0.081), ('segmentation', 0.081), ('zt', 0.08), ('image', 0.074), ('hard', 0.073), ('prior', 0.071), ('texture', 0.067), ('blocks', 0.065), ('link', 0.063), ('xi', 0.06), ('ice', 0.051), ('pendigits', 0.051), ('rearranged', 0.051), ('relevancy', 0.051), ('xt', 0.051), ('cliques', 0.051), ('constraints', 0.051), ('clusters', 0.049), ('classification', 0.049), ('links', 0.048), ('assigned', 0.047), ('em', 0.046), ('continuity', 0.045), ('area', 0.045), ('nasa', 0.045), ('accommodates', 0.045), ('circumstances', 0.044), ('block', 0.043), ('nineteenth', 0.041), ('consistently', 0.041), ('classes', 0.04), ('pick', 0.04), ('mixture', 0.038), ('tagged', 0.038), ('subsection', 0.038), ('shental', 0.038), ('likelihood', 0.037), ('complete', 0.037), ('sample', 0.036), ('waveform', 0.036), ('participate', 0.036), ('iris', 0.036), ('expressed', 0.035), ('sampling', 0.034), ('brought', 0.033), ('exp', 0.032), ('perceived', 0.031), ('replacement', 0.031), ('training', 0.03), ('six', 0.029), ('incorporate', 0.028), ('seven', 0.027), ('knowledge', 0.027), ('white', 0.027), ('ects', 0.026), ('channel', 0.026), ('incorporates', 0.026), ('calculating', 0.026), ('familiar', 0.026), ('items', 0.026), ('remainder', 0.026), ('improve', 0.026), ('averaged', 0.026), ('points', 0.025), ('data', 0.025), ('tted', 0.025), ('penalized', 0.024), ('prefer', 0.024), ('labeled', 0.024), ('arg', 0.023), ('weighting', 0.023), ('kmeans', 0.022), ('algorithmically', 0.022), ('sheet', 0.022), ('brodatz', 0.022), ('rogers', 0.022), ('closure', 0.022), ('hoping', 0.022), ('formulates', 0.022), ('told', 0.022), ('incorporated', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="167-tfidf-1" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>Author: Zhengdong Lu, Todd K. Leen</p><p>Abstract: While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. We express clustering preferences in the prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. We ﬁt the model parameters with EM. Experiments on a variety of data sets show that PPC can consistently improve clustering results.</p><p>2 0.18514213 <a title="167-tfidf-2" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>Author: Le Lu, Gregory D. Hager, Laurent Younes</p><p>Abstract: Visual action recognition is an important problem in computer vision. In this paper, we propose a new method to probabilistically model and recognize actions of articulated objects, such as hand or body gestures, in image sequences. Our method consists of three levels of representation. At the low level, we ﬁrst extract a feature vector invariant to scale and in-plane rotation by using the Fourier transform of a circular spatial histogram. Then, spectral partitioning [20] is utilized to obtain an initial clustering; this clustering is then reﬁned using a temporal smoothness constraint. Gaussian mixture model (GMM) based clustering and density estimation in the subspace of linear discriminant analysis (LDA) are then applied to thousands of image feature vectors to obtain an intermediate level representation. Finally, at the high level we build a temporal multiresolution histogram model for each action by aggregating the clustering weights of sampled images belonging to that action. We discuss how this high level representation can be extended to achieve temporal scaling invariance and to include Bi-gram or Multi-gram transition information. Both image clustering and action recognition/segmentation results are given to show the validity of our three tiered representation.</p><p>3 0.14699851 <a title="167-tfidf-3" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>Author: Linli Xu, James Neufeld, Bryce Larson, Dale Schuurmans</p><p>Abstract: We propose a new method for clustering based on ﬁnding maximum margin hyperplanes through data. By reformulating the problem in terms of the implied equivalence relation matrix, we can pose the problem as a convex integer program. Although this still yields a difﬁcult computational problem, the hard-clustering constraints can be relaxed to a soft-clustering formulation which can be feasibly solved with a semidefinite program. Since our clustering technique only depends on the data through the kernel matrix, we can easily achieve nonlinear clusterings in the same manner as spectral clustering. Experimental results show that our maximum margin clustering technique often obtains more accurate results than conventional clustering methods. The real beneﬁt of our approach, however, is that it leads naturally to a semi-supervised training method for support vector machines. By maximizing the margin simultaneously on labeled and unlabeled training data, we achieve state of the art performance by using a single, integrated learning principle. 1</p><p>4 0.12300111 <a title="167-tfidf-4" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>Author: Erik B. Sudderth, Michael I. Mandel, William T. Freeman, Alan S. Willsky</p><p>Abstract: We describe a three–dimensional geometric hand model suitable for visual tracking applications. The kinematic constraints implied by the model’s joints have a probabilistic structure which is well described by a graphical model. Inference in this model is complicated by the hand’s many degrees of freedom, as well as multimodal likelihoods caused by ambiguous image measurements. We use nonparametric belief propagation (NBP) to develop a tracking algorithm which exploits the graph’s structure to control complexity, while avoiding costly discretization. While kinematic constraints naturally have a local structure, self– occlusions created by the imaging process lead to complex interpendencies in color and edge–based likelihood functions. However, we show that local structure may be recovered by introducing binary hidden variables describing the occlusion state of each pixel. We augment the NBP algorithm to infer these occlusion variables in a distributed fashion, and then analytically marginalize over them to produce hand position estimates which properly account for occlusion events. We provide simulations showing that NBP may be used to reﬁne inaccurate model initializations, as well as track hand motion through extended image sequences. 1</p><p>5 0.12078442 <a title="167-tfidf-5" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>Author: Scott J. Gaffney, Padhraic Smyth</p><p>Abstract: Clustering and prediction of sets of curves is an important problem in many areas of science and engineering. It is often the case that curves tend to be misaligned from each other in a continuous manner, either in space (across the measurements) or in time. We develop a probabilistic framework that allows for joint clustering and continuous alignment of sets of curves in curve space (as opposed to a ﬁxed-dimensional featurevector space). The proposed methodology integrates new probabilistic alignment models with model-based curve clustering algorithms. The probabilistic approach allows for the derivation of consistent EM learning algorithms for the joint clustering-alignment problem. Experimental results are shown for alignment of human growth data, and joint clustering and alignment of gene expression time-course data.</p><p>6 0.12001093 <a title="167-tfidf-6" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>7 0.1093149 <a title="167-tfidf-7" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>8 0.10830852 <a title="167-tfidf-8" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>9 0.10203286 <a title="167-tfidf-9" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>10 0.084933594 <a title="167-tfidf-10" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>11 0.084255628 <a title="167-tfidf-11" href="./nips-2004-Limits_of_Spectral_Clustering.html">103 nips-2004-Limits of Spectral Clustering</a></p>
<p>12 0.075550213 <a title="167-tfidf-12" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>13 0.075304829 <a title="167-tfidf-13" href="./nips-2004-Parametric_Embedding_for_Class_Visualization.html">145 nips-2004-Parametric Embedding for Class Visualization</a></p>
<p>14 0.071271442 <a title="167-tfidf-14" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>15 0.069504127 <a title="167-tfidf-15" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>16 0.068907 <a title="167-tfidf-16" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>17 0.067378655 <a title="167-tfidf-17" href="./nips-2004-Proximity_Graphs_for_Clustering_and_Manifold_Learning.html">150 nips-2004-Proximity Graphs for Clustering and Manifold Learning</a></p>
<p>18 0.063302457 <a title="167-tfidf-18" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>19 0.062713325 <a title="167-tfidf-19" href="./nips-2004-Integrating_Topics_and_Syntax.html">87 nips-2004-Integrating Topics and Syntax</a></p>
<p>20 0.062462859 <a title="167-tfidf-20" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.216), (1, 0.043), (2, -0.055), (3, -0.11), (4, 0.054), (5, -0.032), (6, -0.147), (7, 0.129), (8, -0.151), (9, 0.002), (10, -0.05), (11, 0.209), (12, -0.082), (13, -0.073), (14, 0.097), (15, 0.004), (16, -0.071), (17, 0.047), (18, -0.045), (19, -0.043), (20, -0.033), (21, -0.01), (22, 0.074), (23, -0.005), (24, 0.058), (25, -0.123), (26, 0.04), (27, 0.062), (28, 0.033), (29, -0.036), (30, 0.084), (31, 0.134), (32, -0.012), (33, -0.092), (34, 0.059), (35, 0.016), (36, -0.111), (37, -0.078), (38, 0.083), (39, 0.021), (40, -0.026), (41, 0.07), (42, 0.117), (43, -0.078), (44, 0.038), (45, 0.168), (46, 0.018), (47, 0.063), (48, -0.087), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96185517 <a title="167-lsi-1" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>Author: Zhengdong Lu, Todd K. Leen</p><p>Abstract: While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. We express clustering preferences in the prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. We ﬁt the model parameters with EM. Experiments on a variety of data sets show that PPC can consistently improve clustering results.</p><p>2 0.66508609 <a title="167-lsi-2" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>Author: Le Lu, Gregory D. Hager, Laurent Younes</p><p>Abstract: Visual action recognition is an important problem in computer vision. In this paper, we propose a new method to probabilistically model and recognize actions of articulated objects, such as hand or body gestures, in image sequences. Our method consists of three levels of representation. At the low level, we ﬁrst extract a feature vector invariant to scale and in-plane rotation by using the Fourier transform of a circular spatial histogram. Then, spectral partitioning [20] is utilized to obtain an initial clustering; this clustering is then reﬁned using a temporal smoothness constraint. Gaussian mixture model (GMM) based clustering and density estimation in the subspace of linear discriminant analysis (LDA) are then applied to thousands of image feature vectors to obtain an intermediate level representation. Finally, at the high level we build a temporal multiresolution histogram model for each action by aggregating the clustering weights of sampled images belonging to that action. We discuss how this high level representation can be extended to achieve temporal scaling invariance and to include Bi-gram or Multi-gram transition information. Both image clustering and action recognition/segmentation results are given to show the validity of our three tiered representation.</p><p>3 0.57328969 <a title="167-lsi-3" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>Author: Linli Xu, James Neufeld, Bryce Larson, Dale Schuurmans</p><p>Abstract: We propose a new method for clustering based on ﬁnding maximum margin hyperplanes through data. By reformulating the problem in terms of the implied equivalence relation matrix, we can pose the problem as a convex integer program. Although this still yields a difﬁcult computational problem, the hard-clustering constraints can be relaxed to a soft-clustering formulation which can be feasibly solved with a semidefinite program. Since our clustering technique only depends on the data through the kernel matrix, we can easily achieve nonlinear clusterings in the same manner as spectral clustering. Experimental results show that our maximum margin clustering technique often obtains more accurate results than conventional clustering methods. The real beneﬁt of our approach, however, is that it leads naturally to a semi-supervised training method for support vector machines. By maximizing the margin simultaneously on labeled and unlabeled training data, we achieve state of the art performance by using a single, integrated learning principle. 1</p><p>4 0.57269764 <a title="167-lsi-4" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>Author: Massimiliano Pavan, Marcello Pelillo</p><p>Abstract: Dominant sets are a new graph-theoretic concept that has proven to be relevant in pairwise data clustering problems, such as image segmentation. They generalize the notion of a maximal clique to edgeweighted graphs and have intriguing, non-trivial connections to continuous quadratic optimization and spectral-based grouping. We address the problem of grouping out-of-sample examples after the clustering process has taken place. This may serve either to drastically reduce the computational burden associated to the processing of very large data sets, or to efﬁciently deal with dynamic situations whereby data sets need to be updated continually. We show that the very notion of a dominant set offers a simple and efﬁcient way of doing this. Numerical experiments on various grouping problems show the effectiveness of the approach. 1</p><p>5 0.55712473 <a title="167-lsi-5" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>Author: Scott J. Gaffney, Padhraic Smyth</p><p>Abstract: Clustering and prediction of sets of curves is an important problem in many areas of science and engineering. It is often the case that curves tend to be misaligned from each other in a continuous manner, either in space (across the measurements) or in time. We develop a probabilistic framework that allows for joint clustering and continuous alignment of sets of curves in curve space (as opposed to a ﬁxed-dimensional featurevector space). The proposed methodology integrates new probabilistic alignment models with model-based curve clustering algorithms. The probabilistic approach allows for the derivation of consistent EM learning algorithms for the joint clustering-alignment problem. Experimental results are shown for alignment of human growth data, and joint clustering and alignment of gene expression time-course data.</p><p>6 0.52943444 <a title="167-lsi-6" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>7 0.52941573 <a title="167-lsi-7" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>8 0.50122541 <a title="167-lsi-8" href="./nips-2004-Sampling_Methods_for_Unsupervised_Learning.html">158 nips-2004-Sampling Methods for Unsupervised Learning</a></p>
<p>9 0.48393387 <a title="167-lsi-9" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>10 0.44869533 <a title="167-lsi-10" href="./nips-2004-Proximity_Graphs_for_Clustering_and_Manifold_Learning.html">150 nips-2004-Proximity Graphs for Clustering and Manifold Learning</a></p>
<p>11 0.44435772 <a title="167-lsi-11" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>12 0.39432153 <a title="167-lsi-12" href="./nips-2004-Limits_of_Spectral_Clustering.html">103 nips-2004-Limits of Spectral Clustering</a></p>
<p>13 0.38055557 <a title="167-lsi-13" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>14 0.37777901 <a title="167-lsi-14" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>15 0.37270841 <a title="167-lsi-15" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>16 0.36827323 <a title="167-lsi-16" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>17 0.36638466 <a title="167-lsi-17" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>18 0.35783339 <a title="167-lsi-18" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>19 0.34206274 <a title="167-lsi-19" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>20 0.32991791 <a title="167-lsi-20" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.083), (15, 0.2), (26, 0.053), (27, 0.108), (31, 0.037), (32, 0.033), (33, 0.214), (35, 0.042), (39, 0.014), (50, 0.033), (68, 0.06), (93, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9470908 <a title="167-lda-1" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>Author: Zhengdong Lu, Todd K. Leen</p><p>Abstract: While clustering is usually an unsupervised operation, there are circumstances in which we believe (with varying degrees of certainty) that items A and B should be assigned to the same cluster, while items A and C should not. We would like such pairwise relations to inﬂuence cluster assignments of out-of-sample data in a manner consistent with the prior knowledge expressed in the training set. Our starting point is probabilistic clustering based on Gaussian mixture models (GMM) of the data distribution. We express clustering preferences in the prior distribution over assignments of data points to clusters. This prior penalizes cluster assignments according to the degree with which they violate the preferences. We ﬁt the model parameters with EM. Experiments on a variety of data sets show that PPC can consistently improve clustering results.</p><p>2 0.93707353 <a title="167-lda-2" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>3 0.92618191 <a title="167-lda-3" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>4 0.91258764 <a title="167-lda-4" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>Author: Koji Tsuda, Gunnar Rätsch, Manfred K. Warmuth</p><p>Abstract: We address the problem of learning a symmetric positive deﬁnite matrix. The central issue is to design parameter updates that preserve positive deﬁniteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: On-line learning with a simple square loss and ﬁnding a symmetric positive deﬁnite matrix subject to symmetric linear constraints. The updates generalize the Exponentiated Gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive deﬁnite matrix of trace one instead of a probability vector (which in this context is a diagonal positive deﬁnite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive deﬁniteness. Most importantly, we show how the analysis of each algorithm generalizes to the non-diagonal case. We apply both new algorithms, called the Matrix Exponentiated Gradient (MEG) update and DeﬁniteBoost, to learn a kernel matrix from distance measurements.</p><p>5 0.91162282 <a title="167-lda-5" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>6 0.91078365 <a title="167-lda-6" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>7 0.90851295 <a title="167-lda-7" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>8 0.90844208 <a title="167-lda-8" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>9 0.90536141 <a title="167-lda-9" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>10 0.90437955 <a title="167-lda-10" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>11 0.9042933 <a title="167-lda-11" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>12 0.90333074 <a title="167-lda-12" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>13 0.90117061 <a title="167-lda-13" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>14 0.90009385 <a title="167-lda-14" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>15 0.89967096 <a title="167-lda-15" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>16 0.89882308 <a title="167-lda-16" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>17 0.89879251 <a title="167-lda-17" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>18 0.89738643 <a title="167-lda-18" href="./nips-2004-Hierarchical_Eigensolver_for_Transition_Matrices_in_Spectral_Methods.html">79 nips-2004-Hierarchical Eigensolver for Transition Matrices in Spectral Methods</a></p>
<p>19 0.89615119 <a title="167-lda-19" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>20 0.89613771 <a title="167-lda-20" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
