<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 nips-2004-Common-Frame Model for Object Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-40" href="#">nips2004-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 nips-2004-Common-Frame Model for Object Recognition</h1>
<br/><p>Source: <a title="nips-2004-40-pdf" href="http://papers.nips.cc/paper/2746-common-frame-model-for-object-recognition.pdf">pdf</a></p><p>Author: Pierre Moreels, Pietro Perona</p><p>Abstract: A generative probabilistic model for objects in images is presented. An object consists of a constellation of features. Feature appearance and pose are modeled probabilistically. Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. Occlusion is allowed. We study the case where features from the same object share a common reference frame. Moreover, parameters for shape and appearance densities are shared across features. This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. These two differences allow us to build models containing hundreds of features, as well as to train each model from a single example. Our model may also be thought of as a probabilistic revisitation of Lowe’s model [3, 4]. We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. We test our ideas with experiments on two image databases. We compare with Lowe’s algorithm and demonstrate better performance, in particular in presence of large amounts of background clutter.</p><p>Reference: <a title="nips-2004-40-reference" href="../nips2004_reference/nips-2004-Common-Frame_Model_for_Object_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract A generative probabilistic model for objects in images is presented. [sent-3, score-0.288]
</p><p>2 Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. [sent-6, score-0.492]
</p><p>3 We study the case where features from the same object share a common reference frame. [sent-8, score-0.288]
</p><p>4 Moreover, parameters for shape and appearance densities are shared across features. [sent-9, score-0.223]
</p><p>5 This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. [sent-10, score-0.657]
</p><p>6 We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. [sent-13, score-0.234]
</p><p>7 We test our ideas with experiments on two image databases. [sent-14, score-0.206]
</p><p>8 1 Introduction There is broad agreement in the machine vision literature that objects and object categories should be represented as collections of features or parts with distinctive appearance and mutual position [1, 2, 4, 5, 6, 7, 8, 9]. [sent-16, score-0.69]
</p><p>9 faces) have been proposed by virtually all the cited authors, far fewer for recognition (list all objects and their pose in a given image) where matching would ideally take a logarithmic time with respect to the number of available models [3, 4]. [sent-19, score-0.346]
</p><p>10 This work is based on two complementary efforts: the deterministic recognition system proposed by Lowe [3, 4], and the probabilistic constellation models by Perona and collaborators [1, 2]. [sent-21, score-0.267]
</p><p>11 The ﬁrst line of work has three attractive characteristics: objects are represented with hundreds of features, thus increasing robustness; models are learned from a single training example; last but not least, recognition is efﬁcient with databases of hundreds of objects. [sent-22, score-0.326]
</p><p>12 The drawback of Lowe’s approach is that both modeling decisions and algorithms rely on heuristics, whose design and performance may be far from optimal in  Figure 1: Diagram of our recognition model showing database, test image and two competing hypotheses. [sent-23, score-0.336]
</p><p>13 To avoid a cluttered diagram, only one partial hypothesis is displayed for each hypothesis. [sent-24, score-0.351]
</p><p>14 The predicted position of models according to the hypotheses are overlaid on the test image. [sent-25, score-0.325]
</p><p>15 Conversely, the second line of work is based on principled probabilistic object models which yield principled and, in some respects, optimal algorithms for learning and recognition/detection. [sent-27, score-0.209]
</p><p>16 A major difference with the work described here lies in the probabilistic treatment of hypotheses, which allows us here to use directly hypothesis likelihood as a guide for the search, instead of the arbitrary admissible heuristic required by A*. [sent-32, score-0.259]
</p><p>17 Each model is the set of features extracted from one training image of a given object - although this could be generalized to features from many images of the same object. [sent-35, score-0.658]
</p><p>18 Models are indexed by k and denoted by mk , while indices i and j are used respectively for features extracted from the test image k and from model images: f i denotes the i − th test feature, while f j denotes the j − th feature from the k − th model. [sent-36, score-0.814]
</p><p>19 The features extracted from model images (training set) form the database. [sent-37, score-0.3]
</p><p>20 A feature detected in a test image can be a consequence of the presence of a model object in the image, in which case it should be associated to a feature from the database. [sent-38, score-0.614]
</p><p>21 In the alternative, this feature is attributed to a clutter - or background - detection. [sent-39, score-0.329]
</p><p>22 The geometric information associated to each feature contains position information (x and y coordinates, denoted by the vector x), orientation (denoted by θ) and scale (denoted by k k σ). [sent-40, score-0.343]
</p><p>23 It is denoted by X i = (x, θi , σi ) for test feature f i and Xjk = (xk θj , σj ) for model j k feature fj . [sent-41, score-0.602]
</p><p>24 This geometric information is measured relatively to the standard reference frame of the image in which the feature has been detected. [sent-42, score-0.413]
</p><p>25 All features extracted from the same image share the same reference frame. [sent-43, score-0.343]
</p><p>26 The appearance information associated to a feature is a descriptor characterizing the local image appearance near this feature. [sent-44, score-0.668]
</p><p>27 The measured appearance information is denoted by  k Ai for test feature f i and Ak for model feature f j . [sent-45, score-0.582]
</p><p>28 In our experiments, features are detected j at multiple scales at the extrema of difference-of-gaussians ﬁltered versions of the image [4, 12]. [sent-46, score-0.285]
</p><p>29 A partial hypothesis h explains the observations made in a fraction of the test image. [sent-48, score-0.467]
</p><p>30 It combines a model image m h and a corresponding set of pose parameters X h . [sent-49, score-0.246]
</p><p>31 This allows us to search in parallel for multiple objects in a test image. [sent-56, score-0.241]
</p><p>32 A hypothesis H is the combination of several partial hypotheses, such that it explains completely the observations made in the test image. [sent-57, score-0.467]
</p><p>33 A special notation H 0 or h0 denotes any (partial) hypothesis that states that no model object is present in a given fraction of the test image, and that features that could have been detected there are due to clutter. [sent-58, score-0.616]
</p><p>34 Our objective is to ﬁnd which model objects are present in the test scene, given the observations made in the test scene and the information that is present in the database. [sent-59, score-0.45]
</p><p>35 In probabilistic terms, we look for hypotheses H for which the likelihood ration LR(H) = k P (H|{fi },{fj }) k P (H0 |{fi },{fj })  > 1. [sent-60, score-0.216]
</p><p>36 A key assumption of this work is that once the pose parameters of the objects (and thus their reference frames) are known, the geometric conﬁguration and appearance of the test features are independent from each other. [sent-63, score-0.787]
</p><p>37 We also assume independence between features associated to models and features associated to clutter detections, as well as independence k k between separate clutter detections. [sent-64, score-0.776]
</p><p>38 Assignment vectors v represent matches between features from the test scene, and model features or clutter. [sent-67, score-0.473]
</p><p>39 The dimension of each assignment vector is the number of test features ntest . [sent-68, score-0.45]
</p><p>40 Its i − th component v(i) = (k, j) denotes that the test feature f i is matched to k fv(i) = fj , j − th feature from model m k . [sent-69, score-0.645]
</p><p>41 The set V H of assignment vectors compatible with a hypothesis H are those that assign test features only to models present in H (and to clutter). [sent-71, score-0.671]
</p><p>42 In particular, the only assignment vector compatible with h 0 is v0 such that ∀i, v0 (i) = (0, 0). [sent-72, score-0.219]
</p><p>43 We obtain   LR(H) =  P (H) P (H0 )  k P (v|{fj }, mh , Xh ) · v∈VH h∈H  i|fi ∈h  P (fi |fv(i) , mh , Xh )  (2) P (fi |h0 )  k P (H) is a prior on hypotheses, we assume it is constant. [sent-73, score-0.43]
</p><p>44 The term P (v|{f j }, mh , Xh ) is discussed in 3. [sent-74, score-0.215]
</p><p>45 •P (fi |fv(i) , mh , Xh ) : fi and fv(i) are believed to be one and the same feature. [sent-76, score-0.469]
</p><p>46 This noise probability p n encodes differences in appearance of the descriptors, but also in geometry, i. [sent-78, score-0.193]
</p><p>47 position, scale, orientation Assuming independence between appearance information and geometry information, k pn (fi |fj , mh , Xh ) = pn,A (Ai |Av(i) , mh , Xh ) · pn,X (Xi |Xv(i) , mh , Xh )  (3)  Figure 2: Snapshots from the iterative matching process. [sent-80, score-1.049]
</p><p>48 Two competing hypotheses are displayed (top and bottom row) a) Each assignment vector contains one assignment, suggesting a transformation (red box) b) End of iterative process. [sent-81, score-0.381]
</p><p>49 The correct hypothesis is supported by numerous matches and high belief, while the wrong hypothesis has only a weak support from few matches and low belief. [sent-82, score-0.6]
</p><p>50 The error in geometry is measured by comparing the values observed in the test image, with the predicted values that would be observed if the model features were to be transformed according to the parameters X h . [sent-83, score-0.316]
</p><p>51 3 Search for the best interpretation of the test image The building block of the recognition process is a question, comparing a feature from a database model with a feature of the test image. [sent-89, score-0.738]
</p><p>52 A question selects a feature from the database, and tries to identify if and where this feature appears in the test image. [sent-90, score-0.355]
</p><p>53 1  Assignment vectors compatible with hypotheses  For a given hypothesis H, the set of possible assignment vectors V H is too large for explicit exploration. [sent-92, score-0.57]
</p><p>54 In particular, each assignment vector v and each model referenced in v implies a set of pose parameters X v (extracted e. [sent-95, score-0.311]
</p><p>55 Therefore, the term k P (v|{fj }, mh , Xh ) from (2) will be signiﬁcant only when X v ≈ Xh , i. [sent-98, score-0.215]
</p><p>56 when the pose implied by the assignment vector agrees with the pose speciﬁed by the partial hypothesis. [sent-100, score-0.511]
</p><p>57 (2) becomes LR(H) ≈  P (H) P (H0 )  h∈H i|fi ∈h  P (fi |fvh (i) , mh , Xh ) P (fi |h0 )  (6)  Our recognition system proceeds by asking questions sequentially and adding matches to assignment vectors. [sent-104, score-0.643]
</p><p>58 It is therefore natural to deﬁne, for a given hypothesis H and the corresponding assignment vector v H and t ≤ ntest , the belief in vH by pn (ft |fv(t) , mht , Xht ) B0 (vH ) = 1, Bt (vH ) = · Bt−1 (vH ) (7) pbg (ft |h0 ) The geometric part of the belief (cf. [sent-105, score-0.949]
</p><p>59 (3)-(5) characterizes how close the pose X v implied by the assignments is to the pose X h speciﬁed by the hypothesis. [sent-106, score-0.322]
</p><p>60 The geometric component of the belief characterizes the quality of the appearance match for the pairs (f i , fv(i) ). [sent-107, score-0.485]
</p><p>61 2 Entropy-based optimization Our goal is ﬁnding quickly the hypothesis that best explains the observations, i. [sent-109, score-0.276]
</p><p>62 the hypothesis (models+poses) that has the highest likelihood ratio. [sent-111, score-0.197]
</p><p>63 We compute such hypothesis incrementally by asking questions sequentially. [sent-112, score-0.267]
</p><p>64 a given model is present in the image) as soon as the belief of a corresponding hypothesis exceeds a given conﬁdence threshold. [sent-116, score-0.296]
</p><p>65 Therefore we approximate the MEE strategy with a simple heuristic: The next question consists of attempting to match one feature of the highest-belief model; speciﬁcally, the feature with best appearance match to a feature in the test image. [sent-126, score-0.889]
</p><p>66 Questions to be examined are created by pairing database features to the test features closest in terms of appearance. [sent-129, score-0.437]
</p><p>67 Note that since features encode location, orientation and scale, any single assignment between a test feature and a model feature contains enough information to characterize a similarity transformation. [sent-130, score-0.718]
</p><p>68 It is therefore natural to restrict the set of possible transformations to similarities, and to insert each candidate assignment in the corresponding geometric hash table entry. [sent-131, score-0.509]
</p><p>69 The set of hypotheses is initialized to the center of the hash table entries, and their belief is set to 1. [sent-133, score-0.349]
</p><p>70 A partial hypothesis corresponds to a hash table entry, we consider only the candidate assignments that fall into this same entry. [sent-135, score-0.546]
</p><p>71 The hypothesis H that currently has the highest likelihood ratio is selected. [sent-137, score-0.197]
</p><p>72 1, only the best assignment  Figure 3: Results from our algorithm in various situations (viewpoint change can be seen in Fig. [sent-140, score-0.22]
</p><p>73 Each row shows the best hypothesis in terms of belief. [sent-142, score-0.241]
</p><p>74 The threshold used is the repeatability rate deﬁned in [15] m vector is explored: if p n (fi |fj h , mh , Xh ) > pbg (fi ) the match is accepted and inserted in m the hypothesis. [sent-146, score-0.603]
</p><p>75 In the alternative, f i is considered a clutter detection and f j h is a missed detection. [sent-147, score-0.248]
</p><p>76 After adding an assignment to a hypothesis, frame parameters X h are recomputed using least-squares optimization, based on all assignments currently associated to this hypothesis. [sent-149, score-0.275]
</p><p>77 This parameter estimation step provides a progressive reﬁnement of the model pose parameters as assignments are added. [sent-150, score-0.189]
</p><p>78 The exploration of a partial hypothesis ends when no more candidate match is available in the hash table entry. [sent-153, score-0.603]
</p><p>79 The search ends when all test scene features have been matched or assigned to clutter. [sent-155, score-0.357]
</p><p>80 1 Experimental setting We tested our algorithm on two sets of images, containing respectively 49 and 161 model images, and 101 and 51 test images (sets P M − gadgets − 03 and JP − 3Dobjects − 04 available from http : //www. [sent-157, score-0.204]
</p><p>81 Test images contained from zero (negative examples) to ﬁve objects, for a total of 178 objects in the ﬁrst set, and 79 objects in the second set. [sent-163, score-0.31]
</p><p>82 A large fraction of each test image consists of background. [sent-164, score-0.206]
</p><p>83 The objects were always moved between model images and test images. [sent-168, score-0.321]
</p><p>84 The images of model objects used in the learning stage were downsampled to ﬁt in a 500 × 500 pixels box, the test images were downsampled to 800 × 800 pixels. [sent-169, score-0.469]
</p><p>85 With these settings, the number of features generated by the features detector was of the order of 1000 per training image and 2000-4000 per test image. [sent-170, score-0.466]
</p><p>86 A ground truth model was created by cutting a rectangle from the test image and adding noise. [sent-172, score-0.272]
</p><p>87 The two rows show the best and second best model found by each algorithm (estimated frame position shown by the red box, features that found a match are shown in yellow). [sent-174, score-0.423]
</p><p>88 In challenging situations with multiple objects or textured clutter, our method performs a more systematic check on geometric consistency by updating likelihoods every time a match is added. [sent-182, score-0.343]
</p><p>89 Hypotheses starting with wrong matches due to clutter don’t ﬁnd further supporting matches, and are easily discarded by a threshold based on the number of matches. [sent-183, score-0.309]
</p><p>90 Conversely, Lowe’s algorithm checks geometric consistency as a last step of the recognition process, but needs to allow for a large slop in the transformation parameters. [sent-184, score-0.206]
</p><p>91 Spurious matches induced by clutter detections may still be accepted, thus leading to the acceptance of incorrect hypotheses. [sent-185, score-0.336]
</p><p>92 5: the test image consists of a picture of concrete. [sent-187, score-0.206]
</p><p>93 A rectangular patch was extracted from this image, noise was added to this patch, and it was inserted in the database as a new model. [sent-188, score-0.184]
</p><p>94 With our algorithm, the best hypothesis found the correct match with the patch of concrete, its best contender doesn’t succeed in collecting more than one correspondence and is discarded. [sent-189, score-0.404]
</p><p>95 In Lowe’s case, other models manage to accumulate a high number of correspondences induced by texture matches among clutter detections. [sent-190, score-0.303]
</p><p>96 Both curves conﬁrm that our probabilistic interpretation leads to less false alarms than Lowe’s method for a same detection rate. [sent-195, score-0.201]
</p><p>97 5 Conclusion We have proposed an object recognition method that combines the beneﬁts of a set of rich features with those of a probabilistic model of features positions and appearance. [sent-196, score-0.569]
</p><p>98 The probabilistic model veriﬁes the validity of candidate hypotheses in terms of appearance and geometric conﬁguration. [sent-198, score-0.614]
</p><p>99 Our system improves upon a state-of-the art recognition method based on strict feature matching. [sent-199, score-0.204]
</p><p>100 In particular, the rate of false alarms in the presence  Figure 6: Sample scenes and training objects from the two sets of images. [sent-200, score-0.196]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lowe', 0.294), ('xh', 0.293), ('fi', 0.254), ('pbg', 0.222), ('mh', 0.215), ('fj', 0.213), ('hypothesis', 0.197), ('appearance', 0.193), ('clutter', 0.188), ('assignment', 0.176), ('hypotheses', 0.154), ('vh', 0.148), ('features', 0.13), ('hash', 0.129), ('fv', 0.129), ('objects', 0.117), ('object', 0.117), ('image', 0.111), ('geometric', 0.109), ('feature', 0.107), ('partial', 0.103), ('pose', 0.102), ('mee', 0.099), ('lr', 0.098), ('recognition', 0.097), ('test', 0.095), ('matches', 0.085), ('perona', 0.082), ('match', 0.081), ('constellation', 0.078), ('images', 0.076), ('xv', 0.073), ('scene', 0.073), ('belief', 0.066), ('bg', 0.064), ('pn', 0.064), ('detections', 0.063), ('candidate', 0.063), ('probabilistic', 0.062), ('extracted', 0.061), ('detection', 0.06), ('geometry', 0.058), ('independence', 0.055), ('categories', 0.054), ('assignments', 0.054), ('poses', 0.051), ('displayed', 0.051), ('database', 0.049), ('moreels', 0.049), ('ntest', 0.049), ('accepted', 0.049), ('denoted', 0.047), ('position', 0.046), ('question', 0.046), ('th', 0.045), ('frame', 0.045), ('detected', 0.044), ('best', 0.044), ('compatible', 0.043), ('hashing', 0.043), ('discretizing', 0.043), ('hundreds', 0.041), ('reference', 0.041), ('false', 0.04), ('alarm', 0.039), ('alarms', 0.039), ('questions', 0.039), ('patch', 0.038), ('observations', 0.037), ('inserted', 0.036), ('downsampled', 0.036), ('textured', 0.036), ('wrong', 0.036), ('characterizes', 0.036), ('characterize', 0.036), ('explains', 0.035), ('descriptor', 0.034), ('geman', 0.034), ('attributed', 0.034), ('orientation', 0.034), ('created', 0.033), ('distinctive', 0.033), ('fergus', 0.033), ('model', 0.033), ('transformations', 0.032), ('asking', 0.031), ('densities', 0.03), ('ai', 0.03), ('ends', 0.03), ('characterizing', 0.03), ('ft', 0.03), ('models', 0.03), ('entropy', 0.03), ('box', 0.03), ('occlusion', 0.029), ('search', 0.029), ('strategy', 0.028), ('viewpoint', 0.028), ('lighting', 0.028), ('implied', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999881 <a title="40-tfidf-1" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>Author: Pierre Moreels, Pietro Perona</p><p>Abstract: A generative probabilistic model for objects in images is presented. An object consists of a constellation of features. Feature appearance and pose are modeled probabilistically. Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. Occlusion is allowed. We study the case where features from the same object share a common reference frame. Moreover, parameters for shape and appearance densities are shared across features. This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. These two differences allow us to build models containing hundreds of features, as well as to train each model from a single example. Our model may also be thought of as a probabilistic revisitation of Lowe’s model [3, 4]. We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. We test our ideas with experiments on two image databases. We compare with Lowe’s algorithm and demonstrate better performance, in particular in presence of large amounts of background clutter.</p><p>2 0.15716293 <a title="40-tfidf-2" href="./nips-2004-Learning_Hyper-Features_for_Visual_Identification.html">99 nips-2004-Learning Hyper-Features for Visual Identification</a></p>
<p>Author: Andras D. Ferencz, Erik G. Learned-miller, Jitendra Malik</p><p>Abstract: We address the problem of identifying speciﬁc instances of a class (cars) from a set of images all belonging to that class. Although we cannot build a model for any particular instance (as we may be provided with only one “training” example of it), we can use information extracted from observing other members of the class. We pose this task as a learning problem, in which the learner is given image pairs, labeled as matching or not, and must discover which image features are most consistent for matching instances and discriminative for mismatches. We explore a patch based representation, where we model the distributions of similarity measurements deﬁned on the patches. Finally, we describe an algorithm that selects the most salient patches based on a mutual information criterion. This algorithm performs identiﬁcation well for our challenging dataset of car images, after matching only a few, well chosen patches. 1</p><p>3 0.15232739 <a title="40-tfidf-3" href="./nips-2004-Incremental_Learning_for_Visual_Tracking.html">83 nips-2004-Incremental Learning for Visual Tracking</a></p>
<p>Author: Jongwoo Lim, David A. Ross, Ruei-sung Lin, Ming-Hsuan Yang</p><p>Abstract: Most existing tracking algorithms construct a representation of a target object prior to the tracking task starts, and utilize invariant features to handle appearance variation of the target caused by lighting, pose, and view angle change. In this paper, we present an efﬁcient and effective online algorithm that incrementally learns and adapts a low dimensional eigenspace representation to reﬂect appearance changes of the target, thereby facilitating the tracking task. Furthermore, our incremental method correctly updates the sample mean and the eigenbasis, whereas existing incremental subspace update methods ignore the fact the sample mean varies over time. The tracking problem is formulated as a state inference problem within a Markov Chain Monte Carlo framework and a particle ﬁlter is incorporated for propagating sample distributions over time. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large pose and lighting changes. 1</p><p>4 0.15176944 <a title="40-tfidf-4" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>5 0.13279672 <a title="40-tfidf-5" href="./nips-2004-Generative_Affine_Localisation_and_Tracking.html">73 nips-2004-Generative Affine Localisation and Tracking</a></p>
<p>Author: John Winn, Andrew Blake</p><p>Abstract: We present an extension to the Jojic and Frey (2001) layered sprite model which allows for layers to undergo afﬁne transformations. This extension allows for afﬁne object pose to be inferred whilst simultaneously learning the object shape and appearance. Learning is carried out by applying an augmented variational inference algorithm which includes a global search over a discretised transform space followed by a local optimisation. To aid correct convergence, we use bottom-up cues to restrict the space of possible afﬁne transformations. We present results on a number of video sequences and show how the model can be extended to track an object whose appearance changes throughout the sequence. 1</p><p>6 0.12930462 <a title="40-tfidf-6" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>7 0.12569498 <a title="40-tfidf-7" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>8 0.11425965 <a title="40-tfidf-8" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>9 0.10872893 <a title="40-tfidf-9" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>10 0.10153089 <a title="40-tfidf-10" href="./nips-2004-Contextual_Models_for_Object_Detection_Using_Boosted_Random_Fields.html">47 nips-2004-Contextual Models for Object Detection Using Boosted Random Fields</a></p>
<p>11 0.10054053 <a title="40-tfidf-11" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>12 0.094281033 <a title="40-tfidf-12" href="./nips-2004-Supervised_Graph_Inference.html">177 nips-2004-Supervised Graph Inference</a></p>
<p>13 0.089613594 <a title="40-tfidf-13" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>14 0.085656658 <a title="40-tfidf-14" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>15 0.085578397 <a title="40-tfidf-15" href="./nips-2004-The_Correlated_Correspondence_Algorithm_for_Unsupervised_Registration_of_Nonrigid_Surfaces.html">186 nips-2004-The Correlated Correspondence Algorithm for Unsupervised Registration of Nonrigid Surfaces</a></p>
<p>16 0.079666868 <a title="40-tfidf-16" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>17 0.07815896 <a title="40-tfidf-17" href="./nips-2004-Instance-Based_Relevance_Feedback_for_Image_Retrieval.html">85 nips-2004-Instance-Based Relevance Feedback for Image Retrieval</a></p>
<p>18 0.076322734 <a title="40-tfidf-18" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>19 0.07162331 <a title="40-tfidf-19" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>20 0.071443081 <a title="40-tfidf-20" href="./nips-2004-Joint_Tracking_of_Pose%2C_Expression%2C_and_Texture_using_Conditionally_Gaussian_Filters.html">91 nips-2004-Joint Tracking of Pose, Expression, and Texture using Conditionally Gaussian Filters</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.225), (1, 0.042), (2, -0.057), (3, -0.223), (4, 0.223), (5, 0.06), (6, 0.044), (7, -0.102), (8, -0.061), (9, 0.026), (10, -0.078), (11, 0.118), (12, 0.078), (13, 0.092), (14, -0.095), (15, -0.054), (16, 0.027), (17, 0.034), (18, -0.003), (19, -0.046), (20, 0.026), (21, 0.022), (22, 0.002), (23, 0.024), (24, 0.031), (25, -0.014), (26, 0.011), (27, -0.042), (28, 0.01), (29, -0.027), (30, -0.103), (31, -0.045), (32, 0.073), (33, 0.014), (34, -0.086), (35, 0.024), (36, 0.115), (37, 0.027), (38, -0.113), (39, 0.001), (40, -0.001), (41, 0.009), (42, 0.052), (43, 0.056), (44, -0.033), (45, 0.013), (46, 0.139), (47, -0.051), (48, 0.051), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9636538 <a title="40-lsi-1" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>Author: Pierre Moreels, Pietro Perona</p><p>Abstract: A generative probabilistic model for objects in images is presented. An object consists of a constellation of features. Feature appearance and pose are modeled probabilistically. Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. Occlusion is allowed. We study the case where features from the same object share a common reference frame. Moreover, parameters for shape and appearance densities are shared across features. This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. These two differences allow us to build models containing hundreds of features, as well as to train each model from a single example. Our model may also be thought of as a probabilistic revisitation of Lowe’s model [3, 4]. We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. We test our ideas with experiments on two image databases. We compare with Lowe’s algorithm and demonstrate better performance, in particular in presence of large amounts of background clutter.</p><p>2 0.72488165 <a title="40-lsi-2" href="./nips-2004-Generative_Affine_Localisation_and_Tracking.html">73 nips-2004-Generative Affine Localisation and Tracking</a></p>
<p>Author: John Winn, Andrew Blake</p><p>Abstract: We present an extension to the Jojic and Frey (2001) layered sprite model which allows for layers to undergo afﬁne transformations. This extension allows for afﬁne object pose to be inferred whilst simultaneously learning the object shape and appearance. Learning is carried out by applying an augmented variational inference algorithm which includes a global search over a discretised transform space followed by a local optimisation. To aid correct convergence, we use bottom-up cues to restrict the space of possible afﬁne transformations. We present results on a number of video sequences and show how the model can be extended to track an object whose appearance changes throughout the sequence. 1</p><p>3 0.69810236 <a title="40-lsi-3" href="./nips-2004-Learning_Hyper-Features_for_Visual_Identification.html">99 nips-2004-Learning Hyper-Features for Visual Identification</a></p>
<p>Author: Andras D. Ferencz, Erik G. Learned-miller, Jitendra Malik</p><p>Abstract: We address the problem of identifying speciﬁc instances of a class (cars) from a set of images all belonging to that class. Although we cannot build a model for any particular instance (as we may be provided with only one “training” example of it), we can use information extracted from observing other members of the class. We pose this task as a learning problem, in which the learner is given image pairs, labeled as matching or not, and must discover which image features are most consistent for matching instances and discriminative for mismatches. We explore a patch based representation, where we model the distributions of similarity measurements deﬁned on the patches. Finally, we describe an algorithm that selects the most salient patches based on a mutual information criterion. This algorithm performs identiﬁcation well for our challenging dataset of car images, after matching only a few, well chosen patches. 1</p><p>4 0.60718524 <a title="40-lsi-4" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>5 0.59548664 <a title="40-lsi-5" href="./nips-2004-Incremental_Learning_for_Visual_Tracking.html">83 nips-2004-Incremental Learning for Visual Tracking</a></p>
<p>Author: Jongwoo Lim, David A. Ross, Ruei-sung Lin, Ming-Hsuan Yang</p><p>Abstract: Most existing tracking algorithms construct a representation of a target object prior to the tracking task starts, and utilize invariant features to handle appearance variation of the target caused by lighting, pose, and view angle change. In this paper, we present an efﬁcient and effective online algorithm that incrementally learns and adapts a low dimensional eigenspace representation to reﬂect appearance changes of the target, thereby facilitating the tracking task. Furthermore, our incremental method correctly updates the sample mean and the eigenbasis, whereas existing incremental subspace update methods ignore the fact the sample mean varies over time. The tracking problem is formulated as a state inference problem within a Markov Chain Monte Carlo framework and a particle ﬁlter is incorporated for propagating sample distributions over time. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large pose and lighting changes. 1</p><p>6 0.55882418 <a title="40-lsi-6" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>7 0.5586375 <a title="40-lsi-7" href="./nips-2004-Contextual_Models_for_Object_Detection_Using_Boosted_Random_Fields.html">47 nips-2004-Contextual Models for Object Detection Using Boosted Random Fields</a></p>
<p>8 0.53036898 <a title="40-lsi-8" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>9 0.50199145 <a title="40-lsi-9" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>10 0.49955937 <a title="40-lsi-10" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>11 0.47262996 <a title="40-lsi-11" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>12 0.43934608 <a title="40-lsi-12" href="./nips-2004-The_Variational_Ising_Classifier_%28VIC%29_Algorithm_for_Coherently_Contaminated_Data.html">191 nips-2004-The Variational Ising Classifier (VIC) Algorithm for Coherently Contaminated Data</a></p>
<p>13 0.43573746 <a title="40-lsi-13" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>14 0.42391726 <a title="40-lsi-14" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>15 0.42374304 <a title="40-lsi-15" href="./nips-2004-The_Correlated_Correspondence_Algorithm_for_Unsupervised_Registration_of_Nonrigid_Surfaces.html">186 nips-2004-The Correlated Correspondence Algorithm for Unsupervised Registration of Nonrigid Surfaces</a></p>
<p>16 0.39056516 <a title="40-lsi-16" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>17 0.38900009 <a title="40-lsi-17" href="./nips-2004-Instance-Based_Relevance_Feedback_for_Image_Retrieval.html">85 nips-2004-Instance-Based Relevance Feedback for Image Retrieval</a></p>
<p>18 0.38875517 <a title="40-lsi-18" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<p>19 0.3821207 <a title="40-lsi-19" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>20 0.3786931 <a title="40-lsi-20" href="./nips-2004-Joint_Tracking_of_Pose%2C_Expression%2C_and_Texture_using_Conditionally_Gaussian_Filters.html">91 nips-2004-Joint Tracking of Pose, Expression, and Texture using Conditionally Gaussian Filters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.076), (15, 0.155), (17, 0.017), (24, 0.298), (26, 0.062), (31, 0.034), (33, 0.181), (35, 0.022), (39, 0.017), (50, 0.028), (71, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8206687 <a title="40-lda-1" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>Author: Pierre Moreels, Pietro Perona</p><p>Abstract: A generative probabilistic model for objects in images is presented. An object consists of a constellation of features. Feature appearance and pose are modeled probabilistically. Scene images are generated by drawing a set of objects from a given database, with random clutter sprinkled on the remaining image surface. Occlusion is allowed. We study the case where features from the same object share a common reference frame. Moreover, parameters for shape and appearance densities are shared across features. This is to be contrasted with previous work on probabilistic ‘constellation’ models where features depend on each other, and each feature and model have different pose and appearance statistics [1, 2]. These two differences allow us to build models containing hundreds of features, as well as to train each model from a single example. Our model may also be thought of as a probabilistic revisitation of Lowe’s model [3, 4]. We propose an efﬁcient entropy-minimization inference algorithm that constructs the best interpretation of a scene as a collection of objects and clutter. We test our ideas with experiments on two image databases. We compare with Lowe’s algorithm and demonstrate better performance, in particular in presence of large amounts of background clutter.</p><p>2 0.80195493 <a title="40-lda-2" href="./nips-2004-At_the_Edge_of_Chaos%3A_Real-time_Computations_and_Self-Organized_Criticality_in_Recurrent_Neural_Networks.html">26 nips-2004-At the Edge of Chaos: Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks</a></p>
<p>Author: Nils Bertschinger, Thomas Natschläger, Robert A. Legenstein</p><p>Abstract: In this paper we analyze the relationship between the computational capabilities of randomly connected networks of threshold gates in the timeseries domain and their dynamical properties. In particular we propose a complexity measure which we ﬁnd to assume its highest values near the edge of chaos, i.e. the transition from ordered to chaotic dynamics. Furthermore we show that the proposed complexity measure predicts the computational capabilities very well: only near the edge of chaos are such networks able to perform complex computations on time series. Additionally a simple synaptic scaling rule for self-organized criticality is presented and analyzed. 1</p><p>3 0.69236666 <a title="40-lda-3" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>Author: Saharon Rosset, Ji Zhu, Hui Zou, Trevor J. Hastie</p><p>Abstract: We consider the situation in semi-supervised learning, where the “label sampling” mechanism stochastically depends on the true response (as well as potentially on the features). We suggest a method of moments for estimating this stochastic dependence using the unlabeled data. This is potentially useful for two distinct purposes: a. As an input to a supervised learning procedure which can be used to “de-bias” its results using labeled data only and b. As a potentially interesting learning task in itself. We present several examples to illustrate the practical usefulness of our method.</p><p>4 0.66134977 <a title="40-lda-4" href="./nips-2004-Discriminant_Saliency_for_Visual_Recognition_from_Cluttered_Scenes.html">53 nips-2004-Discriminant Saliency for Visual Recognition from Cluttered Scenes</a></p>
<p>Author: Dashan Gao, Nuno Vasconcelos</p><p>Abstract: Saliency mechanisms play an important role when visual recognition must be performed in cluttered scenes. We propose a computational deﬁnition of saliency that deviates from existing models by equating saliency to discrimination. In particular, the salient attributes of a given visual class are deﬁned as the features that enable best discrimination between that class and all other classes of recognition interest. It is shown that this deﬁnition leads to saliency algorithms of low complexity, that are scalable to large recognition problems, and is compatible with existing models of early biological vision. Experimental results demonstrating success in the context of challenging recognition problems are also presented. 1</p><p>5 0.65505844 <a title="40-lda-5" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>Author: Ruei-sung Lin, David A. Ross, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: This paper presents an adaptive discriminative generative model that generalizes the conventional Fisher Linear Discriminant algorithm and renders a proper probabilistic interpretation. Within the context of object tracking, we aim to ﬁnd a discriminative generative model that best separates the target from the background. We present a computationally efﬁcient algorithm to constantly update this discriminative model as time progresses. While most tracking algorithms operate on the premise that the object appearance or ambient lighting condition does not signiﬁcantly change as time progresses, our method adapts a discriminative generative model to reﬂect appearance variation of the target and background, thereby facilitating the tracking task in ever-changing environments. Numerous experiments show that our method is able to learn a discriminative generative model for tracking target objects undergoing large pose and lighting changes.</p><p>6 0.65471512 <a title="40-lda-6" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>7 0.65239108 <a title="40-lda-7" href="./nips-2004-Edge_of_Chaos_Computation_in_Mixed-Mode_VLSI_-_A_Hard_Liquid.html">58 nips-2004-Edge of Chaos Computation in Mixed-Mode VLSI - A Hard Liquid</a></p>
<p>8 0.6506868 <a title="40-lda-8" href="./nips-2004-Methods_for_Estimating_the_Computational_Power_and_Generalization_Capability_of_Neural_Microcircuits.html">118 nips-2004-Methods for Estimating the Computational Power and Generalization Capability of Neural Microcircuits</a></p>
<p>9 0.65018785 <a title="40-lda-9" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>10 0.64897639 <a title="40-lda-10" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>11 0.64894307 <a title="40-lda-11" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>12 0.64810652 <a title="40-lda-12" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>13 0.64708108 <a title="40-lda-13" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>14 0.64597809 <a title="40-lda-14" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>15 0.64571929 <a title="40-lda-15" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>16 0.64563185 <a title="40-lda-16" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>17 0.64462179 <a title="40-lda-17" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>18 0.64452732 <a title="40-lda-18" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>19 0.64443839 <a title="40-lda-19" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>20 0.64442599 <a title="40-lda-20" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
