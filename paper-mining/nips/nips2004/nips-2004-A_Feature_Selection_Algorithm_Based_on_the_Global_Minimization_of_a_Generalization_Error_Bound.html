<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-3" href="#">nips2004-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</h1>
<br/><p>Source: <a title="nips-2004-3-pdf" href="http://papers.nips.cc/paper/2629-a-feature-selection-algorithm-based-on-the-global-minimization-of-a-generalization-error-bound.pdf">pdf</a></p><p>Author: Dori Peleg, Ron Meir</p><p>Abstract: A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. Highly competitive numerical results on both artiﬁcial and real-world data sets are reported. 1</p><p>Reference: <a title="nips-2004-3-reference" href="../nips2004_reference/nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A feature selection algorithm based on the global minimization of a generalization error bound Dori Peleg Department of Electrical Engineering Technion Haifa, Israel dorip@tx. [sent-1, score-0.623]
</p><p>2 il  Abstract A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. [sent-7, score-0.593]
</p><p>3 Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. [sent-8, score-0.39]
</p><p>4 We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. [sent-9, score-0.449]
</p><p>5 Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. [sent-10, score-0.343]
</p><p>6 1  Introduction  This paper presents a new approach to feature selection for linear classiﬁcation where the n goal is to learn a decision rule from a training set of pairs Sn = x(i) , y (i) i=1 , where x(i) ∈ Rd are input patterns and y (i) ∈ {−1, 1} are the corresponding labels. [sent-12, score-0.507]
</p><p>7 Feature selection schemes often utilize, either explicitly or implicitly, scaling variables, {σ j }d , j=1 which multiply each feature. [sent-16, score-0.29]
</p><p>8 Feature selection can be viewed as the case σj ∈ {0, 1}, j = 1, . [sent-18, score-0.168]
</p><p>9 The more general case of feature scaling is considered here, i. [sent-22, score-0.232]
</p><p>10 Clearly feature selection is a special case of feature scaling. [sent-25, score-0.524]
</p><p>11 The overwhelming majority of feature selection algorithms in the literature, separate the feature selection and classiﬁcation tasks, while solving either a combinatorial or a nonconvex optimization problem (e. [sent-26, score-0.892]
</p><p>12 Moreover, the objective function of many feature selection algorithms is unrelated to the Generalization Error (GE). [sent-31, score-0.448]
</p><p>13 Even for global solutions of such algorithms there is no theoretical guarantee of proximity to the minimum of the GE. [sent-32, score-0.135]
</p><p>14 To overcome the above shortcomings we propose a feature selection algorithm based on the Global Minimization of an Error Bound (GMEB). [sent-33, score-0.346]
</p><p>15 This approach is based on simultaneously ﬁnding the optimal classiﬁer and scaling factors of each feature by minimizing a GE bound. [sent-34, score-0.232]
</p><p>16 As in previous feature selection algorithms, a non-convex optimization problem must be solved. [sent-35, score-0.483]
</p><p>17 A novelty of this paper is the use of the equivalent optimization problems concept, whereby a global optimum is guaranteed in polynomial time. [sent-36, score-0.329]
</p><p>18 The development of the GMEB algorithm begins with the design of a GE bound for feature selection. [sent-37, score-0.249]
</p><p>19 This is followed by formulating an optimization problem which minimizes this bound. [sent-38, score-0.137]
</p><p>20 To avoid the drawbacks of solving non-convex optimization problems, an equivalent convex optimization problem is formulated whereby the exact global optimum of the non-convex problem can be computed. [sent-40, score-0.591]
</p><p>21 Next the dual problem is derived and formulated as a Conic Quadratic Programming (CQP) problem. [sent-41, score-0.125]
</p><p>22 The set of points for which the objective and all the constraint functions are deﬁned is called the domain of the optimization problem, D. [sent-51, score-0.253]
</p><p>23 2  The Generalization Error Bounds  We establish GE bounds which are used to motivate an effective algorithm for feature scaling. [sent-53, score-0.276]
</p><p>24 , σd )T is introduced to allow the additional freedom of feature scaling. [sent-61, score-0.209]
</p><p>25 The scaling variables σ transform the linear classiﬁers from f (x) = w T x + b to f (x) = w T Σx + b, where Σ = diag(σ). [sent-62, score-0.177]
</p><p>26 However the role of σ is to offer an extra degree of freedom to scale the features independently of w, in a way which can be exploited by an optimization algorithm. [sent-64, score-0.193]
</p><p>27 Unfortunately, (1) cannot be used directly when attempting to select optimal values of the variables σ because the bound is not uniform in σ. [sent-70, score-0.153]
</p><p>28 Deﬁnition 2 The indices of training patterns with labels {−1, 1} are denoted by I − , I+ respectively. [sent-72, score-0.12]
</p><p>29 The empirical mean of the second order moment of the jth feature over the training patterns belonging to − indices I− , I+ are vj =  1 n−  (i)  xj  i∈I−  2  + vj =  ,  1 n+  (i)  i∈I+  xj  2  respectively. [sent-74, score-0.706]
</p><p>30 δ  Proof sketch We begin by assuming a ﬁxed upper bound on the values of σj , say σj ≤ sj , j = 1, 2, . [sent-79, score-0.105]
</p><p>31 This allows us to use the methods developed in [6] in order to establish upper bounds on the Rademacher complexity of the class F, where σj ≤ sj for all j. [sent-83, score-0.132]
</p><p>32 Finally, a simple variant of the union bound (the so-called multiple testing lemma) is used in order to obtain a bound which is uniform with respect to σ (see the proof technique of Theorem 10 in [6]). [sent-84, score-0.142]
</p><p>33 However, in this work the focus is only on the data-dependent terms in (2), which include the empirical error term and the weighted norms of σ. [sent-89, score-0.115]
</p><p>34 The data-dependent terms of the GE bound (2) are the basis of the objective function 1 nγ  n  φγ i=1  √ C+ n + y f (x ) + nγ (i)  d + 2 vj σ j  (i)  j=1  √ C− n − + nγ  d − 2 vj σ j ,  (3)  j=1  where C+ = C− = 4 and the variables are subject to w T w ≤ 1, σ 0. [sent-95, score-0.582]
</p><p>35 γ γ  The objective function is comprised of two elements: (1) the mean of the penalty on the training errors (2) and two weighted l2 norms of the scale variables σ. [sent-107, score-0.243]
</p><p>36 The second term acts as the feature selection element. [sent-108, score-0.346]
</p><p>37 To allow more generality and ﬂexibility in practical applications, we propose to turn the norm terms of (3) into inequality constraints which are bounded by hyperparameters R+ , R− respectively. [sent-110, score-0.147]
</p><p>38 Therefore, as a preprocessing step the features of the training patterns should be set to zero mean and the features of the test set shifted accordingly. [sent-115, score-0.286]
</p><p>39 3  The primal non-convex optimization problem  The problem of minimizing (3) with γ = 1 can then be expressed as minimize 1T ξ subject to w T w ≤ 1 (i) d y (i) ( j=1 xj wj σj + b) ≥ 1 − ξi , i = 1, . [sent-116, score-0.707]
</p><p>40 , n d + 2 R+ ≥ j=1 vj σj d − 2 R− ≥ j=1 vj σj ξ, σ 0, with variables w, σ ∈ Rd , ξ ∈ Rn , b ∈ R. [sent-119, score-0.396]
</p><p>41 Remark 4 Consider a solution of problem (4) in which σj = 0 for some feature j. [sent-121, score-0.249]
</p><p>42 Only the constraint w T w ≤ 1 affects the value of wj . [sent-122, score-0.379]
</p><p>43 A unique solution is established by setting σj = 0 ⇒ wj = 0. [sent-123, score-0.357]
</p><p>44 If the original solution w satisﬁes the constraint w T w ≤ 1 then the amended solution will also satisfy the constraint and won’t affect the value of the objective function. [sent-124, score-0.243]
</p><p>45 The functions wj σj in the second inequality constraints are neither convex nor concave (in fact they are quasiconcave [5]). [sent-125, score-0.526]
</p><p>46 To make matters worse, the functions wj σj are multiplied (i) by constants −y (i) xj which can be either positive or negative. [sent-126, score-0.471]
</p><p>47 Consequently problem (4) is not a convex optimization problem. [sent-127, score-0.224]
</p><p>48 1  Convexiﬁcation  In this paper the informal deﬁnition of equivalent optimization problems is adopted from [5, pp. [sent-131, score-0.18]
</p><p>49 130–135]: two optimization problems are called equivalent if from a solution of one, a solution of the other is found, and vice versa. [sent-132, score-0.211]
</p><p>50 The functions wj σj in problem (4) are not convex and the signs of the multiplying constants (i) −y (i) xj are data dependant. [sent-134, score-0.565]
</p><p>51 The only functions that remain convex irrespective of the sign of the constants which multiply them are linear functions. [sent-135, score-0.233]
</p><p>52 However, such a transformation must also maintain the convexity of the objective function and the remaining constraints. [sent-137, score-0.12]
</p><p>53 For this purpose the change of variables equivalence relationship, described in appendix A, was utilized. [sent-138, score-0.13]
</p><p>54 The transformation φ : Rd ×Rd → Rd ×Rd was used on the variables w, σ: σj = +  σj , wj = ˜  wj ˜ , σj ˜  (5)  j = 1, . [sent-139, score-0.783]
</p><p>55 If σj = 0 then σj = wj = 0 without regard to the ˜ value of wj , in accordance with remark 4. [sent-143, score-0.715]
</p><p>56 Lemma 5 The problem minimize subject to  1T ξ y (i) (wT x(i) + b) ≥ 1 − ξi , i = 1, . [sent-145, score-0.127]
</p><p>57 , n ˜ wj ˜2 d j=1 σj ˜  ≤1 R+ ≥ (v + )T σ ˜ R− ≥ (v − )T σ ˜ ξ, σ 0 ˜  (6)  is convex and equivalent to the primal non-convex problem (4) with transformation (5). [sent-148, score-0.62]
</p><p>58 Note that since wj = wj σj , the new classiﬁer is f (x) = w T x + b. [sent-149, score-0.648]
</p><p>59 Also one can use Schur’s complement [5] to transform the non-linear constraint into a sparse linear matrix inequality constraint Σ w 0. [sent-151, score-0.234]
</p><p>60 The primal problem therefore, consists of n + 2d + 1 variables, 2n + d + 2 linear inequality constraints and a linear matrix inequality of [(d + 1) × (d + 1)] dimensions. [sent-153, score-0.358]
</p><p>61 Although the primal problem (6) is convex, it heavily relies on the number of features d which is typically the bottleneck for feature selection datasets. [sent-154, score-0.519]
</p><p>62 Theorem 6 (Dual problem) The dual optimization problem associated with problem (6) is maximize subject to  1 T µ − µ 1 − R + µ+ − R − µ− n + − (i) (i) r i=1 µi y xj , 2µ1 , (µ+ vj + µ− vj ) ∈ K T µ y=0 0 µ 1 µ+ , µ− ≥ 0,  , j = 1, . [sent-156, score-0.671]
</p><p>63 The number of variables is n + 3, there are 2n+2 linear inequality constraints, a single linear equality constraint and d RQC inequality constraints. [sent-162, score-0.385]
</p><p>64 4  Experiments  Several algorithms were comparatively evaluated on a number of artiﬁcial and real world two class problem datasets. [sent-164, score-0.13]
</p><p>65 1  Experimental Methodology  The algorithms are compared by two criteria: the number of selected features and the error rates. [sent-167, score-0.211]
</p><p>66 The weight assigned by a linear classiﬁer to a feature j, determines whether it shall be ‘selected’ or ‘rejected’. [sent-168, score-0.219]
</p><p>67 Note that for the  The deﬁnition of the error rate is intrinsically entwined with the protocol for determining the hyperparameter. [sent-177, score-0.117]
</p><p>68 Given an a-priori partitioning of the dataset into training and test sets, the following protocol for determining the value of R+ , R− and deﬁning the error rate is suggested: 1. [sent-178, score-0.33]
</p><p>69 Calculate the N-fold CV error for each value of R+ , R− from set R on the training set. [sent-183, score-0.117]
</p><p>70 Use the classiﬁer with the value of R+ , R− which produced the lowest CV error to classify the test set. [sent-186, score-0.154]
</p><p>71 If the dataset is not partitioned a-priori into a training and test set, it is randomly divided n −1 into np contiguous training and ‘test’ sets. [sent-188, score-0.327]
</p><p>72 Each training set contains n p p patterns and n n the corresponding test set consists of np patterns. [sent-189, score-0.243]
</p><p>73 The error rate and the number of selected features are then deﬁned as the average on the np problems. [sent-191, score-0.259]
</p><p>74 The value np = 10 was used for all datasets, where an a-priori partitioning was not available. [sent-192, score-0.131]
</p><p>75 2  Data sets  Tests were performed on the ‘Linear problem’ synthetic datasets as described in [2], and eight real-world problems. [sent-206, score-0.16]
</p><p>76 The number of features, the number of patterns and the partitioning into train and test sets of the real-world datasets are detailed in Table 2. [sent-207, score-0.264]
</p><p>77 The datasets were taken form the UCI repository unless stated otherwise. [sent-208, score-0.135]
</p><p>78 Table 1: Mean and standard deviation of the mean of test error rate percentage on synthetic datasets given n training patterns. [sent-210, score-0.317]
</p><p>79 8)  Table 2: The real-world datasets and the performance of the algorithms. [sent-272, score-0.104]
</p><p>80 The set R for the linear SVM algorithm and for datasets 1,5,6 had to be set to Λ to allow convergence. [sent-273, score-0.145]
</p><p>81 Note that the number of features selected by the l1 SVM and the GMEB algorithms increase with the sample size. [sent-359, score-0.142]
</p><p>82 A possible explanation for this observation is that with only a few training patterns a small training error can be achieved by many subsets containing a small number of features, i. [sent-360, score-0.266]
</p><p>83 For all the synthetic datasets the GMEB algorithm clearly attained the lowest error rates. [sent-364, score-0.274]
</p><p>84 On the real-world datasets it produced the lowest error rates and the smallest number of features for the majority of datasets investigated. [sent-365, score-0.385]
</p><p>85 4  Discussion  The GMEB algorithm performs comparatively well against the linear and l1 SVM algorithms, in regard to both the test error and the number of selected features. [sent-367, score-0.287]
</p><p>86 A possible explanation is that the l1 SVM algorithm performs both classiﬁcation and feature selection with the same variable w. [sent-368, score-0.375]
</p><p>87 In contrast, the GMEB algorithm performs the feature selection and classiﬁcation simultaneously, while using variables σ and w respectively. [sent-369, score-0.428]
</p><p>88 The use of two variables also allows the GMEB algorithm to reduce the weight of a feature j with both wj and σj , while the l1 SVM uses only wj . [sent-370, score-0.908]
</p><p>89 Perhaps this property of GMEB could explain why it produces comparable (and at times better) results than the SVM algorithms both in classiﬁcation problems where feature selection is and is not required. [sent-371, score-0.381]
</p><p>90 5  Summary and future work  This paper presented a feature selection algorithm motivated by minimizing a GE bound. [sent-372, score-0.346]
</p><p>91 The global optimum of the objective function is found by solving a non-convex optimization problem. [sent-373, score-0.282]
</p><p>92 The equivalent optimization problems technique reduces this task to a convex problem. [sent-374, score-0.232]
</p><p>93 The dual problem formulation depends more weakly on the number of features d and this enabled an extension of the GMEB algorithm to large scale classiﬁcation problems. [sent-375, score-0.188]
</p><p>94 Linear classiﬁers are the most important type of classiﬁers in a feature selection framework because feature selection is highly susceptible to overﬁtting. [sent-377, score-0.692]
</p><p>95 We believe that the GMEB algorithm is just the ﬁrst of a series of algorithms which may globally minimize increasingly tighter bounds on the generalization error. [sent-378, score-0.176]
</p><p>96 A  Change of variables  Consider optimization problem minimize f0 (x) (8) subject to fi (x) ≤ 0, i = 1, . [sent-382, score-0.378]
</p><p>97 We deﬁne functions fi as fi (z) = fi (φ(z)), i = 0, . [sent-389, score-0.242]
</p><p>98 Now consider the problem ˜ minimize f0 (z) (9) ˜ subject to fi (z) ≤ 0, i = 1, . [sent-393, score-0.197]
</p><p>99 Problem (8) and (9) are said to be related by the change of variable x = φ(z) and are equivalent: if x solves the problem (8), then z = φ−1 (x) solves problem(9); if z solves problem (9), then x = φ(z) solves problem (8). [sent-397, score-0.37]
</p><p>100 Grafting: Fast, incremental feature selection by gradient descent in function space. [sent-434, score-0.346]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gmeb', 0.556), ('wj', 0.324), ('yf', 0.183), ('feature', 0.178), ('selection', 0.168), ('ge', 0.16), ('vj', 0.157), ('svm', 0.152), ('classi', 0.115), ('dom', 0.114), ('datasets', 0.104), ('optimization', 0.099), ('cqp', 0.098), ('technion', 0.098), ('er', 0.091), ('dual', 0.087), ('convex', 0.087), ('cv', 0.087), ('inequality', 0.083), ('np', 0.083), ('variables', 0.082), ('rd', 0.081), ('dataset', 0.077), ('primal', 0.072), ('patterns', 0.072), ('bound', 0.071), ('fi', 0.07), ('error', 0.069), ('whereby', 0.068), ('objective', 0.067), ('global', 0.066), ('solves', 0.064), ('hyperparameters', 0.064), ('features', 0.063), ('rn', 0.061), ('bounds', 0.058), ('comparatively', 0.057), ('conic', 0.057), ('haifa', 0.057), ('rqc', 0.057), ('synthetic', 0.056), ('constraint', 0.055), ('en', 0.054), ('scaling', 0.054), ('transformation', 0.053), ('meir', 0.052), ('optimum', 0.05), ('protocol', 0.048), ('equivalence', 0.048), ('partitioning', 0.048), ('subject', 0.048), ('training', 0.048), ('xj', 0.047), ('equivalent', 0.046), ('norms', 0.046), ('lowest', 0.045), ('selected', 0.044), ('duality', 0.043), ('jason', 0.043), ('generalization', 0.042), ('linear', 0.041), ('minimize', 0.041), ('test', 0.04), ('establish', 0.04), ('problem', 0.038), ('electrical', 0.037), ('constants', 0.037), ('spaced', 0.037), ('multiply', 0.036), ('regard', 0.036), ('adopted', 0.035), ('algorithms', 0.035), ('sn', 0.034), ('march', 0.034), ('sj', 0.034), ('guarantee', 0.034), ('nition', 0.033), ('israel', 0.033), ('cation', 0.033), ('solution', 0.033), ('functions', 0.032), ('nonnegative', 0.032), ('schemes', 0.032), ('theorem', 0.032), ('ers', 0.032), ('remark', 0.031), ('bounding', 0.031), ('unless', 0.031), ('programming', 0.031), ('quadratic', 0.031), ('wt', 0.031), ('freedom', 0.031), ('multiplied', 0.031), ('partitioned', 0.031), ('weston', 0.029), ('minimization', 0.029), ('explanation', 0.029), ('nonconvex', 0.028), ('andr', 0.028), ('bupa', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="3-tfidf-1" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>Author: Dori Peleg, Ron Meir</p><p>Abstract: A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. Highly competitive numerical results on both artiﬁcial and real-world data sets are reported. 1</p><p>2 0.1253994 <a title="3-tfidf-2" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>Author: Isabelle Guyon, Steve Gunn, Asa Ben-Hur, Gideon Dror</p><p>Abstract: The NIPS 2003 workshops included a feature selection competition organized by the authors. We provided participants with ﬁve datasets from diﬀerent application domains and called for classiﬁcation results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the ﬁnal competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diﬀusion trees. Other top entries used a variety of methods for feature selection, which combined ﬁlters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classiﬁcation engine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scoring software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research. 1</p><p>3 0.12061504 <a title="3-tfidf-3" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>Author: Francis R. Bach, Romain Thibaux, Michael I. Jordan</p><p>Abstract: The problem of learning a sparse conic combination of kernel functions or kernel matrices for classiﬁcation or regression can be achieved via the regularization by a block 1-norm [1]. In this paper, we present an algorithm that computes the entire regularization path for these problems. The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter. Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 1</p><p>4 0.11769518 <a title="3-tfidf-4" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>5 0.11722038 <a title="3-tfidf-5" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>6 0.11438186 <a title="3-tfidf-6" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>7 0.11224493 <a title="3-tfidf-7" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>8 0.10322272 <a title="3-tfidf-8" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>9 0.099239953 <a title="3-tfidf-9" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>10 0.098605946 <a title="3-tfidf-10" href="./nips-2004-Parallel_Support_Vector_Machines%3A_The_Cascade_SVM.html">144 nips-2004-Parallel Support Vector Machines: The Cascade SVM</a></p>
<p>11 0.09826912 <a title="3-tfidf-11" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>12 0.095986769 <a title="3-tfidf-12" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>13 0.095366813 <a title="3-tfidf-13" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>14 0.09027651 <a title="3-tfidf-14" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>15 0.087451272 <a title="3-tfidf-15" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>16 0.085737087 <a title="3-tfidf-16" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>17 0.085108057 <a title="3-tfidf-17" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>18 0.084027581 <a title="3-tfidf-18" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>19 0.082937971 <a title="3-tfidf-19" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>20 0.079683907 <a title="3-tfidf-20" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.254), (1, 0.12), (2, -0.014), (3, 0.15), (4, 0.0), (5, 0.03), (6, 0.09), (7, -0.033), (8, 0.054), (9, 0.013), (10, -0.053), (11, 0.008), (12, -0.009), (13, 0.052), (14, 0.001), (15, -0.041), (16, -0.066), (17, 0.096), (18, -0.109), (19, 0.032), (20, 0.076), (21, 0.024), (22, 0.007), (23, 0.04), (24, -0.0), (25, 0.008), (26, -0.029), (27, 0.029), (28, -0.004), (29, -0.07), (30, -0.072), (31, -0.004), (32, 0.066), (33, 0.074), (34, -0.1), (35, -0.021), (36, -0.137), (37, -0.119), (38, 0.022), (39, 0.071), (40, 0.066), (41, -0.095), (42, 0.083), (43, 0.12), (44, -0.063), (45, -0.076), (46, -0.019), (47, 0.035), (48, 0.088), (49, -0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96272796 <a title="3-lsi-1" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>Author: Dori Peleg, Ron Meir</p><p>Abstract: A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. Highly competitive numerical results on both artiﬁcial and real-world data sets are reported. 1</p><p>2 0.69756871 <a title="3-lsi-2" href="./nips-2004-Learning_Syntactic_Patterns_for_Automatic_Hypernym_Discovery.html">101 nips-2004-Learning Syntactic Patterns for Automatic Hypernym Discovery</a></p>
<p>Author: Rion Snow, Daniel Jurafsky, Andrew Y. Ng</p><p>Abstract: Semantic taxonomies such as WordNet provide a rich source of knowledge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hypernym pairs. Using “dependency path” features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (determining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet. 1</p><p>3 0.69517064 <a title="3-lsi-3" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>Author: Isabelle Guyon, Steve Gunn, Asa Ben-Hur, Gideon Dror</p><p>Abstract: The NIPS 2003 workshops included a feature selection competition organized by the authors. We provided participants with ﬁve datasets from diﬀerent application domains and called for classiﬁcation results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the ﬁnal competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diﬀusion trees. Other top entries used a variety of methods for feature selection, which combined ﬁlters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classiﬁcation engine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scoring software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research. 1</p><p>4 0.63750637 <a title="3-lsi-4" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>Author: Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse</p><p>Abstract: We study a method of optimal data-driven aggregation of classiﬁers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse. We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. 1</p><p>5 0.61011177 <a title="3-lsi-5" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<p>Author: Felix A. Wichmann, Arnulf B. Graf, Heinrich H. Bülthoff, Eero P. Simoncelli, Bernhard Schölkopf</p><p>Abstract: We study gender discrimination of human faces using a combination of psychophysical classiﬁcation and discrimination experiments together with methods from machine learning. We reduce the dimensionality of a set of face images using principal component analysis, and then train a set of linear classiﬁers on this reduced representation (linear support vector machines (SVMs), relevance vector machines (RVMs), Fisher linear discriminant (FLD), and prototype (prot) classiﬁers) using human classiﬁcation data. Because we combine a linear preprocessor with linear classiﬁers, the entire system acts as a linear classiﬁer, allowing us to visualise the decision-image corresponding to the normal vector of the separating hyperplanes (SH) of each classiﬁer. We predict that the female-tomaleness transition along the normal vector for classiﬁers closely mimicking human classiﬁcation (SVM and RVM [1]) should be faster than the transition along any other direction. A psychophysical discrimination experiment using the decision images as stimuli is consistent with this prediction. 1</p><p>6 0.5996812 <a title="3-lsi-6" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>7 0.5820964 <a title="3-lsi-7" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>8 0.53256476 <a title="3-lsi-8" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>9 0.51941454 <a title="3-lsi-9" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>10 0.51871198 <a title="3-lsi-10" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>11 0.51661569 <a title="3-lsi-11" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>12 0.51326895 <a title="3-lsi-12" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>13 0.51202035 <a title="3-lsi-13" href="./nips-2004-Triangle_Fixing_Algorithms_for_the_Metric_Nearness_Problem.html">196 nips-2004-Triangle Fixing Algorithms for the Metric Nearness Problem</a></p>
<p>14 0.51032585 <a title="3-lsi-14" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>15 0.50900865 <a title="3-lsi-15" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>16 0.50636011 <a title="3-lsi-16" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>17 0.49090266 <a title="3-lsi-17" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>18 0.48595378 <a title="3-lsi-18" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>19 0.47557741 <a title="3-lsi-19" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>20 0.47319594 <a title="3-lsi-20" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.108), (15, 0.141), (26, 0.062), (31, 0.024), (33, 0.22), (35, 0.012), (39, 0.057), (50, 0.054), (76, 0.012), (97, 0.209)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88638365 <a title="3-lda-1" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>Author: Tzu-kuo Huang, Chih-jen Lin, Ruby C. Weng</p><p>Abstract: The Bradley-Terry model for paired comparison has been popular in many areas. We propose a generalized version in which paired individual comparisons are extended to paired team comparisons. We introduce a simple algorithm with convergence proofs to solve the model and obtain individual skill. A useful application to multi-class probability estimates using error-correcting codes is demonstrated. 1</p><p>same-paper 2 0.87414896 <a title="3-lda-2" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>Author: Dori Peleg, Ron Meir</p><p>Abstract: A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. Highly competitive numerical results on both artiﬁcial and real-world data sets are reported. 1</p><p>3 0.80834359 <a title="3-lda-3" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>Author: Sajama Sajama, Alon Orlitsky</p><p>Abstract: We present a semi-parametric latent variable model based technique for density modelling, dimensionality reduction and visualization. Unlike previous methods, we estimate the latent distribution non-parametrically which enables us to model data generated by an underlying low dimensional, multimodal distribution. In addition, we allow the components of latent variable models to be drawn from the exponential family which makes the method suitable for special data types, for example binary or count data. Simulations on real valued, binary and count data show favorable comparison to other related schemes both in terms of separating different populations and generalization to unseen samples. 1</p><p>4 0.80538577 <a title="3-lda-4" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><p>5 0.80270022 <a title="3-lda-5" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>Author: Aharon Bar-hillel, Adam Spiro, Eran Stark</p><p>Abstract: Spike sorting involves clustering spike trains recorded by a microelectrode according to the source neuron. It is a complicated problem, which requires a lot of human labor, partly due to the non-stationary nature of the data. We propose an automated technique for the clustering of non-stationary Gaussian sources in a Bayesian framework. At a ﬁrst search stage, data is divided into short time frames and candidate descriptions of the data as a mixture of Gaussians are computed for each frame. At a second stage transition probabilities between candidate mixtures are computed, and a globally optimal clustering is found as the MAP solution of the resulting probabilistic model. Transition probabilities are computed using local stationarity assumptions and are based on a Gaussian version of the Jensen-Shannon divergence. The method was applied to several recordings. The performance appeared almost indistinguishable from humans in a wide range of scenarios, including movement, merges, and splits of clusters. 1</p><p>6 0.80250573 <a title="3-lda-6" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>7 0.80229485 <a title="3-lda-7" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>8 0.80200773 <a title="3-lda-8" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>9 0.80177432 <a title="3-lda-9" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>10 0.80009192 <a title="3-lda-10" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>11 0.79990089 <a title="3-lda-11" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>12 0.79966372 <a title="3-lda-12" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>13 0.79902989 <a title="3-lda-13" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>14 0.7987929 <a title="3-lda-14" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>15 0.79768163 <a title="3-lda-15" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>16 0.79766852 <a title="3-lda-16" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>17 0.79693985 <a title="3-lda-17" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>18 0.79664445 <a title="3-lda-18" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>19 0.79591686 <a title="3-lda-19" href="./nips-2004-A_Direct_Formulation_for_Sparse_PCA_Using_Semidefinite_Programming.html">2 nips-2004-A Direct Formulation for Sparse PCA Using Semidefinite Programming</a></p>
<p>20 0.79583591 <a title="3-lda-20" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
