<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-195" href="#">nips2004-195</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</h1>
<br/><p>Source: <a title="nips-2004-195-pdf" href="http://papers.nips.cc/paper/2720-trait-selection-for-assessing-beef-meat-quality-using-non-linear-svm.pdf">pdf</a></p><p>Author: Juan Coz, Gustavo F. Bayón, Jorge Díez, Oscar Luaces, Antonio Bahamonde, Carlos Sañudo</p><p>Abstract: In this paper we show that it is possible to model sensory impressions of consumers about beef meat. This is not a straightforward task; the reason is that when we are aiming to induce a function that maps object descriptions into ratings, we must consider that consumers’ ratings are just a way to express their preferences about the products presented in the same testing session. Therefore, we had to use a special purpose SVM polynomial kernel. The training data set used collects the ratings of panels of experts and consumers; the meat was provided by 103 bovines of 7 Spanish breeds with different carcass weights and aging periods. Additionally, to gain insight into consumer preferences, we used feature subset selection tools. The result is that aging is the most important trait for improving consumers’ appreciation of beef meat. 1</p><p>Reference: <a title="nips-2004-195-reference" href="../nips2004_reference/nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Trait selection for assessing beef meat quality using non-linear SVM  J. [sent-1, score-0.757]
</p><p>2 es  Abstract In this paper we show that it is possible to model sensory impressions of consumers about beef meat. [sent-12, score-0.976]
</p><p>3 This is not a straightforward task; the reason is that when we are aiming to induce a function that maps object descriptions into ratings, we must consider that consumers’ ratings are just a way to express their preferences about the products presented in the same testing session. [sent-13, score-0.671]
</p><p>4 Therefore, we had to use a special purpose SVM polynomial kernel. [sent-14, score-0.037]
</p><p>5 The training data set used collects the ratings of panels of experts and consumers; the meat was provided by 103 bovines of 7 Spanish breeds with different carcass weights and aging periods. [sent-15, score-1.005]
</p><p>6 Additionally, to gain insight into consumer preferences, we used feature subset selection tools. [sent-16, score-0.321]
</p><p>7 The result is that aging is the most important trait for improving consumers’ appreciation of beef meat. [sent-17, score-0.477]
</p><p>8 1  Introduction  The quality of beef meat is appreciated through sensory impressions, and therefore its assessment is very subjective. [sent-18, score-0.947]
</p><p>9 However, it is known that there are objective traits very important for the ﬁnal properties of beef meat; this includes the breed and feeding of animals, weight of carcasses, and aging of meat after slaughter. [sent-19, score-1.032]
</p><p>10 To discover the inﬂuence of these and other attributes, we have applied Machine Learning tools to the results of an experience reported in [8]. [sent-20, score-0.08]
</p><p>11 In the experience, 103 bovines of 7 Spanish breeds were slaughtered to obtain two kinds of carcasses, light and standard [5]; the meat was prepared with 3 aging periods, 1, 7, and 21 days. [sent-21, score-0.775]
</p><p>12 Finally, the meat was consumed by a group, called panel, of 11 experts, and assessed by a panel of untrained consumers. [sent-22, score-0.646]
</p><p>13 The conceptual framework used for the study reported in this paper was the analysis of sensory data. [sent-23, score-0.203]
</p><p>14 In general, this kind of analysis is used for food industries in order to adapt their productive processes to improve the acceptability of their specialties. [sent-24, score-0.142]
</p><p>15 They need to discover the relationship between descriptions of their products and consumers’ sensory degree of satisfaction. [sent-25, score-0.406]
</p><p>16 An excellent survey of the use of sensory data analysis in the food industry can be found in [15, 2]; for a Machine Learning perspective, see [3, 9, 6]. [sent-26, score-0.313]
</p><p>17 The role played by each panel, experts and consumers, is very clear. [sent-27, score-0.092]
</p><p>18 So, the experts’ panel is made up of a usually small group of trained people who rate several traits of products such  as ﬁbrosis, ﬂavor, odor, etc. [sent-28, score-0.374]
</p><p>19 The most essential property of expert panelists, in addition to their discriminatory capacity, is their own coherence, but not necessarily the uniformity of the group. [sent-31, score-0.031]
</p><p>20 Experts’ panel can be viewed as a bundle of sophisticated sensors whose ratings are used to describe each product, in addition to other objective traits. [sent-32, score-0.264]
</p><p>21 On the other hand, the group of untrained consumers (C) are asked to rate their degree of acceptance or satisfaction about the tested products on a given scale. [sent-33, score-0.77]
</p><p>22 Usually, this panel is organized in a set of testing sessions, where a group of potential consumers assess some instances from a sample E of the tested product. [sent-34, score-0.685]
</p><p>23 Frequently, each consumer only participates in a small number (sometimes only one) of testing sessions, usually in the same day. [sent-35, score-0.263]
</p><p>24 In general, the success of sensory analysis relies on the capability to identify, with a precise description, a kind of product that should be reproducible as many times as we need to be tested for as many consumers as possible. [sent-36, score-0.783]
</p><p>25 Therefore, the study of beef meat sensory quality is very difﬁcult. [sent-37, score-0.916]
</p><p>26 The main reason is that there are important individual differences in each piece of meat, and the repeatability of tests can be only partially ensured. [sent-38, score-0.065]
</p><p>27 Notice that from each animal there are only a limited amount of similar pieces of meat, and thus we can only provide pieces of a given breed, weight, and aging period. [sent-39, score-0.258]
</p><p>28 Additionally, it is worthy noting that the cost of acquisition of this kind of sensory data is very high. [sent-40, score-0.334]
</p><p>29 The paper is organized as follows: in the next section we present an approach to deal with testing sessions explicitly. [sent-41, score-0.154]
</p><p>30 The overall idea is to look for a preference or ranking function able to reproduce the implicit ordering of products given by consumers instead of trying to predict the exact value of consumer ratings; such function must return higher values to those products with higher ratings. [sent-42, score-1.342]
</p><p>31 In Section 3 we show how some state of the art FSS methods designed for SVM (Support Vector Machines) with non-linear kernels can be adapted to preference learning. [sent-43, score-0.198]
</p><p>32 Finally, at the end of the paper, we return to the data set of beef meat to show how it is possible to explain consumer behavior, and to interpret the relevance of meat traits in this context. [sent-44, score-1.427]
</p><p>33 2  Learning from sensory data  A straightforward approach to handle sensory data can be based on regression, where sensory descriptions of each object x ∈ E are endowed with the degree of satisfaction r(x) for each consumer (or the average of a group of consumers). [sent-45, score-1.026]
</p><p>34 However, this approach does not faithfully captures people’s preferences [7, 6]: consumers’ ratings actually express a relative ordering, so there is a kind of batch effect that often biases their ratings. [sent-46, score-0.436]
</p><p>35 Thus, a product could obtain a higher (lower) rating depending on if it is assessed together with worse (better) products. [sent-47, score-0.178]
</p><p>36 Therefore, information about batches tested by consumers in each rating session is a very important issue. [sent-48, score-0.623]
</p><p>37 On the other hand, more traditional approaches, such as testing some statistical hypotheses [16, 15, 2] require all available food products in sample E to be assessed by the set of consumers C, a requisite very difﬁcult to fulﬁll. [sent-49, score-0.725]
</p><p>38 In this paper we use an approach to sensory data analysis based on learning consumers’ preferences, see [11, 14, 1], where training examples are represented by preference judgments, i. [sent-50, score-0.401]
</p><p>39 pairs of vectors (v, u) indicating that, for someone, object v is preferable to object u. [sent-52, score-0.159]
</p><p>40 We will show that this approach can induce more useful knowledge than other approaches, like regression based methods. [sent-53, score-0.037]
</p><p>41 The main reason is due to the fact that preference judgments sets can represent more relevant information to discover consumers’ preferences. [sent-54, score-0.362]
</p><p>42 1  A formal framework to learn consumer preferences  In order to learn our preference problems, we will try to ﬁnd a real ranking function f that maximizes the probability of having f (v) > f (u) whenever v is preferable to u [11, 14, 1]. [sent-56, score-0.696]
</p><p>43 Our input data is made up of a set of ratings (ri (x) : x ∈ Ei ) for i ∈ C. [sent-57, score-0.166]
</p><p>44 To avoid the batch effect, we will create a preference judgment set P J = {v j > uj : j = 1, . [sent-58, score-0.341]
</p><p>45 , n} suitable for our needs just considering all pairs (v, u) such that objects v and u were presented in the same session to a given consumer i, and ri (v) > ri (u). [sent-61, score-0.346]
</p><p>46 (1) Then, the ranking function f : Rd → R can be simply deﬁned by f (x) = F (x, 0). [sent-63, score-0.168]
</p><p>47 As we have already constructed a set of preference judgments P J, we can specify F by means of the restrictions F (v j , uj ) > 0 and F (uj , v j ) < 0, ∀j = 1, . [sent-64, score-0.358]
</p><p>48 in [11], and deﬁne a kernel K as follows K(x1 , x2 , x3 , x4 ) = k(x1 , x3 ) − k(x1 , x4 ) − k(x2 , x3 ) + k(x2 , x4 ) (3) where k(x, y) = φ(x), φ(y) is a kernel function deﬁned as the inner product of two objects represented in the feature space by their φ images. [sent-70, score-0.186]
</p><p>49 In the experiments reported in Section 4, we will employ a polynomial kernel, deﬁning k(x, y) = ( x, y + c)g , with c = 1 and g = 2. [sent-71, score-0.037]
</p><p>50 Producers can focus on these features to improve the quality of the ﬁnal product. [sent-73, score-0.071]
</p><p>51 Additionaly, reductions on the number of features often lead to a cheaper data acquisition labour, making these systems suitable for industrial operation [9]. [sent-74, score-0.102]
</p><p>52 There are many feature subset selection methods applied to SVM classiﬁcation. [sent-75, score-0.136]
</p><p>53 It is a ranking method that returns an ordering of the features. [sent-77, score-0.266]
</p><p>54 Following the main idea of RFE, we have used two methods capable of ordering features in non-linear scenarios. [sent-81, score-0.132]
</p><p>55 We must also point that, in this case, preference learning data sets are formed by pairs of objects (v, u), and each object in the pair has the same set of features. [sent-82, score-0.282]
</p><p>56 Thus, we must modify the ranking methods so they can deal with the duplicated features. [sent-83, score-0.199]
</p><p>57 1  Ranking features for non-linear preference learning  Method 1. [sent-85, score-0.232]
</p><p>58 - This method orders the list of features according to their inﬂuence in the variations of the weights. [sent-86, score-0.034]
</p><p>59 It removes in each iteration the feature that minimizes the ranking value R1 (i) = |  i  w 2| =  αk αj zk zj k,j  ∂K(s · xk , s · xj ) , ∂si  i = 1, . [sent-88, score-0.417]
</p><p>60 Due to the fact that we are working on a preference learning problem, we need 4 copies of the scaling factor. [sent-92, score-0.198]
</p><p>61 In this formula, for a polynomial kernel k(x, y) = ( x, y + c)g and a vector s such that ∀i, si = 1 we have that ∂k(s · x, s · y) = 2g(xi yi )(c + x, y )g−1 . [sent-93, score-0.104]
</p><p>62 - This method, introduced in [4], works in an iterative way; removing each time the feature which minimizes the loss of predictive performance. [sent-95, score-0.063]
</p><p>63 Notice that a higher value of R2 (i), that is, a higher accuracy on the training set when replacing feature i-th, means a lower relevance of that feature. [sent-97, score-0.161]
</p><p>64 Therefore, we will remove the feature yielding the highest ranking value, as opposite to the ranking method described previously. [sent-98, score-0.399]
</p><p>65 2  Model selection on an ordered sequence of feature subsets  Once we have an ordering of the features, we must select the subset Fi which maximizes the generalization performance of the system. [sent-100, score-0.264]
</p><p>66 The most common choice for a model selection method is cross-validation (CV), but its efﬁciency and high variance [1] lead us to try another kind of methods. [sent-101, score-0.107]
</p><p>67 This is a metric-based method that selects one from a nested sequence of complexity-increasing models. [sent-103, score-0.08]
</p><p>68 ⊂ Fd , where Fi represents the subset containing only the i most relevant features. [sent-107, score-0.029]
</p><p>69 Then we can create a nested sequence of models fi , each one of these induced by SVM from the corresponding Fi . [sent-108, score-0.141]
</p><p>70 Thus, given two different hypothesis f and g, their distance is calculated as the expected disagreement in their predictions. [sent-110, score-0.081]
</p><p>71 Given that these distances can only be approximated, ADJ establish a ˆ method to compute d(g, t), an adjusted distance estimate between any hypothesis f and the true target classiﬁcation function t. [sent-111, score-0.068]
</p><p>72 Therefore, the selected hypothesis is ˆ fk = arg min d(fl , t). [sent-112, score-0.081]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('consumers', 0.453), ('meat', 0.427), ('beef', 0.249), ('sensory', 0.203), ('preference', 0.198), ('consumer', 0.185), ('aging', 0.178), ('ranking', 0.168), ('ratings', 0.166), ('adj', 0.142), ('fl', 0.124), ('rfe', 0.124), ('sessions', 0.107), ('traits', 0.107), ('ordering', 0.098), ('preferences', 0.098), ('panel', 0.098), ('experts', 0.092), ('products', 0.087), ('judgments', 0.085), ('food', 0.079), ('uj', 0.075), ('bovines', 0.071), ('breed', 0.071), ('breeds', 0.071), ('carcasses', 0.071), ('impressions', 0.071), ('descriptions', 0.071), ('kind', 0.063), ('feature', 0.063), ('spanish', 0.062), ('untrained', 0.062), ('fi', 0.061), ('assessed', 0.059), ('rating', 0.057), ('object', 0.056), ('satisfaction', 0.053), ('zk', 0.053), ('group', 0.052), ('trait', 0.05), ('nested', 0.05), ('svm', 0.049), ('session', 0.047), ('preferable', 0.047), ('testing', 0.047), ('fk', 0.045), ('disagreement', 0.045), ('removes', 0.045), ('discover', 0.045), ('xk', 0.045), ('selection', 0.044), ('express', 0.044), ('rd', 0.044), ('zj', 0.043), ('ri', 0.043), ('pieces', 0.04), ('quality', 0.037), ('batch', 0.037), ('induce', 0.037), ('acquisition', 0.037), ('polynomial', 0.037), ('hypothesis', 0.036), ('experience', 0.035), ('tested', 0.035), ('features', 0.034), ('reason', 0.034), ('notice', 0.034), ('si', 0.034), ('kernel', 0.033), ('higher', 0.033), ('relevance', 0.032), ('adjusted', 0.032), ('zaragoza', 0.031), ('worthy', 0.031), ('duplicated', 0.031), ('batches', 0.031), ('carlos', 0.031), ('repeatability', 0.031), ('appreciated', 0.031), ('bay', 0.031), ('reductions', 0.031), ('fd', 0.031), ('del', 0.031), ('someone', 0.031), ('uniformity', 0.031), ('judgment', 0.031), ('aiming', 0.031), ('participates', 0.031), ('industry', 0.031), ('people', 0.03), ('sequence', 0.03), ('subset', 0.029), ('product', 0.029), ('prepared', 0.028), ('odor', 0.028), ('acceptance', 0.028), ('faithfully', 0.028), ('coherence', 0.028), ('avor', 0.028), ('objects', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="195-tfidf-1" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<p>Author: Juan Coz, Gustavo F. Bayón, Jorge Díez, Oscar Luaces, Antonio Bahamonde, Carlos Sañudo</p><p>Abstract: In this paper we show that it is possible to model sensory impressions of consumers about beef meat. This is not a straightforward task; the reason is that when we are aiming to induce a function that maps object descriptions into ratings, we must consider that consumers’ ratings are just a way to express their preferences about the products presented in the same testing session. Therefore, we had to use a special purpose SVM polynomial kernel. The training data set used collects the ratings of panels of experts and consumers; the meat was provided by 103 bovines of 7 Spanish breeds with different carcass weights and aging periods. Additionally, to gain insight into consumer preferences, we used feature subset selection tools. The result is that aging is the most important trait for improving consumers’ appreciation of beef meat. 1</p><p>2 0.15096551 <a title="195-tfidf-2" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Many interesting multiclass problems can be cast in the general framework of label ranking deﬁned on a given set of classes. The evaluation for such a ranking is generally given in terms of the number of violated order constraints between classes. In this paper, we propose the Preference Learning Model as a unifying framework to model and solve a large class of multiclass problems in a large margin perspective. In addition, an original kernel-based method is proposed and evaluated on a ranking dataset with state-of-the-art results. 1</p><p>3 0.13506615 <a title="195-tfidf-3" href="./nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">8 nips-2004-A Machine Learning Approach to Conjoint Analysis</a></p>
<p>Author: Olivier Chapelle, Za\</p><p>Abstract: Choice-based conjoint analysis builds models of consumer preferences over products with answers gathered in questionnaires. Our main goal is to bring tools from the machine learning community to solve this problem more efﬁciently. Thus, we propose two algorithms to quickly and accurately estimate consumer preferences. 1</p><p>4 0.10919054 <a title="195-tfidf-4" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>Author: Shivani Agarwal, Thore Graepel, Ralf Herbrich, Dan Roth</p><p>Abstract: The area under the ROC curve (AUC) has been advocated as an evaluation criterion for the bipartite ranking problem. We study large deviation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an independent test sequence. A comparison of our result with a corresponding large deviation result for the classiﬁcation error rate suggests that the test sample size required to obtain an -accurate estimate of the expected accuracy of a ranking function with δ-conﬁdence is larger than that required to obtain an -accurate estimate of the expected error rate of a classiﬁcation function with the same conﬁdence. A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from ﬁnite function classes. 1</p><p>5 0.063194647 <a title="195-tfidf-5" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Kai Yu</p><p>Abstract: We present a novel method for learning with Gaussian process regression in a hierarchical Bayesian framework. In a ﬁrst step, kernel matrices on a ﬁxed set of input points are learned from data using a simple and efﬁcient EM algorithm. This step is nonparametric, in that it does not require a parametric form of covariance function. In a second step, kernel functions are ﬁtted to approximate the learned covariance matrix using a generalized Nystr¨ m method, which results in a complex, data o driven kernel. We evaluate our approach as a recommendation engine for art images, where the proposed hierarchical Bayesian method leads to excellent prediction performance. 1</p><p>6 0.062933944 <a title="195-tfidf-6" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>7 0.0587602 <a title="195-tfidf-7" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>8 0.058469813 <a title="195-tfidf-8" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>9 0.057869084 <a title="195-tfidf-9" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>10 0.054245576 <a title="195-tfidf-10" href="./nips-2004-Economic_Properties_of_Social_Networks.html">57 nips-2004-Economic Properties of Social Networks</a></p>
<p>11 0.050223213 <a title="195-tfidf-11" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>12 0.049463905 <a title="195-tfidf-12" href="./nips-2004-Optimal_Information_Decoding_from_Neuronal_Populations_with_Specific_Stimulus_Selectivity.html">140 nips-2004-Optimal Information Decoding from Neuronal Populations with Specific Stimulus Selectivity</a></p>
<p>13 0.049053464 <a title="195-tfidf-13" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>14 0.047316067 <a title="195-tfidf-14" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>15 0.046789207 <a title="195-tfidf-15" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>16 0.044677891 <a title="195-tfidf-16" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>17 0.044169333 <a title="195-tfidf-17" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>18 0.044124562 <a title="195-tfidf-18" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>19 0.043017603 <a title="195-tfidf-19" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>20 0.040968169 <a title="195-tfidf-20" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.145), (1, 0.028), (2, 0.0), (3, 0.029), (4, 0.004), (5, 0.086), (6, 0.034), (7, 0.018), (8, 0.078), (9, -0.057), (10, -0.093), (11, 0.102), (12, 0.076), (13, 0.045), (14, -0.003), (15, 0.132), (16, -0.075), (17, -0.008), (18, 0.037), (19, -0.035), (20, 0.02), (21, -0.076), (22, -0.002), (23, -0.081), (24, 0.045), (25, -0.098), (26, 0.162), (27, 0.149), (28, 0.136), (29, -0.053), (30, -0.02), (31, 0.043), (32, 0.122), (33, -0.017), (34, -0.206), (35, 0.136), (36, -0.005), (37, -0.089), (38, -0.157), (39, 0.081), (40, -0.039), (41, -0.005), (42, 0.058), (43, -0.16), (44, 0.128), (45, -0.108), (46, 0.116), (47, 0.139), (48, 0.143), (49, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94396728 <a title="195-lsi-1" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<p>Author: Juan Coz, Gustavo F. Bayón, Jorge Díez, Oscar Luaces, Antonio Bahamonde, Carlos Sañudo</p><p>Abstract: In this paper we show that it is possible to model sensory impressions of consumers about beef meat. This is not a straightforward task; the reason is that when we are aiming to induce a function that maps object descriptions into ratings, we must consider that consumers’ ratings are just a way to express their preferences about the products presented in the same testing session. Therefore, we had to use a special purpose SVM polynomial kernel. The training data set used collects the ratings of panels of experts and consumers; the meat was provided by 103 bovines of 7 Spanish breeds with different carcass weights and aging periods. Additionally, to gain insight into consumer preferences, we used feature subset selection tools. The result is that aging is the most important trait for improving consumers’ appreciation of beef meat. 1</p><p>2 0.6900242 <a title="195-lsi-2" href="./nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">8 nips-2004-A Machine Learning Approach to Conjoint Analysis</a></p>
<p>Author: Olivier Chapelle, Za\</p><p>Abstract: Choice-based conjoint analysis builds models of consumer preferences over products with answers gathered in questionnaires. Our main goal is to bring tools from the machine learning community to solve this problem more efﬁciently. Thus, we propose two algorithms to quickly and accurately estimate consumer preferences. 1</p><p>3 0.60371882 <a title="195-lsi-3" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>Author: Fabio Aiolli, Alessandro Sperduti</p><p>Abstract: Many interesting multiclass problems can be cast in the general framework of label ranking deﬁned on a given set of classes. The evaluation for such a ranking is generally given in terms of the number of violated order constraints between classes. In this paper, we propose the Preference Learning Model as a unifying framework to model and solve a large class of multiclass problems in a large margin perspective. In addition, an original kernel-based method is proposed and evaluated on a ranking dataset with state-of-the-art results. 1</p><p>4 0.36654684 <a title="195-lsi-4" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>Author: Shivani Agarwal, Thore Graepel, Ralf Herbrich, Dan Roth</p><p>Abstract: The area under the ROC curve (AUC) has been advocated as an evaluation criterion for the bipartite ranking problem. We study large deviation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an independent test sequence. A comparison of our result with a corresponding large deviation result for the classiﬁcation error rate suggests that the test sample size required to obtain an -accurate estimate of the expected accuracy of a ranking function with δ-conﬁdence is larger than that required to obtain an -accurate estimate of the expected error rate of a classiﬁcation function with the same conﬁdence. A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from ﬁnite function classes. 1</p><p>5 0.34756303 <a title="195-lsi-5" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Kai Yu</p><p>Abstract: We present a novel method for learning with Gaussian process regression in a hierarchical Bayesian framework. In a ﬁrst step, kernel matrices on a ﬁxed set of input points are learned from data using a simple and efﬁcient EM algorithm. This step is nonparametric, in that it does not require a parametric form of covariance function. In a second step, kernel functions are ﬁtted to approximate the learned covariance matrix using a generalized Nystr¨ m method, which results in a complex, data o driven kernel. We evaluate our approach as a recommendation engine for art images, where the proposed hierarchical Bayesian method leads to excellent prediction performance. 1</p><p>6 0.34226179 <a title="195-lsi-6" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>7 0.33232293 <a title="195-lsi-7" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>8 0.28923503 <a title="195-lsi-8" href="./nips-2004-Common-Frame_Model_for_Object_Recognition.html">40 nips-2004-Common-Frame Model for Object Recognition</a></p>
<p>9 0.27475175 <a title="195-lsi-9" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>10 0.26771638 <a title="195-lsi-10" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>11 0.26519063 <a title="195-lsi-11" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>12 0.26335934 <a title="195-lsi-12" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>13 0.26334435 <a title="195-lsi-13" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>14 0.24745657 <a title="195-lsi-14" href="./nips-2004-Economic_Properties_of_Social_Networks.html">57 nips-2004-Economic Properties of Social Networks</a></p>
<p>15 0.24551927 <a title="195-lsi-15" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>16 0.24415706 <a title="195-lsi-16" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>17 0.24202791 <a title="195-lsi-17" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>18 0.24109235 <a title="195-lsi-18" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>19 0.23905505 <a title="195-lsi-19" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>20 0.23337947 <a title="195-lsi-20" href="./nips-2004-The_Rescorla-Wagner_Algorithm_and_Maximum_Likelihood_Estimation_of_Causal_Parameters.html">190 nips-2004-The Rescorla-Wagner Algorithm and Maximum Likelihood Estimation of Causal Parameters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.397), (13, 0.066), (15, 0.124), (26, 0.074), (31, 0.025), (33, 0.133), (35, 0.025), (50, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70840311 <a title="195-lda-1" href="./nips-2004-Trait_Selection_for_Assessing_Beef_Meat_Quality_Using_Non-linear_SVM.html">195 nips-2004-Trait Selection for Assessing Beef Meat Quality Using Non-linear SVM</a></p>
<p>Author: Juan Coz, Gustavo F. Bayón, Jorge Díez, Oscar Luaces, Antonio Bahamonde, Carlos Sañudo</p><p>Abstract: In this paper we show that it is possible to model sensory impressions of consumers about beef meat. This is not a straightforward task; the reason is that when we are aiming to induce a function that maps object descriptions into ratings, we must consider that consumers’ ratings are just a way to express their preferences about the products presented in the same testing session. Therefore, we had to use a special purpose SVM polynomial kernel. The training data set used collects the ratings of panels of experts and consumers; the meat was provided by 103 bovines of 7 Spanish breeds with different carcass weights and aging periods. Additionally, to gain insight into consumer preferences, we used feature subset selection tools. The result is that aging is the most important trait for improving consumers’ appreciation of beef meat. 1</p><p>2 0.56217581 <a title="195-lda-2" href="./nips-2004-A_Direct_Formulation_for_Sparse_PCA_Using_Semidefinite_Programming.html">2 nips-2004-A Direct Formulation for Sparse PCA Using Semidefinite Programming</a></p>
<p>Author: Alexandre D'aspremont, Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: We examine the problem of approximating, in the Frobenius-norm sense, a positive, semideﬁnite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to ﬁnance. We use a modiﬁcation of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semideﬁnite programming based relaxation for our problem. 1</p><p>3 0.45937431 <a title="195-lda-3" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><p>4 0.45752084 <a title="195-lda-4" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>Author: Tzu-kuo Huang, Chih-jen Lin, Ruby C. Weng</p><p>Abstract: The Bradley-Terry model for paired comparison has been popular in many areas. We propose a generalized version in which paired individual comparisons are extended to paired team comparisons. We introduce a simple algorithm with convergence proofs to solve the model and obtain individual skill. A useful application to multi-class probability estimates using error-correcting codes is demonstrated. 1</p><p>5 0.45743284 <a title="195-lda-5" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>Author: Ingo Steinwart, Clint Scovel</p><p>Abstract: We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels. Finally, we compare our methods with a recent paper of G. Blanchard et al.. 1</p><p>6 0.45413479 <a title="195-lda-6" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>7 0.45380861 <a title="195-lda-7" href="./nips-2004-Hierarchical_Eigensolver_for_Transition_Matrices_in_Spectral_Methods.html">79 nips-2004-Hierarchical Eigensolver for Transition Matrices in Spectral Methods</a></p>
<p>8 0.45377374 <a title="195-lda-8" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>9 0.45344174 <a title="195-lda-9" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>10 0.45286712 <a title="195-lda-10" href="./nips-2004-Limits_of_Spectral_Clustering.html">103 nips-2004-Limits of Spectral Clustering</a></p>
<p>11 0.45285797 <a title="195-lda-11" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>12 0.45283449 <a title="195-lda-12" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>13 0.45262378 <a title="195-lda-13" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>14 0.45234782 <a title="195-lda-14" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<p>15 0.45207611 <a title="195-lda-15" href="./nips-2004-Sparse_Coding_of_Natural_Images_Using_an_Overcomplete_Set_of_Limited_Capacity_Units.html">172 nips-2004-Sparse Coding of Natural Images Using an Overcomplete Set of Limited Capacity Units</a></p>
<p>16 0.45078 <a title="195-lda-16" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>17 0.45039809 <a title="195-lda-17" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>18 0.44969785 <a title="195-lda-18" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>19 0.44945756 <a title="195-lda-19" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>20 0.44942662 <a title="195-lda-20" href="./nips-2004-Bayesian_inference_in_spiking_neurons.html">28 nips-2004-Bayesian inference in spiking neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
