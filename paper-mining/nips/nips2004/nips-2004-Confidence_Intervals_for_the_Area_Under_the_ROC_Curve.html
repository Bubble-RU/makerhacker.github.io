<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-45" href="#">nips2004-45</a> knowledge-graph by maker-knowledge-mining</p><h1>45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</h1>
<br/><p>Source: <a title="nips-2004-45-pdf" href="http://papers.nips.cc/paper/2645-confidence-intervals-for-the-area-under-the-roc-curve.pdf">pdf</a></p><p>Author: Corinna Cortes, Mehryar Mohri</p><p>Abstract: In many applications, good ranking is a highly desirable performance for a classiﬁer. The criterion commonly used to measure the ranking quality of a classiﬁcation algorithm is the area under the ROC curve (AUC). To report it properly, it is crucial to determine an interval of conﬁdence for its value. This paper provides conﬁdence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples. The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples. The results are of practical use and can be viewed as the equivalent for AUC of the standard conﬁdence intervals given in the case of the error rate. They are compared with previous approaches in several standard classiﬁcation tasks demonstrating the beneﬁts of our analysis. 1 Motivation In many machine learning applications, the ranking quality of a classiﬁer is critical. For example, the ordering of the list of relevant documents returned by a search engine or a document classiﬁcation system is essential. The criterion widely used to measure the ranking quality of a classiﬁcation algorithm is the area under an ROC curve (AUC). But, to measure and report the AUC properly, it is crucial to determine an interval of conﬁdence for its value as it is customary for the error rate and other measures. It is also important to make the computation of the conﬁdence interval practical by relying only on a small and simple number of parameters. In the case of the error rate, such intervals are often derived from just the sample size N . We present an extensive theoretical analysis of the AUC and show that a similar conﬁdence interval can be derived for its value using only simple parameters such as the error rate k/N , the number of positive examples m, and the number of negative examples n = N − m. Thus, our results extend to AUC the computation of conﬁdence intervals from a small number of readily available parameters. Our analysis is distribution-independent in the sense that it makes no assumption about the distribution of the scores of negative or positive examples. The use of the error rate helps determine tight conﬁdence intervals. This contrasts with existing approaches presented in the statistical literature [11, 5, 2] which are based either on weak distribution-independent assumptions resulting in too loose conﬁdence intervals, or strong distribution-dependent assumptions leading to tight but unsafe conﬁdence intervals. We show that our results are of practical use. We also compare them with previous approaches in several standard classiﬁcation tasks demonstrating the beneﬁts of our analysis. Our results are also useful for testing the statistical signiﬁcance of the difference of the AUC values of two classiﬁers. The paper is organized as follows. We ﬁrst introduce the deﬁnition of the AUC, its connection with the Wilcoxon-Mann-Whitney statistic (Section 2), and brieﬂy review some essential aspects of the existing literature related to the computation of conﬁdence intervals for the AUC. Our computation of the expected value and variance of the AUC for a ﬁxed error rate requires establishing several combinatorial identities. Section 4 presents some existing identities and gives the proof of novel ones useful for the computation of the variance. Section 5 gives the reduced expressions for the expected value and variance of the AUC for a ﬁxed error rate. These can be efﬁciently computed and used to determine our conﬁdence intervals for the AUC (Section 6). Section 7 reports the result of the comparison of our method with previous approaches, including empirical results for several standard tasks. 2 Deﬁnition and Properties of the AUC The Receiver Operating Characteristics (ROC) curves were originally introduced in signal detection theory [6] in connection with the study of radio signals, and have been used since then in many other applications, in particular for medical decision-making. Over the last few years, they have found increased interest in the machine learning and data mining communities for model evaluation and selection [14, 13, 7, 12, 16, 3]. The ROC curve for a binary classiﬁcation problem plots the true positive rate as a function of the false positive rate. The points of the curve are obtained by sweeping the classiﬁcation threshold from the most positive classiﬁcation value to the most negative. For a fully random classiﬁcation, the ROC curve is a straight line connecting the origin to (1, 1). Any improvement over random classiﬁcation results in an ROC curve at least partially above this straight line. The AUC is deﬁned as the area under the ROC curve. Consider a binary classiﬁcation task with m positive examples and n negative examples. Let C be a ﬁxed classiﬁer that outputs a strictly ordered list for these examples. Let x1 , . . . , xm be the output of C on the positive examples and y1 , . . . , yn its output on the negative examples and denote by 1X the indicator function of a set X. Then, the AUC, A, associated to C is given by: A= m i=1 n j=1 1xi >yj (1) mn which is the value of the Wilcoxon-Mann-Whitney statistic [10]. Thus, the AUC is closely related to the ranking quality of the classiﬁcation. It can be viewed as a measure based on pairwise comparisons between classiﬁcations of the two classes. It is an estimate of the probability Pxy that the classiﬁer ranks a randomly chosen positive example higher than a negative example. With a perfect ranking, all positive examples are ranked higher than the negative ones and A = 1. Any deviation from this ranking decreases the AUC, and the expected AUC value for a random ranking is 0.5. 3 Overview of Related Work This section brieﬂy describes some previous distribution-dependent approaches presented in the statistical literature to derive conﬁdence intervals for the AUC and compares them to our method. The starting point for these analyses is a formula giving the variance of the AUC, A, for a ﬁxed distribution of the scores Px of the positive examples and Py of the negative examples [10, 1]: 2 σA = A(1 − A) + (m − 1)(Pxxy − A2 ) + (n − 1)(Pxyy − A2 ) mn (2) where Pxxy is the probability that the classiﬁer ranks two randomly chosen positive examples higher than a negative one, and Pxyy the probability that it ranks two randomly chosen negative examples lower than a positive one. To compute the variance exactly using Equation 2, the distributions Px and Py must be known. Hanley and McNeil [10] argue in favor of exponential distributions, loosely claiming that this upper-bounds the variance of normal distributions with various means and ratios of A 2A2 variances. They show that for exponential distributions Pxxy = 2−A and Pxyy = 1+A . The resulting conﬁdence intervals are of course relatively tight, but their validity is questionable since they are based on a strong assumption about the distributions of the positive and negative scores that may not hold in many cases. An alternative considered by several authors to the exact computation of the variance is to determine instead the maximum of the variance over all possible continuous distributions with the same expected value of the AUC. For all such distributions, one can ﬁx m and n and compute the expected AUC and its variance. The maximum variance is denoted by 2 σmax and is given by [5, 2]: 2 σmax = A(1 − A) 1 ≤ min {m, n} 4 min {m, n} (3) Unfortunately, this often yields loose conﬁdence intervals of limited practical use. Our approach for computing the mean and variance of the AUC is distribution-independent and inspired by the machine learning literature where analyses typically center on the error rate. We require only that the error rate be measured and compute the mean and variance of the AUC over all distributions Px and Py that maintain the same error rate. Our approach is in line with that of [5, 2] but it crucially avoids considering the maximum of the variance. We show that it is possible to compute directly the mean and variance of the AUC assigning equal weight to all the possible distributions. Of course, one could argue that not all distributions Px and Py are equally probable, but since these distributions are highly problem-dependent, we ﬁnd it risky to make any general assumption on the distributions and thereby limit the validity of our results. Our approach is further justiﬁed empirically by the experiments reported in the last section. 4 Combinatorial Analysis The analysis of the statistical properties of the AUC given a ﬁxed error rate requires various combinatorial calculations. This section describes several of the combinatorial identities that are used in our computation of the conﬁdence intervals. For all q ≥ 0, let Xq (k, m, n) be deﬁned by: k M M Xq (k, m, n) = xq (4) x x x=0 where M = m − (k − x) + x, M = n + (k − x) − x, and x = k − x. In previous work, we derived the following two identities which we used to compute the expected value of the AUC [4]: k X0 (k, m, n) = x=0 n+m+1 x k X1 (k, m, n) = (k − x)(m − n) + k n + m + 1 2 x x=0 To simplify the expression of the variance of the AUC, we need to compute X2 (k, m, n). Proposition 1 Let k, m, n be non-negative integers such that k ≤ min{m, n}, then: k X2 (k, m, n) = P2 (k, m, n, x) x=0 m+n+1 x (5) where P2 is the following 4th-degree polynomial: P2 (k, m, n, x) = (k − x)/12(−2x3 + 2x2 (2m − n + 2k − 4) + x(−3m2 + 3nm + 3m − 5km − 2k 2 + 2 + k + nk + 6n) + (3(k − 1)m2 − 3nm(k − 1) + 6km + 5m + k 2 m + 8n + 8 − 9nk + 3k + k 2 + k 2 n)). Proof. 5 The proof of the proposition is left to a longer version of this paper. Expectation and Variance of the AUC This section presents the expression of the expectation and variance of the AUC for a ﬁxed error rate k/N assuming that all classiﬁcations or rankings with k errors are equiprobable. For a given classiﬁcation, there may be x, 0 ≤ x ≤ k, false positive examples. Since the number of errors is ﬁxed, there are x = k − x false negative examples. The expression Xq discussed in the previous section represents the q-th moment of x over all classiﬁcations with exactly k errors. In previous work, we gave the exact expression of the expectation of the AUC for a ﬁxed number of errors k: Proposition 2 ([4]) Assume that a binary classiﬁcation task with m positive examples and n negative examples is given. Then, the expected value of the AUC, A, over all classiﬁcations with k errors is given by: E[A] = 1 − k (n − m)2 (m + n + 1) − m+n 4mn k − m+n k−1 m+n x=0 x k m+n+1 x=0 x . Note that the two sums in this expression cannot be further simpliﬁed since they are known not to admit a closed form [9]. We also gave the expression of the variance of the AUC in terms of the function F deﬁned for all Y by: F (Y ) = k M M x=0 x x k M M x=0 x x Y . (6) The following proposition reproduces that result: Proposition 3 ([4]) Assume that a binary classiﬁcation task with m positive examples and n negative examples is given. Then, the variance of the AUC A over all classiﬁcations with k errors is given by: σ 2 (A) = F ((1 − k−x x n+ m )2 ) 2 2 2 F ( mx +n(k−x) +(m(m+1)x+n(n+1)(k−x))−2x(k−x)(m+n+1) ). 12m2 n2 − F ((1 − k−x x n+ m 2 ))2 + Because of the products of binomial terms, the computation of the variance using this expression is inefﬁcient even for relatively small values of m and n. This expression can however be reduced using the identities presented in the previous section which leads to signiﬁcantly more efﬁcient computations that we have been using in all our experiments. Corollary 1 ([4]) Assume that a binary classiﬁcation task with m positive examples and n negative examples is given. Then, the variance of the AUC A over all classiﬁcations with ((m+n−2)Z4 −(2m−n+3k−10)Z3 ) k errors is given by: σ 2 (A) = (m+n+1)(m+n)(m+n−1)T72m2 n2 + (m+n+1)(m+n)T (m2 −nm+3km−5m+2k2 −nk+12−9k)Z2 48m2 n2 (m+n+1)Q1 Z1 kQ0 + 144m2 n2 with: 2 n2 72m Pk−i m+n+1−i Zi = x=0 Pk ( x=0 x (m+n+1) x ) − 2 (m+n+1)2 (m−n)4 Z1 16m2 n2 − , T = 3((m − n)2 + m + n) + 2, and: Q0 = (m + n + 1)T k2 + ((−3n2 + 3mn + 3m + 1)T − 12(3mn + m + n) − 8)k + (−3m2 + 7m + 10n + 3nm + 10)T − 4(3mn + m + n + 1) Q1 = T k3 + 3(m − 1)T k2 + ((−3n2 + 3mn − 3m + 8)T − 6(6mn + m + n))k + (−3m2 + 7(m + n) + 3mn)T − 2(6mn + m + n) Proof. The expression of the variance given in Proposition 3 requires the computation of Xq (k, m, n), q = 0, 1, 2. Using the identities giving the expressions of X0 and X1 and Proposition 1, which provides the expression of X2 , σ 2 (A) can be reduced to the expression given by the corollary. 6 Theory and Analysis Our estimate of the conﬁdence interval for the AUC is based on a simple and natural assumption. The main idea for its computation is the following. Assume that a conﬁdence interval E = [e1 , e2 ] is given for the error rate of a classiﬁer C over a sample S, with the conﬁdence level 1 − . This interval may have have been derived from a binomial model of C, which is a standard assumption for determining a conﬁdence interval for the error rate, or from any other model used to compute that interval. For a given error rate e ∈ E, or equivalently for a given number of misclassiﬁcations, we can use the expectation and variance computed in the previous section and Chebyshev’s inequality to predict a conﬁdence interval Ae for the AUC at the conﬁdence level 1 − . Since our equiprobable model for the classiﬁcations is independent of the model used to compute the interval of conﬁdence for the error rate, we can use E and Ae , e ∈ E, to compute a conﬁdence interval of the AUC at the level (1 − )(1 − ). Theorem 1 Let C be a binary classiﬁer and let S be a data sample of size N with m positive examples and n negative examples, N = m + n. Let E = [e1 , e2 ] be a conﬁdence interval for the error rate of C over S at the conﬁdence level 1 − . Then, for any , 0 ≤ ≤ 1, we can compute a conﬁdence interval for the AUC value of the classiﬁer C at the conﬁdence level (1 − )(1 − ) that depends only on , , m, n, and the interval E. Proof. Let k1 = N e1 and k2 = N e2 be the number of errors associated to the error rates e1 and e2 , and let IK be the interval IK = [k1 , k2 ]. For a ﬁxed k ∈ IK , by Propositions 2 and Corollary 1, we can compute the exact value of the expectation E[Ak ] and variance σ 2 (Ak ) of the AUC Ak . Using Chebyshev’s inequality, for any k ∈ IK and any k > 0, σ(Ak ) P |Ak − E[Ak ]| ≥ √ k ≤ (7) k where E[Ak ] and σ(Ak ) are the expressions given in Propositions 2 and Corollary 1, which depend only on k, m, and n. Let α1 and α2 be deﬁned by: α1 = min k∈IK σ(Ak ) E[Ak ] − √ σ(Ak ) α2 = max E[Ak ] + √ k∈IK k (8) k α1 and α2 only depend on IK (i.e., on e1 and e2 ), and on k, m, and n. Let IA be the conﬁdence interval deﬁned by IA = [α1 , α2 ] and let k = for all k ∈ IK . Using the fact that the conﬁdence interval E is independent of our equiprobability model for ﬁxed-k AUC values and the Bayes’ rule: P(A ∈ IA ) = k∈R+ ≥ k∈IK P (A ∈ IA | K = k)P (K = k) (9) P (A ∈ IA | K = k)P (K = k) (10) ≥ (1 − ) k∈IK P (K = k) ≥ (1 − )(1 − ) (11) where we used the property of Eq. 7 and the deﬁnitions of the intervals IK and IA . Thus, IA constitutes a conﬁdence interval for the AUC value of C at the conﬁdence level (1 − )(1 − ). In practice, the conﬁdence interval E is often determined as a result of the assumption that C follows a binomial law. This leads to the following theorem. .020 .035 Standard deviation Standard deviation .030 .015 .010 Max Distribution−dependent Distribution−independent .005 .025 .020 .015 Max Distribution−dependent Distribution−independent .010 .005 0.75 0.80 0.85 0.90 0.95 1.00 0.6 0.7 0.8 AUC (a) 0.9 1.0 AUC (b) Figure 1: Comparison of the standard deviations for three different methods with: (a) m = n = 500; (b) m = 400 and n = 200. The curves are obtained by computing the expected AUC and its standard deviations for different values of the error rate using the maximum-variance approach (Eq. 3), our distribution-independent method, and the distribution-dependent approach of Hanley [10]. Theorem 2 Let C be a binary classiﬁer, let S be a data sample of size N with m positive examples and n negative examples, N = m + n, and let k0 be the number of misclassiﬁcations of C on S. Assume that C follows a binomial law, then, for any , 0 ≤ ≤ 1, we can compute a conﬁdence interval of the AUC value of the classiﬁer C at the conﬁdence level 1 − that depends only on , k0 , m, and n. Proof. Assume that C follows a binomial law with coefﬁcient p. Then, Chebyshev’s inequality yields: 1 p(1 − p) ≤ (12) P(|C − E[C]| ≥ η) ≤ 2 Nη 4N η 2 1 1 Thus, E = [ k0 − √ √ , k0 + √ √ ] forms a conﬁdence interval for the N 2 (1− 1− )N N 2 √ (1− 1− )N error rate of C at the conﬁdence level 1 − . By Theorem 1, we can√ compute for the √ AUC value a conﬁdence interval at the level (1 − (1 − 1 − ))(1 − (1 − 1 − )) = 1 − depending only on , m, n, and the interval E, i.e., k0 , N = m + n, and . For large N , we can use the normal approximation of the binomial law to determine a ﬁner interval E. Indeed, for large N , √ (13) P(|C − E[C]| ≥ η) ≤ 2Φ(2 N η) with Φ(u) = ∞ e−x2 /2 √ dx. u 2π Thus, E = [ k0 − N √ Φ−1 ( 1− 21− ) k0 √ ,N 2 N √ conﬁdence interval for the error rate at the conﬁdence level √ + Φ−1 ( 1− 21− ) √ ] 2 N is the 1− . For simplicity, in the proof of Theorem 2, k was chosen to be a constant ( k = ) but, in general, it can be another function of k leading to tighter conﬁdence intervals. The results presented in the next section were obtained with k = a0 exp((k − k0 )2 /2a2 ), where a0 1 and a1 are constants selected so that the inequality 11 be veriﬁed. 7 Experiments and Comparisons The analysis in the previous section provides a principled method for computing a conﬁdence interval of the AUC value of a classier C at the conﬁdence level 1 − that depends only on k, n and m. As already discussed, other expressions found in the statistical literature lead to either too loose or unsafely narrow conﬁdence intervals based on questionable assumptions on the probability functions Px and Py [10, 15]. Figure 1 shows a comparison of the standard deviations obtained using the maximum-approach (Eq. 3), the distribution-dependent expression from [10], and our distribution-independent method for NAME m+n n m+n AUC k m+n σindep σA σdep σmax 368 700 303 1159 2473 201 0.63 0.67 0.54 0.17 0.10 0.37 0.70 0.63 0.87 0.85 0.84 0.85 0.24 0.26 0.13 0.05 0.03 0.13 0.0297 0.0277 0.0176 0.0177 0.0164 0.0271 0.0440 0.0330 0.0309 0.0161 0.0088 0.0463 0.0269 0.0215 0.0202 0.0176 0.0161 0.0306 0.0392 0.0317 0.0281 0.0253 0.0234 0.0417 pima yeast credit internet-ads page-blocks ionosphere Table 1: Accuracy and AUC values for AdaBoost [8] and estimated standard deviations for several datasets from the UC Irvine repository. σindep is a distribution-independent standard deviation obtained using our method (Theorem 2). σA is given by Eq. (2) with the values of A, Pxxy , and Pxyy derived from data. σdep is the distribution-dependent standard deviation of Hanley [10], which is based on assumptions that may not always hold. σmax is deﬁned by Eq. (3). All results were obtained on a randomly selected test set of size m + n. various error rates. For m = n = 500, our distribution-independent method consistently leads to tighter conﬁdence intervals (Fig. 1 (a)). It also leads to tighter conﬁdence intervals for AUC values more than .75 for the uneven distribution m = 400 and n = 200 (Fig. 1 (b)). For lower AUC values, the distribution-dependent approach produces tighter intervals, but its underlying assumptions may not hold. A different comparison was made using several datasets available from the UC Irvine repository (Table 1). The table shows that our estimates of the standard deviations (σindep ) are in general close to or tighter than the distribution-dependent standard deviation σdep of Hanley [10]. This is despite we do not make any assumption about the distributions of positive and negative examples. In contrast, Hanley’s method is based on speciﬁc assumptions about these distributions. Plots of the actual ranking distribution demonstrate that these assumptions are often violated however. Thus, the relatively good performance of Hanley’s approach on several data sets can be viewed as fortuitous and is not general. Our distribution-independent method provides tight conﬁdence intervals, in some cases tighter than those derived from σA , in particular because it exploits the information provided by the error rate. Our analysis can also be used to determine if the AUC values produced by two classiﬁers are statistically signiﬁcant by checking if the AUC value of one falls within the conﬁdence interval of the other. 8 Conclusion We presented principled techniques for computing useful conﬁdence intervals for the AUC from simple parameters: the error rate, and the negative and positive sample sizes. We demonstrated the practicality of these conﬁdence intervals by comparing them to previous approaches in several tasks. We also derived the exact expression of the variance of the AUC for a ﬁxed k, which can be of interest in other analyses related to the AUC. The Wilcoxon-Mann-Whitney statistic is a general measure of the quality of a ranking that is an estimate of the probability that the classiﬁer ranks a randomly chosen positive example higher than a negative example. One could argue that accuracy at the top or the bottom of the ranking is of higher importance. This, however, contrarily to some belief, is already captured to a certain degree by the deﬁnition of the Wilcoxon-Mann-Whitney statistic which penalizes more errors at the top or the bottom of the ranking. It is however an interesting research problem to determine how to incorporate this bias in a stricter way in the form of a score-speciﬁc weight in the ranking measure, a weighted WilcoxonMann-Whitney statistic, or how to compute the corresponding expected value and standard deviation in a general way and design machine learning algorithms to optimize such a mea- sure. A preliminary analysis suggests, however, that the calculation of the expectation and the variance are likely to be extremely complex in that case. Finally, it could also be interesting but difﬁcult to adapt our results to the distribution-dependent case and compare them to those of [10]. Acknowledgments We thank Rob Schapire for pointing out to us the problem of the statistical signiﬁcance of the AUC, Daryl Pregibon for the reference to [11], and Saharon Rosset for various discussions about the topic of this paper. References [1] D. Bamber. The Area above the Ordinal Dominance Graph and the Area below the Receiver Operating Characteristic Graph. Journal of Math. Psychology, 12, 1975. [2] Z. W. Birnbaum and O. M. Klose. Bounds for the Variance of the Mann-Whitney Statistic. Annals of Mathematical Statistics, 38, 1957. [3] J-H. Chauchat, R. Rakotomalala, M. Carloz, and C. Pelletier. Targeting Customer Groups using Gain and Cost Matrix; a Marketing Application. Technical report, ERIC Laboratory - University of Lyon 2, 2001. [4] Corinna Cortes and Mehryar Mohri. AUC Optimization vs. Error Rate Minimization. In Advances in Neural Information Processing Systems (NIPS 2003), volume 16, Vancouver, Canada, 2004. MIT Press. [5] D. Van Dantzig. On the Consistency and Power of Wilcoxon’s Two Sample Test. In Koninklijke Nederlandse Akademie van Weterschappen, Series A, volume 54, 1915. [6] J. P. Egan. Signal Detection Theory and ROC Analysis. Academic Press, 1975. [7] C. Ferri, P. Flach, and J. Hern´ ndez-Orallo. Learning Decision Trees Using the Area a Under the ROC Curve. In Proceedings of the 19th International Conference on Machine Learning. Morgan Kaufmann, 2002. [8] Yoav Freund and Robert E. Schapire. A Decision Theoretical Generalization of OnLine Learning and an Application to Boosting. In Proceedings of the Second European Conference on Computational Learning Theory, volume 2, 1995. [9] Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Mathematics. Addison-Wesley, Reading, Massachusetts, 1994. [10] J. A. Hanley and B. J. McNeil. The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve. Radiology, 1982. [11] E. L. Lehmann. Nonparametrics: Statistical Methods Based on Ranks. Holden-Day, San Francisco, California, 1975. [12] M. C. Mozer, R. Dodier, M. D. Colagrosso, C. Guerra-Salcedo, and R. Wolniewicz. Prodding the ROC Curve: Constrained Optimization of Classiﬁer Performance. In Neural Information Processing Systems (NIPS 2002). MIT Press, 2002. [13] C. Perlich, F. Provost, and J. Simonoff. Tree Induction vs. Logistic Regression: A Learning Curve Analysis. Journal of Machine Learning Research, 2003. [14] F. Provost and T. Fawcett. Analysis and Visualization of Classiﬁer Performance: Comparison under Imprecise Class and Cost Distribution. In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining. AAAI, 1997. [15] Saharon Rosset. Ranking-Methods for Flexible Evaluation and Efﬁcient Comparison of 2-Class Models. Master’s thesis, Tel-Aviv University, 1999. [16] L. Yan, R. Dodier, M. C. Mozer, and R. Wolniewicz. Optimizing Classiﬁer Performance via the Wilcoxon-Mann-Whitney Statistics. In Proceedings of the International Conference on Machine Learning, 2003.</p><p>Reference: <a title="nips-2004-45-reference" href="../nips2004_reference/nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('auc', 0.801), ('dent', 0.318), ('interv', 0.243), ('con', 0.141), ('hanley', 0.133), ('roc', 0.109), ('ik', 0.106), ('ak', 0.098), ('rank', 0.092), ('ia', 0.088), ('pxxy', 0.076), ('pxyy', 0.076), ('proposit', 0.075), ('binom', 0.072), ('xq', 0.066), ('py', 0.066), ('tight', 0.063), ('neg', 0.061), ('px', 0.06), ('devy', 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="45-tfidf-1" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<p>2 0.53658336 <a title="45-tfidf-2" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>3 0.18459667 <a title="45-tfidf-3" href="./nips-2004-Semi-supervised_Learning_on_Directed_Graphs.html">165 nips-2004-Semi-supervised Learning on Directed Graphs</a></p>
<p>4 0.14774546 <a title="45-tfidf-4" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>5 0.083328657 <a title="45-tfidf-5" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>6 0.076327957 <a title="45-tfidf-6" href="./nips-2004-Co-Training_and_Expansion%3A_Towards_Bridging_Theory_and_Practice.html">37 nips-2004-Co-Training and Expansion: Towards Bridging Theory and Practice</a></p>
<p>7 0.059473597 <a title="45-tfidf-7" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>8 0.058470987 <a title="45-tfidf-8" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>9 0.054757442 <a title="45-tfidf-9" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>10 0.05343632 <a title="45-tfidf-10" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>11 0.053289 <a title="45-tfidf-11" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>12 0.040568143 <a title="45-tfidf-12" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>13 0.039363671 <a title="45-tfidf-13" href="./nips-2004-Exponential_Family_Harmoniums_with_an_Application_to_Information_Retrieval.html">66 nips-2004-Exponential Family Harmoniums with an Application to Information Retrieval</a></p>
<p>14 0.038535971 <a title="45-tfidf-14" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>15 0.037945483 <a title="45-tfidf-15" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>16 0.037331145 <a title="45-tfidf-16" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>17 0.037005227 <a title="45-tfidf-17" href="./nips-2004-Density_Level_Detection_is_Classification.html">49 nips-2004-Density Level Detection is Classification</a></p>
<p>18 0.035251893 <a title="45-tfidf-18" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>19 0.034493744 <a title="45-tfidf-19" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>20 0.033957694 <a title="45-tfidf-20" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.121), (1, 0.047), (2, -0.001), (3, 0.078), (4, -0.002), (5, 0.125), (6, 0.054), (7, 0.145), (8, 0.312), (9, 0.018), (10, -0.054), (11, 0.383), (12, -0.205), (13, 0.061), (14, 0.036), (15, -0.284), (16, -0.137), (17, -0.09), (18, -0.173), (19, 0.067), (20, -0.036), (21, 0.305), (22, 0.074), (23, -0.036), (24, 0.074), (25, 0.07), (26, -0.077), (27, -0.053), (28, 0.056), (29, -0.066), (30, 0.033), (31, -0.18), (32, 0.031), (33, 0.07), (34, 0.083), (35, 0.049), (36, -0.053), (37, 0.011), (38, -0.049), (39, 0.068), (40, -0.023), (41, 0.021), (42, 0.034), (43, -0.071), (44, 0.025), (45, 0.056), (46, 0.039), (47, -0.025), (48, 0.01), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94235301 <a title="45-lsi-1" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<p>2 0.8534292 <a title="45-lsi-2" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>3 0.39024225 <a title="45-lsi-3" href="./nips-2004-Semi-supervised_Learning_on_Directed_Graphs.html">165 nips-2004-Semi-supervised Learning on Directed Graphs</a></p>
<p>4 0.32487154 <a title="45-lsi-4" href="./nips-2004-Result_Analysis_of_the_NIPS_2003_Feature_Selection_Challenge.html">156 nips-2004-Result Analysis of the NIPS 2003 Feature Selection Challenge</a></p>
<p>5 0.27890474 <a title="45-lsi-5" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>6 0.23839965 <a title="45-lsi-6" href="./nips-2004-Co-Training_and_Expansion%3A_Towards_Bridging_Theory_and_Practice.html">37 nips-2004-Co-Training and Expansion: Towards Bridging Theory and Practice</a></p>
<p>7 0.21043733 <a title="45-lsi-7" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>8 0.20891361 <a title="45-lsi-8" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>9 0.1860351 <a title="45-lsi-9" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>10 0.18567869 <a title="45-lsi-10" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>11 0.168047 <a title="45-lsi-11" href="./nips-2004-Semi-supervised_Learning_via_Gaussian_Processes.html">166 nips-2004-Semi-supervised Learning via Gaussian Processes</a></p>
<p>12 0.16562614 <a title="45-lsi-12" href="./nips-2004-Probabilistic_Inference_of_Alternative_Splicing_Events_in_Microarray_Data.html">149 nips-2004-Probabilistic Inference of Alternative Splicing Events in Microarray Data</a></p>
<p>13 0.16312519 <a title="45-lsi-13" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>14 0.15960254 <a title="45-lsi-14" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<p>15 0.15290909 <a title="45-lsi-15" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>16 0.13600667 <a title="45-lsi-16" href="./nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</a></p>
<p>17 0.13339876 <a title="45-lsi-17" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>18 0.13314827 <a title="45-lsi-18" href="./nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification.html">36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</a></p>
<p>19 0.13212624 <a title="45-lsi-19" href="./nips-2004-Comparing_Beliefs%2C_Surveys%2C_and_Random_Walks.html">41 nips-2004-Comparing Beliefs, Surveys, and Random Walks</a></p>
<p>20 0.1319261 <a title="45-lsi-20" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.014), (15, 0.052), (26, 0.02), (27, 0.066), (37, 0.15), (67, 0.244), (74, 0.051), (77, 0.137), (81, 0.051), (96, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81485516 <a title="45-lda-1" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>same-paper 2 0.77561045 <a title="45-lda-2" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<p>3 0.69216156 <a title="45-lda-3" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>4 0.68828839 <a title="45-lda-4" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>5 0.68495011 <a title="45-lda-5" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<p>6 0.68272454 <a title="45-lda-6" href="./nips-2004-Parallel_Support_Vector_Machines%3A_The_Cascade_SVM.html">144 nips-2004-Parallel Support Vector Machines: The Cascade SVM</a></p>
<p>7 0.68085706 <a title="45-lda-7" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>8 0.67942876 <a title="45-lda-8" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<p>9 0.67927271 <a title="45-lda-9" href="./nips-2004-Binet-Cauchy_Kernels.html">30 nips-2004-Binet-Cauchy Kernels</a></p>
<p>10 0.67718065 <a title="45-lda-10" href="./nips-2004-Bayesian_Regularization_and_Nonnegative_Deconvolution_for_Time_Delay_Estimation.html">27 nips-2004-Bayesian Regularization and Nonnegative Deconvolution for Time Delay Estimation</a></p>
<p>11 0.67638141 <a title="45-lda-11" href="./nips-2004-A_Temporal_Kernel-Based_Model_for_Tracking_Hand_Movements_from_Neural_Activities.html">12 nips-2004-A Temporal Kernel-Based Model for Tracking Hand Movements from Neural Activities</a></p>
<p>12 0.67502713 <a title="45-lda-12" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>13 0.67479473 <a title="45-lda-13" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>14 0.67422718 <a title="45-lda-14" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>15 0.67328727 <a title="45-lda-15" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>16 0.67266101 <a title="45-lda-16" href="./nips-2004-Using_the_Equivalent_Kernel_to_Understand_Gaussian_Process_Regression.html">201 nips-2004-Using the Equivalent Kernel to Understand Gaussian Process Regression</a></p>
<p>17 0.67210037 <a title="45-lda-17" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>18 0.66982323 <a title="45-lda-18" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>19 0.6692062 <a title="45-lda-19" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>20 0.66907769 <a title="45-lda-20" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
