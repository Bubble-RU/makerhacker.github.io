<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2004-Mass Meta-analysis in Talairach Space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-109" href="#">nips2004-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2004-Mass Meta-analysis in Talairach Space</h1>
<br/><p>Source: <a title="nips-2004-109-pdf" href="http://papers.nips.cc/paper/2614-mass-meta-analysis-in-talairach-space.pdf">pdf</a></p><p>Author: Finn \. Nielsen</p><p>Abstract: We provide a method for mass meta-analysis in a neuroinformatics database containing stereotaxic Talairach coordinates from neuroimaging experiments. Database labels are used to group the individual experiments, e.g., according to cognitive function, and the consistent pattern of the experiments within the groups are determined. The method voxelizes each group of experiments via a kernel density estimation, forming probability density volumes. The values in the probability density volumes are compared to null-hypothesis distributions generated by resamplings from the entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. This allows for mass meta-analysis, with the construction of a list with the most prominent associations between brain areas and group labels. Furthermore, the method can be used for functional labeling of voxels. 1</p><p>Reference: <a title="nips-2004-109-reference" href="../nips2004_reference/nips-2004-Mass_Meta-analysis_in_Talairach_Space_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 dk  Abstract We provide a method for mass meta-analysis in a neuroinformatics database containing stereotaxic Talairach coordinates from neuroimaging experiments. [sent-3, score-0.354]
</p><p>2 Database labels are used to group the individual experiments, e. [sent-4, score-0.031]
</p><p>3 , according to cognitive function, and the consistent pattern of the experiments within the groups are determined. [sent-6, score-0.094]
</p><p>4 The method voxelizes each group of experiments via a kernel density estimation, forming probability density volumes. [sent-7, score-0.136]
</p><p>5 The values in the probability density volumes are compared to null-hypothesis distributions generated by resamplings from the entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. [sent-8, score-0.35]
</p><p>6 This allows for mass meta-analysis, with the construction of a list with the most prominent associations between brain areas and group labels. [sent-9, score-0.323]
</p><p>7 Furthermore, the method can be used for functional labeling of voxels. [sent-10, score-0.163]
</p><p>8 1  Introduction  Neuroimaging experimenters usually report their results in the form of 3dimensional coordinates in the standardized stereotaxic Talairach system [1]. [sent-11, score-0.153]
</p><p>9 Automated meta-analytic and information retrieval methods are enabled when such data are represented in databases such as the BrainMap DBJ ([2], www. [sent-12, score-0.052]
</p><p>10 Example methods include outlier detection [4] and identiﬁcation of similar volumes [5]. [sent-15, score-0.053]
</p><p>11 Apart from the stereotaxic coordinates, the databases record details of the experimental situation, e. [sent-16, score-0.083]
</p><p>12 In the Brede database the main annotation is the so-called “external components”1 which are heuristically organized in a simple ontology: A directed graph (more speciﬁcally, a causal network) with the most general components as the roots of the graph, e. [sent-19, score-0.185]
</p><p>13 , 1  External components might be thought of as “cognitive components” or simply “brain functions”, but they are more general, e. [sent-21, score-0.07]
</p><p>14 The components are called “external” since they are external variables to the brain image. [sent-24, score-0.511]
</p><p>15 WOEXT: 41 Cold pain WOEXT: 40 Pain  WOEXT: 261 Thermal pain WOEXT: 69 Hot pain  Figure 1: The external components around “thermal pain” with “pain” as the parent of “thermal pain” and “cold pain” and “hot pain” as children. [sent-25, score-2.049]
</p><p>16 “hot pain” is a child of “thermal pain” that in turn is a child of “pain” (see Figure 1). [sent-26, score-0.046]
</p><p>17 The simple ontology is setup from information typically found in the introduction section of scientiﬁc articles, and it is compared with the Medical Subject Headings ontology of the National Library of Medicine. [sent-27, score-0.166]
</p><p>18 The Brede database is organized, like the BrainMap DBJ, on diﬀerent levels with scientiﬁc papers on the highest level. [sent-29, score-0.089]
</p><p>19 The individual experiments are typically labeled with an external component. [sent-31, score-0.426]
</p><p>20 We will describe a meta-analytic method that identiﬁes important associations between external components and clustered Talairach coordinates. [sent-33, score-0.481]
</p><p>21 We have previously modeled the relation between Talairach coordinates and neuroanatomical terms [4, 6] and the method that we propose here can be seen as an extension describing the relationship between Talairach coordinates and, e. [sent-34, score-0.173]
</p><p>22 2  Method  The data from the Brede database [3] was used, which at the time contained data from 126 scientiﬁc article containing 391 experiments and 2734 locations. [sent-37, score-0.128]
</p><p>23 The locations referenced with respect to the MNI atlas were realigned to the Talairach atlas [7]. [sent-39, score-0.216]
</p><p>24 To form a vectorial representation, each location was voxelized by convolving the location l at position vl = [x, y, z] with a Gaussian kernel [4, 8, 9]. [sent-40, score-0.096]
</p><p>25 This constructed a probability density in Talairach space v (v − vl ) (v − vl ) p(v|l) = (2πσ 2 )−3/2 exp − , (1) 2σ 2 with the width σ ﬁxed to 1 centimeter. [sent-41, score-0.133]
</p><p>26 A paper with many locations and experiments should not be allowed to dominate the results. [sent-44, score-0.109]
</p><p>27 This can be the case if all locations are given equal weight. [sent-45, score-0.07]
</p><p>28 On the other hand a paper which reports just a single coordinate should probably not be weighted as much as one with many experiments and locations: Few reported locations might be due to limited (statistical) power of the experiment. [sent-46, score-0.109]
</p><p>29 As a compromise between the two extremes we used the square root of the number of the locations within an experiment and the square root of the number of experiments within a paper for the prior P (l|e). [sent-47, score-0.161]
</p><p>30 The square root normalization is also an appropriate normalization in certain voting systems [10]. [sent-48, score-0.026]
</p><p>31 The second prior was uniform P (e|t) ∝ 1 for those experiments that were labeled with the t external component. [sent-49, score-0.426]
</p><p>32 The continuous volume were sampled at regular grid points to establish a vector w t for each external component wt ≡ p(v|t). [sent-50, score-0.438]
</p><p>33 The experiments were resampled without regard to the paper they originated from. [sent-52, score-0.143]
</p><p>34 The maximum across voxels was found as: ur (E) = max [wr (j)] , j  (4)  where j is an index over voxels and r is the resample index. [sent-53, score-0.423]
</p><p>35 With R resamplings we obtain a vector u(E) = [u1 (E) . [sent-54, score-0.042]
</p><p>36 uR (E)] that is a function of the number of experiments and which forms an empirical distribution u(E). [sent-60, score-0.039]
</p><p>37 Thus the resampling allows us to convert the probability density value to a probability that is comparable across external components of diﬀerent sizes. [sent-62, score-0.575]
</p><p>38 The maximum statistics deal automatically with the multiple comparison problem across voxels [11]. [sent-63, score-0.216]
</p><p>39 dt,j can be computed by counting the fraction of the resampled values ur that are below the value of wt,j . [sent-64, score-0.17]
</p><p>40 The resampling distribution can also be approximated and smoothed by modeling it with a non-linear function. [sent-65, score-0.066]
</p><p>41 The non-linear function allows for a more compact representation of the empirical distribution of the resampled maximum statistics. [sent-67, score-0.104]
</p><p>42 As a ﬁnal step, the probability volumes for the external components wt were thresholded on selected levels and isosurfaces generated in the distance volume for visualization. [sent-68, score-0.645]
</p><p>43 Connected voxels within the thresholded volume were found by region identiﬁcation and the local maxima in the regions were determined. [sent-69, score-0.266]
</p><p>44 Functional labeling of speciﬁed voxels is also possible: The distances d t,j were collected in a (external component × voxel)-matrix D and the elements in the j column sorted. [sent-70, score-0.279]
</p><p>45 Lastly, the voxel were labeled with the top external component. [sent-71, score-0.526]
</p><p>46 Only the bottom nodes of the causal networks of external components are likely to be directly associated with experiments. [sent-72, score-0.476]
</p><p>47 To label the ancestors, the labels from  6  Randomization test statistics  10  test statistics (max pdf)  0. [sent-73, score-0.048]
</p><p>48 99  5  10  4  10 0 10  1  10  2  10  Number of experiments  Figure 2: The test statistics at various distances to the null-hypothesis (d = 1 − P ) after 1000 resamplings. [sent-78, score-0.093]
</p><p>49 The distance is shown as a function of the number of experiments E in the resampling. [sent-79, score-0.065]
</p><p>50 , a study explicitly labeled as “hot pain” were also be labeled as “thermal pain” and “pain”. [sent-82, score-0.068]
</p><p>51 Apart from this simple back propagation the hierarchical structure of the external components was not incorporated into the prior. [sent-83, score-0.423]
</p><p>52 3  Results  Figure 2 shows isolines in the cumulative distribution of the resampled maximum statistics u(E) as a function of the resampling set size (number of experiments) from E = 1 to E = 100. [sent-84, score-0.194]
</p><p>53 Since the vectorized volume is not normalized to form a probability density the curves are increasing with our selected normalization. [sent-85, score-0.084]
</p><p>54 Table 1 shows the result of sorting the maximum distances across voxel within the external components. [sent-86, score-0.549]
</p><p>55 Topping the list are external components associated with movement. [sent-87, score-0.524]
</p><p>56 The voxel with the largest distance is localized in v = (0, −8, 56) which most likely is due to movement studies activating the supplementary motor area. [sent-88, score-0.318]
</p><p>57 In the Brede database the mean is (6, −7, 55) for the locations in the right hemisphere labeled as supplementary motor area. [sent-89, score-0.259]
</p><p>58 Other voxels with a high distance for the movement external components are located in the primary motor area. [sent-90, score-0.687]
</p><p>59 A number of other entries on the list are associated with pain, with the main voxel at (0, 8, 32) in the right anterior cingulate. [sent-91, score-0.294]
</p><p>60 Other important areas are shown in Figure 3 with isosurfaces in the distance volume for the external component “pain” (WOEXT: 40). [sent-92, score-0.549]
</p><p>61 These are localized in the anterior cingulate, right and left insula and thalamus. [sent-93, score-0.143]
</p><p>62 Other external components high on the list are “audition” together with “voice”  # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  d 1. [sent-94, score-0.497]
</p><p>63 The numbers in the parentheses are the Brede database identiﬁers for the external components (WOEXT). [sent-110, score-0.512]
</p><p>64 This list was generated with coarse 8 × 8 × 8mm3 voxels and using the non-linear model approximation for the cumulative distribution functions. [sent-111, score-0.239]
</p><p>65 appearing in left and right superior temporal gyrus, and memory emerging in the posterior cingulate area. [sent-112, score-0.292]
</p><p>66 Unpleasantness and emotion are high on the list due to, e. [sent-113, score-0.137]
</p><p>67 , “fear” and “disgust” experiments that report activation in the right amygdala and nearby areas. [sent-115, score-0.069]
</p><p>68 An example of the functional labeling of a voxel appears in Table 2. [sent-116, score-0.302]
</p><p>69 The chosen voxel is (0, −56, 16) that appears in the posterior cingulate. [sent-117, score-0.17]
</p><p>70 Memory retrieval is the ﬁrst on the list in accordance with Table 1. [sent-118, score-0.126]
</p><p>71 Many of the other external components on the list are also related to memory. [sent-119, score-0.497]
</p><p>72 4  Discussion  The Brede database contains many thermal pain experiments, and it causes high scores for voxels from external components such as “pain” and “thermal pain”. [sent-120, score-1.407]
</p><p>73 The four focal “brain activations” that appear in Figure 3 are localized in areas (anterior cingulate, insula and thalamus) that an expert reviewer has previously identiﬁed as important in pain [14]. [sent-121, score-0.703]
</p><p>74 Thus there is consistency between our automated metaanalytic technique and a “manual” expert review. [sent-122, score-0.053]
</p><p>75 Many experiments that report activation in the posterior cingulate area have been included in the Brede database, and this is probably why memory is especially associated with this area. [sent-123, score-0.365]
</p><p>76 95 in the distance volume while the gray transparent surface appears at d = 0. [sent-126, score-0.077]
</p><p>77 Yellow glyphs appear at the local maxima in the thresholded volume. [sent-128, score-0.05]
</p><p>78 The viewpoint is situated nearest to the left superior posterior corner of the brain. [sent-129, score-0.054]
</p><p>79 A number of the substantial associations between brain areas and external components are not surprising, e. [sent-131, score-0.612]
</p><p>80 Our method has no inherent knowledge of what is already known, and thus not able distinguish novel associations from trivial. [sent-134, score-0.058]
</p><p>81 A down-side with the present method is that it requires the labeling of experiments during database entry and the construction of the hierarchy of the labels (Figure 1). [sent-135, score-0.178]
</p><p>82 Both are prone to “interpretation” and this is particularly a problem for complex cognitive functions. [sent-136, score-0.055]
</p><p>83 Our methodology, however, does not necessarily impose a single organization of the external components, and it is possible to rearrange these by deﬁning another adjacency matrix for the graph. [sent-137, score-0.353]
</p><p>84 In Table 1 the brain areas are represented in terms of Talairach coordinates. [sent-138, score-0.131]
</p><p>85 It should be possible to convert these coordinates further to neuroanatomical terms  # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  d 0. [sent-139, score-0.129]
</p><p>86 by using the models between coordinates and lobar anatomy that we previously have established [4, 6]. [sent-155, score-0.07]
</p><p>87 Functional labeling should allow us to build a complete functional atlas for the entire brain. [sent-156, score-0.236]
</p><p>88 The utility of this approach is, however, limited by the small size of the Brede database and its bias towards speciﬁc brain regions and external components. [sent-157, score-0.53]
</p><p>89 But such a functional atlas will serve as a neuroinformatic organizer for the increasing number of neuroimaging studies. [sent-158, score-0.269]
</p><p>90 Chen for identifying some of the thermal pain studies and the Villum Kann Rasmussen Foundation for their generous support of the author. [sent-163, score-0.73]
</p><p>91 The Brede database: a small database for functional neuA roimaging. [sent-173, score-0.202]
</p><p>92 Modeling of activation data in A the BrainMapTM database: Detection of outliers. [sent-178, score-0.03]
</p><p>93 Automatic anatomical labeling of Talairach coordinates and generation of volumes of interest via the BrainMap database. [sent-184, score-0.173]
</p><p>94 Meta-analysis of the functional neuroanatomy of single-word reading: method and validation. [sent-202, score-0.113]
</p><p>95 Non-parametric analysis of statistic images from functional mapping experiments. [sent-225, score-0.141]
</p><p>96 In Proceedings of the IEEE International Conference on Neural Networks, San Francisco, California, USA, volume 1, pages 46–51, 1993. [sent-229, score-0.051]
</p><p>97 “lyngby” — a modeler’s Matlab toolbox for spatio-temporal analysis of functional neuroimages. [sent-234, score-0.113]
</p><p>98 Imaging cognition II: An empirical review of 275 PET and fMRI studies. [sent-241, score-0.028]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pain', 0.542), ('external', 0.353), ('talairach', 0.292), ('brede', 0.208), ('thermal', 0.188), ('woext', 0.167), ('voxels', 0.165), ('voxel', 0.139), ('cingulate', 0.125), ('finn', 0.125), ('lars', 0.116), ('memory', 0.113), ('functional', 0.113), ('resampled', 0.104), ('kai', 0.099), ('nielsen', 0.091), ('database', 0.089), ('brain', 0.088), ('brainmap', 0.083), ('neuroimaging', 0.083), ('ontology', 0.083), ('rup', 0.083), ('stereotaxic', 0.083), ('list', 0.074), ('atlas', 0.073), ('hot', 0.073), ('neuroimage', 0.073), ('locations', 0.07), ('components', 0.07), ('coordinates', 0.07), ('resampling', 0.066), ('ur', 0.066), ('emotion', 0.063), ('associations', 0.058), ('cognitive', 0.055), ('disorders', 0.054), ('audition', 0.054), ('anterior', 0.054), ('volumes', 0.053), ('retrieval', 0.052), ('volume', 0.051), ('thresholded', 0.05), ('episodic', 0.05), ('vl', 0.05), ('labeling', 0.05), ('localized', 0.047), ('june', 0.045), ('hansen', 0.044), ('areas', 0.043), ('arup', 0.042), ('dbj', 0.042), ('declarative', 0.042), ('insula', 0.042), ('isosurfaces', 0.042), ('liptrot', 0.042), ('mni', 0.042), ('resamplings', 0.042), ('sensation', 0.042), ('unpleasantness', 0.042), ('matthew', 0.042), ('movement', 0.04), ('denmark', 0.04), ('experiments', 0.039), ('scienti', 0.038), ('january', 0.037), ('labeled', 0.034), ('component', 0.034), ('motor', 0.033), ('lyngby', 0.033), ('supplementary', 0.033), ('cold', 0.033), ('neuroanatomical', 0.033), ('density', 0.033), ('posterior', 0.031), ('voice', 0.031), ('group', 0.031), ('activation', 0.03), ('distances', 0.03), ('stress', 0.029), ('mass', 0.029), ('peter', 0.029), ('expert', 0.029), ('mapping', 0.028), ('identi', 0.028), ('cognition', 0.028), ('across', 0.027), ('associated', 0.027), ('causal', 0.026), ('temperature', 0.026), ('convert', 0.026), ('distance', 0.026), ('root', 0.026), ('statistics', 0.024), ('automated', 0.024), ('superior', 0.023), ('child', 0.023), ('location', 0.023), ('reading', 0.022), ('erent', 0.022), ('march', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="109-tfidf-1" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>Author: Finn \. Nielsen</p><p>Abstract: We provide a method for mass meta-analysis in a neuroinformatics database containing stereotaxic Talairach coordinates from neuroimaging experiments. Database labels are used to group the individual experiments, e.g., according to cognitive function, and the consistent pattern of the experiments within the groups are determined. The method voxelizes each group of experiments via a kernel density estimation, forming probability density volumes. The values in the probability density volumes are compared to null-hypothesis distributions generated by resamplings from the entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. This allows for mass meta-analysis, with the construction of a list with the most prominent associations between brain areas and group labels. Furthermore, the method can be used for functional labeling of voxels. 1</p><p>2 0.05826747 <a title="109-tfidf-2" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>Author: Vladimir Koltchinskii, Manel Martínez-ramón, Stefan Posse</p><p>Abstract: We study a method of optimal data-driven aggregation of classiﬁers in a convex combination and establish tight upper bounds on its excess risk with respect to a convex loss function under the assumption that the solution of optimal aggregation problem is sparse. We use a boosting type algorithm of optimal aggregation to develop aggregate classiﬁers of activation patterns in fMRI based on locally trained SVM classiﬁers. The aggregation coefﬁcients are then used to design a ”boosting map” of the brain needed to identify the regions with most signiﬁcant impact on classiﬁcation. 1</p><p>3 0.054303687 <a title="109-tfidf-3" href="./nips-2004-Detecting_Significant_Multidimensional_Spatial_Clusters.html">51 nips-2004-Detecting Significant Multidimensional Spatial Clusters</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore, Francisco Pereira, Tom M. Mitchell</p><p>Abstract: Assume a uniform, multidimensional grid of bivariate data, where each cell of the grid has a count ci and a baseline bi . Our goal is to ﬁnd spatial regions (d-dimensional rectangles) where the ci are signiﬁcantly higher than expected given bi . We focus on two applications: detection of clusters of disease cases from epidemiological data (emergency department visits, over-the-counter drug sales), and discovery of regions of increased brain activity corresponding to given cognitive tasks (from fMRI data). Each of these problems can be solved using a spatial scan statistic (Kulldorff, 1997), where we compute the maximum of a likelihood ratio statistic over all spatial regions, and ﬁnd the signiﬁcance of this region by randomization. However, computing the scan statistic for all spatial regions is generally computationally infeasible, so we introduce a novel fast spatial scan algorithm, generalizing the 2D scan algorithm of (Neill and Moore, 2004) to arbitrary dimensions. Our new multidimensional multiresolution algorithm allows us to ﬁnd spatial clusters up to 1400x faster than the naive spatial scan, without any loss of accuracy. 1</p><p>4 0.050632041 <a title="109-tfidf-4" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>Author: Sunita Sarawagi, William W. Cohen</p><p>Abstract: We describe semi-Markov conditional random ﬁelds (semi-CRFs), a conditionally trained version of semi-Markov chains. Intuitively, a semiCRF on an input sequence x outputs a “segmentation” of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on ﬁve named entity recognition problems, semi-CRFs generally outperform conventional CRFs. 1</p><p>5 0.037495408 <a title="109-tfidf-5" href="./nips-2004-Saliency-Driven_Image_Acuity_Modulation_on_a_Reconfigurable_Array_of_Spiking_Silicon_Neurons.html">157 nips-2004-Saliency-Driven Image Acuity Modulation on a Reconfigurable Array of Spiking Silicon Neurons</a></p>
<p>Author: R. J. Vogelstein, Udayan Mallik, Eugenio Culurciello, Gert Cauwenberghs, Ralph Etienne-Cummings</p><p>Abstract: We have constructed a system that uses an array of 9,600 spiking silicon neurons, a fast microcontroller, and digital memory, to implement a reconﬁgurable network of integrate-and-ﬁre neurons. The system is designed for rapid prototyping of spiking neural networks that require high-throughput communication with external address-event hardware. Arbitrary network topologies can be implemented by selectively routing address-events to speciﬁc internal or external targets according to a memory-based projective ﬁeld mapping. The utility and versatility of the system is demonstrated by conﬁguring it as a three-stage network that accepts input from an address-event imager, detects salient regions of the image, and performs spatial acuity modulation around a high-resolution fovea that is centered on the location of highest salience. 1</p><p>6 0.037413381 <a title="109-tfidf-6" href="./nips-2004-Instance-Based_Relevance_Feedback_for_Image_Retrieval.html">85 nips-2004-Instance-Based Relevance Feedback for Image Retrieval</a></p>
<p>7 0.036873642 <a title="109-tfidf-7" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>8 0.036792878 <a title="109-tfidf-8" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>9 0.036182094 <a title="109-tfidf-9" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<p>10 0.033069842 <a title="109-tfidf-10" href="./nips-2004-At_the_Edge_of_Chaos%3A_Real-time_Computations_and_Self-Organized_Criticality_in_Recurrent_Neural_Networks.html">26 nips-2004-At the Edge of Chaos: Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks</a></p>
<p>11 0.032584213 <a title="109-tfidf-11" href="./nips-2004-A_Temporal_Kernel-Based_Model_for_Tracking_Hand_Movements_from_Neural_Activities.html">12 nips-2004-A Temporal Kernel-Based Model for Tracking Hand Movements from Neural Activities</a></p>
<p>12 0.032019746 <a title="109-tfidf-12" href="./nips-2004-Joint_MRI_Bias_Removal_Using_Entropy_Minimization_Across_Images.html">89 nips-2004-Joint MRI Bias Removal Using Entropy Minimization Across Images</a></p>
<p>13 0.031815927 <a title="109-tfidf-13" href="./nips-2004-Edge_of_Chaos_Computation_in_Mixed-Mode_VLSI_-_A_Hard_Liquid.html">58 nips-2004-Edge of Chaos Computation in Mixed-Mode VLSI - A Hard Liquid</a></p>
<p>14 0.031634171 <a title="109-tfidf-14" href="./nips-2004-Responding_to_Modalities_with_Different_Latencies.html">155 nips-2004-Responding to Modalities with Different Latencies</a></p>
<p>15 0.031383466 <a title="109-tfidf-15" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>16 0.031356998 <a title="109-tfidf-16" href="./nips-2004-Semi-supervised_Learning_by_Entropy_Minimization.html">164 nips-2004-Semi-supervised Learning by Entropy Minimization</a></p>
<p>17 0.031224867 <a title="109-tfidf-17" href="./nips-2004-Dynamic_Bayesian_Networks_for_Brain-Computer_Interfaces.html">56 nips-2004-Dynamic Bayesian Networks for Brain-Computer Interfaces</a></p>
<p>18 0.028642183 <a title="109-tfidf-18" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>19 0.027746668 <a title="109-tfidf-19" href="./nips-2004-Euclidean_Embedding_of_Co-Occurrence_Data.html">62 nips-2004-Euclidean Embedding of Co-Occurrence Data</a></p>
<p>20 0.027459398 <a title="109-tfidf-20" href="./nips-2004-An_Auditory_Paradigm_for_Brain-Computer_Interfaces.html">20 nips-2004-An Auditory Paradigm for Brain-Computer Interfaces</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.098), (1, -0.008), (2, -0.016), (3, -0.035), (4, 0.002), (5, 0.062), (6, 0.042), (7, 0.026), (8, -0.004), (9, -0.036), (10, -0.021), (11, 0.015), (12, -0.025), (13, -0.048), (14, 0.04), (15, -0.009), (16, 0.025), (17, -0.035), (18, -0.018), (19, -0.07), (20, -0.005), (21, 0.02), (22, -0.013), (23, 0.044), (24, -0.006), (25, -0.013), (26, 0.011), (27, 0.027), (28, -0.057), (29, 0.011), (30, -0.064), (31, 0.012), (32, 0.006), (33, 0.068), (34, 0.096), (35, -0.011), (36, 0.0), (37, -0.084), (38, 0.076), (39, -0.093), (40, -0.076), (41, 0.051), (42, -0.032), (43, -0.076), (44, 0.054), (45, -0.106), (46, -0.166), (47, 0.086), (48, -0.029), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94325781 <a title="109-lsi-1" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>Author: Finn \. Nielsen</p><p>Abstract: We provide a method for mass meta-analysis in a neuroinformatics database containing stereotaxic Talairach coordinates from neuroimaging experiments. Database labels are used to group the individual experiments, e.g., according to cognitive function, and the consistent pattern of the experiments within the groups are determined. The method voxelizes each group of experiments via a kernel density estimation, forming probability density volumes. The values in the probability density volumes are compared to null-hypothesis distributions generated by resamplings from the entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. This allows for mass meta-analysis, with the construction of a list with the most prominent associations between brain areas and group labels. Furthermore, the method can be used for functional labeling of voxels. 1</p><p>2 0.60463631 <a title="109-lsi-2" href="./nips-2004-Detecting_Significant_Multidimensional_Spatial_Clusters.html">51 nips-2004-Detecting Significant Multidimensional Spatial Clusters</a></p>
<p>Author: Daniel B. Neill, Andrew W. Moore, Francisco Pereira, Tom M. Mitchell</p><p>Abstract: Assume a uniform, multidimensional grid of bivariate data, where each cell of the grid has a count ci and a baseline bi . Our goal is to ﬁnd spatial regions (d-dimensional rectangles) where the ci are signiﬁcantly higher than expected given bi . We focus on two applications: detection of clusters of disease cases from epidemiological data (emergency department visits, over-the-counter drug sales), and discovery of regions of increased brain activity corresponding to given cognitive tasks (from fMRI data). Each of these problems can be solved using a spatial scan statistic (Kulldorff, 1997), where we compute the maximum of a likelihood ratio statistic over all spatial regions, and ﬁnd the signiﬁcance of this region by randomization. However, computing the scan statistic for all spatial regions is generally computationally infeasible, so we introduce a novel fast spatial scan algorithm, generalizing the 2D scan algorithm of (Neill and Moore, 2004) to arbitrary dimensions. Our new multidimensional multiresolution algorithm allows us to ﬁnd spatial clusters up to 1400x faster than the naive spatial scan, without any loss of accuracy. 1</p><p>3 0.47324473 <a title="109-lsi-3" href="./nips-2004-Edge_of_Chaos_Computation_in_Mixed-Mode_VLSI_-_A_Hard_Liquid.html">58 nips-2004-Edge of Chaos Computation in Mixed-Mode VLSI - A Hard Liquid</a></p>
<p>Author: Felix Schürmann, Karlheinz Meier, Johannes Schemmel</p><p>Abstract: Computation without stable states is a computing paradigm different from Turing’s and has been demonstrated for various types of simulated neural networks. This publication transfers this to a hardware implemented neural network. Results of a software implementation are reproduced showing that the performance peaks when the network exhibits dynamics at the edge of chaos. The liquid computing approach seems well suited for operating analog computing devices such as the used VLSI neural network. 1</p><p>4 0.39016113 <a title="109-lsi-4" href="./nips-2004-Responding_to_Modalities_with_Different_Latencies.html">155 nips-2004-Responding to Modalities with Different Latencies</a></p>
<p>Author: Fredrik Bissmarck, Hiroyuki Nakahara, Kenji Doya, Okihide Hikosaka</p><p>Abstract: Motor control depends on sensory feedback in multiple modalities with different latencies. In this paper we consider within the framework of reinforcement learning how different sensory modalities can be combined and selected for real-time, optimal movement control. We propose an actor-critic architecture with multiple modules, whose output are combined using a softmax function. We tested our architecture in a simulation of a sequential reaching task. Reaching was initially guided by visual feedback with a long latency. Our learning scheme allowed the agent to utilize the somatosensory feedback with shorter latency when the hand is near the experienced trajectory. In simulations with different latencies for visual and somatosensory feedback, we found that the agent depended more on feedback with shorter latency. 1</p><p>5 0.38493353 <a title="109-lsi-5" href="./nips-2004-Learning_Syntactic_Patterns_for_Automatic_Hypernym_Discovery.html">101 nips-2004-Learning Syntactic Patterns for Automatic Hypernym Discovery</a></p>
<p>Author: Rion Snow, Daniel Jurafsky, Andrew Y. Ng</p><p>Abstract: Semantic taxonomies such as WordNet provide a rich source of knowledge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hypernym pairs. Using “dependency path” features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (determining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet. 1</p><p>6 0.38365006 <a title="109-lsi-6" href="./nips-2004-Synchronization_of_neural_networks_by_mutual_learning_and_its_application_to_cryptography.html">180 nips-2004-Synchronization of neural networks by mutual learning and its application to cryptography</a></p>
<p>7 0.37131244 <a title="109-lsi-7" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>8 0.35999557 <a title="109-lsi-8" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>9 0.35384184 <a title="109-lsi-9" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>10 0.34994665 <a title="109-lsi-10" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>11 0.34956136 <a title="109-lsi-11" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>12 0.34939429 <a title="109-lsi-12" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>13 0.34839976 <a title="109-lsi-13" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>14 0.33238935 <a title="109-lsi-14" href="./nips-2004-The_Correlated_Correspondence_Algorithm_for_Unsupervised_Registration_of_Nonrigid_Surfaces.html">186 nips-2004-The Correlated Correspondence Algorithm for Unsupervised Registration of Nonrigid Surfaces</a></p>
<p>15 0.3269949 <a title="109-lsi-15" href="./nips-2004-A_Temporal_Kernel-Based_Model_for_Tracking_Hand_Movements_from_Neural_Activities.html">12 nips-2004-A Temporal Kernel-Based Model for Tracking Hand Movements from Neural Activities</a></p>
<p>16 0.32533956 <a title="109-lsi-16" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>17 0.31347975 <a title="109-lsi-17" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>18 0.31128481 <a title="109-lsi-18" href="./nips-2004-Saliency-Driven_Image_Acuity_Modulation_on_a_Reconfigurable_Array_of_Spiking_Silicon_Neurons.html">157 nips-2004-Saliency-Driven Image Acuity Modulation on a Reconfigurable Array of Spiking Silicon Neurons</a></p>
<p>19 0.30609724 <a title="109-lsi-19" href="./nips-2004-At_the_Edge_of_Chaos%3A_Real-time_Computations_and_Self-Organized_Criticality_in_Recurrent_Neural_Networks.html">26 nips-2004-At the Edge of Chaos: Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks</a></p>
<p>20 0.30078804 <a title="109-lsi-20" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.083), (15, 0.06), (25, 0.441), (26, 0.029), (31, 0.021), (32, 0.01), (33, 0.164), (35, 0.014), (50, 0.036), (51, 0.012), (87, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82743108 <a title="109-lda-1" href="./nips-2004-Mass_Meta-analysis_in_Talairach_Space.html">109 nips-2004-Mass Meta-analysis in Talairach Space</a></p>
<p>Author: Finn \. Nielsen</p><p>Abstract: We provide a method for mass meta-analysis in a neuroinformatics database containing stereotaxic Talairach coordinates from neuroimaging experiments. Database labels are used to group the individual experiments, e.g., according to cognitive function, and the consistent pattern of the experiments within the groups are determined. The method voxelizes each group of experiments via a kernel density estimation, forming probability density volumes. The values in the probability density volumes are compared to null-hypothesis distributions generated by resamplings from the entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. This allows for mass meta-analysis, with the construction of a list with the most prominent associations between brain areas and group labels. Furthermore, the method can be used for functional labeling of voxels. 1</p><p>2 0.79029655 <a title="109-lda-2" href="./nips-2004-Discrete_profile_alignment_via_constrained_information_bottleneck.html">52 nips-2004-Discrete profile alignment via constrained information bottleneck</a></p>
<p>Author: Sean O'rourke, Gal Chechik, Robin Friedman, Eleazar Eskin</p><p>Abstract: Amino acid proﬁles, which capture position-speciﬁc mutation probabilities, are a richer encoding of biological sequences than the individual sequences themselves. However, proﬁle comparisons are much more computationally expensive than discrete symbol comparisons, making proﬁles impractical for many large datasets. Furthermore, because they are such a rich representation, proﬁles can be diﬃcult to visualize. To overcome these problems, we propose a discretization for proﬁles using an expanded alphabet representing not just individual amino acids, but common proﬁles. By using an extension of information bottleneck (IB) incorporating constraints and priors on the class distributions, we ﬁnd an informationally optimal alphabet. This discretization yields a concise, informative textual representation for proﬁle sequences. Also alignments between these sequences, while nearly as accurate as the full proﬁleproﬁle alignments, can be computed almost as quickly as those between individual or consensus sequences. A full pairwise alignment of SwissProt would take years using proﬁles, but less than 3 days using a discrete IB encoding, illustrating how discrete encoding can expand the range of sequence problems to which proﬁle information can be applied. 1</p><p>3 0.78152293 <a title="109-lda-3" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>4 0.57764387 <a title="109-lda-4" href="./nips-2004-A_Probabilistic_Model_for_Online_Document_Clustering_with_Application_to_Novelty_Detection.html">10 nips-2004-A Probabilistic Model for Online Document Clustering with Application to Novelty Detection</a></p>
<p>Author: Jian Zhang, Zoubin Ghahramani, Yiming Yang</p><p>Abstract: In this paper we propose a probabilistic model for online document clustering. We use non-parametric Dirichlet process prior to model the growing number of clusters, and use a prior of general English language model as the base distribution to handle the generation of novel clusters. Furthermore, cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution. We use empirical Bayes method to estimate hyperparameters based on a historical dataset. Our probabilistic model is applied to the novelty detection task in Topic Detection and Tracking (TDT) and compared with existing approaches in the literature. 1</p><p>5 0.54636806 <a title="109-lda-5" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: We provide a worst-case analysis of selective sampling algorithms for learning linear threshold functions. The algorithms considered in this paper are Perceptron-like algorithms, i.e., algorithms which can be efﬁciently run in any reproducing kernel Hilbert space. Our algorithms exploit a simple margin-based randomized rule to decide whether to query the current label. We obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic counterparts, but using much fewer labels. We complement our theoretical ﬁndings with an empirical comparison on two text categorization tasks. The outcome of these experiments is largely predicted by our theoretical results: Our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classiﬁcation, while observing in practice substantially fewer labels. 1</p><p>6 0.48750144 <a title="109-lda-6" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>7 0.42882538 <a title="109-lda-7" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>8 0.41454637 <a title="109-lda-8" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>9 0.40976545 <a title="109-lda-9" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>10 0.40340877 <a title="109-lda-10" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>11 0.40336722 <a title="109-lda-11" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>12 0.40105173 <a title="109-lda-12" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>13 0.40076536 <a title="109-lda-13" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>14 0.39874402 <a title="109-lda-14" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>15 0.39857835 <a title="109-lda-15" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>16 0.39753833 <a title="109-lda-16" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>17 0.39683419 <a title="109-lda-17" href="./nips-2004-Mistake_Bounds_for_Maximum_Entropy_Discrimination.html">119 nips-2004-Mistake Bounds for Maximum Entropy Discrimination</a></p>
<p>18 0.3967472 <a title="109-lda-18" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>19 0.39670718 <a title="109-lda-19" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>20 0.39669141 <a title="109-lda-20" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
