<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-129" href="#">nips2004-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</h1>
<br/><p>Source: <a title="nips-2004-129-pdf" href="http://papers.nips.cc/paper/2680-new-criteria-and-a-new-algorithm-for-learning-in-multi-agent-systems.pdf">pdf</a></p><p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><p>Reference: <a title="nips-2004-129-reference" href="../nips2004_reference/nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. [sent-5, score-0.187]
</p><p>2 We furthermore require that these average payoffs be achieved quickly. [sent-7, score-0.062]
</p><p>3 We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. [sent-8, score-0.421]
</p><p>4 Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. [sent-10, score-0.332]
</p><p>5 In the agent-centric agenda one asks how an agent can learn optimally in the presence of other independent agents, who may also be learning. [sent-15, score-0.158]
</p><p>6 To make the discussion precise we will concentrate on algorithms for learning in known, fully observable two-player repeated games, with average rewards. [sent-16, score-0.133]
</p><p>7 In a repeated game the stage game is repeated, ﬁnitely or inﬁnitely. [sent-18, score-0.554]
</p><p>8 In the following section we propose a stronger set of criteria that, we believe, does not suffer from these limitations. [sent-21, score-0.187]
</p><p>9 We then present an algorithm that provably meets these stronger requirements. [sent-22, score-0.124]
</p><p>10 However, we believe that all formal requirements – including our own – are merely baseline guarantees, and any proposed algorithm must be subjected to empirical tests. [sent-23, score-0.1]
</p><p>11 While many previous proposals provide empirical results, we think it is fair to say that our level of empirical validation is unprecedented in the literature. [sent-24, score-0.068]
</p><p>12 We show the results of tests for all pairwise comparisons of major existing algorithms, using a recently-developed game theoretic testbed called GAMUT [13] to systematically sample a very large space of games. [sent-25, score-0.336]
</p><p>13 2  Previous criteria for multi-agent learning  To our knowledge, Bowling and Veloso [1] were the ﬁrst in the AI community to explicitly put forth formal requirements. [sent-26, score-0.225]
</p><p>14 Speciﬁcally they proposed two criteria: Rationality: If the other players’ policies converge to stationary policies then the learning algorithm will converge to a stationary policy that is a best-response (in the stage game) to the other players’ policies. [sent-27, score-0.519]
</p><p>15 Convergence: The learner will necessarily converge to a stationary policy. [sent-28, score-0.169]
</p><p>16 Throughout this paper, we deﬁne a stationary policy as one that selects an action at each point during the game by drawing from the same distribution, regardless of past history. [sent-29, score-0.399]
</p><p>17 Bowling and Veloso considered known repeated games and proposed an algorithm that provably meets their criteria in 2x2 games (games with two players and two actions per player). [sent-30, score-0.901]
</p><p>18 Later, Conitzer and Sandholm [5] adopted the same criteria, and demonstrated an algorithm meeting the criteria for all repeated games. [sent-31, score-0.285]
</p><p>19 At ﬁrst glance these criteria are reasonable, but a deeper look is less satisfying. [sent-32, score-0.187]
</p><p>20 First, note that the property of convergence cannot be applied unconditionally, since one cannot ensure that a learning procedure converges against all possible opponents without sacriﬁcing rationality. [sent-33, score-0.382]
</p><p>21 So implicit in that requirement is some limitation on the class of opponents. [sent-34, score-0.053]
</p><p>22 And indeed both [1] and [5] acknowledge this and choose to concentrate on the case of self-play, that is, on opponents that are identical to the agent in question. [sent-35, score-0.573]
</p><p>23 Dare Dare  0, 0  4, 1  Y ield  1, 4  2, 2  Cooperate  Def ect  Cooperate  3, 3  0, 4  Def ect  4, 0  1, 1  Y ield  (a) Chicken  (b) Prisoner’s Dilemma  Figure 1: Example stage games. [sent-36, score-0.128]
</p><p>24 The payoff for the row player is given ﬁrst in each cell, with the payoff for the column player following. [sent-37, score-0.944]
</p><p>25 We will have more to say about self-play later, but there are other aspects of these criteria that bear discussion. [sent-38, score-0.187]
</p><p>26 While it is ﬁne to consider opponents playing stationary policies, there are other classes of opponents that might be as relevant or even more relevant; this should be a degree of freedom in the deﬁnition of the problem. [sent-39, score-0.997]
</p><p>27 For instance, one might be interested in the classes of opponents that can be modeled by ﬁnite automata with at most k states; these include both stationary and non-stationary strategies. [sent-40, score-0.52]
</p><p>28 We ﬁnd the property of requiring convergence to a stationary strategy particularly hard to justify. [sent-41, score-0.282]
</p><p>29 The Tit-for-Tat algorithm1 achieves an average payoff of 3 in self-play, while the unique Nash equilibrium of the stage game has a payoff of only 1. [sent-43, score-1.075]
</p><p>30 Similarly, in the game of Chicken, also shown in Figure 1, a strategy that alternates daring while its opponent yields and yielding while its opponent dares achieves a higher expected payoff in self-play than any stationary policy could guarantee. [sent-44, score-1.532]
</p><p>31 This problem is directly addressed in [2] and a counter-proposal made for how to consider equilibria in repeated games. [sent-45, score-0.1]
</p><p>32 But there is also a fundamental issue with these two criteria; they can both be thought of as a requirement on the play of the agent, rather than the reward the agent receives. [sent-46, score-0.359]
</p><p>33 Our ﬁnal point regarding these two criteria is that they express properties that hold in the limit, with no requirements whatsoever on the algorithm’s performance in any ﬁnite period. [sent-47, score-0.225]
</p><p>34 But this question is not new to the AI community and has been addressed numerous times in game theory, under the names of universal consistency, no-regret learning, and the Bayes envelope, dating back to [9] (see [6] for an overview of this history). [sent-48, score-0.264]
</p><p>35 There is a fundamental similarity in approach throughout, and we will take the two criteria proposed in [7] as being representative. [sent-49, score-0.187]
</p><p>36 Safety: The learning rule must guarantee at least the minimax payoff of the game. [sent-50, score-0.447]
</p><p>37 (The minimax payoff is the maximum expected value a player can guarantee against any possible opponent. [sent-51, score-0.523]
</p><p>38 ) Consistency: The learning rule must guarantee that it does at least as well as the best response to the empirical distribution of play when playing against an opponent whose play is governed by independent draws from any ﬁxed distribution. [sent-52, score-0.799]
</p><p>39 A limitation common to these game theory approaches is that they were designed for large-population games and therefore ignore the effect of the agent’s play on the future play of the opponent. [sent-54, score-0.627]
</p><p>40 Even if the opponent is playing Tit-for-Tat, the only universally consistent strategy would be to defect at every time step, ruling out the higher payoff achievable by cooperating. [sent-57, score-0.916]
</p><p>41 3  A new set of criteria for learning  We will try to take the best of each proposal and create a joint set of criteria with the potential to address some of the limitations mentioned above. [sent-58, score-0.417]
</p><p>42 As a possible motivation for our approach, consider the game of Rock-Paper-Scissors, which despite its simplicity has motivated several international tournaments. [sent-63, score-0.218]
</p><p>43 While the unique Nash equilibrium policy is to randomize, the winners of the tournaments are those players who can most effectively exploit their opponents who deviate without being exploited in turn. [sent-64, score-0.544]
</p><p>44 The question remains of how best to handle self-play. [sent-65, score-0.043]
</p><p>45 One method would be to require that a proposed algorithm be added to the set of opponents it is required to play a best response to. [sent-66, score-0.637]
</p><p>46 While this may seem appealing at ﬁrst glance, it can be a very weak requirement on the actual payoff the agent receives. [sent-67, score-0.576]
</p><p>47 Since our opponent is no longer independent of our choice of strategy, we can do better than settling for just any mutual best response, and try to maximize the value we achieve as well. [sent-68, score-0.386]
</p><p>48 We therefore propose requiring the algorithm achieve at least the value of some Nash equilibrium that is Pareto efﬁcient over the set of Nash equilibria. [sent-69, score-0.139]
</p><p>49 2 Similarly, algorithms exist that satisfy ‘universal consistency’ and if played by all agents will converge to a correlated equilibria[10], but this result provides an even weaker constraint on the actual payoff received than convergence to a Nash equilibrium. [sent-70, score-0.426]
</p><p>50 Let k be the number of outcomes for the game and b the maximum possible difference in payoffs across the outcomes. [sent-71, score-0.25]
</p><p>51 We require that for any choice of ǫ > 0 and δ > 0 there exist a T0 , polynomial in 1 , 1 , k, and b, such that for any number of rounds t > T0 the algorithm ǫ δ achieves the following payoff guarantees with probability at least 1 − δ. [sent-72, score-0.483]
</p><p>52 Targeted Optimality: When the opponent is a member of the selected set of opponents, the average payoff is at least VBR −ǫ, where VBR is the expected value of the best response in terms of average payoff against the actual opponent. [sent-73, score-1.26]
</p><p>53 Compatibility: During self-play, the average payoff is at least Vself P lay − ǫ, where Vself P lay is deﬁned as the minimum value achieved by the player in any Nash equilibrium that is not Pareto dominated by another Nash equilibrium. [sent-74, score-0.582]
</p><p>54 Safety: Against any opponent, the average payoff is at least Vsecurity − ǫ, with Vsecurity deﬁned as maxπ1 ∈Π1 minπ2 ∈Π2 EV (π1 , π2 ). [sent-75, score-0.426]
</p><p>55 3  4  An algorithm  While we feel designing algorithms for use against more complex classes of opponent is critical, as a minimal requirement we ﬁrst show an algorithm that meets the above criteria for the class of stationary opponents that has been the focus of much of the existing work. [sent-76, score-1.256]
</p><p>56 Our method incorporates modiﬁcations of three simple strategies: Fictitious Play [3], Bully [12], and the maximin strategy in order to create a more powerful hybrid algorithm. [sent-77, score-0.285]
</p><p>57 Fictitious Play has been shown to converge in the limit to the best response against a stationary opponent. [sent-78, score-0.296]
</p><p>58 Each round it plays its best response to the most likely stationary opponent given the history of play. [sent-79, score-0.697]
</p><p>59 Our implementation uses a somewhat more generous best-response calculation so as to achieve our performance requirements during self-play. [sent-80, score-0.069]
</p><p>60 BRǫ (π) ← arg max(EOV (x, π)), 4 x∈X(π,ǫ)  where X(π, ǫ) = {y ∈ Π1 : EV (y, π) ≥ max (EV (z, π)) − ǫ} z∈Π1  2  An outcome is Pareto efﬁcient over a set if there is no other outcome in that set with a payoff at least as high for every agent and strictly higher for at least one agent. [sent-81, score-0.641]
</p><p>61 3 Throughout the paper, we use EV (π1 , π2 ) to indicate the expected payoff to a player for playing strategy π1 against an opponent playing π2 and EOV (π1 , π2 ) as the expected payoff the opponent achieves. [sent-82, score-1.795]
</p><p>62 Π1 and Π2 are the sets of mixed strategies for the agent and its opponent respectively. [sent-83, score-0.559]
</p><p>63 4 Note that BR0 (π) is a member of the standard set of best response strategies to π. [sent-84, score-0.216]
</p><p>64 We extend the Bully algorithm to consider the full set of mixed strategies and again maximize our opponent’s value when multiple strategies yield equal payoff for our agent. [sent-85, score-0.571]
</p><p>65 BullyM ixed ← arg max(EOV (x, BR(x))), x∈X  where X = {y ∈ Π1 : EV (y, BR0 (y)) = max (EV (z, BR0 (z)))} z∈Π1  The maximin strategy is deﬁned as M aximin ← arg max min EV (π1 , π2 ) π1 ∈Π1  π2 ∈Π2  We will now show how to combine these strategies into a single method satisfying all three criteria. [sent-86, score-0.434]
</p><p>66 In the code shown below, t is the current round, AvgV aluem is the average value achieved by the agent during the last m rounds, VBully is shorthand for EV (BullyM ixed, BR0 (BullyM ixed)), and dt2 represents the distribution of opponent t1 actions for the period from round t1 to round t2 . [sent-87, score-0.708]
</p><p>67 At the end of this period it chooses one of three strategies for the rest of the game. [sent-89, score-0.089]
</p><p>68 If it determines its opponent may be stationary it settles on a best response to the history up until that point. [sent-90, score-0.577]
</p><p>69 Otherwise, if the BullyMixed strategy has been performing well it maintains it. [sent-91, score-0.144]
</p><p>70 This strategy changes each round, playing the best response to the maximum likelihood opponent strategy based on the last H rounds of play. [sent-93, score-0.881]
</p><p>71 Once one of these strategies has been selected, the algorithm plays according to it whenever the average value meets or exceeds the security level, reverting to the maximin strategy if the value drops too low. [sent-94, score-0.608]
</p><p>72 Theorem 1 Our algorithm satisﬁes the three properties stated in section 3 for the class of stationary opponents, with a T0 proportional to ( b )3 1 . [sent-95, score-0.166]
</p><p>73 ǫ δ This theorem can be proven for all three properties using a combination of basic probability theory and repeated applications of the Hoeffding inequality [11], but the proof itself is prohibitively long for inclusion in this publication. [sent-96, score-0.07]
</p><p>74 5  Empirical results  Although satisfying the criteria we put forth is comforting, we feel this is but a ﬁrst step in making a compelling argument that an approach might be useful in practice. [sent-97, score-0.225]
</p><p>75 Traditionally, researchers suggesting a new algorithm also include an empirical comparison of the algorithm to previous work. [sent-98, score-0.09]
</p><p>76 Combining this set of algorithms with a wide variety of repeated games from GAMUT [13], a game theoretic test suite, we have the beginnings of a comprehensive testbed for multi-agent learning algorithms. [sent-102, score-0.635]
</p><p>77 In the rest of this section, we’ll concentrate on the results for our algorithm, but we hope that this testbed can form the foundation for a broad, consistent framework of empirical testing in multi-agent learning going forward. [sent-103, score-0.119]
</p><p>78 For all of our environments we conducted our tests using a tournament format, where each algorithm plays all other algorithms including itself. [sent-104, score-0.095]
</p><p>79 100  -M ax  Jo in tQ  Hy pe r-Q  Lo ca lQ  St oc hF P St oc hI GA W oL FPH C  m et aS tra te gy  M  Ra nd o  Bu lly  M ini M ax Bu lly M i xe d  -  Figure 2: Average value for last 20K rounds (of 200K) across all games in GAMUT. [sent-110, score-0.408]
</p><p>80 Let us ﬁrst consider the results of a tournament over a full set of games in GAMUT. [sent-111, score-0.244]
</p><p>81 Figure 2 portrays the average value achieved by each agent (y-axis) averaged over all games, when playing different opponents (x-axis). [sent-112, score-0.665]
</p><p>82 Against the four stationary opponents, all of the adaptive learners fared equally well, while ﬁxed strategy players achieved poor rewards. [sent-115, score-0.392]
</p><p>83 In contrast, BullyMixed fared well against the adaptive algorithms. [sent-116, score-0.04]
</p><p>84 As desired, our new algorithm combined the best of these characteristics to achieve the highest value against all opponents except itself. [sent-117, score-0.484]
</p><p>85 It fares worse than BullyMixed since it will always yield to BullyMixed, giving away the more advantageous outcome in games like Chicken. [sent-118, score-0.237]
</p><p>86 However, when comparing how each agent performs in self-play, our algorithm scores quite well, ﬁnishing a close second to Hyper-Q learning while the  two Bully algorithms ﬁnish near last. [sent-119, score-0.186]
</p><p>87 Hyper-Q is able to gain in self-play by occasionally converging to outcomes with high social welfare that our strategy does not consider. [sent-120, score-0.144]
</p><p>88 The rewards were divided by the maximum reward achieved by any agent to make visual comparisons easier. [sent-122, score-0.25]
</p><p>89 So far we’ve seen that our new algorithm performs well when playing against a variety of opponents. [sent-123, score-0.123]
</p><p>90 In Figure 3 we show the reward for each agent, averaged across the set of possible opponents for a selection of games in GAMUT. [sent-124, score-0.639]
</p><p>91 Once again our algorithm outperforms the existing algorithms in nearly all games. [sent-125, score-0.06]
</p><p>92 When it fails to achieve the highest reward it often appears to be due to its policy of “generosity”; in games where it has multiple actions yielding equal value, it chooses a best response that maximizes its opponent’s value. [sent-126, score-0.49]
</p><p>93 The ability to study how individual strategies fare in each class of environment reﬂects an advantage of our more comprehensive testing approach. [sent-127, score-0.141]
</p><p>94 In future work, this data can be used both to aid in the selection of an appropriate algorithm for a new environment and to pinpoint areas where an algorithm might be improved. [sent-128, score-0.056]
</p><p>95 Note that we use environment here to indicate a combination of both the game and the distribution over opponents. [sent-129, score-0.218]
</p><p>96 6  Conclusions and Future Work  Our objective in this work was to put forth a new set of criteria for evaluating the performance of multi-agent learning algorithms as well as propose a more comprehensive method for empirical testing. [sent-130, score-0.311]
</p><p>97 In order to motivate this new approach for vetting algorithms, we have presented a novel algorithm that meets our criteria and outperforms existing algorithms in a wide variety of environments. [sent-131, score-0.343]
</p><p>98 In particular, we wish to demonstrate the generality of our approach by providing algorithms that calculate best response to different sets of opponents (conditional strategies, ﬁnite automata, etc. [sent-133, score-0.509]
</p><p>99 ) Additionally, the criteria need to be generalized for n-player games  and we hope to combine our method for known games with methods for learning the structure of the game, ultimately devising new algorithms for unknown stochastic games. [sent-134, score-0.605]
</p><p>100 The dynamics of reinforcement learning in cooperative multiagent systems. [sent-155, score-0.089]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('opponents', 0.382), ('payoff', 0.365), ('opponent', 0.312), ('bullymixed', 0.221), ('game', 0.218), ('games', 0.209), ('criteria', 0.187), ('agent', 0.158), ('strategy', 0.144), ('maximin', 0.141), ('stationary', 0.138), ('nash', 0.128), ('ev', 0.128), ('player', 0.107), ('play', 0.1), ('meets', 0.096), ('playing', 0.095), ('strategies', 0.089), ('multiagent', 0.089), ('round', 0.088), ('response', 0.084), ('avgv', 0.08), ('beststrategy', 0.08), ('bully', 0.08), ('fictitious', 0.08), ('veloso', 0.08), ('repeated', 0.07), ('players', 0.07), ('shoham', 0.07), ('safety', 0.064), ('bullym', 0.06), ('chicken', 0.06), ('eov', 0.06), ('ixed', 0.06), ('metastrategy', 0.06), ('prisoner', 0.06), ('vbully', 0.06), ('vsecurity', 0.06), ('br', 0.059), ('rounds', 0.059), ('requirement', 0.053), ('bowling', 0.052), ('pareto', 0.052), ('testbed', 0.052), ('dilemma', 0.052), ('gamut', 0.052), ('comprehensive', 0.052), ('minimax', 0.051), ('equilibrium', 0.049), ('reward', 0.048), ('security', 0.048), ('consistency', 0.048), ('stage', 0.048), ('universal', 0.046), ('powers', 0.045), ('cooperate', 0.045), ('rewards', 0.044), ('best', 0.043), ('policy', 0.043), ('stanford', 0.043), ('alueh', 0.04), ('conitzer', 0.04), ('dare', 0.04), ('fared', 0.04), ('fudenberg', 0.04), ('ield', 0.04), ('lly', 0.04), ('sandholm', 0.04), ('stochfp', 0.04), ('stochiga', 0.04), ('vbr', 0.04), ('vself', 0.04), ('forth', 0.038), ('requirements', 0.038), ('else', 0.035), ('tournament', 0.035), ('bestresponse', 0.035), ('bu', 0.035), ('theoretic', 0.034), ('empirical', 0.034), ('concentrate', 0.033), ('actions', 0.032), ('existing', 0.032), ('ctitious', 0.032), ('payoffs', 0.032), ('levine', 0.032), ('plays', 0.032), ('literature', 0.031), ('policies', 0.031), ('least', 0.031), ('achieve', 0.031), ('converge', 0.031), ('agents', 0.03), ('average', 0.03), ('equilibria', 0.03), ('hoeffding', 0.03), ('suite', 0.03), ('oc', 0.03), ('outcome', 0.028), ('algorithm', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="129-tfidf-1" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><p>2 0.26482069 <a title="129-tfidf-2" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>Author: Michael Bowling</p><p>Abstract: Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identiﬁable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). 1</p><p>3 0.15297577 <a title="129-tfidf-3" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>4 0.12399264 <a title="129-tfidf-4" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>Author: David C. Parkes, Dimah Yanovsky, Satinder P. Singh</p><p>Abstract: Online mechanism design (OMD) addresses the problem of sequential decision making in a stochastic environment with multiple self-interested agents. The goal in OMD is to make value-maximizing decisions despite this self-interest. In previous work we presented a Markov decision process (MDP)-based approach to OMD in large-scale problem domains. In practice the underlying MDP needed to solve OMD is too large and hence the mechanism must consider approximations. This raises the possibility that agents may be able to exploit the approximation for selﬁsh gain. We adopt sparse-sampling-based MDP algorithms to implement efﬁcient policies, and retain truth-revelation as an approximate BayesianNash equilibrium. Our approach is empirically illustrated in the context of the dynamic allocation of WiFi connectivity to users in a coffeehouse. 1</p><p>5 0.12016704 <a title="129-tfidf-5" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>Author: Eyal Even-dar, Sham M. Kakade, Yishay Mansour</p><p>Abstract: We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain ﬁxed. Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time. We provide efﬁcient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions. We also show that in the case that the dynamics change over time, the problem becomes computationally hard. 1</p><p>6 0.099534281 <a title="129-tfidf-6" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>7 0.080582827 <a title="129-tfidf-7" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>8 0.078203812 <a title="129-tfidf-8" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>9 0.075770877 <a title="129-tfidf-9" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>10 0.070520513 <a title="129-tfidf-10" href="./nips-2004-Solitaire%3A_Man_Versus_Machine.html">171 nips-2004-Solitaire: Man Versus Machine</a></p>
<p>11 0.068842039 <a title="129-tfidf-11" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>12 0.063104272 <a title="129-tfidf-12" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>13 0.045256983 <a title="129-tfidf-13" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>14 0.044766013 <a title="129-tfidf-14" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>15 0.04348205 <a title="129-tfidf-15" href="./nips-2004-VDCBPI%3A_an_Approximate_Scalable_Algorithm_for_Large_POMDPs.html">202 nips-2004-VDCBPI: an Approximate Scalable Algorithm for Large POMDPs</a></p>
<p>16 0.042048644 <a title="129-tfidf-16" href="./nips-2004-Stable_adaptive_control_with_online_learning.html">175 nips-2004-Stable adaptive control with online learning</a></p>
<p>17 0.040085331 <a title="129-tfidf-17" href="./nips-2004-Euclidean_Embedding_of_Co-Occurrence_Data.html">62 nips-2004-Euclidean Embedding of Co-Occurrence Data</a></p>
<p>18 0.036301676 <a title="129-tfidf-18" href="./nips-2004-Active_Learning_for_Anomaly_and_Rare-Category_Detection.html">15 nips-2004-Active Learning for Anomaly and Rare-Category Detection</a></p>
<p>19 0.035369948 <a title="129-tfidf-19" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<p>20 0.034898393 <a title="129-tfidf-20" href="./nips-2004-Hierarchical_Eigensolver_for_Transition_Matrices_in_Spectral_Methods.html">79 nips-2004-Hierarchical Eigensolver for Transition Matrices in Spectral Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.126), (1, -0.031), (2, 0.245), (3, -0.029), (4, -0.12), (5, 0.13), (6, -0.046), (7, -0.046), (8, -0.032), (9, 0.014), (10, -0.033), (11, 0.045), (12, -0.031), (13, -0.076), (14, -0.093), (15, 0.059), (16, 0.064), (17, 0.113), (18, 0.053), (19, -0.054), (20, -0.014), (21, -0.03), (22, 0.126), (23, -0.039), (24, 0.204), (25, -0.017), (26, -0.012), (27, -0.01), (28, -0.18), (29, -0.096), (30, -0.066), (31, 0.104), (32, 0.049), (33, 0.138), (34, 0.046), (35, 0.115), (36, -0.064), (37, 0.131), (38, -0.106), (39, -0.057), (40, -0.047), (41, -0.128), (42, -0.093), (43, 0.05), (44, -0.017), (45, -0.01), (46, 0.015), (47, 0.037), (48, 0.025), (49, -0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96315366 <a title="129-lsi-1" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><p>2 0.72672093 <a title="129-lsi-2" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>3 0.67104435 <a title="129-lsi-3" href="./nips-2004-Solitaire%3A_Man_Versus_Machine.html">171 nips-2004-Solitaire: Man Versus Machine</a></p>
<p>Author: Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy</p><p>Abstract: In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 1</p><p>4 0.65185702 <a title="129-lsi-4" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>Author: Michael Bowling</p><p>Abstract: Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identiﬁable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). 1</p><p>5 0.58170915 <a title="129-lsi-5" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>Author: David C. Parkes, Dimah Yanovsky, Satinder P. Singh</p><p>Abstract: Online mechanism design (OMD) addresses the problem of sequential decision making in a stochastic environment with multiple self-interested agents. The goal in OMD is to make value-maximizing decisions despite this self-interest. In previous work we presented a Markov decision process (MDP)-based approach to OMD in large-scale problem domains. In practice the underlying MDP needed to solve OMD is too large and hence the mechanism must consider approximations. This raises the possibility that agents may be able to exploit the approximation for selﬁsh gain. We adopt sparse-sampling-based MDP algorithms to implement efﬁcient policies, and retain truth-revelation as an approximate BayesianNash equilibrium. Our approach is empirically illustrated in the context of the dynamic allocation of WiFi connectivity to users in a coffeehouse. 1</p><p>6 0.42732045 <a title="129-lsi-6" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>7 0.41062671 <a title="129-lsi-7" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>8 0.39735016 <a title="129-lsi-8" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>9 0.35696155 <a title="129-lsi-9" href="./nips-2004-Economic_Properties_of_Social_Networks.html">57 nips-2004-Economic Properties of Social Networks</a></p>
<p>10 0.35160336 <a title="129-lsi-10" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>11 0.33478764 <a title="129-lsi-11" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>12 0.32962793 <a title="129-lsi-12" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>13 0.25298902 <a title="129-lsi-13" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>14 0.23895964 <a title="129-lsi-14" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>15 0.23332605 <a title="129-lsi-15" href="./nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</a></p>
<p>16 0.21740992 <a title="129-lsi-16" href="./nips-2004-The_Convergence_of_Contrastive_Divergences.html">185 nips-2004-The Convergence of Contrastive Divergences</a></p>
<p>17 0.21539144 <a title="129-lsi-17" href="./nips-2004-Theories_of_Access_Consciousness.html">193 nips-2004-Theories of Access Consciousness</a></p>
<p>18 0.21406586 <a title="129-lsi-18" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>19 0.21405876 <a title="129-lsi-19" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>20 0.21129654 <a title="129-lsi-20" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.055), (15, 0.101), (26, 0.056), (31, 0.023), (33, 0.184), (35, 0.019), (39, 0.012), (50, 0.034), (71, 0.015), (77, 0.019), (89, 0.384)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79794139 <a title="129-lda-1" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><p>2 0.73859978 <a title="129-lda-2" href="./nips-2004-A_Temporal_Kernel-Based_Model_for_Tracking_Hand_Movements_from_Neural_Activities.html">12 nips-2004-A Temporal Kernel-Based Model for Tracking Hand Movements from Neural Activities</a></p>
<p>Author: Lavi Shpigelman, Koby Crammer, Rony Paz, Eilon Vaadia, Yoram Singer</p><p>Abstract: We devise and experiment with a dynamical kernel-based system for tracking hand movements from neural activity. The state of the system corresponds to the hand location, velocity, and acceleration, while the system’s input are the instantaneous spike rates. The system’s state dynamics is deﬁned as a combination of a linear mapping from the previous estimated state and a kernel-based mapping tailored for modeling neural activities. In contrast to generative models, the activity-to-state mapping is learned using discriminative methods by minimizing a noise-robust loss function. We use this approach to predict hand trajectories on the basis of neural activity in motor cortex of behaving monkeys and ﬁnd that the proposed approach is more accurate than both a static approach based on support vector regression and the Kalman ﬁlter. 1</p><p>3 0.66143066 <a title="129-lda-3" href="./nips-2004-Using_the_Equivalent_Kernel_to_Understand_Gaussian_Process_Regression.html">201 nips-2004-Using the Equivalent Kernel to Understand Gaussian Process Regression</a></p>
<p>Author: Peter Sollich, Christopher Williams</p><p>Abstract: The equivalent kernel [1] is a way of understanding how Gaussian process regression works for large sample sizes based on a continuum limit. In this paper we show (1) how to approximate the equivalent kernel of the widely-used squared exponential (or Gaussian) kernel and related kernels, and (2) how analysis using the equivalent kernel helps to understand the learning curves for Gaussian processes. Consider the supervised regression problem for a dataset D with entries (xi , yi ) for i = 1, . . . , n. Under Gaussian Process (GP) assumptions the predictive mean at a test point x ∗ is given by ¯ f (x∗ ) = k (x∗ )(K + σ 2 I)−1 y, (1) where K denotes the n × n matrix of covariances between the training points with entries k(xi , xj ), k(x∗ ) is the vector of covariances k(xi , x∗ ), σ 2 is the noise variance on the observations and y is a n × 1 vector holding the training targets. See e.g. [2] for further details. ¯ We can deﬁne a vector of functions h(x∗ ) = (K + σ 2 I)−1 k(x∗ ) . Thus we have f (x∗ ) = h (x∗ )y, making it clear that the mean prediction at a point x∗ is a linear combination of the target values y. Gaussian process regression is thus a linear smoother, see [3, section 2.8] for further details. For a ﬁxed test point x∗ , h(x∗ ) gives the vector of weights applied to targets y. Silverman [1] called h (x∗ ) the weight function. Understanding the form of the weight function is made complicated by the matrix inversion of K + σ 2 I and the fact that K depends on the speciﬁc locations of the n datapoints. Idealizing the situation one can consider the observations to be “smeared out” in x-space at some constant density of observations. In this case analytic tools can be brought to bear on the problem, as shown below. By analogy to kernel smoothing Silverman [1] called the idealized weight function the equivalent kernel (EK). The structure of the remainder of the paper is as follows: In section 1 we describe how to derive the equivalent kernel in Fourier space. Section 2 derives approximations for the EK for the squared exponential and other kernels. In section 3 we show how use the EK approach to estimate learning curves for GP regression, and compare GP regression to kernel regression using the EK. 1 Gaussian Process Regression and the Equivalent Kernel It is well known (see e.g. [4]) that the posterior mean for GP regression can be obtained as the function which minimizes the functional J[f ] = 1 f 2 2 H + 1 2 2σn n (yi − f (xi ))2 , (2) i=1 where f H is the RKHS norm corresponding to kernel k. (However, note that the GP framework gives much more than just this mean prediction, for example the predictive variance and the marginal likelihood p(y) of the data under the model.) Let η(x) = E[y|x] be the target function for our regression problem and write E[(y − f (x))2 ] = E[(y − η(x))2 ] + (η(x) − f (x))2 . Using the fact that the ﬁrst term on the RHS is independent of f motivates considering a smoothed version of equation 2, Jρ [f ] = ρ 2σ 2 (η(x) − f (x))2 dx + 1 f 2 2 H, where ρ has dimensions of the number of observations per unit of x-space (length/area/volume etc. as appropriate). If we consider kernels that are stationary, k(x, x ) = k(x − x ), the natural basis in which to analyse equation 1 is the Fourier ˜ basis of complex sinusoids so that f (x) is represented as f (s)e2πis·x ds and similarly for η(x). Thus we obtain Jρ [f ] = 1 2 ˜ ρ ˜ |f (s)|2 |f (s) − η (s)|2 + ˜ 2 σ S(s) ds, ˜ as f 2 = |f (s)|2 /S(s)ds where S(s) is the power spectrum of the kernel k, H −2πis·x S(s) = k(x)e dx. Jρ [f ] can be minimized using calculus of variations to ob˜ tain f (s) = S(s)η(s)/(σ 2 /ρ + S(s)) which is recognized as the convolution f (x∗ ) = h(x∗ − x)η(x)dx. Here the Fourier transform of the equivalent kernel h(x) is ˜ h(s) = 1 S(s) = . S(s) + σ 2 /ρ 1 + σ 2 /(ρS(s)) (3) ˜ The term σ 2 /ρ in the ﬁrst expression for h(s) corresponds to the power spectrum of a white noise process, whose delta-function covariance function becomes a constant in the Fourier domain. This analysis is known as Wiener ﬁltering; see, e.g. [5, §14-1]. Notice that as ρ → ∞, h(x) tends to the delta function. If the input density is non-uniform the analysis above should be interpreted as computing the equivalent kernel for np(x) = ρ. This approximation will be valid if the scale of variation of p(x) is larger than the width of the equivalent kernel. 2 The EK for the Squared Exponential and Related Kernels For certain kernels/covariance functions the EK h(x) can be computed exactly by Fourier inversion. Examples include the Ornstein-Uhlenbeck process in D = 1 with covariance k(x) = e−α|x| (see [5, p. 326]), splines in D = 1 corresponding to the regularizer P f 2 = (f (m) )2 dx [1, 6], and the regularizer P f 2 = ( 2 f )2 dx in two dimensions, where the EK is given in terms of the Kelvin function kei [7]. We now consider the commonly used squared exponential (SE) kernel k(r) = exp(−r 2 /2 2 ), where r 2 = ||x−x ||2 . (This is sometimes called the Gaussian or radial basis function kernel.) Its Fourier transform is given by S(s) = (2π 2 )D/2 exp(−2π 2 2 |s|2 ), where D denotes the dimensionality of x (and s) space. From equation 3 we obtain ˜ hSE (s) = 1 , 1 + b exp(2π 2 2 |s|2 ) where b = σ 2 /ρ(2π 2 )D/2 . We are unaware of an exact result in this case, but the following initial approximation is simple but effective. For large ρ, b will be small. Thus for small ˜ s = |s| we have that hSE 1, but for large s it is approximately 0. The change takes place around the point sc where b exp(2π 2 2 s2 ) = 1, i.e. s2 = log(1/b)/2π 2 2 . As c c ˜ exp(2π 2 2 s2 ) grows quickly with s, the transition of hSE between 1 and 0 can be expected to be rapid, and thus be well-approximated by a step function. Proposition 1 The approximate form of the equivalent kernel for the squared-exponential kernel in D-dimensions is given by sc r hSE (r) = D/2 JD/2 (2πsc r). Proof: hSE (s) is a function of s = |s| only, and for D > 1 the Fourier integral can be simpliﬁed by changing to spherical polar coordinates and integrating out the angular variables to give ∞ hSE (r) = 2πr 0 sc 2πr 0 s r s r ν+1 ν+1 ˜ Jν (2πrs)hSE (s) ds Jν (2πrs) ds = sc r (4) D/2 JD/2 (2πsc r). where ν = D/2 − 1, Jν (z) is a Bessel function of the ﬁrst kind and we have used the identity z ν+1 Jν (z) = (d/dz)[z ν+1 Jν+1 (z)]. Note that in D = 1 by computing the Fourier transform of the boxcar function we obtain hSE (x) = 2sc sinc(2πsc x) where sinc(z) = sin(z)/z. This is consistent with Proposition 1 and J1/2 (z) = (2/πz)1/2 sin(z). The asymptotic form of the EK in D = 2 is shown in Figure 2(left) below. Notice that sc scales as (log(ρ))1/2 so that the width of the EK (which is proportional to 1/sc ) will decay very slowly as ρ increases. In contrast for a spline of order m (with power spectrum ∝ |s|−2m ) the width of the EK scales as ρ−1/2m [1]. If instead of RD we consider the input set to be the unit circle, a stationary kernel can be periodized by the construction kp (x, x ) = n∈Z k(x − x + 2nπ). This kernel will be represented as a Fourier series (rather than with a Fourier transform) because of the periodicity. In this case the step function in Fourier space approximation would give rise to a Dirichlet kernel as the EK (see [8, section 4.4.3] for further details on the Dirichlet kernel). We now show that the result of Proposition 1 is asymptotically exact for ρ → ∞, and calculate the leading corrections for ﬁnite ρ. The scaling of the width of the EK as 1/s c suggests writing hSE (r) = (2πsc )D g(2πsc r). Then from equation 4 and using the deﬁnition of sc z sc (2πsc )D ∞ u =z 2πz 0 ∞ g(z) = 0 ν+1 2πsc s z ν+1 Jν (zs/sc ) ds 1 + exp[2π 2 2 (s2 − s2 )] c Jν (zu) du 1 + exp[2π 2 2 s2 (u2 − 1)] c (5) where we have rescaled s = sc u in the second step. The value of sc , and hence ρ, now enters only in the exponential via a = 2π 2 2 s2 . For a → ∞, the exponential tends to zero c for u < 1 and to inﬁnity for u > 1. The factor 1/[1 + exp(. . .)] is therefore a step function Θ(1 − u) in the limit and Proposition 1 becomes exact, with g∞ (z) ≡ lima→∞ g(z) = (2πz)−D/2 JD/2 (z). To calculate corrections to this, one uses that for large but ﬁnite a the difference ∆(u) = {1 + exp[a(u2 − 1)]}−1 − Θ(1 − u) is non-negligible only in a range of order 1/a around u = 1. The other factors in the integrand of equation 5 can thus be Taylor-expanded around that point to give ∞ g(z) = g∞ (z) + z k=0 I k dk k! duk u 2πz ν+1 ∞ Jν (zu) , ∆(u)(u − 1)k du Ik = 0 u=1 The problem is thus reduced to calculating the integrals Ik . Setting u = 1 + v/a one has 0 ak+1 Ik = −a a = 0 1 − 1 v k dv + 1 + exp(v 2 /a + 2v) (−1)k+1 v k dv + 1 + exp(−v 2 /a + 2v) ∞ 0 ∞ 0 vk dv 1 + exp(v 2 /a + 2v) vk dv 1 + exp(v 2 /a + 2v) In the ﬁrst integral, extending the upper limit to ∞ gives an error that is exponentially small in a. Expanding the remaining 1/a-dependence of the integrand one then gets, to leading order in 1/a, I0 = c0 /a2 , I1 = c1 /a2 while all Ik with k ≥ 2 are smaller by at least 1/a2 . The numerical constants are −c0 = c1 = π 2 /24. This gives, using that (d/dz)[z ν+1 Jν (z)] = z ν Jν (z) + z ν+1 Jν−1 (z) = (2ν + 1)z ν Jν (z) − z ν+1 Jν+1 (z): Proposition 2 The equivalent kernel for the squared-exponential kernel is given for large ρ by hSE (r) = (2πsc )D g(2πsc r) with g(z) = 1 (2πz) D 2 JD/2 (z) + z (c0 + c1 (D − 1))JD/2−1 (z) − c1 zJD/2 (z) a2 +O( 1 ) a4 For e.g. D = 1 this becomes g(z) = π −1 {sin(z)/z − π 2 /(24a2 )[cos(z) + z sin(z)]}. Here and in general, by comparing the second part of the 1/a2 correction with the leading order term, one estimates that the correction is of relative size z 2 /a2 . It will therefore provide a useful improvement as long as z = 2πsc r < a; for larger z the expansion in powers of 1/a becomes a poor approximation because the correction terms (of all orders in 1/a) are comparable to the leading order. 2.1 Accuracy of the approximation To evaluate the accuracy of the approximation we can compute the EK numerically as follows: Consider a dense grid of points in RD with a sampling density ρgrid . For making 2 predictions at the grid points we obtain the smoother matrix K(K + σgrid I)−1 , where1 2 2 σgrid = σ ρgrid /ρ, as per equation 1. Each row of this matrix is an approximation to the EK at the appropriate location, as this is the response to a y vector which is zero at all points except one. Note that in theory one should use a grid over the whole of RD but in practice one can obtain an excellent approximation to the EK by only considering a grid around the point of interest as the EK typically decays with distance. Also, by only considering a ﬁnite grid one can understand how the EK is affected by edge effects. 2 To understand this scaling of σgrid consider the case where ρgrid > ρ which means that the effective variance at each of the ρgrid points per unit x-space is larger, but as there are correspondingly more points this effect cancels out. This can be understood by imagining the situation where there 2 are ρgrid /ρ independent Gaussian observations with variance σgrid at a single x-point; this would 2 be equivalent to one Gaussian observation with variance σ . In effect the ρ observations per unit x-space have been smoothed out uniformly. 1 0.16 0.35 0.35 Numerical Proposition 1 Proposition 2 0.3 0.25 0.14 0.2 0.2 0.15 0.12 0.15 0.1 0.1 0.05 0.1 0.05 0 0 −0.05 0.08 Numerical Proposition 1 Proposition 2 0.3 0.25 −0.05 −0.1 0 5 10 −0.1 0 15 5 10 15 0.06 0.04 0.02 0 −0.02 Numerical Proposition 1 Sample −0.04 −0.5 −0.4 −0.3 −0.2 −0.1 0 0.1 0.2 0.3 0.4 0.5 Figure 1: Main ﬁgure: plot of the weight function corresponding to ρ = 100 training points/unit length, plus the numerically computed equivalent kernel at x = 0.0 and the sinc approximation from Proposition 1. Insets: numerically evaluated g(z) together with sinc and Proposition 2 approximations for ρ = 100 (left) and ρ = 104 (right). Figure 1 shows plots of the weight function for ρ = 100, the EK computed on the grid as described above and the analytical sinc approximation. These are computed for parameter values of 2 = 0.004 and σ 2 = 0.1, with ρgrid /ρ = 5/3. To reduce edge effects, the interval [−3/2, 3/2] was used for computations, although only the centre of this is shown in the ﬁgure. There is quite good agreement between the numerical computation and the analytical approximation, although the sidelobes decay more rapidly for the numerically computed EK. This is not surprising because the absence of a truly hard cutoff in Fourier space means one should expect less “ringing” than the analytical approximation predicts. The ﬁgure also shows good agreement between the weight function (based on the ﬁnite sample) and the numerically computed EK. The insets show the approximation of Proposition 2 to g(z) for ρ = 100 (a = 5.67, left) and ρ = 104 (a = 9.67, right). As expected, the addition of the 1/a2 -correction gives better agreement with the numerical result for z < a. Numerical experiments also show that the mean squared error between the numerically computed EK and the sinc approximation decreases like 1/ log(ρ). The is larger than the na¨ve estimate (1/a2 )2 ∼ 1/(log(ρ))4 based on the ﬁrst correction term from Proposition ı 2, because the dominant part of the error comes from the region z > a where the 1/a expansion breaks down. 2.2 Other kernels Our analysis is not in fact restricted to the SE kernel. Consider an isotropic kernel, for which the power spectrum S(s) depends on s = |s| only. Then we can again deﬁne from equation 3 an effective cutoff sc on the range of s in the EK via σ 2 /ρ = S(sc ), so that ˜ h(s) = [1 + S(sc )/S(s)]−1 . The EK will then have the limiting form given in Proposi˜ tion 1 if h(s) approaches a step function Θ(sc − s), i.e. if it becomes inﬁnitely “steep” around the point s = sc for sc → ∞. A quantitative criterion for this is that the slope ˜ |h (sc )| should become much larger than 1/sc , the inverse of the range of the step func˜ tion. Since h (s) = S (s)S(sc )S −2 (s)[1 + S(sc )/S(s)]−2 , this is equivalent to requiring that −sc S (sc )/4S(sc ) ∝ −d log S(sc )/d log sc must diverge for sc → ∞. The result of Proposition 1 therefore applies to any kernel whose power spectrum S(s) decays more rapidly than any positive power of 1/s. A trivial example of a kernel obeying this condition would be a superposition of ﬁnitely many SE kernels with different lengthscales 2 ; the asymptotic behaviour of sc is then governed by the smallest . A less obvious case is the “rational quadratic” k(r) = [1 + (r/l)2 ]−(D+1)/2 which has an exponentially decaying power spectrum S(s) ∝ exp(−2π s). (This relationship is often used in the reverse direction, to obtain the power spectrum of the Ornstein-Uhlenbeck (OU) kernel exp(−r/ ).) Proposition 1 then applies, with the width of the EK now scaling as 1/sc ∝ 1/ log(ρ). The previous example is a special case of kernels which can be written as superpositions of SE kernels with a distribution p( ) of lengthscales , k(r) = exp(−r 2 /2 2 )p( ) d . This is in fact the most general representation for an isotropic kernel which deﬁnes a valid covariance function in any dimension D, see [9, §2.10]. Such a kernel has power spectrum ∞ S(s) = (2π)D/2 D exp(−2π 2 2 s2 )p( ) d (6) 0 and one easily veriﬁes that the rational quadratic kernel, which has S(s) ∝ exp(−2π 0 s), is obtained for p( ) ∝ −D−2 exp(− 2 /2 2 ). More generally, because the exponential 0 1/s D factor in equation 6 acts like a cutoff for > 1/s, one estimates S(s) ∼ 0 p( ) d for large s. This will decay more strongly than any power of 1/s for s → ∞ if p( ) itself decreases more strongly than any power of for → 0. Any such choice of p( ) will therefore yield a kernel to which Proposition 1 applies. 3 Understanding GP Learning Using the Equivalent Kernel We now turn to using EK analysis to get a handle on average case learning curves for Gaussian processes. Here the setup is that a function η is drawn from a Gaussian process, and we obtain ρ noisy observations of η per unit x-space at random x locations. We are concerned with the mean squared error (MSE) between the GP prediction f and η. Averaging over the noise process, the x-locations of the training data and the prior over η we obtain the average MSE as a function of ρ. See e.g. [10] and [11] for an overview of earlier work on GP learning curves. To understand the asymptotic behaviour of for large ρ, we now approximate the true GP predictions with the EK predictions from noisy data, given by fEK (x) = h(x − x )y(x )dx in the continuum limit of “smoothed out” input locations. We assume as before 2 that y = target + noise, i.e. y(x) = η(x) + ν(x) where E[ν(x)ν(x )] = (σ∗ /ρ)δ(x − x ). 2 Here σ∗ denotes the true noise variance, as opposed to the noise variance assumed in the 2 EK; the scaling of σ∗ with ρ is explained in footnote 1. For a ﬁxed target η, the MSE is = ( dx)−1 [η(x) − fEK (x)]2 dx. Averaging over the noise process ν and target function η gives in Fourier space 2 (σ 2 /ρ)Sη (s)/S 2 (s) + σ∗ /σ 2 ds [1 + σ 2 /(ρS(s))]2 (7) where Sη (s) is the power spectrum of the prior over target functions. In the case S(s) = 2 Sη (s) and σ 2 = σ∗ where the kernel is exactly matched to the structure of the target, equation 7 gives the Bayes error B and simpliﬁes to B = (σ 2 /ρ) [1 + σ 2 /(ρS(s))]−1 ds (see also [5, eq. 14-16]). Interestingly, this is just the analogue (for a continuous power spectrum of the kernel rather than a discrete set of eigenvalues) of the lower bound of [10] = σ2 2 ˜ ˜ Sη (s)[1 − h(s)]2 + (σ∗ /ρ)h2 (s) ds = ρ α=2 0.5 0.03 0.025 ε 0.02 0.015 0.01 α=4 0.1 0.005 0 −0.005 1 0.05 1 0.5 0.5 0 0 −0.5 −0.5 −1 25 −1 50 100 ρ 250 500 Figure 2: Left: plot of the asymptotic form of the EK (sc /r)J1 (2πsc r) for D = 2 and ρ = 1225. Right: log-log plot of against log(ρ) for the OU and Matern-class processes (α = 2, 4 respectively). The dashed lines have gradients of −1/2 and −3/2 which are the predicted rates. on the MSE of standard GP prediction from ﬁnite datasets. In experiments this bound provides a good approximation to the actual average MSE for large dataset size n [11]. This supports our approach of using the EK to understand the learning behaviour of GP regression. Treating the denominator in the expression for B again as a hard cutoff at s = sc , which is justiﬁed for large ρ, one obtains for an SE target and learner ≈ σ 2 sc /ρ ∝ (log(ρ))D/2 /ρ. To get analogous predictions for the mismatched case, one can write equation 7 as = 2 σ∗ ρ [1 + σ 2 /(ρS(s))] − σ 2 /(ρS(s)) ds + [1 + σ 2 /(ρS(s))]2 Sη (s) ds. [S(s)ρ/σ 2 + 1]2 2 The ﬁrst integral is smaller than (σ∗ /σ 2 ) B and can be neglected as long as B . In the second integral we can again make the cutoff approximation—though now with s having ∞ to be above sc – to get the scaling ∝ sc sD−1 Sη (s) ds. For target functions with a power-law decay Sη (s) ∝ s−α of the power spectrum at large s this predicts ∝ sD−α ∝ c (log(ρ))(D−α)/2 . So we generically get slow logarithmic learning, consistent with the observations in [12]. For D = 1 and an OU target (α = 2) we obtain ∼ (log(ρ)) −1/2 , and for the Matern-class covariance function k(r) = (1 + r/ ) exp(−r/ ) (which has power spectrum ∝ (3/ 2 + 4π 2 s2 )−2 , so α = 4) we get ∼ (log(ρ))−3/2 . These predictions were tested experimentally using a GP learner with SE covariance function ( = 0.1 and assumed noise level σ 2 = 0.1) against targets from the OU and Matern-class priors (with 2 = 0.05) and with noise level σ∗ = 0.01, averaging over 100 replications for each value of ρ. To demonstrate the predicted power-law dependence of on log(ρ), in Figure 2(right) we make a log-log plot of against log(ρ). The dashed lines show the gradients of −1/2 and −3/2 and we observe good agreement between experimental and theoretical results for large ρ. 3.1 Using the Equivalent Kernel in Kernel Regression Above we have used the EK to understand how standard GP regression works. One could alternatively envisage using the EK to perform kernel regression, on given ﬁnite data sets, producing a prediction ρ−1 i h(x∗ − xi )yi at x∗ . Intuitively this seems appealing as a cheap alternative to full GP regression, particularly for kernels such as the SE where the EK can be calculated analytically, at least to a good approximation. We now analyze brieﬂy how such an EK predictor would perform compared to standard GP prediction. Letting · denote averaging over noise, training input points and the test point and setting fη (x∗ ) = h(x, x∗ )η(x)dx, the average MSE of the EK predictor is pred = [η(x) − (1/ρ) i h(x, xi )yi ]2 = [η(x) − fη (x)]2 + = σ2 ρ 2 σ∗ ρ h2 (x, x )dx + 1 ρ h2 (x, x )η 2 (x )dx − 2 (σ 2 /ρ)Sη (s)/S 2 (s) + σ∗ /σ 2 η2 ds + 2 /(ρS(s))]2 [1 + σ ρ 1 ρ 2 fη (x) ds [1 + σ 2 /(ρS(s))]2 Here we have set η 2 = ( dx)−1 η 2 (x) dx = Sη (s) ds for the spatial average of the 2 squared target amplitude. Taking the matched case, (Sη (s) = S(s) and σ∗ = σ 2 ) as an example, the ﬁrst term (which is the one we get for the prediction from “smoothed out” training inputs, see eq. 7) is of order σ 2 sD /ρ, while the second one is ∼ η 2 sD /ρ. Thus c c both terms scale in the same way, but the ratio of the second term to the ﬁrst is the signal2 2 to-noise ratio η /σ , which in practice is often large. The EK predictor will then perform signiﬁcantly worse than standard GP prediction, by a roughly constant factor, and we have conﬁrmed this prediction numerically. This result is somewhat surprising given the good agreement between the weight function h(x∗ ) and the EK that we saw in ﬁgure 1, leading to the conclusion that the detailed structure of the weight function is important for optimal prediction from ﬁnite data sets. In summary, we have derived accurate approximations for the equivalent kernel (EK) of GP regression with the widely used squared exponential kernel, and have shown that the same analysis in fact extends to a whole class of kernels. We have also demonstrated that EKs provide a simple means of understanding the learning behaviour of GP regression, even in cases where the learner’s covariance function is not well matched to the structure of the target function. In future work, it will be interesting to explore in more detail the use of the EK in kernel smoothing. This is suboptimal compared to standard GP regression as we saw. However, it does remain feasible even for very large datasets, and may then be competitive with sparse methods for approximating GP regression. From the theoretical point of view, the average error of the EK predictor which we calculated may also provide the basis for useful upper bounds on GP learning curves. Acknowledgments: This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reﬂects the authors’ views. References [1] B. W. Silverman. Annals of Statistics, 12:898–916, 1984. [2] C. K. I. Williams. In M. I. Jordan, editor, Learning in Graphical Models, pages 599–621. Kluwer Academic, 1998. [3] T. J. Hastie and R. J. Tibshirani. Generalized Additive Models. Chapman and Hall, 1990. [4] F. Girosi, M. Jones, and T. Poggio. Neural Computation, 7(2):219–269, 1995. [5] A. Papoulis. Probability, Random Variables, and Stochastic Processes. McGraw-Hill, New York, 1991. Third Edition. [6] C. Thomas-Agnan. Numerical Algorithms, 13:21–32, 1996. [7] T. Poggio, H. Voorhees, and A. Yuille. Tech. Report AI Memo 833, MIT AI Laboratory, 1985. [8] B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, 2002. o [9] M. L. Stein. Interpolation of Spatial Data. Springer-Verlag, New York, 1999. [10] M. Opper and F. Vivarelli. In NIPS 11, pages 302–308, 1999. [11] P. Sollich and A. Halees. Neural Computation, 14:1393–1428, 2002. [12] P. Sollich. In NIPS 14, pages 519–526, 2002.</p><p>4 0.59364539 <a title="129-lda-4" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>Author: Michael Bowling</p><p>Abstract: Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identiﬁable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). 1</p><p>5 0.5181433 <a title="129-lda-5" href="./nips-2004-Dynamic_Bayesian_Networks_for_Brain-Computer_Interfaces.html">56 nips-2004-Dynamic Bayesian Networks for Brain-Computer Interfaces</a></p>
<p>Author: Pradeep Shenoy, Rajesh P. Rao</p><p>Abstract: We describe an approach to building brain-computer interfaces (BCI) based on graphical models for probabilistic inference and learning. We show how a dynamic Bayesian network (DBN) can be used to infer probability distributions over brain- and body-states during planning and execution of actions. The DBN is learned directly from observed data and allows measured signals such as EEG and EMG to be interpreted in terms of internal states such as intent to move, preparatory activity, and movement execution. Unlike traditional classiﬁcation-based approaches to BCI, the proposed approach (1) allows continuous tracking and prediction of internal states over time, and (2) generates control signals based on an entire probability distribution over states rather than binary yes/no decisions. We present preliminary results of brain- and body-state estimation using simultaneous EEG and EMG signals recorded during a self-paced left/right hand movement task. 1</p><p>6 0.50488925 <a title="129-lda-6" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>7 0.50335026 <a title="129-lda-7" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>8 0.50148606 <a title="129-lda-8" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>9 0.50119466 <a title="129-lda-9" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>10 0.50073504 <a title="129-lda-10" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>11 0.50052094 <a title="129-lda-11" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<p>12 0.5003323 <a title="129-lda-12" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>13 0.50016117 <a title="129-lda-13" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>14 0.50013483 <a title="129-lda-14" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>15 0.50011992 <a title="129-lda-15" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>16 0.49984837 <a title="129-lda-16" href="./nips-2004-An_Information_Maximization_Model_of_Eye_Movements.html">21 nips-2004-An Information Maximization Model of Eye Movements</a></p>
<p>17 0.49955964 <a title="129-lda-17" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>18 0.49936965 <a title="129-lda-18" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>19 0.49867749 <a title="129-lda-19" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>20 0.49863511 <a title="129-lda-20" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
