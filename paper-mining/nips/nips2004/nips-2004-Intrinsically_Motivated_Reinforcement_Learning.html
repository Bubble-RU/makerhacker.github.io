<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2004-Intrinsically Motivated Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-88" href="#">nips2004-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 nips-2004-Intrinsically Motivated Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2004-88-pdf" href="http://papers.nips.cc/paper/2552-intrinsically-motivated-reinforcement-learning.pdf">pdf</a></p><p>Author: Nuttapong Chentanez, Andrew G. Barto, Satinder P. Singh</p><p>Abstract: Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a speciﬁc problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 1</p><p>Reference: <a title="nips-2004-88-reference" href="../nips2004_reference/nips-2004-Intrinsically_Motivated_Reinforcement_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a speciﬁc problem of clear practical value. [sent-9, score-0.23]
</p><p>2 But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. [sent-10, score-0.337]
</p><p>3 In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. [sent-11, score-0.53]
</p><p>4 1  Introduction  Psychologists distinguish between extrinsic motivation, which means being moved to do something because of some speciﬁc rewarding outcome, and intrinsic motivation, which refers to being moved to do something because it is inherently enjoyable. [sent-12, score-0.483]
</p><p>5 The skills making up general competence act as the “building blocks” out of which an agent can form solutions to new problems as they arise. [sent-20, score-0.639]
</p><p>6 This paper presents an elaboration of the reinforcement learning (RL) framework [11] that encompasses the autonomous development of skill hierarchies through intrinsically motivated reinforcement learning. [sent-23, score-0.455]
</p><p>7 We illustrate its ability to allow an agent to learn broad competence in a simple “playroom” environment. [sent-24, score-0.41]
</p><p>8 Many researchers have argued for this kind of devel-  opmental approach in which an agent undergoes an extended developmental period during which collections of reusable skills are autonomously learned that will be useful for a wide range of later challenges (e. [sent-27, score-0.708]
</p><p>9 , [8]) on conﬁdence-based curiosity and the ideas of exploration and shaping bonuses [6, 10], although our deﬁnition of intrinsic reward differs from these. [sent-32, score-0.577]
</p><p>10 The neuromodulator dopamine has long been associated with reward learning [9]. [sent-34, score-0.386]
</p><p>11 Recent studies [2, 3] have focused on the idea that dopamine not only plays a critical role in the extrinsic motivational control of behaviors aimed at harvesting explicit rewards, but also in the intrinsic motivational control of behaviors associated with novelty and exploration. [sent-35, score-0.673]
</p><p>12 The agent learns to improve its skill in controlling the environment in the sense of learning how to increase the total amount of reward it receives over time from the critic. [sent-44, score-0.793]
</p><p>13 "Organism"  Sutton and Barto [11] point out that one should not identify this RL agent with an entire animal or robot. [sent-48, score-0.393]
</p><p>14 An an animal’s reward signals are determined by processes within its brain that monitor not only external state but also the animal’s internal state. [sent-49, score-0.402]
</p><p>15 1A into an external environment and an internal environment, the latter of which contains the critic which determines primary reward. [sent-53, score-0.214]
</p><p>16 This scheme still includes cases in which reward is essentially an external stimulus (e. [sent-54, score-0.337]
</p><p>17 The usual practice in applying RL algorithms is to formulate the problem one wants the agent to learn how to solve (e. [sent-58, score-0.377]
</p><p>18 , win at backgammon) and deﬁne a reward function specially tailored for this problem (e. [sent-60, score-0.31]
</p><p>19 Sometimes considerable ingenuity is required to craft an appropriate reward function. [sent-63, score-0.31]
</p><p>20 The point of departure for our approach is to note that the internal environment contains, among other things, the organism’s motivational system, which needs to be a sophisticated system that  should not have to be redesigned for different problems. [sent-64, score-0.213]
</p><p>21 Our approach to skills builds on the theory of options [12]. [sent-68, score-0.453]
</p><p>22 It consists of 1) an option policy that directs the agent’s behavior for a subset of the environment states, 2) an initiation set consisting of all the states in which the option can be initiated, and 3) a termination condition, which speciﬁes the conditions under which the option terminates. [sent-70, score-1.47]
</p><p>23 It is important to note that an option is not a sequence of actions; it is a closed-loop control rule, meaning that it is responsive to on-going state changes. [sent-71, score-0.418]
</p><p>24 Furthermore, because options can invoke other options as actions, hierarchical skills and algorithms for learning them naturally emerge from the conception of skills as options. [sent-72, score-0.931]
</p><p>25 Theoretically, when options are added to the set of admissible agent actions, the usual Markov decision process (MDP) formulation of RL extends to semi-Markov decision processes (SMDPs), with the one-step actions now becoming the “primitive actions. [sent-73, score-0.678]
</p><p>26 ” All of the theory and algorithms applicable to SMDPs can be appropriated for decision making and learning with options [12]. [sent-74, score-0.224]
</p><p>27 Two components of the the options framework are especially important for our approach: 1. [sent-75, score-0.224]
</p><p>28 Option Models: An option model is a probabilistic description of the effects of executing an option. [sent-76, score-0.384]
</p><p>29 As a function of an environment state where the option is initiated, it gives the probability with which the option will terminate at any other state, and it gives the total amount of reward expected over the option’s execution. [sent-77, score-1.198]
</p><p>30 Intra-option Learning Methods: These methods allow the policies of many options to be updated simultaneously during an agent’s interaction with the environment. [sent-81, score-0.276]
</p><p>31 If an option could have produced a primitive action in a given state, its policy can be updated on the basis of the observed consequences even though it was not directing the agent’s behavior at the time. [sent-82, score-0.653]
</p><p>32 In most of the work with options, the set of options must be provided by the system designer. [sent-83, score-0.224]
</p><p>33 While an option’s policy can be improved through learning, each option has to be predeﬁned by providing its initiation set, termination condition, and the reward function that evaluates its performance. [sent-84, score-0.856]
</p><p>34 For the most part, these methods extract options from the learning system’s attempts to solve a particular problem, whereas our approach creates options outside of the context of solving any particular problem. [sent-88, score-0.448]
</p><p>35 Developing Hierarchical Collections of Skills—Children accumulate skills while they engage in intrinsically motivated behavior, e. [sent-89, score-0.443]
</p><p>36 We claim that the concepts of an option and an option model are exactly appropriate to model this type of behavior. [sent-94, score-0.768]
</p><p>37 3  Intrinsically Motivated RL  Our main departure from the usual application of RL is that our agent maintains a knowledge base of skills that it learns using intrinsic rewards. [sent-96, score-0.828]
</p><p>38 to QB // — Choose next action //— Determine next extrinsic reward e Set rt+1 to the extrinsic reward for transition st , at → st+1 e e i i Set st ← st+1 ; at ← at+1 ; rt ← rt+1 ; rt ← rt+1  Figure 2: Learning Algorithm. [sent-99, score-1.622]
</p><p>39 Extrinsic reward is denoted re while intrinsic reward is denoted ri . [sent-100, score-0.819]
</p><p>40 The option action value functions Qo are updated using intra-option Q-learning. [sent-104, score-0.479]
</p><p>41 Note that the intrinsic reward is only used in updating QB and not any of the Qo . [sent-105, score-0.509]
</p><p>42 Behavior The agent behaves in its environment according to an -greedy policy with respect to an action-value function QB that is learned using a mix of Q-learning and SMDP planning as described in Fig. [sent-107, score-0.564]
</p><p>43 Over time, skills represented internally as options and their models also become available to the agent as action choices. [sent-110, score-0.89]
</p><p>44 Thus, QB maps states s and actions a (both primitive and options) to the expected long-term utility of taking that action a in state s. [sent-111, score-0.284]
</p><p>45 Salient Events In our current implementation we assume that the agent has intrinsic or hardwired notions of interesting or “salient” events in its environment. [sent-112, score-0.603]
</p><p>46 For example, in the playroom environment we present shortly, the agent ﬁnds changes in light and sound intensity to be salient. [sent-113, score-0.765]
</p><p>47 Reward In addition to the usual extrinsic rewards there are occasional intrinsic rewards generated by the agent’s critic (see Fig. [sent-115, score-0.621]
</p><p>48 In this implementation, the agent’s intrinsic reward is generated in a way suggested by the novelty response of dopamine neurons. [sent-117, score-0.616]
</p><p>49 The intrinsic reward for each salient event is proportional to the error in the prediction of the salient event according to the learned option model for that event (see Fig. [sent-118, score-1.725]
</p><p>50 Skill-KB The agent maintains a knowledge base of skills that it has learned in its environment. [sent-120, score-0.62]
</p><p>51 The ﬁrst time a salient event occurs, say light turned on, structures to learn an option that achieves that salient event (turn-light-on option) are created in the skill-KB. [sent-122, score-1.217]
</p><p>52 In addition, structures to learn an option model are also created. [sent-123, score-0.384]
</p><p>53 So for option o, Qo maps states s and actions a (again, both primitive and options) to the long-term utility of taking action a in state s. [sent-124, score-0.668]
</p><p>54 The option for a salient event terminates with probability one in any state that achieves that event and never terminates in any other state. [sent-125, score-0.859]
</p><p>55 The initiation set, I o , for an option o is incrementally expanded to includes states that lead to states in the current initiation set. [sent-126, score-0.594]
</p><p>56 In the playroom are a number of objects: a light switch, a ball, a bell, two movable blocks that are also buttons for turning music on and off, as well as a toy monkey that can make sounds. [sent-132, score-0.574]
</p><p>57 The agent has an eye, a hand, and a visual marker (seen as a cross hair in the ﬁgure). [sent-133, score-0.397]
</p><p>58 At any time step, the agent has the following actions available to it: 1) move eye to hand, 2) move eye to marker, 3) move eye one step north, south, east or west, 4) move eye to random object, 5) move hand to eye, and 6) move marker to eye. [sent-135, score-1.456]
</p><p>59 In addition, if both the eye and and hand are on some object, then natural operations suggested by the object become available, e. [sent-136, score-0.198]
</p><p>60 , if both the hand and the eye are on the light switch, then the action of ﬂicking the light switch becomes available, and if both the hand and eye are on the ball, then the action of kicking the ball becomes available (which when pushed, moves in a straight line to the marker). [sent-138, score-0.919]
</p><p>61 The light switch controls the lighting in the room. [sent-141, score-0.204]
</p><p>62 The blue block if pressed turns music on, while the red block if pressed turns music off. [sent-143, score-0.232]
</p><p>63 The toy monkey makes frightened sounds if simultaneously the room is dark and the music is on and the bell is rung. [sent-145, score-0.262]
</p><p>64 Notice that if the agent has already learned how to turn the light on and off, how to turn music on, and how to make the bell ring, then those learned skills would be of obvious use in simplifying this process of engaging the toy monkey. [sent-148, score-1.038]
</p><p>65 The effect of intrinsically motivated learning when extrinsic reward is present. [sent-157, score-0.674]
</p><p>66 See text for details  For this simple example, changes in light and sound intensity are considered salient by the playroom agent. [sent-158, score-0.584]
</p><p>67 Because the initial action value function, QB , is uninformative, the agent starts by exploring its environment randomly. [sent-159, score-0.5]
</p><p>68 Each ﬁrst encounter with a salient event initiates the learning of an option and an option model for that salient event. [sent-160, score-1.39]
</p><p>69 For example, the ﬁrst time the agent happens to turn the light on, it initiates the data structures necessary for learning and storing the light-on option. [sent-161, score-0.538]
</p><p>70 As the agent moves around the environment, all the options (initiated so far) and their models are simultaneously updated using intra-option learning. [sent-162, score-0.632]
</p><p>71 As a result, when the agent encounters an unpredicted salient event a few times, its updated action value function drives it to repeatedly attempt to achieve that salient event. [sent-165, score-1.102]
</p><p>72 Of course, the option policy and model become accurate in states the agent encounters frequently. [sent-167, score-0.874]
</p><p>73 Occasionally, the agent encounters the salient event in a state (set of sensor readings) that it has not encountered before, and it generates intrinsic reward again (it is “surprised”). [sent-168, score-1.27]
</p><p>74 Each panel of the ﬁgure is for a distinct salient event. [sent-171, score-0.251]
</p><p>75 The graph in each panel shows both the time steps at which the event occurs as well as the intrinsic reward associated by the agent to each occurrence. [sent-172, score-0.95]
</p><p>76 Each occurrence is denoted by a vertical bar whose height denotes the amount of associated intrinsic reward. [sent-173, score-0.199]
</p><p>77 Note that as one goes from top to bottom in this ﬁgure, the salient events become harder to achieve and, in fact, become more hierarchical. [sent-174, score-0.355]
</p><p>78 Indeed, the lowest one for turning on the monkey noise (Non) needs light on, music on, light off, sound on in sequence. [sent-175, score-0.545]
</p><p>79 First note that the salient events that are simpler to achieve occur earlier in time. [sent-177, score-0.309]
</p><p>80 For example, Lon (light turning on) and Loff (light turning off) are the simplest salient events, and the agent makes these happen quite early. [sent-178, score-0.713]
</p><p>81 The agent tries them a large number of times before getting bored and moving on to other salient events. [sent-179, score-0.641]
</p><p>82 The reward obtained for each of these events diminishes after repeated exposure to the event. [sent-180, score-0.391]
</p><p>83 Each panel depicts the occurrences of salient events as well as the associated intrinsic rewards. [sent-183, score-0.508]
</p><p>84 Of course, the events keep happening despite their diminished capacity to reward because they are needed to achieve the more complex events. [sent-185, score-0.368]
</p><p>85 Consequently, the agent continues to turn the light on and off even after it has learned this skill because this is a step along the way toward turning on the music, as well as along the way toward turning on the monkey noise. [sent-186, score-0.807]
</p><p>86 Finally note that the more complex skills are learned relatively quickly once the required sub-skills are in place, as one can see by the few rewards the agent receives for them. [sent-187, score-0.693]
</p><p>87 The agent is able to bootstrap and build upon the options it has already learned for the simpler events. [sent-188, score-0.615]
</p><p>88 We conﬁrmed the hierarchical nature of the learned options by inspecting the greedy policies for the more complex options like Non and Noff. [sent-189, score-0.518]
</p><p>89 The fact that all the options are successfully learned is also seen in Fig. [sent-190, score-0.269]
</p><p>90 This ﬁgure also shows that the simpler skills are learned earlier than the more complex ones. [sent-192, score-0.274]
</p><p>91 An agent having a collection of skills learned through intrinsic reward can learn a wide variety of extrinsically rewarded tasks more easily than an agent lacking these skills. [sent-193, score-1.475]
</p><p>92 To illustrate, we looked at a playroom task in which extrinsic reward was available only if the agent succeeded in making the monkey cry out. [sent-194, score-1.103]
</p><p>93 This is difﬁcult for an agent to learn if only the extrinsic reward is available, but much easier if the agent can use intrinsic reward to learn a collection of skills, some of which are relevant to the overall task. [sent-196, score-1.686]
</p><p>94 Each starts out with no knowledge of task, but one employs the intrinsic reward mechanism we have discussed above. [sent-199, score-0.509]
</p><p>95 The extrinsic reward is always available, but only when the monkey cries out. [sent-200, score-0.567]
</p><p>96 The ﬁgure, which shows the average of 100 repetitions of the experiment, clearly shows the advantage of learning with intrinsic reward. [sent-201, score-0.199]
</p><p>97 Discussion One of the key aspects of the Playroom example was that intrinsic reward was generated only by unexpected salient events. [sent-202, score-0.76]
</p><p>98 ” In the future, we intend to implement computational analogs of other forms of intrinsic motivation as suggested in the psychological, statistical, and neuroscience literatures. [sent-205, score-0.239]
</p><p>99 Moreover, they were achieved quite directly by combining a collection of existing RL algorithms for learning options and option-models with a simple notion of intrinsic reward. [sent-207, score-0.423]
</p><p>100 Policy invariance under reward transformations: Theory and application to reward shaping. [sent-251, score-0.62]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('option', 0.384), ('agent', 0.346), ('reward', 0.31), ('salient', 0.251), ('skills', 0.229), ('options', 0.224), ('st', 0.209), ('intrinsic', 0.199), ('extrinsic', 0.175), ('playroom', 0.161), ('eye', 0.153), ('light', 0.141), ('intrinsically', 0.126), ('rl', 0.126), ('qb', 0.119), ('event', 0.095), ('music', 0.092), ('qo', 0.089), ('oe', 0.088), ('environment', 0.086), ('rt', 0.083), ('monkey', 0.082), ('actions', 0.077), ('initiation', 0.076), ('primitive', 0.076), ('dopamine', 0.076), ('motivational', 0.073), ('rewards', 0.073), ('critic', 0.07), ('action', 0.068), ('competence', 0.064), ('motivated', 0.063), ('switch', 0.063), ('turning', 0.058), ('events', 0.058), ('move', 0.058), ('policy', 0.057), ('ball', 0.053), ('reinforcement', 0.052), ('autonomous', 0.052), ('skill', 0.051), ('marker', 0.051), ('bell', 0.048), ('barto', 0.047), ('animal', 0.047), ('learned', 0.045), ('bored', 0.044), ('curiosity', 0.044), ('maxa', 0.043), ('behavior', 0.041), ('something', 0.04), ('toy', 0.04), ('motivation', 0.04), ('ro', 0.036), ('moves', 0.035), ('encounters', 0.035), ('initiated', 0.035), ('reusable', 0.035), ('state', 0.034), ('usual', 0.031), ('sound', 0.031), ('internal', 0.031), ('novelty', 0.031), ('development', 0.03), ('planning', 0.03), ('states', 0.029), ('chentanez', 0.029), ('cry', 0.029), ('elaboration', 0.029), ('nuttapong', 0.029), ('rewarding', 0.029), ('unpredicted', 0.029), ('termination', 0.029), ('collections', 0.028), ('external', 0.027), ('stimuli', 0.027), ('updated', 0.027), ('sutton', 0.026), ('turn', 0.026), ('ccf', 0.025), ('ipto', 0.025), ('smdp', 0.025), ('smdps', 0.025), ('engage', 0.025), ('psychologists', 0.025), ('developmental', 0.025), ('competent', 0.025), ('initiates', 0.025), ('hierarchical', 0.025), ('singh', 0.025), ('interaction', 0.025), ('block', 0.024), ('bring', 0.024), ('exploration', 0.024), ('become', 0.023), ('departure', 0.023), ('behaviors', 0.023), ('diminishes', 0.023), ('pushed', 0.023), ('hand', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="88-tfidf-1" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>Author: Nuttapong Chentanez, Andrew G. Barto, Satinder P. Singh</p><p>Abstract: Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a speciﬁc problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 1</p><p>2 0.30861008 <a title="88-tfidf-2" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>Author: Eyal Even-dar, Sham M. Kakade, Yishay Mansour</p><p>Abstract: We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain ﬁxed. Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time. We provide efﬁcient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions. We also show that in the case that the dynamics change over time, the problem becomes computationally hard. 1</p><p>3 0.2376049 <a title="88-tfidf-3" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>Author: Pieter Abbeel, Andrew Y. Ng</p><p>Abstract: First-order Markov models have been successfully applied to many problems, for example in modeling sequential data using Markov chains, and modeling control problems using the Markov decision processes (MDP) formalism. If a ﬁrst-order Markov model’s parameters are estimated from data, the standard maximum likelihood estimator considers only the ﬁrst-order (single-step) transitions. But for many problems, the ﬁrstorder conditional independence assumptions are not satisﬁed, and as a result the higher order transition probabilities may be poorly approximated. Motivated by the problem of learning an MDP’s parameters for control, we propose an algorithm for learning a ﬁrst-order Markov model that explicitly takes into account higher order interactions during training. Our algorithm uses an optimization criterion different from maximum likelihood, and allows us to learn models that capture longer range effects, but without giving up the beneﬁts of using ﬁrst-order Markov models. Our experimental results also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problems where the MDP’s parameters are estimated from data. 1</p><p>4 0.18269548 <a title="88-tfidf-4" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>Author: David C. Parkes, Dimah Yanovsky, Satinder P. Singh</p><p>Abstract: Online mechanism design (OMD) addresses the problem of sequential decision making in a stochastic environment with multiple self-interested agents. The goal in OMD is to make value-maximizing decisions despite this self-interest. In previous work we presented a Markov decision process (MDP)-based approach to OMD in large-scale problem domains. In practice the underlying MDP needed to solve OMD is too large and hence the mechanism must consider approximations. This raises the possibility that agents may be able to exploit the approximation for selﬁsh gain. We adopt sparse-sampling-based MDP algorithms to implement efﬁcient policies, and retain truth-revelation as an approximate BayesianNash equilibrium. Our approach is empirically illustrated in the context of the dynamic allocation of WiFi connectivity to users in a coffeehouse. 1</p><p>5 0.1564883 <a title="88-tfidf-5" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>Author: Khashayar Rohanimanesh, Robert Platt, Sridhar Mahadevan, Roderic Grupen</p><p>Abstract: We investigate an approach for simultaneously committing to multiple activities, each modeled as a temporally extended action in a semi-Markov decision process (SMDP). For each activity we deﬁne a set of admissible solutions consisting of the redundant set of optimal policies, and those policies that ascend the optimal statevalue function associated with them. A plan is then generated by merging them in such a way that the solutions to the subordinate activities are realized in the set of admissible solutions satisfying the superior activities. We present our theoretical results and empirically evaluate our approach in a simulated domain. 1</p><p>6 0.15160221 <a title="88-tfidf-6" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>7 0.11871778 <a title="88-tfidf-7" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>8 0.11391422 <a title="88-tfidf-8" href="./nips-2004-Synergies_between_Intrinsic_and_Synaptic_Plasticity_in_Individual_Model_Neurons.html">181 nips-2004-Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons</a></p>
<p>9 0.099534281 <a title="88-tfidf-9" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>10 0.099185929 <a title="88-tfidf-10" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>11 0.094443254 <a title="88-tfidf-11" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>12 0.090040632 <a title="88-tfidf-12" href="./nips-2004-Responding_to_Modalities_with_Different_Latencies.html">155 nips-2004-Responding to Modalities with Different Latencies</a></p>
<p>13 0.084275037 <a title="88-tfidf-13" href="./nips-2004-Maximum_Likelihood_Estimation_of_Intrinsic_Dimension.html">114 nips-2004-Maximum Likelihood Estimation of Intrinsic Dimension</a></p>
<p>14 0.08275298 <a title="88-tfidf-14" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>15 0.077628583 <a title="88-tfidf-15" href="./nips-2004-Brain_Inspired_Reinforcement_Learning.html">33 nips-2004-Brain Inspired Reinforcement Learning</a></p>
<p>16 0.073309094 <a title="88-tfidf-16" href="./nips-2004-An_Information_Maximization_Model_of_Eye_Movements.html">21 nips-2004-An Information Maximization Model of Eye Movements</a></p>
<p>17 0.071138166 <a title="88-tfidf-17" href="./nips-2004-VDCBPI%3A_an_Approximate_Scalable_Algorithm_for_Large_POMDPs.html">202 nips-2004-VDCBPI: an Approximate Scalable Algorithm for Large POMDPs</a></p>
<p>18 0.063550867 <a title="88-tfidf-18" href="./nips-2004-Discriminant_Saliency_for_Visual_Recognition_from_Cluttered_Scenes.html">53 nips-2004-Discriminant Saliency for Visual Recognition from Cluttered Scenes</a></p>
<p>19 0.058288157 <a title="88-tfidf-19" href="./nips-2004-Temporal-Difference_Networks.html">183 nips-2004-Temporal-Difference Networks</a></p>
<p>20 0.049547307 <a title="88-tfidf-20" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.137), (1, -0.085), (2, 0.341), (3, -0.12), (4, -0.2), (5, 0.216), (6, -0.031), (7, -0.163), (8, -0.067), (9, 0.136), (10, 0.045), (11, 0.034), (12, 0.086), (13, -0.019), (14, 0.013), (15, 0.04), (16, -0.065), (17, -0.055), (18, 0.019), (19, 0.084), (20, 0.017), (21, -0.018), (22, 0.081), (23, -0.058), (24, -0.096), (25, -0.037), (26, 0.023), (27, 0.0), (28, -0.05), (29, -0.069), (30, -0.109), (31, 0.048), (32, 0.093), (33, -0.019), (34, 0.036), (35, -0.216), (36, 0.006), (37, -0.077), (38, 0.091), (39, -0.007), (40, -0.045), (41, -0.021), (42, -0.046), (43, -0.141), (44, 0.022), (45, -0.058), (46, 0.024), (47, -0.102), (48, -0.078), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98861969 <a title="88-lsi-1" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>Author: Nuttapong Chentanez, Andrew G. Barto, Satinder P. Singh</p><p>Abstract: Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a speciﬁc problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 1</p><p>2 0.71534729 <a title="88-lsi-2" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>Author: Eyal Even-dar, Sham M. Kakade, Yishay Mansour</p><p>Abstract: We consider an MDP setting in which the reward function is allowed to change during each time step of play (possibly in an adversarial manner), yet the dynamics remain ﬁxed. Similar to the experts setting, we address the question of how well can an agent do when compared to the reward achieved under the best stationary policy over time. We provide efﬁcient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions. We also show that in the case that the dynamics change over time, the problem becomes computationally hard. 1</p><p>3 0.68976802 <a title="88-lsi-3" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>Author: Pieter Abbeel, Andrew Y. Ng</p><p>Abstract: First-order Markov models have been successfully applied to many problems, for example in modeling sequential data using Markov chains, and modeling control problems using the Markov decision processes (MDP) formalism. If a ﬁrst-order Markov model’s parameters are estimated from data, the standard maximum likelihood estimator considers only the ﬁrst-order (single-step) transitions. But for many problems, the ﬁrstorder conditional independence assumptions are not satisﬁed, and as a result the higher order transition probabilities may be poorly approximated. Motivated by the problem of learning an MDP’s parameters for control, we propose an algorithm for learning a ﬁrst-order Markov model that explicitly takes into account higher order interactions during training. Our algorithm uses an optimization criterion different from maximum likelihood, and allows us to learn models that capture longer range effects, but without giving up the beneﬁts of using ﬁrst-order Markov models. Our experimental results also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problems where the MDP’s parameters are estimated from data. 1</p><p>4 0.55519634 <a title="88-lsi-4" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>Author: Khashayar Rohanimanesh, Robert Platt, Sridhar Mahadevan, Roderic Grupen</p><p>Abstract: We investigate an approach for simultaneously committing to multiple activities, each modeled as a temporally extended action in a semi-Markov decision process (SMDP). For each activity we deﬁne a set of admissible solutions consisting of the redundant set of optimal policies, and those policies that ascend the optimal statevalue function associated with them. A plan is then generated by merging them in such a way that the solutions to the subordinate activities are realized in the set of admissible solutions satisfying the superior activities. We present our theoretical results and empirically evaluate our approach in a simulated domain. 1</p><p>5 0.53487259 <a title="88-lsi-5" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>Author: Daniela D. Farias, Nimrod Megiddo</p><p>Abstract: A reactive environment is one that responds to the actions of an agent rather than evolving obliviously. In reactive environments, experts algorithms must balance exploration and exploitation of experts more carefully than in oblivious ones. In addition, a more subtle deﬁnition of a learnable value of an expert is required. A general exploration-exploitation experts method is presented along with a proper deﬁnition of value. The method is shown to asymptotically perform as well as the best available expert. Several variants are analyzed from the viewpoint of the exploration-exploitation tradeoff, including explore-then-exploit, polynomially vanishing exploration, constant-frequency exploration, and constant-size exploration phases. Complexity and performance bounds are proven. 1</p><p>6 0.50395644 <a title="88-lsi-6" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>7 0.49819598 <a title="88-lsi-7" href="./nips-2004-Resolving_Perceptual_Aliasing_In_The_Presence_Of_Noisy_Sensors.html">154 nips-2004-Resolving Perceptual Aliasing In The Presence Of Noisy Sensors</a></p>
<p>8 0.385822 <a title="88-lsi-8" href="./nips-2004-Responding_to_Modalities_with_Different_Latencies.html">155 nips-2004-Responding to Modalities with Different Latencies</a></p>
<p>9 0.37859163 <a title="88-lsi-9" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>10 0.36585838 <a title="88-lsi-10" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>11 0.32089946 <a title="88-lsi-11" href="./nips-2004-Synergies_between_Intrinsic_and_Synaptic_Plasticity_in_Individual_Model_Neurons.html">181 nips-2004-Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons</a></p>
<p>12 0.31721139 <a title="88-lsi-12" href="./nips-2004-An_Information_Maximization_Model_of_Eye_Movements.html">21 nips-2004-An Information Maximization Model of Eye Movements</a></p>
<p>13 0.30522871 <a title="88-lsi-13" href="./nips-2004-Schema_Learning%3A_Experience-Based_Construction_of_Predictive_Action_Models.html">159 nips-2004-Schema Learning: Experience-Based Construction of Predictive Action Models</a></p>
<p>14 0.30375627 <a title="88-lsi-14" href="./nips-2004-Temporal-Difference_Networks.html">183 nips-2004-Temporal-Difference Networks</a></p>
<p>15 0.29562965 <a title="88-lsi-15" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>16 0.28771305 <a title="88-lsi-16" href="./nips-2004-Discriminant_Saliency_for_Visual_Recognition_from_Cluttered_Scenes.html">53 nips-2004-Discriminant Saliency for Visual Recognition from Cluttered Scenes</a></p>
<p>17 0.27844098 <a title="88-lsi-17" href="./nips-2004-Brain_Inspired_Reinforcement_Learning.html">33 nips-2004-Brain Inspired Reinforcement Learning</a></p>
<p>18 0.27160656 <a title="88-lsi-18" href="./nips-2004-Maximum_Likelihood_Estimation_of_Intrinsic_Dimension.html">114 nips-2004-Maximum Likelihood Estimation of Intrinsic Dimension</a></p>
<p>19 0.26217312 <a title="88-lsi-19" href="./nips-2004-Planning_for_Markov_Decision_Processes_with_Sparse_Stochasticity.html">147 nips-2004-Planning for Markov Decision Processes with Sparse Stochasticity</a></p>
<p>20 0.25163904 <a title="88-lsi-20" href="./nips-2004-Online_Bounds_for_Bayesian_Algorithms.html">138 nips-2004-Online Bounds for Bayesian Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.094), (15, 0.097), (26, 0.045), (31, 0.022), (33, 0.171), (35, 0.023), (39, 0.012), (50, 0.029), (52, 0.01), (58, 0.362), (71, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84959996 <a title="88-lda-1" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>Author: Nuttapong Chentanez, Andrew G. Barto, Satinder P. Singh</p><p>Abstract: Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a speciﬁc problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efﬁciently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artiﬁcial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy. 1</p><p>2 0.75876522 <a title="88-lda-2" href="./nips-2004-Identifying_Protein-Protein_Interaction_Sites_on_a_Genome-Wide_Scale.html">80 nips-2004-Identifying Protein-Protein Interaction Sites on a Genome-Wide Scale</a></p>
<p>Author: Haidong Wang, Eran Segal, Asa Ben-Hur, Daphne Koller, Douglas L. Brutlag</p><p>Abstract: Protein interactions typically arise from a physical interaction of one or more small sites on the surface of the two proteins. Identifying these sites is very important for drug and protein design. In this paper, we propose a computational method based on probabilistic relational model that attempts to address this task using high-throughput protein interaction data and a set of short sequence motifs. We learn the model using the EM algorithm, with a branch-and-bound algorithm as an approximate inference for the E-step. Our method searches for motifs whose presence in a pair of interacting proteins can explain their observed interaction. It also tries to determine which motif pairs have high afﬁnity, and can therefore lead to an interaction. We show that our method is more accurate than others at predicting new protein-protein interactions. More importantly, by examining solved structures of protein complexes, we ﬁnd that 2/3 of the predicted active motifs correspond to actual interaction sites. 1</p><p>3 0.70758653 <a title="88-lda-3" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>Author: Mario Marchand, Mohak Shah</p><p>Abstract: We propose a “soft greedy” learning algorithm for building small conjunctions of simple threshold functions, called rays, deﬁned on single real-valued attributes. We also propose a PAC-Bayes risk bound which is minimized for classiﬁers achieving a non-trivial tradeoﬀ between sparsity (the number of rays used) and the magnitude of the separating margin of each ray. Finally, we test the soft greedy algorithm on four DNA micro-array data sets. 1</p><p>4 0.52649283 <a title="88-lda-4" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>Author: Pieter Abbeel, Andrew Y. Ng</p><p>Abstract: First-order Markov models have been successfully applied to many problems, for example in modeling sequential data using Markov chains, and modeling control problems using the Markov decision processes (MDP) formalism. If a ﬁrst-order Markov model’s parameters are estimated from data, the standard maximum likelihood estimator considers only the ﬁrst-order (single-step) transitions. But for many problems, the ﬁrstorder conditional independence assumptions are not satisﬁed, and as a result the higher order transition probabilities may be poorly approximated. Motivated by the problem of learning an MDP’s parameters for control, we propose an algorithm for learning a ﬁrst-order Markov model that explicitly takes into account higher order interactions during training. Our algorithm uses an optimization criterion different from maximum likelihood, and allows us to learn models that capture longer range effects, but without giving up the beneﬁts of using ﬁrst-order Markov models. Our experimental results also show the new algorithm outperforming conventional maximum likelihood estimation in a number of control problems where the MDP’s parameters are estimated from data. 1</p><p>5 0.52509075 <a title="88-lda-5" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We develop a family of upper and lower bounds on the worst-case expected KL loss for estimating a discrete distribution on a ﬁnite number m of points, given N i.i.d. samples. Our upper bounds are approximationtheoretic, similar to recent bounds for estimating discrete entropy; the lower bounds are Bayesian, based on averages of the KL loss under Dirichlet distributions. The upper bounds are convex in their parameters and thus can be minimized by descent methods to provide estimators with low worst-case error; the lower bounds are indexed by a one-dimensional parameter and are thus easily maximized. Asymptotic analysis of the bounds demonstrates the uniform KL-consistency of a wide class of estimators as c = N/m → ∞ (no matter how slowly), and shows that no estimator is consistent for c bounded (in contrast to entropy estimation). Moreover, the bounds are asymptotically tight as c → 0 or ∞, and are shown numerically to be tight within a factor of two for all c. Finally, in the sparse-data limit c → 0, we ﬁnd that the Dirichlet-Bayes (add-constant) estimator with parameter scaling like −c log(c) optimizes both the upper and lower bounds, suggesting an optimal choice of the “add-constant” parameter in this regime.</p><p>6 0.52458769 <a title="88-lda-6" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>7 0.52354729 <a title="88-lda-7" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>8 0.52292889 <a title="88-lda-8" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>9 0.52171254 <a title="88-lda-9" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>10 0.52107102 <a title="88-lda-10" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>11 0.52020437 <a title="88-lda-11" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>12 0.519853 <a title="88-lda-12" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>13 0.51984608 <a title="88-lda-13" href="./nips-2004-Message_Errors_in_Belief_Propagation.html">116 nips-2004-Message Errors in Belief Propagation</a></p>
<p>14 0.51899707 <a title="88-lda-14" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>15 0.5188818 <a title="88-lda-15" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>16 0.51875824 <a title="88-lda-16" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>17 0.5186857 <a title="88-lda-17" href="./nips-2004-Blind_One-microphone_Speech_Separation%3A_A_Spectral_Learning_Approach.html">31 nips-2004-Blind One-microphone Speech Separation: A Spectral Learning Approach</a></p>
<p>18 0.51809502 <a title="88-lda-18" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>19 0.51790196 <a title="88-lda-19" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>20 0.51761293 <a title="88-lda-20" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
