<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-71" href="#">nips2004-71</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</h1>
<br/><p>Source: <a title="nips-2004-71-pdf" href="http://papers.nips.cc/paper/2700-generalization-error-bounds-for-collaborative-prediction-with-low-rank-matrices.pdf">pdf</a></p><p>Author: Nathan Srebro, Noga Alon, Tommi S. Jaakkola</p><p>Abstract: We prove generalization error bounds for predicting entries in a partially observed matrix by ﬁtting the observed entries with a low-rank matrix. In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of ﬁnite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. 1</p><p>Reference: <a title="nips-2004-71-reference" href="../nips2004_reference/nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We prove generalization error bounds for predicting entries in a partially observed matrix by ﬁtting the observed entries with a low-rank matrix. [sent-8, score-0.851]
</p><p>2 In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of ﬁnite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. [sent-9, score-0.76]
</p><p>3 1  Introduction  “Collaborative ﬁltering” refers to the general task of providing users with information on what items they might like, or dislike, based on their preferences so far and how they relate to the preferences of other users. [sent-10, score-0.264]
</p><p>4 For feature-based approaches, we are accustomed to studying prediction methods in terms of probabilistic post-hoc generalization error bounds. [sent-12, score-0.273]
</p><p>5 Such results provide us a (probabilistic) bound on the performance of our predictor on future examples, in terms of its performance on the training data. [sent-13, score-0.164]
</p><p>6 These bounds hold without any assumptions on the true “model”, that is the true dependence of the labels on the features, other than the central assumptions that the training examples are drawn i. [sent-14, score-0.166]
</p><p>7 In this paper we suggest studying the generalization ability of collaborative prediction methods. [sent-18, score-0.344]
</p><p>8 By “collaborative prediction” we indicate that the objective is to be able to predict user preferences for items, that is, entries in some unknown target matrix Y of useritem “ratings”, based on observing a subset YS of the entries in this matrix1 . [sent-19, score-0.679]
</p><p>9 We present 1 In other collaborative ﬁltering tasks, the objective is to be able to provide each user with a few items that overlap his top-rated items, while it is not important to be able to correctly predict the users ratings for other items. [sent-20, score-0.313]
</p><p>10 Note that it is possible to derive generalization error bounds for this objective based on bounds for the “prediction” objective. [sent-21, score-0.542]
</p><p>11 What we do assume is that the subset S of entries that we observe is chosen uniformly at random. [sent-23, score-0.277]
</p><p>12 In particular, we present generalization error bounds on prediction using low-rank models. [sent-28, score-0.439]
</p><p>13 A low-rank matrix X is sought that minimizes the average observed error DS (X; Y ). [sent-30, score-0.126]
</p><p>14 Unobserved entries in Y are then predicted according to X. [sent-31, score-0.215]
</p><p>15 Different methods differ in how they relate real-valued entries in X to preferences in Y , and in the associated measure of discrepancy. [sent-33, score-0.309]
</p><p>16 For example, entries in X can be seen as parameters for a probabilistic models of the entries in Y , either mean parameters [1] or natural parameters [2], and a maximum likelihood criterion used. [sent-34, score-0.43]
</p><p>17 Or, other loss functions, such as squared error [3, 2], or zero-one loss versus the signs of entries in X, can be minimized. [sent-35, score-0.654]
</p><p>18 Prior Work Previous results bounding the error of collaborative prediction using a lowrank matrix all assume the true target matrix Y is well-approximated by a low-rank matrix. [sent-36, score-0.527]
</p><p>19 Drineas et al [4] analyze the sample complexity needed to be able to predict a matrix with an eigengap, and suggests strategies for actively querying entries in the target matrix. [sent-39, score-0.362]
</p><p>20 To our knowledge, this is the ﬁrst analysis of the generalization error of low-rank methods that do not make any assumptions on the true target matrix. [sent-40, score-0.239]
</p><p>21 Generalization error bounds (and related online learning bounds) were previously discussed for collaborative prediction applications, but only when prediction was done for each user separately, using a feature-based method, with the other user’s preferences as features [5, 6]. [sent-41, score-0.672]
</p><p>22 Although these address a collaborative prediction application, the learning setting is a standard feature-based setting. [sent-42, score-0.215]
</p><p>23 Shaw-Taylor et al [7] discuss assumption-free post-hoc bounds on the residual errors of low-rank approximation. [sent-44, score-0.194]
</p><p>24 These results apply to a different setting, where a subset of the rows are fully observed, and bound a different quantity—the distance between rows and the learned subspace, rather then the distance to predicted entries. [sent-45, score-0.262]
</p><p>25 Organization In Section 2 we present a generalization error bound for zero-one loss, based on a combinatorial result which we prove in Section 3. [sent-46, score-0.374]
</p><p>26 In Section 4 we generalize the bound to arbitrary loss functions. [sent-47, score-0.3]
</p><p>27 Finally, in Section 5 we justify the combinatorial  approach taken, by considering an alternate approach (viewing rank-k matrices as combination of k rank-1 matrices) and showing why it does not work. [sent-48, score-0.221]
</p><p>28 2  Generalization Error Bound for Zero-One Error  We begin by considering binary labels Yia ∈ ± and a zero-one sign agreement loss: loss± (Xia ; Yia ) = 1Yia Xia ≤0  (1)  Theorem 1. [sent-49, score-0.24]
</p><p>29 Noting that loss(Xia ; Yia ) depends only on the sign of Xia , it is enough to consider the equivalence classes of matrices with the same sign patterns. [sent-56, score-0.76]
</p><p>30 the number of possible sign conﬁgurations of n × m matrices of rank at most k: F (n, m, k) = {sign X ∈ {−, 0, +}n×m |X ∈ Rn×m , rank X ≤ k} f (n, m, k) = F (n, m, k) where sign X denotes the element-wise sign matrix (sign X)ia =  1 0 −1  If Xia > 0 If Xia = 0 . [sent-59, score-1.075]
</p><p>31 2  All logarithms are base two  3  Sign Conﬁgurations of a Low-Rank Matrix  In this section, we bound the number f (n, m, k) of sign conﬁgurations of n × m rankk matrices over the reals. [sent-63, score-0.673]
</p><p>32 Such a bound was previously considered in the context of unbounded error communication complexity. [sent-64, score-0.327]
</p><p>33 Here, we follow a general course outlined by Alon [9] to obtain a simpler, and slightly tighter, bound based on the following result due to Warren: Let P1 , . [sent-66, score-0.193]
</p><p>34 , Pr be real polynomials in q variables, and let C be the complement of the variety deﬁned by Πi Pi , i. [sent-69, score-0.217]
</p><p>35 the set of points in which all the m polynomials are non-zero: C = {x ∈ Rq |∀i Pi (x) = 0} Theorem 2 (Warren [10]). [sent-71, score-0.217]
</p><p>36 If all r polynomials are of degree at most d, then the number of connected components of C is at most: q  c(C) ≤ 2(2d)q  2i i=0  r i  4edr q  ≤  q  where the second inequality holds when r > q > 2. [sent-72, score-0.249]
</p><p>37 And so, c(C) bounds the number of sign conﬁgurations of P1 , . [sent-77, score-0.406]
</p><p>38 To bound the overall number of sign conﬁgurations the polynomials are modiﬁed slightly (see Appendix), yielding: Corollary 3 ([9, Proposition 5. [sent-81, score-0.621]
</p><p>39 The number of -/0/+ sign conﬁgurations of r polynomials, each of degree at most d, over q variables, is at most (8edr/q)q (for r > q > 2). [sent-83, score-0.272]
</p><p>40 In order to apply these bounds to low-rank matrices, recall that any matrix X of rank at most k can be written as a product X = U V where U ∈ Rn×k and V ∈ Rk×m . [sent-84, score-0.269]
</p><p>41 Consider the k(n+m) entries of U, V as variables, and the nm entries of X as polynomials of degree two over these variables: k  Uiα Vaα  Xia = α=1  Applying Corollary 3 we obtain: Lemma 4. [sent-85, score-0.741]
</p><p>42 f (n, m, k) ≤  8e·2·nm k(n+m)  k(n+m)  ≤ (16em/k)k(n+m)  Substituting this bound in (3) establishes Theorem 1. [sent-86, score-0.204]
</p><p>43 The upper bound on f (n, m, k) is tight up to a multiplicative factor in the exponent: 1  Lemma 5. [sent-87, score-0.164]
</p><p>44 Fix any matrix V ∈ Rm×k with rows in general position, and consider the number f (n, V, k) of sign conﬁgurations of matrices U V , where U varies over all n × k matrices. [sent-89, score-0.514]
</p><p>45 Focusing only on +/− sign conﬁgurations (no zeros in U V ), each row of sign U V is a homogeneous linear classiﬁcation of the rows of V , i. [sent-90, score-0.541]
</p><p>46 There are exactly 2 i=0 m possible homogeneous linear classiﬁcations of m i vectors in general position in Rk , and so these many options for each row of sign U V . [sent-93, score-0.266]
</p><p>47 We can therefore bound: n  k−1  f (n, m, k) ≥ f (n, V, k) ≥  m i  2 i=0  ≥  m n k−1  ≥  m k−1  n(k−1)  1  = m 2 (k−1)n  4  Generalization Error Bounds for Other Loss Functions  In Section 2 we considered generalization error bounds for a zero-one loss function. [sent-94, score-0.512]
</p><p>48 More commonly, though, other loss functions are used, and it is desirable to obtain generalization error bounds for general loss functions. [sent-95, score-0.736]
</p><p>49 When dealing with other loss functions, the magnitude of the entries in the matrix are important, and not only their signs. [sent-96, score-0.396]
</p><p>50 It is therefore no longer enough to bound the number of sign conﬁgurations. [sent-97, score-0.404]
</p><p>51 Instead, we will bound not only the number of ways low rank matrices behave with regards to a threshold of zero, but the number of possible ways lowrank matrices can behave relative to any set of thresholds. [sent-98, score-0.652]
</p><p>52 That is, for any threshold matrix T ∈ Rn×m , we will show that the number of possible sign conﬁgurations of (X − T ), where X is low-rank, is small. [sent-99, score-0.285]
</p><p>53 Intuitively, this captures the complexity of the class of low-rank matrices not only around zero, but throughout all possible values. [sent-100, score-0.306]
</p><p>54 We then use standard results from statistical machine learning to obtain generalization error bounds from the bound on the number of relative sign conﬁgurations. [sent-101, score-0.809]
</p><p>55 The number of relative sign conﬁgurations serves as a bound on the pseudodimension—the maximum number of entries for which there exists a set of thresholds such that all relative sign conﬁgurations (limited to these entries) is possible. [sent-102, score-0.906]
</p><p>56 The pseudodimension can in turn be used to show the existence of a small -net, which is used to obtain generalization error bounds. [sent-103, score-0.663]
</p><p>57 Recall the deﬁnition of the pseudodimension of a class of real-valued functions: Deﬁnition 1. [sent-104, score-0.491]
</p><p>58 A class F of real-valued functions pseudo-shatters the points x1 , . [sent-105, score-0.126]
</p><p>59 The pseudodimension of a class F is the supremum over n for which there exist n points and thresholds that can be shattered. [sent-117, score-0.538]
</p><p>60 In order to apply known results linking the pseudodimension to covering numbers, we consider matrices X ∈ Rn×m as real-valued functions X : [n] × [m] → R over index pairs to entries in the matrix. [sent-118, score-0.947]
</p><p>61 The class Xk of rank-k matrices can now be seen as a class of real-valued functions over the domain [n] × [m]. [sent-119, score-0.387]
</p><p>62 We bound the pseudodimension of this class by bounding, for any threshold matrix T ∈ Rn×m the number of relative sign matrices: FT (n, m, k) = {sign (X − T ) ∈ {−, 0, +}n×m |X ∈ Rn×m , rank X ≤ k} fT (n, m, k) = FT (n, m, k) Lemma 6. [sent-120, score-0.998]
</p><p>63 We take a similar approach to that of Lemma 4, writing rank-k matrices as a product X = U V where U ∈ Rn×k and V ∈ Rk×m . [sent-123, score-0.194]
</p><p>64 Consider the k(n + m) entries of U, V as variables, and the nm entries of X − T as polynomials of degree two over these variables: k  (X − T )ia =  Uiα Vaα − Tia α=1  Applying Corollary 10 yields the desired bound. [sent-124, score-0.741]
</p><p>65 The pseudodimension of the class Xk of n × m matrices over the reals of rank at most k, is at most k(n + m) log 16em . [sent-126, score-0.863]
</p><p>66 k We can now invoke standard generalization error bounds in terms of the pseudodimension (Theorem 11 in the Appendix) to obtain:  Theorem 8. [sent-127, score-0.8]
</p><p>67 If we view matrices as functions from pairs of indices to the reals, we can think of rank-k matrices as “combined” classiﬁers, and attempt to bound their complexity as such, based on the low complexity of the “basis” functions, i. [sent-129, score-0.701]
</p><p>68 dependent on the margin, or the slope of the loss function) generalization error bounds for this class are developed based on the graceful behavior of scale-sensitive complexity measures (e. [sent-135, score-0.624]
</p><p>69 log covering numbers and the Rademacher complexity) with respect to convex combinations. [sent-137, score-0.127]
</p><p>70 Taking a similar view, it is possible to obtain scale-sensitive generalization error bounds for low-rank matrices. [sent-138, score-0.405]
</p><p>71 In this Section we question whether it is possible to obtain scale-insensitive bounds, similar to Theorems 1 and 8, by viewing low-rank matrices as combined classiﬁers. [sent-139, score-0.268]
</p><p>72 It cannot be expected that scale-insensitive complexity would be preserved when taking convex combinations of an unbounded number of base functions. [sent-140, score-0.162]
</p><p>73 However, the VCdimension, a scale-insensitive measure of complexity, does scale gracefully when taking linear combinations of a bounded number of functions from a low VC-dimension class of indicator function. [sent-141, score-0.192]
</p><p>74 Using this, we can obtain generalization error bounds for linear combinations of signs of rank-one matrices, but not signs of linear combinations of rank-one matrices. [sent-142, score-0.649]
</p><p>75 An alternate candidate scale-insensitive complexity measure is the pseudodimension of a class of real-valued functions. [sent-143, score-0.563]
</p><p>76 If we could bound the pseudodimension of the class of sums of k functions from a bounded-pseudodimension base class of real valued functions, we could avoid the sign-conﬁguration counting and obtain generalization error bounds for rank-k matrices. [sent-144, score-1.24]
</p><p>77 There exists a family F closed under scalar multiplication whose pseudodimension is at most ﬁve, and such that {f1 + f2 |f1 , f2 ∈ F} does not have a ﬁnite pseudodimension. [sent-147, score-0.424]
</p><p>78 We describe a class F of real-valued functions over the positive integers N. [sent-149, score-0.172]
</p><p>79 For every A ⊂ N , fA − gA is the indicator function of A, implying that every ﬁnite subset can be shattered, and the pseudodimension of {f1 + f2 : f1 , f2 ∈ F} is unbounded. [sent-153, score-0.452]
</p><p>80 It remains to show that the pseudodimension of F is less than six. [sent-154, score-0.424]
</p><p>81 The three functions realizing these labellings must cross each other at least twice, but by the above arguments, there are no three functions in F such that every pair crosses each other at least twice. [sent-163, score-0.16]
</p><p>82 Milnor’s o and Warren’s theorems were previously used for bounding the VC-dimension of certain geometric classes [13], and of general concept classes parametrized by real numbers, in terms of the complexity of the boolean formulas over polynomials used to represent them [14]. [sent-165, score-0.463]
</p><p>83 This last general result can be used to bound the VC-dimension of signs of n×m rankk matrices by 2k(n + m) log(48enm), yielding a bound similar to Theorem 1 with an extra log |S| term. [sent-166, score-0.704]
</p><p>84 In this paper, we take a simpler path, applying Warren’s theorem directly, and thus avoiding the log |S| term and reducing the other logarithmic term. [sent-167, score-0.195]
</p><p>85 Applying Warren’s theorem directly also enables us to bound the pseudodimension and obtain the bound of Theorem 8 for general loss functions. [sent-168, score-1.031]
</p><p>86 Viewing signs of rank-k n × m matrices as n linear classiﬁcation of m points in Rk , this bound can be used to bound f (n, m, k) < 2km log 2n+k(k+1)n log n without using Warren’s Theorem directly [8, 12]. [sent-170, score-0.702]
</p><p>87 The bound of Lemma 4 avoids the quadratic dependence on k in the exponent. [sent-171, score-0.164]
</p><p>88 A  Proof of Corollary 3  Consider a set R ⊂ Rq containing one variable conﬁguration for each possible sign pattern. [sent-178, score-0.24]
</p><p>89 Now consider the 2q polynomials Pi+ (x) = Pi (x) + 2 and Pi− (x) = Pi (x) − and C = x ∈ Rq |∀i Pi+ (x) = 0, Pi− (x) = 0 . [sent-181, score-0.217]
</p><p>90 Different points in R (representing all sign conﬁgurations) lie in different connected components of C . [sent-182, score-0.24]
</p><p>91 The number of -/+ sign conﬁgurations (where zero is considered negative) of r polynomials, each of degree at most d, over q variables, is at most (4edr/q)q (for r > q > 2). [sent-187, score-0.272]
</p><p>92 Applying Corollary 10 on the nm degree-two polynomials Yia k Uiα Vaα establishes that for α=1 any Y , the number of conﬁgurations of sign agreements of rank-k matrices with Y is bounded by (8em/k)k(n+m) and yields a constant of 8 instead of 16 inside the logarithm in Theorem 1. [sent-188, score-0.783]
</p><p>93 Applying Corollary 10 instead of Corollary 3 allows us to similarly tighten in the bounds in Corollary 7 and in Theorem 8. [sent-189, score-0.166]
</p><p>94 4  A more careful analysis shows that F has pseudodimension three. [sent-190, score-0.424]
</p><p>95 Let F be a class of real-valued functions f : X → R with pseudodimension d, and loss : R × Y → R be a bounded monotone loss function (i. [sent-192, score-0.929]
</p><p>96 for all y, loss(x, y) is monotone in x), with loss < M . [sent-194, score-0.213]
</p><p>97 Then for any > 0: Pr ∃f ∈F EX,Y [loss(f (X), Y )] > S  1 n  n  loss(f (Xi ), Yi ) +  < 4e(d + 1)  32eM  d  e−  2n 32  i=1  The bound is a composition of a generalization error bound in terms of the L1 covering number [17, Theorem 17. [sent-202, score-0.623]
</p><p>98 1], a bound on the L1 covering number in terms of the pseudodimension [18] and the observation that composition with a monotone function does not increase the pseudodimension [17, Theorem 12. [sent-203, score-1.174]
</p><p>99 A theoretical analysis of query selection for collaborative ﬁltering. [sent-229, score-0.152]
</p><p>100 Upper bounds for conﬁgurations and polytopes in Rd . [sent-266, score-0.215]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pseudodimension', 0.424), ('xia', 0.252), ('sign', 0.24), ('polynomials', 0.217), ('entries', 0.215), ('matrices', 0.194), ('gurations', 0.19), ('warren', 0.171), ('yia', 0.17), ('bounds', 0.166), ('bound', 0.164), ('collaborative', 0.152), ('corollary', 0.144), ('loss', 0.136), ('generalization', 0.129), ('theorem', 0.114), ('fa', 0.102), ('pi', 0.1), ('preferences', 0.094), ('alon', 0.09), ('signs', 0.086), ('nathan', 0.085), ('rn', 0.082), ('error', 0.081), ('monotone', 0.077), ('rq', 0.077), ('tommi', 0.077), ('con', 0.077), ('ga', 0.074), ('frankl', 0.073), ('milnor', 0.073), ('noga', 0.073), ('reals', 0.073), ('bounding', 0.07), ('pr', 0.067), ('class', 0.067), ('rk', 0.065), ('gb', 0.065), ('srebro', 0.064), ('prediction', 0.063), ('ds', 0.062), ('nm', 0.062), ('ft', 0.059), ('functions', 0.059), ('fb', 0.058), ('rank', 0.058), ('unbounded', 0.055), ('covering', 0.055), ('user', 0.053), ('va', 0.051), ('discrepancy', 0.051), ('items', 0.049), ('aviv', 0.049), ('azar', 0.049), ('drineas', 0.049), ('eigengap', 0.049), ('polytopes', 0.049), ('rankk', 0.049), ('tia', 0.049), ('ia', 0.049), ('equivalence', 0.048), ('log', 0.047), ('thresholds', 0.047), ('integers', 0.046), ('matrix', 0.045), ('complexity', 0.045), ('viewing', 0.045), ('labellings', 0.042), ('invoking', 0.042), ('lowrank', 0.042), ('lemma', 0.041), ('establishes', 0.04), ('classes', 0.038), ('combinations', 0.036), ('rows', 0.035), ('uniformly', 0.034), ('applying', 0.034), ('integer', 0.034), ('ratings', 0.032), ('dl', 0.032), ('arguments', 0.032), ('degree', 0.032), ('bounded', 0.03), ('ui', 0.03), ('composition', 0.03), ('obtain', 0.029), ('variables', 0.029), ('target', 0.029), ('al', 0.028), ('subset', 0.028), ('counting', 0.028), ('boolean', 0.028), ('subsets', 0.027), ('communication', 0.027), ('theorems', 0.027), ('alternate', 0.027), ('users', 0.027), ('homogeneous', 0.026), ('base', 0.026), ('numbers', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="71-tfidf-1" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>Author: Nathan Srebro, Noga Alon, Tommi S. Jaakkola</p><p>Abstract: We prove generalization error bounds for predicting entries in a partially observed matrix by ﬁtting the observed entries with a low-rank matrix. In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of ﬁnite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. 1</p><p>2 0.34346199 <a title="71-tfidf-2" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>Author: Nathan Srebro, Jason Rennie, Tommi S. Jaakkola</p><p>Abstract: We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-deﬁnite program, and discuss generalization error bounds for them. 1</p><p>3 0.13095026 <a title="71-tfidf-3" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We develop a family of upper and lower bounds on the worst-case expected KL loss for estimating a discrete distribution on a ﬁnite number m of points, given N i.i.d. samples. Our upper bounds are approximationtheoretic, similar to recent bounds for estimating discrete entropy; the lower bounds are Bayesian, based on averages of the KL loss under Dirichlet distributions. The upper bounds are convex in their parameters and thus can be minimized by descent methods to provide estimators with low worst-case error; the lower bounds are indexed by a one-dimensional parameter and are thus easily maximized. Asymptotic analysis of the bounds demonstrates the uniform KL-consistency of a wide class of estimators as c = N/m → ∞ (no matter how slowly), and shows that no estimator is consistent for c bounded (in contrast to entropy estimation). Moreover, the bounds are asymptotically tight as c → 0 or ∞, and are shown numerically to be tight within a factor of two for all c. Finally, in the sparse-data limit c → 0, we ﬁnd that the Dirichlet-Bayes (add-constant) estimator with parameter scaling like −c log(c) optimizes both the upper and lower bounds, suggesting an optimal choice of the “add-constant” parameter in this regime.</p><p>4 0.10177311 <a title="71-tfidf-4" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>Author: Balázs Kégl</p><p>Abstract: We have recently proposed an extension of A DA B OOST to regression that uses the median of the base regressors as the ﬁnal regressor. In this paper we extend theoretical results obtained for A DA B OOST to median boosting and to its localized variant. First, we extend recent results on efﬁcient margin maximizing to show that the algorithm can converge to the maximum achievable margin within a preset precision in a ﬁnite number of steps. Then we provide conﬁdence-interval-type bounds on the generalization error. 1</p><p>5 0.098818287 <a title="71-tfidf-5" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>Author: Anton Schwaighofer, Volker Tresp, Kai Yu</p><p>Abstract: We present a novel method for learning with Gaussian process regression in a hierarchical Bayesian framework. In a ﬁrst step, kernel matrices on a ﬁxed set of input points are learned from data using a simple and efﬁcient EM algorithm. This step is nonparametric, in that it does not require a parametric form of covariance function. In a second step, kernel functions are ﬁtted to approximate the learned covariance matrix using a generalized Nystr¨ m method, which results in a complex, data o driven kernel. We evaluate our approach as a recommendation engine for art images, where the proposed hierarchical Bayesian method leads to excellent prediction performance. 1</p><p>6 0.092317268 <a title="71-tfidf-6" href="./nips-2004-Exponential_Family_Harmoniums_with_an_Application_to_Information_Retrieval.html">66 nips-2004-Exponential Family Harmoniums with an Application to Information Retrieval</a></p>
<p>7 0.090571873 <a title="71-tfidf-7" href="./nips-2004-Co-Training_and_Expansion%3A_Towards_Bridging_Theory_and_Practice.html">37 nips-2004-Co-Training and Expansion: Towards Bridging Theory and Practice</a></p>
<p>8 0.09027651 <a title="71-tfidf-8" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>9 0.087781735 <a title="71-tfidf-9" href="./nips-2004-Online_Bounds_for_Bayesian_Algorithms.html">138 nips-2004-Online Bounds for Bayesian Algorithms</a></p>
<p>10 0.081643656 <a title="71-tfidf-10" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>11 0.080194779 <a title="71-tfidf-11" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>12 0.078674093 <a title="71-tfidf-12" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>13 0.075400755 <a title="71-tfidf-13" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>14 0.074463479 <a title="71-tfidf-14" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>15 0.074068159 <a title="71-tfidf-15" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>16 0.064672567 <a title="71-tfidf-16" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<p>17 0.063416503 <a title="71-tfidf-17" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>18 0.061982144 <a title="71-tfidf-18" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>19 0.060501508 <a title="71-tfidf-19" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>20 0.0599172 <a title="71-tfidf-20" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.21), (1, 0.087), (2, 0.067), (3, 0.113), (4, -0.012), (5, -0.028), (6, -0.028), (7, 0.105), (8, 0.183), (9, 0.048), (10, -0.069), (11, -0.103), (12, 0.204), (13, 0.174), (14, 0.095), (15, -0.042), (16, 0.004), (17, 0.237), (18, -0.225), (19, -0.039), (20, 0.163), (21, 0.2), (22, -0.022), (23, -0.052), (24, 0.007), (25, -0.014), (26, 0.021), (27, 0.126), (28, 0.073), (29, 0.062), (30, -0.021), (31, -0.057), (32, 0.191), (33, 0.017), (34, 0.152), (35, 0.032), (36, -0.065), (37, 0.058), (38, 0.001), (39, -0.119), (40, 0.037), (41, 0.083), (42, 0.097), (43, -0.005), (44, -0.038), (45, 0.085), (46, -0.028), (47, -0.007), (48, -0.005), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96789384 <a title="71-lsi-1" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>Author: Nathan Srebro, Noga Alon, Tommi S. Jaakkola</p><p>Abstract: We prove generalization error bounds for predicting entries in a partially observed matrix by ﬁtting the observed entries with a low-rank matrix. In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of ﬁnite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. 1</p><p>2 0.88224179 <a title="71-lsi-2" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>Author: Nathan Srebro, Jason Rennie, Tommi S. Jaakkola</p><p>Abstract: We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-deﬁnite program, and discuss generalization error bounds for them. 1</p><p>3 0.42886215 <a title="71-lsi-3" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>Author: Liam Paninski</p><p>Abstract: We develop a family of upper and lower bounds on the worst-case expected KL loss for estimating a discrete distribution on a ﬁnite number m of points, given N i.i.d. samples. Our upper bounds are approximationtheoretic, similar to recent bounds for estimating discrete entropy; the lower bounds are Bayesian, based on averages of the KL loss under Dirichlet distributions. The upper bounds are convex in their parameters and thus can be minimized by descent methods to provide estimators with low worst-case error; the lower bounds are indexed by a one-dimensional parameter and are thus easily maximized. Asymptotic analysis of the bounds demonstrates the uniform KL-consistency of a wide class of estimators as c = N/m → ∞ (no matter how slowly), and shows that no estimator is consistent for c bounded (in contrast to entropy estimation). Moreover, the bounds are asymptotically tight as c → 0 or ∞, and are shown numerically to be tight within a factor of two for all c. Finally, in the sparse-data limit c → 0, we ﬁnd that the Dirichlet-Bayes (add-constant) estimator with parameter scaling like −c log(c) optimizes both the upper and lower bounds, suggesting an optimal choice of the “add-constant” parameter in this regime.</p><p>4 0.41997418 <a title="71-lsi-4" href="./nips-2004-Exponential_Family_Harmoniums_with_an_Application_to_Information_Retrieval.html">66 nips-2004-Exponential Family Harmoniums with an Application to Information Retrieval</a></p>
<p>Author: Max Welling, Michal Rosen-zvi, Geoffrey E. Hinton</p><p>Abstract: Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research ﬁelds. Although this approach has met with considerable success, the causal semantics of these models can make it difﬁcult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these “exponential family harmoniums” is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.</p><p>5 0.39746898 <a title="71-lsi-5" href="./nips-2004-Online_Bounds_for_Bayesian_Algorithms.html">138 nips-2004-Online Bounds for Bayesian Algorithms</a></p>
<p>Author: Sham M. Kakade, Andrew Y. Ng</p><p>Abstract: We present a competitive analysis of Bayesian learning algorithms in the online learning setting and show that many simple Bayesian algorithms (such as Gaussian linear regression and Bayesian logistic regression) perform favorably when compared, in retrospect, to the single best model in the model class. The analysis does not assume that the Bayesian algorithms’ modeling assumptions are “correct,” and our bounds hold even if the data is adversarially chosen. For Gaussian linear regression (using logloss), our error bounds are comparable to the best bounds in the online learning literature, and we also provide a lower bound showing that Gaussian linear regression is optimal in a certain worst case sense. We also give bounds for some widely used maximum a posteriori (MAP) estimation algorithms, including regularized logistic regression. 1</p><p>6 0.38254595 <a title="71-lsi-6" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>7 0.36213434 <a title="71-lsi-7" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>8 0.35282096 <a title="71-lsi-8" href="./nips-2004-Co-Validation%3A_Using_Model_Disagreement_on_Unlabeled_Data_to_Validate_Classification_Algorithms.html">38 nips-2004-Co-Validation: Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms</a></p>
<p>9 0.31913954 <a title="71-lsi-9" href="./nips-2004-Co-Training_and_Expansion%3A_Towards_Bridging_Theory_and_Practice.html">37 nips-2004-Co-Training and Expansion: Towards Bridging Theory and Practice</a></p>
<p>10 0.3133018 <a title="71-lsi-10" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>11 0.30205593 <a title="71-lsi-11" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>12 0.29342529 <a title="71-lsi-12" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>13 0.28951603 <a title="71-lsi-13" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>14 0.28441584 <a title="71-lsi-14" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>15 0.28417674 <a title="71-lsi-15" href="./nips-2004-Triangle_Fixing_Algorithms_for_the_Metric_Nearness_Problem.html">196 nips-2004-Triangle Fixing Algorithms for the Metric Nearness Problem</a></p>
<p>16 0.27890933 <a title="71-lsi-16" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<p>17 0.27728453 <a title="71-lsi-17" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>18 0.27138311 <a title="71-lsi-18" href="./nips-2004-On_the_Adaptive_Properties_of_Decision_Trees.html">137 nips-2004-On the Adaptive Properties of Decision Trees</a></p>
<p>19 0.26430425 <a title="71-lsi-19" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>20 0.26080427 <a title="71-lsi-20" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.097), (15, 0.091), (26, 0.069), (31, 0.047), (32, 0.011), (33, 0.168), (35, 0.039), (39, 0.079), (50, 0.057), (80, 0.269)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80851698 <a title="71-lda-1" href="./nips-2004-Generalization_Error_Bounds_for_Collaborative_Prediction_with_Low-Rank_Matrices.html">71 nips-2004-Generalization Error Bounds for Collaborative Prediction with Low-Rank Matrices</a></p>
<p>Author: Nathan Srebro, Noga Alon, Tommi S. Jaakkola</p><p>Abstract: We prove generalization error bounds for predicting entries in a partially observed matrix by ﬁtting the observed entries with a low-rank matrix. In justifying the analysis approach we take to obtain the bounds, we present an example of a class of functions of ﬁnite pseudodimension such that the sums of functions from this class have unbounded pseudodimension. 1</p><p>2 0.65572673 <a title="71-lda-2" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>Author: Sajama Sajama, Alon Orlitsky</p><p>Abstract: We present a semi-parametric latent variable model based technique for density modelling, dimensionality reduction and visualization. Unlike previous methods, we estimate the latent distribution non-parametrically which enables us to model data generated by an underlying low dimensional, multimodal distribution. In addition, we allow the components of latent variable models to be drawn from the exponential family which makes the method suitable for special data types, for example binary or count data. Simulations on real valued, binary and count data show favorable comparison to other related schemes both in terms of separating different populations and generalization to unseen samples. 1</p><p>3 0.65106159 <a title="71-lda-3" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>Author: Ingo Steinwart, Clint Scovel</p><p>Abstract: We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels. Finally, we compare our methods with a recent paper of G. Blanchard et al.. 1</p><p>4 0.64471865 <a title="71-lda-4" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: Regularization plays a central role in the analysis of modern data, where non-regularized ﬁtting is likely to lead to over-ﬁtted models, useless for both prediction and interpretation. We consider the design of incremental algorithms which follow paths of regularized solutions, as the regularization varies. These approaches often result in methods which are both efﬁcient and highly ﬂexible. We suggest a general path-following algorithm based on second-order approximations, prove that under mild conditions it remains “very close” to the path of optimal solutions and illustrate it with examples.</p><p>5 0.64452326 <a title="71-lda-5" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>Author: Francis R. Bach, Romain Thibaux, Michael I. Jordan</p><p>Abstract: The problem of learning a sparse conic combination of kernel functions or kernel matrices for classiﬁcation or regression can be achieved via the regularization by a block 1-norm [1]. In this paper, we present an algorithm that computes the entire regularization path for these problems. The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter. Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 1</p><p>6 0.64091313 <a title="71-lda-6" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>7 0.6401369 <a title="71-lda-7" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>8 0.63985294 <a title="71-lda-8" href="./nips-2004-Synergies_between_Intrinsic_and_Synaptic_Plasticity_in_Individual_Model_Neurons.html">181 nips-2004-Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons</a></p>
<p>9 0.63984275 <a title="71-lda-9" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>10 0.63878077 <a title="71-lda-10" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>11 0.63752472 <a title="71-lda-11" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>12 0.63743049 <a title="71-lda-12" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>13 0.63511807 <a title="71-lda-13" href="./nips-2004-Unsupervised_Variational_Bayesian_Learning_of_Nonlinear_Models.html">198 nips-2004-Unsupervised Variational Bayesian Learning of Nonlinear Models</a></p>
<p>14 0.63379979 <a title="71-lda-14" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>15 0.63377714 <a title="71-lda-15" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>16 0.63065517 <a title="71-lda-16" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<p>17 0.63005966 <a title="71-lda-17" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>18 0.62966466 <a title="71-lda-18" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>19 0.62928993 <a title="71-lda-19" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>20 0.62852824 <a title="71-lda-20" href="./nips-2004-Bayesian_inference_in_spiking_neurons.html">28 nips-2004-Bayesian inference in spiking neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
