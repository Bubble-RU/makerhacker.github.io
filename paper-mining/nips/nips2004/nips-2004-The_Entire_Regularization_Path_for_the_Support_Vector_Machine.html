<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 nips-2004-The Entire Regularization Path for the Support Vector Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-187" href="#">nips2004-187</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>187 nips-2004-The Entire Regularization Path for the Support Vector Machine</h1>
<br/><p>Source: <a title="nips-2004-187-pdf" href="http://papers.nips.cc/paper/2713-the-entire-regularization-path-for-the-support-vector-machine.pdf">pdf</a></p><p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>Reference: <a title="nips-2004-187-reference" href="../nips2004_reference/nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we argue that the choice of the SVM cost parameter can be critical. [sent-10, score-0.113]
</p><p>2 We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. [sent-11, score-0.522]
</p><p>3 1  Introduction  We have a set of n training pairs xi , yi , where xi ∈ Rp is a p-vector of real valued predictors (attributes) for the ith observation, yi ∈ {−1, +1} codes its binary response. [sent-12, score-0.852]
</p><p>4 The standard criterion for ﬁtting the linear SVM )[1, 2, 3] is 1 min ||β||2 + C β0 ,β 2 subject to, for each i:  n  ξi , i=1 yi (β0 + xT β) i  (1) ≥  1 − ξi . [sent-13, score-0.425]
</p><p>5 Here the ξi are non-negative slack variables that allow points to be on the wrong side of their “soft margin” (f (x) = ±1), as well as the decision boundary, and C is a cost parameter that controls the amount of overlap. [sent-14, score-0.201]
</p><p>6 If the data are separable, then for sufﬁciently large C the solution achieves the maximal margin separator; if not, the solution achieves the minimum overlap solution with largest margin. [sent-15, score-0.446]
</p><p>7 Alternatively, we can formulate the problem using a (hinge) Loss + Penalty criterion [4, 5]: n  [1 − yi (β0 + β T xi )]+ +  min β0 ,β  i=1  λ ||β||2 . [sent-16, score-0.545]
</p><p>8 2  The regularization parameter λ in (2) corresponds to 1/C, with C in (1). [sent-17, score-0.137]
</p><p>9 (2)  This latter formulation emphasizes the role of regularization. [sent-18, score-0.072]
</p><p>10 We may nevertheless avoid the maximum margin separator (λ ↓ 0), which is governed by observations on the boundary, in favor of a more regularized solution involving more observations. [sent-22, score-0.453]
</p><p>11 The nonlinear kernel SVMs can be represented in this form as well. [sent-23, score-0.11]
</p><p>12 With kernel K and n f (x) = β0 + i=1 θi K(x, xi ), we solve [5] n  n  [1 − yi (β0 +  min β0 ,θ  i=1  θi K(xi , xj ))] + j=1  λ 2  n  n  θj θj K(xj , xj ). [sent-24, score-0.796]
</p><p>13 (3)  j=1 j =1  Often the regularization parameter C (or λ) is regarded as a genuine “nuisance”. [sent-25, score-0.176]
</p><p>14 We used an SVM with a radial kernel K(x, x ) = exp(−γ||x − x ||2 ). [sent-28, score-0.234]
</p><p>15 35  γ=5  1e−01  1e+01  1e+03  1e−01  1e+01  1e+03  1e−01  1e+01  1e+03  1e−01  1e+01  1e+03  C = 1/λ Figure 1: Test error curves for the mixture example, using four different values for the radial kernel parameter γ. [sent-39, score-0.331]
</p><p>16 One of the reasons that investigators avoid extensive exploration of C is the computational cost involved. [sent-40, score-0.072]
</p><p>17 In this paper we develop an algorithm which ﬁts the entire path of SVM solutions [β0 (C), β(C)], for all possible values of C, with essentially the computational cost of ﬁtting a single model for a particular value of C. [sent-41, score-0.494]
</p><p>18 Our algorithm exploits the fact that the Lagrange multipliers implicit in (1) are piecewise-linear in C. [sent-42, score-0.126]
</p><p>19 2  Problem Setup  We use a criterion equivalent to (1), implementing the formulation in (2): n  min β,β0  ξi + i=1  λ T β β subject to 1 − yi f (xi ) ≤ ξi ; ξi ≥ 0; f (x) = β0 + β T x. [sent-45, score-0.385]
</p><p>20 2  (4)  Initially we consider only linear SVMs to get the intuitive ﬂavor of our procedure; we then generalize to kernel SVMs. [sent-46, score-0.15]
</p><p>21 We construct the Lagrange primal function n  LP :  n  n  λ ξi + β T β + αi (1 − yi f (xi ) − ξi ) − γi ξi 2 i=1 i=1 i=1  (5)  and set the derivatives to zero. [sent-47, score-0.266]
</p><p>22 This gives ∂ : ∂β ∂ : ∂β0  β=  1 λ  n  αi yi xi  (6)  i=1  n  yi αi = 0,  (7)  i=1  along with the KKT conditions αi (1 − yi f (xi ) − ξi ) = 0 γi ξi = 0 1 − αi − γi = 0  (8) (9) (10)  We see that 0 ≤ αi ≤ 1, with αi = 1 when ξi > 0 (which is when yi f (xi ) < 1). [sent-48, score-1.224]
</p><p>23 Also when yi f (xi ) > 1, ξi = 0 since no cost is incurred, and αi = 0. [sent-49, score-0.338]
</p><p>24 The usual Lagrange multipliers associated with the solution to (1) are αi = αi /λ = Cαi . [sent-51, score-0.143]
</p><p>25 We prefer our formulation here since our αi ∈ [0, 1], and this simpliﬁes the deﬁnition of the paths we deﬁne. [sent-52, score-0.083]
</p><p>26 We wish to ﬁnd the entire solution path for all values of λ ≥ 0. [sent-53, score-0.38]
</p><p>27 As λ decreases, ||β|| increases, and hence the width of the margin decreases. [sent-56, score-0.338]
</p><p>28 As this width decreases, points move from being inside to outside their margins. [sent-57, score-0.198]
</p><p>29 Their corresponding αi change from αi = 1 when they are inside their margin (yi f (xi ) < 1) to αi = 0 when they are outside their margin (yi f (xi ) > 1). [sent-58, score-0.702]
</p><p>30 By continuity, points must linger on the margin (yi f (xi ) = 1) while their αi decrease from 1 to 0. [sent-59, score-0.363]
</p><p>31 We will see that the αi (λ) trajectories are piecewise-linear in λ, which affords a great computational savings: as long as we can establish the break points, all values in between can be found by simple linear interpolation. [sent-60, score-0.113]
</p><p>32 Note that points can return to the margin, after having passed through it. [sent-61, score-0.088]
</p><p>33 It is easy to show that if the αi (λ) are piecewise linear in λ, then both αi (C) = Cαi (C) and β(C) are piecewise linear in C. [sent-62, score-0.686]
</p><p>34 It turns out that β0 (C) is also piecewise linear in C. [sent-63, score-0.343]
</p><p>35 For very large λ, ||β|| is small, and the the margin is very wide,  all points are in O, and hence αi = 1∀i. [sent-67, score-0.426]
</p><p>36 The margin narrows as λ decreases, but the orientation remains ﬁxed. [sent-69, score-0.316]
</p><p>37 Because of (7), the narrowing margin must connect with an outermost member of each class simultaneously. [sent-70, score-0.314]
</p><p>38 These points are easily identiﬁed, and this establishes the ﬁrst event, the ﬁrst tenants of M, and β0 . [sent-71, score-0.088]
</p><p>39 In order to satisfy the constraint (7), a quadratic programming algorithm is needed to obtain the initial conﬁguration. [sent-73, score-0.113]
</p><p>40 It is easy to see that the entire development carries through with “kernels” as well. [sent-76, score-0.138]
</p><p>41 In this case f (x) = β0 + g(x), and the only change that occurs is that (6) is changed to g(xi ) =  1 λ  n  αj yj K(xi , xj ), i = 1, . [sent-77, score-0.361]
</p><p>42 , n,  (11)  j=1  or θj (λ) = αj yj /λ using the notation in (3). [sent-80, score-0.149]
</p><p>43 Hereafter we will develop our algorithm for this more general kernel case. [sent-81, score-0.154]
</p><p>44 The Path The algorithm hinges on the set of points M sitting on the margin. [sent-82, score-0.088]
</p><p>45 We consider M at the point that an event has occurred: 1. [sent-83, score-0.146]
</p><p>46 The initial event, which means 2 or more points start in M, with their initial values of α ∈ [0, 1]. [sent-84, score-0.088]
</p><p>47 A point from I has just entered M, with its value of αi initially 1. [sent-86, score-0.089]
</p><p>48 One or more points in M has left the set, to join either O or I. [sent-90, score-0.127]
</p><p>49 Whichever the case, for continuity reasons this set will stay stable until the next event occurs, since to pass through M, a point’s αi must change from 0 to 1 or vice versa. [sent-91, score-0.286]
</p><p>50 Since all points in M have yi f (xi ) = 1, we can establish a path for their αi . [sent-92, score-0.649]
</p><p>51 We use the subscript to index the sets above immediately after the th event has occurred. [sent-93, score-0.146]
</p><p>52 For convenience we deﬁne α0 = λβ0 , and hence α0 = λ β0 . [sent-96, score-0.063]
</p><p>53   n 1 yj αj K(x, xj ) + α0  , f (x) = λ j=1  Since  for λ > λ > λ  +1  f (x) = =  (12)  we can write λ λ f (x) − f (x) + f (x) λ λ   1 (αj − αj )yj K(x, xj ) + (α0 − α0 ) + λ f (x) . [sent-97, score-0.345]
</p><p>54 Since each of the m points xi ∈ M are to stay on the margin, we have that   1 (αj − αj )yi yj K(xi , xj ) + yi (α0 − α0 ) + λ  = 1, ∀i ∈ M . [sent-99, score-0.806]
</p><p>55 (14) λ j∈M  Writing δj = αj − αj , from (14) we have δj yi yj K(xi , xj ) + yi δ0 = λ − λ, ∀i ∈ M . [sent-100, score-0.779]
</p><p>56 (15)  j∈M  Furthermore, since at all times  n i=1  yi αi = 0, we have that yj δj = 0. [sent-101, score-0.415]
</p><p>57 (16)  j∈M  Equations (15) and (16) constitute m + 1 linear equations in m + 1 unknowns δj , and can be solved. [sent-102, score-0.117]
</p><p>58 The δj and hence αj will change linearly in λ, until the next event occurs: αj = αj − (λ − λ)bj , j ∈ {0} ∪ M . [sent-103, score-0.251]
</p><p>59 From (13) we have f (x) =  λ f (x) − h (x) + h (x), λ  (18)  where h (x) =  yj bj K(x, xj ) + b0  (19)  j∈M  Thus the function itself changes in a piecewise-inverse manner in λ. [sent-105, score-0.304]
</p><p>60 Finding λ  +1  The paths continue until one of the following events occur: 1. [sent-106, score-0.083]
</p><p>61 One of the αi for i ∈ M reaches a boundary (0 or 1). [sent-107, score-0.063]
</p><p>62 For each i the value of λ for which this occurs is easily established. [sent-108, score-0.072]
</p><p>63 One of the points in I or O attains yi f (xi ) = 1. [sent-110, score-0.354]
</p><p>64 By examining these conditions, we can establish the largest λ < λ for which an event occurs, and hence establish λ +1 and update the sets. [sent-111, score-0.355]
</p><p>65 For this to happen without f “blowing up” in (18), we must have f − h = 0, and hence the boundary and margins remain ﬁxed at a point where i ξi is as small as possible, and the margin is as wide as possible subject to this constraint. [sent-115, score-0.443]
</p><p>66 Beyond that, several checks of cost O(n) are needed to evaluate the next move. [sent-122, score-0.072]
</p><p>67 1  0  50  100  150  200  Sequence Number  Figure 2: [Left] The margin sizes |M | as a function of λ, for different values of the radial-kernel parameter γ. [sent-125, score-0.316]
</p><p>68 [Right] The eigenvalues (on the log scale) for the kernel matrices Kγ corresponding to the four values of γ. [sent-127, score-0.171]
</p><p>69 The larger eigenvalues correspond in this case to smoother eigenfunctions, the small ones to rougher. [sent-128, score-0.116]
</p><p>70 The rougher eigenfunctions get penalized exponentially more than the smoother ones. [sent-129, score-0.191]
</p><p>71 Although we have no hard results, our experience so far suggests that the total number Λ of moves is O(k min(n+ , n− )), for k around 4 − 6; hence typically some small multiple c of n. [sent-131, score-0.063]
</p><p>72 If the average size of M is m, this suggests the total computational burden is O(cn2 m + nm2 ), which is similar to that of a single SVM ﬁt. [sent-132, score-0.063]
</p><p>73 Our R function SvmPath computes all 632 steps in the mixture example (n+ = n− = 100, radial kernel, γ = 1) in 1. [sent-133, score-0.18]
</p><p>74 02) secs on a Pentium 4, 2Ghz Linux machine; the svm function (using the optimized code libsvm, from the R library e1071) takes 9. [sent-135, score-0.186]
</p><p>75 06) seconds to compute the solution at 10 points along the path. [sent-137, score-0.145]
</p><p>76 Hence it takes our procedure about 50% more time to compute the entire path, than it costs libsvm to compute a typical single solution. [sent-138, score-0.156]
</p><p>77 4  Mixture simulation continued  The λ in Figure 1 are the entire collection of change points as described in Section 3. [sent-139, score-0.231]
</p><p>78 We were at ﬁrst surprised to discover that not all these sequences achieved zero training errors on the 200 training data points, at their least regularized ﬁt. [sent-140, score-0.106]
</p><p>79 It is sometimes argued that the implicit feature space is “inﬁnite dimensional” for this kernel, which suggests that perfect separation is always possible. [sent-142, score-0.08]
</p><p>80 The last row of the table shows the effective rank of the 200×200 kernel Gram matrix K (which we deﬁned to be the number of singular values greater than 10−12 ). [sent-143, score-0.212]
</p><p>81 In general a full rank K is required to achieve perfect separation. [sent-144, score-0.101]
</p><p>82 This emphasizes the fact that not all features in the feature map implied by K are of equal  γ Training Errors Effective Rank  5 0 200  1 12 177  0. [sent-146, score-0.072]
</p><p>83 1 33 76  Table 1: The number of minimal training errors for different values of the radial kernel scale parameter γ, for the mixture simulation example. [sent-148, score-0.376]
</p><p>84 Also shown is the effective rank of the 200 × 200 Gram matrix Kγ . [sent-149, score-0.102]
</p><p>85 Rephrasing, the regularization in (3) penalizes unit-norm features by the inverse of their eigenvalues, which effectively annihilates some, depending on γ. [sent-151, score-0.096]
</p><p>86 Writing (3) in matrix form, min L[y, Kθ] + β0 ,θ  λ T θ Kθ, 2  (20)  we reparametrize using the eigen-decomposition of K = UDUT . [sent-153, score-0.103]
</p><p>87 Then (20) becomes min L[y, Uθ ∗ ] + ∗  β0 ,θ  λ ∗ T −1 ∗ θ D θ . [sent-155, score-0.064]
</p><p>88 2  (21)  Now the columns of U are unit-norm basis functions (in R2 ) spanning the column space of K; from (21) we see that those members corresponding to near-zero eigenvalues (the elements of the diagonal matrix D) get heavily penalized and hence ignored. [sent-156, score-0.166]
</p><p>89 5  Discussion  Our work on the SVM path algorithm was inspired by early work on exact path algorithms in other settings. [sent-158, score-0.444]
</p><p>90 “Least Angle Regression” [8] show that the coefﬁcient path for the sequence of “lasso” coefﬁcients is piecewise linear. [sent-159, score-0.525]
</p><p>91 The lasso uses a quadratic criterion, with an L1 constraint. [sent-160, score-0.185]
</p><p>92 In fact, any model with an L1 constraint and a quadratic, piecewise quadratic, piecewise linear, or mixed quadratic and linear loss function, will have piecewise linear coefﬁcient paths, which can be calculated exactly and efﬁciently for all values of λ [9]. [sent-161, score-1.102]
</p><p>93 The SVM model has a quadratic constraint and a piecewise linear (“hinge”) loss function. [sent-163, score-0.456]
</p><p>94 This leads to a piecewise linear path in the dual space, hence the Lagrange coefﬁcients αi are piecewise linear. [sent-164, score-0.931]
</p><p>95 Of course, quadratic criterion + quadratic constraints also lead to exact path solutions, as in the classic ridge regression case, since a closed form solution is obtained via the SVD. [sent-165, score-0.56]
</p><p>96 After completing this work, it was brought to our attention that [11] reported on the picewise-linear nature of the lagrange multipliers, although they did not develop the path algorithm. [sent-167, score-0.394]
</p><p>97 [12, 13] employ techniques similar to ours in incremental learning for SVMs. [sent-168, score-0.093]
</p><p>98 These authors do not construct exact paths as we do, but rather focus on updating and downdating the solutions as more (or less) data arises. [sent-169, score-0.185]
</p><p>99 [14] allow for updating the parameters as well, but again do not construct entire solution paths. [sent-170, score-0.205]
</p><p>100 The entire regularization path for the support vector machine. [sent-214, score-0.475]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('piecewise', 0.303), ('margin', 0.275), ('yi', 0.266), ('path', 0.222), ('svm', 0.186), ('hastie', 0.164), ('xi', 0.16), ('yj', 0.149), ('event', 0.146), ('stanford', 0.136), ('saharon', 0.134), ('lagrange', 0.128), ('trevor', 0.126), ('radial', 0.124), ('rosset', 0.12), ('quadratic', 0.113), ('kernel', 0.11), ('ji', 0.106), ('entire', 0.101), ('xj', 0.098), ('regularization', 0.096), ('incremental', 0.093), ('svmpath', 0.09), ('tibshirani', 0.09), ('points', 0.088), ('multipliers', 0.086), ('paths', 0.083), ('establish', 0.073), ('occurs', 0.072), ('lasso', 0.072), ('emphasizes', 0.072), ('cost', 0.072), ('gram', 0.07), ('robert', 0.07), ('min', 0.064), ('boundary', 0.063), ('burden', 0.063), ('hence', 0.063), ('rank', 0.061), ('regularized', 0.061), ('eigenvalues', 0.061), ('inside', 0.061), ('separator', 0.06), ('bj', 0.057), ('solution', 0.057), ('support', 0.056), ('mixture', 0.056), ('criterion', 0.055), ('libsvm', 0.055), ('smoother', 0.055), ('ibm', 0.055), ('eigenfunctions', 0.055), ('solutions', 0.055), ('coef', 0.053), ('continuity', 0.053), ('hinge', 0.051), ('initially', 0.05), ('balanced', 0.05), ('outside', 0.049), ('kernels', 0.048), ('updating', 0.047), ('chapter', 0.047), ('decreases', 0.045), ('errors', 0.045), ('stay', 0.045), ('develop', 0.044), ('separable', 0.043), ('change', 0.042), ('wide', 0.042), ('penalized', 0.042), ('orientation', 0.041), ('writing', 0.041), ('parameter', 0.041), ('effective', 0.041), ('tting', 0.041), ('zhu', 0.041), ('linear', 0.04), ('perfect', 0.04), ('implicit', 0.04), ('haifa', 0.039), ('jizhu', 0.039), ('genuine', 0.039), ('fine', 0.039), ('jerome', 0.039), ('nuisance', 0.039), ('dut', 0.039), ('rougher', 0.039), ('unknowns', 0.039), ('bernard', 0.039), ('reparametrize', 0.039), ('entered', 0.039), ('join', 0.039), ('outermost', 0.039), ('wiggly', 0.039), ('svmlight', 0.039), ('technical', 0.039), ('setup', 0.038), ('track', 0.038), ('equations', 0.038), ('development', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="187-tfidf-1" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>2 0.33555362 <a title="187-tfidf-2" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: Regularization plays a central role in the analysis of modern data, where non-regularized ﬁtting is likely to lead to over-ﬁtted models, useless for both prediction and interpretation. We consider the design of incremental algorithms which follow paths of regularized solutions, as the regularization varies. These approaches often result in methods which are both efﬁcient and highly ﬂexible. We suggest a general path-following algorithm based on second-order approximations, prove that under mild conditions it remains “very close” to the path of optimal solutions and illustrate it with examples.</p><p>3 0.27058789 <a title="187-tfidf-3" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>Author: Francis R. Bach, Romain Thibaux, Michael I. Jordan</p><p>Abstract: The problem of learning a sparse conic combination of kernel functions or kernel matrices for classiﬁcation or regression can be achieved via the regularization by a block 1-norm [1]. In this paper, we present an algorithm that computes the entire regularization path for these problems. The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter. Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 1</p><p>4 0.25570238 <a title="187-tfidf-4" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>5 0.20933507 <a title="187-tfidf-5" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>Author: Linli Xu, James Neufeld, Bryce Larson, Dale Schuurmans</p><p>Abstract: We propose a new method for clustering based on ﬁnding maximum margin hyperplanes through data. By reformulating the problem in terms of the implied equivalence relation matrix, we can pose the problem as a convex integer program. Although this still yields a difﬁcult computational problem, the hard-clustering constraints can be relaxed to a soft-clustering formulation which can be feasibly solved with a semidefinite program. Since our clustering technique only depends on the data through the kernel matrix, we can easily achieve nonlinear clusterings in the same manner as spectral clustering. Experimental results show that our maximum margin clustering technique often obtains more accurate results than conventional clustering methods. The real beneﬁt of our approach, however, is that it leads naturally to a semi-supervised training method for support vector machines. By maximizing the margin simultaneously on labeled and unlabeled training data, we achieve state of the art performance by using a single, integrated learning principle. 1</p><p>6 0.18484154 <a title="187-tfidf-6" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>7 0.16776021 <a title="187-tfidf-7" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>8 0.16060388 <a title="187-tfidf-8" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>9 0.14121276 <a title="187-tfidf-9" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>10 0.14068212 <a title="187-tfidf-10" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>11 0.13682504 <a title="187-tfidf-11" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>12 0.13422951 <a title="187-tfidf-12" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>13 0.1257523 <a title="187-tfidf-13" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>14 0.12428856 <a title="187-tfidf-14" href="./nips-2004-Parallel_Support_Vector_Machines%3A_The_Cascade_SVM.html">144 nips-2004-Parallel Support Vector Machines: The Cascade SVM</a></p>
<p>15 0.12122815 <a title="187-tfidf-15" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>16 0.11702821 <a title="187-tfidf-16" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>17 0.11357851 <a title="187-tfidf-17" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>18 0.098239936 <a title="187-tfidf-18" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>19 0.097744592 <a title="187-tfidf-19" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>20 0.096715041 <a title="187-tfidf-20" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.33), (1, 0.174), (2, -0.048), (3, 0.302), (4, -0.065), (5, -0.039), (6, 0.078), (7, -0.185), (8, -0.157), (9, -0.0), (10, 0.246), (11, 0.065), (12, -0.104), (13, 0.078), (14, -0.008), (15, 0.053), (16, -0.032), (17, -0.061), (18, -0.143), (19, -0.05), (20, 0.04), (21, -0.173), (22, 0.02), (23, -0.074), (24, 0.067), (25, 0.008), (26, -0.094), (27, -0.044), (28, 0.097), (29, -0.031), (30, 0.031), (31, -0.008), (32, -0.07), (33, -0.102), (34, 0.029), (35, -0.01), (36, -0.029), (37, 0.044), (38, -0.031), (39, -0.008), (40, -0.125), (41, -0.082), (42, -0.0), (43, -0.06), (44, -0.044), (45, 0.042), (46, 0.031), (47, 0.018), (48, -0.053), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97152579 <a title="187-lsi-1" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>2 0.80043793 <a title="187-lsi-2" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: Regularization plays a central role in the analysis of modern data, where non-regularized ﬁtting is likely to lead to over-ﬁtted models, useless for both prediction and interpretation. We consider the design of incremental algorithms which follow paths of regularized solutions, as the regularization varies. These approaches often result in methods which are both efﬁcient and highly ﬂexible. We suggest a general path-following algorithm based on second-order approximations, prove that under mild conditions it remains “very close” to the path of optimal solutions and illustrate it with examples.</p><p>3 0.7882154 <a title="187-lsi-3" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>Author: Francis R. Bach, Romain Thibaux, Michael I. Jordan</p><p>Abstract: The problem of learning a sparse conic combination of kernel functions or kernel matrices for classiﬁcation or regression can be achieved via the regularization by a block 1-norm [1]. In this paper, we present an algorithm that computes the entire regularization path for these problems. The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter. Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 1</p><p>4 0.66275889 <a title="187-lsi-4" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>Author: Changjiang Yang, Ramani Duraiswami, Larry S. Davis</p><p>Abstract: The computation and memory required for kernel machines with N training samples is at least O(N 2 ). Such a complexity is signiﬁcant even for moderate size problems and is prohibitive for large datasets. We present an approximation technique based on the improved fast Gauss transform to reduce the computation to O(N ). We also give an error bound for the approximation, and provide experimental results on the UCI datasets. 1</p><p>5 0.62463701 <a title="187-lsi-5" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>6 0.6133728 <a title="187-lsi-6" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>7 0.60634059 <a title="187-lsi-7" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>8 0.56062776 <a title="187-lsi-8" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>9 0.54346299 <a title="187-lsi-9" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>10 0.54242516 <a title="187-lsi-10" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>11 0.53058618 <a title="187-lsi-11" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>12 0.51447558 <a title="187-lsi-12" href="./nips-2004-Parallel_Support_Vector_Machines%3A_The_Cascade_SVM.html">144 nips-2004-Parallel Support Vector Machines: The Cascade SVM</a></p>
<p>13 0.50474477 <a title="187-lsi-13" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>14 0.50117916 <a title="187-lsi-14" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>15 0.49988425 <a title="187-lsi-15" href="./nips-2004-Learning%2C_Regularization_and_Ill-Posed_Inverse_Problems.html">96 nips-2004-Learning, Regularization and Ill-Posed Inverse Problems</a></p>
<p>16 0.44419628 <a title="187-lsi-16" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<p>17 0.44412392 <a title="187-lsi-17" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>18 0.41569525 <a title="187-lsi-18" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>19 0.40576521 <a title="187-lsi-19" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>20 0.40326145 <a title="187-lsi-20" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.112), (15, 0.157), (26, 0.076), (31, 0.05), (33, 0.188), (35, 0.027), (39, 0.062), (50, 0.026), (59, 0.222), (81, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88102961 <a title="187-lda-1" href="./nips-2004-The_power_of_feature_clustering%3A_An_application_to_object_detection.html">192 nips-2004-The power of feature clustering: An application to object detection</a></p>
<p>Author: Shai Avidan, Moshe Butman</p><p>Abstract: We give a fast rejection scheme that is based on image segments and demonstrate it on the canonical example of face detection. However, instead of focusing on the detection step we focus on the rejection step and show that our method is simple and fast to be learned, thus making it an excellent pre-processing step to accelerate standard machine learning classiﬁers, such as neural-networks, Bayes classiﬁers or SVM. We decompose a collection of face images into regions of pixels with similar behavior over the image set. The relationships between the mean and variance of image segments are used to form a cascade of rejectors that can reject over 99.8% of image patches, thus only a small fraction of the image patches must be passed to a full-scale classiﬁer. Moreover, the training time for our method is much less than an hour, on a standard PC. The shape of the features (i.e. image segments) we use is data-driven, they are very cheap to compute and they form a very low dimensional feature space in which exhaustive search for the best features is tractable. 1</p><p>same-paper 2 0.87524521 <a title="187-lda-2" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>3 0.80279291 <a title="187-lda-3" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: Regularization plays a central role in the analysis of modern data, where non-regularized ﬁtting is likely to lead to over-ﬁtted models, useless for both prediction and interpretation. We consider the design of incremental algorithms which follow paths of regularized solutions, as the regularization varies. These approaches often result in methods which are both efﬁcient and highly ﬂexible. We suggest a general path-following algorithm based on second-order approximations, prove that under mild conditions it remains “very close” to the path of optimal solutions and illustrate it with examples.</p><p>4 0.77950853 <a title="187-lda-4" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>Author: Yoshua Bengio, Martin Monperrus</p><p>Abstract: We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation suggests to explore non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions. A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails. 1</p><p>5 0.7756294 <a title="187-lda-5" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>Author: Sajama Sajama, Alon Orlitsky</p><p>Abstract: We present a semi-parametric latent variable model based technique for density modelling, dimensionality reduction and visualization. Unlike previous methods, we estimate the latent distribution non-parametrically which enables us to model data generated by an underlying low dimensional, multimodal distribution. In addition, we allow the components of latent variable models to be drawn from the exponential family which makes the method suitable for special data types, for example binary or count data. Simulations on real valued, binary and count data show favorable comparison to other related schemes both in terms of separating different populations and generalization to unseen samples. 1</p><p>6 0.77502835 <a title="187-lda-6" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>7 0.77205586 <a title="187-lda-7" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>8 0.76859283 <a title="187-lda-8" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>9 0.7685824 <a title="187-lda-9" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>10 0.76825142 <a title="187-lda-10" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>11 0.76765448 <a title="187-lda-11" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>12 0.76485527 <a title="187-lda-12" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>13 0.76469779 <a title="187-lda-13" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>14 0.76380086 <a title="187-lda-14" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>15 0.7637589 <a title="187-lda-15" href="./nips-2004-Hierarchical_Distributed_Representations_for_Statistical_Language_Modeling.html">78 nips-2004-Hierarchical Distributed Representations for Statistical Language Modeling</a></p>
<p>16 0.76308596 <a title="187-lda-16" href="./nips-2004-Adaptive_Discriminative_Generative_Model_and_Its_Applications.html">16 nips-2004-Adaptive Discriminative Generative Model and Its Applications</a></p>
<p>17 0.76243097 <a title="187-lda-17" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>18 0.7622112 <a title="187-lda-18" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>19 0.76205283 <a title="187-lda-19" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>20 0.76131171 <a title="187-lda-20" href="./nips-2004-Bayesian_inference_in_spiking_neurons.html">28 nips-2004-Bayesian inference in spiking neurons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
