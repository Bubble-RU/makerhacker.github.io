<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2004-Support Vector Classification with Input Data Uncertainty</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-178" href="#">nips2004-178</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>178 nips-2004-Support Vector Classification with Input Data Uncertainty</h1>
<br/><p>Source: <a title="nips-2004-178-pdf" href="http://papers.nips.cc/paper/2743-support-vector-classification-with-input-data-uncertainty.pdf">pdf</a></p><p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>Reference: <a title="nips-2004-178-reference" href="../nips2004_reference/nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tsvc', 0.564), ('xi', 0.534), ('yi', 0.307), ('svc', 0.165), ('uncertainty', 0.163), ('wt', 0.146), ('sw', 0.121), ('corrupt', 0.115), ('socp', 0.113), ('uncertain', 0.103), ('tikhonov', 0.09), ('nois', 0.09), ('input', 0.087), ('tls', 0.078), ('xj', 0.075), ('yj', 0.075), ('hyperpl', 0.073), ('contamin', 0.072), ('dual', 0.067), ('digit', 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="178-tfidf-1" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>2 0.34252304 <a title="178-tfidf-2" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>3 0.27342775 <a title="178-tfidf-3" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>4 0.26974145 <a title="178-tfidf-4" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>5 0.18267369 <a title="178-tfidf-5" href="./nips-2004-Rate-_and_Phase-coded_Autoassociative_Memory.html">151 nips-2004-Rate- and Phase-coded Autoassociative Memory</a></p>
<p>Author: Máté Lengyel, Peter Dayan</p><p>Abstract: Areas of the brain involved in various forms of memory exhibit patterns of neural activity quite unlike those in canonical computational models. We show how to use well-founded Bayesian probabilistic autoassociative recall to derive biologically reasonable neuronal dynamics in recurrently coupled models, together with appropriate values for parameters such as the membrane time constant and inhibition. We explicitly treat two cases. One arises from a standard Hebbian learning rule, and involves activity patterns that are coded by graded ﬁring rates. The other arises from a spike timing dependent learning rule, and involves patterns coded by the phase of spike times relative to a coherent local ﬁeld potential oscillation. Our model offers a new and more complete understanding of how neural dynamics may support autoassociation. 1</p><p>6 0.15977667 <a title="178-tfidf-6" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>7 0.14721836 <a title="178-tfidf-7" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>8 0.14466621 <a title="178-tfidf-8" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>9 0.13582678 <a title="178-tfidf-9" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>10 0.1353026 <a title="178-tfidf-10" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>11 0.13365752 <a title="178-tfidf-11" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>12 0.13186291 <a title="178-tfidf-12" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>13 0.12022268 <a title="178-tfidf-13" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>14 0.11979244 <a title="178-tfidf-14" href="./nips-2004-Adaptive_Manifold_Learning.html">17 nips-2004-Adaptive Manifold Learning</a></p>
<p>15 0.11784673 <a title="178-tfidf-15" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>16 0.11567633 <a title="178-tfidf-16" href="./nips-2004-Joint_Probabilistic_Curve_Clustering_and_Alignment.html">90 nips-2004-Joint Probabilistic Curve Clustering and Alignment</a></p>
<p>17 0.10987726 <a title="178-tfidf-17" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>18 0.10838321 <a title="178-tfidf-18" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>19 0.10799558 <a title="178-tfidf-19" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>20 0.10538359 <a title="178-tfidf-20" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.29), (1, 0.099), (2, 0.053), (3, 0.224), (4, -0.108), (5, 0.056), (6, 0.004), (7, 0.172), (8, -0.308), (9, 0.011), (10, -0.02), (11, -0.179), (12, 0.071), (13, -0.085), (14, -0.106), (15, -0.161), (16, -0.11), (17, -0.176), (18, -0.118), (19, -0.103), (20, -0.083), (21, 0.083), (22, 0.086), (23, 0.001), (24, -0.08), (25, -0.011), (26, 0.049), (27, 0.035), (28, 0.174), (29, -0.052), (30, -0.058), (31, -0.053), (32, 0.032), (33, -0.083), (34, 0.004), (35, 0.073), (36, 0.022), (37, 0.065), (38, -0.006), (39, -0.009), (40, 0.025), (41, -0.008), (42, 0.028), (43, -0.044), (44, 0.041), (45, -0.059), (46, 0.076), (47, -0.04), (48, -0.035), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9806419 <a title="178-lsi-1" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>2 0.88086671 <a title="178-lsi-2" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>3 0.79366136 <a title="178-lsi-3" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>4 0.66959357 <a title="178-lsi-4" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>5 0.63040286 <a title="178-lsi-5" href="./nips-2004-Adaptive_Manifold_Learning.html">17 nips-2004-Adaptive Manifold Learning</a></p>
<p>Author: Jing Wang, Zhenyue Zhang, Hongyuan Zha</p><p>Abstract: Recently, there have been several advances in the machine learning and pattern recognition communities for developing manifold learning algorithms to construct nonlinear low-dimensional manifolds from sample data points embedded in high-dimensional spaces. In this paper, we develop algorithms that address two key issues in manifold learning: 1) the adaptive selection of the neighborhood sizes; and 2) better ﬁtting the local geometric structure to account for the variations in the curvature of the manifold and its interplay with the sampling density of the data set. We also illustrate the effectiveness of our methods on some synthetic data sets. 1</p><p>6 0.60772872 <a title="178-lsi-6" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>7 0.59380966 <a title="178-lsi-7" href="./nips-2004-Rate-_and_Phase-coded_Autoassociative_Memory.html">151 nips-2004-Rate- and Phase-coded Autoassociative Memory</a></p>
<p>8 0.5818609 <a title="178-lsi-8" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>9 0.55701834 <a title="178-lsi-9" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>10 0.54888904 <a title="178-lsi-10" href="./nips-2004-Comparing_Beliefs%2C_Surveys%2C_and_Random_Walks.html">41 nips-2004-Comparing Beliefs, Surveys, and Random Walks</a></p>
<p>11 0.54015648 <a title="178-lsi-11" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>12 0.49415636 <a title="178-lsi-12" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>13 0.47869524 <a title="178-lsi-13" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>14 0.47561616 <a title="178-lsi-14" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>15 0.46525693 <a title="178-lsi-15" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>16 0.4525066 <a title="178-lsi-16" href="./nips-2004-Incremental_Algorithms_for_Hierarchical_Classification.html">82 nips-2004-Incremental Algorithms for Hierarchical Classification</a></p>
<p>17 0.44721141 <a title="178-lsi-17" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>18 0.42783004 <a title="178-lsi-18" href="./nips-2004-PAC-Bayes_Learning_of_Conjunctions_and_Classification_of_Gene-Expression_Data.html">143 nips-2004-PAC-Bayes Learning of Conjunctions and Classification of Gene-Expression Data</a></p>
<p>19 0.42071292 <a title="178-lsi-19" href="./nips-2004-Class-size_Independent_Generalization_Analsysis_of_Some_Discriminative_Multi-Category_Classification.html">36 nips-2004-Class-size Independent Generalization Analsysis of Some Discriminative Multi-Category Classification</a></p>
<p>20 0.4174343 <a title="178-lsi-20" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.023), (9, 0.014), (15, 0.062), (26, 0.026), (27, 0.105), (33, 0.144), (37, 0.193), (74, 0.069), (77, 0.136), (81, 0.063), (96, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94319052 <a title="178-lda-1" href="./nips-2004-An_Auditory_Paradigm_for_Brain-Computer_Interfaces.html">20 nips-2004-An Auditory Paradigm for Brain-Computer Interfaces</a></p>
<p>Author: N. J. Hill, Thomas N. Lal, Karin Bierig, Niels Birbaumer, Bernhard Schölkopf</p><p>Abstract: Motivated by the particular problems involved in communicating with “locked-in” paralysed patients, we aim to develop a braincomputer interface that uses auditory stimuli. We describe a paradigm that allows a user to make a binary decision by focusing attention on one of two concurrent auditory stimulus sequences. Using Support Vector Machine classiﬁcation and Recursive Channel Elimination on the independent components of averaged eventrelated potentials, we show that an untrained user’s EEG data can be classiﬁed with an encouragingly high level of accuracy. This suggests that it is possible for users to modulate EEG signals in a single trial by the conscious direction of attention, well enough to be useful in BCI. 1</p><p>2 0.92307955 <a title="178-lda-2" href="./nips-2004-Saliency-Driven_Image_Acuity_Modulation_on_a_Reconfigurable_Array_of_Spiking_Silicon_Neurons.html">157 nips-2004-Saliency-Driven Image Acuity Modulation on a Reconfigurable Array of Spiking Silicon Neurons</a></p>
<p>Author: R. J. Vogelstein, Udayan Mallik, Eugenio Culurciello, Gert Cauwenberghs, Ralph Etienne-Cummings</p><p>Abstract: We have constructed a system that uses an array of 9,600 spiking silicon neurons, a fast microcontroller, and digital memory, to implement a reconﬁgurable network of integrate-and-ﬁre neurons. The system is designed for rapid prototyping of spiking neural networks that require high-throughput communication with external address-event hardware. Arbitrary network topologies can be implemented by selectively routing address-events to speciﬁc internal or external targets according to a memory-based projective ﬁeld mapping. The utility and versatility of the system is demonstrated by conﬁguring it as a three-stage network that accepts input from an address-event imager, detects salient regions of the image, and performs spatial acuity modulation around a high-resolution fovea that is centered on the location of highest salience. 1</p><p>same-paper 3 0.88822567 <a title="178-lda-3" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>4 0.87882936 <a title="178-lda-4" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>Author: Francis R. Bach, Romain Thibaux, Michael I. Jordan</p><p>Abstract: The problem of learning a sparse conic combination of kernel functions or kernel matrices for classiﬁcation or regression can be achieved via the regularization by a block 1-norm [1]. In this paper, we present an algorithm that computes the entire regularization path for these problems. The path is obtained by using numerical continuation techniques, and involves a running time complexity that is a constant times the complexity of solving the problem for one value of the regularization parameter. Working in the setting of kernel linear regression and kernel logistic regression, we show empirically that the effect of the block 1-norm regularization differs notably from the (non-block) 1-norm regularization commonly used for variable selection, and that the regularization path is of particular value in the block case. 1</p><p>5 0.86274689 <a title="178-lda-5" href="./nips-2004-Parallel_Support_Vector_Machines%3A_The_Cascade_SVM.html">144 nips-2004-Parallel Support Vector Machines: The Cascade SVM</a></p>
<p>Author: Hans P. Graf, Eric Cosatto, Léon Bottou, Igor Dourdanovic, Vladimir Vapnik</p><p>Abstract: We describe an algorithm for support vector machines (SVM) that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors. Instead of analyzing the whole training set in one optimization step, the data are split into subsets and optimized separately with multiple SVMs. The partial results are combined and filtered again in a ‘Cascade’ of SVMs, until the global optimum is reached. The Cascade SVM can be spread over multiple processors with minimal communication overhead and requires far less memory, since the kernel matrices are much smaller than for a regular SVM. Convergence to the global optimum is guaranteed with multiple passes through the Cascade, but already a single pass provides good generalization. A single pass is 5x – 10x faster than a regular SVM for problems of 100,000 vectors when implemented on a single processor. Parallel implementations on a cluster of 16 processors were tested with over 1 million vectors (2-class problems), converging in a day or two, while a regular SVM never converged in over a week. 1</p><p>6 0.86150509 <a title="178-lda-6" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>7 0.8596915 <a title="178-lda-7" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<p>8 0.85966569 <a title="178-lda-8" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<p>9 0.85561508 <a title="178-lda-9" href="./nips-2004-Binet-Cauchy_Kernels.html">30 nips-2004-Binet-Cauchy Kernels</a></p>
<p>10 0.85415399 <a title="178-lda-10" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>11 0.85244715 <a title="178-lda-11" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>12 0.84929729 <a title="178-lda-12" href="./nips-2004-Using_the_Equivalent_Kernel_to_Understand_Gaussian_Process_Regression.html">201 nips-2004-Using the Equivalent Kernel to Understand Gaussian Process Regression</a></p>
<p>13 0.84813952 <a title="178-lda-13" href="./nips-2004-Bayesian_Regularization_and_Nonnegative_Deconvolution_for_Time_Delay_Estimation.html">27 nips-2004-Bayesian Regularization and Nonnegative Deconvolution for Time Delay Estimation</a></p>
<p>14 0.84799737 <a title="178-lda-14" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<p>15 0.84685838 <a title="178-lda-15" href="./nips-2004-Breaking_SVM_Complexity_with_Cross-Training.html">34 nips-2004-Breaking SVM Complexity with Cross-Training</a></p>
<p>16 0.84381354 <a title="178-lda-16" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>17 0.84340525 <a title="178-lda-17" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>18 0.84263819 <a title="178-lda-18" href="./nips-2004-Methods_Towards_Invasive_Human_Brain_Computer_Interfaces.html">117 nips-2004-Methods Towards Invasive Human Brain Computer Interfaces</a></p>
<p>19 0.84193462 <a title="178-lda-19" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>20 0.84102881 <a title="178-lda-20" href="./nips-2004-Adaptive_Manifold_Learning.html">17 nips-2004-Adaptive Manifold Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
