<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 nips-2004-Efficient Kernel Discriminant Analysis via QR Decomposition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-59" href="#">nips2004-59</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 nips-2004-Efficient Kernel Discriminant Analysis via QR Decomposition</h1>
<br/><p>Source: <a title="nips-2004-59-pdf" href="http://papers.nips.cc/paper/2686-efficient-kernel-discriminant-analysis-via-qr-decomposition.pdf">pdf</a></p><p>Author: Tao Xiong, Jieping Ye, Qi Li, Ravi Janardan, Vladimir Cherkassky</p><p>Abstract: Linear Discriminant Analysis (LDA) is a well-known method for feature extraction and dimension reduction. It has been used widely in many applications such as face recognition. Recently, a novel LDA algorithm based on QR Decomposition, namely LDA/QR, has been proposed, which is competitive in terms of classiﬁcation accuracy with other LDA algorithms, but it has much lower costs in time and space. However, LDA/QR is based on linear projection, which may not be suitable for data with nonlinear structure. This paper ﬁrst proposes an algorithm called KDA/QR, which extends the LDA/QR algorithm to deal with nonlinear data by using the kernel operator. Then an efﬁcient approximation of KDA/QR called AKDA/QR is proposed. Experiments on face image data show that the classiﬁcation accuracy of both KDA/QR and AKDA/QR are competitive with Generalized Discriminant Analysis (GDA), a general kernel discriminant analysis algorithm, while AKDA/QR has much lower time and space costs. 1</p><p>Reference: <a title="nips-2004-59-reference" href="../nips2004_reference/nips-2004-Efficient_Kernel_Discriminant_Analysis_via_QR_Decomposition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Linear Discriminant Analysis (LDA) is a well-known method for feature extraction and dimension reduction. [sent-11, score-0.112]
</p><p>2 It has been used widely in many applications such as face recognition. [sent-12, score-0.131]
</p><p>3 Recently, a novel LDA algorithm based on QR Decomposition, namely LDA/QR, has been proposed, which is competitive in terms of classiﬁcation accuracy with other LDA algorithms, but it has much lower costs in time and space. [sent-13, score-0.192]
</p><p>4 This paper ﬁrst proposes an algorithm called KDA/QR, which extends the LDA/QR algorithm to deal with nonlinear data by using the kernel operator. [sent-15, score-0.286]
</p><p>5 Experiments on face image data show that the classiﬁcation accuracy of both KDA/QR and AKDA/QR are competitive with Generalized Discriminant Analysis (GDA), a general kernel discriminant analysis algorithm, while AKDA/QR has much lower time and space costs. [sent-17, score-0.741]
</p><p>6 It has been used widely in many applications such as face recognition [2]. [sent-19, score-0.17]
</p><p>7 Classical LDA aims to ﬁnd optimal transformation by minimizing the within-class distance and maximizing the between-class distance simultaneously, thus achieving maximum discrimination. [sent-20, score-0.072]
</p><p>8 The optimal transformation can be readily computed by computing the eigen-decomposition on the scatter matrices. [sent-21, score-0.216]
</p><p>9 To deal with such a limitation, nonlinear extensions through kernel functions have been proposed. [sent-23, score-0.224]
</p><p>10 The main idea of kernel-based methods is to map the input data to a feature space through a nonlinear mapping, where the inner products in the feature  space can be computed by a kernel function without knowing the nonlinear mapping explicitly [9]. [sent-24, score-0.433]
</p><p>11 To our knowledge, there are few efﬁcient algorithms for general kernel based discriminant algorithms — most known algorithms effectively scale as O(n3 ) where n is the sample size. [sent-26, score-0.49]
</p><p>12 Moreover, experiments in [11, 12] show that the classiﬁcation accuracy of LDA/QR is competitive with other LDA algorithms. [sent-35, score-0.128]
</p><p>13 In this paper, we ﬁrst propose an algorithm, namely KDA/QR1 , which is a nonlinear extension of LDA/QR. [sent-36, score-0.093]
</p><p>14 Since KDA/QR involves the whole kernel matrix, which is not scalable for large datasets, we also propose an approximation of KDA/QR, namely AKDA/QR. [sent-37, score-0.197]
</p><p>15 A distinct property of AKDA/QR is that it scales as O(ndc), where n is the size of the data, d is the dimension of the data, and c is the number of classes. [sent-38, score-0.091]
</p><p>16 We apply the proposed algorithms on face image datasets and compare them with LDA/QR, and Generalized Discriminant Analysis (GDA) [1], a general method for kernel discriminant analysis. [sent-39, score-0.642]
</p><p>17 Experiments show that: (1) AKDA/QR is competitive with KDA/QR and GDA in classiﬁcation; (2) both KDA/QR and AKDA/QR outperform LDA/QR in classiﬁcation; and (3) AKDA/QR has much lower costs in time and space than GDA. [sent-40, score-0.129]
</p><p>18 The ﬁrst stage maximizes the separation between different classes via QR Decomposition [4]. [sent-43, score-0.124]
</p><p>19 The second stage addresses the issue of minimizing the within-class distance, while maintaining low time/space complexity. [sent-44, score-0.124]
</p><p>20 Let A ∈ IRd×n be the data matrix, where each column ai is a vector in d-dimensional space. [sent-45, score-0.139]
</p><p>21 Assume A is partitioned into c classes {Πi }c , and the size of the ith class |Πi | = ni . [sent-46, score-0.187]
</p><p>22 The ﬁrst stage of LDA/QR aims to solve the following optimization problem, G = arg max trace(Gt Sb G). [sent-49, score-0.207]
</p><p>23 The solution can also be obtained through QR Decomposition on the centroid matrix C [12], where C = [m1 , m2 , · · · , mc ] consists of the c centroids. [sent-52, score-0.286]
</p><p>24 B ← Y t Y ; /*Reduced between-class scatter matrix*/ 6. [sent-58, score-0.139]
</p><p>25 Compute the c eigenvectors φi of (T + µIc )−1 B with decreasing eigenvalues; 8. [sent-60, score-0.07]
</p><p>26 Then G = QV , for any orthogonal matrix V , solves the optimization problem in Eq. [sent-63, score-0.162]
</p><p>27 Note that the choice of orthogonal matrix V is arbitrary, since trace(Gt Sb G) = trace(V t Gt Sb GV ), for any orthogonal matrix V . [sent-65, score-0.23]
</p><p>28 The second stage of LDA/QR reﬁnes the ﬁrst stage by addressing the issue of minimizing the within-class distance. [sent-66, score-0.248]
</p><p>29 It incorporates the within-class scatter information by applying a relaxation scheme on V (relaxing V from an orthogonal matrix to an arbitrary matrix). [sent-67, score-0.254]
</p><p>30 In the second stage of LDA/QR, we look for a transformation matrix G such that G = QV , for some V . [sent-68, score-0.233]
</p><p>31 Since Gt Sb G = V t (Qt Sb Q)V , Gt Sw G = V t (Qt Sw Q)V , and Gt St G = V t (Qt St Q)V , the original problem of ﬁnding optimal G is equivalent to ﬁnding V , with B = Qt Sb Q, W = Qt Sw Q, and T = Qt St Q as the “reduced” betweenclass, within-class and total scatter matrices, respectively. [sent-71, score-0.139]
</p><p>32 Note that B has much smaller size than the original scatter matrix Sb (similarly for W and T ). [sent-72, score-0.245]
</p><p>33 The optimal V can be computed efﬁciently using many existing LDA-based methods, since we are dealing with matrices B, W , and T of size c by c. [sent-73, score-0.105]
</p><p>34 We can compute the optimal V by simply applying regularized LDA; that is, we compute V , by solving a small eigenvalue problem on (W + µIc )−1 B or (T + µIc )−1 B (note T = B + W ), for some positive constant µ [3]. [sent-74, score-0.091]
</p><p>35 We use the total scatter instead of the within-class scatter in Lines 4, 6, and 7, mainly for convenience of presentation of the kernel methods in Section 3 and Section 4. [sent-76, score-0.442]
</p><p>36 3  Kernel discriminant analysis via QR-decomposition (KDA/QR)  In this section, the KDA/QR algorithm, a nonlinear extension of LDA/QR through kernel functions, is presented. [sent-77, score-0.501]
</p><p>37 Let Φ be a mapping to the feature space and Φ(A) be the data matrix in the feature space. [sent-78, score-0.181]
</p><p>38 Then, the centroid matrix C Φ in the feature space is C Φ = mΦ , · · · , mΦ = 1 c  1 n1  Φ(ai ), · · · , i∈Π1  1 nc  Φ(ai ) . [sent-79, score-0.388]
</p><p>39 (2)  i∈Πc  1 The global centroid in the feature space can be computed as mΦ = n i ni mΦ . [sent-80, score-0.331]
</p><p>40 To maxii mize between-class distance in the feature space, as discussed in Section 2, we perform QR decomposition on C Φ , i. [sent-81, score-0.169]
</p><p>41 A key observation is that RΦ can be computed as (C Φ )t C Φ = (RΦ )t RΦ by applying the Cholesky decomposition on (C Φ )t C Φ [4]. [sent-84, score-0.156]
</p><p>42 Φ(an )], and the ith column 1 1 of M is (0, · · · , 0, ni , · · · , ni , 0, · · · , 0)t . [sent-88, score-0.201]
</p><p>43 Let K be the kernel matrix with K(i, j) = Φ(ai ), Φ(aj ) . [sent-89, score-0.237]
</p><p>44 Compute the c eigenvectors φΦ of (T Φ + µIc )−1 B Φ , with decreasing eigenvalues; i 9. [sent-100, score-0.07]
</p><p>45 The matrices Y Φ , Z Φ , B Φ , and W Φ in the feature space (corresponding to the second stage in LDA/QR) can be computed as follows. [sent-103, score-0.25]
</p><p>46 Φ In the feature space, we have Hb = C Φ N , where the ith column of N is √ √ ni Φ ((0, · · · , ni , · · · 0)t − n (n1 , · · · , nc )t . [sent-104, score-0.359]
</p><p>47 We proceed by computing the c eigenvectors {φΦ }c of (T Φ + µIc )−1 B Φ . [sent-108, score-0.068]
</p><p>48 The ﬁnal transformation matrix can be computed as c 1 2 GΦ = QΦ V Φ = C Φ (RΦ )−1 V Φ . [sent-110, score-0.15]
</p><p>49 1  Complexity analysis of KDA/QR  The cost to formulate the kernel matrix in Line 1 is O(n2 d). [sent-114, score-0.272]
</p><p>50 The Cholesky decomposition in Line 3 takes O(c3 ) [4]. [sent-116, score-0.162]
</p><p>51 Lines 4 takes O(c3 ), as M t KM is already computed in 1 Line 2. [sent-117, score-0.088]
</p><p>52 In Line 5, the computation of Z Φ = E t KM (RΦ )−1 = (I − n eet )KM (RΦ )−1 = 1 KM (RΦ )−1 − n e (et KM )(RΦ )−1 in the given order takes O(nc2 ), assuming KM is kept in Line 2. [sent-118, score-0.092]
</p><p>53 Hence, the total complexity of the kernel LDA/QR algorithm is O(n2 d). [sent-120, score-0.195]
</p><p>54 Omitting the cost for evaluating the kernel matrix K, which is required in all kernel-based algorithms, the total cost is O(n2 ). [sent-121, score-0.237]
</p><p>55 Note that all other general discriminant analysis algorithms scale as O(n3 ). [sent-122, score-0.305]
</p><p>56 Note that the bottleneck of KDA/QR is the explicit formation of the large kernel matrix K for the computation of (C Φ )t C Φ in Line 2 of Algorithm 2. [sent-124, score-0.237]
</p><p>57 Mathematically, the optimal x∗ can be computed j j j by solving the following optimization problem: min  xj ∈Rd  Φ(xj ) −  1 nj  Φ(ai )  2  for j = 1, · · · , c. [sent-128, score-0.183]
</p><p>58 Consider Gaussian kernel function exp(− x − y 2 /σ), where σ is the bandwidth parameter. [sent-133, score-0.164]
</p><p>59 The optimization problem in (5) is convex if for each j = 1, · · · , c  and for all i ∈ Πj ,  2 (xj − ai ) ≤ 1 σ  (6)  Proof. [sent-134, score-0.186]
</p><p>60 It is easy to check that, for the Gaussian kernel, the optimization problem in (5) reduces to: min f (xj )  xj ∈Rd  where f (x) = fi (x) is H(fi ) =  for j = 1, · · · , c,  (7)  2 i∈Πj fi (x) and fi (x) = −exp(− x − ai /σ). [sent-135, score-0.446]
</p><p>61 The Hessian matrix of 2 2 2 t σ exp(− x − ai /σ)(I − σ (x − ai )(x − ai ) ). [sent-136, score-0.49]
</p><p>62 It is easy to show that  2 if σ (x − ai ) ≤ 1, for all i ∈ Πj , then H(fi ) is positive semi-deﬁnite, that is, fi (x) is convex. [sent-137, score-0.212]
</p><p>63 For applications involving high-dimensional data, such as face recognition, σ is usually large (typically ranging from thousands to hundreds of thousands [13]), and the condition in Lemma 4. [sent-139, score-0.187]
</p><p>64 A key observation is that for relatively large σ, the centroid of each class in the original space will map very close to the centroid in the feature space [9], which can serve as the approximate solution 1 of the optimization problem in (7). [sent-142, score-0.447]
</p><p>65 Experiments show that choosing x∗ = nj i∈Πj ai j produces results close to the one by solving the optimization problem in (7). [sent-143, score-0.24]
</p><p>66 , c, the centroid matrix C Φ can be approximated by j ˆ C Φ ≈ [Φ(x∗ ) . [sent-148, score-0.23]
</p><p>67 Compute x∗ = nj i∈Πj ai , for j = 1, · · · , c; j ˆ 2. [sent-152, score-0.193]
</p><p>68 Compute the c eigenvectors φΦ of (T Φ + µIc )−1 B Φ , with decreasing eigenvalues; i Φ Φ ˆΦ Φ ˆ ˆ ˆ 9. [sent-160, score-0.07]
</p><p>69 The Cholesky decomposition of K will i j Φ Φ t ˆΦ ˆ ˆ ˆ give us R by K = (R ) R . [sent-164, score-0.115]
</p><p>70 1  Complexity analysis of AKDA/QR  ˆ It takes O(dn) in Line 1. [sent-170, score-0.082]
</p><p>71 The construction of the matrix K in Line 2 takes O(c2 d). [sent-171, score-0.12]
</p><p>72 It then takes O(c3 ) and O(nc2 ) for matrix multiplications in Lines 6 and 7, respectively. [sent-174, score-0.12]
</p><p>73 Table 1 lists the time and space complexities of several dimension reduction algorithms. [sent-177, score-0.136]
</p><p>74 It is clear from the table that AKDA/QR is more efﬁcient than other kernel based methods. [sent-178, score-0.197]
</p><p>75 Note that both KDA/QR and AKDA/QR have two parameters: σ for the kernel function and µ for the regularization. [sent-181, score-0.164]
</p><p>76 We randomly select p samples of each person from the dataset for training and the rest for  0. [sent-187, score-0.07]
</p><p>77 2 3  4 5 6 7 Number of training samples per class  8  3  4 5 6 7 Number of training samples per class  8  Figure 1: Comparison of classiﬁcation accuracy on PIX (left) and AR (right). [sent-202, score-0.172]
</p><p>78 We repeat the experiments 20 times and report the average recognition accuracy of each method. [sent-204, score-0.087]
</p><p>79 Datasets: We use the following three datasets in our study, which are publicly available: PIX contains 300 face images of 30 persons. [sent-209, score-0.202]
</p><p>80 We subsample the images down to a size of 100 × 100 = 10000; ORL is a well-known dataset for face recognition. [sent-211, score-0.284]
</p><p>81 It contains ten different face images of 40 persons, for a total of 400 images. [sent-212, score-0.166]
</p><p>82 The image size is 92 × 112 = 10304; AR is a large face image datasets. [sent-213, score-0.246]
</p><p>83 This subset contains 1638 face images of 126 persons. [sent-215, score-0.166]
</p><p>84 We subsample the images down to a size of 60 × 40 = 2400. [sent-217, score-0.113]
</p><p>85 LDA/QR: In this experiment, we compare the performance of AKDA/QR and KDA/QR with that of several other linear dimension reduction algorithms including PCA, LDA/QR on two face datasets. [sent-220, score-0.256]
</p><p>86 1, where the x-axis denotes the number of samples per class in the training set and the y-axis denotes the classiﬁcation accuracy. [sent-223, score-0.062]
</p><p>87 It is known that the images in the AR dataset contain pretty large area of occlusion due to sun glasses and scarves, which makes linear algorithms such as LDA/QR less effective. [sent-227, score-0.103]
</p><p>88 Another interesting observation is that the approximate AKQA/QR algorithm is competitive with its exact version KDA/QR in all cases. [sent-228, score-0.111]
</p><p>89 The comparison is made on the ORL face dataset, as the result of GDA on ORL is available in [5]. [sent-231, score-0.131]
</p><p>90 The main observation from this experiment is that both KDA/QR and AKDA/QR are competitive with GDA, while AKDA/QR is much more efﬁcient than GDA (see Table 1). [sent-234, score-0.08]
</p><p>91 Similar to the ﬁrst experiment, Table 2 shows that KDA/QR and AKDA/QR consistently outperform the PCA and LDA/QR algorithms in terms of recognition accuracy. [sent-235, score-0.116]
</p><p>92 6  Conclusions  In this paper, we ﬁrst present a general kernel discriminant analysis algorithm, called KDA/QR. [sent-236, score-0.441]
</p><p>93 9875  Table 2: Comparison of classiﬁcation accuracy on ORL face image dataset. [sent-267, score-0.22]
</p><p>94 Our experimental results show that the accuracy achieved by the two algorithms is very competitive with GDA, a general kernel discriminant algorithms, while AKDA/QR is much more efﬁcient. [sent-271, score-0.562]
</p><p>95 Kernel-based optimized feature vectors selection and discriminant analysis for face recognition. [sent-307, score-0.462]
</p><p>96 A mathematical programming approach to the kernel a u ﬁsher algorithm. [sent-314, score-0.164]
</p><p>97 An improved training algorithm for kernel ﬁsher diso criminants. [sent-330, score-0.195]
</p><p>98 Nonlinear component analysis as a kernel eigenvalue o u problem. [sent-341, score-0.23]
</p><p>99 LDA/QR: An efﬁcient and effective dimension reduction algorithm and its theoretical foundation. [sent-346, score-0.128]
</p><p>100 IDR/QR: An incremental dimension reduction algorithm via QR decomposition. [sent-355, score-0.128]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gda', 0.366), ('km', 0.27), ('sb', 0.243), ('discriminant', 0.242), ('hb', 0.216), ('ht', 0.187), ('qr', 0.172), ('kernel', 0.164), ('centroid', 0.157), ('cholesky', 0.147), ('ndc', 0.141), ('scatter', 0.139), ('lda', 0.139), ('ai', 0.139), ('ic', 0.134), ('face', 0.131), ('stage', 0.124), ('decomposition', 0.115), ('sw', 0.112), ('gt', 0.107), ('nc', 0.104), ('pca', 0.1), ('qt', 0.099), ('janardan', 0.098), ('minnesota', 0.098), ('orl', 0.098), ('ye', 0.098), ('st', 0.089), ('kopf', 0.084), ('ktc', 0.084), ('ktz', 0.084), ('competitive', 0.08), ('ni', 0.079), ('mika', 0.075), ('hw', 0.074), ('pix', 0.074), ('matrix', 0.073), ('fi', 0.073), ('qv', 0.067), ('army', 0.067), ('line', 0.065), ('ar', 0.062), ('nonlinear', 0.06), ('dimension', 0.058), ('kfda', 0.056), ('xiong', 0.056), ('mc', 0.056), ('feature', 0.054), ('nj', 0.054), ('outperform', 0.049), ('ece', 0.049), ('irc', 0.049), ('jieping', 0.049), ('accuracy', 0.048), ('optimization', 0.047), ('takes', 0.047), ('subsample', 0.045), ('cse', 0.045), ('eet', 0.045), ('rd', 0.043), ('ith', 0.043), ('generalized', 0.043), ('eigenvectors', 0.042), ('ird', 0.042), ('irn', 0.042), ('sher', 0.042), ('orthogonal', 0.042), ('xj', 0.041), ('image', 0.041), ('computed', 0.041), ('dataset', 0.04), ('trace', 0.04), ('classi', 0.039), ('complexities', 0.039), ('recognition', 0.039), ('reduction', 0.039), ('transformation', 0.036), ('datasets', 0.036), ('aims', 0.036), ('images', 0.035), ('analysis', 0.035), ('table', 0.033), ('namely', 0.033), ('size', 0.033), ('class', 0.032), ('lines', 0.032), ('algorithm', 0.031), ('matrices', 0.031), ('eigenvalue', 0.031), ('sch', 0.03), ('samples', 0.03), ('compute', 0.03), ('kernels', 0.03), ('eigenvalues', 0.029), ('decreasing', 0.028), ('algorithms', 0.028), ('thousands', 0.028), ('ef', 0.028), ('department', 0.027), ('proceed', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="59-tfidf-1" href="./nips-2004-Efficient_Kernel_Discriminant_Analysis_via_QR_Decomposition.html">59 nips-2004-Efficient Kernel Discriminant Analysis via QR Decomposition</a></p>
<p>Author: Tao Xiong, Jieping Ye, Qi Li, Ravi Janardan, Vladimir Cherkassky</p><p>Abstract: Linear Discriminant Analysis (LDA) is a well-known method for feature extraction and dimension reduction. It has been used widely in many applications such as face recognition. Recently, a novel LDA algorithm based on QR Decomposition, namely LDA/QR, has been proposed, which is competitive in terms of classiﬁcation accuracy with other LDA algorithms, but it has much lower costs in time and space. However, LDA/QR is based on linear projection, which may not be suitable for data with nonlinear structure. This paper ﬁrst proposes an algorithm called KDA/QR, which extends the LDA/QR algorithm to deal with nonlinear data by using the kernel operator. Then an efﬁcient approximation of KDA/QR called AKDA/QR is proposed. Experiments on face image data show that the classiﬁcation accuracy of both KDA/QR and AKDA/QR are competitive with Generalized Discriminant Analysis (GDA), a general kernel discriminant analysis algorithm, while AKDA/QR has much lower time and space costs. 1</p><p>2 0.38620266 <a title="59-tfidf-2" href="./nips-2004-Two-Dimensional_Linear_Discriminant_Analysis.html">197 nips-2004-Two-Dimensional Linear Discriminant Analysis</a></p>
<p>Author: Jieping Ye, Ravi Janardan, Qi Li</p><p>Abstract: Linear Discriminant Analysis (LDA) is a well-known scheme for feature extraction and dimension reduction. It has been used widely in many applications involving high-dimensional data, such as face recognition and image retrieval. An intrinsic limitation of classical LDA is the so-called singularity problem, that is, it fails when all scatter matrices are singular. A well-known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using Principal Component Analysis (PCA) before LDA. The algorithm, called PCA+LDA, is used widely in face recognition. However, PCA+LDA has high costs in time and space, due to the need for an eigen-decomposition involving the scatter matrices. In this paper, we propose a novel LDA algorithm, namely 2DLDA, which stands for 2-Dimensional Linear Discriminant Analysis. 2DLDA overcomes the singularity problem implicitly, while achieving efﬁciency. The key difference between 2DLDA and classical LDA lies in the model for data representation. Classical LDA works with vectorized representations of data, while the 2DLDA algorithm works with data in matrix representation. To further reduce the dimension by 2DLDA, the combination of 2DLDA and classical LDA, namely 2DLDA+LDA, is studied, where LDA is preceded by 2DLDA. The proposed algorithms are applied on face recognition and compared with PCA+LDA. Experiments show that 2DLDA and 2DLDA+LDA achieve competitive recognition accuracy, while being much more efﬁcient. 1</p><p>3 0.12399817 <a title="59-tfidf-3" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>Author: Volker Roth</p><p>Abstract: The problem of detecting “atypical objects” or “outliers” is one of the classical topics in (robust) statistics. Recently, it has been proposed to address this problem by means of one-class SVM classiﬁers. The main conceptual shortcoming of most one-class approaches, however, is that in a strict sense they are unable to detect outliers, since the expected fraction of outliers has to be speciﬁed in advance. The method presented in this paper overcomes this problem by relating kernelized one-class classiﬁcation to Gaussian density estimation in the induced feature space. Having established this relation, it is possible to identify “atypical objects” by quantifying their deviations from the Gaussian model. For RBF kernels it is shown that the Gaussian model is “rich enough” in the sense that it asymptotically provides an unbiased estimator for the true density. In order to overcome the inherent model selection problem, a cross-validated likelihood criterion for selecting all free model parameters is applied. 1</p><p>4 0.10681912 <a title="59-tfidf-4" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>Author: Xiaojin Zhu, Jaz Kandola, Zoubin Ghahramani, John D. Lafferty</p><p>Abstract: We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning. The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion. Unlike previous work using diffusion kernels and Gaussian random ﬁeld kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization. This results in ﬂexible kernels and avoids the need to choose among different parametric forms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets. We evaluate the kernels on real datasets using support vector machines, with encouraging results. 1</p><p>5 0.099667668 <a title="59-tfidf-5" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>Author: Marco Cuturi, Jean-philippe Vert</p><p>Abstract: Complex objects can often be conveniently represented by ﬁnite sets of simpler components, such as images by sets of patches or texts by bags of words. We study the class of positive deﬁnite (p.d.) kernels for two such objects that can be expressed as a function of the merger of their respective sets of components. We prove a general integral representation of such kernels and present two particular examples. One of them leads to a kernel for sets of points living in a space endowed itself with a positive deﬁnite kernel. We provide experimental results on a benchmark experiment of handwritten digits image classiﬁcation which illustrate the validity of the approach. 1</p><p>6 0.094265677 <a title="59-tfidf-6" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>7 0.089931853 <a title="59-tfidf-7" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>8 0.089120165 <a title="59-tfidf-8" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>9 0.086712487 <a title="59-tfidf-9" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>10 0.086607553 <a title="59-tfidf-10" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>11 0.085662588 <a title="59-tfidf-11" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>12 0.085190661 <a title="59-tfidf-12" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>13 0.084134243 <a title="59-tfidf-13" href="./nips-2004-Hierarchical_Eigensolver_for_Transition_Matrices_in_Spectral_Methods.html">79 nips-2004-Hierarchical Eigensolver for Transition Matrices in Spectral Methods</a></p>
<p>14 0.083852448 <a title="59-tfidf-14" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>15 0.081233904 <a title="59-tfidf-15" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>16 0.075588353 <a title="59-tfidf-16" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>17 0.071236953 <a title="59-tfidf-17" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>18 0.070213079 <a title="59-tfidf-18" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>19 0.069867983 <a title="59-tfidf-19" href="./nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">8 nips-2004-A Machine Learning Approach to Conjoint Analysis</a></p>
<p>20 0.069320515 <a title="59-tfidf-20" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.228), (1, 0.084), (2, -0.019), (3, -0.029), (4, -0.033), (5, -0.019), (6, -0.13), (7, -0.182), (8, 0.168), (9, 0.11), (10, -0.057), (11, -0.103), (12, -0.095), (13, 0.131), (14, 0.028), (15, -0.127), (16, -0.24), (17, -0.038), (18, 0.277), (19, 0.157), (20, 0.159), (21, -0.132), (22, 0.113), (23, 0.17), (24, 0.101), (25, 0.064), (26, -0.142), (27, 0.016), (28, -0.021), (29, 0.089), (30, -0.023), (31, 0.027), (32, 0.051), (33, 0.016), (34, 0.002), (35, 0.061), (36, 0.092), (37, -0.107), (38, -0.03), (39, 0.049), (40, -0.036), (41, 0.107), (42, 0.039), (43, -0.011), (44, -0.007), (45, 0.004), (46, 0.011), (47, 0.036), (48, -0.023), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93544745 <a title="59-lsi-1" href="./nips-2004-Two-Dimensional_Linear_Discriminant_Analysis.html">197 nips-2004-Two-Dimensional Linear Discriminant Analysis</a></p>
<p>Author: Jieping Ye, Ravi Janardan, Qi Li</p><p>Abstract: Linear Discriminant Analysis (LDA) is a well-known scheme for feature extraction and dimension reduction. It has been used widely in many applications involving high-dimensional data, such as face recognition and image retrieval. An intrinsic limitation of classical LDA is the so-called singularity problem, that is, it fails when all scatter matrices are singular. A well-known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using Principal Component Analysis (PCA) before LDA. The algorithm, called PCA+LDA, is used widely in face recognition. However, PCA+LDA has high costs in time and space, due to the need for an eigen-decomposition involving the scatter matrices. In this paper, we propose a novel LDA algorithm, namely 2DLDA, which stands for 2-Dimensional Linear Discriminant Analysis. 2DLDA overcomes the singularity problem implicitly, while achieving efﬁciency. The key difference between 2DLDA and classical LDA lies in the model for data representation. Classical LDA works with vectorized representations of data, while the 2DLDA algorithm works with data in matrix representation. To further reduce the dimension by 2DLDA, the combination of 2DLDA and classical LDA, namely 2DLDA+LDA, is studied, where LDA is preceded by 2DLDA. The proposed algorithms are applied on face recognition and compared with PCA+LDA. Experiments show that 2DLDA and 2DLDA+LDA achieve competitive recognition accuracy, while being much more efﬁcient. 1</p><p>same-paper 2 0.93168068 <a title="59-lsi-2" href="./nips-2004-Efficient_Kernel_Discriminant_Analysis_via_QR_Decomposition.html">59 nips-2004-Efficient Kernel Discriminant Analysis via QR Decomposition</a></p>
<p>Author: Tao Xiong, Jieping Ye, Qi Li, Ravi Janardan, Vladimir Cherkassky</p><p>Abstract: Linear Discriminant Analysis (LDA) is a well-known method for feature extraction and dimension reduction. It has been used widely in many applications such as face recognition. Recently, a novel LDA algorithm based on QR Decomposition, namely LDA/QR, has been proposed, which is competitive in terms of classiﬁcation accuracy with other LDA algorithms, but it has much lower costs in time and space. However, LDA/QR is based on linear projection, which may not be suitable for data with nonlinear structure. This paper ﬁrst proposes an algorithm called KDA/QR, which extends the LDA/QR algorithm to deal with nonlinear data by using the kernel operator. Then an efﬁcient approximation of KDA/QR called AKDA/QR is proposed. Experiments on face image data show that the classiﬁcation accuracy of both KDA/QR and AKDA/QR are competitive with Generalized Discriminant Analysis (GDA), a general kernel discriminant analysis algorithm, while AKDA/QR has much lower time and space costs. 1</p><p>3 0.59230411 <a title="59-lsi-3" href="./nips-2004-Neighbourhood_Components_Analysis.html">127 nips-2004-Neighbourhood Components Analysis</a></p>
<p>Author: Jacob Goldberger, Geoffrey E. Hinton, Sam T. Roweis, Ruslan Salakhutdinov</p><p>Abstract: In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classiﬁcation algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classiﬁcation. Unlike other methods, our classiﬁcation model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction. 1</p><p>4 0.44333744 <a title="59-lsi-4" href="./nips-2004-Outlier_Detection_with_One-class_Kernel_Fisher_Discriminants.html">142 nips-2004-Outlier Detection with One-class Kernel Fisher Discriminants</a></p>
<p>Author: Volker Roth</p><p>Abstract: The problem of detecting “atypical objects” or “outliers” is one of the classical topics in (robust) statistics. Recently, it has been proposed to address this problem by means of one-class SVM classiﬁers. The main conceptual shortcoming of most one-class approaches, however, is that in a strict sense they are unable to detect outliers, since the expected fraction of outliers has to be speciﬁed in advance. The method presented in this paper overcomes this problem by relating kernelized one-class classiﬁcation to Gaussian density estimation in the induced feature space. Having established this relation, it is possible to identify “atypical objects” by quantifying their deviations from the Gaussian model. For RBF kernels it is shown that the Gaussian model is “rich enough” in the sense that it asymptotically provides an unbiased estimator for the true density. In order to overcome the inherent model selection problem, a cross-validated likelihood criterion for selecting all free model parameters is applied. 1</p><p>5 0.38525483 <a title="59-lsi-5" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<p>Author: Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: This paper provides a foundation for multi–task learning using reproducing kernel Hilbert spaces of vector–valued functions. In this setting, the kernel is a matrix–valued function. Some explicit examples will be described which go beyond our earlier results in [7]. In particular, we characterize classes of matrix– valued kernels which are linear and are of the dot product or the translation invariant type. We discuss how these kernels can be used to model relations between the tasks and present linear multi–task learning algorithms. Finally, we present a novel proof of the representer theorem for a minimizer of a regularization functional which is based on the notion of minimal norm interpolation. 1</p><p>6 0.37042874 <a title="59-lsi-6" href="./nips-2004-Face_Detection_---_Efficient_and_Rank_Deficient.html">68 nips-2004-Face Detection --- Efficient and Rank Deficient</a></p>
<p>7 0.36861107 <a title="59-lsi-7" href="./nips-2004-Kernel_Projection_Machine%3A_a_New_Tool_for_Pattern_Recognition.html">93 nips-2004-Kernel Projection Machine: a New Tool for Pattern Recognition</a></p>
<p>8 0.34339273 <a title="59-lsi-8" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>9 0.33190787 <a title="59-lsi-9" href="./nips-2004-Nonparametric_Transforms_of_Graph_Kernels_for_Semi-Supervised_Learning.html">133 nips-2004-Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning</a></p>
<p>10 0.31210729 <a title="59-lsi-10" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<p>11 0.29847947 <a title="59-lsi-11" href="./nips-2004-Hierarchical_Eigensolver_for_Transition_Matrices_in_Spectral_Methods.html">79 nips-2004-Hierarchical Eigensolver for Transition Matrices in Spectral Methods</a></p>
<p>12 0.29288787 <a title="59-lsi-12" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>13 0.28325802 <a title="59-lsi-13" href="./nips-2004-Efficient_Kernel_Machines_Using_the_Improved_Fast_Gauss_Transform.html">60 nips-2004-Efficient Kernel Machines Using the Improved Fast Gauss Transform</a></p>
<p>14 0.27927995 <a title="59-lsi-14" href="./nips-2004-Who%27s_In_the_Picture.html">205 nips-2004-Who's In the Picture</a></p>
<p>15 0.27478799 <a title="59-lsi-15" href="./nips-2004-Object_Classification_from_a_Single_Example_Utilizing_Class_Relevance_Metrics.html">134 nips-2004-Object Classification from a Single Example Utilizing Class Relevance Metrics</a></p>
<p>16 0.26974493 <a title="59-lsi-16" href="./nips-2004-Supervised_Graph_Inference.html">177 nips-2004-Supervised Graph Inference</a></p>
<p>17 0.26955968 <a title="59-lsi-17" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>18 0.26816779 <a title="59-lsi-18" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>19 0.26204807 <a title="59-lsi-19" href="./nips-2004-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">182 nips-2004-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>20 0.25533393 <a title="59-lsi-20" href="./nips-2004-A_Machine_Learning_Approach_to_Conjoint_Analysis.html">8 nips-2004-A Machine Learning Approach to Conjoint Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.06), (15, 0.691), (26, 0.037), (33, 0.088), (35, 0.012), (39, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97880816 <a title="59-lda-1" href="./nips-2004-Dependent_Gaussian_Processes.html">50 nips-2004-Dependent Gaussian Processes</a></p>
<p>Author: Phillip Boyle, Marcus Frean</p><p>Abstract: Gaussian processes are usually parameterised in terms of their covariance functions. However, this makes it difﬁcult to deal with multiple outputs, because ensuring that the covariance matrix is positive deﬁnite is problematic. An alternative formulation is to treat Gaussian processes as white noise sources convolved with smoothing kernels, and to parameterise the kernel instead. Using this, we extend Gaussian processes to handle multiple, coupled outputs. 1</p><p>same-paper 2 0.97084796 <a title="59-lda-2" href="./nips-2004-Efficient_Kernel_Discriminant_Analysis_via_QR_Decomposition.html">59 nips-2004-Efficient Kernel Discriminant Analysis via QR Decomposition</a></p>
<p>Author: Tao Xiong, Jieping Ye, Qi Li, Ravi Janardan, Vladimir Cherkassky</p><p>Abstract: Linear Discriminant Analysis (LDA) is a well-known method for feature extraction and dimension reduction. It has been used widely in many applications such as face recognition. Recently, a novel LDA algorithm based on QR Decomposition, namely LDA/QR, has been proposed, which is competitive in terms of classiﬁcation accuracy with other LDA algorithms, but it has much lower costs in time and space. However, LDA/QR is based on linear projection, which may not be suitable for data with nonlinear structure. This paper ﬁrst proposes an algorithm called KDA/QR, which extends the LDA/QR algorithm to deal with nonlinear data by using the kernel operator. Then an efﬁcient approximation of KDA/QR called AKDA/QR is proposed. Experiments on face image data show that the classiﬁcation accuracy of both KDA/QR and AKDA/QR are competitive with Generalized Discriminant Analysis (GDA), a general kernel discriminant analysis algorithm, while AKDA/QR has much lower time and space costs. 1</p><p>3 0.96098799 <a title="59-lda-3" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>Author: Le Lu, Gregory D. Hager, Laurent Younes</p><p>Abstract: Visual action recognition is an important problem in computer vision. In this paper, we propose a new method to probabilistically model and recognize actions of articulated objects, such as hand or body gestures, in image sequences. Our method consists of three levels of representation. At the low level, we ﬁrst extract a feature vector invariant to scale and in-plane rotation by using the Fourier transform of a circular spatial histogram. Then, spectral partitioning [20] is utilized to obtain an initial clustering; this clustering is then reﬁned using a temporal smoothness constraint. Gaussian mixture model (GMM) based clustering and density estimation in the subspace of linear discriminant analysis (LDA) are then applied to thousands of image feature vectors to obtain an intermediate level representation. Finally, at the high level we build a temporal multiresolution histogram model for each action by aggregating the clustering weights of sampled images belonging to that action. We discuss how this high level representation can be extended to achieve temporal scaling invariance and to include Bi-gram or Multi-gram transition information. Both image clustering and action recognition/segmentation results are given to show the validity of our three tiered representation.</p><p>4 0.95247865 <a title="59-lda-4" href="./nips-2004-Validity_Estimates_for_Loopy_Belief_Propagation_on_Binary_Real-world_Networks.html">203 nips-2004-Validity Estimates for Loopy Belief Propagation on Binary Real-world Networks</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We introduce a computationally efﬁcient method to estimate the validity of the BP method as a function of graph topology, the connectivity strength, frustration and network size. We present numerical results that demonstrate the correctness of our estimates for the uniform random model and for a real-world network (“C. Elegans”). Although the method is restricted to pair-wise interactions, no local evidence (zero “biases”) and binary variables, we believe that its predictions correctly capture the limitations of BP for inference and MAP estimation on arbitrary graphical models. Using this approach, we ﬁnd that BP always performs better than MF. Especially for large networks with broad degree distributions (such as scale-free networks) BP turns out to signiﬁcantly outperform MF. 1</p><p>5 0.91725314 <a title="59-lda-5" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>Author: Joachim Giesen, Simon Spalinger, Bernhard Schölkopf</p><p>Abstract: We describe methods for computing an implicit model of a hypersurface that is given only by a ﬁnite sampling. The methods work by mapping the sample points into a reproducing kernel Hilbert space and then determining regions in terms of hyperplanes. 1</p><p>6 0.91679156 <a title="59-lda-6" href="./nips-2004-Two-Dimensional_Linear_Discriminant_Analysis.html">197 nips-2004-Two-Dimensional Linear Discriminant Analysis</a></p>
<p>7 0.90532029 <a title="59-lda-7" href="./nips-2004-Temporal-Difference_Networks.html">183 nips-2004-Temporal-Difference Networks</a></p>
<p>8 0.83182025 <a title="59-lda-8" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>9 0.77481192 <a title="59-lda-9" href="./nips-2004-Using_the_Equivalent_Kernel_to_Understand_Gaussian_Process_Regression.html">201 nips-2004-Using the Equivalent Kernel to Understand Gaussian Process Regression</a></p>
<p>10 0.76540089 <a title="59-lda-10" href="./nips-2004-Probabilistic_Computation_in_Spiking_Populations.html">148 nips-2004-Probabilistic Computation in Spiking Populations</a></p>
<p>11 0.753667 <a title="59-lda-11" href="./nips-2004-Hierarchical_Eigensolver_for_Transition_Matrices_in_Spectral_Methods.html">79 nips-2004-Hierarchical Eigensolver for Transition Matrices in Spectral Methods</a></p>
<p>12 0.7519294 <a title="59-lda-12" href="./nips-2004-Semigroup_Kernels_on_Finite_Sets.html">168 nips-2004-Semigroup Kernels on Finite Sets</a></p>
<p>13 0.74354273 <a title="59-lda-13" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>14 0.72012371 <a title="59-lda-14" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>15 0.71932441 <a title="59-lda-15" href="./nips-2004-Binet-Cauchy_Kernels.html">30 nips-2004-Binet-Cauchy Kernels</a></p>
<p>16 0.71745032 <a title="59-lda-16" href="./nips-2004-Kernels_for_Multi--task_Learning.html">94 nips-2004-Kernels for Multi--task Learning</a></p>
<p>17 0.71738398 <a title="59-lda-17" href="./nips-2004-Learning_Gaussian_Process_Kernels_via_Hierarchical_Bayes.html">98 nips-2004-Learning Gaussian Process Kernels via Hierarchical Bayes</a></p>
<p>18 0.71714824 <a title="59-lda-18" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>19 0.70834905 <a title="59-lda-19" href="./nips-2004-Algebraic_Set_Kernels_with_Application_to_Inference_Over_Local_Image_Representations.html">18 nips-2004-Algebraic Set Kernels with Application to Inference Over Local Image Representations</a></p>
<p>20 0.7037164 <a title="59-lda-20" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
