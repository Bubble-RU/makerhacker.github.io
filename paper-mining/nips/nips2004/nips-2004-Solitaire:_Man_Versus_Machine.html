<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2004-Solitaire: Man Versus Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-171" href="#">nips2004-171</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>171 nips-2004-Solitaire: Man Versus Machine</h1>
<br/><p>Source: <a title="nips-2004-171-pdf" href="http://papers.nips.cc/paper/2568-solitaire-man-versus-machine.pdf">pdf</a></p><p>Author: Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy</p><p>Abstract: In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 1</p><p>Reference: <a title="nips-2004-171-reference" href="../nips2004_reference/nips-2004-Solitaire%3A_Man_Versus_Machine_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. [sent-5, score-0.438]
</p><p>2 This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. [sent-6, score-0.431]
</p><p>3 A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. [sent-7, score-0.302]
</p><p>4 For discounted or average reward Markov decision problems with n states and two possible actions per state, the tightest known worst-case upper bound in terms of n on the number of iterations taken to ﬁnd an optimal policy is O(2n /n) [9]. [sent-9, score-0.183]
</p><p>5 In particular, a policy is represented by a table with one action per state and each iteration improves the policy by updating each entry of this table. [sent-14, score-0.224]
</p><p>6 In such large problems, one might resort to a suboptimal heuristic policy, taking the form of an algorithm that accepts a state as input and generates an action as output. [sent-15, score-0.148]
</p><p>7 An interesting recent development in dynamic programming is the rollout method. [sent-16, score-0.357]
</p><p>8 Pioneered by Tesauro and Galperin [13, 2], the rollout method leverages the policy improvement concept to amplify the performance of any given heuristic. [sent-17, score-0.423]
</p><p>9 Unlike the conventional policy improvement algorithm, which computes an optimal policy off-line so that it may later be used in decision-making, the rollout method performs its computations on-line at the time when a decision is to be made. [sent-18, score-0.518]
</p><p>10 When making a decision, rather than applying the heuristic policy directly, the rollout method computes an action that would result from an iteration of policy improvement applied to the heuristic policy. [sent-19, score-0.685]
</p><p>11 The way in which actions are generated by the rollout method may be considered an alternative heuristic that improves on the original. [sent-21, score-0.425]
</p><p>12 One might consider applying the rollout method to this new heuristic. [sent-22, score-0.318]
</p><p>13 For this reason, prior applications of the rollout method have involved only one iteration [3, 4, 5, 6, 8, 11, 12, 13]. [sent-27, score-0.336]
</p><p>14 A second iteration of the rollout method would have been infeasible – requiring about six orders of magnitude more time per move. [sent-29, score-0.336]
</p><p>15 In this paper, we apply the rollout method to a version of solitaire, modeled as a deterministic Markov decision problem with over 52! [sent-30, score-0.347]
</p><p>16 Our study represents an important contribution both to the study of the rollout method and to the study of solitaire. [sent-34, score-0.318]
</p><p>17 According to Parlett [10], solitaire came into existence when fortune-telling with cards gained popularity in the eighteenth century. [sent-40, score-0.471]
</p><p>18 Many variations of solitaire exist today, such as Klondike, Freecell, and Carpet. [sent-41, score-0.153]
</p><p>19 Klondike is played with a standard deck of cards: there are four suits (Spades, Clubs, Hearts, and Diamonds) each made up of thirteen cards ranked 1 through 13: Ace, 2, 3, . [sent-43, score-0.377]
</p><p>20 During the game, each card resides in one of thirteen stacks2 : the pile, the talon, four suit stacks and seven build stacks. [sent-47, score-0.864]
</p><p>21 Each suit stack corresponds to a particular suit and build stacks are labeled 1 through 7. [sent-48, score-1.168]
</p><p>22 At the beginning of the game, cards are dealt so that there is one card in the ﬁrst build stack, two cards in the second build stack, . [sent-49, score-1.344]
</p><p>23 The top card on each of the seven build stacks is turned face-up while the rest of the cards in the build stacks face down. [sent-53, score-1.247]
</p><p>24 The goal of the game is to move all cards into the suit stacks, aces ﬁrst, then two’s, and so on, with each suit stack evolving as an ordered increasing arrangement of cards of the same suit. [sent-56, score-1.812]
</p><p>25 In some solitaire literature, stacks are referred to as piles. [sent-59, score-0.237]
</p><p>26 We will study a version of solitaire in which the identity of each card at each position is revealed to the player at the beginning of the game but the usual Klondike rules still apply. [sent-60, score-0.814]
</p><p>27 This version is played by a number of serious solitaire players as a much more difﬁcult version than standard Klondike. [sent-61, score-0.238]
</p><p>28 We call this game thoughtful solitaire and now spell out the rules. [sent-63, score-0.351]
</p><p>29 On each turn, the player can move cards from one stack to another in the following manner: • Face-up cards of a build stack, called a card block, can be moved to the top of another build stack provided that the build stack to which the block is being moved accepts the block. [sent-64, score-3.802]
</p><p>30 Note that all face-up cards on the source stack must be moved together. [sent-65, score-1.005]
</p><p>31 After the move, these cards would then become the top cards of the stack to which they are moved, and their ordering is preserved. [sent-66, score-1.26]
</p><p>32 The card originally immediately beneath the card block, now the top card in its stack, is turned faceup. [sent-67, score-1.299]
</p><p>33 In the event that all cards in the source stack are moved, the player has an empty stack. [sent-68, score-1.055]
</p><p>34 3 • The top face-up card of a build stack can be moved to the top of a suit stack, provided that the suit stack accepts the card. [sent-69, score-2.3]
</p><p>35 • The top card of a suit stack can be moved to the top of a build stack, provided that the build stack accepts the card. [sent-70, score-2.267]
</p><p>36 • If the pile is not empty, a move can deal its top three cards to the talon, which maintains its cards in a ﬁrst-in-last-out order. [sent-71, score-0.911]
</p><p>37 If the pile becomes empty, the player can redeal all the cards on the talon back to the pile in one card move. [sent-72, score-1.206]
</p><p>38 • A card on the top of the talon can be moved to the top of a build stack or a suit stack, provided that the stack to which the card is being moved accepts the card. [sent-75, score-2.773]
</p><p>39 3  It would seem to some that since the identity of all cards is revealed to the player, whether a card is face-up or face-down is irrelevant. [sent-76, score-0.751]
</p><p>40 We retain this property of cards as it is still important in describing the rules and formulating our strategy. [sent-77, score-0.318]
</p><p>41 • A build stack can only accept an incoming card block if the top card on the build stack is adjacent to and braided with the bottom card of the block. [sent-78, score-2.848]
</p><p>42 A card is adjacent to another card of rank r if it is of rank r + 1. [sent-79, score-0.843]
</p><p>43 A card is braided with a card of suit s if its suit is of a color different from s. [sent-80, score-1.212]
</p><p>44 Additionally, if a build stack is empty, it can only accept a card block whose bottom card is a King. [sent-81, score-1.62]
</p><p>45 • A suit stack can only accept an incoming card of its corresponding suit. [sent-82, score-1.221]
</p><p>46 If a suit stack is empty, it can only accept an Ace. [sent-83, score-0.79]
</p><p>47 If it is not empty, the incoming card must be adjacent to the current top card of the suit stack. [sent-84, score-1.074]
</p><p>48 As stated earlier, the objective is to end up with all cards on suit stacks. [sent-85, score-0.498]
</p><p>49 3  Expert Play  We were introduced to thoughtful solitaire by a senior American mathematician (former president of the American Mathematical Society and indeed a famous combinatorialist) who had spent a number of years studying the game. [sent-87, score-0.247]
</p><p>50 He ﬁnds this version of solitaire much more thought-provoking and challenging than the standard Klondike. [sent-88, score-0.168]
</p><p>51 For instance, while the latter is usually played quickly, our esteemed expert averages about 20 minutes for each game of thoughtful solitaire. [sent-89, score-0.316]
</p><p>52 With this background, it is natural to wonder how well an optimal player can perform at thoughtful solitaire. [sent-92, score-0.203]
</p><p>53 If all cards are on suit stacks, declare victory and terminate. [sent-102, score-0.564]
</p><p>54 If the new card conﬁguration repeats a previous one, declare loss and terminate 4 . [sent-103, score-0.449]
</p><p>55 We will ﬁrst describe a heuristic strategy for selecting a legal move based on a card conﬁguration. [sent-106, score-0.754]
</p><p>56 1  A Heuristic Strategy  Our heuristic strategy is based on part of the Microsoft Windows Klondike scoring system: • The player starts the game with an initial score of 0. [sent-109, score-0.422]
</p><p>57 4  One straight-forward way to determine if a card conﬁguration has previously occurred is to store all encountered card conﬁgurations. [sent-110, score-0.828]
</p><p>58 Instead of doing so, however, we notice that there are three kinds of moves that could lead us into an inﬁnite loop: pile-talon moves, moves that could juggle a card block between two build stacks, and moves that could juggle a card block between a build stack and a suit stack. [sent-111, score-2.234]
</p><p>59 For the ﬁrst kind, we record if any card move other than a pile-talon move has occurred since the last redeal. [sent-114, score-0.684]
</p><p>60 • Whenever a card is moved from a build stack to a suit stack, the player gains 5 points. [sent-116, score-1.537]
</p><p>61 • Whenever a card is moved from the talon to a build stack, the player gains 5 points. [sent-117, score-0.909]
</p><p>62 • Whenever a card is moved from a suit stack to a build stack, the player loses 10 points. [sent-118, score-1.537]
</p><p>63 In our heuristic strategy, we assign a score to each card move based on the above scoring system. [sent-119, score-0.712]
</p><p>64 We assign the score zero to any moves not covered by the above rules. [sent-120, score-0.161]
</p><p>65 The player has incentive to move cards from the talon to a build stack and from a build stack to a suit stack. [sent-123, score-2.319]
</p><p>66 One important element that the heuristic fails to capture, however, is what move to make when multiple moves maximize the score. [sent-124, score-0.289]
</p><p>67 – If the move empties a stack, we assign this move a priority of 1. [sent-127, score-0.387]
</p><p>68 • If the card move is from the talon to a build stack, one of the following three assignments of priority occurs: – If the card being moved is not a King, we assign the move priority 1. [sent-128, score-1.683]
</p><p>69 – If the card being moved is a King and its matching Queen is in the pile, in the talon, in a suit stack, or is face-up in a build stack, we assign the move priority 1. [sent-129, score-1.103]
</p><p>70 – If the card being moved is a King and its matching Queen is face-down in a build stack, we assign the move priority -1. [sent-130, score-0.923]
</p><p>71 • For card moves not covered by the description above, we assign them a priority of 0. [sent-131, score-0.623]
</p><p>72 In addition to introducing priorities, we modify the Windows Klondike scoring system further by adding the following change: in a card move, if the card being moved is a King and its matching Queen is face-down in a build stack, we assign the move a score of 0. [sent-132, score-1.308]
</p><p>73 Note that given our assignment of scores and priorities, we practically disable card moves from a suit stack to a build stack. [sent-133, score-1.446]
</p><p>74 Because such moves have a negative score and a card move from the pile to the talon or from the talon to the pile has zero score and is almost always available, our strategy would always choose the pile-talon move over the moves from a suit stack to a build stack. [sent-134, score-2.365]
</p><p>75 In the case when multiple moves equal in priority maximize the score, we randomly select a move among them. [sent-135, score-0.309]
</p><p>76 The introduction of priority improves our original game-playing strategy in two ways: when we encounter a situation where we can move either one of two blocks on two separate build stacks atop the top card of a third build stack, we prefer moving the block whose stack has more face-down cards. [sent-136, score-1.754]
</p><p>77 Intuitively, such a move would strive to balance the number of face-down cards in stacks. [sent-137, score-0.453]
</p><p>78 The second way in which our prioritization scheme helps is that we are more deliberate in which King to select to enter an empty build stack. [sent-139, score-0.235]
</p><p>79 For instance, consider a situation where the King of Hearts and the King of Spades, both on the pile, are vying for an empty build stack and there is a face-up Queen of Diamonds on a build stack. [sent-140, score-0.922]
</p><p>80 We should certainly move the King of Spades to the empty build stack so that the Queen of Diamonds can be moved on top of it. [sent-141, score-1.054]
</p><p>81 2  Rollouts  Consider a strategy h that maps a card conﬁguration x to a legal move h(x). [sent-144, score-0.679]
</p><p>82 In this section, we will discuss the rollout method as a procedure for amplifying the performance of any strategy. [sent-146, score-0.331]
</p><p>83 Given a strategy h, this procedure generates an improved strategy h , called a rollout strategy. [sent-147, score-0.493]
</p><p>84 This idea was originally proposed by Tesauro and Galperin [13] and builds on the policy improvement algorithm of dynamic programming [1, 7]. [sent-148, score-0.167]
</p><p>85 A rollout strategy would make a move h (x), determined as follows: 1. [sent-151, score-0.534]
</p><p>86 For each legal move a, simulate the remainder of the game, taking move a and then employing strategy h thereafter. [sent-152, score-0.416]
</p><p>87 If any of these simulations leads to victory, choose one of them randomly and let h (x) be the corresponding move a5 . [sent-154, score-0.135]
</p><p>88 We can then iterate this procedure to generate a further improved strategy h that is a rollout strategy relative to h . [sent-157, score-0.493]
</p><p>89 5  Results  We implemented in Java the heuristic strategy and the procedure for computing rollout strategies. [sent-161, score-0.487]
</p><p>90 We randomly generated a large number of games and played them with our algorithms in an effort to approximate the success probability with the percentage of games actually won. [sent-163, score-0.151]
</p><p>91 For the original heuristic and 1 through 3 rollout iterations, we managed to achieve conﬁdence bounds of [-1. [sent-165, score-0.393]
</p><p>92 For 4 and 5 rollout iterations, due to time constraints, we simulated fewer games and obtained weaker conﬁdence bounds. [sent-168, score-0.361]
</p><p>93 Interestingly, however, after 5 rollout iterations, the resulting strategy wins almost twice as frequently as our esteemed mathematician. [sent-169, score-0.439]
</p><p>94 Player Human expert heuristic 1 rollout 2 rollouts 3 rollouts 4 rollouts 5 rollouts  6  Success Rate 36. [sent-173, score-0.704]
</p><p>95 13 seconds 1 minute 36 seconds 18 minutes 7 seconds 1 hour 45 minutes  99% Conﬁdence Bounds ±2. [sent-183, score-0.162]
</p><p>96 34%  Future Challenges  One limitation of our rollout method lies in its recursive nature. [sent-190, score-0.318]
</p><p>97 One possible direction for further exploration would be to compute a value function, mapping the state of the game to an estimate of whether or not the game can be won. [sent-192, score-0.221]
</p><p>98 Certainly, this function could not be represented exactly, but we could try approximating it in terms of a linear combination of features of the game state, as is common in the approximate dynamic programming literature [2]. [sent-193, score-0.143]
</p><p>99 We have also attempted proving an upper bound for the success rate of thoughtful solitaire by enumerating sets of initial card conﬁgurations that would force loss. [sent-194, score-0.7]
</p><p>100 If the success rate bound is improved and we are able to run additional rollout iterations, we may produce a veriﬁable near-optimal strategy for thoughtful solitaire. [sent-198, score-0.532]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stack', 0.577), ('card', 0.414), ('cards', 0.318), ('rollout', 0.318), ('suit', 0.18), ('solitaire', 0.153), ('build', 0.147), ('move', 0.135), ('talon', 0.129), ('moved', 0.11), ('player', 0.109), ('pile', 0.106), ('game', 0.104), ('klondike', 0.094), ('thoughtful', 0.094), ('stacks', 0.084), ('priority', 0.082), ('strategy', 0.081), ('policy', 0.081), ('moves', 0.079), ('heuristic', 0.075), ('king', 0.075), ('rollouts', 0.071), ('empty', 0.051), ('queen', 0.049), ('legal', 0.049), ('accepts', 0.047), ('games', 0.043), ('played', 0.039), ('iterations', 0.035), ('disable', 0.035), ('galperin', 0.035), ('priorities', 0.035), ('spades', 0.035), ('block', 0.035), ('tesauro', 0.035), ('declare', 0.035), ('assign', 0.035), ('top', 0.034), ('score', 0.034), ('accept', 0.033), ('bertsekas', 0.031), ('victory', 0.031), ('seconds', 0.03), ('minutes', 0.028), ('diamonds', 0.028), ('expert', 0.027), ('iterated', 0.026), ('tightest', 0.026), ('con', 0.026), ('success', 0.026), ('play', 0.025), ('improvement', 0.024), ('bertsimas', 0.024), ('braided', 0.024), ('esteemed', 0.024), ('hearts', 0.024), ('juggle', 0.024), ('parlett', 0.024), ('prioritization', 0.024), ('redeal', 0.024), ('originally', 0.023), ('winning', 0.022), ('dence', 0.021), ('dynamic', 0.021), ('thirteen', 0.02), ('backgammon', 0.02), ('scoring', 0.019), ('revealed', 0.019), ('seven', 0.019), ('heuristics', 0.019), ('java', 0.019), ('programming', 0.018), ('offers', 0.018), ('guration', 0.018), ('improves', 0.018), ('windows', 0.018), ('iteration', 0.018), ('win', 0.017), ('incoming', 0.017), ('wins', 0.016), ('players', 0.016), ('routing', 0.016), ('hour', 0.016), ('simulate', 0.016), ('adjacent', 0.015), ('version', 0.015), ('markov', 0.014), ('actions', 0.014), ('decision', 0.014), ('practically', 0.014), ('whenever', 0.013), ('management', 0.013), ('covered', 0.013), ('procedure', 0.013), ('ordering', 0.013), ('select', 0.013), ('bound', 0.013), ('state', 0.013), ('action', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="171-tfidf-1" href="./nips-2004-Solitaire%3A_Man_Versus_Machine.html">171 nips-2004-Solitaire: Man Versus Machine</a></p>
<p>Author: Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy</p><p>Abstract: In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 1</p><p>2 0.074698329 <a title="171-tfidf-2" href="./nips-2004-A_Direct_Formulation_for_Sparse_PCA_Using_Semidefinite_Programming.html">2 nips-2004-A Direct Formulation for Sparse PCA Using Semidefinite Programming</a></p>
<p>Author: Alexandre D'aspremont, Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: We examine the problem of approximating, in the Frobenius-norm sense, a positive, semideﬁnite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to ﬁnance. We use a modiﬁcation of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semideﬁnite programming based relaxation for our problem. 1</p><p>3 0.070520513 <a title="171-tfidf-3" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><p>4 0.053907841 <a title="171-tfidf-4" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>Author: Michael Bowling</p><p>Abstract: Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identiﬁable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). 1</p><p>5 0.051142946 <a title="171-tfidf-5" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>Author: David H. Stern, Thore Graepel, David MacKay</p><p>Abstract: Go is an ancient oriental game whose complexity has defeated attempts to automate it. We suggest using probability in a Bayesian sense to model the uncertainty arising from the vast complexity of the game tree. We present a simple conditional Markov random ﬁeld model for predicting the pointwise territory outcome of a game. The topology of the model reﬂects the spatial structure of the Go board. We describe a version of the Swendsen-Wang process for sampling from the model during learning and apply loopy belief propagation for rapid inference and prediction. The model is trained on several hundred records of professional games. Our experimental results indicate that the model successfully learns to predict territory despite its simplicity. 1</p><p>6 0.048490733 <a title="171-tfidf-6" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>7 0.044855375 <a title="171-tfidf-7" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>8 0.042425685 <a title="171-tfidf-8" href="./nips-2004-VDCBPI%3A_an_Approximate_Scalable_Algorithm_for_Large_POMDPs.html">202 nips-2004-VDCBPI: an Approximate Scalable Algorithm for Large POMDPs</a></p>
<p>9 0.03680503 <a title="171-tfidf-9" href="./nips-2004-A_Cost-Shaping_LP_for_Bellman_Error_Minimization_with_Performance_Guarantees.html">1 nips-2004-A Cost-Shaping LP for Bellman Error Minimization with Performance Guarantees</a></p>
<p>10 0.035723977 <a title="171-tfidf-10" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>11 0.031960189 <a title="171-tfidf-11" href="./nips-2004-Exploration-Exploitation_Tradeoffs_for_Experts_Algorithms_in_Reactive_Environments.html">65 nips-2004-Exploration-Exploitation Tradeoffs for Experts Algorithms in Reactive Environments</a></p>
<p>12 0.030091926 <a title="171-tfidf-12" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>13 0.02968272 <a title="171-tfidf-13" href="./nips-2004-Intrinsically_Motivated_Reinforcement_Learning.html">88 nips-2004-Intrinsically Motivated Reinforcement Learning</a></p>
<p>14 0.024333848 <a title="171-tfidf-14" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>15 0.022875881 <a title="171-tfidf-15" href="./nips-2004-Planning_for_Markov_Decision_Processes_with_Sparse_Stochasticity.html">147 nips-2004-Planning for Markov Decision Processes with Sparse Stochasticity</a></p>
<p>16 0.021903781 <a title="171-tfidf-16" href="./nips-2004-Optimal_Aggregation_of_Classifiers_and_Boosting_Maps_in_Functional_Magnetic_Resonance_Imaging.html">139 nips-2004-Optimal Aggregation of Classifiers and Boosting Maps in Functional Magnetic Resonance Imaging</a></p>
<p>17 0.020974668 <a title="171-tfidf-17" href="./nips-2004-Heuristics_for_Ordering_Cue_Search_in_Decision_Making.html">75 nips-2004-Heuristics for Ordering Cue Search in Decision Making</a></p>
<p>18 0.020346705 <a title="171-tfidf-18" href="./nips-2004-A_Three_Tiered_Approach_for_Articulated_Object_Action_Modeling_and_Recognition.html">13 nips-2004-A Three Tiered Approach for Articulated Object Action Modeling and Recognition</a></p>
<p>19 0.020103164 <a title="171-tfidf-19" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>20 0.018503519 <a title="171-tfidf-20" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.061), (1, -0.004), (2, 0.09), (3, -0.024), (4, -0.051), (5, 0.064), (6, -0.019), (7, -0.007), (8, -0.022), (9, -0.01), (10, -0.022), (11, -0.007), (12, -0.011), (13, 0.022), (14, 0.0), (15, 0.019), (16, 0.017), (17, 0.054), (18, -0.018), (19, -0.026), (20, 0.021), (21, -0.008), (22, 0.031), (23, -0.0), (24, 0.075), (25, -0.001), (26, -0.01), (27, -0.004), (28, -0.029), (29, -0.014), (30, 0.032), (31, 0.014), (32, 0.033), (33, 0.087), (34, 0.036), (35, 0.037), (36, -0.091), (37, 0.107), (38, 0.026), (39, 0.071), (40, -0.073), (41, -0.072), (42, -0.138), (43, 0.075), (44, -0.045), (45, -0.026), (46, -0.0), (47, 0.114), (48, -0.054), (49, -0.18)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9647401 <a title="171-lsi-1" href="./nips-2004-Solitaire%3A_Man_Versus_Machine.html">171 nips-2004-Solitaire: Man Versus Machine</a></p>
<p>Author: Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy</p><p>Abstract: In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 1</p><p>2 0.61334652 <a title="171-lsi-2" href="./nips-2004-New_Criteria_and_a_New_Algorithm_for_Learning_in_Multi-Agent_Systems.html">129 nips-2004-New Criteria and a New Algorithm for Learning in Multi-Agent Systems</a></p>
<p>Author: Rob Powers, Yoav Shoham</p><p>Abstract: We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justiﬁed than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a speciﬁed class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms. 1</p><p>3 0.56993431 <a title="171-lsi-3" href="./nips-2004-A_Direct_Formulation_for_Sparse_PCA_Using_Semidefinite_Programming.html">2 nips-2004-A Direct Formulation for Sparse PCA Using Semidefinite Programming</a></p>
<p>Author: Alexandre D'aspremont, Laurent E. Ghaoui, Michael I. Jordan, Gert R. Lanckriet</p><p>Abstract: We examine the problem of approximating, in the Frobenius-norm sense, a positive, semideﬁnite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to ﬁnance. We use a modiﬁcation of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semideﬁnite programming based relaxation for our problem. 1</p><p>4 0.43470877 <a title="171-lsi-4" href="./nips-2004-Multi-agent_Cooperation_in_Diverse_Population_Games.html">123 nips-2004-Multi-agent Cooperation in Diverse Population Games</a></p>
<p>Author: K. Wong, S. W. Lim, Z. Gao</p><p>Abstract: We consider multi-agent systems whose agents compete for resources by striving to be in the minority group. The agents adapt to the environment by reinforcement learning of the preferences of the policies they hold. Diversity of preferences of policies is introduced by adding random biases to the initial cumulative payoffs of their policies. We explain and provide evidence that agent cooperation becomes increasingly important when diversity increases. Analyses of these mechanisms yield excellent agreement with simulations over nine decades of data. 1</p><p>5 0.41751826 <a title="171-lsi-5" href="./nips-2004-Beat_Tracking_the_Graphical_Model_Way.html">29 nips-2004-Beat Tracking the Graphical Model Way</a></p>
<p>Author: Dustin Lang, Nando D. Freitas</p><p>Abstract: We present a graphical model for beat tracking in recorded music. Using a probabilistic graphical model allows us to incorporate local information and global smoothness constraints in a principled manner. We evaluate our model on a set of varied and difﬁcult examples, and achieve impressive results. By using a fast dual-tree algorithm for graphical model inference, our system runs in less time than the duration of the music being processed. 1</p><p>6 0.39907274 <a title="171-lsi-6" href="./nips-2004-Modelling_Uncertainty_in_the_Game_of_Go.html">122 nips-2004-Modelling Uncertainty in the Game of Go</a></p>
<p>7 0.37771165 <a title="171-lsi-7" href="./nips-2004-Convergence_and_No-Regret_in_Multiagent_Learning.html">48 nips-2004-Convergence and No-Regret in Multiagent Learning</a></p>
<p>8 0.36489415 <a title="171-lsi-8" href="./nips-2004-Harmonising_Chorales_by_Probabilistic_Inference.html">74 nips-2004-Harmonising Chorales by Probabilistic Inference</a></p>
<p>9 0.36031556 <a title="171-lsi-9" href="./nips-2004-Economic_Properties_of_Social_Networks.html">57 nips-2004-Economic Properties of Social Networks</a></p>
<p>10 0.32088655 <a title="171-lsi-10" href="./nips-2004-A_Cost-Shaping_LP_for_Bellman_Error_Minimization_with_Performance_Guarantees.html">1 nips-2004-A Cost-Shaping LP for Bellman Error Minimization with Performance Guarantees</a></p>
<p>11 0.31266904 <a title="171-lsi-11" href="./nips-2004-Approximately_Efficient_Online_Mechanism_Design.html">24 nips-2004-Approximately Efficient Online Mechanism Design</a></p>
<p>12 0.2899411 <a title="171-lsi-12" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>13 0.28705773 <a title="171-lsi-13" href="./nips-2004-Planning_for_Markov_Decision_Processes_with_Sparse_Stochasticity.html">147 nips-2004-Planning for Markov Decision Processes with Sparse Stochasticity</a></p>
<p>14 0.28221101 <a title="171-lsi-14" href="./nips-2004-Using_Machine_Learning_to_Break_Visual_Human_Interaction_Proofs_%28HIPs%29.html">199 nips-2004-Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</a></p>
<p>15 0.26916572 <a title="171-lsi-15" href="./nips-2004-Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem.html">126 nips-2004-Nearly Tight Bounds for the Continuum-Armed Bandit Problem</a></p>
<p>16 0.26735747 <a title="171-lsi-16" href="./nips-2004-Sparse_Coding_of_Natural_Images_Using_an_Overcomplete_Set_of_Limited_Capacity_Units.html">172 nips-2004-Sparse Coding of Natural Images Using an Overcomplete Set of Limited Capacity Units</a></p>
<p>17 0.2472319 <a title="171-lsi-17" href="./nips-2004-Coarticulation_in_Markov_Decision_Processes.html">39 nips-2004-Coarticulation in Markov Decision Processes</a></p>
<p>18 0.23931222 <a title="171-lsi-18" href="./nips-2004-Sampling_Methods_for_Unsupervised_Learning.html">158 nips-2004-Sampling Methods for Unsupervised Learning</a></p>
<p>19 0.23375884 <a title="171-lsi-19" href="./nips-2004-Experts_in_a_Markov_Decision_Process.html">64 nips-2004-Experts in a Markov Decision Process</a></p>
<p>20 0.22753936 <a title="171-lsi-20" href="./nips-2004-Machine_Learning_Applied_to_Perception%3A_Decision_Images_for_Gender_Classification.html">106 nips-2004-Machine Learning Applied to Perception: Decision Images for Gender Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.051), (15, 0.073), (17, 0.011), (26, 0.043), (31, 0.013), (33, 0.147), (35, 0.012), (50, 0.036), (71, 0.012), (95, 0.473)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80613375 <a title="171-lda-1" href="./nips-2004-Solitaire%3A_Man_Versus_Machine.html">171 nips-2004-Solitaire: Man Versus Machine</a></p>
<p>Author: Xiang Yan, Persi Diaconis, Paat Rusmevichientong, Benjamin V. Roy</p><p>Abstract: In this paper, we use the rollout method for policy improvement to analyze a version of Klondike solitaire. This version, sometimes called thoughtful solitaire, has all cards revealed to the player, but then follows the usual Klondike rules. A strategy that we establish, using iterated rollouts, wins about twice as many games on average as an expert human player does. 1</p><p>2 0.54227734 <a title="171-lda-2" href="./nips-2004-Edge_of_Chaos_Computation_in_Mixed-Mode_VLSI_-_A_Hard_Liquid.html">58 nips-2004-Edge of Chaos Computation in Mixed-Mode VLSI - A Hard Liquid</a></p>
<p>Author: Felix Schürmann, Karlheinz Meier, Johannes Schemmel</p><p>Abstract: Computation without stable states is a computing paradigm different from Turing’s and has been demonstrated for various types of simulated neural networks. This publication transfers this to a hardware implemented neural network. Results of a software implementation are reproduced showing that the performance peaks when the network exhibits dynamics at the edge of chaos. The liquid computing approach seems well suited for operating analog computing devices such as the used VLSI neural network. 1</p><p>3 0.49263293 <a title="171-lda-3" href="./nips-2004-Efficient_Out-of-Sample_Extension_of_Dominant-Set_Clusters.html">61 nips-2004-Efficient Out-of-Sample Extension of Dominant-Set Clusters</a></p>
<p>Author: Massimiliano Pavan, Marcello Pelillo</p><p>Abstract: Dominant sets are a new graph-theoretic concept that has proven to be relevant in pairwise data clustering problems, such as image segmentation. They generalize the notion of a maximal clique to edgeweighted graphs and have intriguing, non-trivial connections to continuous quadratic optimization and spectral-based grouping. We address the problem of grouping out-of-sample examples after the clustering process has taken place. This may serve either to drastically reduce the computational burden associated to the processing of very large data sets, or to efﬁciently deal with dynamic situations whereby data sets need to be updated continually. We show that the very notion of a dominant set offers a simple and efﬁcient way of doing this. Numerical experiments on various grouping problems show the effectiveness of the approach. 1</p><p>4 0.35063773 <a title="171-lda-4" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>Author: Dori Peleg, Ron Meir</p><p>Abstract: A novel linear feature selection algorithm is presented based on the global minimization of a data-dependent generalization error bound. Feature selection and scaling algorithms often lead to non-convex optimization problems, which in many previous approaches were addressed through gradient descent procedures that can only guarantee convergence to a local minimum. We propose an alternative approach, whereby the global solution of the non-convex optimization problem is derived via an equivalent optimization problem. Moreover, the convex optimization task is reduced to a conic quadratic programming problem for which efﬁcient solvers are available. Highly competitive numerical results on both artiﬁcial and real-world data sets are reported. 1</p><p>5 0.35024029 <a title="171-lda-5" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>Author: Ingo Steinwart, Clint Scovel</p><p>Abstract: We establish learning rates to the Bayes risk for support vector machines (SVMs) with hinge loss. In particular, for SVMs with Gaussian RBF kernels we propose a geometric condition for distributions which can be used to determine approximation properties of these kernels. Finally, we compare our methods with a recent paper of G. Blanchard et al.. 1</p><p>6 0.34996226 <a title="171-lda-6" href="./nips-2004-%E2%84%93%E2%82%80-norm_Minimization_for_Basis_Selection.html">207 nips-2004-ℓ₀-norm Minimization for Basis Selection</a></p>
<p>7 0.34959194 <a title="171-lda-7" href="./nips-2004-Variational_Minimax_Estimation_of_Discrete_Distributions_under_KL_Loss.html">204 nips-2004-Variational Minimax Estimation of Discrete Distributions under KL Loss</a></p>
<p>8 0.34956574 <a title="171-lda-8" href="./nips-2004-Self-Tuning_Spectral_Clustering.html">161 nips-2004-Self-Tuning Spectral Clustering</a></p>
<p>9 0.34909213 <a title="171-lda-9" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>10 0.34892991 <a title="171-lda-10" href="./nips-2004-Confidence_Intervals_for_the_Area_Under_the_ROC_Curve.html">45 nips-2004-Confidence Intervals for the Area Under the ROC Curve</a></p>
<p>11 0.34882975 <a title="171-lda-11" href="./nips-2004-Instance-Specific_Bayesian_Model_Averaging_for_Classification.html">86 nips-2004-Instance-Specific Bayesian Model Averaging for Classification</a></p>
<p>12 0.34816971 <a title="171-lda-12" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>13 0.34804517 <a title="171-lda-13" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>14 0.34797403 <a title="171-lda-14" href="./nips-2004-Hierarchical_Clustering_of_a_Mixture_Model.html">77 nips-2004-Hierarchical Clustering of a Mixture Model</a></p>
<p>15 0.34792334 <a title="171-lda-15" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>16 0.34750938 <a title="171-lda-16" href="./nips-2004-Limits_of_Spectral_Clustering.html">103 nips-2004-Limits of Spectral Clustering</a></p>
<p>17 0.34737581 <a title="171-lda-17" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>18 0.34683666 <a title="171-lda-18" href="./nips-2004-Learning_Hyper-Features_for_Visual_Identification.html">99 nips-2004-Learning Hyper-Features for Visual Identification</a></p>
<p>19 0.3468169 <a title="171-lda-19" href="./nips-2004-Generalization_Error_and_Algorithmic_Convergence_of_Median_Boosting.html">72 nips-2004-Generalization Error and Algorithmic Convergence of Median Boosting</a></p>
<p>20 0.34680861 <a title="171-lda-20" href="./nips-2004-Surface_Reconstruction_using_Learned_Shape_Models.html">179 nips-2004-Surface Reconstruction using Learned Shape Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
