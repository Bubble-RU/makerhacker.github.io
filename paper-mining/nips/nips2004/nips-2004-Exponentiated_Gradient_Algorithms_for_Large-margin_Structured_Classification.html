<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2004" href="../home/nips2004_home.html">nips2004</a> <a title="nips-2004-67" href="#">nips2004-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</h1>
<br/><p>Source: <a title="nips-2004-67-pdf" href="http://papers.nips.cc/paper/2677-exponentiated-gradient-algorithms-for-large-margin-structured-classification.pdf">pdf</a></p><p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>Reference: <a title="nips-2004-67-reference" href="../nips2004_reference/nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. [sent-11, score-0.34]
</p><p>2 Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. [sent-12, score-0.097]
</p><p>3 We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. [sent-13, score-0.387]
</p><p>4 The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. [sent-14, score-0.287]
</p><p>5 The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. [sent-15, score-0.974]
</p><p>6 For example x might be a word string and y a sequence of part of speech labels, or x might be a Markov random ﬁeld and y a labeling of x, or x might be a word string and y a parse of x. [sent-17, score-0.465]
</p><p>7 In these examples the number of possible labels y is exponential in the size of x. [sent-18, score-0.148]
</p><p>8 This paper presents a training algorithm for a general deﬁnition of structured classiﬁcation covering both Markov random ﬁelds and parsing. [sent-19, score-0.335]
</p><p>9 We assume that pairs x, y can be embedded in a linear feature space Φ(x, y), and that a predictive rule is determined by a direction (weight vector) w in that feature space. [sent-21, score-0.156]
</p><p>10 However, the case of structured labels has only recently been considered [2, 12, 3, 13]. [sent-24, score-0.339]
</p><p>11 The structured-label case takes into account the internal structure of y in the assignment of feature vectors, the computation of loss, and the deﬁnition and use of margins. [sent-25, score-0.176]
</p><p>12 Moreover, we assume that the feature vector for y and the loss for y are both linear in the individual bits of y. [sent-27, score-0.269]
</p><p>13 The starting-point for these methods is a  primal problem that has one constraint for each possible labeling y; or equivalently a dual problem where each y has an associated dual variable. [sent-30, score-0.655]
</p><p>14 We give a new training algorithm that relies on an exponential-family (Gibbs distribution) representation of structured objects. [sent-31, score-0.443]
</p><p>15 The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. [sent-32, score-0.287]
</p><p>16 The optimization method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient (EG) updates [7, 8] to quadratic programs (QPs). [sent-34, score-1.082]
</p><p>17 The algorithm uses multiplicative updates on dual parameters in the problem. [sent-36, score-0.513]
</p><p>18 In addition to their application to the structured-labels task, the EG updates lead to simple algorithms for optimizing “conventional” binary or multiclass SVM problems. [sent-37, score-0.229]
</p><p>19 [5] describe exponentiated gradient algorithms for SVMs, but for binary classiﬁcation in the “hard-margin” case, without slack variables. [sent-41, score-0.353]
</p><p>20 We show that the EG-QP algorithm converges signiﬁcantly faster than the rates shown in [5]. [sent-42, score-0.097]
</p><p>21 Multiplicative updates for SVMs are also described in [11], but unlike our method, the updates in [11] do not appear to factor in a way that allows algorithms for MRFs and WCFGs based on Gibbsdistribution representations. [sent-43, score-0.35]
</p><p>22 CRFs deﬁne a linear model for structured problems, in a similar way to the models in our work, and also rely on the efﬁcient computation of marginals in the training phase. [sent-45, score-0.364]
</p><p>23 The function L(x, y, y ) ˆ measures the loss when y is the true label for x, and y is a predicted label; typically, y is ˆ ˆ the label proposed by some function f (x). [sent-49, score-0.213]
</p><p>24 Given a parameter vector w ∈ Rd , we consider functions of the form fw (x) = arg max Φ(x, y), w . [sent-56, score-0.182]
</p><p>25 y∈G(x)  Given n independent training examples (xi , yi ) with the same distribution as (X, Y ), we will formalize a large-margin optimization problem that is a generalization of support vector methods for binary classiﬁers, and is essentially the same as the formulation in [12]. [sent-57, score-0.24]
</p><p>26 The optimal parameters are taken to minimize the following regularized empirical risk function: 1 2 w +C max (L(xi , yi , y) − mi,y (w)) y 2 + i where mi,y (w) = w, φ(xi , yi ) − w, φ(xi , y) is the “margin” on (i, y) and (z)+ = max{z, 0}. [sent-58, score-0.254]
</p><p>27 This optimization can be expressed as the primal problem in Figure 1. [sent-59, score-0.199]
</p><p>28 Following [12], the dual of this problem is also shown in Figure 1. [sent-60, score-0.224]
</p><p>29 We use the deﬁnitions Li,y = L(xi , yi , y), and Φi,y = Φ(xi , yi ) − Φ(xi , y). [sent-63, score-0.186]
</p><p>30 The constant C dictates the relative penalty for values of the slack variables i which are greater than 0. [sent-65, score-0.121]
</p><p>31 program F (¯ ) in the dual variables αi,y for all i = 1 . [sent-66, score-0.311]
</p><p>32 The dual variables α for each example are constrained to form a probability distribution over Y. [sent-70, score-0.311]
</p><p>33 1 Models for structured classiﬁcation The problems we are interested in concern structured labels, which have a natural decomposition into “parts”. [sent-72, score-0.534]
</p><p>34 Formally, we assume some countable set of parts, R. [sent-73, score-0.094]
</p><p>35 Thus R(x, y) is the set of parts belonging to a particular object. [sent-75, score-0.107]
</p><p>36 For convenience we deﬁne indicator variables I(x, y, r) which are 1 if r ∈ R(x, y), 0 otherwise. [sent-78, score-0.087]
</p><p>37 Example 1: Markov Random Fields (MRFs) In an MRF the space of labels G(x), and their underlying structure, can be represented by a graph. [sent-83, score-0.093]
</p><p>38 Each clique in the graph has a set of possible conﬁgurations: for example, if a particular clique contains vertices {v3 , v5 , v6 }, the set of possible conﬁgurations of this clique is Y3 × Y5 × Y6 . [sent-93, score-0.453]
</p><p>39 The feature vector representation φ(x, c, a) for each part can essentially track any characteristics of the assignment a for clique c, together with any features of the input x. [sent-96, score-0.415]
</p><p>40 A number of choices for the loss function l(x, y, (c, a)) are possible. [sent-97, score-0.111]
</p><p>41 For example, consider the Hamming loss used in [12], deﬁned as L(x, y, y ) = i Iyi =ˆi . [sent-98, score-0.111]
</p><p>42 To achieve this, ﬁrst assign each ˆ y vertex vi to a single one of the cliques in which it appears. [sent-99, score-0.153]
</p><p>43 Second, deﬁne l(x, y, (c, a)) to be the number of labels in the assignment (c, a) which are both incorrect and correspond to vertices which have been assigned to the clique c (note that assigning each vertex to a single clique avoids “double counting” of label errors). [sent-100, score-0.578]
</p><p>44 Deﬁnitions: αi,y (θ) = exp( r∈R(xi ,y)  Algorithm: ¯ • Choose initial values θ1 for the θi,r variables (these values can be arbitrary). [sent-108, score-0.087]
</p><p>45 i,r – Set wt = C  i,r∈R(xi ,yi )  φi,r −  i,r∈R(xi )  µt φi,r i,r  – For i = 1 . [sent-116, score-0.21]
</p><p>46 n, r ∈ R(xi ), t+1 t calculate updates θi,r = θi,r + ηC (li,r + wt , φi,r ) Output: Parameter values wT +1 Figure 2: The EG algorithm for structured problems. [sent-119, score-0.734]
</p><p>47 We use φi,r = φ(xi , r) and li,r = l(xi , yi , r). [sent-120, score-0.093]
</p><p>48 For convenience, we restrict the grammar to be in Chomsky-normal form, where all rules in the grammar are of the form A → B C or A → a , where A, B, C are non-terminal symbols, and a is some terminal symbol. [sent-122, score-0.12]
</p><p>49 The function R(x, y) maps a derivation y to the set of parts which it includes. [sent-134, score-0.143]
</p><p>50 In WCFGs φ(x, r) can be any function mapping a rule production and its position in the sentence x, to a feature vector. [sent-135, score-0.112]
</p><p>51 One example of a loss function would be to deﬁne l(x, y, r) to be 1 only if r’s non-terminal A is not seen spanning words s . [sent-136, score-0.158]
</p><p>52 3  EG updates for structured objects  We now consider an algorithm for computing α∗ = arg maxα∈∆ F (¯ ), where F (¯ ) is the ¯ α α ¯ dual form of the maximum margin problem, as in Figure 1. [sent-141, score-0.79]
</p><p>53 In particular, we are interested in the optimal values of the primal form parameters, which are related to α ∗ by w∗ = ¯ ∗ C i,y αi,y Φi,y . [sent-142, score-0.152]
</p><p>54 A key problem is that in many of our examples, the number of dual variables αi,y precludes dealing with these variables directly. [sent-143, score-0.398]
</p><p>55 For example, in the MRF case or the WCFG cases, the set G(x) is exponential in size, and the number of dual variables αi,y is therefore also exponential. [sent-144, score-0.366]
</p><p>56 We describe an algorithm that is efﬁcient for certain examples of structured objects such as MRFs or WCFGs. [sent-145, score-0.298]
</p><p>57 Instead of representing the αi,y variables explicitly, we will instead ¯ manipulate a vector θ of variables θi,r for i = 1 . [sent-146, score-0.237]
</p><p>58 Thus we have one of these “mini-dual” variables for each part seen in the training data. [sent-150, score-0.164]
</p><p>59 We now deﬁne the dual variables αi,y as a function of ¯ the vector θ, which takes the form of a Gibbs distribution: exp( r∈R(xi ,y) θi,r ) ¯ αi,y (θ) = . [sent-152, score-0.374]
</p><p>60 The algorithm deﬁnes a sequence of α ¯ ¯ values θ1 , θ2 , . [sent-154, score-0.098]
</p><p>61 The algorithm can be implemented efﬁciently, independently α of the dimensionality of α, provided that there is an efﬁcient algorithm for computing ¯ ¯ ¯ marginal terms µi,r = i,y αi,y (θ)I(xi , y, r) for all i = 1 . [sent-162, score-0.15]
</p><p>62 For example, in the WCFG case, the inside-outside algorithm can be used, provided that each part r is a context-free rule production, as described in Example 2 above. [sent-168, score-0.092]
</p><p>63 ¯ Note that the main storage requirements of the algorithm in Figure 2 concern the vector θ. [sent-170, score-0.157]
</p><p>64 This is a vector which has as many components as there are parts in the training set. [sent-171, score-0.207]
</p><p>65 In practice, the number of parts in the training data can become extremely large. [sent-172, score-0.144]
</p><p>66 Rather than explicitly storing the θ i,r variables, we can store a vector zt of the same dimensionality as wt . [sent-174, score-0.362]
</p><p>67 In the next section we show that the original algorithm converges for any choice of 1 ¯ initial values θ1 , so this restriction on θi,r should not be signiﬁcant. [sent-184, score-0.097]
</p><p>68 4  Exponentiated gradient (EG) updates for quadratic programs  We now prove convergence properties of the algorithm in Figure 2. [sent-185, score-0.498]
</p><p>69 We show that it is an instantiation of a general algorithm for optimizing quadratic programs (QPs), which relies on Exponentiated Gradient (EG) updates [7, 8]. [sent-186, score-0.429]
</p><p>70 In the general problem we assume a positive semi-deﬁnite matrix A ∈ Rm×m , and a vector b ∈ Rm , specifying a loss function Q(¯ ) = b α + 1 α A¯ . [sent-187, score-0.208]
</p><p>71 In the next section we give a proof of its convergence properties. [sent-199, score-0.102]
</p><p>72 The EG-QP algorithm can be used to ﬁnd the minimum of −F (¯ ), and hence the maximum α of the dual objective F (¯ ). [sent-200, score-0.276]
</p><p>73 Consider the sequence α(θ in Figure 2, and the sequence α1 . [sent-206, score-0.092]
</p><p>74 Inputs: A positive semi-deﬁnite matrix A, and a vector b, specifying a loss function Q(¯ ) = b · α + 1 α A¯ . [sent-214, score-0.174]
</p><p>75 ¯ Figure 3: The EG-QP algorithm for quadratic programs. [sent-224, score-0.127]
</p><p>76 We can write F (α) = C i,y αi,y Li,y − 2 C 2 ¯ i Φ(xi , yi ) − i,y αi,y Φ(xi , y) . [sent-226, score-0.093]
</p><p>77 It t α ¯ follows that ∂F (i,y ) = CLi,y + C Φ(xi , y), wt = C r∈R(xi ,y) li,r + φi,r , wt where as ∂α t before wt = C( i Φ(xi , yi ) − i,y αi,y Φ(xi , y)). [sent-227, score-0.723]
</p><p>78 The rest of the proof proceeds by induction; due to space constraints we give a sketch of the proof here. [sent-228, score-0.114]
</p><p>79 This follows immediately ¯ ¯ ¯ ¯ ¯ ¯ from the deﬁnitions of the mappings α(θt ) → α(θt+1 ) and αt → αt+1 in the two algo¯ ¯ ¯ ¯ ¯ ¯ ∂F (αt ) ¯ t rithms, together with the identities si,y = − ∂αi,y = −C r∈R(xi ,y) (li,r + φi,r , wt )  t+1 t and θi,r − θi,r = ηC (li,r + φi,r , wt ). [sent-230, score-0.42]
</p><p>80 1  Convergence of the exponentiated gradient QP algorithm  The following theorem shows how the optimization algorithm converges to an optimal solution. [sent-232, score-0.562]
</p><p>81 The theorem compares the value of the objective function for the algorithm’s vector αt to the value for a comparison vector u ∈ ∆. [sent-233, score-0.173]
</p><p>82 ) The convergence result is in terms of several properties of the algorithm and the comparison vector u. [sent-235, score-0.16]
</p><p>83 Two other key parameters ¯ ¯ are λ, the largest eigenvalue of the positive semideﬁnite symmetric matrix A, and α α B = max max ( Q(¯ ))i − min ( Q(¯ ))i ≤ 2 n max |Aij | + max |bi | . [sent-245, score-0.272]
</p><p>84 Deﬁne (i) Q(¯ ) as the segment of the gradient vector corα responding to the component αi of α, and deﬁne the random variable Xi,t , satisfying ¯ ¯ Pr Xi,t = −  α (i) Q(¯  t  )  j  = αi,j . [sent-253, score-0.153]
</p><p>85 The second part of the proof of the theorem involves bounding this variance in terms of the loss. [sent-260, score-0.144]
</p><p>86 The following lemma relies on the fact that this variance is, to ﬁrst order, the decrease in the quadratic loss, and that the second order term in the Taylor series expansion of the loss is small compared to the variance, provided the steps are not too large. [sent-261, score-0.321]
</p><p>87 We shall work in the ¯t be the exponential parameters at step t, so that the exponential parameter space: let θ ¯ ¯ ¯t updates are θt+1 = θt − η Q(¯ t ), and the QP variables satisfy αi = σ(θi ). [sent-264, score-0.372]
</p><p>88 5  Experiments  We compared an online1 version of the Exponentiated Gradient algorithm with the factored Sequential Minimal Optimization (SMO) algorithm in [12] on a sequence segmentation task. [sent-274, score-0.15]
</p><p>89 Each word is labelled by 9 possible tags (beginning of one of the four entity types, continuation of one of the types, or not-an-entity). [sent-277, score-0.206]
</p><p>90 We trained a ﬁrst-order Markov chain over the tags, In the online algorithm we calculate marginal terms, and updates to the w t parameters, one training example at a time. [sent-278, score-0.361]
</p><p>91 where our cliques are just the nodes for the tag of each word and edges between tags of consecutive words. [sent-286, score-0.245]
</p><p>92 The feature vector for each node assignment consists of the word itself, its capitalization and morphological features, etc. [sent-287, score-0.264]
</p><p>93 Likewise, the feature vector for each edge assignment consists of the two words and their features as well as surrounding words. [sent-289, score-0.243]
</p><p>94 Figure 4 shows the growth of the dual objective function after each pass through the data for SMO and EG, for several settings of the learning rate η and the initial setting of the θ parameters. [sent-290, score-0.224]
</p><p>95 These preliminary results suggest that a hybrid algorithm could get the beneﬁts of both, by starting out with several SMO updates and then switching to EG. [sent-292, score-0.227]
</p><p>96 The key issue is to switch from the marginal µ representation SMO maintains to the Gibbs θ representation that EG uses. [sent-293, score-0.13]
</p><p>97 dividing edge marginals by node marginals in this case) and then letting θ’s be the logs of the conditional probabilities. [sent-296, score-0.162]
</p><p>98 On the algorithmic implementation of multiclass kernel-based vector machines. [sent-315, score-0.117]
</p><p>99 Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. [sent-339, score-0.101]
</p><p>100 Support vector machine learning for interdependent and structured output spaces. [sent-362, score-0.309]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eg', 0.303), ('smo', 0.273), ('structured', 0.246), ('xi', 0.24), ('exponentiated', 0.229), ('dual', 0.224), ('wt', 0.21), ('updates', 0.175), ('primal', 0.152), ('mrfs', 0.138), ('wcfgs', 0.138), ('clique', 0.137), ('var', 0.137), ('loss', 0.111), ('parts', 0.107), ('qps', 0.103), ('gibbs', 0.097), ('yi', 0.093), ('labels', 0.093), ('gradient', 0.09), ('zt', 0.089), ('variables', 0.087), ('kivinen', 0.082), ('mrf', 0.082), ('marginals', 0.081), ('markov', 0.076), ('quadratic', 0.075), ('assignment', 0.072), ('tags', 0.072), ('string', 0.071), ('elds', 0.071), ('eta', 0.069), ('tsochantaridis', 0.069), ('wcfg', 0.069), ('lemma', 0.069), ('cliques', 0.069), ('word', 0.068), ('max', 0.068), ('relies', 0.066), ('entity', 0.066), ('vector', 0.063), ('multiplicative', 0.062), ('feature', 0.061), ('programs', 0.061), ('countable', 0.06), ('constituent', 0.06), ('grammar', 0.06), ('grammars', 0.06), ('spans', 0.059), ('proof', 0.057), ('exp', 0.055), ('exponential', 0.055), ('labeling', 0.055), ('multiclass', 0.054), ('rd', 0.053), ('algorithm', 0.052), ('arg', 0.051), ('calculate', 0.051), ('label', 0.051), ('mcallester', 0.051), ('production', 0.051), ('expectations', 0.051), ('ij', 0.05), ('gurations', 0.047), ('optimization', 0.047), ('theorem', 0.047), ('words', 0.047), ('vertex', 0.046), ('sequence', 0.046), ('marginal', 0.046), ('derivations', 0.046), ('parse', 0.046), ('crfs', 0.046), ('convergence', 0.045), ('converges', 0.045), ('taskar', 0.044), ('parsing', 0.044), ('qp', 0.044), ('internal', 0.043), ('ui', 0.043), ('concern', 0.042), ('representation', 0.042), ('margin', 0.042), ('nitions', 0.042), ('vertices', 0.042), ('st', 0.041), ('classi', 0.04), ('part', 0.04), ('vi', 0.038), ('training', 0.037), ('calculated', 0.036), ('consecutive', 0.036), ('cristianini', 0.036), ('derivation', 0.036), ('ne', 0.035), ('rm', 0.035), ('ik', 0.035), ('assume', 0.034), ('slack', 0.034), ('bartlett', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="67-tfidf-1" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>2 0.26750091 <a title="67-tfidf-2" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>Author: Koji Tsuda, Gunnar Rätsch, Manfred K. Warmuth</p><p>Abstract: We address the problem of learning a symmetric positive deﬁnite matrix. The central issue is to design parameter updates that preserve positive deﬁniteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: On-line learning with a simple square loss and ﬁnding a symmetric positive deﬁnite matrix subject to symmetric linear constraints. The updates generalize the Exponentiated Gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive deﬁnite matrix of trace one instead of a probability vector (which in this context is a diagonal positive deﬁnite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive deﬁniteness. Most importantly, we show how the analysis of each algorithm generalizes to the non-diagonal case. We apply both new algorithms, called the Matrix Exponentiated Gradient (MEG) update and DeﬁniteBoost, to learn a kernel matrix from distance measurements.</p><p>3 0.2466252 <a title="67-tfidf-3" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>4 0.17057188 <a title="67-tfidf-4" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>5 0.13682504 <a title="67-tfidf-5" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>Author: Saharon Rosset, Robert Tibshirani, Ji Zhu, Trevor J. Hastie</p><p>Abstract: In this paper we argue that the choice of the SVM cost parameter can be critical. We then derive an algorithm that can ﬁt the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as ﬁtting one SVM model. 1</p><p>6 0.12625133 <a title="67-tfidf-6" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>7 0.12572968 <a title="67-tfidf-7" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>8 0.12525909 <a title="67-tfidf-8" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>9 0.12499857 <a title="67-tfidf-9" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>10 0.11931521 <a title="67-tfidf-10" href="./nips-2004-Boosting_on_Manifolds%3A_Adaptive_Regularization_of_Base_Classifiers.html">32 nips-2004-Boosting on Manifolds: Adaptive Regularization of Base Classifiers</a></p>
<p>11 0.11722038 <a title="67-tfidf-11" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>12 0.11281808 <a title="67-tfidf-12" href="./nips-2004-Maximum-Margin_Matrix_Factorization.html">113 nips-2004-Maximum-Margin Matrix Factorization</a></p>
<p>13 0.10390649 <a title="67-tfidf-13" href="./nips-2004-On_Semi-Supervised_Classification.html">136 nips-2004-On Semi-Supervised Classification</a></p>
<p>14 0.099513143 <a title="67-tfidf-14" href="./nips-2004-Distributed_Information_Regularization_on_Graphs.html">54 nips-2004-Distributed Information Regularization on Graphs</a></p>
<p>15 0.098311342 <a title="67-tfidf-15" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<p>16 0.098256677 <a title="67-tfidf-16" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>17 0.097777247 <a title="67-tfidf-17" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>18 0.096253321 <a title="67-tfidf-18" href="./nips-2004-Maximum_Margin_Clustering.html">115 nips-2004-Maximum Margin Clustering</a></p>
<p>19 0.09303233 <a title="67-tfidf-19" href="./nips-2004-Computing_regularization_paths_for_learning_multiple_kernels.html">42 nips-2004-Computing regularization paths for learning multiple kernels</a></p>
<p>20 0.090040118 <a title="67-tfidf-20" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2004_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.31), (1, 0.102), (2, 0.077), (3, 0.16), (4, 0.119), (5, -0.061), (6, 0.047), (7, 0.058), (8, -0.106), (9, 0.071), (10, 0.159), (11, 0.034), (12, -0.154), (13, 0.144), (14, 0.095), (15, 0.125), (16, -0.045), (17, 0.137), (18, -0.074), (19, 0.052), (20, 0.022), (21, 0.054), (22, -0.053), (23, -0.023), (24, -0.098), (25, -0.008), (26, 0.039), (27, -0.038), (28, -0.142), (29, -0.076), (30, -0.078), (31, -0.017), (32, 0.008), (33, 0.079), (34, -0.027), (35, -0.069), (36, 0.02), (37, -0.064), (38, 0.027), (39, 0.039), (40, -0.045), (41, 0.098), (42, -0.015), (43, -0.02), (44, 0.162), (45, -0.053), (46, 0.053), (47, -0.006), (48, -0.065), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95907587 <a title="67-lsi-1" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>2 0.72106951 <a title="67-lsi-2" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>Author: Jinbo Bi, Tong Zhang</p><p>Abstract: This paper investigates a new learning model in which the input data is corrupted with noise. We present a general statistical framework to tackle this problem. Based on the statistical reasoning, we propose a novel formulation of support vector classiﬁcation, which allows uncertainty in input data. We derive an intuitive geometric interpretation of the proposed formulation, and develop algorithms to efﬁciently solve it. Empirical results are included to show that the newly formed method is superior to the standard SVM for problems with noisy input. 1</p><p>3 0.68263853 <a title="67-lsi-3" href="./nips-2004-Matrix_Exponential_Gradient_Updates_for_On-line_Learning_and_Bregman_Projection.html">110 nips-2004-Matrix Exponential Gradient Updates for On-line Learning and Bregman Projection</a></p>
<p>Author: Koji Tsuda, Gunnar Rätsch, Manfred K. Warmuth</p><p>Abstract: We address the problem of learning a symmetric positive deﬁnite matrix. The central issue is to design parameter updates that preserve positive deﬁniteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: On-line learning with a simple square loss and ﬁnding a symmetric positive deﬁnite matrix subject to symmetric linear constraints. The updates generalize the Exponentiated Gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive deﬁnite matrix of trace one instead of a probability vector (which in this context is a diagonal positive deﬁnite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive deﬁniteness. Most importantly, we show how the analysis of each algorithm generalizes to the non-diagonal case. We apply both new algorithms, called the Matrix Exponentiated Gradient (MEG) update and DeﬁniteBoost, to learn a kernel matrix from distance measurements.</p><p>4 0.64542937 <a title="67-lsi-4" href="./nips-2004-Conditional_Random_Fields_for_Object_Recognition.html">44 nips-2004-Conditional Random Fields for Object Recognition</a></p>
<p>Author: Ariadna Quattoni, Michael Collins, Trevor Darrell</p><p>Abstract: We present a discriminative part-based approach for the recognition of object classes from unsegmented cluttered scenes. Objects are modeled as ﬂexible constellations of parts conditioned on local observations found by an interest operator. For each object class the probability of a given assignment of parts to local features is modeled by a Conditional Random Field (CRF). We propose an extension of the CRF framework that incorporates hidden variables and combines class conditional CRFs into a uniﬁed framework for part-based object recognition. The parameters of the CRF are estimated in a maximum likelihood framework and recognition proceeds by ﬁnding the most likely class under our model. The main advantage of the proposed CRF framework is that it allows us to relax the assumption of conditional independence of the observed data (i.e. local features) often used in generative approaches, an assumption that might be too restrictive for a considerable number of object classes.</p><p>5 0.64151436 <a title="67-lsi-5" href="./nips-2004-A_Second_Order_Cone_programming_Formulation_for_Classifying_Missing_Data.html">11 nips-2004-A Second Order Cone programming Formulation for Classifying Missing Data</a></p>
<p>Author: Chiranjib Bhattacharyya, Pannagadatta K. Shivaswamy, Alex J. Smola</p><p>Abstract: We propose a convex optimization based strategy to deal with uncertainty in the observations of a classiﬁcation problem. We assume that instead of a sample (xi , yi ) a distribution over (xi , yi ) is speciﬁed. In particular, we derive a robust formulation when the distribution is given by a normal distribution. It leads to Second Order Cone Programming formulation. Our method is applied to the problem of missing data, where it outperforms direct imputation. 1</p><p>6 0.59901655 <a title="67-lsi-6" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<p>7 0.57028729 <a title="67-lsi-7" href="./nips-2004-Conditional_Models_of_Identity_Uncertainty_with_Application_to_Noun_Coreference.html">43 nips-2004-Conditional Models of Identity Uncertainty with Application to Noun Coreference</a></p>
<p>8 0.53032452 <a title="67-lsi-8" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>9 0.52717954 <a title="67-lsi-9" href="./nips-2004-An_Application_of_Boosting_to_Graph_Classification.html">19 nips-2004-An Application of Boosting to Graph Classification</a></p>
<p>10 0.51110643 <a title="67-lsi-10" href="./nips-2004-Maximal_Margin_Labeling_for_Multi-Topic_Text_Categorization.html">111 nips-2004-Maximal Margin Labeling for Multi-Topic Text Categorization</a></p>
<p>11 0.50127041 <a title="67-lsi-11" href="./nips-2004-Comparing_Beliefs%2C_Surveys%2C_and_Random_Walks.html">41 nips-2004-Comparing Beliefs, Surveys, and Random Walks</a></p>
<p>12 0.47832912 <a title="67-lsi-12" href="./nips-2004-A_Method_for_Inferring_Label_Sampling_Mechanisms_in_Semi-Supervised_Learning.html">9 nips-2004-A Method for Inferring Label Sampling Mechanisms in Semi-Supervised Learning</a></p>
<p>13 0.47691265 <a title="67-lsi-13" href="./nips-2004-A_Topographic_Support_Vector_Machine%3A_Classification_Using_Local_Label_Configurations.html">14 nips-2004-A Topographic Support Vector Machine: Classification Using Local Label Configurations</a></p>
<p>14 0.46096694 <a title="67-lsi-14" href="./nips-2004-A_Feature_Selection_Algorithm_Based_on_the_Global_Minimization_of_a_Generalization_Error_Bound.html">3 nips-2004-A Feature Selection Algorithm Based on the Global Minimization of a Generalization Error Bound</a></p>
<p>15 0.44323969 <a title="67-lsi-15" href="./nips-2004-Kernel_Methods_for_Implicit_Surface_Modeling.html">92 nips-2004-Kernel Methods for Implicit Surface Modeling</a></p>
<p>16 0.44210446 <a title="67-lsi-16" href="./nips-2004-Distributed_Occlusion_Reasoning_for_Tracking_with_Nonparametric_Belief_Propagation.html">55 nips-2004-Distributed Occlusion Reasoning for Tracking with Nonparametric Belief Propagation</a></p>
<p>17 0.44189805 <a title="67-lsi-17" href="./nips-2004-Adaptive_Manifold_Learning.html">17 nips-2004-Adaptive Manifold Learning</a></p>
<p>18 0.4344959 <a title="67-lsi-18" href="./nips-2004-Rate-_and_Phase-coded_Autoassociative_Memory.html">151 nips-2004-Rate- and Phase-coded Autoassociative Memory</a></p>
<p>19 0.43250561 <a title="67-lsi-19" href="./nips-2004-The_Laplacian_PDF_Distance%3A_A_Cost_Function_for_Clustering_in_a_Kernel_Feature_Space.html">188 nips-2004-The Laplacian PDF Distance: A Cost Function for Clustering in a Kernel Feature Space</a></p>
<p>20 0.41406435 <a title="67-lsi-20" href="./nips-2004-Learning_Preferences_for_Multiclass_Problems.html">100 nips-2004-Learning Preferences for Multiclass Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2004_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.134), (15, 0.139), (26, 0.075), (31, 0.024), (32, 0.255), (33, 0.184), (35, 0.01), (39, 0.04), (50, 0.035), (53, 0.014), (87, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87231344 <a title="67-lda-1" href="./nips-2004-A_Large_Deviation_Bound_for_the_Area_Under_the_ROC_Curve.html">7 nips-2004-A Large Deviation Bound for the Area Under the ROC Curve</a></p>
<p>Author: Shivani Agarwal, Thore Graepel, Ralf Herbrich, Dan Roth</p><p>Abstract: The area under the ROC curve (AUC) has been advocated as an evaluation criterion for the bipartite ranking problem. We study large deviation properties of the AUC; in particular, we derive a distribution-free large deviation bound for the AUC which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on an independent test sequence. A comparison of our result with a corresponding large deviation result for the classiﬁcation error rate suggests that the test sample size required to obtain an -accurate estimate of the expected accuracy of a ranking function with δ-conﬁdence is larger than that required to obtain an -accurate estimate of the expected error rate of a classiﬁcation function with the same conﬁdence. A simple application of the union bound allows the large deviation bound to be extended to learned ranking functions chosen from ﬁnite function classes. 1</p><p>2 0.86619931 <a title="67-lda-2" href="./nips-2004-Analysis_of_a_greedy_active_learning_strategy.html">23 nips-2004-Analysis of a greedy active learning strategy</a></p>
<p>Author: Sanjoy Dasgupta</p><p>Abstract: We abstract out the core search problem of active learning schemes, to better understand the extent to which adaptive labeling can improve sample complexity. We give various upper and lower bounds on the number of labels which need to be queried, and we prove that a popular greedy active learning rule is approximately as good as any other strategy for minimizing this number of labels. 1</p><p>same-paper 3 0.84418601 <a title="67-lda-3" href="./nips-2004-Exponentiated_Gradient_Algorithms_for_Large-margin_Structured_Classification.html">67 nips-2004-Exponentiated Gradient Algorithms for Large-margin Structured Classification</a></p>
<p>Author: Peter L. Bartlett, Michael Collins, Ben Taskar, David A. McAllester</p><p>Abstract: We consider the problem of structured classiﬁcation, where the task is to predict a label y from an input x, and y has meaningful internal structure. Our framework includes supervised training of Markov random ﬁelds and weighted context-free grammars as special cases. We describe an algorithm that solves the large-margin optimization problem deﬁned in [12], using an exponential-family (Gibbs distribution) representation of structured objects. The algorithm is efﬁcient—even in cases where the number of labels y is exponential in size—provided that certain expectations under Gibbs distributions can be calculated efﬁciently. The method for structured labels relies on a more general result, speciﬁcally the application of exponentiated gradient updates [7, 8] to quadratic programs. 1</p><p>4 0.75569642 <a title="67-lda-4" href="./nips-2004-The_Power_of_Selective_Memory%3A_Self-Bounded_Learning_of_Prediction_Suffix_Trees.html">189 nips-2004-The Power of Selective Memory: Self-Bounded Learning of Prediction Suffix Trees</a></p>
<p>Author: Ofer Dekel, Shai Shalev-shwartz, Yoram Singer</p><p>Abstract: Prediction sufﬁx trees (PST) provide a popular and effective tool for tasks such as compression, classiﬁcation, and language modeling. In this paper we take a decision theoretic view of PSTs for the task of sequence prediction. Generalizing the notion of margin to PSTs, we present an online PST learning algorithm and derive a loss bound for it. The depth of the PST generated by this algorithm scales linearly with the length of the input. We then describe a self-bounded enhancement of our learning algorithm which automatically grows a bounded-depth PST. We also prove an analogous mistake-bound for the self-bounded algorithm. The result is an efﬁcient algorithm that neither relies on a-priori assumptions on the shape or maximal depth of the target PST nor does it require any parameters. To our knowledge, this is the ﬁrst provably-correct PST learning algorithm which generates a bounded-depth PST while being competitive with any ﬁxed PST determined in hindsight. 1</p><p>5 0.74364138 <a title="67-lda-5" href="./nips-2004-Worst-Case_Analysis_of_Selective_Sampling_for_Linear-Threshold_Algorithms.html">206 nips-2004-Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Claudio Gentile, Luca Zaniboni</p><p>Abstract: We provide a worst-case analysis of selective sampling algorithms for learning linear threshold functions. The algorithms considered in this paper are Perceptron-like algorithms, i.e., algorithms which can be efﬁciently run in any reproducing kernel Hilbert space. Our algorithms exploit a simple margin-based randomized rule to decide whether to query the current label. We obtain selective sampling algorithms achieving on average the same bounds as those proven for their deterministic counterparts, but using much fewer labels. We complement our theoretical ﬁndings with an empirical comparison on two text categorization tasks. The outcome of these experiments is largely predicted by our theoretical results: Our selective sampling algorithms tend to perform as good as the algorithms receiving the true label after each classiﬁcation, while observing in practice substantially fewer labels. 1</p><p>6 0.73540539 <a title="67-lda-6" href="./nips-2004-Assignment_of_Multiplicative_Mixtures_in_Natural_Images.html">25 nips-2004-Assignment of Multiplicative Mixtures in Natural Images</a></p>
<p>7 0.73110914 <a title="67-lda-7" href="./nips-2004-Non-Local_Manifold_Tangent_Learning.html">131 nips-2004-Non-Local Manifold Tangent Learning</a></p>
<p>8 0.72527146 <a title="67-lda-8" href="./nips-2004-Semi-parametric_Exponential_Family_PCA.html">163 nips-2004-Semi-parametric Exponential Family PCA</a></p>
<p>9 0.7252686 <a title="67-lda-9" href="./nips-2004-Following_Curved_Regularized_Optimization_Solution_Paths.html">70 nips-2004-Following Curved Regularized Optimization Solution Paths</a></p>
<p>10 0.72433037 <a title="67-lda-10" href="./nips-2004-Learning_first-order_Markov_models_for_control.html">102 nips-2004-Learning first-order Markov models for control</a></p>
<p>11 0.72205973 <a title="67-lda-11" href="./nips-2004-Message_Errors_in_Belief_Propagation.html">116 nips-2004-Message Errors in Belief Propagation</a></p>
<p>12 0.72199309 <a title="67-lda-12" href="./nips-2004-Support_Vector_Classification_with_Input_Data_Uncertainty.html">178 nips-2004-Support Vector Classification with Input Data Uncertainty</a></p>
<p>13 0.7208209 <a title="67-lda-13" href="./nips-2004-Semi-supervised_Learning_with_Penalized_Probabilistic_Clustering.html">167 nips-2004-Semi-supervised Learning with Penalized Probabilistic Clustering</a></p>
<p>14 0.72001964 <a title="67-lda-14" href="./nips-2004-A_Generalized_Bradley-Terry_Model%3A_From_Group_Competition_to_Individual_Skill.html">4 nips-2004-A Generalized Bradley-Terry Model: From Group Competition to Individual Skill</a></p>
<p>15 0.71970773 <a title="67-lda-15" href="./nips-2004-Synergies_between_Intrinsic_and_Synaptic_Plasticity_in_Individual_Model_Neurons.html">181 nips-2004-Synergies between Intrinsic and Synaptic Plasticity in Individual Model Neurons</a></p>
<p>16 0.71947533 <a title="67-lda-16" href="./nips-2004-Spike_Sorting%3A_Bayesian_Clustering_of_Non-Stationary_Data.html">174 nips-2004-Spike Sorting: Bayesian Clustering of Non-Stationary Data</a></p>
<p>17 0.71903151 <a title="67-lda-17" href="./nips-2004-Fast_Rates_to_Bayes_for_Kernel_Machines.html">69 nips-2004-Fast Rates to Bayes for Kernel Machines</a></p>
<p>18 0.71855301 <a title="67-lda-18" href="./nips-2004-The_Entire_Regularization_Path_for_the_Support_Vector_Machine.html">187 nips-2004-The Entire Regularization Path for the Support Vector Machine</a></p>
<p>19 0.71850222 <a title="67-lda-19" href="./nips-2004-Multiple_Alignment_of_Continuous_Time_Series.html">124 nips-2004-Multiple Alignment of Continuous Time Series</a></p>
<p>20 0.71674567 <a title="67-lda-20" href="./nips-2004-Semi-Markov_Conditional_Random_Fields_for_Information_Extraction.html">162 nips-2004-Semi-Markov Conditional Random Fields for Information Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
